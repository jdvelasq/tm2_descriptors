Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Fuzzy Hindi wordnet and word sense disambiguation using fuzzy graph connectivity measures,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041448357&doi=10.1145%2f2790079&partnerID=40&md5=33780ddb091c711c8511bd10dac6db9b,"In this article, we propose Fuzzy Hindi WordNet, which is an extended version of Hindi WordNet. The proposed idea of fuzzy relations and their role in modeling Fuzzy Hindi WordNet is explained. We mathematically define fuzzy relations and the composition of these fuzzy relations for this extended version. We show that the concept of composition of fuzzy relations can be used to infer a relation between two words that otherwise are not directly related in Hindi WordNet. Then we propose fuzzy graph connectivity measures that include both local and global measures. These measures are used in determining the significance of a concept (which is represented as a vertex in the fuzzy graph) in a specific context. Finally, we show how these extended measures solve the problem of word sense disambiguation (WSD) effectively, which is useful in many natural language processing applications to improve their performance. Experiments on standard sense tagged corpus for WSD show better results when Fuzzy Hindi WordNet is used in place of Hindi WordNet. © 2015 ACM",Centrality; Fuzzy graph; Fuzzy logic; Hindi WordNet; Lexicon; Word sense disambiguation,Fuzzy sets; Fuzzy systems; Graph theory; Natural language processing systems; Ontology; Centrality; Fuzzy graph; Hindi WordNet; Lexicon; Word Sense Disambiguation; Fuzzy logic
Collective web-based parenthetical translation extraction using markov logic networks,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057535778&doi=10.1145%2f2794399&partnerID=40&md5=834a01fb352a69bed3f90ee84b88c796,"Parenthetical translations are translations of terms in otherwise monolingual text that appear inside parentheses. Parenthetical translations extraction (PTE) is the task of extracting parenthetical translations from natural language documents. One of the main difficulties in PTE is to detect the left boundary of the translated term in preparenthetical text. In this article, we propose a collective approach that employs Markov logic to model multiple constraints used in the PTE task. We show how various constraints can be formulated and combined in a Markov logic network (MLN). Our experimental results show that the proposed collective PTE approach significantly outperforms a current state-of-the-art method, improving the average F-measure up to 27.11% compared to the previous word alignment approach. It also outperforms an individual MLN-based system by 8.2% and a system based on conditional random fields by 5.9%. © 2015 ACM",Entity translation; Markov logic network; Named entity translation; Parenthetical translation extraction,Extraction; Markov processes; Natural language processing systems; Probabilistic logics; Conditional random field; Markov logic networks; Monolingual texts; Multiple constraint; Named entity translation; Natural languages; State-of-the-art methods; Word alignment; Computer circuits
Integrated parallel sentence and fragment extraction from comparable corpora: A case study on Chinese–Japanese Wikipedia,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034850494&doi=10.1145%2f2833089&partnerID=40&md5=79a75ba8c8d1d517afb1d88c3ce8b4fa,"Parallel corpora are crucial for statistical machine translation (SMT); however, they are quite scarce for most language pairs and domains. As comparable corpora are far more available, many studies have been conducted to extract either parallel sentences or fragments from them for SMT. In this article, we propose an integrated system to extract both parallel sentences and fragments from comparable corpora. We first apply parallel sentence extraction to identify parallel sentences from comparable sentences. We then extract parallel fragments from the comparable sentences. Parallel sentence extraction is based on a parallel sentence candidate filter and classifier for parallel sentence identification. We improve it by proposing a novel filtering strategy and three novel feature sets for classification. Previous studies have found it difficult to accurately extract parallel fragments from comparable sentences. We propose an accurate parallel fragment extraction method that uses an alignment model to locate the parallel fragment candidates and an accurate lexicon-based filter to identify the truly parallel fragments. A case study on the Chinese–Japanese Wikipedia indicates that our proposed methods outperform previously proposed methods, and the parallel data extracted by our system significantly improves SMT performance. © 2015 ACM",And Phrases: Integrated system; Comparable corpora; Parallel fragment; Parallel sentence,Computer aided language translation; Integrated control; Speech transmission; Comparable corpora; Filtering strategies; Integrated systems; Parallel fragment; Parallel sentence; Sentence extraction; Sentence identifications; Statistical machine translation; Extraction
Description of the Chinese-to-Spanish rule-based machine translation system developed using a hybrid combination of human annotation and statistical techniques,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020869893&doi=10.1145%2f2738045&partnerID=40&md5=039fd5ce7dc16f61ebf3c8389e3b1df2,"Two of the most popular Machine Translation (MT) paradigms are rule based (RBMT) and corpus based, which include the statistical systems (SMT). When scarce parallel corpus is available, RBMT becomes particularly attractive. This is the case of the Chinese–Spanish language pair. This article presents the first RBMT system for Chinese to Spanish. We describe a hybrid method for constructing this system taking advantage of available resources such as parallel corpora that are used to extract dictionaries and lexical and structural transfer rules. The final system is freely available online and open source. Although performance lags behind standard SMT systems for an in-domain test set, the results show that the RBMT’s coverage is competitive and it outperforms the SMT system in an out-of-domain test set. This RBMT system is available to the general public, it can be further enhanced, and it opens up the possibility of creating future hybrid MT systems. © 2015 ACM",Chinese-to-Spanish; Rule-based Machine Translation; Statistical techniques,Computational linguistics; Computer aided language translation; Chinese-to-Spanish; Human annotations; Machine translations; Parallel corpora; Rule-based machine translations; Spanish language; Statistical systems; Statistical techniques; Open systems
A hybrid feature extraction algorithm for Devanagari script,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045506947&doi=10.1145%2f2710018&partnerID=40&md5=6864b57b0ec2a70c572714a9902a7199,"The efficiency of any character recognition technique is directly dependent on the accuracy of the generated feature set that could uniquely represent a character and hence correctly recognize it. This article proposes a hybrid approach combining the structural features of the character and a mathematical model of curve fitting to simulate the best features of a character. As a preprocessing step, skeletonization of the character is performed using an iterative thinning algorithm based on Raster scan of the character image. Then, a combination of structural features of the character like number of endpoints, loops, and intersection points is calculated. Further, the thinned character image is statistically zoned into partitions, and a quadratic curve-fitting model is applied on each partition forming a feature vector of the coefficients of the optimally fitted curve. This vector is combined with the spatial distribution of the foreground pixels for each zone and hence script-independent feature representation. The approach has been evaluated experimentally on Devanagari scripts. The algorithm achieves an average recognition accuracy of 93.4%. © 2015 ACM",Curve fitting; Raster scan; Thinning; Zoning,Character recognition; Curve fitting; Iterative methods; Rasterization; Zoning; Feature representation; Hybrid-feature extraction; Intersection points; Pre-processing step; Quadratic curve fittings; Raster scans; Recognition accuracy; Thinning; Image thinning
Acoustic features for hidden conditional random fields–based Thai tone classification,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057550646&doi=10.1145%2f2833088&partnerID=40&md5=c4e861e455e3c0672042512841031a69,"In the Thai language, tone information is necessary for Thai speech recognition systems. Previous studies show that many acoustic cues are attributed to shapes of tones. Nevertheless, most Thai tone classification studies mainly adopted F0 values and their derivatives without considering other acoustic features. In this article, other acoustic features for Thai tone classification are investigated. In the experiment, energy values and spectral information represented by three spectral-based features including the LPC-based feature, PLP-based feature, and MFCC-based feature are applied to the HCRF-based Thai tone classification, which was reported as the best approach for Thai tone classification. The energy values provide an error rate reduction of 22.40% in the isolated word scenario, while there are slight improvements in the continuous speech scenario. On the contrary, spectral-based features greatly contribute to Thai tone classification in the continuous-speech scenario, whereas spectral-based features slightly degrade performances in the isolated-word scenario. The best achievement in the continuous-speech scenario is obtained from the PLP-based feature, which yields an error rate reduction of 13.90%. Therefore, findings in this article are that energy values and spectral-based features, especially the PLP-based feature, are the main contributors to the improvement of the performances of Thai tone classification in the isolated-word scenario and the continuous-speech scenario, respectively. © 2015 ACM",Acoustic features; Energy; Hidden conditional random fields; Spectral information; Thai tone classification; Tone features,Acoustic fields; Random processes; Speech; Speech recognition; Acoustic features; Energy; Hidden conditional random fields; Spectral information; Tone features; Classification (of information)
Improving handwritten Arabic character recognition by modeling human handwriting distortions,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050751365&doi=10.1145%2f2764456&partnerID=40&md5=03006c31c35d28c51780f551cd242351,"Handwritten Arabic character recognition systems face several challenges, including the unlimited variation in human handwriting and the unavailability of large public databases of handwritten characters and words. The use of synthetic data for training and testing handwritten character recognition systems is one of the possible solutions to provide several variations for these characters and to overcome the lack of large databases. While this can be using arbitrary distortions, such as image noise and randomized affine transformations, such distortions are not realistic. In this work, we model real distortions in handwriting using real handwritten Arabic character examples and then use these distortion models to synthesize handwritten examples that are more realistic. We show that the use of our proposed approach leads to significant improvements across different machine-learning classification algorithms. © 2015 ACM",Affine transformations; Arabic character recognition; Congealing; Human handwriting distortion; Synthetic data,Learning systems; Affine transformations; Arabic character recognition; Congealing; Hand written character recognition; Hand-written characters; Machine learning classification; Synthetic data; Training and testing; Character recognition
A constraint approach to pivot-based bilingual dictionary induction,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984790400&doi=10.1145%2f2723144&partnerID=40&md5=d4bf4a855ae417522aedf030caaad55a,"High-quality bilingual dictionaries are very useful, but such resources are rarely available for lower-density language pairs, especially for those that are closely related. Using a third language to link two other languages is a well-known solution and usually requires only two input bilingual dictionaries A-B and B-C to automatically induce the new one, A-C. This approach, however, has never been demonstrated to utilize the complete structures of the input bilingual dictionaries, and this is a key failing because the dropped meanings negatively influence the result. This article proposes a constraint approach to pivot-based dictionary induction where language A and C are closely related. We create constraints from language similarity and model the structures of the input dictionaries as a Boolean optimization problem, which is then formulated within the Weighted Partial Max-SAT framework, an extension of Boolean Satisfiability (SAT). All of the encoded CNF (Conjunctive Normal Form), the predominant input language of modern SAT/MAX-SAT solvers, formulas are evaluated by a solver to produce the target (output) bilingual dictionary. Moreover, we discuss alternative formalizations as a comparison study. We designed a tool that uses the Sat4j library as the default solver to implement our method and conducted an experiment in which the output bilingual dictionary achieved better quality than the baseline method. © 2015 ACM",Bilingual dictionary induction; Constraint satisfaction problem; Low-resource languages; Pivot language; Weighted Partial Max-SAT,Constraint satisfaction problems; Baseline methods; Bilingual dictionary; Boolean optimizations; Boolean satisfiability; Conjunctive normal forms; Low resource languages; Max-SAT; Pivot language; C (programming language)
Correcting Chinese spelling errors with word lattice decoding,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028859823&doi=10.1145%2f2791389&partnerID=40&md5=4ff76ef710b1fcec7be65fd97064d172,"Chinese spell checkers are more difficult to develop because of two language features: 1) there are no word boundaries, and a character may function as a word or a word morpheme; and 2) the Chinese character set contains more than ten thousand characters. The former makes it difficult for a spell checker to detect spelling errors, and the latter makes it difficult for a spell checker to construct error models. We develop a word lattice decoding model for a Chinese spell checker that addresses these difficulties. The model performs word segmentation and error correction simultaneously, thereby solving the word boundary problem. The model corrects nonword errors as well as real-word errors. In order to better estimate the error distribution of large character sets for error models, we also propose a methodology to extract spelling error samples automatically from the Google web 1T corpus. Due to the large quantity of data in the Google web 1T corpus, many spelling error samples can be extracted, better reflecting spelling error distributions in the real world. Finally, in order to improve the spell checker for real applications, we produce n-best suggestions for spelling error corrections. We test our proposed approach with the Bakeoff 2013 CSC Datasets; the results show that the proposed methods with the error model significantly outperform the performance of Chinese spell checkers that do not use error models. © 2015 ACM.",Chinese spelling error checking; Computer-assisted language learning; Noisy channel model; Unknown word detection; Word lattice; Word segmentation,Character sets; Computational linguistics; Computer aided instruction; Decoding; Error correction; Extraction; Natural language processing systems; Computer assisted language learning; Noisy channel models; Spelling errors; Word lattice; Word segmentation; Errors
A probabilistic framework for Chinese spelling check,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028851006&doi=10.1145%2f2826234&partnerID=40&md5=597fa06d123ca9e260c3bf04224916cc,"Chinese spelling check (CSC) is still an unsolved problem today since there are many homonymous or homomorphous characters. Recently, more and more CSC systems have been proposed. To the best of our knowledge, language modeling is one of the major components among these systems because of its simplicity and moderately good predictive power. After deeply analyzing the school of research, we are aware that most of the systems only employ the conventional n-gram language models. The contributions of this article are threefold. First, we propose a novel probabilistic framework for CSC, which naturally combines several important components, such as the substitution model and the language model, to inherit their individual merits as well as to overcome their limitations. Second, we incorporate the topic language models into the CSC system in an unsupervised fashion. The topic language models can capture the long-span semantic information from a word (character) string while the conventional n-gram language models can only preserve the local regularity information. Third, we further integrate Web resources with the proposed framework to enhance the overall performance. Our rigorously empirical experiments demonstrate the consistent and utility performance of the proposed framework in the CSC task. © 2015 ACM.",Chinese; Language model; Probabilistic; Spelling check; Topic modeling,Modeling languages; Natural language processing systems; Network function virtualization; Semantics; Chinese; Language model; Probabilistic; Spelling checks; Topic Modeling; Computational linguistics
Chinese spelling checker based on an inverted index list with a rescoring mechanism,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021738470&doi=10.1145%2f2826235&partnerID=40&md5=2d1f1d6c3ce6dd8b244151aee4be2378,"An approach is proposed for Chinese spelling error detection and correction, in which an inverted index list with a rescoring mechanism is used. The inverted index list is a structure for mapping from word to desired sentence, and for representing nodes in lattices constructed through character expansion (according to predefined phonologically and visually similar character sets). Pruning based on a contextual dependency confidence measure was used to markedly reduce the search space and computational complexity. Relevant mapping relations between the original input and desired input were obtained using a scoring mechanism composed of class-based language and maximum entropy correction models containing character, word, and contextual features. The proposed method was evaluated using data sets provided by SigHan 7 bakeoff. The experimental results show that the proposed method achieved acceptable performance in terms of recall rate or precision rate in error sentence detection and error location detection, and it outperformed other approaches in error location detection and correction. © 2015 ACM.",Contextual information; Correction model; Inverted index list; Language model; Maximum entropy; Spelling checker,Character sets; Entropy; Errors; Indexing (of information); Mapping; Maximum entropy methods; Contextual information; Correction models; Inverted indices; Language model; Spelling checker; Error detection
A hybrid ranking approach to Chinese spelling check,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021714999&doi=10.1145%2f2822264&partnerID=40&md5=d80e16c1841558210cf43fd7c0010554,"We propose a novel framework for Chinese Spelling Check (CSC), which is an automatic algorithm to detect and correct Chinese spelling errors. Our framework contains two key components: candidate generation and candidate ranking. Our framework differs from previous research, such as Statistical Machine Translation (SMT) based model or Language Model (LM) based model, in that we use both SMT and LM models as components of our framework for generating the correction candidates, in order to obtain maximum recall; to improve the precision, we further employ a Support Vector Machines (SVM) classifier to rank the candidates generated by the SMT and the LM. Experiments show that our framework outperforms other systems, which adopted the same or similar resources as ours in the SIGHAN 7 shared task; even comparing with the state-of-the-art systems, which used more resources, such as a considerable large dictionary, an idiom dictionary and other semantic information, our framework still obtains competitive results. Furthermore, to address the resource scarceness problem for training the SMT model, we generate around 2 million artificial training sentences using the Chinese character confusion sets, which include a set of Chinese characters with similar shapes and similar pronunciations, provided by the SIGHAN 7 shared task. © 2015 ACM.",Candidate generation; Candidate ranking; Chinese spelling check,Character sets; Computational linguistics; Computer aided language translation; Machine components; Semantics; Speech transmission; Support vector machines; Artificial training; Automatic algorithms; Candidate generation; Candidate ranking; Semantic information; Spelling checks; State-of-the-art system; Statistical machine translation; Network function virtualization
TALLIP perspectives: Editorial commentary the state of the journal,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028799488&doi=10.1145%2f2823512&partnerID=40&md5=da36df793592aca1bca48861eb28cda2,[No abstract available],,
Introduction to the special issue on Chinese spell checking,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028814833&doi=10.1145%2f2818354&partnerID=40&md5=6b59b01b1aee11b27f9b004440721923,[No abstract available],Confusion sets; Correction models; Detection models; Spelling errors,
A unified model for solving the OOV problem of Chinese word segmentation,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049044856&doi=10.1145%2f2699940&partnerID=40&md5=036f45dec5362c051fac443d37782716,"This article proposes a unified, character-based, generative model to incorporate additional resources for solving the out-of-vocabulary (OOV) problem of Chinese word segmentation, within which different types of additional information can be utilized independently in corresponding submodels. This article mainly addresses the following three types of OOV: unseen dictionary words, named entities, and suffix-derived words, none of which are handled well by current approaches. The results show that our approach can effectively improve the performance of the first two types with positive interaction in F-score. Additionally, we also analyze reason that suffix information is not helpful. After integrating the proposed generative model with the corresponding discriminative approach, our evaluation on various corpora—including SIGHAN-2005, CIPS-SIGHAN-2010, and the Chinese Treebank (CTB)—shows that our integrated approach achieves the best performance reported in the literature on all testing sets when additional information and resources are allowed. © 2015 ACM.",And Phrases: Chinese word segmentation; Domain adaptation; Model integration; Out-of-vocabulary words,Computational linguistics; Natural language processing systems; Chinese word segmentation; Discriminative approach; Domain adaptation; Generative model; Integrated approach; Model integration; Out of vocabulary words; Positive interaction; Integration testing
Conditional random fields for Korean morpheme segmentation and POS tagging,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009480341&doi=10.1145%2f2700051&partnerID=40&md5=1df2a06c8f051cf5bfbcba4933cbc4cc,"There has been recent interest in statistical approaches to Korean morphological analysis. However, previous studies have been based mostly on generative models, including a hidden Markov model (HMM), without utilizing discriminative models such as a conditional random field (CRF). We present a two-stage discriminative approach based on CRFs for Korean morphological analysis. Similar to methods used for Chinese, we perform two disambiguation procedures based on CRFs: (1) morpheme segmentation and (2) POS tagging. In morpheme segmentation, an input sentence is segmented into sequences of morphemes, where a morpheme unit is either atomic or compound. In the POS tagging procedure, each morpheme (atomic or compound) is assigned a POS tag. Once POS tagging is complete, we carry out a post-processing of the compound morphemes, where each compound morpheme is further decomposed into atomic morphemes, which is based on pre-analyzed patterns and generalized HMMs obtained from the given tagged corpus. Experimental results show the promise of our proposed method. © 2015 ACM",And Phrases: Conditional random fields; Korean morphological analysis; Morpheme segmentation; POS tagging,Atoms; Hidden Markov models; Image segmentation; Morphology; Semiotics; Conditional random field; Discriminative approach; Discriminative models; Generative model; Morphological analysis; PoS tagging; Post processing; Statistical approach; Computational linguistics
Multilingual topic models for bilingual dictionary extraction,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045833404&doi=10.1145%2f2699939&partnerID=40&md5=15688bed81cea0cb47ace5c2fdca1625,"A machine-readable bilingual dictionary plays a crucial role in many natural language processing tasks, such as statistical machine translation and cross-language information retrieval. In this article, we propose a framework for extracting a bilingual dictionary from comparable corpora by exploiting a novel combination of topic modeling and word aligners such as the IBM models. Using a multilingual topic model, we first convert a comparable document-aligned corpus into a parallel topic-aligned corpus. This novel topic-aligned corpus is similar in structure to the sentence-aligned corpus frequently employed in statistical machine translation and allows us to extract a bilingual dictionary using a word alignment model. The main advantages of our framework is that (1) no seed dictionary is necessary for bootstrapping the process, and (2) multilingual comparable corpora in more than two languages can also be exploited. In our experiments on a large-scale Wikipedia dataset, we demonstrate that our approach can extract higher precision dictionaries compared to previous approaches and that our method improves further as we add more languages to the dataset. 2015 Copyright held by the Owner/Author. © 2018 Association for Computing Machinery. All Rights Reserved.",Algorithms; Experimentation; Languages,Algorithms; Computational linguistics; Computer aided language translation; Natural language processing systems; Query languages; Bilingual dictionary; Comparable corpora; Cross language information retrieval; Experimentation; Statistical machine translation; Topic model; Topic Modeling; Word alignment; Data mining
Preordering using a target-language parser via cross-language syntactic projection for statistical machine translation,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009355261&doi=10.1145%2f2699925&partnerID=40&md5=1a91b8c05e50685b7939550d15b58efb,"When translating between languages with widely different word orders, word reordering can present a major challenge. Although some word reordering methods do not employ source-language syntactic structures, such structures are inherently useful for word reordering. However, high-quality syntactic parsers are not available for many languages. We propose a preordering method using a target-language syntactic parser to process source-language syntactic structures without a source-language syntactic parser. To train our preordering model based on ITG, we produced syntactic constituent structures for source-language training sentences by (1) parsing target-language training sentences, (2) projecting constituent structures of the target-language sentences to the corresponding source-language sentences, (3) selecting parallel sentences with highly synchronized parallel structures, (4) producing probabilistic models for parsing using the projected partial structures and the Pitman-Yor process, and (5) parsing to produce full binary syntactic structures maximally synchronized with the corresponding target-language syntactic structures, using the constraints of the projected partial structures and the probabilistic models. Our ITG-based preordering model is trained using the produced binary syntactic structures and word alignments. The proposed method facilitates the learning of ITG by producing highly synchronized parallel syntactic structures based on crosslanguage syntactic projection and sentence selection. The preordering model jointly parses input sentences and identifies their reordered structures. Experiments with Japanese-English and Chinese-English patent translation indicate that our method outperforms existing methods, including string-to-tree syntax-based SMT, a preordering method that does not require a parser, and a preordering method that uses a sourcelanguage dependency parser.",Constituent structure; Inversion transduction grammar; Preordering; Syntactic projection,Computational linguistics; Computer aided language translation; Context free grammars; Linguistics; Natural language processing systems; Synchronization; Translation (languages); Inversion transduction grammars; Parallel structures; Partial structures; Pre orderings; Probabilistic models; Statistical machine translation; Syntactic projection; Syntactic structure; Syntactics
Model generation of accented speech using model transformation and verification for bilingual speech recognition,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051344891&doi=10.1145%2f2661637&partnerID=40&md5=3018fee022993cbc9dca8a3c67d54554,"Nowadays, bilingual or multilingual speech recognition is confronted with the accent-related problem caused by non-native speech in a variety of real-world applications. Accent modeling of non-native speech is definitely challenging, because the acoustic properties in highly-accented speech pronounced by non-native speakers are quite divergent. The aim of this study is to generate highly Mandarin-accented English models for speakers whose mother tongue is Mandarin. First, a two-stage, state-based verification method is proposed to extract the state-level, highly-accented speech segments automatically. Acoustic features and articulatory features are successively used for robust verification of the extracted speech segments. Second, Gaussian components of the highly-accented speech models are generated from the corresponding Gaussian components of the native speech models using a linear transformation function. A decision tree is constructed to categorize the transformation functions and used for transformation function retrieval to deal with the data sparseness problem. Third, a discrimination function is further applied to verify the generated accented acoustic models. Finally, the successfully verified accented English models are integrated into the native bilingual phone model set for Mandarin-English bilingual speech recognition. Experimental results show that the proposed approach can effectively alleviate recognition performance degradation due to accents and can obtain absolute improvements of 4.1%, 1.8%, and 2.7% in word accuracy for bilingual speech recognition compared to that using traditional ASR approaches, MAP-adapted, and MLLR-adapted ASR methods, respectively. c 2015 ACM",Accented speech; Articulatory feature; Bilingual speech recognition,Acoustic properties; Decision trees; Linear transformations; Mathematical transformations; Metadata; Speech; Speech analysis; Trees (mathematics); Accented speech; Articulatory features; Data sparseness problem; Gaussian components; Model transformation; Multilingual speech recognition; Performance degradation; Transformation functions; Speech recognition
Towards machine translation in semantic vector space,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057526627&doi=10.1145%2f2699927&partnerID=40&md5=47bf2fe05c285bca5de014fa99d9fc98,"Measuring the quality of the translation rules and their composition is an essential issue in the conventional statistical machine translation (SMT) framework. To express the translation quality, the previous lexical and phrasal probabilities are calculated only according to the co-occurrence statistics in the bilingual corpus and may be not reliable due to the data sparseness problem. To address this issue, we propose measuring the quality of the translation rules and their composition in the semantic vector embedding space (VES). We present a recursive neural network (RNN)-based translation framework, which includes two submodels. One is the bilingually-constrained recursive auto-encoder, which is proposed to convert the lexical translation rules into compact real-valued vectors in the semantic VES. The other is a type-dependent recursive neural network, which is proposed to perform the decoding process by minimizing the semantic gap (meaning distance) between the source language string and its translation candidates at each state in a bottom-up structure. The RNN-based translation model is trained using a max-margin objective function that maximizes the margin between the reference translation and the n-best translations in forced decoding. In the experiments, we first show that the proposed vector representations for the translation rules are very reliable for application in translation modeling. We further show that the proposed type-dependent, RNN-based model can significantly improve the translation quality in the large-scale, end-to-end Chinese-to-English translation evaluation. © 2015 ACM",Max-margin training; Recursive neural network; Semantic meaning distance; Statistical machine translation; Vector embedding space,Computational linguistics; Computer aided language translation; Decoding; Natural language processing systems; Neural networks; Semantics; Vectors; Co-occurrence statistics; Data sparseness problem; Machine translations; Objective functions; Recursive neural networks; Statistical machine translation; Translation quality; Vector representations; Vector spaces
Bigram language models and reevaluation strategy for improved recognition of online handwritten Tamil words,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012877185&doi=10.1145%2f2671014&partnerID=40&md5=2cd293b1da24c30d8e4f22da4eb93b30,"This article describes a postprocessing strategy for online, handwritten, isolated Tamil words. Contributions have been made with regard to two issues hardly addressed in the online Indic word recognition literature, namely, use of (1) language models exploiting the idiosyncrasies of Indic scripts and (2) expert classifiers for the disambiguation of confused symbols. The input word is first segmented into its individual symbols, which are recognized using a primary support vector machine (SVM) classifier. Thereafter, we enhance the recognition accuracy by utilizing (i) a bigram language model at the symbol or character level and (ii) expert classifiers for reevaluating and disambiguating the different sets of confused symbols. The symbol-level bigram model is used in a traditional Viterbi framework. The concept of a character comprising multiple symbols is unique to Dravidian languages such as Tamil. This multi-symbol feature of Tamil characters has been exploited in proposing a novel, prefix-tree-based character-level bigram model that does not use Viterbi search; rather it reduces the search space for each input symbol based on its left context. For disambiguating confused symbols, a dynamic time-warping approach is proposed to automatically identify the parts of the online trace that discriminates between the confused classes. Fine classification of these regions by dedicated expert SVMs reduces the extent of confusions between such symbols. The integration of segmentation, prefix-tree-based language model and disambiguation of confused symbols is presented on a set of 15,000 handwritten isolated online Tamil words. Our results show recognition accuracies of 93.0% and 81.6% at the symbol and word level, respectively, as compared to the baseline classifier performance of 88.4% and 65.1%, respectively. © 2015 ACM",And Phrases: Online Tamil words; Expert classifiers; Language models; Reevaluation; Support vector machines (SVM),Computational linguistics; Support vector machines; And Phrases: Online Tamil words; Bi-gram language models; Classifier performance; Dynamic time warping; Language model; Recognition accuracy; Reevaluation; Word recognition; Character recognition
Integrating multiple dependency corpora for inducing wide-coverage Japanese CCG resources,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044569355&doi=10.1145%2f2658997&partnerID=40&md5=21e3b8762f572f487b373e07065072d2,"A novel method to induce wide-coverage Combinatory Categorial Grammar (CCG) resources for Japanese is proposed in this article. For some languages including English, the availability of large annotated corpora and the development of data-based induction of lexicalized grammar have enabled deep parsing, i.e., parsing based on lexicalized grammars. However, deep parsing for Japanese has not been widely studied. This is mainly because most Japanese syntactic resources are represented in chunk-based dependency structures, while previous methods for inducing grammars are dependent on tree corpora. To translate syntactic information presented in chunk-based dependencies to phrase structures as accurately as possible, integration of annotation from multiple dependency-based corpora is proposed. Our method first integrates dependency structures and predicate-argument information and converts them into phrase structure trees. The trees are then transformed into CCG derivations in a similar way to previously proposed methods. The quality of the conversion is empirically evaluated in terms of the coverage of the obtained CCG lexicon and the accuracy of the parsing with the grammar. While the transforming process used in this study is specialized for Japanese, the framework of our method would be applicable to other languages for which dependency-based analysis has been regarded as more appropriate than phrase structure-based analysis due to morphosyntactic features. © 2015 ACM",Combinatory Categorial Grammar; Dependency annotation; Grammar development; Japanese parsing,Computational grammars; Computational linguistics; Forestry; Formal languages; Quality control; Combinatory categorial grammar; Combinatory categorial grammar (CCG); Dependency annotation; Grammar development; Japanese parsing; Multiple dependencies; Phrase structure trees; Syntactic information; Syntactics
Improving telugu dependency parsing using combinatory categorial grammar supertags,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044599309&doi=10.1145%2f2693190.2693191&partnerID=40&md5=d4f367dc8a91cd8f1093a7fa27a38455,"We show that Combinatory Categorial Grammar (CCG) supertags can improve Telugu dependency parsing. In this process, we first extract a CCG lexicon from the dependency treebank. Using both the CCG lexicon and the dependency treebank, we create a CCG treebank using a chart parser. Exploring different morphological features of Telugu, we develop a supertagger using maximum entropy models. We provide CCG supertags as features to the Telugu dependency parser (MST parser). We get an improvement of 1.8% in the unlabelled attachment score and 2.2% in the labelled attachment score. Our results show that CCG supertags improve the MST parser, especially on verbal arguments for which it has weak rates of recovery. © 2015 ACM",And Phrases: Dependency parsing; Combinatory categorial grammar; Indian languages; MST parser; Telugu,Computational grammars; Forestry; Formal languages; Combinatory categorial grammar; Dependency parsing; Indian languages; MST parser; Telugu; Computational linguistics
TALLIP perspectives: Editorial commentary the broadened focus of the journal,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057739784&doi=10.1145%2f2710043&partnerID=40&md5=9a30fe1109cb51d5528033556ff767a3,[No abstract available],,
An EDU-based approach for Thai multi-document summarization and its application,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047774534&doi=10.1145%2f2641567&partnerID=40&md5=490f9972ad1840cde2dde349abc0d14f,"Due to lack of a word/phrase/sentence boundary, summarization of Thai multiple documents has several challenges in unit segmentation, unit selection, duplication elimination, and evaluation dataset construction. In this article, we introduce Thai Elementary Discourse Units (TEDUs) and their derivatives, called Combined TEDUs (CTEDUs), and then present our three-stage method of Thai multi-document summarization, that is, unit segmentation, unit-graph formulation, and unit selection and summary generation. To examine performance of our proposed method, a number of experiments are conducted using 50 sets of Thai news articles with their manually constructed reference summaries. Based on measures of ROUGE-1, ROUGE-2, and ROUGE-SU4, the experimental results show that: (1) the TEDU-based summarization outperforms paragraph-based summarization; (2) our proposed graph-based TEDU weighting with importance-based selection achieves the best performance; and (3) unit duplication consideration and weight recalculation help improve summary quality. © 2015 ACM",EDU-based approach; Multi-document summarization; Thai text summarization; Unit selection,Graphic methods; EDU-based approach; Graph formulations; Multi-document summarization; Multiple documents; Summary generation; Text summarization; Three-stage method; Unit selection; Natural language processing systems
Approaches to temporal expression recognition in Hindi,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048736210&doi=10.1145%2f2629574&partnerID=40&md5=8770cd22930e01f4ab066ceb03c9dc27,"Temporal annotation of plain text is considered a useful component of modern information retrieval tasks. In this work, different approaches for identification and classification of temporal expressions in Hindi are developed and analyzed. First, a rule-based approach is developed, which takes plain text as input and based on a set of hand-crafted rules, produces a tagged output with identified temporal expressions. This approach performs with a strict F1-measure of 0.83. In another approach, a CRF-based classifier is trained with human tagged data and is then tested on a test dataset. The trained classifier identifies the time expressions from plain text and further classifies them to various classes. This approach performs with a strict F1-measure of 0.78. Next, the CRF is replaced by an SVM-based classifier and the same experiment is performed with the same features. This approach is shown to be comparable to the CRF and performs with a strict F1-measure of 0.77. Using the rule base information as an additional feature enhances the performances to 0.86 and 0.84 for the CRF and SVM respectively. With three different comparable systems performing the extraction task, merging them to take advantage of their positives is the next step. As the first merge experiment, rule-based tagged data is fed to the CRF and SVM classifiers as additional training data. Evaluation results report an increase in F1-measure of the CRF from 0.78 to 0.8. Second, a voting-based approach is implemented, which chooses the best class for each token from the outputs of the three approaches. This approach results in the best performance for this task with a strict F1-measure of 0.88. In this process a reusable gold standard dataset for temporal tagging in Hindi is also developed. Named the ILTIMEX2012 corpus, it consists of 300 manually tagged Hindi news documents. © 2015 ACM",Indian language time tagging; Temporal annotation; Time tagging,Statistical tests; Syntactics; Evaluation results; Handcrafted rules; Rule-based approach; SVM classifiers; SVM-based classifiers; Temporal annotation; Temporal expressions; Time tagging; Classification (of information)
Keyword extraction from Arabic documents using term equivalence classes,2015,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017255118&doi=10.1145%2f2665077&partnerID=40&md5=b7318b2c8303c29369a27a85607c4863,"The rapid growth of the Internet and other computing facilities in recent years has resulted in the creation of a large amount of text in electronic form, which has increased the interest in and importance of different automatic text processing applications, including keyword extraction and term indexing. Although keywords are very useful for many applications, most documents available online are not provided with keywords. We describe a method for extracting keywords from Arabic documents. This method identifies the keywords by combining linguistics and statistical analysis of the text without using prior knowledge from its domain or information from any related corpus. The text is preprocessed to extract the main linguistic information, such as the roots and morphological patterns of derivative words. A cleaning phase is then applied to eliminate the meaningless words from the text. The most frequent terms are clustered into equivalence classes in which the derivative words generated from the same root and the non-derivative words generated from the same stem are placed together, and their count is accumulated. A vector space model is then used to capture the most frequent N-gram in the text. Experiments carried out using a real-world dataset show that the proposed method achieves good results with an average precision of 31% and average recall of 53% when tested against manually assigned keywords. © 2015 ACM",Arabic natural language processing; Extraction; Term equivalence classes; Text analysis,Extraction; Indexing (materials working); Linguistics; Natural language processing systems; Text processing; Vector spaces; Arabic natural language processing; Automatic text processing; Computing facilities; Keyword extraction; Linguistic information; Morphological patterns; Text analysis; Vector space models; Equivalence classes
Low-Power Feature-Attention Chinese Keyword Spotting Framework with Distillation Learning,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152910069&doi=10.1145%2f3558002&partnerID=40&md5=9ec9b49476c82685c38416e6c705846b,"In this paper, we propose a novel Low-Power Feature-Attention Chinese Keyword Spotting Framework based on a depthwise separable convolution neural network (DSCNN) with distillation learning to recognize speech signals of Chinese wake-up words. The framework consists of a low-power feature-Attention acoustic model and its learning methods. Different from the existing model, the proposed acoustic model based on connectionist temporal classification (CTC) focuses on the reduction of power consumption by reducing model network parameters and multiply-Accumulate (MAC) operations through our designed feature-Attention network and DSCNN. In particular, the feature-Attention network is specially designed to extract effective syllable features from a large number of MFCC features. This could refine MFCC features by selectively focusing on important speech signal features and removing invalid speech signal features to reduce the number of speech signal features, which helps to significantly reduce the parameters and MAC operations of the whole acoustic model. Moreover, DSCNN with fewer parameters and MAC operations compared with traditional convolution neural networks is adopted to extract effective high-dimensional features from syllable features. Furthermore, we apply a distillation learning algorithm to efficiently train the proposed low-power acoustic model by utilizing the knowledge of the trained large acoustic model. Experimental results thoroughly verify the effectiveness of our model and show that the proposed acoustic model still has better accuracy than other acoustic models with the lowest power consumption and smallest latency measured by NVIDIA JETSON TX2. It has only 14.524KB parameters and consumes only 0.141J energy per query and 17.9ms latency on the platform, which is hardware-friendly.  © 2022 Association for Computing Machinery.",Acoustic model; deep learning; hardware; keyword spotting; low power,Audio signal processing; Convolution; Deep neural networks; Electric power utilization; Learning algorithms; Learning systems; Low power electronics; Speech communication; Acoustics model; Chinese keywords; Convolution neural network; Deep learning; Hardware; Keyword spotting; Low Power; Multiply accumulate operations; Signal features; Speech signals; Distillation
The Comparison of Language Models with a Novel Text Filtering Approach for Turkish Sentiment Analysis,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152960514&doi=10.1145%2f3557892&partnerID=40&md5=f678d0c1d998dc1b7cd2de5537de16ad,"Today, comments can be made on many topics on web platforms with the development of the internet. Analyzing the data of these comments is essential for companies and data scientists. There are many methods for analyzing data. Recently, language models have also been used in many studies for sentiment analysis or text classification. In this study, Turkish sentiment analysis is performed using language models on hotel and movie review datasets. The language models are chosen because they are rarely used in Turkish literature. The pre-Trained BERT, ALBERT, ELECTRA, and DistilBERT models for the Turkish language are trained and tested with these datasets. In addition, a text filtering method, which removes the words that can provide the opposition sentiment in the positive or negative labeled text, is proposed for sentiment analysis. These datasets obtained by this method are also retrained with language models and the accuracy values of their models are measured. The results of this study are compared with previous studies using the same datasets. As a result of the analysis, the accuracy values obtain state-of-The-Art results with language models compared to previous studies. The best performance has been achieved by training the ELECTRA language model using the proposed text filtering method.  © 2022 Association for Computing Machinery.",data analysis; Language model; natural language processing; sentiment analysis; social network; text classification,Classification (of information); Computational linguistics; Social networking (online); Filtering method; Language model; Language processing; Natural language processing; Natural languages; Sentiment analysis; Social network; Text classification; Text filtering; Turkishs; Sentiment analysis
An Object Localization-based Dense Image Captioning Framework in Hindi,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152910618&doi=10.1145%2f3558391&partnerID=40&md5=6fb37253a0a1058da18fd49e18be327f,"Dense image captioning is a task that requires generating localized captions in natural language for multiple regions of an image. This task leverages its functionalities from both computer vision for recognizing regions in an image and natural language processing for generating captions. Numerous works have been carried out on dense image captioning for resource-rich languages like English; however, resource-poor languages like Hindi are not explored. Hindi is one of India's official languages and is the third most spoken language in the world. This article proposes a dense image captioning model to describe different segments of an image by generating more than one caption in the Hindi language. For localized image recognition and language modeling, we employ Faster R-CNN and Long Short-Term Memory (LSTM), respectively. Apart from this, we conduct various experiments using gated recurrent units (GRUs) and attention mechanism. By manually translating the well-known Visual Genome dataset from English to Hindi, a dataset has been created for dense image captioning in Hindi. The experiments conducted on the newly constructed Hindi dense image captioning dataset illustrate the efficacy of the proposed method over the state-of-The-Art methods.  © 2022 Association for Computing Machinery.",attention; deep-learning; Dense image captioning; Hindi,Image recognition; Modeling languages; Natural language processing systems; Object recognition; Attention; Deep-learning; Dense image captioning; Hindi; Image captioning; Language processing; Localised; Multiple regions; Natural languages; Object localization; Long short-term memory
Stance Detection with a Multi-Target Adversarial Attention Network,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146924905&doi=10.1145%2f3544490&partnerID=40&md5=658e617f2e47e3ccc778dc157049b613,"Stance detection aims to assign a stance label (in favor or against) to a post towards a specific target. In the literature, there are many studies focusing on this topic, and most of them treat stance detection as a supervised learning task. Therefore, a new classifier needs to be built from scratch on a well-prepared set of ground-Truth data whenever predictions are needed for an unseen target. However, it is difficult to annotate the stance of a post, since a stance is a subjective attitude towards a target. Hence, it is necessary to learn the information from unlabeled data or other target data to help stance detection with a certain target. In this study, we propose a multi-Target stance detection framework to integrate multi-Target data together for stance detection. Since topic and sentiment are two important factors to identify the stance of a post in multi-Target data, we propose an adversarial attention network to integrate multi-Target data by detecting and connecting topic and sentiment information. In particular, the adversarial network is utilized to determine the topic and the sentiment of each post to collect some target-invariant information for stance detection. In addition, the attention mechanism is utilized to connect posts with a similar topic or sentiment to acquire some key information for stance detection. The experimental results not only demonstrate the effectiveness of the proposed model, but also indicate the importance of the topic and the sentiment information for stance detection using multi-Target data.  © 2022 Association for Computing Machinery.",adversarial attention network; multi-Target data; natural language processing; Stance detection,Natural language processing systems; Adversarial attention network; Ground truth data; Language processing; Learn+; Learning tasks; Multi-target data; Multi-targets; Natural language processing; Natural languages; Stance detection; Data handling
Clustering-based Sequence to Sequence Model for Generative Question Answering in a Low-resource Language,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152908240&doi=10.1145%2f3563036&partnerID=40&md5=534c22de399b18495fe3a925fb18bbc1,"Despite the impressive success of sequence to sequence models for generative question answering, they need a vast amount of question-answer pairs during training, which is hard and expensive to obtain, especially for low-resource languages. In this article, we present a framework that exploits the semantic clusters among the question-answer pairs to compensate for the lack of enough training data. In the training phase, the question-answer pairs are clustered, and a cluster predictor is trained to identify the cluster each question belongs to. Then, a sequence to sequence model is trained, where there is a different generator for each cluster in the decoder component. During the test phase, the cluster of the input question is first identified using the trained cluster predictor, and the appropriate decoder is exploited. Our experiments on a Persian religious dataset show that the proposed method outperforms the standard sequence to sequence model by a large margin in terms of ROUGE and BLEU scores. This is traced back to the lower number of words in each cluster, leading to a reduction in the number of effective parameters each generator needs to learn, which help the model learn from fewer training data with less overfitting.  © 2022 Association for Computing Machinery.",clustering; deep learning; encoder-decoder model; Generative question answering; low-resource language,Decoding; Deep learning; Large dataset; Signal encoding; Clusterings; Deep learning; Encoder-decoder; Encoder-decoder model; Generative question answering; Low resource languages; Question Answering; Question-answer pairs; Sequence models; Training data; Semantics
"A Survey on NLP Resources, Tools, and Techniques for Marathi Language Processing",2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152908926&doi=10.1145%2f3548457&partnerID=40&md5=2f13d2b7534254897855c78bbbc0e326,"Natural Language Processing (NLP) has been in practice for the past couple of decades, and extensive work has been done for the Western languages, particularly the English language. The Eastern counterpart, especially the languages of the Indian subcontinent, needs attention as not much language processing work has been done on these languages. Western languages are rich in dictionaries, WordNet, and associated tools, while Indian languages are lagging behind in this segment. Marathi is the third most spoken language in India and the 15th most spoken language worldwide. Lack of resources, complex linguistic facts, and the inclusion of prevalent dialects of neighbors have resulted in limited work for Marathi. The aim of this study is to provide an insight into the various linguistic resources, tools, and state-of-The-Art techniques applied to the processing of the Marathi language. Initially, morphological descriptions of the Marathi language are provided, followed by a discussion on the characteristics of the Marathi language. Thereafter, for Marathi language, the availability of corpus, tools, and techniques to be used to develop NLP tasks is reviewed. Finally, gap analysis is discussed in current research and future directions for this new and dynamic area of research are listed that will benefit the Marathi Language Processing research community.  © 2022 Association for Computing Machinery.",Marathi language; Marathi morphology; Marathi resources; Named Entity Recognition (NER); Part-of-Speech (POS) tagging; Word Sense Disambiguation (WSD),Computational linguistics; Natural language processing systems; Speech recognition; Syntactics; Language processing; Marathi languages; Marathi morphology; Marathi resource; Named entity recognition; Part of speech tagging; Parts-of-speech tagging; Word sense disambiguation; Morphology
Multi-Turn and Multi-Granularity Reader for Document-Level Event Extraction,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149986327&doi=10.1145%2f3542925&partnerID=40&md5=73ee6b27f457412f157f0d7a5efaff4e,"Most existing event extraction works mainly focus on extracting events from one sentence. However, in real-world applications, arguments of one event may scatter across sentences and multiple events may co-occur in one document. Thus, these scenarios require document-level event extraction (DEE), which aims to extract events and their arguments across sentences from a document. Previous works cast DEE as a two-step paradigm: sentence-level event extraction (SEE) to document-level event fusion. However, this paradigm lacks integrating document-level information for SEE and suffers from the inherent limitations of error propagation. In this article, we propose a multi-Turn and multi-granularity reader for DEE that can extract events from the document directly without the stage of preliminary SEE. Specifically, we propose a new paradigm of DEE by formulating it as a machine reading comprehension task (i.e., the extraction of event arguments is transformed to identify the answer span from the document). Beyond the framework of machine reading comprehension, we introduce a multi-Turn and multi-granularity reader to capture the dependencies between arguments explicitly and model long texts effectively. The empirical results demonstrate that our method achieves superior performance on the MUC-4 and the ChFinAnn datasets.  © 2022 Association for Computing Machinery.",Document-level event extraction; machine reading comprehension; multi-granularity reader,Document-level event extraction; Events extractions; Machine reading comprehension; Multi-granularity; Multi-granularity reader; Multi-turn; Multiple events; Reading comprehension; Real-world; Sentence level; Extraction
Annotation Projection-based Dependency Parser Development for Nepali,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152625529&doi=10.1145%2f3542696&partnerID=40&md5=3281abdf02b09431393d4be4783b1dc2,"Building computational resources and tools for the under-resourced languages is strenuous for any Natural Language Processing task. This article presents the first dependency parser for an under-resourced Indian language, Nepali. A prerequisite for developing a parser for a language is a corpus annotated with the desired linguistic representations known as a treebank. With an aim of cross-lingual learning and typological research, we use a Bengali treebank to build a Bengali-Nepali parallel corpus and apply the method of annotation projection from the Bengali treebank to build a treebank for Nepali. With the developed treebank, MaltParser (with all algorithms for projective dependency structures) and a Neural network-based parser have been used to build Nepali parser models. The Neural network-based parser produced state-of-The-Art results with 81.2 Unlabeled Attachment Score, 73.2 Label Accuracy, and 66.1 Labeled Attachment Score on the gold test data. The parser models have also been evaluated with the predicted Part-of-speech (POS)-Tagged test data. A statistical POS tagger using Conditional Random Field has been developed for predicting the POS tags of the test data.  © 2022 Association for Computing Machinery.",Nepali dependency parser; Nepali dependency treebank; Nepali-Bengali parallel treebank,Natural language processing systems; Syntactics; Bengalis; Dependency parser; Nepali dependency parse; Nepali dependency treebank; Nepali-bengali parallel treebank; Network-based; Neural-networks; Test data; Treebanks; Computational linguistics
Contrastive Learning between Classical and Modern Chinese for Classical Chinese Machine Reading Comprehension,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152900769&doi=10.1145%2f3551637&partnerID=40&md5=3024ef8dbee8f3f9496b02b5f7817321,"By leveraging self-supervised tasks, pre-Trained language model (PLM) has made significant progress in the field of machine reading comprehension (MRC). However, in classical Chinese MRC (CCMRC), the passage is typically in classical style, but the question and options are given in modern style. Existing pre-Trained methods seldom model the relationship between classical and modern styles, resulting in overall misunderstanding of the passage. In this paper, we propose a contrastive learning method between classical and modern Chinese in order to reach a deep understanding of the two different styles. In particular, a novel pre-Training task and an enhanced co-matching network have been defined: (1) The synonym discrimination (SD) task is used to identify whether modern meaning corresponds to classical Chinese. (2) The enhanced dual co-matching (EDCM) network is employed for a more interactive understanding of the classical passage and the modern options. The experimental results show that our proposed method improves language understanding ability and outperforms existing PLMs on the Haihua, CCLUE, and ChID datasets.  © 2022 Association for Computing Machinery.",classical Chinese understanding; contrastive learning; enhanced dual co-matching; Machine reading comprehension,Classical chinese understanding; Contrastive learning; Enhanced dual co-matching; Language model; Learning methods; Machine reading comprehension; Matching networks; Matchings; Pre-training; Reading comprehension; Learning systems
An Understanding-oriented Robust Machine Reading Comprehension Model,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152958235&doi=10.1145%2f3546190&partnerID=40&md5=b914f195e5ae30e96fe25822932b0ab3,"Although existing machine reading comprehension models are making rapid progress on many datasets, they are far from robust. In this article, we propose an understanding-oriented machine reading comprehension model to address three kinds of robustness issues, which are over-sensitivity, over-stability, and generalization. Specifically, we first use a natural language inference module to help the model understand the accurate semantic meanings of input questions to address the issues of over-sensitivity and over-stability. Then, in the machine reading comprehension module, we propose a memory-guided multi-head attention method that can further well understand the semantic meanings of input questions and passages. Third, we propose a multi-language learning mechanism to address the issue of generalization. Finally, these modules are integrated with a multi-Task learning-based method. We evaluate our model on three benchmark datasets that are designed to measure models' robustness, including DuReader (robust) and two SQuAD-related datasets. Extensive experiments show that our model can well address the mentioned three kinds of robustness issues. And it achieves much better results than the compared state-of-The-Art models on all these datasets under different evaluation metrics, even under some extreme and unfair evaluations. The source code of our work is available at https://github.com/neukg/RobustMRC.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",DuReader (robust); generalization; memory-guided multi-head attention; multi-Task and multi-language learning; over-sensitivity; over-stability; Question & answering; robust machine reading comprehension; SQuAD,Learning systems; Natural language processing systems; Dureader (robust); Generalisation; Language learning; Memory-guided multi-head attention; Multi languages; Multi tasks; Multi-task and multi-language learning; Over-sensitivity; Over-stability; Question Answering; Reading comprehension; Robust machine reading comprehension; SQuAD; Semantics
Script Event Prediction via Multilingual Event Graph Networks,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153477048&doi=10.1145%2f3557893&partnerID=40&md5=de2785563a0361e002d8cb04bf50ecb4,"Predicting what happens next in text plays a critical role in building NLP applications. Many methods including count-based and neural-network-based have been proposed to tackle the task called script event prediction: predicting the most suitable subsequent event from a candidate list given a chain of narrative events (context). However, two problems including event ambiguity and evidence bias hinder the performance of these monolingual approaches. The former means that some events in the event chain are ambiguous. The latter means that both the wrong and correct candidate events can obtain sufficient support from the event context. In this article, we propose a novel multilingual approach to address two issues simultaneously. Specifically, to alleviate the event ambiguity problem, we project the monolingual event chains to parallel cross-lingual event chains, which can provide complementary information for monolingual event disambiguation. To deal with the evidence bias problem, we construct two monolingual event graphs and a cross-lingual event aligned graph to fully explore connections between events. What's more, we design a graph attention mechanism to model the confidence of the complement clues, which controls the information integration from various languages. By modeling the events with graphs instead of pairs or chains, the model can compare the candidate subsequent events simultaneously and choose the more suitable subsequent event as the final answer. Extensive experiments were conducted on the widely used New York Times corpus for script event prediction task and experimental results show that our approach outperforms previous models.  © 2022 Association for Computing Machinery.",event graph network; multilingual; Script event prediction,Forecasting; Modeling languages; Cross-lingual; Event context; Event graph network; Event graphs; Event prediction; Graph networks; In-buildings; Multilingual; Neural-networks; Script event prediction; Graph neural networks
Forward-backward Transliteration of Punjabi Gurmukhi Script Using N-gram Language Model,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152673452&doi=10.1145%2f3542924&partnerID=40&md5=b8cd128fe283deb8e9ee5b7a8c61e410,"Transliterating the text of a language to a foreign script is called forward transliteration and transliterating the text back to the original script is called backward transliteration. In this work, we perform both forward as well as backward transliteration on Punjabi. We transliterate Punjabi person names from Gurmukhi script to English Roman script and from English Roman script back to Gurmukhi script using n-gram language model. We used more than one million parallel entities of person names in Gurmukhi and Roman script as the training corpus. We generated English to Punjabi and Punjabi to English n-grams databases from the corpus. To get better results, we tried to create as long n-grams as possible ranging from bi-gram to 30-gram. Our n-grams database contains more than 10 million n-grams, with each n-gram having multiple mappings of the other script. The most challenging part is to find the mapping for the given n-gram from the parallel name entity while creating n-grams databases. As per the orthography rules, the same combination of letters may have different pronunciation, depending upon its location in the word. Therefore, we categorized n-grams into starting, middle, and ending n-grams and used them accordingly in the transliteration process. The transliteration process works like the merge sort. We start searching the longest possible n-gram in the database and split the string recursively until the match is found. The transliterated strings are merged back to form the final output. In English to Punjabi transliteration, we achieved 96% accuracy using gold standard and 99.14% accuracy using minimum edit distance. In Punjabi to English transliteration, the result showed 96.85% and 99.35% accuracy for the gold standard and minimum edit distance, respectively.  © 2022 Association for Computing Machinery.",computational linguistics etc; Punjabi; Transliteration,Computational linguistics; Mapping; AND splits; Computational linguistic etc; Gold standards; Minimum edit distance; N-gram language models; N-grams; Punjabi; Training corpus; Transliteration; Database systems
Turkish Data-To-Text Generation Using Sequence-To-Sequence Neural Networks,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152906599&doi=10.1145%2f3543826&partnerID=40&md5=4a65f49b1032f21d1af771f1f998315a,"End-To-end data-driven approaches lead to rapid development of language generation and dialogue systems. Despite the need for large amounts of well-organized data, these approaches jointly learn multiple components of the traditional generation pipeline without requiring costly human intervention. End-To-end approaches also enable the use of loosely aligned parallel datasets in system development by relaxing the degree of semantic correspondences between training data representations and text spans. However, their potential in Turkish language generation has not yet been fully exploited. In this work, we apply sequence-To-sequence (Seq2Seq) neural models to Turkish data-To-Text generation where the input data given in the form of a meaning representation is verbalized. We explore encoder-decoder architectures with attention mechanism in unidirectional, bidirectional, and stacked recurrent neural network (RNN) models. Our models generate one-sentence biographies and dining venue descriptions using a crowdsourced dataset where all field value pairs that appear in meaning representations are fully captured in reference sentences. To support this work, we also explore the performances of our models on a more challenging dataset, where the content of a meaning representation is too large to fit into a single sentence, and hence content selection and surface realization need to be learned jointly. This dataset is retrieved by coupling introductory sentences of person-related Turkish Wikipedia articles with their contained infobox tables. Our empirical experiments on both datasets demonstrate that Seq2Seq models are capable of generating coherent and fluent biographies and venue descriptions from field value pairs. We argue that the wealth of knowledge residing in our datasets and the insights obtained from this study hold the potential to give rise to the development of new end-To-end generation approaches for Turkish and other morphologically rich languages.  © 2022 Association for Computing Machinery.",Data-To-Text generation; sequence-To-sequence model; Turkish; Wikipedia,Large dataset; Recurrent neural networks; Speech processing; Data-to-text generation; End to end; Field values; Language generation; Neural-networks; Sequence models; Sequence-to-sequence model; Text generations; Turkishs; Wikipedia; Semantics
A Preliminary Analysis on the Correlates of Stress and Tones in Mizo,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152913099&doi=10.1145%2f3546950&partnerID=40&md5=15cda69a3f88af47bf25bb8988f2fd02,"Stress is the property of a language to exhibit prominence or distinction in one or more syllables in a given domain. The existence of word stress has not been suitably explored in previous acoustic studies of the Mizo language, which is a tonal language of the Kuki-Chin sub-category in Tibeto-Burman language families. In this study, we attempt to analyze word stress on disyllabic target words, specifically in three lexical categories-Adjectives, nouns, and verbs. Utterances of the target words are recorded in isolated setting (out of focus) and in sentence frames (in focus). First, averages of features, namely-duration, intensity, F0, formants, and spectral tilt, are extracted and investigated for identification of stressed and unstressed syllables on a total of 2,880 samples. Next, the interaction of word stress on the four tones of Mizo is investigated. While it is found that H-Tone is generally stressed, inferences are made that stressed syllables are not unique to a specific tone. Third, significance of the selected features are validated using a two-Tailed paired sample t-Test. Our analysis indicates that the mean differences in duration, intensity, and F0 of the stressed and unstressed syllables are significant across the lexical categories at p < 0.05. Next, validations on the significance of the mean differences are carried out using Cohen's d effect size and Pearson's Correlation Coefficient (r). Finally, three machine learning models-Support Vector Machines (SVM), Naive Baye's, and Ensemble learning methods (AdaBoost and Boosted Aggregation), are used to identify stressed and unstressed syllables associated with tones in Mizo. Discriminating differences, especially in disyllabic verbs, are observed between stressed vs. unstressed syllables. Conclusions are drawn that duration is a strong and robust cue for acoustic correlates of stress, while intensity is a medium cue for stress and F0 a weak cue for stress.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",correlates of stress; low-resource language; Mizo; Tonal language; tones; word stress,Adaptive boosting; Correlation methods; Support vector machines; Correlate of stress; Lexical categories; Low resource languages; Mizo; Preliminary analysis; Property; Target words; Tonal languages; Tone; Word stress; Learning systems
IsiXhosa Named Entity Recognition Resources,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152615328&doi=10.1145%2f3531478&partnerID=40&md5=a42c2b6c9af1960ee7c4e291ff840682,"Named entity recognition has been one of the most widely researched natural language processing technologies over the past two decades. For the South African languages, however, relatively little research and development work has been done. This changed with the release of the NCHLT named entity annotated resources, a collection of named entity annotated data and Conditional Random Field-based named entity recognisers for ten of the official languages.In this work, we provide a detailed description and linguistic analysis of the named entity (NE) annotated data for the agglutinative isiXhosa language, by analysing the morphosyntactic features relevant to the three main types of NE, viz. person, location, and organisation. From the data, we identify suffix and capitalisation features that may be good predictors of the different NE types. Based on these features, we describe the named entity recogniser and feature set developed as part of the NCHLT release. The recogniser has high precision, 0.9713 overall, but relatively low recall, 0.7409, especially for person names, 0.5963, resulting in an overall F-score of 0.8406. Although there are various avenues to improve the named entity recogniser, this is a significant release for a historically under-resourced language.  © 2022 Association for Computing Machinery.",isiXhosa; Named entity recognition; natural language processing,Linguistics; African languages; Isixhosa; Language processing; Named entities; Named entity recognition; Natural language processing; Natural languages; Processing technologies; Random fields; Research and development; Natural language processing systems
Radial Basis Function Attention for Named Entity Recognition,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146368221&doi=10.1145%2f3539014&partnerID=40&md5=92e68a248aaea47d532bf7e77121baca,"Attention mechanism is an increasingly important approach in the field of natural language processing (NLP). In the attention-based named entity recognition (NER) model, most attention mechanisms can calculate attention coefficient to express the importance of sentence semantic information but cannot adjust the position distribution of contextual feature vectors in the semantic space. To address this issue, a radial basis function attention (RBF-attention) layer is proposed to adaptively regulate the position distribution of sequence contextual feature vectors, which can minimize the relative distance of within-category named entities and maximize the relative distance of between-category named entities in the semantic space. The experimental results on CoNLL2003 English and MSRA Chinese NER datasets indicate that the proposed model performs better than other baseline approaches without relying on any external feature engineering.  © 2022 Association for Computing Machinery.",BiLSTM; NER; RBF-attention; self-attention,Learning algorithms; Natural language processing systems; Semantics; Vector spaces; Attention mechanisms; Base function; BiLSTM; Contextual feature; Features vector; Named entity recognition; Radial base function attention; Radial basis; Self-attention; Semantic Space; Radial basis function networks
Multi-Label Annotation and Classification of Arabic Texts Based on Extracted Seed Keyphrases and Bi-Gram Alphabet Feed Forward Neural Networks Model,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145769022&doi=10.1145%2f3539607&partnerID=40&md5=0cd6ef1bd2be5ecb1cc680a7c64ec5ba,"In natural language processing, text classification is a fundamental problem. Multi-label classification of textual data is a challenging topic in text classification where an instance can be associated with more than one label. This paper presents a multi-label annotation and classification methodology for Arabic text data that is not currently classified as multi-label, aiming to analyze and compare the performance of various multi-label learning approaches. The current work includes two phases: The first involves automatic annotation of hotel reviews with more than one label based on the aspects found in the reviews. In this phase, review data instances were automatically annotated as multi-label based on the extracted seed keyphrases clusters. The second phase involves experiments to compare the performance of various multi-label classification learning methods. In this phase, we introduced different models including a feed-forward networks model that learns a vector representation based on the bi-gram alphabet rather than the commonly used bag-of-words model. The bi-gram alphabet vector representation model has the advantage of having reduced feature dimensions and not requiring natural language processing tools. The results indicated that employing the bi-gram alphabet vector representation feed forward neural network is a competitive solution for the multi-label text classification problem. It has achieved an accuracy of about 75.2%, and standard deviation (0.062).  © 2022 Association for Computing Machinery.",Arabic text; bi-gram alphabet; multi-label annotation; Multi-label classification; neural networks; sentiment analysis; vector representation,Classification (of information); Feedforward neural networks; Information retrieval; Learning systems; Vectors; Arabic texts; Bi-gram alphabet; Feed forward neural net works; Key-phrase; Multi-label annotation; Multi-label classifications; Neural-networks; Sentiment analysis; Text classification; Vector representations; Sentiment analysis
Unsupervised Word Segmentation with Bi-directional Neural Language Model,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145588552&doi=10.1145%2f3529387&partnerID=40&md5=25d83f991e98f695e5c9301bff84f891,"We propose an unsupervised word segmentation model, in which for each unlabelled sentence sample, the learning objective is to maximize the generation probability of the sentence given its all possible segmentations. Such a generation probability can be factorized into the likelihood of each possible segment given the context in a recursive way. To capture both the long- and short-term dependencies, we propose to use a bi-directional neural language model to better extract the features of the segment's context. Two decoding algorithms were also developed to combine the context features from both directions to generate the final segmentation at the inference time, which helps to reconcile word-boundary ambiguities. Experimental results show that our context-sensitive unsupervised segmentation model achieved state-of-the-art at different evaluation settings on various datasets for Chinese, and the comparable result for Thai.  © 2022 Association for Computing Machinery.",bi-directional neural language model; context-sensitive segmentation; decoding algorithms; recurrent neural networks; Unsupervised word segmentation,Computational linguistics; Decoding; Inference engines; Bi-directional; Bi-directional neural language model; Context-sensitive; Context-sensitive segmentation; Decoding algorithm; Language model; Segmentation models; Sensitive segmentation; Unsupervised word segmentation; Word segmentation; Recurrent neural networks
Neural Topic Model Training with the REBAR Gradient Estimator,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149864611&doi=10.1145%2f3517336&partnerID=40&md5=a082deb122216ef0ab128b427fa3354d,"Topic modelling is an important approach of unsupervised machine learning that allows automatically extracting the main ""topics""from large collections of documents. In addition, topic modelling is able to identify the topic proportions of each individual document, which can be helpful for organizing the collections. Many topic modelling algorithms have been proposed to date, including several that leverage advanced techniques such as variational inference and deep autoencoders. However, to date topic modelling has made limited use of reinforcement learning, a framework that has obtained vast success in many other unsupervised learning tasks. For this reason, in this article we propose training a neural topic model using a reinforcement learning objective and minimizing the objective with the recently-proposed REBAR gradient estimator. Experiments performed over two probing datasets have shown that the proposed model has achieved improvements over all the compared models in terms of both model perplexity and topic coherence, and produced topics that appear qualitatively informative and consistent.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep neural networks; REBAR; reinforcement learning; Topic models; variational autoencoders; variational-autoencoder topic models,Data mining; Inference engines; Learning algorithms; Learning systems; Reinforcement learning; Variational techniques; Auto encoders; Gradient estimator; Model training; Reinforcement learnings; Topic Modeling; Unsupervised machine learning; Variational autoencoder; Variational-autoencoder topic model; Deep neural networks
An Integrated Topic Modelling and Graph Neural Network for Improving Cross-lingual Text Classification,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145781773&doi=10.1145%2f3530260&partnerID=40&md5=f0a2d33ca9f9048927f7ea1eb46473d8,"In recent years, along with the dramatic developments of deep learning in the natural language processing (NLP) domain, notable multilingual pre-trained language techniques have been proposed. These recent multilingual text analysis and mining models have demonstrated state-of-the-art performance in several primitive NLP tasks, including cross-lingual text classification (CLC). However, these recent multilingual pre-trained language models still suffer limitations regarding their adaptation for specific task-driven fine-tuning in the context of low-resource languages. Moreover, they also encounter problems related to the capability of preserving the global semantic (e.g., topic, etc.) and long-range relationships between words to better fine-tune and effectively handle the cross-lingual text classification task. To meet these challenges, in this article, we propose a novel topic-driven multi-typed text graph attention-based representation learning method for dealing with the cross-lingual text classification problem called TG-CTC. In the proposed TG-CTC model, we utilize a novel fused topic-driven multi-typed text graph representation to jointly learn the rich-schematic structural and global semantic information of texts to effectively handle the CLC task. More specifically, we integrate the heterogeneous text graph attention network with the neural topic modelling approach to enrich the semantic information of learned textual representations in the context of multiple languages. Extensive experiments in benchmark multilingual datasets showed the effectiveness of the proposed TG-CTC model compared with the contemporary state-of-the-art baselines.  © 2022 Association for Computing Machinery.",BERT; cross-lingual text classification; graph attention network; Topic modelling,Classification (of information); Deep learning; Graph neural networks; Learning systems; Modeling languages; Natural language processing systems; BERT; Classification tasks; Cross-lingual; Cross-lingual text classification; Graph attention network; Language processing; Natural languages; Semantics Information; Text classification; Topic Modeling; Semantics
Low-resource Neural Machine Translation: Methods and Trends,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151829731&doi=10.1145%2f3524300&partnerID=40&md5=8724037670a581ee8b85c6392aa8ea4a,"Neural Machine Translation (NMT) brings promising improvements in translation quality, but until recently, these models rely on large-scale parallel corpora. As such corpora only exist on a handful of language pairs, the translation performance is far from the desired effect in the majority of low-resource languages. Thus, developing low-resource language translation techniques is crucial and it has become a popular research field in neural machine translation. In this article, we make an overall review of existing deep learning techniques in low-resource NMT. We first show the research status as well as some widely used low-resource datasets. Then, we categorize the existing methods and show some representative works detailedly. Finally, we summarize the common characters among them and outline the future directions in this field.  © 2022 Association for Computing Machinery.",data augmentation; Low-resource; neural machine translation; pivot-based methods; semi-supervised; transfer learning; unsupervised,Computational linguistics; Computer aided language translation; Deep learning; Learning systems; Transfer learning; Data augmentation; Large-scales; Low resource languages; Low-resource; Machine translation methods; Pivot-based method; Semi-supervised; Transfer learning; Translation quality; Unsupervised; Neural machine translation
Exploiting Japanese-Chinese Cognates with Shared Private Representations for NMT,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145768344&doi=10.1145%2f3533429&partnerID=40&md5=dd6e8d7a54b0a71e71445f05133da922,"Neural machine translation has achieved remarkable progress over the past several years; however, little attention has been paid to machine translation (MT) between Japanese and Chinese, which share a large proportion of cognate words that can be utilized as additional linguistic knowledge to enhance translation performance. In this article, we seek to strengthen the semantic correlation between Japanese and Chinese by leveraging cognate words that share common Chinese characters. Specifically, we experiment with three strategies: (1) a shared vocabulary with cognate lexicon induction, which models the commonality between source and target cognates; (2) a shared private representation with a dynamic gating mechanism, which models the language-specific features on the source side; and (3) an embedding shortcut, which enables the decoder to access the shared private representation with shortest distance and aids the training process. The experiments and analysis presented in this article demonstrate that our proposed approaches can significantly improve the performance of both Japanese-to-Chinese and Chinese-to-Japanese translations and verify the effectiveness of exploiting Japanese-Chinese cognates for MT.  © 2022 Association for Computing Machinery.",Chinese character; Cognate; Japanese-Chinese,Computational linguistics; Computer aided language translation; Neural machine translation; Chinese characters; Cognate; Embeddings; Experiment and analysis; Gating mechanisms; Japanese-chinese; Linguistic knowledge; Machine translations; Performance; Training process; Semantics
"Am I a Resource-Poor Language? Data Sets, Embeddings, Models and Analysis for four different NLP Tasks in Telugu Language",2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143261820&doi=10.1145%2f3531535&partnerID=40&md5=88d3681d5d3742c9f499771fc032f4f6,"Due to the lack of a large annotated corpus, many resource-poor Indian languages struggle to reap the benefits of recent deep feature representations in Natural Language Processing (NLP). Moreover, adopting existing language models trained on large English corpora for Indian languages is often limited by data availability, rich morphological variation, syntax, and semantic differences. In this paper, we explore the traditional to recent efficient representations to overcome the challenges of a low resource language, Telugu. In particular, our main objective is to mitigate the low-resource problem for Telugu. Overall, we present several contributions to a resource-poor language viz. Telugu. (i) a large annotated data (35,142 sentences in each task) for multiple NLP tasks such as sentiment analysis, emotion identification, hate-speech detection, and sarcasm detection, (ii) we create different lexicons for sentiment, emotion, and hate-speech for improving the efficiency of the models, (iii) pretrained word and sentence embeddings, and (iv) different pretrained language models for Telugu such as ELMo-Te, BERT-Te, RoBERTa-Te, ALBERT-Te, and DistilBERT-Te on a large Telugu corpus consisting of 8,015,588 sentences (1,637,408 sentences from Telugu Wikipedia and 6,378,180 sentences crawled from different Telugu websites). Further, we show that these representations significantly improve the performance of four NLP tasks and present the benchmark results for Telugu. We argue that our pretrained embeddings are competitive or better than the existing multilingual pretrained models: mBERT, XLM-R, and IndicBERT. Lastly, the fine-tuning of pretrained models show higher performance than linear probing results on four NLP tasks with the following F1-scores: Sentiment (68.72), Emotion (58.04), Hate-Speech (64.27), and Sarcasm (77.93). We also experiment on publicly available Telugu datasets (Named Entity Recognition, Article Genre Classification, and Sentiment Analysis) and find that our Telugu pretrained language models (BERT-Te and RoBERTa-Te) outperform the state-of-the-art system except for the sentiment task. We open-source our corpus, four different datasets, lexicons, embeddings, and code https://github.com/Cha14ran/DREAM-T. The pretrained Transformer models for Telugu are available at https://huggingface.co/ltrctelugu.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",BERT-Te; ELMo-Te; low-resource languages; resource creation; RoBERTa-Te; text classification,Benchmarking; Classification (of information); Computational linguistics; Embeddings; Semantics; Speech recognition; BERT-te; ELMo-te; Embeddings; Language model; Language processing; Low resource languages; Natural languages; Resource creation; RoBERTa-te; Text classification; Sentiment analysis
A Task-oriented Chatbot Based on LSTM and Reinforcement Learning,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145783501&doi=10.1145%2f3529649&partnerID=40&md5=9d781899e521e26b8133137ae72dbd44,"Thanks to the advancements in deep learning, chatbots are widely used in messaging applications. Undoubtedly, a chatbot is a new way of interaction between humans and machines. However, most of the chatbots act as a simple question answering system that responds with formulated answers. Traditional conversational chatbots usually adopt a retrieval-based model that requires a large amount of conversational data for retrieving various intents. Hence, training a chatbot model that uses low-resource conversational data to generate more diverse dialogues is desirable. We propose a method to build a task-oriented chatbot using a sentence generation model that generates sequences based on the generative adversarial network. The architecture of our model contains a generator that generates a diverse sentence and a discriminator that judges the sentences by comparing the generated and the ground-truth sentences. In the generator, we combine the attention model with the sequence-to-sequence model using hierarchical long short-term memory to extract sentence information. For the discriminator, our reward mechanism assigns low rewards for repeated sentences and high rewards for diverse sentences. Extensive experiments are presented to demonstrate the utility of our model that generates more diverse and information-rich sentences than those of the existing approaches.  © 2022 Association for Computing Machinery.",chatbots; deep learning; Dialogue generation; natural language processing; reinforcement learning,Generative adversarial networks; Information retrieval; Long short-term memory; Natural language processing systems; Chatbots; Deep learning; Dialogue generations; Language processing; Natural language processing; Natural languages; Question answering systems; Reinforcement learnings; Simple++; Task-oriented; Reinforcement learning
Token Relation Aware Chinese Named Entity Recognition,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145775176&doi=10.1145%2f3531534&partnerID=40&md5=d00ed910977aec1c1857c9196a50d236,"Due to the lack of natural delimiters, most Chinese Named Entity Recognition (NER) approaches are character-based and utilize an external lexicon to leverage the word-level information. Although they have achieved promising results, the latent words they introduced are still non-contextualized. In this paper, we investigate three relations, i.e., adjacent relation between characters, character co-occurrence relation between latent words, and dependency relation among tokens, to address this issue. Specifically, we first establish the local context for latent words and then propose a masked self-attention mechanism to incorporate such local contextual information. Besides, since introducing external knowledge such as lexicon and dependency relation inevitably brings in some noises, we propose a gated information controller to handle this problem. Extensive experimental results show that the proposed approach surpasses most similar methods on public datasets and demonstrates its promising potential.  © 2022 Association for Computing Machinery.",character adjacency; character co-occurrence; Chinese NER; dependency relation; gated mechanism,Adjacent relation; Character adjacency; Character co-occurrence; Chinese named entity recognition; Co-occurrence; Delimiters; Dependency relation; Gated mechanism; Local contexts; Word level
Character-based Joint Word Segmentation and Part-of-Speech Tagging for Tibetan Based on Deep Learning,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151914390&doi=10.1145%2f3511600&partnerID=40&md5=90e53c7c5010f6082366e980087b3d47,"Tibetan word segmentation and POS tagging are the primary tasks of Tibetan natural language processing. Most of existing methods of Tibetan word segmentation and POS tagging are based on rules and statistics, which need manual construction of features. In addition, the joint mode has shown stronger capabilities for word segmentation and POS tagging and have received great interests. In this paper, we propose Bi-LSTM+IDCNN+CRF structures, a simple yet effective end-to-end neural network model, for joint Tibetan word segmentation and POS tagging. We conduct step-by-step and joint experiments on the Tibetan datasets. The results demonstrate that the performance of the Bi-LSTM+IDCNN+CRF model is the best regardless of the step-by-step or joint mode. We obtain state-of-the-art performance in the joint tagging mode. The F1 score of the word segmentation task reached 92.31%, and the F1 score of the POS tagging task reached 81.26%.  © 2022 Association for Computing Machinery.",deep learning; POS tagging; Tibetan; word segmentation,Computational linguistics; Natural language processing systems; Deep learning; F1 scores; Natural languages; Part of speech tagging; Parts-of-speech tagging; POS tagging; Primary task; Tibetan word segmentations; Tibetans; Word segmentation; Long short-term memory
Static and Dynamic Isolated Indian and Russian Sign Language Recognition with Spatial and Temporal Feature Detection Using Hybrid Neural Network,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145773955&doi=10.1145%2f3530989&partnerID=40&md5=0cd39d1b4e4efed51e3dcc7a65fb7236,"The Sign Language Recognition system intends to recognize the Sign language used by the hearing and vocally impaired populace. The interpretation of isolated sign language from static and dynamic gestures is a difficult study field in machine vision. Managing quick hand movement, facial expression, illumination variations, signer variation, and background complexity are amongst the most serious challenges in this arena. While deep learning-based models have been used to accomplish the entirety of the field's state-of-the-art outcomes, the previous issues have not been fully addressed. To overcome these issues, we propose a Hybrid Neural Network Architecture for the recognition of Isolated Indian and Russian Sign Language. In the case of static gesture recognition, the proposed framework deals with the 3D Convolution Net with an atrous convolution mechanism for spatial feature extraction. For dynamic gesture recognition, the proposed framework is an integration of semantic spatial multi-cue feature detection, extraction, and Temporal-Sequential feature extraction. The semantic spatial multi-cue feature detection and extraction module help in the generation of feature maps for Full-frame, pose, face, and hand. For face and hand detection, GradCam and Camshift algorithm have been used. The temporal and sequential module consists of a modified auto-encoder with a GELU activation function for abstract high-level feature extraction and a hybrid attention layer. The hybrid attention layer is an integration of segmentation and spatial attention mechanism. The proposed work also involves creating a novel multi-signer, single, and double-handed Isolated Sign representation dataset for Indian and Russian Sign Language. The experimentation was done on the novel dataset created. The accuracy obtained for Static Isolated Sign Recognition was 99.76%, and the accuracy obtained for Dynamic Isolated Sign Recognition was 99.85%. We have also compared the performance of our proposed work with other baseline models with benchmark datasets, and our proposed work proved to have better performance in terms of Accuracy metrics.  © 2022 Association for Computing Machinery.",bidirectional LSTM; convolutional neural network; gesture recognition; Indian sign language; Isolated sign language recognition,Audition; Benchmarking; Convolutional neural networks; Deep neural networks; Dynamics; Extraction; Feature extraction; Gesture recognition; Long short-term memory; Network architecture; Palmprint recognition; Semantics; Bidirectional LSTM; Convolutional neural network; Features detections; Features extraction; Gestures recognition; Indian sign languages; Isolated sign language recognition; Sign language; Sign Language recognition; Statics and dynamics; Convolution
Albanian Fake News Detection,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136704749&doi=10.1145%2f3487288&partnerID=40&md5=edf3acd3e3e3357d52d9146538490909,"Recent years have witnessed the vast increase of the phenomenon known as the fake news. Among the main reasons for this increase are the continuous growth of internet and social media usage and the real-time information dissemination opportunity offered by them. Deceiving, misleading content, such as the fake news, especially the type made by and for social media users, is becoming eminently hazardous. Hence, the fake news detection problem has become an important research topic. Despite the recent advances in fake news detection, the lack of fake news corpora for the under-resourced languages is compromising the development and the evaluation of existing approaches in these languages. To fill this huge gap, in this article, we investigate the issue of fake news detection for the Albanian language. In it, we present a new public dataset of labeled true and fake news in Albanian and perform an extensive analysis of machine learning methods for fake news detection. We performed a comprehensive feature engineering and feature selection experiments. In doing so, we explored the Albanian language-related feature categories such as the lexical, syntactic, lying-detection, and psycho-linguistic features. Each article was also modeled in four different ways: with the traditional bag-of-words (BoW) and with three distributed text representations using the state-of-the-art Word2Vec, FastText, and BERT methods. Additionally, we investigated the best combination of features and various types of classification methods. The conducted experiments and obtained results from evaluations are finally used to draw some conclusions. They shed light on the potentiality of the methods and the challenges that the Albanian fake news detection presents.  © 2022 Association for Computing Machinery.",corpus construction; Fake news; machine learning; natural language processing; text categorization,Classification (of information); Fake detection; Feature extraction; Learning algorithms; Linguistics; Natural language processing systems; Social networking (online); Text processing; Albanian languages; Albanians; Corpus construction; Fake news; Language processing; Machine-learning; Natural language processing; Natural languages; Social media; Text categorization; Information dissemination
Text Implicates Prosodic Ambiguity: A Corpus for Intention Identification of the Korean Spoken Language,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145777394&doi=10.1145%2f3529648&partnerID=40&md5=0920f2503bee186e9072b4e41f4c5104,"Phonetic features are indispensable in understanding the spoken language. Especially in Korean, which is wh-in-situ and head-final, the addressee of spoken language sometimes finds it hard to discern the speaker's original intention if not provided with the sentence prosody. However, acoustic information may not be guaranteed for all spoken language processing, due to the difficulty of managing and computing speech data. This article suggests a corpus that aims to distinguish utterances with ambiguous intention from clear-cut ones, utilizing the prosodic ambiguity of the text input. In detail, the resulting classification system decides whether the given text input is one of fragment, statement, question, command, rhetorical question/command, or indecisive, taking into account the intonation-dependency of the text. Based on an intuitive understanding of the Korean language engaged in the data annotation, we construct a corpus with seven intention categories, train classification systems, and validate the utility of our dataset with quantitative and qualitative analyses.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",directiveness; intention identification; intonation-dependency; Korean spoken language; prosodic ambiguity; rhetoricalness; speech act,Linguistics; Text processing; Directiveness; Intention identification; Intonation-dependency; Korean spoken language; Prosodic ambiguity; Prosodics; Rhetoricalness; Speech acts; Spoken languages; Text input; Classification (of information)
"Exploring Multi-lingual, Multi-task, and Adversarial Learning for Low-resource Sentiment Analysis",2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151134086&doi=10.1145%2f3514498&partnerID=40&md5=9e5fedb255403ad3e09cd6621047a610,"Deep learning has become most prominent in solving various Natural Language Processing (NLP) tasks including sentiment analysis. However, these techniques require a considerably large amount of annotated corpus, which is not easy to obtain for most of the languages, especially under the scenario of low-resource settings. In this article, we propose a deep multi-task multi-lingual adversarial framework to solve the resource-scarcity problem of sentiment analysis by leveraging the useful and relevant knowledge from a high-resource language. To transfer the knowledge between the different languages, both the languages are mapped to the shared semantic space using cross-lingual word embeddings. We evaluate our proposed architecture on a low-resource language, Hindi, using English as the high-resource language. Experiments show that our proposed model achieves an accuracy of 60.09% for the movie review dataset and 72.14% for the product review dataset. The effectiveness of our proposed approach is demonstrated with significant performance gains over the state-of-the-art systems and translation-based baselines.  © 2022 Association for Computing Machinery.",adversarial training; low-resource language; multi-lingual; multi-task; Sentiment analysis,Deep learning; Knowledge management; Semantics; Adversarial learning; Adversarial training; Language processing; Large amounts; Low resource languages; Multi tasks; Multi-lingual; Multitask learning; Natural languages; Sentiment analysis; Sentiment analysis
New Vietnamese Corpus for Machine Reading Comprehension of Health News Articles,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147023093&doi=10.1145%2f3527631&partnerID=40&md5=8e4d5a996a4df16a75a8bf019a8beca4,"Machine reading comprehension is a natural language understanding task where the computing system is required to read a text and then find the answer to a specific question posed by a human. Large-scale and high-quality corpora are necessary for evaluating machine reading comprehension models. Furthermore, machine reading comprehension (MRC) for the health sector has potential for practical applications; nevertheless, MRC research in this domain is currently scarce. This article presents UIT-ViNewsQA, a new corpus for the Vietnamese language to evaluate MRC models for the healthcare textual domain. The corpus consists of 22,057 human-generated question-answer pairs. Crowd-workers create the questions and answers on a collection of 4,416 online Vietnamese healthcare news articles, where the answers are textual spans extracted from the corresponding articles. We introduce a process for creating a high-quality corpus for the Vietnamese machine reading comprehension task. Linguistically, our corpus accommodates diversity in question and answer types. In addition, we conduct experiments and compare the effectiveness of different MRC methods based on the neural networks and transformer architectures. Experimental results on our corpus show that the MRC system based on ALBERT architecture outperforms the neural network architectures and the BERT-based approach, an exact match score of 65.26% and an F1-score of 84.89%. The best machine model achieves about 10.90% F1-score less efficiently than humans, which proves that exploring machine models on UIT-ViNewsQA to surpass humans is challenging for researchers in the future. Our corpus is publicly available on our website: http://nlp.uit.edu.vn/datasets for research purposes.  © 2022 Association for Computing Machinery.",Machine reading comprehension; question answering; Vietnamese,Health care; Natural language processing systems; Neural networks; Comprehension models; F1 scores; High quality; Machine modelling; Machine reading comprehension; Natural language understanding; News articles; Question Answering; Reading comprehension; Vietnamese; Network architecture
Exploiting Morpheme and Cross-lingual Knowledge to Enhance Mongolian Named Entity Recognition,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151800671&doi=10.1145%2f3511098&partnerID=40&md5=37bd7a184920fdd411c2d709188627a4,"Mongolian named entity recognition (NER) is not only one of the most crucial and fundamental tasks in Mongolian natural language processing, but also an important step to improve the performance of downstream tasks such as information retrieval, machine translation, and dialog system. However, traditional Mongolian NER models heavily rely on the feature engineering. Even worse, the complex morphological structure of Mongolian words makes the data sparser. To alleviate the feature engineering and data sparsity in Mongolian named entity recognition, we propose a novel NER framework with Multi-Knowledge Enhancement (MKE-NER). Specifically, we introduce both linguistic knowledge through Mongolian morpheme representation and cross-lingual knowledge from Mongolian-Chinese parallel corpus. Furthermore, we design two methods to exploit cross-lingual knowledge sufficiently, i.e., cross-lingual representation and cross-lingual annotation projection. Experimental results demonstrate the effectiveness of our MKE-NER model, which outperforms strong baselines and achieves the best performance (94.04% F1 score) on the traditional Mongolian benchmark. Particularly, extensive experiments with different data scales highlight the superiority of our method in low-resource scenarios.  © 2022 Association for Computing Machinery.",cross-lingual; Mongolian; Named entity recognition; neural network,Benchmarking; Natural language processing systems; Cross-lingual; Feature engineerings; Language processing; Mongolians; Named entity recognition; Natural languages; Neural-networks; Performance; Recognition models; Traditional mongolians; Search engines
Topic Sentiment Analysis for Twitter Data in Indian Languages Using Composite Kernel SVM and Deep Learning,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147124423&doi=10.1145%2f3519297&partnerID=40&md5=deda6acf3c28b65e423cbf95f68190d2,"Sentiment analysis of public opinions on social networks, such as Twitter or Facebook, can provide us with valuable information, which has a wide range of applications. But the efficiency and accuracy of the automated methods for Twitter sentiment analysis are hindered by the special characteristics of the Twitter data. The Twitter data is generally noisy, high-dimensional, and it has complex syntactic and semantic structures. Sentiment analysis of Twitter data in Indian languages is more challenging because the data is multilingual and code-mixed. In this article, we propose various composite kernel functions, each of which is used with Support Vector Machines (SVM) for developing a model for topic sentiment analysis of Twitter data in Indian languages. Each composite kernel function is constructed by taking the weighted summation of multiple single kernel functions defined by us. In addition to our proposed composite kernel SVM method, we use several state-of-the-art deep learning classifiers for topic sentiment classification. Since any suitable Twitter dataset in Indian languages is not available for conducting our experiments, we have developed our own datasets by collecting tweets related to five different Twitter trending topics in India. To prove the robustness and generalization capability of the proposed models, they are also evaluated on the US airline Twitter dataset which is a publicly available benchmark English dataset. The empirical study exhibits that the proposed composite kernel SVM method is effective for the sentiment classification task. In the case of Indian language datasets, the proposed composite kernel SVM method achieves the highest average accuracy of 74% and the highest average F-score of 0.73. On the other hand, the deep learning-based method achieves the average accuracy and the average F-score of 71.31% and 0.70, respectively. In the case of the US airline Twitter dataset, the proposed composite kernel SVM method achieves the average accuracy of 83% and the average F-score of 0.82, which are higher than that of the deep learning-based method.  © 2022 Association for Computing Machinery.",code-mixed; composite kernel; deep learning; Indian languages; Kernel methods; sentiment classification; SVM,Classification (of information); Codes (symbols); Deep learning; Learning systems; Semantics; Social networking (online); Support vector machines; Code-mixed; Composite kernels; Deep learning; Indian languages; Kernel function; Kernel-methods; Sentiment analysis; Sentiment classification; Support vector machine method; Support vectors machine; Sentiment analysis
A Deep OCR for Degraded Bangla Documents,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151898646&doi=10.1145%2f3511807&partnerID=40&md5=643c03fdbd7469ac2a51bccd33faf09a,"Despite the significant success of document image analysis techniques, efficient Optical Character Recognition (OCR) of degraded document images still remains an open problem. Although a body of work has been reported on degraded document recognition for English language, only little attention has been paid to Indic scripts. In this work, we focus on developing a degraded OCR for Bangla, a major Indian language. In general, an OCR system includes segmentation of the foreground text part from the background followed by recognition of the extracted text. The text segmentation module aims to assign the foreground or background label to each pixel of the document image. In this paper, we present a new OCR system which is particularly suitable for degraded quality Bangla document images. The contribution is two fold. In the first phase, we use a semi-supervised Markov Random Field (MRF)-based Generative Adversarial Network (GAN) model (which we call MRF-GAN) for foreground segmentation of texts from degraded text. In the proposed MRF-GAN, we extend the concept of GAN to a multitask learning mechanism where discriminator-classifier networks differentiate between real/fake images and also assign a foreground or background label to each pixel. In the second phase, we propose to use a new encoder-decoder based recognizer that incorporates an attention-based character to a word prediction model, which has the capability of minimizing Word Error Rate (WER). We optimize this network using a Multitask based Transfer Learning scheme (MTTL). We perform experiments on a publicly available degraded Bangla document image dataset as well as on a new degraded printed Hindi document image dataset, which has been created as a part of the present study. Results of the experimentations demonstrate the efficacy of the proposed OCR.  © 2022 Association for Computing Machinery.",BLSTM; CTC; Degraded OCR; GAN; GMM; MRF,Image segmentation; Learning systems; Magnetorheological fluids; Markov processes; Optical character recognition; Pixels; BLSTM; CTC; Degraded optical character recognition; Document image analysis; Document images; GMM; Image analysis techniques; Image datasets; Markov Random Fields; Optical character recognition system; Generative adversarial networks
Research on UAV teaching application and technological innovation with 5G technology and development of high-pole throwing Hydrangea,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139211633&doi=10.1145%2f3529391&partnerID=40&md5=fe6763b18780098708f1b91dd2a16385,"The technical innovation of high pole throwing Hydrangea is increasing, this study compares with the existing projection technology theory from two aspects of teaching and competition, promotes the corresponding improvement of competition rules and referee law enforcement this paper studies the history and culture of Ethnic Zhuangs' embroidered ball and the development process of modern throwing embroidered ball by using the method of literature review; studies the technical characteristics of hing-pole throwing Hydrangea by using the method of mathematical analysis; compare the landing points of two main Hydrangea throwing techniques after passing through the circle by using UAV aerial photography of Hydrangea flying tracks; and studies the Self-learning Systems and Pattern Recognition and Exploitation for Multimedia Asian Information Processing in which the disputes existing in the process of competition rules and referee enforcement. The research shows that: the competition rules and the referee's decision are controversial, the existing projection teaching technology is lack of practical guidance for high-level competition, innovative projection technology is divided into three matching situations, the best shot angle range is 640<α<720; the shot speed range is 13.04m/s < V0< 13.70m/s. Suggestions: According to the observation and analysis of the competition scene using UAV and Internet and other science and technology, improve the teaching and competitive level of Hydrangea throwing, improve the competition rules, enhance the referee's law enforcement ability, so as to improve the competitive level of national sports, and take the standardized and international road to inherit the excellent national sports culture,and provide new development ideas for the popularization and promotion of national traditional sports and the inheritance of excellent national traditional sports culture. © 2022 Association for Computing Machinery.",competition rules; enforcement ability; hing-pole throwing Hydrangea; judging ofreferees; mathematical analysis; projection technology; UAV teaching,Aerial photography; Antennas; Engineering education; Learning systems; Pattern recognition; Poles; Competition rules; Enforcement ability; Hing-pole throwing hydrangea; Judging ofreferee; Mathematical analysis; Projection technology; Sports cultures; Teaching applications; Technological innovation; UAV teaching; Unmanned aerial vehicles (UAV)
A Novel Social Interaction Assistive Device for Arab Deaf People,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151909342&doi=10.1145%2f3508374&partnerID=40&md5=8d7d56ac4b4a4612b0de39017edb1906,"Many deaf people worldwide face problems with integrating into society and interacting with people who do not understand sign language. This can lead to isolation and difficulty in expressing feelings. In this research, our primary goal is to help deaf people communicate, express their feelings, and socialize with others. Toward that end, 40 Arabic words that are commonly used in social interactions were used to build a dataset of hand movements used by deaf people to express these words. These movements were recorded using a Leap Motion Controller (LMC). The resulting dataset consists of 1,579 instances and 112 features, recorded with the help of five deaf persons. Feature reduction and oversampling techniques were applied to analyze the dataset. Machine learning algorithms were then used to build a model that is able to classify any given hand posture or gesture into one of those 40 words. This work compared the performance of nine classification algorithms: Random Forest, Decision Table, Classification via Regression, K-Nearest Neighbor (KNN), Simple Logistic, Input Mapped Classifier, Random Tree, J48, and Bayes network. Results show that the Random Forest model achieved the highest accuracy with over 90%, outperforming the other eight models. Subsequently, a usability study was conducted by 10 deaf people to test the effectiveness of the proposed assistive device. The results suggest that the proposed device is useful for facilitating social communication with deaf people. It also suggests that the device was preferred, when compared with other relevant devices.  © 2022 Association for Computing Machinery.",Arabic sign language; assistive device; deaf people; leap motion; machine learning; social communication; social interaction,Bayesian networks; Decision trees; Learning algorithms; Machine learning; Nearest neighbor search; Arabic sign language; Assistive devices; Deaf peoples; Hands movement; Leap motion; Machine-learning; Motion controller; Sign language; Social communications; Social interactions; Decision tables
DEAF-BSL: Deep lEArning Framework for British Sign Language recognition,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141708776&doi=10.1145%2f3513004&partnerID=40&md5=de0e5ebdfdfab4cf00e6c043514dcfdb,"The recent development of disability studies in academic bodies has expedited the promotion of investigation on disability. With computer-aided tools, communication between the impaired person and someone who does not understand sign language could be accessible. A large number of people across the world are using sign language (e.g., British Sign Language (BSL), Asian Sign Language (ASL), Indian Sign Language (ISL), etc.) with hand gestures for communication. In BSL recognition, the involvement of both hands overlapping each other becomes the main challenge. Moreover, BSL comprises ambiguous signs concerning viewpoint. However, existing traditional techniques seem in-stable, less accurate, and inefficient. In this work, the BSL fingerspelling alphabet recognition problem explores using a Deep learning framework to address the above-mentioned concerns. Convolutional Neural Network (CNN) is employed to detect and recognize for classification of 26 alphabets after being trained on the BSL corpus dataset. The proposed work outperforms the existing works with better precision (6%), recall (4%), and F-measure (5%). It reported better results on the BSL corpus dataset and webcam videos. The model achieved better accuracy (98.0%) for a large lexicon of words than previous models (Goh & Holden [6]: 69.5%, Rambhau [9]: 79.2%, and Liwicki et al. [8]: 92.5%). The 3D CNN-based proposal performs robust hand detection, much more accurate sign recognition, more scalability, and less ambiguity in BSL finger-spelling recognition.  © 2022 Association for Computing Machinery.",3D CNN; ASL; BSL corpus; disability; finger-spelling; hand gestures,Convolutional neural networks; Deep learning; Palmprint recognition; 3d convolutional neural network; Asian sign language; British sign language corpus; Convolutional neural network; Disability; Finger spelling; Hand gesture; Learning frameworks; Sign language; Classification (of information)
Investigating the Effect of Preprocessing Arabic Text on Offensive Language and Hate Speech Detection,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130502532&doi=10.1145%2f3501398&partnerID=40&md5=90c1802c93aa74301afd90e34b6c5788,"Preprocessing of input text can play a key role in text classification by reducing dimensionality and removing unnecessary content. This study aims to investigate the impact of preprocessing on Arabic offensive language classification. We explore six preprocessing techniques: conversion of emojis to Arabic textual labels, normalization of different forms of Arabic letters, normalization of selected nouns from dialectal Arabic to Modern Standard Arabic, conversion of selected hyponyms to hypernyms, hashtag segmentation, and basic cleaning such as removing numbers, kashidas, diacritics, and HTML tags. We also experiment with raw text and a combination of all six preprocessing techniques. We apply different types of classifiers in our experiments including traditional machine learning, ensemble machine learning, Artificial Neural Networks, and Bidirectional Encoder Representations from Transformers (BERT)-based models to analyze the impact of preprocessing. Our results demonstrate significant variations in the effects of preprocessing on each classifier type and on each dataset. Classifiers that are based on BERT do not benefit from preprocessing, while traditional machine learning classifiers do. However, these results can benefit from validation on larger datasets that cover broader domains and dialects.  © 2022 Association for Computing Machinery.",Arabic language; Artificial neural networks; Machine learning; Natural language processing; Offensive language detection,Classification (of information); Learning algorithms; Machine learning; Natural language processing systems; Text processing; Arabic languages; Arabic texts; Dialectal arabics; Language detection; Normalisation; Offensive language detection; Offensive languages; Pre-processing techniques; Speech detection; Textual labels; Neural networks
Contextual-Aware Information Extractor with Adaptive Objective for Chinese Medical Dialogues,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147935436&doi=10.1145%2f3511602&partnerID=40&md5=d1cf24178a7af610fd5a982c7a3aff09,"Electronic Medical Records (EMRs) are the foundation of modern medical information systems. Despite the benefits of EMRs, the exhausting process of constructing EMRs decreases the efficiency of medical consultation. Therefore, it becomes an emerging research field to automatically extract EMRs from medical dialogues. In Chinese medical dialogues, the phenomena of omission and reference are extremely common, leading to strong contextual relevance among utterances. However, recent studies on converting Chinese medical dialogues to EMRs lack a reliable mechanism to effectively exploit the contextual relevance information among utterances. Moreover, they neglect the frequency imbalance of different items and treat these items indiscriminately, which eventually degrade the overall system performance. In this article, we proposed a Contextual-Aware Information Extractor (CANE), which employs a local-to-global mechanism over utterances to model the contextual relevance among utterances. Furthermore, an adaptive objective is introduced to alleviate the frequency imbalance of items by dynamically assigning weights to each sample. Experimental results indicate that CANE outperforms previous state-of-the-arts with considerable improvements (+6.11% and +3.39% on F1-score).  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive objective; Chinese medical dialogue; contextual-aware; electronic medical records,Data mining; Medical computing; Adaptive objective; Chinese medical dialog; Contextual-aware; Electronic medical record; Exhausting process; Global mechanisms; Medical record; Research fields; State of the art; Systems performance; Medical information systems
Arabic Word Sense Disambiguation for Information Retrieval,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130478605&doi=10.1145%2f3510451&partnerID=40&md5=03b36be7e1caadcca00c3a5273605280,"In the context of using semantic resources for information retrieval, the relationship and distance between concepts are considered important for word sense disambiguation. In this article, we experiment with Conceptual Density and Random Walk with graph methods to enhance the performance of the Arabic Information Retrieval System. To do this, a medium-sized corpus was used. The results proved that Random Walk can enhance the performance of the information retrieval system by achieving a mean improvement of 13%, 16%, and 12% in terms of recall, precision, and F-score, respectively.  © 2022 Association for Computing Machinery.",Arabic WordNet; Conceptual density; Information retrieval; IRS; PageRank; WSD,Information retrieval systems; Natural language processing systems; Random processes; Search engines; Semantics; Arabic wordnet; Conceptual density; Information-retrieval systems; IRS; Page ranks; Performance; Random Walk; Semantic resources; Word Sense Disambiguation; WSD; Information retrieval
Mulan: A Multiple Residual Article-Wise Attention Network for Legal Judgment Prediction,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130465941&doi=10.1145%2f3503157&partnerID=40&md5=ac74286f91d401cfef4d28a0f52d8792,"Legal judgment prediction (LJP) is used to predict judgment results based on the description of individual legal cases. In order to be more suitable for actual application scenarios in which the case has cited multiple articles and has multiple charges, we formulate legal judgment prediction as a multiple label learning problem and present a deep learning model that can effectively encode the content of each legal case via a multi-residual convolution neural network and the semantics of law articles via an article encoder. An article-wise attention mechanism is proposed to fuse the two types of encoded information. Experimental results derived on the CAIL2018 datasets show that our model provides a significant performance improvement over the existing neural models in predicting relevant law articles and charges.  © 2022 Association for Computing Machinery.",Legal judgment prediction; Neural networks,Deep learning; Semantics; Application scenario; Convolution neural network; Learning models; Learning problem; Legal case; Legal judgements; Legal judgment prediction; Multiple charge; Multiple labels; Neural-networks; Forecasting
Multi-domain Spoken Language Understanding Using Domain-And Task-Aware Parameterization,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130504701&doi=10.1145%2f3502198&partnerID=40&md5=724953fb0f4a38c0da6f5969463975e1,"Spoken language understanding (SLU) has been addressed as a supervised learning problem, where a set of training data is available for each domain. However, annotating data for a new domain can be both financially costly and non-scalable. One existing approach solves the problem by conducting multi-domain learning where parameters are shared for joint training across domains, which is domain-Agnostic and task-Agnostic. In the article, we propose to improve the parameterization of this method by using domain-specific and task-specific model parameters for fine-grained knowledge representation and transfer. Experiments on five domains show that our model is more effective for multi-domain SLU and obtain the best results. In addition, we show its transferability when adapting to a new domain with little data, outperforming the prior best model by 12.4%. Finally, we explore the strong pre-Trained model in our framework and find that the contributions from our framework do not fully overlap with contextualized word representations (RoBERTa).  © 2022 Association for Computing Machinery.",Domain-specific and task-specific model; Fine-grained knowledge representation and transfer; Multi-domain spoken language understanding,Knowledge management; Domain specific; Domain-specific and task-specific model; Fine grained; Fine-grained knowledge representation and transfer; Knowledge transfer; Knowledge-representation; Multi-domain spoken language understanding; Multi-domains; Spoken language understanding; Task-specific models; Knowledge representation
Chinese Event Extraction via Graph Attention Network,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130497079&doi=10.1145%2f3494533&partnerID=40&md5=aabff41c9ff4aa1a6157d441f0e435f7,"Event extraction plays an important role in natural language processing (NLP) applications, including question answering and information retrieval. Most of the previous state-of-The-Art methods were lack of ability in capturing features in long range. Recent methods applied dependency tree via dependency-bridge and attention-based graph. However, most of the automatic processing tools used in those methods show poor performance on Chinese texts due to mismatching between word segmentation and labels, which results in error propagation. In this article, we propose a novel character-level Chinese event extraction framework via graph attention network (CAEE). We build our model upon the sequence labeling model, but enhance it with word information by incorporating the word lexicon into the character representations. We further exploit the inter-dependencies between event triggers and argument by building a word-character-based graph network via syntactic shortcut arcs with dependency-parsing. The architecture of the graph minimizes error propagation, which is the result of the error detection of the word boundaries in the processing of Chinese texts. To demonstrate the effectiveness of our work, we build a large-scale real-world corpus consisting of announcements of Chinese financial news without golden entities. Experiments on the corpus show that our approach achieves competitive results compared with previous work in the field of Chinese texts.  © 2022 Association for Computing Machinery.",Event extraction; Graph neural network,Backpropagation; Errors; Graph neural networks; Natural language processing systems; Syntactics; Trees (mathematics); Automatic processing; Chinese text; Dependency trees; Error propagation; Events extractions; Graph neural networks; Natural language processing applications; Processing tools; Question Answering; State-of-the-art methods; Extraction
Interactive Gated Decoder for Machine Reading Comprehension,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130437501&doi=10.1145%2f3501399&partnerID=40&md5=b6923c244395f26f7a4b7607c925281e,"Owing to the availability of various large-scale Machine Reading Comprehension (MRC) datasets, building an effective model to extract passage spans for question answering has been well studied in previous works. However, in reality, there are some questions that cannot be answered through the passage information, which brings more challenges to this task. In this article, we propose an Interactive Gated Decoder (IG Decoder), which focuses on modeling the interactions between the answer span prediction and no-Answer prediction with a gating mechanism. We also propose a simple but effective approach for automatically generating pseudo training data, which aims to enrich the training data of the unanswerable questions. Experimental results on popular benchmark SQuAD 2.0 and NewsQA show that the proposed approaches yield consistent improvements over traditional BERT-large and strong ALBERT-xxlarge baseline systems. We also provide detailed ablations of the proposed method and error analysis on hard samples, which could be helpful in future research.  © 2022 Association for Computing Machinery.",Machine reading comprehension; Neural networks; Question answering,Large dataset; Baseline systems; Effective approaches; Gating mechanisms; Large-scales; Machine reading comprehension; Neural-networks; Question Answering; Reading comprehension; Simple++; Training data; Decoding
Word Level Script Identification Using Convolutional Neural Network Enhancement for Scenic Images,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130418773&doi=10.1145%2f3506699&partnerID=40&md5=963132165be48eedcb6cc85e0155c9f4,"Script identification from complex and colorful images is an integral part of the text recognition and classification system. Such images may contain twofold challenges: (1) Challenges related to the camera like blurring effect, non-uniform illumination and noisy background, and so on, and (2) Challenges related to the text shape, orientation, and text size. The present work in this area is much focused on non-Indian scripts. In contrast, Gurumukhi, Hindi, and English scripts play a vital role in communication among Indians and foreigners. In this article, we focus on the above said challenges in the field of identifying the script. Additionally, we have introduced a new dataset that contains Hindi, Gurumukhi, and English scripts from scenic images collected from different sources. We also proposed a CNN-based model, which is capable of distinguishing between the scripts with good accuracy. Performance of the method has been evaluated for own dataset, i.e., NITJDATASET and other benchmarked datasets available for Indian scripts, i.e., CVSI-2015 (Task-1 and Task 4) and ILST. This work is an extension to find the script from strict text background.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Benchmarked datasets; Convolutional neural network; Natural scene images; Script identification; Transfer learning,Character recognition; Convolution; Image enhancement; Text processing; Benchmarked dataset; Colorful images; Complex image; Convolutional neural network; Indian scripts; Integral part; Natural scene images; Script identification; Transfer learning; Word level; Convolutional neural networks
Dual Discriminator GAN: Restoring Ancient Yi Characters,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130486246&doi=10.1145%2f3490031&partnerID=40&md5=1622b8862f722162262feed52b1b3042,"In China, the damage of ancient Yi books are serious. Due to the lack of ancient Yi experts, the repairation of ancient Yi books is progressing very slowly. The artificial intelligence is successful in the field of image and text, so it is feasible for the automatic restoration of ancient books. In this article, a generative adversarial networks with dual discriminator (DDGAN) is designed to restore incomplete characters in the ancient Yi literature. The DDGAN integrates the deep convolution generative adversarial network with an ancient Yi comparison discriminator. Through two training stages, it could iteratively optimizes the ancient Yi character generation networks to obtain the text generator According to the loss of comparison discriminator, DDGAN mode could be optimized. The DDGAN model can generate characters to restore the missing stroke in the ancient Yi. The experiment shows that the proposed method achieves a restoration rate of 77.3% when no more than one third of the characters are missing. This work is effective for the protection of Yi ancient books.  © 2022 Association for Computing Machinery.",Deep learning; Generative adversarial networks; Gradient descent; Yi characters,Deep learning; Discriminators; Generative adversarial networks; Optimization; Restoration; Character generation; Deep learning; Gradient-descent; Text generators; Yi character; Gradient methods
A Lemmatizer for Low-resource Languages: WSD and Its Role in the Assamese Language,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130464116&doi=10.1145%2f3502157&partnerID=40&md5=88b0340f5a380c9f07a3d10bfab8b0a9,"The morphological variations of highly inflected languages that appear in a text impede the progress of computer processing and root word determination tasks while extracting an abstract. As a remedy to this difficulty, a lemmatization algorithm is developed, and its effectiveness is evaluated for Word Sense Disambiguation (WSD). Having observed its usefulness, lemmatizer is considered for developing Natural Language Processing tools for languages rich in morphological variations. Among various Indian highly inflected languages, Assamese, spoken by over 14 million people in the North-Eastern region of India, is also one of them. In this present work, after a detailed study on the possible transformations through which surface words are created from lemmas, we have designed an Assamese lemmatizer in such a manner that suitable reverse transformations can be employed on a surface word to derive the co-relative (similar) lemma back. And it has been observed that the lemmatizer is competent to deal with inflectional and derivational morphology in Assamese, and the same was evaluated on various Assamese articles extracted from the Assamese Corpus consisting of 50,000 surface words (excluding proper nouns), and the result that it yielded with 82% accuracy was quite encouraging and satisfying, as Assamese is a low-level language and no research work has been done in the Assamese language regarding the lemmatization of words. Considering the result obtained, the lemmatizer is then evaluated for Assamese WSD. For this purpose, 10 highly polysemous Assamese words are taken into account for sense disambiguation. We have also regarded varied WSD systems and observed that such systems enhance the effectiveness of all the WSD systems, which is statistically significant.  © 2022 Association for Computing Machinery.",Assamese; Lemmatizer; Natural Language Processing (NLP); Word sense disambiguation (WSD),Abstracting; Morphology; Assamese; Computer processing; Lemmatization algorithms; Lemmatizer; Low resource languages; Morphological variation; Natural language processing; Word Sense Disambiguation; Word sense disambiguation systems; Natural language processing systems
Combining a Novel Scoring Approach with Arabic Stemming Techniques for Arabic Chatbots Conversation Engine,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130457555&doi=10.1145%2f3511215&partnerID=40&md5=7518266e212750ab1a9217ea3ebd7c4e,"Arabic is recognized as one of the main languages around the world. Many attempts and efforts have been done to provide computing solutions to support the language. Developing Arabic chatbots is still an evolving research field and requires extra efforts due to the nature of the language. One of the common tasks of any natural language processing application is the stemming step. It is important for developing chatbots, since it helps with pre-processing the input data and it can be involved with different phases of the chatbot development process. The aim of this article is to combine a scoring approach with Arabic stemming techniques for developing an Arabic chatbot conversation engine. Two experiments are conducted to evaluate the proposed solution. The first experiment is to select which stemmer is more accurate when applying our solution, since our algorithm can support various stemmers. The second experiment was conducted to evaluate our proposed approach against various machine learning models. The results show that the ISRIS stemming algorithm is the best fit for our solution with accuracy 78.06%. The results also indicate that our novel solution achieved an F1 score of 65.5%, while the other machine learning models achieved slightly lower scores. Our study presents a novel technique by combining scoring mechanisms with stemming processes to produce the best answer for every query sent by chatbots users compared to other approaches. This can be helpful for developing Arabic chatbot and can support many domains such as education, business, and health. This technique is among the first techniques that developed purposefully to serve the development of Arabic chatbots conversation engine.  © 2022 Association for Computing Machinery.",Arabic language; Chatbot; Machine learning; Natural language processing; Stemming,Data handling; Engines; Learning algorithms; Machine learning; Query processing; Arabic languages; Arabic stemming; Chatbots; Computing solutions; Conversation engines; Machine learning models; Natural language processing applications; Pre-processing; Research fields; Stemming; Natural language processing systems
Handwritten New Tai Lue Character Recognition Using Convolutional Prior Features and Deep Variationally Sparse Gaussian Process Modeling,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130424501&doi=10.1145%2f3506700&partnerID=40&md5=e638dee307a559a947a4968d875fbf83,"New Tai Lue is widely used in Southwest China and Southeast Asia. Hence, it is important to study related handwritten character recognition. Considering the many similar characters in handwritten New Tai Lue, this paper proposes an offline handwritten New Tai Lue character recognition method based on convolutional prior features and deep variationally sparse Gaussian process (DVSGP) modeling. An offline handwritten database is constructed, a convolutional neural network is trained to extract the convolutional features of New Tai Lue character images as prior features, and a DVSGP model is built. The extracted features are input into the DVSGP model to construct a recognition model. The experimental results show that the accuracy of the model is 97.67% and that the precision, recall, and F1-score are 0.9769, 0.9767, and 0.9767, respectively, which are better than those of other methods. The proposed method also achieves high accuracy on the MNIST recognition task, verifying its universal applicability.  © 2022 Association for Computing Machinery.",Convolutional prior features; Deep Gaussian processes; Handwritten New Tai Lue character recognition,Character recognition; Convolutional neural networks; Gaussian distribution; Gaussian noise (electronic); Convolutional prior feature; Deep gaussian process; Gaussian process models; Handwritten character recognition; Handwritten new tai lue character recognition; Off-line handwritten; Recognition methods; Southeast Asia; Southwest China; Sparse Gaussian process; Convolution
Arabic Fake News Detection: A Fact Checking Based Deep Learning Approach,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130441754&doi=10.1145%2f3501401&partnerID=40&md5=a91aab85a1a60aafe6d4f73025284a04,"Fake news stories can polarize society, particularly during political events. They undermine confidence in the media in general. Current NLP systems are still lacking the ability to properly interpret and classify Arabic fake news. Given the high stakes involved, determining truth in social media has recently become an emerging research that is attracting tremendous attention. Our literature review indicates that applying the state-of-The-Art approaches on news content address some challenges in detecting fake news' characteristics, which needs auxiliary information to make a clear determination. Moreover, the Social-context-based' and propagation-based' approaches can be either an alternative or complementary strategy to content-based approaches. The main goal of our research is to develop a model capable of automatically detecting truth given an Arabic news or claim. In particular, we propose a deep neural network approach that can classify fake and real news claims by exploiting Convolutional Neuron Networks'. Our approach attempts to solve the problem from the fact checking perspective, where the fact-checking task involves predicting whether a given news text claim is factually authentic or fake. We opt to use an Arabic balanced corpus to build our model because it unifies stance detection, stance rationale, relevant document retrieval and fact-checking. The model is trained on different well selected attributes. An extensive evaluation has been conducted to demonstrate the ability of the fact-checking task in detecting the Arabic fake news. Our model outperforms the performance of the state-of-The-Art approaches when applied to the same Arabic dataset with the highest accuracy of 91%.  © 2022 Association for Computing Machinery.",Arabic corpus; Convolutional Neural Network; Deep learning; Fact checking; Fake news detection; Social media,Backpropagation; Convolution; Convolutional neural networks; Deep neural networks; Fake detection; Arabic corpus; Convolutional neural network; Deep learning; Fact checking; Fake news detection; Learning approach; Polaris; Political events; Social media; State-of-the-art approach; Social networking (online)
Advancing Chinese Event Detection via Revisiting Character Information,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130447858&doi=10.1145%2f3502197&partnerID=40&md5=9e875eb73b72e53c94afafd0e4eb29a1,"Recently, character information has been successfully introduced into the encoder-decoder event detection model to relieve the trigger-word mismatch problem, thus achieving impressive results in the languages without natural delimiters (i.e., Chinese). However, it is introduced into the encoder or the decoder separately, which makes the advantage of character information not be captured and represented adequately for event detection. In this article, we proposed a novel method to model character information in both the encoding and decoding stages to advance the neural event detection model. In particular, the proposed method can encode both words and characters and predict their event types jointly and further leverage interactions between word and its characters to optimize the inference. Experimental results show that the proposed model outperforms previous event detection methods on the ACE2005 Chinese benchmark. We release our code at Github.1  © 2022 Association for Computing Machinery.",Chinese; Event detection; Integer linear programming; Word and character,Decoding; Integer programming; Signal encoding; Chinese; Delimiters; Detection models; Encoder-decoder; Encoding and decoding; Events detection; Integer Linear Programming; Mismatch problems; Novel methods; Word and characters; Encoding (symbols)
Emotion Recognition with Conversational Generation Transfer,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130507294&doi=10.1145%2f3494532&partnerID=40&md5=b8cf3ce0443d899a73cffeb3efc2b3a3,"Emotion recognition in conversation is one of the essential tasks of natural language processing. However, this task's annotation data is insufficient since such data is hard to collect and annotate. Meanwhile, there is large-scale data for conversational generation, and this data does not need annotation manually. But, whether the vector space between different datasets is similar will be a problem. Therefore, we utilize a same dataset to train the conversational generator and the classifier, and transfer knowledge between them. In particular, we propose an Emotion Recognition with Conversational Generation Transfer (ERCGT) framework to model the interaction among utterances by transfer learning. First, we train a conversational generator. In the second step, a transfer learning model is used to transfer the knowledge of generator to the emotion recognition model. Empirical studies illustrate the effectiveness of the proposed framework over several strong baselines on three benchmark emotion classification datasets.  © 2022 Association for Computing Machinery.",Conversational generation; Emotion recognition in conversation; Transfer learning,Classification (of information); Learning systems; Natural language processing systems; Vector spaces; Classification datasets; Conversational generation; Emotion recognition; Emotion recognition in conversation; Empirical studies; Large scale data; Learning models; Recognition models; Space between; Transfer learning; Speech recognition
Konkani WordNet: Corpus-Based Enhancement using Crowdsourcing,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130484155&doi=10.1145%2f3503156&partnerID=40&md5=cd068a61c4ad74b0878bef25fee75c01,"Konkani is one of the languages included in the eighth schedule of the Indian constitution. It is the official language of Goa and is spoken mainly in Goa and some places in Karnataka and Kerala. Konkani WordNet or Konkani Shabdamalem (kakanaabdam) as it has been referred to, was developed under the Indradhanush WordNet Project Consortium during the period from August 2010 to October 2013. This project was funded by Technology Development for Indian Languages (TDIL), Department of Electronics & Information Technology (Deity), and Ministry of Communication and Information Technology (MCIT). The work on Konkani WordNet has halted since the end of the project. Currently, the Konkani WordNet contains around 32,370 synsets. However, to make it a powerful resource for NLP applications in the Konkani language, a need is felt for research work toward enhancement of the Konkani WordNet via community involvement. Crowdsourcing is a technique in which the knowledge of the crowd is utilized to accomplish a particular task.In this article, we have presented the details of the crowdsourcing platform named ""Konkani Shabdarth""(kákanśabdrth). Konkani Shabdarth attempts to use the knowledge of Konkani speaking people for creating new synsets and perform the quantitative enhancement of the wordnet. It also intends to work toward enhancing the overall quality of the Konkani WordNet by validating the existing synsets, and adding the missing words to the existing synsets. A text corpus named ""Konkani Shabdarth Corpus"", has been created from the Konkani literature while implementing the Konkani Shabdarth tool. Using this corpus, 572 root words that are missing from the Konkani WordNet have been identified which are given as input to Konkani Shabdarth. As of now, total 94 users have registered on the platform, out of which 25 users have actually played the game. Currently, 71 new synsets have been obtained for 21 words. For some of the words, multiple entries for the concept definition have been received. This overlap is essential for automating the process of validating the synsets. Due to the pandemic period, it has been difficult to train and get players to actually play the game and contribute. We studied the impact of adding missing words from other existing Konkani text corpus on the coverage of Konkani WordNet. The expected increase in the percentage coverage of Konkani WordNet has been found to be in the range 20-27 after adding the missing words from the Konkani Shabdarth corpus in comparison to the other corpora for which the increase is in the range 1-10.  © 2022 Association for Computing Machinery.",Crowdsourcing; Konkani Shabdarth; Konkani Wordnet; WordNet,Crowdsourcing; Corpus-based; Karnataka; Konkani shabdarth; Konkani wordnet; Official languages; Synsets; Technology development; Text corpora; Wordnet; Ontology
Word Sense Disambiguation using Cooperative Game Theory and Fuzzy Hindi WordNet based on ConceptNet,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130471916&doi=10.1145%2f3502739&partnerID=40&md5=ce1c70a91bbbedab898d4c9132b18fc0,"Natural Language is fuzzy in nature. The fuzziness of Hindi language was captured in the Fuzzy Hindi WordNet (FHWN). FHWN assigned membership values to fuzzy relationships by consulting experts from various domains. However, these membership values need to be corrected. In the proposed work, we compute the membership values of fuzzy semantic relations using ConceptNet. Later, we perform WSD of Hindi text using cooperative game theoretic approach. We used the Shapley Value centrality measure where we predict which coalition of players (word senses) proves to be the most beneficial. We tested and compared our algorithm with the existing state-of-The-Art approaches of Hindi on three datasets and results are better on all the three datasets. One more notable aspect is that the results are quite stable even if the fuzzy membership values of fuzzy graphs changes.  © 2022 Association for Computing Machinery.",ConceptNet; Cooperative game theory; Fuzzy Hindi WordNet; Shapley Value; Word sense disambiguation,Fuzzy sets; Graph theory; Natural language processing systems; Ontology; Semantics; ConceptNet; Cooperative game theory; Fuzzy hindi wordnet; Fuzzy relationship; Fuzzy semantics; Membership values; Natural languages; Shapley value; Word Sense Disambiguation; Wordnet; Game theory
Text-To-Speech Synthesis: Literature Review with an Emphasis on Malayalam Language,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130427715&doi=10.1145%2f3501397&partnerID=40&md5=653025d34e23995060c2954f95bcbcfc,"Text-To-Speech Synthesis (TTS) is an active area of research to generate synthetic speech from underlying text. The identified syllables are uttered with proper duration and prosody characteristics to emulate natural speech. It falls under the category of Natural Language Processing (NLP), which aims to bridge the gap in communication between human and machine. So far as Western languages like English are concerned, the research to produce intelligent and natural synthetic speech has advanced considerably. But in a multilingual state like India, many regional languages viz. Malayalam is underexplored when it comes to NLP. In this article, we try to amalgamate the major research works performed in the area of TTS in English and the prominent Indian languages, with a special emphasis on the South Indian language, Malayalam. This review intends to provide right direction to the research activities in the language, in the area of TTS.  © 2022 Association for Computing Machinery.",Indian language TTS; Malayalam TTS; Text to speech synthesis; TTS literature review,Natural language processing systems; Active area; Indian language text-to-speech synthesis; Indian languages; Literature reviews; Malayalam text-to-speech synthesis; Malayalams; Natural speech; Research activities; Synthetic speech; Text-to-speech synthesis literature review; Speech synthesis
Chinese EmoBank: Building Valence-Arousal Resources for Dimensional Sentiment Analysis,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130453840&doi=10.1145%2f3489141&partnerID=40&md5=19440d37132877b503d4611eb29a64d3,"An increasing amount of research has recently focused on dimensional sentiment analysis that represents affective states as continuous numerical values on multiple dimensions, such as valence-Arousal (VA) space. Compared to the categorical approach that represents affective states as distinct classes (e.g., positive and negative), the dimensional approach can provide more fine-grained (real-valued) sentiment analysis. However, dimensional sentiment resources with valence-Arousal ratings are very rare, especially for the Chinese language. Therefore, this study aims to: (1) Build a Chinese valence-Arousal resource called Chinese EmoBank, the first Chinese dimensional sentiment resource featuring various levels of text granularity including 5,512 single words, 2,998 multi-word phrases, 2,582 single sentences, and 2,969 multi-sentence texts. The valence-Arousal ratings are annotated by crowdsourcing based on the Self-Assessment Manikin (SAM) rating scale. A corpus cleanup procedure is then performed to improve annotation quality by removing outlier ratings and improper texts. (2) Evaluate the proposed resource using different categories of classifiers such as lexicon-based, regression-based, and neural-network-based methods, and comparing their performance to a similar evaluation of an English dimensional sentiment resource.  © 2022 Copyright held by the owner/author(s).",Affective computing; Dimensional sentiment analysis; Valence-Arousal prediction,Affective Computing; Affective state; Chinese language; Dimensional sentiment analyse; Fine grained; Multiple dimensions; Numerical values; Sentiment analysis; Single words; Valence-arousal prediction; Sentiment analysis
"Hypernymy Detection for Low-resource Languages: A Study for Hindi, Bengali, and Amharic",2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130438188&doi=10.1145%2f3490389&partnerID=40&md5=c87f5eac58be1884e8c0c224743fee36,"Numerous attempts for hypernymy relation (e.g., dog ""is-A""animal) detection have been made for resourceful languages like English, whereas efforts made for low-resource languages are scarce primarily due to lack of gold-standard datasets and suitable distributional models. Therefore, we introduce four gold-standard datasets for hypernymy detection for each of the two languages, namely, Hindi and Bengali, and two gold-standard datasets for Amharic. Another major contribution of this work is to prepare distributional thesaurus (DT) embeddings for all three languages using three different network embedding methods (DeepWalk, role2vec, and M-NMF) for the first time on these languages and to show their utility for hypernymy detection. Posing this problem as a binary classification task, we experiment with supervised classifiers like Support Vector Machine, Random Forest, and so on, and we show that these classifiers fed with DT embeddings can obtain promising results while evaluated against proposed gold-standard datasets, specifically in an experimental setup that counteracts lexical memorization. We further incorporate DT embeddings and pre-Trained fastText embeddings together using two different hybrid approaches, both of which produce an excellent performance. Additionally, we validate our methodology on gold-standard English datasets as well, where we reach a comparable performance to state-of-The-Art models for hypernymy detection.  © 2022 Association for Computing Machinery.",Distributional thesaurus; Hypernymy detection; Low resource datasets; Network embeddings,Classification (of information); Decision trees; Support vector machines; Thesauri; Bengalis; Distributional models; Distributional thesaurus; Embeddings; Gold standards; Hypernymy detection; Low resource dataset; Low resource languages; Network embedding; Performance; Embeddings
Q-Learning for Shift-Reduce Parsing in Indonesian Tree-LSTM-Based Text Generation,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130483451&doi=10.1145%2f3490501&partnerID=40&md5=9f610c8743117b486623ca5c4260b0c5,"Tree-LSTM algorithm accommodates tree structure processing to extract information outside the linear sequence pattern. The use of Tree-LSTM in text generation problems requires the help of an external parser at each generation iteration. Developing a good parser demands the representation of complex features and relies heavily on the grammar of the corpus. The limited corpus results in an insufficient number of vocabs for a grammar-based parser, making it less natural to link the text generation process. This research aims to solve the problem of limited corpus by proposing the use of a Reinforcement Learning algorithm in the formation of constituency trees, which link the sentence generation process given a seed phrase as the input in the Tree-LSTM model. The tree production process is modeled as a Markov's decision process, where a set of states consists of word embedding vectors, and a set of actions of {Shift, Reduce}. The Deep Q-Network model as an approximator of the Q-Learning algorithm is trained to obtain optimal weights in representing the Q-value function.The test results on perplexity-based evaluation show that the proposed Tree-LSTM and Q-Learning combination model achieves values 9.60 and 4.60 for two kinds of corpus with 205 and 1,000 sentences, respectively, better than the Shift-All model. Human evaluation of Friedman test and posthoc analysis showed that all five respondents tended to give the same assessment for the combination model of Tree-LSTM and Q-Learning, which on average outperforms two other nongrammar models, i.e., Shift-All and Reduce-All.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Shift reduce parsing; Tree-structured neural network,Computational linguistics; Forestry; Iterative methods; Learning algorithms; Long short-term memory; Syntactics; Trees (mathematics); Combination models; Extract informations; Generation process; Linear sequence; Q-learning; Sequence patterns; Shift reduce parsing; Text generations; Tree structures; Tree-structured neural networks; Reinforcement learning
Linguistically Driven Multi-Task Pre-Training for Low-Resource Neural Machine Translation,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130418987&doi=10.1145%2f3491065&partnerID=40&md5=4a88da43e9cb35f832e1653851e15a4c,"In the present study, we propose novel sequence-To-sequence pre-Training objectives for low-resource machine translation (NMT): Japanese-specific sequence to sequence (JASS) for language pairs involving Japanese as the source or target language, and English-specific sequence to sequence (ENSS) for language pairs involving English. JASS focuses on masking and reordering Japanese linguistic units known as bunsetsu, whereas ENSS is proposed based on phrase structure masking and reordering tasks. Experiments on ASPEC Japanese-English & Japanese-Chinese, Wikipedia Japanese-Chinese, News English-Korean corpora demonstrate that JASS and ENSS outperform MASS and other existing language-Agnostic pre-Training methods by up to +2.9 BLEU points for the Japanese-English tasks, up to +7.0 BLEU points for the Japanese-Chinese tasks and up to +1.3 BLEU points for English-Korean tasks. Empirical analysis, which focuses on the relationship between individual parts in JASS and ENSS, reveals the complementary nature of the subtasks of JASS and ENSS. Adequacy evaluation using LASER, human evaluation, and case studies reveals that our proposed methods significantly outperform pre-Training methods without injected linguistic knowledge and they have a larger positive impact on the adequacy as compared to the fluency.  © 2022 Association for Computing Machinery.",Linguistically-driven; Low-resource neural machine translation; Pre-Training,Computational linguistics; Computer aided language translation; Language pairs; Linguistically-driven; Low-resource neural machine translation; Machine translations; Multi tasks; Pre-training; Source language; Specific sequences; Target language; Training methods; Neural machine translation
Part-of-Speech (POS) Tagging Using Deep Learning-Based Approaches on the Designed Khasi POS Corpus,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128230151&doi=10.1145%2f3488381&partnerID=40&md5=78df3dd100946b5f8c04007150da8dde,"Part-of-speech (POS) tagging is one of the research challenging fields in natural language processing (NLP). It requires good knowledge of a particular language with large amounts of data or corpora for feature engineering, which can lead to achieving a good performance of the tagger. Our main contribution in this research work is the designed Khasi POS corpus. Till date, there has been no form of any kind of Khasi corpus developed or formally developed. In the present designed Khasi POS corpus, each word is tagged manually using the designed tagset. Methods of deep learning have been used to experiment with our designed Khasi POS corpus. The POS tagger based on BiLSTM, combinations of BiLSTM with CRF, and character-based embedding with BiLSTM are presented. The main challenges of understanding and handling Natural Language toward Computational linguistics to encounter are anticipated. In the presently designed corpus, we have tried to solve the problems of ambiguities of words concerning their context usage, and also the orthography problems that arise in the designed POS corpus. The designed Khasi corpus size is around 96,100 tokens and consists of 6,616 distinct words. Initially, while running the first few sets of data of around 41,000 tokens in our experiment the taggers are found to yield considerably accurate results. When the Khasi corpus size has been increased to 96,100 tokens, we see an increase in accuracy rate and the analyses are more pertinent. As results, accuracy of 96.81% is achieved for the BiLSTM method, 96.98% for BiLSTM with CRF technique, and 95.86% for character-based with LSTM. Concerning substantial research from the NLP perspectives for Khasi, we also present some of the recently existing POS taggers and other NLP works on the Khasi language for comparative purposes.  © 2021 Association for Computing Machinery.",ambiguity; BiLSTM; Deep learning; khasi corpus; khasi language; POS tagger; word embedding,Computational linguistics; Embeddings; Long short-term memory; Ambiguity; BiLSTM; Deep learning; Embeddings; Khasi corpus; Khasi language; Part-of-speech tagger; Part-of-speech tags; Speech corpora; Word embedding; Natural language processing systems
Toxic Comment Classification Based on Bidirectional Gated Recurrent Unit and Convolutional Neural Network,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128214184&doi=10.1145%2f3488366&partnerID=40&md5=f2b30f64a775d8c7e6c63b182acad013,"For English toxic comment classification, this paper presents the model that combines Bi-GRU and CNN optimized by global average pooling (BG-GCNN) based on the bidirectional gated recurrent unit (Bi-GRU) and global pooling optimized convolution neural network (CNN). The model treats each type of toxic comment as a binary classification. First, Bi-GRU is used to extract the time-series features of the comment and then the dimensionality is reduced through global pooling optimized convolution neural network. Finally, the classification result is output by Sigmoid function. Comparative experiments show the BG-GCNN model has a better classification effect than Text-CNN, LSTM, Bi-GRU, and other models. The Macro-F1 value of the toxic comment dataset on the Kaggle competition platform is 0.62. The F1 values of the three toxic label classification results (toxic, obscene, and insult label) are 0.81, 0.84, and 0.74, respectively, which are the highest values in the comparative experiment.  © 2021 Association for Computing Machinery.",bidirectional gated recurrent unit; convolution neural network; global pooling; Toxic comments classification,Convolution; Convolutional neural networks; Long short-term memory; Text processing; Bidirectional gated recurrent unit; Binary classification; Classification results; Comparative experiments; Convolution neural network; Convolutional neural network; F1 values; Global pooling; Time series features; Toxic comment classification; Classification (of information)
Legal Judgment Elements Extraction Approach with Law Article-Aware Mechanism,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128244799&doi=10.1145%2f3485244&partnerID=40&md5=4fd5d9ebd93f6bcb54ec6485fbb321b8,"Legal judgment elements extraction (LJEE) aims to identify the different judgment features from the fact description in legal documents automatically, which helps to improve the accuracy and interpretability of the judgment results. In real court rulings, judges usually need to scan both the fact descriptions and the law articles repeatedly to find out the relevant information, and it is hard to acquire the key judgment features quickly, so legal judgment elements extraction is a crucial and challenging task for legal judgment prediction. However, most existing methods follow the text classification framework, which fails to model the attentive relations of the law articles and the legal judgment elements. To address this issue, we simulate the working process of human judges, and propose a legal judgment elements extraction method with a law article-Aware mechanism, which captures the complex semantic correlations of the law article and the legal judgment elements. Experimental results show that our proposed method achieves significant improvements than other state-of-The-Art baselines on the element recognition task dataset. Compared with the BERT-CNN model, the proposed ""All labels Law Articles Embedding Model (ALEM)""improves the accuracy, recall, and F1 value by 0.5, 1.4 and 1.0, respectively.  © 2021 Association for Computing Machinery.",fact description; intelligent justice; law article-Aware mechanism; Legal judgment elements extraction; wisdom justice,Classification (of information); Extraction; Semantics; Court rulings; Element extraction; Fact descriptions; Intelligent justice; Interpretability; Law article-aware mechanism; Legal documents; Legal judgements; Legal judgment element extraction; Wisdom justice; Text processing
Adversarial Cross-domain Community Question Retrieval,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128237636&doi=10.1145%2f3487291&partnerID=40&md5=10f4b1f9c6acacc9d37dfba1889f64a7,"Community Q&A forum is a special type of social media that provides a platform to raise questions and to answer them (both by forum participants), to facilitate online information sharing. Currently, community Q&A forums in professional domains have attracted a large number of users by offering professional knowledge. To support information access and save users' efforts of raising new questions, they usually come with a question retrieval function, which retrieves similar existing questions (and their answers) to a user's query. However, it can be difficult for community Q&A forums to cover all domains, especially those emerging lately with little labeled data but great discrepancy from existing domains. We refer to this scenario as cross-domain question retrieval. To handle the unique challenges of cross-domain question retrieval, we design a model based on adversarial training, namely, X-QR, which consists of two modules-a domain discriminator and a sentence matcher. The domain discriminator aims at aligning the source and target data distributions and unifying the feature space by domain-Adversarial training. With the assistance of the domain discriminator, the sentence matcher is able to learn domain-consistent knowledge for the final matching prediction. To the best of our knowledge, this work is among the first to investigate the domain adaption problem of sentence matching for community Q&A forums question retrieval. The experiment results suggest that the proposed X-QR model offers better performance than conventional sentence matching methods in accomplishing cross-domain community Q&A tasks.  © 2022 Association for Computing Machinery.",adversarial training; Community Q&A; community question retrieval; domain adaption,Adversarial training; Community Q&A; Community question retrieval; Cross-domain; Domain adaptions; Information sharing; Matchings; Online information; Professional knowledge; Social media; Information retrieval
Enhancing Lexical Translation Consistency for Document-Level Neural Machine Translation,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128237119&doi=10.1145%2f3485469&partnerID=40&md5=8cfd48bfa50a246507d17664cd05ba56,"Document-level neural machine translation (DocNMT) has yielded attractive improvements. In this article, we systematically analyze the discourse phenomena in Chinese-To-English translation, and focus on the most obvious ones, namely lexical translation consistency. To alleviate the lexical inconsistency, we propose an effective approach that is aware of the words which need to be translated consistently and constrains the model to produce more consistent translations. Specifically, we first introduce a global context extractor to extract the document context and consistency context, respectively. Then, the two types of global context are integrated into a encoder enhancer and a decoder enhancer to improve the lexical translation consistency. We create a test set to evaluate the lexical consistency automatically. Experiments demonstrate that our approach can significantly alleviate the lexical translation inconsistency. In addition, our approach can also substantially improve the translation quality compared to sentence-level Transformer.  © 2021 Association for Computing Machinery.",discourse phenomena; Document-level translation; lexical consistency; neural machine translation,Computational linguistics; Computer aided language translation; Discourse phenomenon; Document context; Document-level translation; Effective approaches; Global context; Lexical consistency; Sentence level; Test sets; Translation quality; Neural machine translation
Reduplication in Assamese: Identification and Modeling,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143964528&doi=10.1145%2f3510419&partnerID=40&md5=d10b66a0f1580db7c8bea9c377e19fa6,"Reduplication is a productive morphological process widely used in a substantial number of languages in the world. Reduplication is a well-studied phenomenon, and several typological works have provided evidence for different types of reduplication in most of the languages around the world. Addressing reduplication plays a vital role in the efficiency of POS tagger, sentiment analysis, as well as other NLP tasks. However, it is an understudied area in computational linguistics, especially in low-resource languages like Assamese. This article first describes different types of reduplication and their shapes that occur in Assamese. Second, an exhaustive set of reduplication formation rules is compiled that is incorporated to build a system to identify reduplication in Assamese text. The results of the experiments performed on three different domain datasets showed that the rule-based system can identify reduplicated expressions with an average precision, recall, and F1 scores of 94.19%, 98.07%, and 96.07%, respectively. Third, it is shown that the Assamese reduplication processes can be captured through a two-way finite-state transducer (2-way FST). Finally, two broad categories of reduplicative processes along with their corresponding 2-way FST model are presented.  © 2022 Association for Computing Machinery.",2 way FST; Assamese reduplication; POS tagger; Reduplication; reduplication identification; reduplication modeling,Sentiment analysis; 2 way FST; Assamese reduplication; Different domains; Low resource languages; Morphological process; POS tag; Reduplication; Reduplication identification; Reduplication modeling; Sentiment analysis; Computational linguistics
Authorship Attribution for a Resource Poor Language-Urdu,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128243852&doi=10.1145%2f3487061&partnerID=40&md5=2f46873956005355ad40b0bca0d55764,"Authorship attribution refers to examining the writing style of authors to determine the likelihood of the original author of a document from a given set of potential authors. Due to the wide range of authorship attribution applications, a plethora of studies have been conducted for various Western, as well as Asian, languages. However, authorship attribution research in the Urdu language has just begun, although Urdu is widely acknowledged as a prominent South Asian language. Furthermore, the existing studies on authorship attribution in Urdu have addressed a considerably easier problem of having less than 20 candidate authors, which is far from the real-world settings. Therefore, the findings from these studies may not be applicable to the real-world settings. To that end, we have made three key contributions: First, we have developed a large authorship attribution corpus for Urdu, which is a low-resource language. The corpus is composed of over 2.6 million tokens and 21,938 news articles by 94 authors, which makes it a closer substitute to the real-world settings. Second, we have analyzed hundreds of stylometry features used in the literature to identify 194 features that are applicable to the Urdu language and developed a taxonomy of these features. Finally, we have performed 66 experiments using two heterogeneous datasets to evaluate the effectiveness of four traditional and three deep learning techniques. The experimental results show the following: (a) Our developed corpus is many folds larger than the existing corpora, and it is more challenging than its counterparts for the authorship attribution task, and (b) Convolutional Neutral Networks is the most effective technique, as it achieved a nearly perfect F1 score of 0.989 for an existing corpus and 0.910 for our newly developed corpus.  © 2021 Association for Computing Machinery.",authorship attribution; corpus generation; Low-resource languages; machine learning techniques; south asian languages; urdu,Deep learning; Asian languages; Authorship attribution; Corpus generation; Low resource languages; Machine learning techniques; News articles; Real world setting; South Asian languages; Urdu; Writing style; Learning algorithms
Joined Type Length Encoding for Nested Named Entity Recognition,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128241881&doi=10.1145%2f3487057&partnerID=40&md5=b1837308d353d426524f28e70c476a02,"In this article, we propose a new encoding scheme for named entity recognition (NER) called Joined Type-Length encoding (JoinedTL). Unlike most existing named entity encoding schemes, which focus on flat entities, JoinedTL can label nested named entities in a single sequence. JoinedTL uses a packed encoding to represent both type and span of a named entity, which not only results in less tagged tokens compared to existing encoding schemes, but also enables it to support nested NER. We evaluate the effectiveness of JoinedTL for nested NER on three nested NER datasets: GENIA in English, GermEval in German, and PerNest, our newly created nested NER dataset in Persian. We apply CharLSTM+WordLSTM+CRF, a three-layer sequence tagging model on three datasets encoded using JoinedTL and two existing nested NE encoding schemes, i.e., JoinedBIO and JoinedBILOU. Our experiment results show that CharLSTM+WordLSTM+CRF trained with JoinedTL encoded datasets can achieve competitive F1 scores as the ones trained with datasets encoded by two other encodings, but with 27%-48% less tagged tokens. To leverage the power of three different encodings, i.e., JoinedTL, JoinedBIO, and JoinedBILOU, we propose an encoding-based ensemble method for nested NER. Evaluation results show that the ensemble method achieves higher F1 scores on all datasets than the three models each trained using one of the three encodings. By using nested NE encodings including JoinedTL with CharLSTM+WordLSTM+CRF, we establish new state-of-The-Art performance with an F1 score of 83.7 on PerNest, 74.9 on GENIA, and 70.5 on GermEval, surpassing two recent neural models specially designed for nested NER.  © 2021 Association for Computing Machinery.",LSTM-CRF model; NER encoding scheme; Nested named entity recognition; Persian NER corpus,Long short-term memory; Natural language processing systems; Signal encoding; Encoding schemes; Length encoding; LSTM-CRF model; Named entity recognition; Named entity recognition encoding scheme; Nested named entity recognition; Persian named entity recognition corpus; Persians; Encoding (symbols)
Combining Self-supervised Learning and Active Learning for Disfluency Detection,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128223093&doi=10.1145%2f3487290&partnerID=40&md5=6de10b03219d5375ef097220c5f47786,"Spoken language is fundamentally different from the written language in that it contains frequent disfluencies or parts of an utterance that are corrected by the speaker. Disfluency detection (removing these disfluencies) is desirable to clean the input for use in downstream NLP tasks. Most existing approaches to disfluency detection heavily rely on human-Annotated data, which is scarce and expensive to obtain in practice. To tackle the training data bottleneck, in this work, we investigate methods for combining self-supervised learning and active learning for disfluency detection. First, we construct large-scale pseudo training data by randomly adding or deleting words from unlabeled data and propose two self-supervised pre-Training tasks: (i) a tagging task to detect the added noisy words and (ii) sentence classification to distinguish original sentences from grammatically incorrect sentences. We then combine these two tasks to jointly pre-Train a neural network. The pre-Trained neural network is then fine-Tuned using human-Annotated disfluency detection training data. The self-supervised learning method can capture task-special knowledge for disfluency detection and achieve better performance when fine-Tuning on a small annotated dataset compared to other supervised methods. However, limited in that the pseudo training data are generated based on simple heuristics and cannot fully cover all the disfluency patterns, there is still a performance gap compared to the supervised models trained on the full training dataset. We further explore how to bridge the performance gap by integrating active learning during the fine-Tuning process. Active learning strives to reduce annotation costs by choosing the most critical examples to label and can address the weakness of self-supervised learning with a small annotated dataset. We show that by combining self-supervised learning with active learning, our model is able to match state-of-The-Art performance with just about 10% of the original training data on both the commonly used English Switchboard test set and a set of in-house annotated Chinese data.  © 2021 Association for Computing Machinery.",active learning; Disfluency detection; pre-Training technology; self-supervised learning,Classification (of information); Active Learning; Annotated datasets; Disfluencies; Disfluency detections; Fine tuning; Performance gaps; Pre-training; Pre-training technology; Self-supervised learning; Training data; Supervised learning
Arabic Handwritten Word Recognition Based on Stationary Wavelet Transform Technique using Machine Learning,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128227634&doi=10.1145%2f3474391&partnerID=40&md5=fe739c6b6b5cafe2edf5c57f84636da6,"This paper is aimed at improving the performance of the word recognition system (WRS) of handwritten Arabic text by extracting features in the frequency domain using the Stationary Wavelet Transform (SWT) method using machine learning, which is a wavelet transform approach created to compensate for the absence of translation invariance in the Discrete Wavelets Transform (DWT) method. The proposed SWT-WRS of Arabic handwritten text consists of three main processes: word normalization, feature extraction based on SWT, and recognition. The proposed SWT-WRS based on the SWT method is evaluated on the IFN/ENIT database applying the Gaussian, linear, and polynomial support vector machine, the k-nearest neighbors, and ANN classifiers. ANN performance was assessed by applying the Bayesian Regularization (BR) and Levenberg-Marquardt (LM) training methods. Numerous wavelet transform (WT) families are applied, and the results prove that level 19 of the Daubechies family is the best WT family for the proposed SWT-WRS. The results also confirm the effectiveness of the proposed SWT-WRS in improving the performance of handwritten Arabic word recognition using machine learning. Therefore, the suggested SWT-WRS overcomes the lack of translation invariance in the DWT method by eliminating the up-And-down samplers from the proposed machine learning method.  © 2021 Association for Computing Machinery.",artificial neural network; handwritten arabic word; holistic recognition; k-nearest neighbors; Machine learning; stationary wavelet transform; support vector machine,Character recognition; Classification (of information); Discrete wavelet transforms; Frequency domain analysis; Motion compensation; Nearest neighbor search; Neural networks; Signal reconstruction; Support vector machines; Text processing; Discrete-wavelet-transform; Handwritten arabic word; Holistic recognition; Performance; Recognition systems; Stationary wavelet transforms; Support vectors machine; Transform methods; Translation invariance; Word recognition; k-nearest neighbors
Memorizing All for Implicit Discourse Relation Recognition,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128210233&doi=10.1145%2f3485016&partnerID=40&md5=55bb5d05291fca870c1073a634d5bb98,"Implicit discourse relation recognition is a challenging task due to the absence of the necessary informative clues from explicit connectives. An implicit discourse relation recognizer has to carefully tackle the semantic similarity of sentence pairs and the severe data sparsity issue. In this article, we learn token embeddings to encode the structure of a sentence from a dependency point of view in their representations and use them to initialize a baseline model to make it really strong. Then, we propose a novel memory component to tackle the data sparsity issue by allowing the model to master the entire training set, which helps in achieving further performance improvement. The memory mechanism adequately memorizes information by pairing representations and discourse relations of all training instances, thus filling the slot of the data-hungry issue in the current implicit discourse relation recognizer. The proposed memory component, if attached with any suitable baseline, can help in performance enhancement. The experiments show that our full model with memorizing the entire training data provides excellent results on PDTB and CDTB datasets, outperforming the baselines by a fair margin.  © 2021 Association for Computing Machinery.",CDTB; contextual information; Discourse relations; IDRC; memory network; natural language processing; PDTB; semantic similarity; syntax; unsupervised,Semantic Web; Semantics; CDTB; Contextual information; Data sparsity; Discourse relation; IDRC; Memory component; Memory network; PDTB; Semantic similarity; Unsupervised; Natural language processing systems
Dual-View Conditional Variational Auto-Encoder for Emotional Dialogue Generation,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128230185&doi=10.1145%2f3481890&partnerID=40&md5=b0f1bc9cd3403fc23097d7e4ca1e2852,"Emotional dialogue generation aims to generate appropriate responses that are content relevant with the query and emotion consistent with the given emotion tag. Previous work mainly focuses on incorporating emotion information into the sequence to sequence or conditional variational auto-encoder (CVAE) models, and they usually utilize the given emotion tag as a conditional feature to influence the response generation process. However, emotion tag as a feature cannot well guarantee the emotion consistency between the response and the given emotion tag. In this article, we propose a novel Dual-View CVAE model to explicitly model the content relevance and emotion consistency jointly. These two views gather the emotional information and the content-relevant information from the latent distribution of responses, respectively. We jointly model the dual-view via VAE to get richer and complementary information. Extensive experiments on both English and Chinese emotion dialogue datasets demonstrate the effectiveness of our proposed Dual-View CVAE model, which significantly outperforms the strong baseline models in both aspects of content relevance and emotion consistency.  © 2021 Association for Computing Machinery.",dialogue; neural networks; Sentiment,Knowledge management; Auto encoders; Chinese emotions; Dialog; Dialogue generations; Emotional information; Generation process; Neural-networks; Response generation; Sentiment; Two views; Network coding
Improving Deep Learning based Automatic Speech Recognition for Gujarati,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128236049&doi=10.1145%2f3483446&partnerID=40&md5=0272bb3e85a081caba6b7dd7f39dc46c,"We present a novel approach for improving the performance of an End-To-End speech recognition system for the Gujarati language. We follow a deep learning-based approach that includes Convolutional Neural Network, Bi-directional Long Short Term Memory layers, Dense layers, and Connectionist Temporal Classification as a loss function. To improve the performance of the system with the limited size of the dataset, we present a combined language model (Word-level language Model and Character-level language model)-based prefix decoding technique and Bidirectional Encoder Representations from Transformers-based post-processing technique. To gain key insights from our Automatic Speech Recognition (ASR) system, we used the inferences from the system and proposed different analysis methods. These insights help us in understanding and improving the ASR system as well as provide intuition into the language used for the ASR system. We have trained the model on the Microsoft Speech Corpus, and we observe a 5.87% decrease in Word Error Rate (WER) with respect to base-model WER.  © 2021 Association for Computing Machinery.",automatic speech recognition; BERT; Deep learning; prefix decoding; recurrent neural network,Computational linguistics; Convolutional neural networks; Multilayer neural networks; Recurrent neural networks; Speech; Speech recognition; Automatic speech recognition; Automatic speech recognition system; BERT; Deep learning; End to end; Language model; Performance; Prefix decoding; Speech recognition systems; Word error rate; Decoding
Improving Neural Machine Translation by Transferring Knowledge from Syntactic Constituent Alignment Learning,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151831640&doi=10.1145%2f3510580&partnerID=40&md5=e01b924b3e8a1e6669eb655afdb2af5f,"Statistical machine translation (SMT) models rely on word-, phrase-, and syntax-level alignments. But neural machine translation (NMT) models rarely explicitly learn the phrase- and syntax-level alignments. In this article, we propose to improve NMT by explicitly learning the bilingual syntactic constituent alignments. Specifically, we first utilize syntactic parsers to induce syntactic structures of sentences, and then we propose two ways to utilize the syntactic constituents in a perceptual (not adversarial) generator-discriminator training framework. One way is to use them to measure the alignment score of sentence-level training examples, and the other is to directly score the alignments of constituent-level examples generated with an algorithm based on word-level alignments from SMT. In our generator-discriminator framework, the discriminator is pre-trained to learn constituent alignments and distinguish the ground-truth translation from the fake ones, while the generative translation model is fine-tuned to receive the alignment knowledge and to generate translations that best approximate the true ones. Experiments and analysis show that the learned constituent alignments can help improve the translation results.  © 2022 Association for Computing Machinery.",constituent alignment; discriminator-generator framework; Neural machine translation; syntactic constituent,Alignment; Computational linguistics; Computer aided language translation; Discriminators; Learning systems; Neural machine translation; Speech transmission; Bilinguals; Constituent alignment; Discriminator-generator framework; Learn+; Machine translation models; Statistical machine translation; Syntactic constituent; Syntactic parsers; Syntactic structure; Two ways; Syntactics
Aspect-based Sentiment Analysis using Dependency Parsing,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128102351&doi=10.1145%2f3485243&partnerID=40&md5=70e32527e9164365683ffd1d8e003f39,"In this paper, an aspect-based Sentiment Analysis (SA) system for Hindi is presented. The proposed system assigns a separate sentiment towards the different aspects of a sentence as well as it evaluates the overall sentiment expressed in a sentence. In this work, Hindi Dependency Parser (HDP) is used to determine the association between an aspect word and a sentiment word (using Hindi SentiWordNet) and works on the idea that closely connected words come together to express a sentiment about a certain aspect. By generating a dependency graph, the system assigns the sentiment to an aspect having a minimum distance between them and computes the overall polarity of the sentence. The system achieves an accuracy of 83.2% on a corpus of movie reviews and its results are compared with baselines as well as existing works on SA. From the results, it has been observed that the proposed system has the potential to be used in emerging applications like SA of product reviews, social media analysis, etc.  © 2021 Association for Computing Machinery.",Aspect-based; Hindi dependency parser; Hindi SentiWordNet; movie reviews; sentiment analysis,Syntactics; Analysis system; Aspect-based; Dependency graphs; Dependency parser; Dependency parsing; Hindi dependency parse; Hindi sentiwordnet; Movie reviews; Sentiment analysis; SentiWordNet; Sentiment analysis
Acoustic Analysis of Vowels in Konkani,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151892878&doi=10.1145%2f3474358&partnerID=40&md5=d260dee6825524f8b097c952c8e1df68,"Konkani is an under-resourced language mainly spoken on the west coast of India. Although linguistic analyses of vowel sounds in various dialects of Konkani have been done in the past, more accurate analysis of Konkani vowels, especially an acoustic-phonetic analysis, was never carried out. In this article, we present a detailed analysis of nine Konkani vowels, namely /i/, /e/, /ϵ/, /u/, /o/, /I""/, /a/, /I™/, and /I/. The dataset used for the analysis was created from audio recordings of 28 native speakers of Goan Konkani. Based on the experimental results, we propose a vowel chart for Konkani. We also observed a partial loss of Konkani vowel / in the regular speech of native speakers. This change is also evident in the substitution analysis of vowel phonemes that was carried out by us as a part of this study.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Acoustic analysis; formant analysis; Konkani; vowel phoneme,Linguistics; Accurate analysis; Acoustic analysis; Acoustic-phonetic analysis; Formant analyse; Konkani; Linguistic analysis; Under-resourced languages; Vowel phoneme; Vowel sounds; West coast; Audio recordings
One-Shot Relation Learning for Knowledge Graphs via Neighborhood Aggregation and Paths Encoding,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128194538&doi=10.1145%2f3484729&partnerID=40&md5=b5b302fe4f10986a3490c91bfdae0edb,"The relation learning between two entities is an essential task in knowledge graph (KG) completion that has received much attention recently. Previous work almost exclusively focused on relations widely seen in the original KGs, which means that enough training data are available for modeling. However, long-Tail relations that only show in a few triples are actually much more common in practical KGs. Without sufficiently large training data, the performance of existing models on predicting long-Tail relations drops impressively. This work aims to predict the relation under a challenging setting where only one instance is available for training. We propose a path-based one-shot relation prediction framework, which can extract neighborhood information of an entity based on the relation query attention mechanism to learn transferable knowledge among the same relation. Simultaneously, to reduce the impact of long-Tail entities on relation prediction, we selectively fuse path information between entity pairs as auxiliary information of relation features. Experiments in three one-shot relation learning datasets show that our proposed framework substantially outperforms existing models on one-shot link prediction and relation prediction.  © 2021 Association for Computing Machinery.",knowledge graph completion; One-shot learning; relation learning,Forecasting; Signal encoding; Knowledge graph completion; Knowledge graphs; Long tail; Neighbourhood; One-shot learning; Path encoding; Path-based; Performance; Relation learning; Training data; Knowledge graph
Breaking the Curse of Class Imbalance: Bangla Text Classification,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151843874&doi=10.1145%2f3511601&partnerID=40&md5=e06d59b82edfec02c820c124b1c826e7,"This article addresses the class imbalance issue in a low-resource language called Bengali. As a use-case, we choose one of the most fundamental NLP tasks, i.e., text classification, where we utilize three benchmark text corpora: fake-news dataset, sentiment analysis dataset, and song lyrics dataset. Each of them contains a critical class imbalance. We attempt to tackle the problem by applying several strategies that include data augmentation with synthetic samples via text and embedding generation in order to augment the proportion of the minority samples. Moreover, we apply ensembling of deep learning models by subsetting the majority samples. Additionally, we enforce the focal loss function for class-imbalanced data classification. We also apply the outlier detection technique, data resampling, and hidden feature extraction to improve the minority-f1 score. All of our experimentations are entirely focused on textual content analysis, which results in a more than 90% minority f1 score for each of the three tasks. It is an excellent outcome on such highly class-imbalanced datasets.  © 2022 Association for Computing Machinery.",Class imbalance; data augmentation; ensembling; fake news; hidden feature extraction; neural networks; resampling; sentiment analysis; song lyrics; text classification,Classification (of information); Deep learning; Extraction; Fake detection; Feature extraction; Class imbalance; Data augmentation; Ensembling; Fake news; Features extraction; Hidden feature extraction; Neural-networks; Resampling; Sentiment analysis; Song lyric; Text classification; Sentiment analysis
Fusion Based AER System Using Deep Learning Approach for Amplitude and Frequency Analysis,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128203733&doi=10.1145%2f3488369&partnerID=40&md5=0bd37c89a920bab20df3f71bd4ce911a,"Automatic emotion recognition from Speech (AERS) systems based on acoustical analysis reveal that some emotional classes persist with ambiguity. This study employed an alternative method aimed at providing deep understanding into the amplitude-frequency, impacts of various emotions in order to aid in the advancement of near term, more effectively in classifying AER approaches. The study was undertaken by converting narrow 20 ms frames of speech into RGB or grey-scale spectrogram images. The features have been used to fine-Tune a feature selection system that had previously been trained to recognise emotions. Two different Linear and Mel spectral scales are used to demonstrate a spectrogram. An inductive approach for in sighting the amplitude and frequency features of various emotional classes. We propose a two-channel profound combination of deep fusion network model for the efficient categorization of images. Linear and Mel-spectrogram is acquired from Speech-signal, which is prepared in the recurrence area to input Deep Neural Network. The proposed model Alex-Net with five convolutional layers and two fully connected layers acquire most vital features form spectrogram images plotted on the amplitude-frequency scale. The state-of-The-Art is compared with benchmark dataset (EMO-DB). RGB and saliency images are fed to pre-Trained Alex-Net tested both EMO-DB and Telugu dataset with an accuracy of 72.18% and fused image features less computations reaching to an accuracy 75.12%. The proposed model show that Transfer learning predict efficiently than Fine-Tune network. When tested on Emo-DB dataset, the propE sed system adequately learns discriminant features from speech spectrE grams and outperforms many stEte-of-The-Art techniques.  © 2021 Association for Computing Machinery.",AlexNet; Deep-CNN; Emotion recognition; Fusion; spectrogram's,Convolutional neural networks; Deep neural networks; Spectrographs; Alexnet; Amplitude analysis; Amplitude-frequency; Automatic emotion recognition; Deep-CNN; Emotion recognition; Emotion recognition from speech; Frequency Analysis; Learning approach; Spectrograms; Speech recognition
Towards Developing Uniform Lexicon Based Sorting Algorithm for Three Prominent Indo-Aryan Languages,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128178977&doi=10.1145%2f3488371&partnerID=40&md5=109984adfb5a9dc7d3234d9e97a1d144,"Three different Indic/Indo-Aryan languages-Bengali, Hindi and Nepali have been explored here in character level to find out similarities and dissimilarities. Having shared the same root, the Sanskrit, Indic languages bear common characteristics. That is why computer and language scientists can take the opportunity to develop common Natural Language Processing (NLP) techniques or algorithms. Bearing the concept in mind, we compare and analyze these three languages character by character. As an application of the hypothesis, we also developed a uniform sorting algorithm in two steps, first for the Bengali and Nepali languages only and then extended it for Hindi in the second step. Our thorough investigation with more than 30,000 words from each language suggests that, the algorithm maintains total accuracy as set by the local language authorities of the respective languages and good efficiency.  © 2021 Association for Computing Machinery.",algorithm; application; Bengali; collation; Hindi; Indic; Indo-Aryan; language family tree; Lexicon; natural language processing; Nepali; South-Asian; Unicode,Trees (mathematics); Bengalis; Collation; Family tree; Hindi; Indic; Indo-aryan; Language family tree; Lexicon; Nepali; South-asian; Unicodes; Natural language processing systems
Critical Analysis of Existing Punjabi Grammar Checker and a Proposed Hybrid Framework Involving Machine Learning and Rule-Base Criteria,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144921659&doi=10.1145%2f3514237&partnerID=40&md5=5fd4b655c6c82b56666c9c7433d08132,"An important area of research involving Artificial Intelligence (AI) is Natural Language Processing (NLP). The objective of training a machine is to imitate and manipulate text and speech of humans. Progressive research is undertaken to find connections between humans and their usage of language commonly used being referred as Natural Language. Various tools for different languages have been developed for operating the natural languages widely used by public. NLP integrates various disciplines and works cohesively for processing text, Information Retrieval, AI and so on. One such tool used for checking the accuracy of a given sentence in any language is referred to as a Grammar Checker. So a Grammar checker of a particular language explores grammatical errors (if any) and provides remedial suggestions for correction of the same. Such feature is imbibed by virtue of Natural Language Processing using Computational Linguistics. We have justified the need of an emerging Machine Learning technique by critically evaluating the existing Punjabi Grammar checker that was developed earlier in light of certain real-time cases. This process is accomplished by critically evaluating the output of each phase and identifying the component accountable for generating maximum errors and false alarms. Based on this analysis, we have proposed a hybrid framework as an efficient way of analyzing correction in sentences. This is attainable through the said booming technique of Machine Learning explicitly using Deep Neural Networks in combination with the existing rule-based approach. It's a novel approach as no work using machine learning has been done earlier in Punjabi Grammar Checker.  © 2022 Association for Computing Machinery.",Deep Learning; Deep Neural Networks; linguistics; Machine Learning; Natural Language Processing; Punjabi Grammar checking,Computational linguistics; Learning algorithms; Learning systems; Natural language processing systems; Critical analysis; Deep learning; Grammar checkers; Hybrid framework; Language processing; Machine-learning; Natural language processing; Natural languages; Punjabi grammar checking; Rule basis; Deep neural networks
Tri-Training for Dependency Parsing Domain Adaptation,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128201283&doi=10.1145%2f3488367&partnerID=40&md5=b3be62a9b065d00470937d669256fda9,"In recent years, the research on dependency parsing focuses on improving the accuracy of the domain-specific (in-domain) test datasets and has made remarkable progress. However, there are innumerable scenarios in the real world that are not covered by the dataset, namely, the out-of-domain dataset. As a result, parsers that perform well on the in-domain data usually suffer from significant performance degradation on the out-of-domain data. Therefore, to adapt the existing in-domain parsers with high performance to a new domain scenario, cross-domain transfer learning methods are essential to solve the domain problem in parsing. This paper examines two scenarios for cross-domain transfer learning: semi-supervised and unsupervised cross-domain transfer learning. Specifically, we adopt a pre-Trained language model BERT for training on the source domain (in-domain) data at the subword level and introduce self-Training methods varied from tri-Training for these two scenarios. The evaluation results on the NLPCC-2019 shared task and universal dependency parsing task indicate the effectiveness of the adopted approaches on cross-domain transfer learning and show the potential of self-learning to cross-lingual transfer learning.  © 2021 Association for Computing Machinery.",dependency parsing; domain adaptation; transfer learning; Tri-Training,Computational linguistics; Cross-domain; Dependency parsing; Domain adaptation; Domain specific; Domain transfers; Performance; Performance degradation; Real-world; Transfer learning; Tri-training; Learning systems
An Intelligent Unsupervised Approach for Handling Context-Dependent Words in Urdu Sentiment Analysis,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139812742&doi=10.1145%2f3510830&partnerID=40&md5=afa470a09615db40c08f258e037dff59,"The characteristic of context dependency in Urdu words needs to be handled carefully while performing Urdu sentiment analysis. In this research, an already constructed Urdu sentiment lexicon of positive and negative words is further expanded by the addition of context-dependent words. These context-dependent words are used with or without conjunctions. Rules are formulated for assigning polarities to those context-dependent words that are surrounded by the positive or negative words. These rules were incorporated in the Urdu sentiment analyzer. Fusion of these rules for handling context-dependent words and the expanded Urdu sentiment lexicon resulted in increasing the accuracy of the Urdu sentiment analyzer from 83.43% to 89.03% with 0.8655 precision, 0.9053 recall, and 0.8799 F-measure, which is a statistically significant improvement.  © 2022 Association for Computing Machinery.",accuracy; Context-dependent words; rules; Urdu sentiment analyzer; Urdu sentiment lexicon,Accuracy; Context dependency; Context dependent; Context-dependent word; Rule; Sentiment analysis; Sentiment lexicons; Unsupervised approaches; Urdu sentiment analyzer; Urdu sentiment lexicon; Sentiment analysis
Light Diacritic Restoration to Disambiguate Homographs in Modern Arabic Texts,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128201748&doi=10.1145%2f3486675&partnerID=40&md5=f0f956c7eab4fef601fcd60aa140407b,"Diacritic restoration (also known as diacritization or vowelization) is the process of inserting the correct diacritical markings into a text. Modern Arabic is typically written without diacritics, e.g., newspapers. This lack of diacritical markings often causes ambiguity, and though natives are adept at resolving, there are times they may fail. Diacritic restoration is a classical problem in computer science. Still, as most of the works tackle the full (heavy) diacritization of text, we, however, are interested in diacritizing the text using a fewer number of diacritics. Studies have shown that a fully diacritized text is visually displeasing and slows down the reading. This article proposes a system to diacritize homographs using the least number of diacritics, thus the name ""light.""There is a large class of words that fall under the homograph category, and we will be dealing with the class of words that share the spelling but not the meaning. With fewer diacritics, we do not expect any effect on reading speed, while eye strain is reduced. The system contains morphological analyzer and context similarities. The morphological analyzer is used to generate all word candidates for diacritics. Then, through a statistical approach and context similarities, we resolve the homographs. Experimentally, the system shows very promising results, and our best accuracy is 85.6%.  © 2021 Association for Computing Machinery.",Arabic language; automatic diacritization; disambiguation; homographs; morphological analysis,Arabic languages; Arabic texts; Automatic diacritization; Classical problems; Diacritics restorations; Disambiguation; Homograph; Morphological analysis; Morphological analyzer; Reading speed; Restoration
Efficient Channel Attention Based Encoder-Decoder Approach for Image Captioning in Hindi,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128202831&doi=10.1145%2f3483597&partnerID=40&md5=d9d4c5538735004904d51791d128703a,"Image captioning refers to the process of generating a textual description that describes objects and activities present in a given image. It connects two fields of artificial intelligence, computer vision, and natural language processing. Computer vision and natural language processing deal with image understanding and language modeling, respectively. In the existing literature, most of the works have been carried out for image captioning in the English language. This article presents a novel method for image captioning in the Hindi language using encoder-decoder based deep learning architecture with efficient channel attention. The key contribution of this work is the deployment of an efficient channel attention mechanism with bahdanau attention and a gated recurrent unit for developing an image captioning model in the Hindi language. Color images usually consist of three channels, namely red, green, and blue. The channel attention mechanism focuses on an image's important channel while performing the convolution, which is basically to assign higher importance to specific channels over others. The channel attention mechanism has been shown to have great potential for improving the efficiency of deep convolution neural networks (CNNs). The proposed encoder-decoder architecture utilizes the recently introduced ECA-NET CNN to integrate the channel attention mechanism. Hindi is the fourth most spoken language globally, widely spoken in India and South Asia; it is India's official language. By translating the well-known MSCOCO dataset from English to Hindi, a dataset for image captioning in Hindi is manually created. The efficiency of the proposed method is compared with other baselines in terms of Bilingual Evaluation Understudy (BLEU) scores, and the results obtained illustrate that the method proposed outperforms other baselines. The proposed method has attained improvements of 0.59%, 2.51%, 4.38%, and 3.30% in terms of BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores, respectively, with respect to the state-of-The-Art. Qualities of the generated captions are further assessed manually in terms of adequacy and fluency to illustrate the proposed method's efficacy.  © 2021 Association for Computing Machinery.",attention; channel attention; deep-learning; Hindi; Image captioning,Channel coding; Computer vision; Decoding; Deep neural networks; Efficiency; Modeling languages; Natural language processing systems; Network architecture; Signal encoding; Attention; Attention mechanisms; Bilinguals; Channel attention; Convolution neural network; Deep-learning; Efficient channels; Encoder-decoder; Hindi; Image captioning; Convolution
Impact of Feature Extraction and Feature Selection Algorithms on Punjabi Speech Emotion Recognition Using Convolutional Neural Network,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151057854&doi=10.1145%2f3511888&partnerID=40&md5=13b39413546caf82d163d27bbd1fc53d,"As a challenge to refine the spontaneity and productivity of a machine and human coherence, speech emotion recognition has been an overriding area of research. The trustability and fulfillment of emotion recognition are largely involved with the feature extraction and selection processes. An important role is played in exploring and distinguishing audio content during the feature extraction phase. Also, the features that have been extracted should be resilient to a number of disturbances and reliable enough for an adequate classification system. This article focuses on three main components of a Speech Emotion Recognition (SER) process. The first one is the optimal feature extraction method for a Punjabi SER system. The second one is the use of an appropriate feature selection method that selects effectual features from the ones extracted in the first step and removes the redundant features to improve the conduct of emotion recognition. The third one is the classification model that has been used further for emotion recognition. So the scope of this article is to explain the three main steps of the Punjabi SER system: feature extraction, feature selection, and emotion recognition with classifier. The results have been calculated and compared for number of feature set combinations, with and without a feature selection process. A total of 10 experiments are carried out, and various performance metrics such as precision, recall, F1-score, accuracy, and so on, are used to demonstrate the results.  © 2022 Association for Computing Machinery.",CNN; feature extraction; feature selection; features; Punjabi; speech,Convolutional neural networks; Extraction; Feature Selection; Speech recognition; Convolutional neural network; Emotion recognition; Feature; Feature extraction/selection; Feature selection algorithm; Features extraction; Features selection; Punjabi; Speech emotion recognition; Speech emotion recognition systems; Emotion Recognition
Fuzzy Contrast Set Based Deep Attention Network for Lexical Analysis and Mental Health Treatment,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148313572&doi=10.1145%2f3506701&partnerID=40&md5=7088c6feb838f715228301c34ea76f1a,"Internet-delivered psychological treatments (IDPT) consider mental problems based on Internet interaction. With such increased interaction because of the COVID-19 pandemic, more online tools have been widely used to provide evidence-based mental health services. This increase helps cover more population by using fewer resources for mental health treatments. Adaptivity and customization for the remedy routine can help solve mental health issues quickly. In this research, we propose a fuzzy contrast-based model that uses an attention network for positional weighted words and classifies mental patient authored text into distinct symptoms. After that, the trained embedding is used to label mental data. Then the attention network expands its lexicons to adapt to the usage of transfer learning techniques. The proposed model uses similarity and contrast sets to classify the weighted attention words. The fuzzy model then uses the sets to classify the mental health data into distinct classes. Our method is compared with non-embedding and traditional techniques to demonstrate the proposed model. From the experiments, the feature vector can achieve a high ROC curve of 0.82 with problems associated with nine symptoms.  © 2022 Association for Computing Machinery.",constraint sets; deep learning; Fuzzy system; human intervention,COVID-19; Embeddings; Constraint set; Deep learning; Human intervention; Lexical analysis; Mental health; Mental health treatments; Mental problems; On-line tools; Problem-based; Psychological treatments; Deep learning
BERIS: An mBERT-based Emotion Recognition Algorithm from Indian Speech,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151864778&doi=10.1145%2f3517195&partnerID=40&md5=f767770aa1b4e3bfb7f3f9c0ccde4df4,"Emotions, the building blocks of the human intellect, play a vital role in Artificial Intelligence (AI). For a robust AI-based machine, it is important that the machine understands human emotions. COVID-19 has introduced the world to no-touch intelligent systems. With an influx of users, it is critical to create devices that can communicate in a local dialect. A multilingual system is required in countries like India, which has a large population and a diverse range of languages. Given the importance of multilingual emotion recognition, this research introduces BERIS, an Indian language emotion detection system. From the Indian sound recording, BERIS estimates both acoustic and textual characteristics. To extract the textual features, we used Multilingual Bidirectional Encoder Representations from Transformers. For acoustics, BERIS computes the Mel Frequency Cepstral Coefficients and Linear Prediction coefficients, and Pitch. The features extracted are merged in a linear array. Since the dialogues are of varied lengths, the data are normalized to have arrays of equal length. Finally, we split the data into training and validated set to construct a predictive model. The model can predict emotions from the new input. On all the datasets presented, quantitative and qualitative evaluations show that the proposed algorithm outperforms state-of-the-art approaches.  © 2022 Association for Computing Machinery.",emotion recognition; LPC; mBERT; MFCC; Pitch,Continuous speech recognition; Intelligent systems; Building blockes; Emotion recognition; Human emotion; Large population; LPC; MBERT; MFCC; Multilingual system; Pitch; Recognition algorithm; Emotion Recognition
Automatic Labeling of Clusters for a Low-Resource Urdu Language,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151792819&doi=10.1145%2f3511097&partnerID=40&md5=b9d872219cc43e22f4cc55754106b674,"Document clustering techniques often produce clusters that require human intervention to interpret the meaning of such clusters. Automatic cluster labeling refers to the process of assigning a meaningful phrase to a cluster as a label. This article proposes an unsupervised method for cluster labeling that is based on noun phrase chunking. The proposed method is compared with four other statistical-based methods, including Z-Order, M-Order, T-Order, and YAKE. In addition to the statistical measures based labeling schemes, the approach is also compared with two graph-based techniques: TextRank and PositionRank. The experiments were performed on the low-resource Urdu language corpus of News Headlines. The proposed approach's effectiveness was evaluated using cosine similarity, the Jaccard index, and feedback received from human evaluators. The results show that the proposed method outperforms other methods. It was found that the labels produced were more relevant and semantically rich in contrast to other approaches.  © 2022 Association for Computing Machinery.",Cluster labeling; low-resource language; urdu language processing,Cluster analysis; Information retrieval; Automatic labelling; Cluster labeling; Clustering techniques; Document Clustering; Human intervention; Language processing; Low resource languages; Noun phrase; Unsupervised method; Urdu language processing; Graphic methods
A Statistical Language Model for Pre-Trained Sequence Labeling: A Case Study on Vietnamese,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128165065&doi=10.1145%2f3483524&partnerID=40&md5=bf38d1bc5d8c6d1f7adc5026b5424c81,"By defining the computable word segmentation unit and studying its probability characteristics, we establish an unsupervised statistical language model (SLM) for a new pre-Trained sequence labeling framework in this article. The proposed SLM is an optimization model, and its objective is to maximize the total binding force of all candidate word segmentation units in sentences under the condition of no annotated datasets and vocabularies. To solve SLM, we design a recursive divide-And-conquer dynamic programming algorithm. By integrating SLM with the popular sequence labeling models, Vietnamese word segmentation, part-of-speech tagging and named entity recognition experiments are performed. The experimental results show that our SLM can effectively promote the performance of sequence labeling tasks. Just using less than 10% of training data and without using a dictionary, the performance of our sequence labeling framework is better than the state-of-The-Art Vietnamese word segmentation toolkit VnCoreNLP on the cross-dataset test. SLM has no hyper-parameter to be tuned, and it is completely unsupervised and applicable to any other analytic language. Thus, it has good domain adaptability.  © 2021 Association for Computing Machinery.",sequence labeling; statistical language model; Unsupervised,Computational linguistics; Dynamic programming; Natural language processing systems; Speech recognition; Binding forces; Case-studies; Condition; Optimization models; Performance; Sequence Labeling; Statistical language modelling; Unsupervised; Vietnamese; Word segmentation; Statistical tests
Handwritten Annotation Spotting in Printed Documents Using Top-Down Visual Saliency Models,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128191726&doi=10.1145%2f3485468&partnerID=40&md5=4daae6a2e9be588dfe9c5f4c95107c65,"In this article, we address the problem of localizing text and symbolic annotations on the scanned image of a printed document. Previous approaches have considered the task of annotation extraction as binary classification into printed and handwritten text. In this work, we further subcategorize the annotations as underlines, encirclements, inline text, and marginal text. We have collected a new dataset of 300 documents constituting all classes of annotations marked around or in-between printed text. Using the dataset as a benchmark, we report the results of two saliency formulations-CRF Saliency and Discriminant Saliency, for predicting salient patches, which can correspond to different types of annotations. We also compare our work with recent semantic segmentation techniques using deep models. Our analysis shows that Discriminant Saliency can be considered as the preferred approach for fast localization of patches containing different types of annotations. The saliency models were learned on a small dataset, but still, give comparable performance to the deep networks for pixel-level semantic segmentation. We show that saliency-based methods give better outcomes with limited annotated data compared to more sophisticated segmentation techniques that require a large training set to learn the model.  © 2021 Association for Computing Machinery.",CRF (conditional random field); Discriminant saliency; FCN (fully convolutional network); handwritten annotations; sparse codes,Character recognition; Convolution; Convolutional neural networks; Network coding; Semantic Segmentation; Semantics; Text processing; Conditional random field; Convolutional networks; Discriminant saliency; Fully convolutional network; Handwritten annotation; Printed documents; Printed texts; Segmentation techniques; Semantic segmentation; Sparse codes; Random processes
Deep Understanding Based Multi-Document Machine Reading Comprehension,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151826002&doi=10.1145%2f3519296&partnerID=40&md5=386b34548ce962fe562e0f7c20a5df8c,"Most existing multi-document machine reading comprehension models mainly focus on understanding the interactions between the input question and documents, but ignore the following two kinds of understandings. First, to understand the semantic meaning of words in the input question and documents from the perspective of each other. Second, to understand the supporting cues for a correct answer from the perspective of intra-document and inter-documents. Ignoring these two kinds of important understandings would make the models overlook some important information that may be helpful for finding correct answers. To overcome this deficiency, we propose a deep understanding based model for multi-document machine reading comprehension. It has three cascaded deep understanding modules which are designed to understand the accurate semantic meaning of words, the interactions between the input question and documents, and the supporting cues for the correct answer. We evaluate our model on two large scale benchmark datasets, namely TriviaQA Web and DuReader. Extensive experiments show that our model achieves state-of-the-art results on both datasets.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",accurate word semantic meaning understanding; answer supporting cue understanding; DuReader; interaction understanding; multi-document machine reading comprehension; Question and answering; TriviaQA Web,Large dataset; Accurate word semantic meaning understanding; Answer supporting cue understanding; Dureader; Interaction understanding; Multi-document machine reading comprehension; Multidocuments; Question and answering; Reading comprehension; Triviaqa web; Word Semantics; Semantics
Leveraging Multilingual News Websites for Building a Kurdish Parallel Corpus,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151866221&doi=10.1145%2f3511806&partnerID=40&md5=14e5952ffc9bc6f70cf905cdc757a77e,"Machine translation has been a major motivation of development in natural language processing. Despite the burgeoning achievements in creating more efficient machine translation systems, thanks to deep learning methods, parallel corpora have remained indispensable for progress in the field. In an attempt to create parallel corpora for the Kurdish language, in this article, we describe our approach in retrieving potentially alignable news articles from multi-language websites and manually align them across dialects and languages based on lexical similarity and transliteration of scripts. We present a corpus containing 12,327 translation pairs in the two major dialects of Kurdish, Sorani and Kurmanji. We also provide 1,797 and 650 translation pairs in English-Kurmanji and English-Sorani. The corpus is publicly available under the CC BY-NC-SA 4.0 license.1  © 2022 Association for Computing Machinery.",Kurdish; less-resourced languages; machine translation; natural language processing; Parallel corpus,Computational linguistics; Computer aided language translation; Deep learning; Learning algorithms; Learning systems; Machine translation; Natural language processing systems; Kurdish; Language processing; Less-resourced language; Machine translation systems; Machine translations; Natural language processing; Natural languages; News websites; Parallel corpora; Translation pair; Websites
An Unsupervised and Robust Line and Word Segmentation Method for Handwritten and Degraded Printed Document,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127890201&doi=10.1145%2f3474118&partnerID=40&md5=b1700aa1786e31059f2a7907c7317661,"Segmentation of text lines and words in an unconstrained handwritten or a machine-printed degraded document is a challenging document analysis problem due to the heterogeneity in the document structure. Often there is un-even skew between the lines and also broken words in a document. In this article, the contribution lies in segmentation of a document page image into lines and words. We have proposed an unsupervised, robust, and simple statistical method to segment a document image that is either handwritten or machine-printed (degraded or otherwise). In our proposed method, the segmentation is treated as a two-class classification problem. The classification is done by considering the distribution of gap size (between lines and between words) in a binary page image. Our method is very simple and easy to implement. Other than the binarization of the input image, no pre-processing is necessary. There is no need of high computational resources. The proposed method is unsupervised in the sense that no annotated document page images are necessary. Thus, the issue of a training database does not arise. In fact, given a document page image, the parameters that are needed for segmentation of text lines and words are learned in an unsupervised manner. We have applied our proposed method on several popular publicly available handwritten and machine-printed datasets (ISIDDI, IAM-Hist, IAM, PBOK) of different Indian and other languages containing different fonts. Several experimental results are presented to show the effectiveness and robustness of our method. We have experimented on ICDAR-2013 handwriting segmentation contest dataset and our method outperforms the winning method. In addition to this, we have suggested a quantitative measure to compute the level of degradation of a document page image. © 2021 Association for Computing Machinery.",degraded machine printed document segmentation; handwriting segmentation; Line segmentation; unsupervised gap classification; word segmentation,Binary images; Computational linguistics; Information retrieval systems; Degraded machine printed document segmentation; Document segmentation; Handwriting segmentation; Line segmentation; Printed documents; Simple++; Text lines; Text words; Unsupervised gap classification; Word segmentation; Image segmentation
A Comprehensive Guideline for Bengali Sentiment Annotation,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127900323&doi=10.1145%2f3474363&partnerID=40&md5=214bbd360ead614ce7767745d4a7117b,"Sentiment Analysis (SA) is a Natural Language Processing (NLP) and an Information Extraction (IE) task that primarily aims to obtain the writer's feelings expressed in positive or negative by analyzing a large number of documents. SA is also widely studied in the fields of data mining, web mining, text mining, and information retrieval. The fundamental task in sentiment analysis is to classify the polarity of a given content as Positive, Negative, or Neutral. Although extensive research has been conducted in this area of computational linguistics, most of the research work has been carried out in the context of English language. However, Bengali sentiment expression has varying degree of sentiment labels, which can be plausibly distinct from English language. Therefore, sentiment assessment of Bengali language is undeniably important to be developed and executed properly. In sentiment analysis, the prediction potential of an automatic modeling is completely dependent on the quality of dataset annotation. Bengali sentiment annotation is a challenging task due to diversified structures (syntax) of the language and its different degrees of innate sentiments (i.e., weakly and strongly positive/negative sentiments). Thus, in this article, we propose a novel and precise guideline for the researchers, linguistic experts, and referees to annotate Bengali sentences immaculately with a view to building effective datasets for automatic sentiment prediction efficiently. © 2021 Association for Computing Machinery.",Annotation guideline; natural language processing; sentiment analysis,Data mining; Information retrieval; Linguistics; Quality control; Annotation guideline; Automatic modeling; Bengali language; Bengalis; English languages; Positive/negative; Sentiment analysis; Text information; Text-mining; Web Mining; Sentiment analysis
STEMUR: An Automated Word Conflation Algorithm for the Urdu Language,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127912944&doi=10.1145%2f3476226&partnerID=40&md5=692e6aa3c44890acd32b3c87cb8dec38,"Stemming is a common word conflation method that perceives stems embedded in the words and decreases them to their stem (root) by conflating all the morphologically related terms into a single term, without doing a complete morphological analysis. This article presents STEMUR, an enhanced stemming algorithm for automatic word conflation for Urdu language. In addition to handling words with prefixes and suffixes, STEMUR also handles words with infixes. Rather than using a totally unsupervised approach, we utilized the linguistic knowledge to develop a collection of patterns for Urdu infixes to enhance the accuracy of the stems and affixes acquired during the training process. Additionally, STEMUR also handles English loan words and can handle words with more than one affix. STEMUR is compared with four existing Urdu stemmers including Assas-Band and the template-based stemmer that are also implemented in this study. Results are processed on two corpora containing 89,437 and 30,907 words separately. Results show clear improvements regarding strength and accuracy of STEMUR. The use of maximum possible infix rules boosted our stemmer's accuracy up to 93.1% and helped us achieve a precision of 98.9%. © 2021 Association for Computing Machinery.",Conflation; index processing; prefix; suffix and infix stemming; Urdu language processing,Conflation; Index processing; Language processing; Morphological analysis; Prefix; Stemming algorithms; Suffix and infix stemming; Unsupervised approaches; Urdu language processing; Word conflation
Investigating the Feasibility of Deep Learning Methods for Urdu Word Sense Disambiguation,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127919527&doi=10.1145%2f3477578&partnerID=40&md5=e86c5342ba56d7e78ce24566e4b86c04,"Word Sense Disambiguation (WSD), the process of automatically identifying the correct meaning of a word used in a given context, is a significant challenge in Natural Language Processing. A range of approaches to the problem has been explored by the research community. The majority of these efforts has focused on a relatively small set of languages, particularly English. Research on WSD for South Asian languages, particularly Urdu, is still in its infancy. In recent years, deep learning methods have proved to be extremely successful for a range of Natural Language Processing tasks. The main aim of this study is to apply, evaluate, and compare a range of deep learning methods approaches to Urdu WSD (both Lexical Sample and All-Words) including Simple Recurrent Neural Networks, Long-Short Term Memory, Gated Recurrent Units, Bidirectional Long-Short Term Memory, and Ensemble Learning. The evaluation was carried out on two benchmark corpora: (1) the ULS-WSD-18 corpus and (2) the UAW-WSD-18 corpus. Results (Accuracy = 63.25% and F1-Measure = 0.49) show that a deep learning approach outperforms previously reported results for the Urdu All-Words WSD task, whereas performance using deep learning approaches (Accuracy = 72.63% and F1-Measure = 0.60) are low in comparison to previously reported for the Urdu Lexical Sample task. © 2021 Association for Computing Machinery.",Bidirectional Long-Short Term Memory; deep learning; Gated Recurrent Units; Long-Short Term Memory; Recurrent Neural Network; Urdu All-Words WSD task; Urdu Lexical Sample WSD task; Word Sense Disambiguation,Brain; Natural language processing systems; Bidirectional long-short term memory; Deep learning; Gated recurrent unit; Learning approach; Learning methods; Urdu all-word word sense disambiguation task; Urdu lexical sample word sense disambiguation task; Word Sense Disambiguation; Long short-term memory
Generating Factoid Questions with Question Type Enhanced Representation and Attention-based Copy Mechanism,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127884015&doi=10.1145%2f3474555&partnerID=40&md5=6cbd132a2b61cea7893eaa8806a587ca,"Question generation over knowledge bases is an important research topic. How to deal with rare and low-frequency words in traditional generation models is a key challenge for question generation. Although the copy mechanism provides significant performance improvements, the original copy mechanism weakens the focus on aspect generation in the overall representations. In this article, we present a novel method to improve question generation with a question type enhanced representation and attention-based copy mechanism. The proposed method exploits the advantages of the generate mode in the copy mechanism and replaces objects in the factual triples with question types, which attempts to improve the output quality in the generate mode and effectively generate questions with proper interrogative words. We evaluate the proposed method on two standard benchmark datasets. The experimental results demonstrate that our proposed method can produce higher-quality questions than these of the Encoder-Decoder-based and CopyNet-based methods. © 2022 Association for Computing Machinery.",knowledge base; question answering; Question generation; text generation,Factoid questions; Lower frequencies; Novel methods; Output quality; Performance; Question Answering; Question generation; Question type; Research topics; Text generations; Knowledge based systems
Recurrent Neural Hidden Markov Model for High-order Transition,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127903512&doi=10.1145%2f3476511&partnerID=40&md5=9d7c07ed335d9c057e649dddfd55da90,"We propose a method to pay attention to high-order relations among latent states to improve the conventional HMMs that focus only on the latest latent state, since they assume Markov property. To address the high-order relations, we apply an RNN to each sequence of latent states, because the RNN can represent the information of an arbitrary-length sequence with their cell: a fixed-size vector. However, the simplest way, which provides all latent sequences explicitly for the RNN, is intractable due to the combinatorial explosion of the search space of latent states.Thus, we modify the RNN to represent the history of latent states from the beginning of the sequence to the current state with a fixed number of RNN cells whose number is equal to the number of possible states. We conduct experiments on unsupervised POS tagging and synthetic datasets. Experimental results show that the proposed method achieves better performance than previous methods. In addition, the results on the synthetic dataset indicate that the proposed method can capture the high-order relations. © 2021 Association for Computing Machinery.",Neural networks; POS tagging,Computational linguistics; Recurrent neural networks; Hidden-Markov models; High-order; High-order transitions; Higher-order; Latent state; Markov property; Neural-networks; Ordering relations; POS tagging; Synthetic datasets; Hidden Markov models
Low-Resource Language Discrimination toward Chinese Dialects with Transfer Learning and Data Augmentation,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127919142&doi=10.1145%2f3473499&partnerID=40&md5=00bb8c2b05020a4125e5e33295bb8699,"Chinese dialects discrimination is a challenging natural language processing task due to scarce annotation resource. In this article, we develop a novel Chinese dialects discrimination framework with transfer learning and data augmentation (CDDTLDA) in order to overcome the shortage of resources. To be more specific, we first use a relatively larger Chinese dialects corpus to train a source-side automatic speech recognition (ASR) model. Then, we adopt a simple but effective data augmentation method (i.e., speed, pitch, and noise disturbance) to augment the target-side low-resource Chinese dialects, and fine-tune another target ASR model based on the previous source-side ASR model. Meanwhile, the potential common semantic features between source-side and target-side ASR models can be captured by using self-attention mechanism. Finally, we extract the hidden semantic representation in the target ASR model to conduct Chinese dialects discrimination. Our extensive experimental results demonstrate that our model significantly outperforms state-of-the-art methods on two benchmark Chinese dialects corpora. © 2021 Association for Computing Machinery.",chinese dialects; data augmentation; dialects discrimination; Low-resource; transfer learning,Natural language processing systems; Semantics; Automatic speech recognition; Chinese dialects; Data augmentation; Dialect discrimination; Language discrimination; Low resource languages; Low-resource; Recognition models; Simple++; Transfer learning; Speech recognition
I3rab: A New Arabic Dependency Treebank Based on Arabic Grammatical Theory,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127921491&doi=10.1145%2f3472295&partnerID=40&md5=acffc05f1fec9f2bacf718daa3cc6a9f,"Treebanks are valuable linguistic resources that include the syntactic structure of a language sentence in addition to part-of-speech tags and morphological features. They are mainly utilized in modeling statistical parsers. Although the statistical natural language parser has recently become more accurate for languages such as English, those for the Arabic language still have low accuracy. The purpose of this article is to construct a new Arabic dependency treebank based on the traditional Arabic grammatical theory and the characteristics of the Arabic language, to investigate their effects on the accuracy of statistical parsers. The proposed Arabic dependency treebank, called I3rab, contrasts with existing Arabic dependency treebanks in two main concepts. The first concept is the approach of determining the main word of the sentence, and the second concept is the representation of the joined and covert pronouns. To evaluate I3rab, we compared its performance against a subset of Prague Arabic Dependency Treebank that shares a comparable level of details. The conducted experiments show that the percentage improvement reached up to 10.24% in UAS and 18.42% in LAS. © 2021 Association for Computing Machinery.",arabic grammatical theory; Arabic language; dependency parsing; dependency treebank; nominal sentence; verbal sentence,Computational linguistics; Forestry; Natural language processing systems; Arabic grammatical theory; Arabic languages; Dependency parsing; Dependency treebank; Linguistic resources; Nominal sentence; Statistical parser; Syntactic structure; Treebanks; Verbal sentence; Syntactics
"A Case Study on Handwritten Indic Script Classification: Benchmarking of the Results at Page, Block, Text-line, and Word Levels",2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127900674&doi=10.1145%2f3476102&partnerID=40&md5=0a82d34c3198419c2a63fe63f1987a50,"Handwritten script classification is still considered as a challenging research problem in the domain of document image analysis. Although some research attempts have been made by the researchers for solving the challenging issues, a comprehensive solution is yet to be achieved. The case study, undertaken here, analyzes the performances of various state-of-the art handwritten script classification methods for Indian scripts where features, needed for the script classification task, are extracted from the script images at four different granularity levels, i.e., page, block, text line, or word. The results of handwritten script classification at each level have been obtained and compared using eight different feature sets and six different state-of-the-art classifiers. Based on the classification results, an ideal level for performing the handwritten script classification task is suggested among these four classification levels. The results have also been improved by using two feature dimensionality reduction methods. All these experiments are done on two different handwritten Indic script databases, of which one is an in-house developed dataset and the other one is a freely available dataset. Finally, some future research directions that may be undertaken by the researchers as an application of the handwritten Indic script classification problem are also highlighted. The work presented here provides a basic foundation for the construction of a comprehensive handwritten script classification method for official Indian scripts. © 2021 Association for Computing Machinery.",case study; Handwritten script classification; Indic scripts; structure based features; visual appearance-based features,Character recognition; Image analysis; Information retrieval systems; Text processing; Appearance based; Case-studies; Handwritten script classification; Indic script; Structure based feature; Structure-based; Text lines; Text words; Visual appearance; Visual appearance-based feature; Classification (of information)
Cross-lingual Text Reuse Detection Using Translation Plus Monolingual Analysis for English-Urdu Language Pair,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127871243&doi=10.1145%2f3473331&partnerID=40&md5=5efcada4716a099245975cabfb2c1844,"Cross-Lingual Text Reuse Detection (CLTRD) has recently attracted the attention of the research community due to a large amount of digital text readily available for reuse in multiple languages through online digital repositories. In addition, efficient machine translation systems are freely and readily available to translate text from one language into another, which makes it quite easy to reuse text across languages, and consequently difficult to detect it. In the literature, the most prominent and widely used approach for CLTRD is Translation plus Monolingual Analysis (T+MA). To detect CLTR for English-Urdu language pair, T+MA has been used with lexical approaches, namely, N-gram Overlap, Longest Common Subsequence, and Greedy String Tiling. This clearly shows that T+MA has not been thoroughly explored for the English-Urdu language pair. To fulfill this gap, this study presents an in-depth and detailed comparison of 26 approaches that are based on T+MA. These approaches include semantic similarity approaches (semantic tagger based approaches, WordNet-based approaches), probabilistic approach (Kullback-Leibler distance approach), monolingual word embedding-based approaches siamese recurrent architecture, and monolingual sentence transformer-based approaches for English-Urdu language pair. The evaluation was carried out using the CLEU benchmark corpus, both for the binary and the ternary classification tasks. Our extensive experimentation shows that our proposed approach that is a combination of 26 approaches obtained an F1 score of 0.77 and 0.61 for the binary and ternary classification tasks, respectively, and outperformed the previously reported approaches [41] (F1 = 0.73) for the binary and (F1 = 0.55) for the ternary classification tasks) on the CLEU corpus. © 2021 Association for Computing Machinery.",deep Learning approach; English-Urdu language pairs; monolingual sentence transformer based approaches; probabilistic approach; semantic approaches; word embedding approach,Computational linguistics; Deep learning; Semantics; Translation (languages); Deep learning approach; Embeddings; English-urdu language pair; Language pairs; Learning approach; Monolingual sentence transformer based approach; Probabilistics approach; Reuse; Semantic approach; Word embedding approach; Embeddings
Development of Automatic Rule-based Semantic Tagger and Karaka Analyzer for Hindi,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127852563&doi=10.1145%2f3479155&partnerID=40&md5=822e93818119d69686eea870f2cb84b7,"Hindi is the third most-spoken language in the world (615 million speakers) and has the fourth highest native speakers (341 million). It is an inflectionally rich and relatively free word-order language with an immense vocabulary set. Despite being such a celebrated language across the globe, very few Natural Language Processing (NLP) applications and tools have been developed to support it computationally. Moreover, most of the existing ones are not efficient enough due to the lack of semantic information (or contextual knowledge). Hindi grammar is based on Paninian grammar and derives most of its rules from it. Paninian grammar very aggressively highlights the role of karaka theory in free-word order languages. In this article, we present an application that extracts all possible karakas from simple Hindi sentences with an accuracy of 84.2% and an F1 score of 88.5%. We consider features such as Parts of Speech tags, post-position markers (vibhaktis), semantic tags for nouns and syntactic structure to grab the context in different-sized word windows within a sentence. With the help of these features, we built a rule-based inference engine to extract karakas from a sentence. The application takes in a text file with clean (without punctuation) simple Hindi sentences and gives back karaka tagged sentences in a separate text file as output. © 2021 Association for Computing Machinery.",feature extraction; Karaka analyzer; language resource; semantic tagging,Inference engines; Natural language processing systems; Semantics; Features extraction; Free word order languages; Karaka analyzer; Language resources; Rule-based semantics; Semantic tagging; Semantic tags; Simple++; Spoken languages; Text file; Syntactics
An Unsupervised Approach for Sentiment Analysis on Social Media Short Text Classification in Roman Urdu,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127315579&doi=10.1145%2f3474119&partnerID=40&md5=302b1ecbaf744d1609d983336b2a2c0f,"During the last two decades, sentiment analysis, also known as opinion mining, has become one of the most explored research areas in Natural Language Processing (NLP) and data mining. Sentiment analysis focuses on the sentiments or opinions of consumers expressed over social media or different web sites. Due to exposure on the Internet, sentiment analysis has attracted vast numbers of researchers over the globe. A large amount of research has been conducted in English, Chinese, and other languages used worldwide. However, Roman Urdu has been neglected despite being the third most used language for communication in the world, covering millions of users around the globe. Although some techniques have been proposed for sentiment analysis in Roman Urdu, these techniques are limited to a specific domain or developed incorrectly due to the unavailability of language resources available for Roman Urdu. Therefore, in this article, we are proposing an unsupervised approach for sentiment analysis in Roman Urdu. First, the proposed model normalizes the text to overcome spelling variations of different words. After normalizing text, we have used Roman Urdu and English opinion lexicons to correctly identify users' opinions from the text. We have also incorporated negation terms and stemming to assign polarities to each extracted opinion. Furthermore, our model assigns a score to each sentence on the basis of the polarities of extracted opinions and classifies each sentence as positive, negative, or neutral. In order to verify our approach, we have conducted experiments on two publicly available datasets for Roman Urdu and compared our approach with the existing model. Results have demonstrated that our approach outperforms existing models for sentiment analysis tasks in Roman Urdu. Furthermore, our approach does not suffer from domain dependency. © 2021 Association for Computing Machinery.",opinion extraction; roman urdu; roman urdu text classification; Sentiment analysis; text normalization,Classification (of information); Data mining; Social networking (online); Opinion extraction; Research areas; Roman urdu; Roman urdu text classification; Sentiment analysis; Short text classifications; Social media; Text Normalisation; Unsupervised approaches; Web-sites; Sentiment analysis
UrduAI: Writeprints for Urdu Authorship Identification,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124641568&doi=10.1145%2f3476467&partnerID=40&md5=96615bff27c590ef52a87daefdffc814,"The authorship identification task aims at identifying the original author of an anonymous text sample from a set of candidate authors. It has several application domains such as digital text forensics and information retrieval. These application domains are not limited to a specific language. However, most of the authorship identification studies are focused on English and limited attention has been paid to Urdu. However, existing Urdu authorship identification solutions drop accuracy as the number of training samples per candidate author reduces and when the number of candidate authors increases. Consequently, these solutions are inapplicable to real-world cases. Moreover, due to the unavailability of reliable POS taggers or sentence segmenters, all existing authorship identification studies on Urdu text are limited to the word n-grams features only. To overcome these limitations, we formulate a stylometric feature space, which is not limited to the word n-grams feature only. Based on this feature space, we use an authorship identification solution that transforms each text sample into a point set, retrieves candidate text samples, and relies on the nearest neighbors classifier to predict the original author of the anonymous text sample. To evaluate our solution, we create a significantly larger corpus than existing studies and conduct several experimental studies that show that our solution can overcome the limitations of existing studies and report an accuracy level of 94.03%, which is higher than all previous authorship identification works. © 2021 Association for Computing Machinery.",Authorship identification; forensic investigation; stylometry; text classification,Classification (of information); Computational linguistics; Computer crime; Digital forensics; Applications domains; Authorship identification; Digital text; Feature space; Forensic investigation; Limited attentions; Specific languages; Stylometry; Word n-grams; Write-print; Text processing
Adversarial Separation Network for Text Style Transfer,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127861795&doi=10.1145%2f3472621&partnerID=40&md5=266d0ed495048ccd171238672c6d1272,"This article considers the task of text style transfer: transforming a specific style of sentence into another while preserving its style-independent content. A dominate approach to text style transfer is to learn a good content factor of text, define a fixed vector for every style and recombine them to generate text in the required style. In fact, there are a large number of different words to convey the same style from different aspects. Thus, using a fixed vector to represent one style is very inefficient, which causes the weak representation power of the style vector and limits text diversity of the same style. To address this problem, we propose a novel neural generative model called Adversarial Separation Network (ASN), which can learn the content and style vector jointly and the learnt vectors have strong representation power and good interpretabilities. In our method, adversarial learning is implemented to enhance our model's capability of disentangling the two factors. To evaluate our method, we conduct experiments on two benchmark datasets. Experimental results show our method can perform style transfer better than strong comparison systems. We also demonstrate the strong interpretability of the learnt latent vectors. © 2021 Association for Computing Machinery.",Adversarial learning; adversarial separation network; latent factor; mapping; neural generative model; text style transfer; variational autoencoder,Learning systems; Separation; Adversarial learning; Adversarial separation network; Auto encoders; Generative model; Latent factor; Learn+; Neural generative model; Separation network; Text style transfer; Variational autoencoder; Vectors
Simple Extensible Deep Learning Model for Automatic Arabic Diacritization,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127852451&doi=10.1145%2f3480938&partnerID=40&md5=b2574fe53889de175ab54d2d4907a7f5,"Automatic diacritization is an Arabic natural language processing topic based on the sequence labeling task where the labels are the diacritics and the letters are the sequence elements. A letter can have from zero up to two diacritics. The dataset used was a subset of the preprocessed version of the Tashkeela corpus. We developed a deep learning model composed of a stack of four bidirectional long short-term memory hidden layers of the same size and an output layer at every level. The levels correspond to the groups that we classified the diacritics into (short vowels, double case-endings, Shadda, and Sukoon). Before training, the data were divided into input vectors containing letter indexes and outputs vectors containing the indexes of diacritics regarding their groups. Both input and output vectors are concatenated, then a sliding window operation with overlapping is performed to generate continuous and fixed-size data. Such data is used for both training and evaluation. Finally, we realize some tests using the standard metrics with all of their variations and compare our results with two recent state-of-the-art works. Our model achieved 3% diacritization error rate and 8.99% word error rate when including all letters. We have also generated the confusion matrix to show the performances per output and analyzed the mismatches of the first 500 lines to classify the model errors according to their linguistic nature. © 2021 Association for Computing Machinery.",Arabic; diacritics; diacritics restoration; diacritization; natural language processing,Deep learning; Errors; Natural language processing systems; Arabic; Arabic natural language processing; Diacritic; Diacritics restorations; Diacritization; Input vector; Learning models; Output vectors; Sequence Labeling; Simple++; Linguistics
Improved Word Sense Determination in Malayalam using Latent Dirichlet Allocation and Semantic Features,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127873897&doi=10.1145%2f3476978&partnerID=40&md5=8926359cbad95a5be1d9105dcd162056,"Recent years have witnessed phenomenal developments worldwide in the field of NLP. But developments in Indian regional languages are very few compared to them. This work is a step towards the construction of a target word sense disambiguation system in Malayalam, which is the regional language of the state of Kerala, India. Word Sense Disambiguation/Determination refers to the task of correctly identifying the sense of an ambiguous word from its context. This is considered an AI-Complete problem in the field of Natural Language Processing. For this purpose, an exclusive corpus of 1,147 contexts of target ambiguous words has been created, which to the best of our knowledge is the first attempt in Malayalam. This work describes how the performance of an unsupervised LDA-based approach towards WSD could be improved using semantic features like synonyms and co-occurrence information. © 2021 Association for Computing Machinery.",Artificial intelligence; Indian language computing; Latent Dirichlet allocation (LDA); Malayalam processing; pattern recognition,Artificial intelligence; Natural language processing systems; Pattern recognition; Semantics; Indian language computing; Indian languages; Latent Dirichlet allocation; Malayalam processing; Malayalams; Semantic features; Target words; Word sense; Word sense disambiguation systems; Statistics
Developing a Cross-lingual Semantic Word Similarity Corpus for English-Urdu Language Pair,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127858195&doi=10.1145%2f3472618&partnerID=40&md5=1119b881c0c63e8090b1b3be05fc986e,"Semantic word similarity is a quantitative measure of how much two words are contextually similar. Evaluation of semantic word similarity models requires a benchmark corpus. However, despite the millions of speakers and the large digital text of the Urdu language on the Internet, there is a lack of benchmark corpus for the Cross-lingual Semantic Word Similarity task for the Urdu language. This article reports our efforts in developing such a corpus. The newly developed corpus is based on the SemEval-2017 task 2 English dataset, and it contains 1,945 cross-lingual English-Urdu word pairs. For each of these pairs of words, semantic similarity scores were assigned by 11 native Urdu speakers. In addition to corpus generation, this article also reports the evaluation results of a baseline approach, namely ""Translation Plus Monolingual Analysis""for automated identification of semantic similarity between English-Urdu word pairs. The results showed that the path length similarity measure performs better for the Google and Bing translated words. The newly created corpus and evaluation results are freely available online for further research and development. © 2021 Association for Computing Machinery.",Cross-lingual semantic word similarity; natural language processing; Urdu; WordNet,Natural language processing systems; Translation (languages); Cross-lingual; Cross-lingual semantic word similarity; Evaluation results; Language pairs; Quantitative measures; Semantic similarity; Urdu; Word similarity; Word-pairs; Wordnet; Semantics
Domain-Aware Word Segmentation for Chinese Language: A Document-Level Context-Aware Model,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126186092&doi=10.1145%2f3481298&partnerID=40&md5=f03a198575435811db1a1bf254d5a75e,"Word segmentation is an essential and challenging task in natural language processing, especially for the Chinese language due to its high linguistic complexity. Existing methods for Chinese word segmentation, including statistical machine learning methods and neural network methods, usually have good performance in specific knowledge domains. Given the increasing importance of interdisciplinary and cross-domain studies, one of the challenges in cross-domain word segmentation is to handle the out-of-vocabulary (OOV) words. Existing methods show unsatisfactory performance to meet the practical standard. To this end, we propose a document-level context-aware model that can automatically perceive and identify OOV words from different domains. Our method jointly implements a word-based and a character-based model and then processes the results with a newly proposed reconstruction model. We evaluate the new method by designing and conducting comprehensive experiments on two real-world datasets (e.g., news from different domains). The results demonstrate the superiority of our method over the state-of-the-art models in handling texts from different domains. Importantly, when doing the word segmentation under the cross-domain scenario, our proposed method can improve the performance of OOV words recognition. © 2021 Association for Computing Machinery.",Chinese word segmentation; document level; real-world scenario,Computational linguistics; Learning algorithms; Learning systems; Chinese language; Chinese word segmentation; Context-aware models; Cross-domain; Different domains; Document level; Outof-vocabulary words (OOV); Performance; Real-world scenario; Word segmentation; Natural language processing systems
Using Pre-trained Language Model to Enhance Active Learning for Sentence Matching,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127864574&doi=10.1145%2f3480937&partnerID=40&md5=404e61bb253e2ce93ef1065c2770262c,"Active learning is an effective method to substantially alleviate the problem of expensive annotation cost for data-driven models. Recently, pre-trained language models have been demonstrated to be powerful for learning language representations. In this article, we demonstrate that the pre-trained language model can also utilize its learned textual characteristics to enrich criteria of active learning. Specifically, we provide extra textual criteria with the pre-trained language model to measure instances, including noise, coverage, and diversity. With these extra textual criteria, we can select more efficient instances for annotation and obtain better results. We conduct experiments on both English and Chinese sentence matching datasets. The experimental results show that the proposed active learning approach can be enhanced by the pre-trained language model and obtain better performance. © 2021 Association for Computing Machinery.",active learning; pre-trained language model; Sentence matching,Artificial intelligence; Computational linguistics; Active Learning; Chinese sentence; Data-driven model; English sentences; Language model; Learning approach; Learning languages; Matchings; Pre-trained language model; Sentence matching; Learning systems
Multi-task Fuzzy Clustering-Based Multi-task TSK Fuzzy System for Text Sentiment Classification,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127855378&doi=10.1145%2f3476103&partnerID=40&md5=164200f6615f3e4f873c7bed019273df,"Text sentiment classification is an important technology for natural language processing. A fuzzy system is a strong tool for processing imprecise or ambiguous data, and it can be used for text sentiment analysis. This article proposes a new formulation of a multi-task Takagi-Sugeno-Kang fuzzy system (TSK FS) modeling, which can be used for text sentiment image classification. Using a novel multi-task fuzzy c-means clustering algorithm, the common (public) information among all tasks and the individual (private) information for each task are extracted. The information about clustering, for example, cluster centers, can be used to learn the antecedent parameters of multi-task TSK fuzzy systems. With the common and individual antecedent parameters obtained, a corresponding multi-task learning mechanism for learning consequent parameters is devised. Accordingly, a multi-task fuzzy clustering-based multi-task TSK fuzzy system (MTFCM-MT-TSK-FS) is proposed. When the proposed model is built, the information conveyed by the fuzzy rules formed is two-fold, including (1) common fuzzy rules representing the inter-task correlation information and (2) individual fuzzy rules depicting the independent information of each task. The experimental results on several text sentiment datasets demonstrate the validity of the proposed model. © 2021 Association for Computing Machinery.",Common fuzzy rules; individual fuzzy rules; multi-task fuzzy c-means; multi-task Takagi-Sugeno-Kang fuzzy systems; text sentiment classification,Classification (of information); Clustering algorithms; Fuzzy clustering; Fuzzy rules; Learning algorithms; Learning systems; Sentiment analysis; Common fuzzy rule; Fuzzy-c means; Individual fuzzy rule; Multi tasks; Multi-task fuzzy c-mean; Multi-task takagi-sugeno-kang fuzzy system; Sentiment classification; Takagi-Sugeno-Kang fuzzy system; Text sentiment classification; Fuzzy inference
Improving Chinese-Vietnamese Neural Machine Translation with Linguistic Differences,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127838808&doi=10.1145%2f3477536&partnerID=40&md5=1989944e470f053dbc1c59b9a436e181,"We present a simple, efficient data augmentation approach for boosting Chinese-Vietnamese neural machine translation performance by leveraging the linguistic difference between the two languages. We first define the formalized representation of modifier symmetry, which is one of the most representative linguistic differences between Chinese and Vietnamese. We then propose and test two data augmentation strategies for leveraging the linguistic difference, which can be integrated naturally with different translation models. Results indicate that both strategies can introduce linguistic rules to boost translation accuracy. Tests on Chinese-Vietnamese benchmarks show significant accuracy improvements. To facilitate studies in this domain, we also release an open-source toolkit1 with flexible implementation for Chinese-Vietnamese linguistic difference tagging. © 2022 Association for Computing Machinery.",Chinese-Vietnamese; data augmentation; linguistic difference; Neural machine translation,Computational linguistics; Computer aided language translation; Accuracy Improvement; Chinese-vietnamese; Data augmentation; Linguistic differences; Linguistic rules; Open-source; Performance; Simple++; Translation models; Vietnamese; Neural machine translation
Denigrate Comment Detection in Low-Resource Hindi Language Using Attention-Based Residual Networks,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124253187&doi=10.1145%2f3431729&partnerID=40&md5=9650ef43b4b527e418e3de6939e19520,"Cyberspace has been recognized as a conducive environment for use of various hostile, direct, and indirect behavioural tactics to target individuals or groups. Denigration is one of the most frequently used cyberbullying ploys to actively damage, humiliate, and disparage the online reputation of target by sending, posting, or publishing cruel rumours, gossip, and untrue statements. Previous pertinent studies report detecting profane, vulgar, and offensive words primarily in the English language. This research puts forward a model to detect online denigration bullying in low-resource Hindi language using attention residual networks. The proposed model Hindi Denigrate Comment-Attention Residual Network (HDC-ARN) intends to uncover defamatory posts (denigrate comments) written in Hindi language which stake and vilify a person or an entity in public. Data with 942 denigrate comments and 1499 non-denigrate comments is scraped using certain hashtags from two recent trending events in India: Tablighi Jamaat spiked Covid-19 (April 2020, Event 1) and Sushant Singh Rajput Death (June 2020: Event 2). Only text-based features, that is, the actual content of the post, are considered. The pre-Trained word embedding for Hindi language from fastText is used. The model has three ResNet blocks with an attention layer that generates a post vector for a single input, which is passed through a sigmoid activation function to get the final output as either denigrate (positive class) or non-denigrate (negative class). An F-1 score of 0.642 is achieved on the dataset.  © 2021 Association for Computing Machinery.",attention; cyberbullying; deep learning; Denigration; residual networks,Deep learning; Attention; Cyber bullying; Cyberspaces; Deep learning; Denigration; Embeddings; English languages; Hashtags; Residual network; Text-based features; Computer crime
Empirical Evaluation of Shallow and Deep Learning Classifiers for Arabic Sentiment Analysis,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124284402&doi=10.1145%2f3466171&partnerID=40&md5=a15664ce106d031a895e53cc40e0b635,"This work presents a detailed comparison of the performance of deep learning models such as convolutional neural networks, long short-Term memory, gated recurrent units, their hybrids, and a selection of shallow learning classifiers for sentiment analysis of Arabic reviews. Additionally, the comparison includes state-of-The-Art models such as the transformer architecture and the araBERT pre-Trained model. The datasets used in this study are multi-dialect Arabic hotel and book review datasets, which are some of the largest publicly available datasets for Arabic reviews. Results showed deep learning outperforming shallow learning for binary and multi-label classification, in contrast with the results of similar work reported in the literature. This discrepancy in outcome was caused by dataset size as we found it to be proportional to the performance of deep learning models. The performance of deep and shallow learning techniques was analyzed in terms of accuracy and F1 score. The best performing shallow learning technique was Random Forest followed by Decision Tree, and AdaBoost. The deep learning models performed similarly using a default embedding layer, while the transformer model performed best when augmented with araBERT.  © 2021 Association for Computing Machinery.",Deep learning; embedding; learning curve; misclassification; shallow learning,Adaptive boosting; Classification (of information); Convolutional neural networks; Decision trees; Embeddings; Recurrent neural networks; Reviews; Deep learning; Embeddings; Learning classifiers; Learning curves; Learning models; Learning techniques; Misclassifications; Performance; Sentiment analysis; Shallow learning; Sentiment analysis
A New Concept of Electronic Text Based on Semantic Coding System for Machine Translation,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124304734&doi=10.1145%2f3469655&partnerID=40&md5=6a05d8d014a6c057c885587cff09e891,"In the field of machine translation of texts, the ambiguity in both lexical (dictionary) and structural aspects is still one of the difficult problems. Researchers in this field use different approaches, the most important of which is machine learning in its various types. The goal of the approach that we propose in this article is to define a new concept of electronic text, which makes the electronic text free from any lexical or structural ambiguity. We used a semantic coding system that relies on attaching the original electronic text (via the text editor interface) with the meanings intended by the author. The author defines the meaning desired for each word that can be a source of ambiguity. The proposed approach in this article can be used with any type of electronic text (text processing applications, web pages, email text, etc.). Thanks to the approach that we propose and through the experiments that we have conducted using it, we can obtain a very high accuracy rate. We can say that the problem of lexical and structural ambiguity can be completely solved. With this new concept of electronic text, the text file contains not only the text but also with it the true sense of the exact meaning intended by the writer in the form of symbols. These semantic symbols are used during machine translation to obtain a translated text completely free of any lexical and structural ambiguity.  © 2021 Association for Computing Machinery.",code; dictionary ambiguity; Semantic; structural ambiguity; text; word,Computational linguistics; Computer aided language translation; Machine translation; Signal encoding; Text processing; Websites; Code; Coding system; Dictionary ambiguity; Lexical ambiguity; Machine translations; Semantic coding; Structural ambiguity; Structural aspects; Text; Word; Semantics
An Effective Approach for Rumor Detection of Arabic Tweets Using eXtreme Gradient Boosting Method,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124256038&doi=10.1145%2f3461697&partnerID=40&md5=bce57e31ece428115783a1a46255631b,"Twitter is currently one of the most popular microblogging platforms allowing people to post short messages, news, thoughts, and so on. The Twitter user community is growing very fast. It has an average of 328 million active accounts today, making it one of the most common media for getting information during any influential or important event. Because it is freely used by the public, some credibility checking is required, especially when it comes to events of high importance. Automatic rumor detection in Arabic tweets is a challenging task due to the changes in the structural and morphological nature of the Arabic language, which makes the detection of rumors more difficult than in other languages. In this article, we proposed an effective approach for rumor detection of Arabic tweets using an eXtreme gradient boosting (XGBoost) classifier. We conducted a set of experiments on a public dataset that contained a large number of rumor and non-rumor tweets. The model uses a comprehensive set of features, including content-based, user-based, and topic-based features, allowing one to look at credibility from different angles. The experimental results demonstrated that the proposed XGBoost-based approach achieves 97.18% accuracy on 60% of the dataset as a training set, which is the highest accuracy rate compared with the other methods used in recent related work.  © 2022 Association for Computing Machinery.",Arabic; machine learning; Rumor detection; Twitter; XGBoost method,Large dataset; Machine learning; Arabic; Boosting method; Community IS; Effective approaches; Gradient boosting; Micro-blogging platforms; Rumor detection; Short message; User communities; Xgboost method; Social networking (online)
Fake News Classification: A Quantitative Research Description,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124315738&doi=10.1145%2f3447650&partnerID=40&md5=79492727a427e2ef8738ed5fb71cba31,"Social media can render content circulating to reach millions with a knack to influence people, despite the questionable authencity of the facts. Internet sources are the most convenient and easy approach to obtain any information these days. Fake news has become the topic of interest for academicians and the rest of society. This kind of propaganda has the power to influence the general perception, offering political groups the ability to control the results of democratic affairs such as elections. Automatic identification of fake news has emerged as one of the significant problems due to the high risks involved. It is challenging in a way because of the complexity levels of accurately interpreting the data. An extensive search has already been performed on English language news data. Our work presents a comparative analysis of fake news classifiers on the low resource Bengali language ĝ€ban fake news' dataset from Kaggle. The analysis presented compares deep learning techniques such as LSTM (Long short-Term Memory) and BiLSTM (Bi-directional Long short-Term Memory) and machine learning methods like Naive Bayes, Passive Aggressive Classifier (PAC), and Random Forest. The comparison has been drawn based on classification metrics such as accuracy, precision, recall, and F1 score. The deep learning method BiLSTM shows 55.92% accuracy while Random Forest, in contrast, has outperformed all the other methods with an accuracy of 62.37%. The work presented in this paper sets a basis for researchers to select the optimum classifiers for their approach towards fake news detection.  © 2021 Association for Computing Machinery.",BiLSTM; Deep learning; LSTM; Machine learning; PAC; tokenizer,Automation; Brain; Classification (of information); Decision trees; Fake detection; Authencity; Bi-directional; Bi-directional long short-term memory; Deep learning; Internet sources; Passive aggressive classifier; Quantitative research; Random forests; Social media; Tokenizer; Long short-term memory
Event Graph Neural Network for Opinion Target Classification of Microblog Comments,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124276202&doi=10.1145%2f3469725&partnerID=40&md5=02d21d6979a965f4f16c05666f6cb4ed,"Opinion target classification of microblog comments is one of the most important tasks for public opinion analysis about an event. Due to the high cost of manual labeling, opinion target classification is generally considered as a weak-supervised task. This article attempts to address the opinion target classification of microblog comments through an event graph convolution network (EventGCN) in a weak-supervised manner. Specifically, we take microblog contents and comments as document nodes, and construct an event graph with three typical relationships of event microblogs, including the co-occurrence relationship of event keywords extracted from microblogs, the reply relationship of comments, and the document similarity. Finally, under the supervision of a small number of labels, both word features and comment features can be represented well to complete the classification. The experimental results on two event microblog datasets show that EventGCN can significantly improve the classification performance compared with other baseline models.  © 2021 Association for Computing Machinery.",graph neural network; Opinion target; social media analysis; text classification; weak-supervised classification,Classification (of information); Social aspects; Social networking (online); Text processing; Event graphs; Graph neural networks; Micro-blog; Opinion analysis; Opinion targets; Public opinions; Social media analysis; Supervised classification; Target Classification; Weak-supervised classification; Graph neural networks
Roman-Urdu-Parl: Roman-Urdu and Urdu Parallel Corpus for Urdu Language Understanding,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124288779&doi=10.1145%2f3464424&partnerID=40&md5=eb57845e50812d47742db5a87bd4be6e,"Availability of corpora is a basic requirement for conducting research in a particular language. Unfortunately, for a morphologically rich language like Urdu, despite being used by over a 100 million people around the globe, the dearth of corpora is a major reason for the lack of attention and advancement in research. To this end, we present the first-ever large-scale publicly available Roman-Urdu parallel corpus, Roman-Urdu-Parl, with 6.37 million sentence-pairs. It is a huge corpus collected from diverse sources, annotated using crowd-sourcing techniques, and also assured for quality. It has a total of 92.76 million Roman-Urdu words, 92.85 million Urdu words, Roman-Urdu vocabulary of 42.9 K words, and Urdu vocabulary of 43.8 K words. Roman-Urdu-Parl has been built to ensure that it not only captures the morphological and linguistic features of the language but also the heterogeneity and variations arising due to demographic conditions. We validate the authenticity and quality of our corpus by using it to address two natural language processing research problems, that is, on learning word embeddings and building a machine transliteration system. Our contribution of the corpus leads to exceptional results in both settings, for example, our machine transliteration system sets a new state-of-The-Art with a Bilingual Evaluation Understudy (BLEU) score of 84.67. We believe that Roman-Urdu-Parl can serve as fuel for igniting and advancing works in many research areas related to the Urdu language.  © 2022 Association for Computing Machinery.",deep learning; machine transliteration; neural machine translation; Roman-Urdu to Urdu transliteration,Deep learning; Linguistics; Crowd sourcing; Deep learning; Language understanding; Large-scales; Linguistic features; Machine transliteration; Morphological features; Parallel corpora; Roman-urdu to urdu transliteration; Transliteration system; Natural language processing systems
Enriching Conventional Ensemble Learner with Deep Contextual Semantics to Detect Fake News in Urdu,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124261357&doi=10.1145%2f3461614&partnerID=40&md5=53595bb9d6d8fd7ac78322dfa0e2dffb,"Increased connectivity has contributed greatly in facilitating rapid access to information and reliable communication. However, the uncontrolled information dissemination has also resulted in the spread of fake news. Fake news might be spread by a group of people or organizations to serve ulterior motives such as political or financial gains or to damage a country's public image. Given the importance of timely detection of fake news, the research area has intrigued researchers from all over the world. Most of the work for detecting fake news focuses on the English language. However, automated detection of fake news is important irrespective of the language used for spreading false information. Recognizing the importance of boosting research on fake news detection for low resource languages, this work proposes a novel semantically enriched technique to effectively detect fake news in Urdu-a low resource language. A model based on deep contextual semantics learned from the convolutional neural network is proposed. The features learned from the convolutional neural network are combined with other n-gram-based features and are fed to a conventional majority voting ensemble classifier fitted with three base learners: Adaptive Boosting, Gradient Boosting, and Multi-Layer Perceptron. Experiments are performed with different models, and results show that enriching the traditional ensemble learner with deep contextual semantics along with other standard features shows the best results and outperforms the state-of-The-Art Urdu fake news detection model.  © 2021 Association for Computing Machinery.",convolutional neural network; Deep contextual semantics; ensemble learning; majority voting; word embeddings,Convolutional neural networks; Deep neural networks; Information dissemination; Multilayer neural networks; Semantic Web; Semantics; Contextual semantics; Convolutional neural network; Deep contextual semantic; Embeddings; Ensemble learning; Information communication; Low resource languages; Majority voting; Reliable communication; Word embedding; Convolution
Multilingual Offensive Language Identification for Low-resource Languages,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124261725&doi=10.1145%2f3457610&partnerID=40&md5=7cf2c1d828c3b14c5458b9563f0bf82f,"Offensive content is pervasive in social media and a reason for concern to companies and government organizations. Several studies have been recently published investigating methods to detect the various forms of such content (e.g., hate speech, cyberbullying, and cyberaggression). The clear majority of these studies deal with English partially because most annotated datasets available contain English data. In this article, we take advantage of available English datasets by applying cross-lingual contextual word embeddings and transfer learning to make predictions in low-resource languages. We project predictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi, Spanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in TRAC-2 shared task [23], 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in OffensEval 2020 [58], 0.8568 F1 macro for Hindi in HASOC 2019 shared task [27], and 0.7513 F1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) [7], showing that our approach compares favorably to the best systems submitted to recent shared tasks on these three languages. Additionally, we report competitive performance on Arabic and Turkish using the training and development sets of OffensEval 2020 shared task. The results for all languages confirm the robustness of cross-lingual contextual embeddings and transfer learning for this task.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cross-lingual embeddings; low-resource languages; Offensive language identification,Computer crime; Natural language processing systems; Bengalis; Cross-lingual; Cross-lingual embedding; Embeddings; Language identification; Low resource languages; Offensive language identification; Offensive languages; Transfer learning; Turkishs; Embeddings
A Transformer-Based Approach to Multilingual Fake News Detection in Low-Resource Languages,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124270042&doi=10.1145%2f3472619&partnerID=40&md5=462df9e070c013757dc55cfa6cd68c46,"Fake news classification is one of the most interesting problems that has attracted huge attention to the researchers of artificial intelligence, natural language processing, and machine learning (ML). Most of the current works on fake news detection are in the English language, and hence this has limited its widespread usability, especially outside the English literate population. Although there has been a growth in multilingual web content, fake news classification in low-resource languages is still a challenge due to the non-Availability of an annotated corpus and tools. This article proposes an effective neural model based on the multilingual Bidirectional Encoder Representations from Transformer (BERT) for domain-Agnostic multilingual fake news classification. Large varieties of experiments, including language-specific and domain-specific settings, are conducted. The proposed model achieves high accuracy in domain-specific and domain-Agnostic experiments, and it also outperforms the current state-of-The-Art models. We perform experiments on zero-shot settings to assess the effectiveness of language-Agnostic feature transfer across different languages, showing encouraging results. Cross-domain transfer experiments are also performed to assess language-independent feature transfer of the model. We also offer a multilingual multidomain fake news detection dataset of five languages and seven different domains that could be useful for the research and development in resource-scarce scenarios.  © 2021 Association for Computing Machinery.",Fake news detection; Hindi; Indonesian; low-resource languages; multilingual; Swahili; Vietnamese,Artificial intelligence; Fake detection; Learning algorithms; 'current; Domain agnostics; Domain specific; Fake news detection; Feature transfers; Hindi; Indonesian; Low resource languages; Swahilus; Vietnamese; Natural language processing systems
Synonymy Expansion Using Link Prediction Methods: A Case Study of Assamese WordNet,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124304497&doi=10.1145%2f3467966&partnerID=40&md5=0d9614229ef3d1ef2541688fdccab4ad,"WordNets built for low-resource languages, such as Assamese, often use the expansion methodology. This may result in missing lexical entries and missing synonymy relations. As the Assamese WordNet is also built using the expansion method, using the Hindi WordNet, it also has missing synonymy relations. As WordNets can be visualized as a network of unique words connected by synonymy relations, link prediction in complex network analysis is an effective way of predicting missing relations in a network. Hence, to predict the missing synonyms in the Assamese WordNet, link prediction methods were used in the current work that proved effective. It is also observed that for discovering missing relations in the Assamese WordNet, simple local proximity-based methods might be more effective as compared to global and complex supervised models using network embedding. Further, it is noticed that though a set of retrieved words are not synonyms per se, they are semantically related to the target word and may be categorized as semantic cohorts.  © 2021 Association for Computing Machinery.",assamese; Automatic extraction; Indian languages; low resource languages; neural networks; semantic cohort; social network analysis; synonymy; synonymy network,Complex networks; Forecasting; Ontology; Semantic Web; Social networking (online); Assamese; Automatic extraction; Indian languages; Low resource languages; Neural-networks; Semantic cohort; Social Network Analysis; Synonymy; Synonymy network; Wordnet; Semantics
A Deep Content-Based Model for Persian Rumor Verification,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124280117&doi=10.1145%2f3487289&partnerID=40&md5=1764476f08ce519e085d878e56dde2d0,"During the development of social media, there has been a transformation in social communication. Despite their positive applications in social interactions and news spread, it also provides an ideal platform for spreading rumors. Rumors can endanger the security of society in normal or critical situations. Therefore, it is important to detect and verify the rumors in the early stage of their spreading. Many research works have focused on social attributes in the social network to solve the problem of rumor detection and verification, while less attention has been paid to content features. The social and structural features of rumors develop over time and are not available in the early stage of rumor. Therefore, this study presented a content-based model to verify the Persian rumors on Twitter and Telegram early. The proposed model demonstrates the important role of content in spreading rumors and generates a better-integrated representation for each source rumor document by fusing its semantic, pragmatic, and syntactic information. First, contextual word embeddings of the source rumor are generated by a hybrid model based on ParsBERT and parallel CapsNets. Then, pragmatic and syntactic features of the rumor are extracted and concatenated with embeddings to capture the rich information for rumor verification. Experimental results on real-world datasets demonstrated that the proposed model significantly outperforms the state-of-The-Art models in the early rumor verification task. Also, it can enhance the performance of the classifier from 2% to 11% on Twitter and from 5% to 23% on Telegram. These results validate the model's effectiveness when limited content information is available.  © 2021 Association for Computing Machinery.",contextual features; neural language model; ParsBERT; Persian rumor classification; Rumor verification; speech act,Embeddings; Semantics; Syntactics; Content-based; Contextual feature; Embeddings; Language model; Neural language model; ParsBERT; Persian rumor classification; Persians; Rumor verification; Speech acts; Social networking (online)
Detecting Arabic Spam Reviews in Social Networks Based on Classification Algorithms,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124295352&doi=10.1145%2f3476115&partnerID=40&md5=1b99728e40a15f138454745ea49eb68e,"Reviews or comments that users leave on social media have great importance for companies and business entities. New product ideas can be evaluated based on customer reactions. However, this use of social media is complicated by those who post spam on social media in the form of reviews and comments.Designing methodologies to automatically detect and block social media spam is complicated by the fact that spammers continuously develop new ways to leave their spam comments. Researchers have proposed several methods to detect English spam reviews. However, few studies have been conducted to detect Arabic spam reviews. This article proposes a keyword-based method for detecting Arabic spam reviews. Keywords or Features are subsets of words from the original text that are labelled as important. A term's weight, Term Frequency-Inverse Document Frequency (TF-IDF) matrix, and filter methods (such as information gain, chi-squared, deviation, correlation, and uncertainty) have been used to extract keywords from Arabic text.The method proposed in this article detects Arabic spam in Facebook comments. The dataset consists of 3,000 Arabic comments extracted from Facebook pages. Four different machine learning algorithms are used in the detection process, including C4.5, kNN, SVM, and Naïve Bayes classifiers. The results show that the Decision Tree classifier outperforms the other classification algorithms, with a detection accuracy of 92.63%.  © 2021 Association for Computing Machinery.",Arabic language; classification algorithms; Facebook; social networks; spam detection,Decision trees; Inverse problems; Learning algorithms; Support vector machines; Text processing; Arabic languages; Business entities; Classification algorithm; Facebook; Network-based; Product ideas; Social media; Social network; Spam detection; Spammers; Social networking (online)
Confidence Indexing of Automated Detected Synsets: A Case Study on Contemporary Turkish Dictionary,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124264817&doi=10.1145%2f3469724&partnerID=40&md5=021759ec195df2eb06ae6bfc90fb98dd,"In this study, a novel confidence indexing algorithm is proposed to minimize human labor in controlling the reliability of automatically extracted synsets from a non-machine-readable monolingual dictionary. Contemporary Turkish Dictionary of Turkish Language Association is used as the monolingual dictionary data. First, the synonym relations are extracted by traditional text processing methods from dictionary definitions and a graph is prepared in Lemma-Sense network architecture. After each synonym relation is labeled by a proper confidence index, synonym pairs with desired confidence indexes are analyzed to detect synsets with a spanning tree-based method. This approach can label synsets with one of three cumulative confidence levels (CL-1, CL-2, and CL-3). According to the confidence levels, synsets are compared with KeNet which is the only open access Turkish Wordnet. Consequently, while most matches with the synsets of KeNet is determined in CL-1 and CL-2 confidence levels, the synsets determined at CL-3 level reveal errors in the dictionary definitions. This novel approach does not find only the reliability of automatically detected synsets, but it can also point out errors of detected synsets from the dictionary.  © 2021 Association for Computing Machinery.",confidence indexing; Machine-readable dictionary; spanning tree-based synset detection; synset confidence levels; WordNet,Indexing (of information); Network architecture; Semantics; Text processing; Confidence indexing; Confidence levels; Machine-readable dictionaries; Spanning tree; Spanning tree-based synset detection; Synset confidence level; Synsets; Tree-based; Wordnet; Ontology
A Multimodal Deep Framework for Derogatory Social Media Post Identification of a Recognized Person,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124300510&doi=10.1145%2f3447651&partnerID=40&md5=efc5f8e7590c033288832b6d09059eb2,"In today's era of digitization, social media platforms play a significant role in networking and influencing the perception of the general population. Social network sites have recently been used to carry out harmful attacks against individuals, including political and theological figures, intellectuals, sports and movie stars, and other prominent dignitaries, which may or may not be intentional. However, the exchange of such information across the general population inevitably contributes to social-economic, socio-political turmoil, and even physical violence in society. By classifying the derogatory content of a social media post, this research work helps to eradicate and discourage the upsetting propagation of such hate campaigns. Social networking posts today often include the picture of Memes along with textual remarks and comments, which throw new challenges and opportunities to the research community while identifying the attacks. This article proposes a multimodal deep learning framework by utilizing ensembles of computer vision and natural language processing techniques to train an encapsulated transformer network for handling the classification problem. The proposed framework utilizes the fine-Tuned state-of-The-Art deep learning-based models (e.g., BERT, Electra) for multilingual text analysis along with face recognition and the optical character recognition model for Meme picture comprehension. For the study, a new Facebook meme-post dataset is created with recorded baseline results. The subject of the created dataset and context of the work is more geared toward multilingual Indian society. The findings demonstrate the efficacy of the proposed method in the identification of social media meme posts featuring derogatory content about a famous/recognized individual.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; derogatory content; Indic languages; NLP; Social media analysis and security; transformer network,Deep learning; Face recognition; Natural language processing systems; Optical character recognition; Deep learning; Derogatory content; General population; Indic language; Media security; Multi-modal; Social media; Social media analysis; Social medium analyse and security; Transformer network; Social networking (online)
Low Resource Neural Machine Translation: Assamese to/from Other Indo-Aryan (Indic) Languages,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124312037&doi=10.1145%2f3469721&partnerID=40&md5=4e1f5eac157bf425f06550f445b7cee9,"Machine translation (MT) systems have been built using numerous different techniques for bridging the language barriers. These techniques are broadly categorized into approaches like Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). End-To-end NMT systems significantly outperform SMT in translation quality on many language pairs, especially those with the adequate parallel corpus. We report comparative experiments on baseline MT systems for Assamese to other Indo-Aryan languages (in both translation directions) using the traditional Phrase-Based SMT as well as some more successful NMT architectures, namely basic sequence-To-sequence model with attention, Transformer, and finetuned Transformer. The results are evaluated using the most prominent and popular standard automatic metric BLEU (BiLingual Evaluation Understudy), as well as other well-known metrics for exploring the performance of different baseline MT systems, since this is the first such work involving Assamese. The evaluation scores are compared for SMT and NMT models for the effectiveness of bi-directional language pairs involving Assamese and other Indo-Aryan languages (Bangla, Gujarati, Hindi, Marathi, Odia, Sinhalese, and Urdu). The highest BLEU scores obtained are for Assamese to Sinhalese for SMT (35.63) and the Assamese to Bangla for NMT systems (seq2seq is 50.92, Transformer is 50.01, and finetuned Transformer is 50.19). We also try to relate the results with the language characteristics, distances, family trees, domains, data sizes, and sentence lengths. We find that the effect of the domain is the most important factor affecting the results for the given data domains and sizes. We compare our results with the only existing MT system for Assamese (Bing Translator) and also with pairs involving Hindi.  © 2021 Association for Computing Machinery.",Assamese; finetuned Transformer; Indo-Aryan; low resource; Machine translation; NMT; sequence-To-sequence; SMT; Transformer,Computational linguistics; Neural machine translation; Petroleum reservoir evaluation; Speech transmission; Trees (mathematics); Assamese; Bilinguals; Finetuned transformer; Indo-aryan; Language pairs; Low resource; Machine translation systems; Sequence-to-sequence; Statistical machine translation; Transformer; Computer aided language translation
Sentiment Analysis in Hindi A Survey on the State-of-The-Art Techniques,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124293074&doi=10.1145%2f3469722&partnerID=40&md5=83acda65b3bbff78e0710fbf58ddf9c6,"Sentiment Analysis (SA) has been a core interest in the field of text mining research, dealing with computational processing of sentiments, views, and subjective nature of the text. Due to the availability of extensive web-based data in Indian languages such as Hindi, Marathi, Kannada, Tamil, and so on. It has become extremely significant to analyze this data and recover valuable and relevant information. Hindi being the first language of the majority of the population in India, SA in Hindi has turned out to be a critical task particularly for companies and government organizations. This research portrays a systematic review specifically in the field of Hindi SA. The major contribution of this article includes the categorization of numerous articles based on techniques that have attracted researchers in performing SA tasks in Hindi language. This survey classifies these state-of-The-Art computational intelligence techniques into four major categories namely lexicon-based techniques, machine learning techniques, deep learning techniques, and hybrid techniques. It discusses the importance of these techniques based on different aspects such as their impact on the issues of SA, levels of analysis, and performance evaluation measures. The research puts forward a comprehensive overview of the majority of the work done in Hindi SA. This study will help researchers in finding out resources such as annotated datasets, linguistic resources, and lexical resources. This survey delivers some significant findings and presents overall future research directions in the field of Hindi SA.  © 2021 Association for Computing Machinery.",hindi language; lexicon technique; opinion mining; Sentiment analysis; systematic review,Data mining; Deep learning; Learning algorithms; Surveys; Computational processing; Critical tasks; Hindi language; Indian languages; Lexicon technique; Sentiment analysis; State-of-the-art techniques; Systematic Review; Text-mining; Web based; Sentiment analysis
Deep Level Analysis of Legitimacy in Bengali News Sentences,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124251115&doi=10.1145%2f3459928&partnerID=40&md5=3e79ee0431d152efd7f526acfc38337a,"The tremendous increase in the growth of misinformation in news articles has the potential threat for the adverse effects on society. Hence, the detection of misinformation in news data has become an appealing research area. The task of annotating and detecting distorted news article sentences is the immediate need in this research direction. Therefore, an attempt has been made to formulate the legitimacy annotation guideline followed by annotation and detection of the legitimacy in Bengali e-papers. The sentence-level manual annotation of Bengali news has been carried out in two levels, namely ""Level-1 Shallow Level Classification""and ""Level-2 Deep Level Classification""based on semantic properties of Bengali sentences. The tagging of 1,300 anonymous Bengali e-paper sentences has been done using the formulated guideline-based tags for both levels. The validation of the annotation guideline has been done by applying benchmark supervised machine learning algorithms using the lexical feature, syntactic feature, domain-specific feature, and Level-2 specific feature in both levels. Performance evaluation of these classifiers is done in terms of Accuracy, Precision, Recall, and F-Measure. In both levels, Support Vector Machine outperforms other benchmark classifiers with an accuracy of 72% and 65% in Level-1 and Level-2, respectively.  © 2021 Association for Computing Machinery.",annotation guideline; Bengali news; legitimacy; multi-level classification; semantics,Learning algorithms; Support vector machines; Annotation guideline; Bengali news; Bengalis; Deep-levels; E-papers; Legitimacy; Level 2; Level-1; Multi-level classifications; News articles; Semantics
Persian Fake News Detection: Neural Representation and Classification at Word and Text Levels,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124299339&doi=10.1145%2f3472620&partnerID=40&md5=4348be1e0288caa9597510f372c4204e,"Nowadays, broadcasting news on social media and websites has grown at a swifter pace, which has had negative impacts on both the general public and governments; hence, this has urged us to build a fake news detection system. Contextualized word embeddings have achieved great success in recent years due to their power to embed both syntactic and semantic features of textual contents. In this article, we aim to address the problem of the lack of fake news datasets in Persian by introducing a new dataset crawled from different news agencies, and propose two deep models based on the Bidirectional Encoder Representations from Transformers model (BERT), which is a deep contextualized pre-Trained model for extracting valuable features. In our proposed models, we benefit from two different settings of BERT, namely pool-based representation, which provides a representation for the whole document, and sequence representation, which provides a representation for each token of the document. In the former one, we connect a Single Layer Perceptron (SLP) to the BERT to use the embedding directly for detecting fake news. The latter one uses Convolutional Neural Network (CNN) after the BERT's embedding layer to extract extra features based on the collocation of words in a corpus. Furthermore, we present the TAJ dataset, which is a new Persian fake news dataset crawled from news agencies' websites. We evaluate our proposed models on the newly provided TAJ dataset as well as the two different Persian rumor datasets as baselines. The results indicate the effectiveness of using deep contextualized embedding approaches for the fake news detection task. We also show that both BERT-SLP and BERT-CNN models achieve superior performance to the previous baselines and traditional machine learning models, with 15.58% and 17.1% improvement compared to the reported results by Zamani et al. [30], and 11.29% and 11.18% improvement compared to the reported results by Jahanbakhsh-Nagadeh et al. [9].  © 2021 Association for Computing Machinery.",contextualized text representation; deep neural networks; fake news detection; Misinformation; Persian,Convolutional neural networks; Embeddings; Fake detection; Multilayer neural networks; Semantics; Text processing; Websites; Contextualized text representation; Convolutional neural network; Embeddings; Fake news detection; Misinformation; News agencies; Persians; Single-layer perceptrons; Text representation; Transformer modeling; Deep neural networks
Neural Arabic Text Diacritization: State-of-The-Art Results and a Novel Approach for Arabic NLP Downstream Tasks,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124320691&doi=10.1145%2f3470849&partnerID=40&md5=5becc090f78b28649119c408446f571b,"In this work, we present several deep learning models for the automatic diacritization of Arabic text. Our models are built using two main approaches, viz. Feed-Forward Neural Network (FFNN) and Recurrent Neural Network (RNN), with several enhancements such as 100-hot encoding, embeddings, Conditional Random Field (CRF), and Block-Normalized Gradient (BNG). The models are tested on the only freely available benchmark dataset and the results show that our models are either better or on par with other models even those requiring human-crafted language-dependent post-processing steps, unlike ours. Moreover, we show how diacritics in Arabic can be used to enhance the models of downstream NLP tasks such as Machine Translation (MT) and Sentiment Analysis (SA) by proposing novel Translation over Diacritization (ToD) and Sentiment over Diacritization (SoD) approaches.  © 2021 Association for Computing Machinery.",Arabic; datasets; diacritization; sentiment analysis; translation,Computational linguistics; Feedforward neural networks; Neural machine translation; Random processes; Recurrent neural networks; Arabic; Arabic texts; Dataset; Diacritization; Down-stream; Feed forward neural net works; Learning models; Sentiment analysis; State of the art; Translation; Sentiment analysis
Blockchain-based Framework for Reducing Fake or Vicious News Spread on Social Media/Messaging Platforms,2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124128391&doi=10.1145%2f3467019&partnerID=40&md5=1fbdd8da4bfedea29637dca314f8f9ec,"With social media becoming the most frequently used mode of modern-day communications, the propagation of fake or vicious news through such modes of communication has emerged as a serious problem. The scope of the problem of fake or vicious news may range from rumour-mongering, with intent to defame someone, to manufacturing false opinions/trends impacting elections and stock exchanges to much more alarming and mala fide repercussions of inciting violence by bad actors, especially in sensitive law-And-order situations. Therefore, curbing fake or vicious news and identifying the source of such news to ensure strict accountability is the need of the hour. Researchers have been working in the area of using text analysis, labelling, artificial intelligence, and machine learning techniques for detecting fake news, but identifying the source or originator of such news for accountability is still a big challenge for which no concrete approach exists as of today. Also, there is another common problematic trend on social media whereby targeted vicious content goes viral to mobilize or instigate people with malicious intent to destabilize normalcy in society. In the proposed solution, we treat both problems of fake news and vicious news together. We propose a blockchain and keyed watermarking-based framework for social media/messaging platforms that will allow the integrity of the posted content as well as ensure accountability on the owner/user of the post. Intrinsic properties of blockchain-like transparency and immutability are advantageous for curbing fake or vicious news. After identification of fake or vicious news, its spread will be immediately curbed through backtracking as well as forward tracking. Also, observing transactions on the blockchain, the density and rate of forwarding of a particular original message going beyond a threshold can easily be checked, which could be identified as a possible malicious attempt to spread objectionable content. If the content is deemed dangerous or inappropriate, its spread will be curbed immediately. The use of the Raft consensus algorithm and bloXroute servers is proposed to enhance throughput and network scalability, respectively. Thus, the framework offers a proactive as well as reactive, practically feasible, and effective solution for curtailment of fake or vicious news on social media/messaging platforms. The proposed work is a framework for solving fake or vicious news spread problems on social media; the complete design specifications are beyond scope of the current work and will be addressed in the future.  © 2021 Association for Computing Machinery.",digital keyed-watermarking; distributed network; fake or vicious news; overlay network; Scalable blockchain,Artificial intelligence; Blockchain; Fake detection; Learning systems; Overlay networks; Block-chain; Digital keyed-watermarking; Distributed networks; Fake or vicious news; Labelings; Machine learning techniques; Rumor-mongering; Scalable blockchain; Social media; Stock exchange; Social networking (online)
"Introduction to Special Issue on Misinformation, Fake News and Rumor Detection in Low-Resource Languages",2022,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124271505&doi=10.1145%2f3505588&partnerID=40&md5=3f51a26b98728ac082e60c725891906e,[No abstract available],,
Neural Unsupervised Semantic Role Labeling,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135098215&doi=10.1145%2f3461613&partnerID=40&md5=b8a1d9372929f99d9f506b3a73226d69,"The task of semantic role labeling (SRL) is dedicated to finding the predicate-Argument structure. Previous works on SRL are mostly supervised and do not consider the difficulty in labeling each example which can be very expensive and time-consuming. In this article, we present the first neural unsupervised model for SRL. To decompose the task as two argument related subtasks, identification and clustering, we propose a pipeline that correspondingly consists of two neural modules. First, we train a neural model on two syntax-Aware statistically developed rules. The neural model gets the relevance signal for each token in a sentence, to feed into a BiLSTM, and then an adversarial layer for noise-Adding and classifying simultaneously, thus enabling the model to learn the semantic structure of a sentence. Then we propose another neural model for argument role clustering, which is done through clustering the learned argument embeddings biased toward their dependency relations. Experiments on the CoNLL-2009 English dataset demonstrate that our model outperforms the previous state-of-The-Art baseline in terms of non-neural models for argument identification and classification.  © 2021 Association for Computing Machinery.",argument classification; argument identification; CoNLL-2009; semantic parsing; syntax; Unsupervised semantic role labeling,Semantics; Syntactics; Argument classification; Argument identifications; Argument structures; Clusterings; CoNLL-2009; Neural modelling; Semantic parsing; Semantic role labeling; Syntax; Unsupervised semantic role labeling; Classification (of information)
A Multi-Classification Sentiment Analysis Model of Chinese Short Text Based on Gated Linear Units and Attention Mechanism,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124885537&doi=10.1145%2f3464425&partnerID=40&md5=51d2d7d1d6e8fe223cca8cbb206e665f,"Sentiment analysis of social media texts has become a research hotspot in information processing. Sentiment analysis methods based on the combination of machine learning and sentiment lexicon need to select features. Selected emotional features are often subjective, which can easily lead to overfitted models and poor generalization ability. Sentiment analysis models based on deep learning can automatically extract effective text emotional features, which will greatly improve the accuracy of text sentiment analysis. However, due to the lack of a multi-classification emotional corpus, it cannot accurately express the emotional polarity. Therefore, we propose a multi-classification sentiment analysis model, GLU-RCNN, based on Gated Linear Units and attention mechanism. Our model uses the Gated Linear Units based attention mechanism to integrate the local features extracted by CNN with the semantic features extracted by the LSTM. The local features of short text are extracted and concatenated by using multi-size convolution kernels. At the classification layer, the emotional features extracted by CNN and LSTM are respectively concatenated to express the emotional features of the text. The detailed evaluation on two benchmark datasets shows that the proposed model outperforms state-of-The-Art approaches.  © 2021 Association for Computing Machinery.",attention mechanism; CNN; LSTM; Multi-classification sentiment analysis,Learning systems; Long short-term memory; Semantics; Analysis models; Attention mechanisms; Chinese short-text; Linear units; Local feature; LSTM; Multi-classification; Multi-classification sentiment analyse; Sentiment analysis; Social media; Sentiment analysis
BYANJON: A Ground Truth Preparation System for Online Handwritten Bangla Documents,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129309084&doi=10.1145%2f3464379&partnerID=40&md5=1c06b39ef38817a007f4d8a351421187,"The work reported in this article deals with the ground truth generation scheme for online handwritten Bangla documents at text-line, word, and stroke levels. The aim of the proposed scheme is twofold: firstly, to build a document level database so that future researchers can use the database to do research in this field. Secondly, the ground truth information will help other researchers to evaluate the performance of their algorithms developed for text-line extraction, word extraction, word segmentation, stroke recognition, and word recognition. The reported ground truth generation scheme starts with text-line extraction from the online handwritten Bangla documents, then words extraction from the text-lines, and finally segmentation of those words into basic strokes. After word segmentation, the basic strokes are assigned appropriate class labels by using modified distance-based feature extraction procedure and the MLP (Multi-layer Perceptron) classifier. The Unicode for the words are then generated from the sequence of stroke labels. XML files are used to store the stroke, word, and text-line levels ground truth information for the corresponding documents. The proposed system is semi-Automatic and each step such as text-line extraction, word extraction, word segmentation, and stroke recognition has been implemented by using different algorithms. Thus, the proposed ground truth generation procedure minimizes huge manual intervention by reducing the number of mouse clicks required to extract text-lines, words from the document, and segment the words into basic strokes. The integrated stroke recognition module also helps to minimize the manual labor needed to assign appropriate stroke labels. The freely available and can be accessed at https://byanjon.herokuapp.com/.  © 2021 Association for Computing Machinery.",Bangla script; ground truth preparation; Online handwriting recognition; stroke extraction; text-line extraction; word segmentation,Character recognition; Computational linguistics; Mammals; Vocabulary control; Bangla scripts; Ground truth; Ground truth preparations; Handwritten bangla; Online handwriting recognition; Stroke extraction; Stroke recognition; Text lines; Text-line extractions; Word segmentation; Extraction
A Novel Classification Model SA-MPCNN for Power Equipment Defect Text,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135074890&doi=10.1145%2f3464380&partnerID=40&md5=60a0f4ba9d2af2d443da91ac86b24020,"The text classification of power equipment defect is of great significance to equipment health condition evaluation and power equipment maintenance decisions. Most of the existing classification methods do not sufficiently consider the semantic relation between words in the same sentence and cannot extract deep semantic features. To tackle those problems, this article proposes a novel classification method by combining the self-Attention mechanism and multi-channel pyramid convolution neural networks. We utilize the bidirectional gated recurrent unit to model the text sequence and, on this basis, improve self-Attention layer to dot multiplication on the forward and backward features to obtain the global attention score. Thereby, effective features are enhanced, invalid features are weakened, and important text representation vectors are obtained. To solve the problem that the shallow network structure cannot extract deep semantic features, we design a multi-channel pyramid convolution network, which first extracts deep text features from the channels of different windows and then fuses the text features of each channel. By comparing with the state-of-The-Art methods, the model in this article has better performance in text classification of power equipment defects.  © 2021 Association for Computing Machinery.",bidirectional gated recurrent unit; defect texts; pyramid convolution; self-Attention mechanism; Text classification,Classification (of information); Defects; Semantics; Text processing; Attention mechanisms; Bidirectional gated recurrent unit; Classification methods; Defect text; Equipment defects; Power equipment; Pyramid convolution; Self-attention mechanism; Semantic features; Text classification; Convolution
"Linguistic Resources for Bhojpuri, Magahi, and Maithili: Statistics about Them, Their Similarity Estimates, and Baselines for Three Applications",2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118188191&doi=10.1145%2f3458250&partnerID=40&md5=2e2549235a6e7a497c070ae9e9404467,"Corpus preparation for low-resource languages and for development of human language technology to analyze or computationally process them is a laborious task, primarily due to the unavailability of expert linguists who are native speakers of these languages and also due to the time and resources required. Bhojpuri, Magahi, and Maithili, languages of the Purvanchal region of India (in the north-eastern parts), are low-resource languages belonging to the Indo-Aryan (or Indic) family. They are closely related to Hindi, which is a relatively high-resource language, which is why we compare them with Hindi. We collected corpora for these three languages from various sources and cleaned them to the extent possible, without changing the data in them. The text belongs to different domains and genres. We calculated some basic statistical measures for these corpora at character, word, syllable, and morpheme levels. These corpora were also annotated with parts-of-speech (POS) and chunk tags. The basic statistical measures were both absolute and relative and were expected to indicate linguistic properties, such as morphological, lexical, phonological, and syntactic complexities (or richness). The results were compared with a standard Hindi corpus. For most of the measures, we tried to match the corpus size across the languages to avoid the effect of corpus size, but in some cases it turned out that using the full corpus was better, even if sizes were very different. Although the results are not very clear, we tried to draw some conclusions about the languages and the corpora. For POS tagging and chunking, the BIS tagset was used to manually annotate the data. The POS-Tagged data sizes are 16,067, 14,669, and 12,310 sentences, respectively, for Bhojpuri, Magahi, and Maithili. The sizes for chunking are 9,695 and 1,954 sentences for Bhojpuri and Maithili, respectively. The inter-Annotator agreement for these annotations, using Cohen's Kappa, was 0.92, 0.64, and 0.74, respectively, for the three languages. These (annotated) corpora have been used for developing preliminary automated tools, which include POS tagger, Chunker, and Language Identifier. We have also developed the Bilingual dictionary (Purvanchal languages to Hindi) and a Synset (that can be integrated later in the Indo-WordNet) as additional resources. The main contribution of the work is the creation of basic resources for facilitating further language processing research for these languages, providing some quantitative measures about them and their similarities among themselves and with Hindi. For similarities, we use a somewhat novel measure of language similarity based on an n-gram-based language identification algorithm. An additional contribution is providing baselines for three basic NLP applications (POS tagging, chunking, and language identification) for these closely related languages.  © 2021 Association for Computing Machinery.",chunking; Corpus; inter-Annotator agreement; language identification; language similarity; low resource language; POS tagging; syntactic annotation,Computational linguistics; Syntactics; Chunking; Corpus; Inter-annotator agreement; Language identification; Language similarity; Low resource languages; Part of speech tagging; Parts-of-speech tagging; Statistical measures; Syntactic annotation; Natural language processing systems
Exploration of Effective Attention Strategies for Neural Automatic Post-editing with Transformer,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127392702&doi=10.1145%2f3465383&partnerID=40&md5=b097f8e44f9eeebeb2cd59544512b346,"Automatic post-editing (APE) is the study of correcting translation errors in the output of an unknown machine translation (MT) system and has been considered as a method of improving translation quality without any modification to conventional MT systems. Recently, several variants of Transformer that take both the MT output and its corresponding source sentence as inputs have been proposed for APE; and models introducing an additional attention layer into the encoder to jointly encode the MT output with its source sentence recorded a high-rank in the WMT19 APE shared task. We examine the effectiveness of such joint-encoding strategy in a controlled environment and compare four types of decoder multi-source attention strategies that have been introduced into previous APE models. The experimental results indicate that the joint-encoding strategy is effective and that taking the final encoded representation of the source sentence is the more proper strategy than taking such representation within the same encoder stack. Furthermore, among the multi-source attention strategies combined with the joint-encoding, the strategy that applies attention to the concatenated input representation and the strategy that adds up the individual attention to each input improve the quality of APE results over the strategy using the joint-encoding only.  © 2021 Association for Computing Machinery.",attention mechanism; Automatic post-editing; machine translation; neural networks,Computational linguistics; Computer aided language translation; Encoding (symbols); Neural machine translation; Attention mechanisms; Automatic post-editing; Encoding strategy; Joint encoding; Machine translation systems; Machine translations; Multi-Sources; Neural-networks; Post-editing; Translation quality; Signal encoding
Improving Data Augmentation for Low-Resource NMT Guided by POS-Tagging and Paraphrase Embedding,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135048887&doi=10.1145%2f3464427&partnerID=40&md5=6cda24596a324ebf5b775b8e045a988c,"Data augmentation is an approach for several text generation tasks. Generally, in the machine translation paradigm, mainly in low-resource language scenarios, many data augmentation methods have been proposed. The most used approaches for generating pseudo data mainly lay in word omission, random sampling, or replacing some words in the text. However, previous methods barely guarantee the quality of augmented data. In this work, we try to build the data by using paraphrase embedding and POS-Tagging. Namely, we generate the fake monolingual corpus by replacing the main four POS-Tagging labels, such as noun, adjective, adverb, and verb, based on both the paraphrase table and their similarity. We select the bigger corpus size of the paraphrase table with word level and obtain the word embedding of each word in the table, then calculate the cosine similarity between these words and tagged words in the original sequence. In addition, we exploit the ranking algorithm to choose highly similar words to reduce semantic errors and leverage the POS-Tagging replacement to mitigate syntactic error to some extent. Experimental results show that our augmentation method consistently outperforms all previous SOTA methods on the low-resource language pairs in seven language pairs from four corpora by 1.16 to 2.39 BLEU points.  © 2021 Association for Computing Machinery.",Artificial intelligence; data augmentation; low-resource languages; machine translation; natural language processing; neural network; paraphrase; POS-Tagging,Computational linguistics; Computer aided language translation; Natural language processing systems; Neural machine translation; Neural networks; Semantics; Syntactics; Data augmentation; Embeddings; Language processing; Low resource languages; Machine translations; Natural language processing; Natural languages; Neural-networks; Paraphrase; POS-tagging; Embeddings
Exploring Topic-language Preferences in Multilingual Swahili Information Retrieval in Tanzania,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135047152&doi=10.1145%2f3458671&partnerID=40&md5=6a0da15c045924451cd6abde2c135506,"Habitual switching of languages is a common behaviour among polyglots when searching for information on the Web. Studies in information retrieval (IR) and multilingual information retrieval (MLIR) suggest that part of the reason for such regular switching of languages is the topic of search. Unlike survey-based studies, this study uses query and click-Through logs. It exploits the querying and results selection behaviour of Swahili MLIR system users to explore how topic of search (query) is associated with language preferences-topic-language preferences. This article is based on a carefully controlled study using Swahili-speaking Web users in Tanzania who interacted with a guided multilingual search engine. From the statistical analysis of queries and click-Through logs, it was revealed that language preferences may be associated with the topics of search. The results also suggest that language preferences are not static; they vary along the course of Web search from query to results selection. In most of the topics, users either had significantly no language preference or preferred to query in Kiswahili and changed their preference to either English or no preference for language when selecting/clicking on the results. The findings of this study might provide researchers with more insights in developing better MLIR systems that support certain types of users and in certain scenarios.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",guided multilingual search; Language preferences; MLIR; Swahili; topic-language,Search engines; Websites; Click through; Guided multilingual search; Language preference; Multi-lingual search; Multilingual information retrieval; Multilingual information retrieval system; Searching for informations; Swahilus; Tanzania; Topic languages; Information retrieval
Deep Learning Approach for the Morphological Synthesis in Malayalam and Tamil at the Character Level,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134222076&doi=10.1145%2f3457976&partnerID=40&md5=32642d688497995356e16e0b381a6254,"Morphological synthesis is one of the main components of Machine Translation (MT) frameworks, especially when any one or both of the source and target languages are morphologically rich. Morphological synthesis is the process of combining two words or two morphemes according to the Sandhi rules of the morphologically rich language. Malayalam and Tamil are two languages in India which are morphologically abundant as well as agglutinative. Morphological synthesis of a word in these two languages is challenging basically because of the following reasons: (1) Abundance in morphology; (2) Complex Sandhi rules; (3) The possibilty in Malayalam to form words by combining words that belong to different syntactic categories (for example, noun and verb); and (4) The construction of a sentence by combining multiple words. We formulated the task of the morphological generation of nouns and verbs of Malayalam and Tamil as a character-To-character sequence tagging problem. In this article, we used deep learning architectures like Recurrent Neural Network (RNN), Long Short-Term Memory Networks (LSTM), Gated Recurrent Unit (GRU), and their stacked and bidirectional versions for the implementation of morphological synthesis at the character level. In addition to that, we investigated the performance of the combination of the aforementioned deep learning architectures and the Conditional Random Field (CRF) in the morphological synthesis of nouns and verbs in Malayalam and Tamil. We observed that the addition of CRF to the Bidirectional LSTM/GRU architecture achieved more than 99% accuracy in the morphological synthesis of Malayalam and Tamil nouns and verbs.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bidirectional RNN; conditional random field; gated recurrent unit; long short-Term memory networks; Morphological generation; recurrent neural networks; stacked RNN,Brain; Computational linguistics; Machine components; Machine translation; Memory architecture; Network architecture; Random processes; Syntactics; Bidirectional recurrent neural networks; Conditional random field; Gated recurrent unit; Long short-term memory network; Malayalams; Memory network; Morphological generation; Morphological synthesis; Random fields; Stacked recurrent neural network; Long short-term memory
Coherent Dialog Generation with Query Graph,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135097859&doi=10.1145%2f3462551&partnerID=40&md5=eece0c3ae2d3163097d45b9b6df639a1,"Learning to generate coherent and informative dialogs is an enduring challenge for open-domain conversation generation. Previous work leverage knowledge graph or documents to facilitate informative dialog generation, with little attention on dialog coherence. In this article, to enhance multi-Turn open-domain dialog coherence, we propose to leverage a new knowledge source, web search session data, to facilitate hierarchical knowledge sequence planning, which determines a sketch of a multi-Turn dialog. Specifically, we formulate knowledge sequence planning or dialog policy learning as a graph grounded Reinforcement Learning (RL) problem. To this end, we first build a two-level query graph with queries as utterance-level vertices and their topics (entities in queries) as topic-level vertices. We then present a two-level dialog policy model that plans a high-level topic sequence and a low-level query sequence over the query graph to guide a knowledge aware response generator. In particular, to foster forward-looking knowledge planning decisions for better dialog coherence, we devise a heterogeneous graph neural network to incorporate neighbouring vertex information, or possible future RL action information, into each vertex (as an RL action) representation. Experiment results on two benchmark dialog datasets demonstrate that our framework can outperform strong baselines in terms of dialog coherence, informativeness, and engagingness.  © 2021 Association for Computing Machinery.",heterogeneous graph neural network and reinforcement learning; hierarchical knowledge sequence planning; Open-domain dialog policy; query graph,Data mining; Graph neural networks; Graph theory; Knowledge graph; Knowledge management; Graph neural networks; Heterogeneous graph; Heterogeneous graph neural network and reinforcement learning; Hierarchical knowledge; Hierarchical knowledge sequence planning; Neural network learning; Open-domain dialog policy; Query graph; Reinforcement learnings; Sequence planning; Reinforcement learning
A BERT-Based Two-Stage Model for Chinese Chengyu Recommendation,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125206383&doi=10.1145%2f3453185&partnerID=40&md5=f2c15e0112dc66d07a10a77c69324cb6,"In Chinese, Chengyu are fixed phrases consisting of four characters. As a type of idioms, their meanings usually cannot be derived from their component characters. In this article, we study the task of recommending a Chengyu given a textual context. Observing some of the limitations with existing work, we propose a two-stage model, where during the first stage we re-Train a Chinese BERT model by masking out Chengyu from a large Chinese corpus with a wide coverage of Chengyu. During the second stage, we fine-Tune the re-Trained, Chengyu-oriented BERT on a specific Chengyu recommendation dataset. We evaluate this method on ChID and CCT datasets and find that it can achieve the state of the art on both datasets. Ablation studies show that both stages of training are critical for the performance gain.  © 2021 Association for Computing Machinery.",Chengyu recommendation; idiom understanding; Question answering,Chengyu recommendation; Chinese corpus; Idiom understanding; Performance Gain; Question Answering; State of the art; Textual contexts; Two stage model
Robust Cross-lingual Task-oriented Dialogue,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132019397&doi=10.1145%2f3457571&partnerID=40&md5=397ffe433e9c913015f527459991840c,"Cross-lingual dialogue systems are increasingly important in e-commerce and customer service due to the rapid progress of globalization. In real-world system deployment, machine translation (MT) services are often used before and after the dialogue system to bridge different languages. However, noises and errors introduced in the MT process will result in the dialogue system's low robustness, making the system's performance far from satisfactory. In this article, we propose a novel MT-oriented noise enhanced framework that exploits multi-granularity MT noises and injects such noises into the dialogue system to improve the dialogue system's robustness. Specifically, we first design a method to automatically construct multi-granularity MT-oriented noises and multi-granularity adversarial examples, which contain abundant noise knowledge oriented to MT. Then, we propose two strategies to incorporate the noise knowledge: (i) Utterance-level adversarial learning and (ii) Knowledge-level guided method. The former adopts adversarial learning to learn a perturbation-invariant encoder, guiding the dialogue system to learn noise-independent hidden representations. The latter explicitly incorporates the multi-granularity noises, which contain the noise tokens and their possible correct forms, into the training and inference process, thus improving the dialogue system's robustness. Experimental results on three dialogue models, two dialogue datasets, and two language pairs have shown that the proposed framework significantly improves the performance of the cross-lingual dialogue system.  © 2021 Association for Computing Machinery.",adversarial learning; Cross-lingual; dialogue system; knowledge; robustness,Speech processing; Adversarial learning; Cross-lingual; Dialogue systems; Knowledge; Machine translations; Multi-granularity; Noise knowledge; Robustness; System robustness; Task-oriented; Learning systems
SGATS: Semantic Graph-based Automatic Text Summarization from Hindi Text Documents,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134882989&doi=10.1145%2f3464381&partnerID=40&md5=60f000c1765466290889038f9ef00fb6,"Creating a coherent summary of the text is a challenging task in the field of Natural Language Processing (NLP). Various Automatic Text Summarization techniques have been developed for abstractive as well as extractive summarization. This study focuses on extractive summarization which is a process containing selected delineative paragraphs or sentences from the original text and combining these into smaller forms than the document(s) to generate a summary. The methods that have been used for extractive summarization are based on a graph-Theoretic approach, machine learning, Latent Semantic Analysis (LSA), neural networks, cluster, and fuzzy logic. In this paper, a semantic graph-based approach SGATS (Semantic Graph-based approach for Automatic Text Summarization) is proposed to generate an extractive summary. The proposed approach constructs a semantic graph of the original Hindi text document by establishing a semantic relationship between sentences of the document using Hindi Wordnet ontology as a background knowledge source. Once the semantic graph is constructed, fourteen different graph theoretical measures are applied to rank the document sentences depending on their semantic scores. The proposed approach is applied to two data sets of different domains of Tourism and Health. The performance of the proposed approach is compared with the state-of-The-Art TextRank algorithm and human-Annotated summary. The performance of the proposed system is evaluated using widely accepted ROUGE measures. The outcomes exhibit that our proposed system produces better results than TextRank for health domain corpus and comparable results for tourism corpus. Further, correlation coefficient methods are applied to find a correlation between eight different graphical measures and it is observed that most of the graphical measures are highly correlated.  © 2021 Association for Computing Machinery.",graphical measures; ROUGE correlation coefficient; Semantic network,Fuzzy neural networks; Graphic methods; Health; Learning algorithms; Natural language processing systems; Ontology; Semantic Web; Semantics; Text processing; Automatic text summarization; Correlation coefficient; Extractive summarizations; Graph-based; Graphical measure; Performance; ROUGE correlation coefficient; Semantic graphs; Semantics networks; Text document; Fuzzy logic
Construction of a Corpus of Rhetorical Devices in Slogans and Structural Analysis of Antitheses,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135084366&doi=10.1145%2f3465218&partnerID=40&md5=b03d37c60fde7ced7d6f21f77e7b59a0,"An advertising slogan is a sentence that expresses a product or a work of art in a straightforward manner and is used for advertising and publicity. Moving the consumer's mind and attracting their interest can significantly influence sales. Although rhetorical techniques in a slogan are known to improve the effectiveness of advertising, not much attention has been devoted to analyze or automatically generate sentences with the techniques. Therefore, we constructed a large corpus of slogans and revealed the linguistic characteristics of the basic statistics and rhetorical devices. Another point of focus was antitheses, of which the usage rates are relatively high and which have a specific sentence structure and lexical constraints. The generation of a slogan that contains an antithesis necessitates the structure of sentences, known as templates, to be extracted and also requires knowledge of word pairs with semantic contrast. Thus, the next step involved analysis of the structure to extract the sentence structure and lexical knowledge about the antithesis. Despite its simple architecture, the proposed method exceeds the prediction accuracy and efficiency of a comparable method. Lexical knowledge that is not available in existing dictionaries was also extracted.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Advertising slogan; antithesis; corpus construction; rhetorical device; structural analysis,Marketing; Semantics; Advertizing; Advertizing slogan; Antithesis; Consumers' minds; Corpus construction; Large corpora; Lexical knowledge; Rhetorical device; Sentence structures; Work of art; Structural analysis
A Unified Dialogue Management Strategy for Multi-intent Dialogue Conversations in Multiple Languages,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121762569&doi=10.1145%2f3461763&partnerID=40&md5=2486cc29dd0ef79585591a4011f7e0e7,"Building Virtual Agents capable of carrying out complex queries of the user involving multiple intents of a domain is quite a challenge, because it demands that the agent manages several subtasks simultaneously. This article presents a universal Deep Reinforcement Learning framework that can synthesize dialogue managers capable of working in a task-oriented dialogue system encompassing various intents pertaining to a domain. The conversation between agent and user is broken down into hierarchies, to segregate subtasks pertinent to different intents. The concept of Hierarchical Reinforcement Learning, particularly options, is used to learn policies in different hierarchies that operates in distinct time steps to fulfill the user query successfully. The dialogue manager comprises top-level intent meta-policy to select among subtasks or options and a low-level controller policy to pick primitive actions to communicate with the user to complete the subtask provided to it by the top-level policy in varying intents of a domain. The proposed dialogue management module has been trained in a way such that it can be reused for any language for which it has been developed with little to no supervision. The developed system has been demonstrated for ""Air Travel""and ""Restaurant""domain in English and Hindi languages. Empirical results determine the robustness and efficacy of the learned dialogue policy as it outperforms several baselines and a state-of-The-Art system.  © 2021 Association for Computing Machinery.",Dialogue management; hierarchical reinforcement learning; hierarchies; multi-intent; options,Deep learning; Learning systems; Managers; Speech processing; Dialogue management; Dialogue manager; Hierarchical reinforcement learning; Hierarchy; Management strategies; Multi-intent; Multiple languages; Option; Subtask; Virtual agent; Reinforcement learning
Normalization of Transliterated Mongolian Words Using Seq2Seq Model with Limited Data,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135058066&doi=10.1145%2f3464361&partnerID=40&md5=944e6746cbba93dedd3f82dcb7ca1f84,"The huge increase in social media use in recent years has resulted in new forms of social interaction, changing our daily lives. Due to increasing contact between people from different cultures as a result of globalization, there has also been an increase in the use of the Latin alphabet, and as a result a large amount of transliterated text is being used on social media. In this study, we propose a variety of character level sequence-To-sequence (seq2seq) models for normalizing noisy, transliterated text written in Latin script into Mongolian Cyrillic script, for scenarios in which there is a limited amount of training data available. We applied performance enhancement methods, which included various beam search strategies, N-gram-based context adoption, edit distance-based correction and dictionary-based checking, in novel ways to two basic seq2seq models. We experimentally evaluated these two basic models as well as fourteen enhanced seq2seq models, and compared their noisy text normalization performance with that of a transliteration model and a conventional statistical machine translation (SMT) model. The proposed seq2seq models improved the robustness of the basic seq2seq models for normalizing out-of-vocabulary (OOV) words, and most of our models achieved higher normalization performance than the conventional method. When using test data during our text normalization experiment, our proposed method which included checking each hypothesis during the inference period achieved the lowest word error rate (WER = 13.41%), which was 4.51% fewer errors than when using the conventional SMT method.  © 2021 Association for Computing Machinery.",character conversion; language model; neural network; noisy text; seq2seq model; Text normalization; transliterated text,Computational linguistics; Neural machine translation; Social networking (online); Speech transmission; Character conversion; Language model; Mongolians; Neural-networks; Noisy text; Normalisation; Seq2seq model; Social media; Text Normalisation; Transliterated text; Computer aided language translation
The Research on Rejoining of the Oracle Bone Rubbings Based on Curve Matching,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135018208&doi=10.1145%2f3460393&partnerID=40&md5=748c9e751482ed909b6fb02ce0b4d863,"The rejoining of oracle bone rubbings is a fundamental topic for oracle research. However, it is a tough task to reassemble severely broken oracle bone rubbings because of detail loss in manual labeling, the great time consumption of rejoining, and the low accuracy of results. To overcome the challenges, we introduce a novel CFDA&CAP algorithm that consists of the Curve Fitting Degree Analysis (CFDA) algorithm and the Correlation Analysis of Pearson (CAP) algorithm. First, the orthogonalization system is constructed to extract local features based on the curve features analysis. Second, the global feature descriptor is depicted by using coordinate points sequences. Third, we screen candidate curves based on the features as well as the CFDA algorithm, so the search range of the candidates is narrowed down. Finally, image recommendation libraries for target curves are generated by adopting the CAP algorithm, and the rank for each target matching curve generates simultaneously for result evaluation. With experiments, the proposed method shows a good effect in rejoining oracle bone rubbings automatically: (1) it improves the average accuracy rate of curve matching up to 84%, and (2) for a low-resource task, the accuracy of our method has 25% higher accuracy than that of other methods.  © 2021 Association for Computing Machinery.",correlation analysis; curve matching; low-resource; Oracle bone rubbings; rejoining,Correlation methods; Database systems; Analysis algorithms; Correlation analysis; Curve matching; Curves fittings; Fitting degree; Low-resource; Manual labeling; Oracle bone rubbing; Rejoining; Time consumption; Curve fitting
Towards Tokenization and Part-of-Speech Tagging for Khmer: Data and Discussion,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135090466&doi=10.1145%2f3464378&partnerID=40&md5=f88b911ab5839bb4825095b96eb0ce7f,"As a highly analytic language, Khmer has considerable ambiguities in tokenization and part-of-speech (POS) tagging processing. This topic is investigated in this study. Specifically, a 20,000-sentence Khmer corpus with manual tokenization and POS-Tagging annotation is released after a series of work over the last 4 years. This is the largest morphologically annotated Khmer dataset as of 2020, when this article was prepared. Based on the annotated data, experiments were conducted to establish a comprehensive benchmark on the automatic processing of tokenization and POS-Tagging for Khmer. Specifically, a support vector machine, a conditional random field (CRF), a long short-Term memory (LSTM)-based recurrent neural network, and an integrated LSTM-CRF model have been investigated and discussed. As a primary conclusion, processing at morpheme-level is satisfactory for the provided data. However, it is intrinsically difficult to identify further grammatical constituents of compounds or phrases because of the complex analytic features of the language. Syntactic annotation and automatic parsing for Khmer will be scheduled in the near future.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",annotated data; Khmer; machine learning; POS-Tagging; tokenization,Computational linguistics; Random processes; Support vector machines; Syntactics; Annotated data; Automatic processing; Khmer; Machine-learning; Part of speech tagging; Parts-of-speech tagging; Random field model; Random fields; Support vectors machine; Tokenization; Long short-term memory
SE4ExSum: An Integrated Semantic-Aware Neural Approach with Graph Convolutional Network for Extractive Text Summarization,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125581706&doi=10.1145%2f3464426&partnerID=40&md5=61d0c6094fa1a4788af7bb099bacb794,"Recently, advanced techniques in deep learning such as recurrent neural network (GRU, LSTM and Bi-LSTM) and auto-encoding (attention-based transformer and BERT) have achieved great successes in multiple application domains including text summarization. Recent state-of-The-Art encoding-based text summarization models such as BertSum, PreSum and DiscoBert have demonstrated significant improvements on extractive text summarization tasks. However, recent models still encounter common problems related to the language-specific dependency which requires the supports of the external NLP tools. Besides that, recent advanced text representation methods, such as BERT as the sentence-level textual encoder, also fail to fully capture the representation of a full-length document. To address these challenges, in this paper we proposed a novel semantic-ware embedding approach for extractive text summarization, called as: SE4ExSum. Our proposed SE4ExSum is an integration between the use of feature graph-of-words (FGOW) with BERT-based encoder for effectively learning the word/sentence-level representations of a given document. Then, the graph convolutional network (GCN) based encoder is applied to learn the global document's representation which is then used to facilitate the text summarization task. Extensive experiments on benchmark datasets show the effectiveness of our proposed model in comparing with recent state-of-The-Art text summarization models.  © 2021 Association for Computing Machinery.",BERT; graph convolutional network; graph-of-words; Text summarization,Convolution; Convolutional neural networks; Encoding (symbols); Long short-term memory; Network coding; Semantic Web; Text processing; BERT; Convolutional networks; Encodings; Graph convolutional network; Graph-of-word; Recent state; Sentence level; State of the art; Summarization models; Text Summarisation; Semantics
Mining Domain Terminologies Using Search Engine's Query Log,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135015950&doi=10.1145%2f3462327&partnerID=40&md5=523ee746d918e9ed3ecfc8974d9c04ed,"Domain terminologies are a basic resource for various natural language processing tasks. To automatically discover terminologies for a domain of interest, most traditional approaches mostly rely on a domain-specific corpus given in advance; thus, the performance of traditional approaches can only be guaranteed when collecting a high-quality domain-specific corpus, which requires extensive human involvement and domain expertise. In this article, we propose a novel approach that is capable of automatically mining domain terminologies using search engine's query log-a type of domain-independent corpus of higher availability, coverage, and timeliness than a manually collected domain-specific corpus. In particular, we represent query log as a heterogeneous network and formulate the task of mining domain terminology as transductive learning on the heterogeneous network. In the proposed approach, the manifold structure of domain-specificity inherent in query log is captured by using a novel network embedding algorithm and further exploited to reduce the need for the manual annotation efforts for domain terminology classification. We select Agriculture and Healthcare as the target domains and experiment using a real query log from a commercial search engine. Experimental results show that the proposed approach outperforms several state-of-The-Art approaches.  © 2021 Association for Computing Machinery.",Domain terminology; network embedding; query log; search engine; transductive learning,Heterogeneous networks; Information retrieval; Natural language processing systems; Network embeddings; Search engines; Domain specific; Domain terminology; High quality; Language processing; Natural languages; Network embedding; Performance; Query logs; Traditional approaches; Transductive learning; Terminology
Sentiment Analysis Using XLM-R Transformer and Zero-shot Transfer Learning on Resource-poor Indian Language,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120309827&doi=10.1145%2f3461764&partnerID=40&md5=974c336f3b4d57d5df264e304cf62853,"Sentiment analysis on social media relies on comprehending the natural language and using a robust machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. The cultural miscellanies, geographically limited trending topic hash-tags, access to aboriginal language keyboards, and conversational comfort in native language compound the linguistic challenges of sentiment analysis. This research evaluates the performance of cross-lingual contextual word embeddings and zero-shot transfer learning in projecting predictions from resource-rich English to resource-poor Hindi language. The cross-lingual XLM-RoBERTa classification model is trained and fine-tuned using the English language Benchmark SemEval 2017 dataset Task 4 A and subsequently zero-shot transfer learning is used to evaluate the classification model on two Hindi sentence-level sentiment analysis datasets, namely, IITP-Movie and IITP-Product review datasets. The proposed model compares favorably to state-of-the-art approaches and gives an effective solution to sentence-level (tweet-level) analysis of sentiments in a resource-poor scenario. The proposed model compares favorably to state-of-the-art approaches and achieves an average performance accuracy of 60.93 on both the Hindi datasets.  © 2021 Association for Computing Machinery.",deep learning; resource-poor language; Sentiment analysis; transformer,
An Investigational Approach for Vowels of the Salar Language Based on a Database of Speech Acoustic Parameters,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120338670&doi=10.1145%2f3459927&partnerID=40&md5=bf67e5e7410806ce5ae2b268c00b59c8,"According to relevant specifications, this article divides, marks, and extracts the acquired speech signals of the Salar language, and establishes the speech acoustic parameter database of the Salar language. Then, the vowels of the Salar language are analyzed and studied by using the parameter database. The vowel bitmap (average value at the beginning of words), the vowel bitmap (average value at the abdomen of words), the vowel bitmap (average value at the ending of words), and the vowel bitmap (average value) are obtained. Through the vowel bitmaps, we can observe the vowel in different positions of the word, the overall appearance of an obtuse triangle. The high vowel [i], [o], and low vowel [a] occupy three vertices, respectively. Among the three lines, [i] to [o] are the longest, [i] to [a] are the second longest, and [a] to [o] are the shortest. The lines between [a] to [o] and [a] and [i] are asymmetric. Combining with the vowel bitmap, the vowels were discretized, and the second formant (F2) frequency parameter was used as the coordinate of the X axis, and the first formant (F1) frequency was used as the coordinate of the Y axis to draw the region where the vowel was located, and then the vowel pattern was formed. These studies provide basic data and parameters for the future development of modern phonetics such as the database of Sarah language speech, speech recognition, and speech synthesis. It also provides the basic parameters of speech acoustics for the rare minority acoustic research work of the national language project.  © 2021 Association for Computing Machinery.",acoustic parameter; Salar language; vowel bitmap; vowel pattern,
Two-channel Attention Mechanism Fusion Model of Stock Price Prediction Based on CNN-LSTM,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120317212&doi=10.1145%2f3453693&partnerID=40&md5=d3501faa03859ed3a890f51e1ce13ca5,"Using hierarchical CNN, the company's multiple news is characterized as three levels: sentence vectors, chapter vectors, and enterprise sentiment vectors. By combining the stock price data with the news lyric data at the same time, the influence of news on price is used to achieve correlation analysis of news information and stock prices. A two-channel attention mechanism fusion model based on CNN-LSTM is proposed. After the dual-channel feature extraction, the attention layer fusion layer is used to convert the weighted values of LSTM hidden variables, so the stock price can be predicted with the news text.  © 2021 Association for Computing Machinery.",attention mechanism; CNN-LSTM; stock prediction; two-channel,
Research on Pre-Training Method and Generalization Ability of Big Data Recognition Model of the Internet of Things,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116319690&doi=10.1145%2f3433539&partnerID=40&md5=626f3c5ae078ef909b608b3955db796a,"The Internet of Things and big data are currently hot concepts and research fields. The mining, classification, and recognition of big data in the Internet of Things system are the key links that are widely of concern at present. The artificial neural network is beneficial for multi-dimensional data classification and recognition because of its strong feature extraction and self-learning ability. Pre-training is an effective method to address the gradient diffusion problem in deep neural networks and could result in better generalization. This article focuses on the performance of supervised pre-training that uses labelled data. In particular, this pre-training procedure is a simulation that shows the changes in judgment patterns as they progress from primary to mature within the human brain. In this article, the state-of-the-art of neural network pre-training is reviewed. Then, the principles of the auto-encoder and supervised pre-training are introduced in detail. Furthermore, an extended structure of supervised pre-training is proposed. A set of experiments are carried out to compare the performances of different pre-training methods. These experiments include a comparison between the original and pre-trained networks as well as a comparison between the networks with two types of sub-network structures. In addition, a homemade database is established to analyze the influence of pre-training on the generalization ability of neural networks. Finally, an ordinary convolutional neural network is used to verify the applicability of supervised pre-training.  © 2021 Association for Computing Machinery.",Big data; convergence; generalization; neural network; pre-training procedure,
Deep Neural Network Based Noised Asian Speech Enhancement and Its Implementation on a Hearing Aid App,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120325739&doi=10.1145%2f3439797&partnerID=40&md5=c0435c3c3d2f060193c12d1dcf9f9475,"This article studies noised Asian speech enhancement based on the deep neural network (DNN) and its implementation on an app. We use the THCHS-30 speech dataset and the common noise dataset in daily life as training and testing data of the DNN. To stack the frequency data of multiple audio frames to improve the effect of speech enhancement, the system compares the best number of stacked frames during training and testing. At the same time, the influence of training rounds on the PESQ is compared, and the best number of rounds is obtained. On this basis, the best model is implemented on the hearing aid app, and the real-time performance of the device is tested. The experiment shows that based on the DNN, using an appropriate number of rounds for training and using an appropriate number of audio frames stacking to improve the speech enhancement effect, and transplanting this speech enhancement model to the hearing aid app, can effectively improve speech clarity and intelligibility within a reasonable time delay range.  © 2021 Association for Computing Machinery.",Feature selection; health care information systems; natural language processing; neural networks; noise reduction,
A Hybrid CNN-LSTM: A Deep Learning Approach for Consumer Sentiment Analysis Using Qualitative User-Generated Contents,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120344433&doi=10.1145%2f3457206&partnerID=40&md5=eb158a68f75bd0b0671b5a4e9182fa38,"With the fastest growth of information and communication technology (ICT), the availability of web content on social media platforms is increasing day by day. Sentiment analysis from online reviews drawing researchers' attention from various organizations such as academics, government, and private industries. Sentiment analysis has been a hot research topic in Machine Learning (ML) and Natural Language Processing (NLP). Currently, Deep Learning (DL) techniques are implemented in sentiment analysis to get excellent results. This study proposed a hybrid convolutional neural network-long short-term memory (CNN-LSTM) model for sentiment analysis. Our proposed model is being applied with dropout, max pooling, and batch normalization to get results. Experimental analysis carried out on Airlinequality and Twitter airline sentiment datasets. We employed the Keras word embedding approach, which converts texts into vectors of numeric values, where similar words have small vector distances between them. We calculated various parameters, such as accuracy, precision, recall, and F1-measure, to measure the model's performance. These parameters for the proposed model are better than the classical ML models in sentiment analysis. Our results analysis demonstrates that the proposed model outperforms with 91.3% accuracy in sentiment analysis.  © 2021 Association for Computing Machinery.",Convolutional neural network; deep learning; long short-term memory; sentiment analysis; social media; word embedding,
GeoGAT: Graph Model Based on Attention Mechanism for Geographic Text Classification,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120343040&doi=10.1145%2f3434239&partnerID=40&md5=637bf81cad8a7eedcc8ac8936119cdd7,"In the area of geographic information processing, there are few researches on geographic text classification. However, the application of this task in Chinese is relatively rare. In our work, we intend to implement a method to extract text containing geographical entities from a large number of network texts. The geographic information in these texts is of great practical significance to transportation, urban and rural planning, disaster relief, and other fields. We use the method of graph convolutional neural network with attention mechanism to achieve this function. Graph attention networks (GAT) is an improvement of graph convolutional neural networks (GCN). Compared with GCN, the advantage of GAT is that the attention mechanism is proposed to weight the sum of the characteristics of adjacent vertices. In addition, We construct a Chinese dataset containing geographical classification from multiple datasets of Chinese text classification. The Macro-F Score of the geoGAT we used reached 95% on the new Chinese dataset.  © 2021 Association for Computing Machinery.",attention mechanism; Graph neural networks; text classification; toponym recognition,
Research on Extraction of Useful Tourism Online Reviews Based on Multimodal Feature Fusion,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120317868&doi=10.1145%2f3453694&partnerID=40&md5=62ae4e343bd35522d1d2d4ff7c579e30,"To effectively identify the influencing factors of the perceived usefulness of multimodal data in online reviews of tourism products, this article explores the optimization method of online tourism products based on user-generated content and conducts feature fusion of multimodal data in online reviews of tourism products from the perspective of data fusion analysis. Therefore, based on the word vector model, this article proposes a method to select the seed word set of emotion dictionary. In this method, emotional words are represented in vector form and the distance between word vectors is calculated to form the selection criteria and classification basis of seed word set, and then the sentiment dictionary of online review is formed by category judgment. This article takes the real online review data of tourism products as the research object, carries out descriptive statistical analysis, uses machine learning and deep learning methods, carries out text vector embedding and image content recognition, integrates image and text feature vector, constructs multimodal online review usefulness classification model, and conducts model test. The experimental results show that, compared with the single-mode reviews containing only text or pictures, the multimodal reviews combined with text and pictures can better predict the usefulness of online reviews, improve the quality of online reviews, give full play to the potential value of user-generated content, provide optimization ideas for product providers, and provide decision support for product consumers.  © 2021 Association for Computing Machinery.",deep learning; feature fusion; image recognition; Multi-modal; tourism online reviews,Character recognition; Decision support systems; E-learning; Image fusion; Image recognition; Learning systems; Modal analysis; Vectors; Deep learning; Features fusions; Multi-modal; Multi-modal data; Multimodal feature fusions; Online reviews; Seed words; Tourism online review; User-generated; Word vectors; Deep learning
A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115874355&doi=10.1145%2f3434237&partnerID=40&md5=2380861ac315bcd7273e2a79a875c5e6,"Word representation has always been an important research area in the history of natural language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP-related tasks. In the end, this survey briefly discusses the commonly used ML- and DL-based classifiers, evaluation metrics, and the applications of these word embeddings in different NLP tasks.  © 2021 Association for Computing Machinery.",language models; natural language processing; Text mining; word representation,
A Computer Corpus-Based Study of Chinese EFL Learners Use of Adverbial Connectors and Its Implications for Building a Language-Based Learning Environment,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120359599&doi=10.1145%2f3457987&partnerID=40&md5=bda55e40ea88066a4e707a6169e4fbea,"This research adopts the methodology of corpus-based analysis and contrastive interlanguage analysis (CIA), using three corpora as the data source to analyze the adverbial connectors used by Chinese EFL (English as a foreign language) learners (i.e., university students in Guangzhou, China) in their written English. Major findings show that Chinese EFL learners have displayed a general tendency to overuse English adverbial connectors in terms of total tokens when compared with native speakers of English, and Chinese EFL learners deviate notably from the native speakers of English in the use of some individual English adverbial connectors. The research explores that Chinese EFL learners' use of English adverbial connectors might be influenced by L1 transfer, writing handbooks' and teachers' instruction, learners' lack of audience awareness, and lack of stylistic awareness. The research has some implications for language learning: a large collection of learner corpora, a target language's native speakers corpus, a learner's mother language corpus, and corpus software AntConc can complement textbooks in language learners' deep learning process, constituting a language-based learning environment for human languages with reduced perplexity and increased accuracy.  © 2021 Association for Computing Machinery.",adverbial connectors; Computer corpus-based study; contrastive interlanguage analysis; deep learning; language-based learning environment,Computer aided instruction; Handbooks; Learning systems; Semantics; Adverbial connector; Computer corpus-based study; Contrastive interlanguage analyse; Corpus-based; Data-source; Deep learning; English-as-a-Foreign-Language; Language-based learning environment; Learning environments; University students; Deep learning
"Introduction to the Special Issue on Deep Structured Learning for Natural Language Processing, Part 3",2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120330461&doi=10.1145%2f3476464&partnerID=40&md5=0b3722f75453e54508aedc79e0d820e8,[No abstract available],,
Sequence Alignment with Q-Learning Based on the Actor-Critic Model,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120310694&doi=10.1145%2f3433540&partnerID=40&md5=b30bbb6691943ef4b4a117f1ff004e93,"Multiple sequence alignment methods refer to a series of algorithmic solutions for the alignment of evolutionary-related sequences while taking into account evolutionary events such as mutations, insertions, deletions, and rearrangements under certain conditions. In this article, we propose a method with Q-learning based on the Actor-Critic model for sequence alignment. We transform the sequence alignment problem into an agent's autonomous learning process. In this process, the reward of the possible next action taken is calculated, and the cumulative reward of the entire process is calculated. The results show that the method we propose is better than the gene algorithm and the dynamic programming method.  © 2021 Association for Computing Machinery.",Actor-Critic model; Q-learning; reinforcement learning; Sequence alignment,Alignment; Autonomous agents; Dynamic programming; Evolutionary algorithms; Learning systems; Actor critic models; Algorithmic solutions; Condition; Evolutionary events; Multiple sequence alignments; Q-learning; Reinforcement learnings; Sequence Alignment Methods; Sequence alignment problem; Sequence alignments; Reinforcement learning
Multi-Objective Heuristic Decision Making and Benchmarking for Mobile Applications in English Language Learning,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120338111&doi=10.1145%2f3439799&partnerID=40&md5=e75ef07f03b9d14eabcab3da9d65d519,"This research proposes to evaluate and analyze the decision matrix for learner's English mobile applications (EMAs) based on multi-objective heuristic decision making with a view to listening, speaking, reading, and writing. Because of the number of criteria, the significance of parameters, and variance in results, EMAs are difficult. Decision making has built on the combination of listening, speaking, reading, and writing and EMA evaluation criteria for students. The requirements are adapted from a framework of pre-school education. Six alternatives and 17 skills as a requirement are included in decision-making results. The six EMA are then assessed, with six English learning experts distributing a review form. The application subsequently is evaluated using the best-worst method and preference-order technique (TOPSIS) using multi-objective heuristic decision making methods. The best-worst method is used to measure requirements, whereas TOPSIS is used to test and assess the applications. In two cases, namely person and group, TOPSIS is used. Internal and external aggregations are used throughout the group context. In effect, the aim of evaluating the proposed study and comparing it to six relative studies with scenarios and benchmarking checklists is to develop an objectives validation framework for e-apps.  © 2021 Association for Computing Machinery.",BWM; EMAs; English mobile applications; learning techniques; TOPSIS,Decision making; Heuristic methods; Learning systems; Mobile computing; BWM; Decisions makings; English mobile application; Heuristic decision makings; Learning techniques; Mobile applications; Multi objective; TOPSIS; Benchmarking
Toward Integrated CNN-based Sentiment Analysis of Tweets for Scarce-resource Language Hindi,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120345322&doi=10.1145%2f3450447&partnerID=40&md5=cc84d0391b95e03a0ad4b9e8f5da2a31,"Linguistic resources for commonly used languages such as English and Mandarin Chinese are available in abundance, hence the existing research in these languages. However, there are languages for which linguistic resources are scarcely available. One of these languages is the Hindi language. Hindi, being the fourth-most popular language, still lacks in richly populated linguistic resources, owing to the challenges involved in dealing with the Hindi language. This article first explores the machine learning-based approaches - Naïve Bayes, Support Vector Machine, Decision Tree, and Logistic Regression - to analyze the sentiment contained in Hindi language text derived from Twitter.Further, the article presents lexicon-based approaches (Hindi Senti-WordNet, NRC Emotion Lexicon) for sentiment analysis in Hindi while also proposing a Domain-specific Sentiment Dictionary. Finally, an integrated convolutional neural network (CNN) - Recurrent Neural Network and Long Short-term Memory - is proposed to analyze sentiment from Hindi language tweets, a total of 23,767 tweets classified into positive, negative, and neutral. The proposed CNN approach gives an accuracy of 85%.  © 2021 Association for Computing Machinery.",Convolutional neural network; Hindi; lexicon; linguistic; scarce-resource language (SRL); sentiment analysis; Twitter,
Bi-directional Long Short-Term Memory Model with Semantic Positional Attention for the Question Answering System,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120344297&doi=10.1145%2f3439800&partnerID=40&md5=8b81501816f8a9b9936458d48e5539eb,"The intelligent question answering system aims to provide quick and concise feedback on the questions of users. Although the performance of phrase-level and numerous attention models have been improved, the sentence components and position information are not emphasized enough. This article combines Ci-Lin and word2vec to divide all of the words in the question-answer pairs into groups according to the semantics and select one kernel word in each group. The remaining words are common words and realize the semantic mapping mechanism between kernel words and common words. With this Chinese semantic mapping mechanism, the common words in all questions and answers are replaced by the semantic kernel words to realize the normalization of the semantic representation. Meanwhile, based on the bi-directional LSTM model, this article introduces a method of the combination of semantic role labeling and positional context, dividing the sentence into multiple semantic segments according to semantic logic. The weight is given to the neighboring words in the same semantic segment and propose semantic role labeling position attention based on the bi-directional LSTM model (BLSTM-SRLP). The good performance of the BLSTM-SRLP model has been demonstrated in comparative experiments on the food safety field dataset (FS-QA).  © 2021 Association for Computing Machinery.",BLSTM model; Chinese semantic mapping mechanism; Question answering; semantic positional-based attention,Long short-term memory; Mapping; Bi-directional; BLSTM model; Chinese semantic mapping mechanism; Mapping mechanism; Memory modeling; Performance; Question Answering; Semantic positional-based attention; Semantic role labeling; Semantics mappings; Semantics
Betalogger: Smartphone Sensor-based Side-channel Attack Detection and Text Inference Using Language Modeling and Dense MultiLayer Neural Network,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117611301&doi=10.1145%2f3460392&partnerID=40&md5=90efd2ac3c06f104d5c3093e6f789e3a,"With the recent advancement of smartphone technology in the past few years, smartphone usage has increased on a tremendous scale due to its portability and ability to perform many daily life tasks. As a result, smartphones have become one of the most valuable targets for hackers to perform cyberattacks, since the smartphone can contain individuals' sensitive data. Smartphones are embedded with highly accurate sensors. This article proposes BetaLogger, an Android-based application that highlights the issue of leaking smartphone users' privacy using smartphone hardware sensors (accelerometer, magnetometer, and gyroscope). BetaLogger efficiently infers the typed text (long or short) on a smartphone keyboard using Language Modeling and a Dense Multi-layer Neural Network (DMNN). BetaLogger is composed of two major phases: In the first phase, Text Inference Vector is given as input to the DMNN model to predict the target labels comprising the alphabet, and in the second phase, sequence generator module generate the output sequence in the shape of a continuous sentence. The outcomes demonstrate that BetaLogger generates highly accurate short and long sentences, and it effectively enhances the inference rate in comparison with conventional machine learning algorithms and state-of-the-art studies.  © 2021 Association for Computing Machinery.",cyber security; dense neural network; keylogger; language modeling; Natural language processing (NLP); side-channel attack; text inference,
An OT-ET Analysis of Polish Singular-Plural Pairs,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120332231&doi=10.1145%2f3434238&partnerID=40&md5=2c099829ed8f2d78522ca857b52da5cf,"Optimality Theory (OT) and Exemplar Theory (ET) are two enchanting theories to many scholars, but each still faces criticism and remaining persistent problems. Application of both theories to areas in linguistics where conflicts may arise has been attempted, but still the suitability of combining the two theories to resolve contradictions awaits further analysis and verification. This article takes Polish singular-plural pairs as the object of study and argues in favor of an OT-ET combined model of analyzing the linguistic phenomenon. First, an underlying representation is identified to be the input in an OT analysis. Then two main changes are recognized between the input and output, and are regarded as instances of positional neutralization, and their relevant constraints and constraint hierarchies are presented. Following this, challenges are posed to OT despite its merits. It turns out that the combined OT-ET model works well, with historical development, underspecification, constraint hierarchy, and resemblance to existing word clouds, among others, all playing relevant parts. The current study adds to the extensiveness of language data analyzed for or against combining OT and ET, and sketches the analysis pattern of thus doing, with a view to offering more real-life language materials for an OT-ET combined model.  © 2021 Association for Computing Machinery.",constraint hierarchy; Exemplar Theory (ET); Optimality Theory (OT); Polish; underspecification,Analysis and verifications; Combined modeling; Constraint hierarchy; Exemplar theory; Input and outputs; Linguistic phenomena; Optimality theories; Optimality theory; Polish; Underspecification; Linguistics
Configurational Path to Chinese Reading Stickiness of Digital Library,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120323304&doi=10.1145%2f3459092&partnerID=40&md5=762920e275858968f04fe03e428d379c,"Attracting and retaining readers in an increasingly competitive environment is an urgent problem for digital libraries of original literature. However, few empirical studies address online reading stickiness, particularly the factors affecting the promotion of online reading stickiness, in what combinations or paths these effects exist, and whether there are complementary, alternative, and inhibitory relationships among the factors. To solve the practical problems and fill the theoretical gap, we use a fuzzy-set qualitative comparative analysis to study the interaction effects of the flow experience (feeling of immersion and perceived pleasure), technology acceptance model (perceived usefulness and perceived ease of use), and customer participation (information sharing and interpersonal interaction) to identify the critical configurations leading to a high level of stickiness in online reading and to verify the complementarity, substitution, and inhibition relationships among these variables. The findings provide implications for further research on complexity theory in digital libraries of original literature, and for managers to view and redesign online reading stickiness as configurations of IT and psychological capabilities. This study enriches and develops the existing theories and expands the application of the qualitative comparative analysis method in the field of digital libraries.  © 2021 Association for Computing Machinery.",digital customer participation; flow experience; Online reading stickiness; perceived ease of use; perceived usefulness; qualitative comparative analysis,
Movie Recommendation System to Solve Data Sparsity Using Collaborative Filtering Approach,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120359655&doi=10.1145%2f3459091&partnerID=40&md5=849dfe7dab061a5788276b018d382cb7,"With the increase in numbers of multimedia technologies around us, movies and videos on social media and OTT platforms are growing, making it confusing for users to decide which one to watch for. For this, movie recommendation systems are widely used. It has been observed that two-thirds of the films watched on Netflix are the recommended ones to its users. The target of this work is to use implicit feedback given by other users to recommend movies, i.e., ratings given by them. Implicit feedback will help to enhance Data Sparsity as for a replacement logged-in user, the system won't have details of their past liked movies. So, matching the similarity with other users is often a plus point to recommend movies that they would like. The anticipated result will depend upon the positive attitude; i.e., if the predicted rating is high, then it'll be recommended; otherwise it'll not be recommended. The performance of the methodology is measured with accuracy and precision values for different strategies. It gives the best accuracy and highest precision values using Logistic Regression (LR) and lowest recall value as compared to other algorithms. This technique gives an accuracy, precision, and recall value of 81.9%, 69.82%, and 32.5%, respectively, using LR.  © 2021 Association for Computing Machinery.",big data analysis; Collaborative filtering; data sparsity; recommender system,Big data; Motion pictures; Multimedia systems; Recommender systems; Big data analyse; Data sparsity; Implicit feedback; Logistics regressions; Matchings; Movie recommendations; Multimedia technologies; Netflix; Positive attitude; Social media; Collaborative filtering
A Novel Resource Optimization Algorithm Based on Clustering and Improved Differential Evolution Strategy under a Cloud Environment,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120324865&doi=10.1145%2f3462761&partnerID=40&md5=05bfaa2ddde1c6e60614d2314f166132,"Resource optimization algorithm based on clustering and improved differential evolution strategy, as a new global optimized algorithm, has wide applications in language translation, language processing, document understanding, cloud computing, and edge computing due to high efficiency. With the development of deep learning technology and the rise of big data, the resource optimization algorithm encounters a series of challenges, such as the workload imbalance and low resource utilization. To address the preceding problems, this study proposes a novel resource optimization algorithm based on clustering and an improved differential evolution strategy (Multi-objective Task Scheduling Strategy (MTSS)). Three indexes, namely task completion time, execution cost, and workload, of virtual machines are selected and used to build the fitness function of the MTSS algorithm. At the same time, the preprocessing state is set up to cluster according to the resource and task characteristics to reduce the magnitude of their matching scale. Moreover, to solve the workload imbalance among different resource sets, local resource tasks are reallocated using the Q-value method in the MTSS strategy to achieve workload balance of global resources and improve the resource utilization rate. Experiments are carried out to evaluate the effectiveness of the proposed algorithm. Results show that the proposed algorithm outperforms other algorithms in terms of task completion time, execution cost, and workload balancing.  © 2021 Association for Computing Machinery.",deep learning; low resource utilization; multi-objective optimization; Natural language processing; resource optimization,
Reinforced NMT for Sentiment and Content Preservation in Low-resource Scenario,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120329950&doi=10.1145%2f3450970&partnerID=40&md5=c2f715a02041c4ffa325a3e1fa418f45,"The preservation of domain knowledge from source to the target is crucial in any translation workflows. Hence, translation service providers that use machine translation (MT) in production could reasonably expect that the translation process should transfer both the underlying pragmatics and the semantics of the source-side sentences into the target language. However, recent studies suggest that the MT systems often fail to preserve such crucial information (e.g., sentiment, emotion, gender traits) embedded in the source text in the target. In this context, the raw automatic translations are often directly fed to other natural language processing (NLP) applications (e.g., sentiment classifier) in a cross-lingual platform. Hence, the loss of such crucial information during the translation could negatively affect the performance of such downstream NLP tasks that heavily rely on the output of the MT systems. In our current research, we carefully balance both the sides (i.e., sentiment and semantics) during translation, by controlling a global-attention-based neural MT (NMT), to generate translations that encode the underlying sentiment of a source sentence while preserving its non-opinionated semantic content. Toward this, we use a state-of-the-art reinforcement learning method, namely, actor-critic, that includes a novel reward combination module, to fine-tune the NMT system so that it learns to generate translations that are best suited for a downstream task, viz. sentiment classification while ensuring the source-side semantics is intact in the process. Experimental results for Hindi-English language pair show that our proposed method significantly improves the performance of the sentiment classifier and alongside results in an improved NMT system.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",actor-critic; BERT; Machine translation; neural machine translation; reinforcement learning; sentiment preservation,Computational linguistics; Computer aided language translation; Domain Knowledge; Embedded systems; Learning algorithms; Natural language processing systems; Neural machine translation; Reinforcement learning; Actor critic; BERT; Domain knowledge; Down-stream; Machine translation systems; Performance; Sentiment preservation; Service provider; Translation services; Work-flows; Semantics
A Novel Attack on Monochrome and Greyscale Devanagari CAPTCHAs,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120328579&doi=10.1145%2f3439798&partnerID=40&md5=90c3c2e83cdb530e59ca3f5e1b7dc6b7,"The use of computer programs in breaching web site security is common today. CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) and human interaction proofs are the cost-effective solution to these kinds of computer attacks on web sites. These CAPTCHAs are available in many forms, such as those based on text, images and audio. A CAPTCHA must be secure enough that it cannot be broken by a computer program, and it must be usable enough that humans can easily understand it. The most popular is the text-based scheme. Most text-based CAPTCHAs are based on the English language and are not usable by the native people of India. Research has proven that native people are more comfortable with native language-based CAPTCHA. Devanagari-based CAPTCHAs are also available, but the security aspect has not been tested. Unfortunately, English language-based CAPTCHAs are successfully broken. Therefore, it is important to test the security of Devanagari script-based CAPTCHAs. We picked five unique monochrome CAPTCHAs and five unique greyscale CAPTCHAs for testing security. We achieved 88.13% to 97.6% segmentation rates on these schemes and generated six types of features for these segmented characters, such as raw pixels, zoning, projection, Scale-Invariant Feature Transform (SIFT), Speeded-Up Robust Features (SURF) and Oriented Fast and Rotated BRIEF (ORB). For classification, we used three classifiers for comparative analyses. Using k-Nearest Neighbour (k-NN), Support Vector Machine (SVM) and Random Forest, we achieved high recognition on monochrome and greyscale schemes. For monochrome Devanagari CAPTCHAs, the recognition rate of k-NN ranges from 64.78% to 82.39%, SVM ranges from 76.46% to 91.34% and Random Forest ranges from 80.34% to 91.28%. For greyscale Devanagari CAPTCHAs, the recognition rate of k-NN ranges from 67.52% to 85.47%, SVM ranges from 76.9% to 91.71% and Random Forest ranges from 83.07% to 92.13%. We achieved a breaking rate for monochrome schemes of 66% to 85% and for greyscale schemes of 73% to 93%.  © 2021 Association for Computing Machinery.",bot; Devanagari CAPTCHA; greyscale Devanagari CAPTCHA; human interaction proofs; Internet security; monochrome Devanagari CAPTCHA; reverse Turing test,Botnet; Cost effectiveness; Electronic mail filters; Nearest neighbor search; Network security; Support vector machines; Websites; CAPTCHAs; Devanagari completely automated public turing test to tell computer and human apart; Greyscale; Greyscale devanagari completely automated public turing test to tell computer and human apart; Human-interaction-proof; Internet security; Monochrome devanagari completely automated public turing test to tell computer and human apart; Random forests; Reverse turing test; Support vectors machine; Decision trees
Intermodal Sentiment Analysis for Images with Text Captions Using the VGGNET Technique,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120309726&doi=10.1145%2f3450971&partnerID=40&md5=013d60378f2fc665bdeba0b6475d76ab,"More individuals actively express their opinions and attitudes in social media through advanced improvements such as visual content and text captions. Sentiment analysis for visuals such as images, video, and GIFs has become an emerging research trend in understanding social involvement and opinion prediction. Numerous individual researchers have obtained good progress in outcomes for text sentiment analysis and image sentiment analysis. The combination of image sentiment analysis with text caption analysis needs more research. This article presents a VGG Network-based Intermodal Sentiment Analysis Model (VGGNET-ISAM) for transferring the connection between texts to images. A mapping process is developed using the VGG Network for gathering the opinion information as numerical vectors. The Active Deep Learning (ADL) classifier is used for opinion prediction from the obtained information vectors. Simulation experiments are carried out to evaluate the proposed approach. The findings show that the model outperforms and gives better solutions with high accuracy, precision with low delay, and low error rate.  © 2021 Association for Computing Machinery.",active deep learning; Image sentiment analysis; intermodal sentimental analysis; opinion prediction; VGGNET,Classification (of information); Deep learning; Forecasting; Image analysis; Active deep learning; Image sentiment analyse; Intermodal sentimental analyse; Network-based; Opinion prediction; Research trends; Sentiment analysis; Social media; VGGNET; Visual content; Sentiment analysis
Source-side Reordering to Improve Machine Translation between Languages with Distinct Word Orders,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120317076&doi=10.1145%2f3448252&partnerID=40&md5=96f17a6684fb2097394d8c88e9afacdf,"English and Hindi have significantly different word orders. English follows the subject-verb-object (SVO) order, while Hindi primarily follows the subject-object-verb (SOV) order. This difference poses challenges to modeling this pair of languages for translation. In phrase-based translation systems, word reordering is governed by the language model, the phrase table, and reordering models. Reordering in such systems is generally achieved during decoding by transposing words within a defined window. These systems can handle local reorderings, and while some phrase-level reorderings are carried out during the formation of phrases, they are weak in learning long-distance reorderings. To overcome this weakness, researchers have used reordering as a step in pre-processing to render the reordered source sentence closer to the target language in terms of word order. Such approaches focus on using parts-of-speech (POS) tag sequences and reordering the syntax tree by using grammatical rules, or through head finalization. This study shows that mere head finalization is not sufficient for the reordering of sentences in the English-Hindi language pair. It describes various grammatical constructs and presents a comparative evaluation of reorderings with the original and the head-finalized representations. The impact of the reordering on the quality of translation is measured through the BLEU score in phrase-based statistical systems and neural machine translation systems. A significant gain in BLEU score was noted for reorderings in different grammatical constructs.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Machine translation; SOV; SVO; word reordering,Computer aided language translation; Machine translation; Modeling languages; Trees (mathematics); Language model; Machine translations; Reordering models; Source-side reordering; Subject-object-verb; Subject-verb-object; Table modeling; Translation systems; Word orders; Word reordering; Computational linguistics
A Framework for Indonesian Grammar Error Correction,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120344524&doi=10.1145%2f3440993&partnerID=40&md5=a2a35930ce1517c407d245913376f39f,"Grammatical Error Correction (GEC) is a challenge in Natural Language Processing research. Although many researchers have been focusing on GEC in universal languages such as English or Chinese, few studies focus on Indonesian, which is a low-resource language. In this article, we proposed a GEC framework that has the potential to be a baseline method for Indonesian GEC tasks. This framework treats GEC as a multi-classification task. It integrates different language embedding models and deep learning models to correct 10 types of Part of Speech (POS) error in Indonesian text. In addition, we constructed an Indonesian corpus that can be utilized as an evaluation dataset for Indonesian GEC research. Our framework was evaluated on this dataset. Results showed that the Long Short-Term Memory model based on word-embedding achieved the best performance. Its overall macro-average F0.5 in correcting 10 POS error types reached 0.551. Results also showed that the framework can be trained on a low-resource dataset.  © 2021 Association for Computing Machinery.",Grammatical error correction; indonesian language processing; low-resource language; word-embedding,Deep learning; Embeddings; Natural language processing systems; Embeddings; Errors correction; Grammatical error correction; Grammatical errors; Indonesian language processing; Indonesian languages; Language processing; Low resource languages; Part Of Speech; Word-embedding; Error correction
Approaches for Multilingual Phone Recognition in Code-switched and Non-code-switched Scenarios Using Indian Languages,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120317863&doi=10.1145%2f3437256&partnerID=40&md5=22f453871e1369225e3a2d5d4c578de5,"In this study, we evaluate and compare two different approaches for multilingual phone recognition in code-switched and non-code-switched scenarios. First approach is a front-end Language Identification (LID)-switched to a monolingual phone recognizer (LID-Mono), trained individually on each of the languages present in multilingual dataset. In the second approach, a common multilingual phone-set derived from the International Phonetic Alphabet (IPA) transcription of the multilingual dataset is used to develop a Multilingual Phone Recognition System (Multi-PRS). The bilingual code-switching experiments are conducted using Kannada and Urdu languages. In the first approach, LID is performed using the state-of-the-art i-vectors. Both monolingual and multilingual phone recognition systems are trained using Deep Neural Networks. The performance of LID-Mono and Multi-PRS approaches are compared and analysed in detail. It is found that the performance of Multi-PRS approach is superior compared to more conventional LID-Mono approach in both code-switched and non-code-switched scenarios. For code-switched speech, the effect of length of segments (that are used to perform LID) on the performance of LID-Mono system is studied by varying the window size from 500 ms to 5.0 s, and full utterance. The LID-Mono approach heavily depends on the accuracy of the LID system and the LID errors cannot be recovered. But, the Multi-PRS system by virtue of not having to do a front-end LID switching and designed based on the common multilingual phone-set derived from several languages, is not constrained by the accuracy of the LID system, and hence performs effectively on code-switched and non-code-switched speech, offering low Phone Error Rates than the LID-Mono system.  © 2021 Association for Computing Machinery.",code-switching; common multilingual phone-set; Indian languages; LID-switched monolingual PRS; multilingual phone recognition,Codes (symbols); Deep neural networks; Natural language processing systems; Speech recognition; Code-switching; Common multilingual phone-set; Front end; Indian languages; Language identification; Language identification-switched monolingual PRS; Multilingual phone recognition; Performance; Phone recognition; Recognition systems; Telephone sets
Graph-based Multimodal Ranking Models for Multimodal Summarization,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120338263&doi=10.1145%2f3445794&partnerID=40&md5=f535de29c15a1218b068efb97f168095,"Multimodal summarization aims to extract the most important information from the multimedia input. It is becoming increasingly popular due to the rapid growth of multimedia data in recent years. There are various researches focusing on different multimodal summarization tasks. However, the existing methods can only generate single-modal output or multimodal output. In addition, most of them need a lot of annotated samples for training, which makes it difficult to be generalized to other tasks or domains. Motivated by this, we propose a unified framework for multimodal summarization that can cover both single-modal output summarization and multimodal output summarization. In our framework, we consider three different scenarios and propose the respective unsupervised graph-based multimodal summarization models without the requirement of any manually annotated document-summary pairs for training: (1) generic multimodal ranking, (2) modal-dominated multimodal ranking, and (3) non-redundant text-image multimodal ranking. Furthermore, an image-text similarity estimation model is introduced to measure the semantic similarity between image and text. Experiments show that our proposed models outperform the single-modal summarization methods on both automatic and human evaluation metrics. Besides, our models can also improve the single-modal summarization with the guidance of the multimedia information. This study can be applied as the benchmark for further study on multimodal summarization task.  © 2021 Association for Computing Machinery.",multimodal ranking; Multimodal summarization; single-modal; unsupervised,Information retrieval; Semantics; Graph-based; Multi-modal; Multimodal output; Multimodal ranking; Multimodal summarization; Ranking model; Rapid growth; Single-modal; Unsupervised; Graphic methods
Two New Large Corpora for Vietnamese Aspect-based Sentiment Analysis at Sentence Level,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116752995&doi=10.1145%2f3446678&partnerID=40&md5=d056c72f217854aae8345b0e913a05f4,"Aspect-based sentiment analysis has been studied in both research and industrial communities over recent years. For the low-resource languages, the standard benchmark corpora play an important role in the development of methods. In this article, we introduce two benchmark corpora with the largest sizes at sentence-level for two tasks: Aspect Category Detection and Aspect Polarity Classification in Vietnamese. Our corpora are annotated with high inter-annotator agreements for the restaurant and hotel domains. The release of our corpora would push forward the low-resource language processing community. In addition, we deploy and compare the effectiveness of supervised learning methods with a single and multi-task approach based on deep learning architectures. Experimental results on our corpora show that the multi-task approach based on BERT architecture outperforms the neural network architectures and the single approach. Our corpora and source code are published on this footnoted site.1  © 2021 Association for Computing Machinery.",Aspect-based sentiment analysis; deep neural network; multi-task learning; Vietnamese corpora,Deep neural networks; Industrial research; Network architecture; Aspect-based sentiment analyse; Industrial communities; Large corpora; Low resource languages; Multi tasks; Research communities; Sentence level; Sentiment analysis; Vietnamese; Vietnamese corpus; Sentiment analysis
Building Arabic Paraphrasing Benchmark based on Transformation Rules,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120335397&doi=10.1145%2f3446770&partnerID=40&md5=c36fba14c56391e4fd298b2add08f175,"Measuring semantic similarity between short texts is an important task in many applications of natural language processing, such as paraphrasing identification. This process requires a benchmark of sentence pairs that are labeled by Arab linguists and considered a standard that can be used by researchers when evaluating their results. This research describes an Arabic paraphrasing benchmark to be a good standard for evaluation algorithms that are developed to measure semantic similarity for Arabic sentences to detect paraphrasing in the same language. The transformed sentences are in accordance with a set of rules for Arabic paraphrasing. These sentences are constructed from the words in the Arabic word semantic similarity dataset and from different Arabic books, educational texts, and lexicons. The proposed benchmark consists of 1,010 sentence pairs wherein each pair is tagged with scores determining semantic similarity and paraphrasing. The quality of the data is assessed using statistical analysis for the distribution of the sentences over the Arabic transformation rules and exploration through hierarchical clustering (HCL). Our exploration using HCL shows that the sentences in the proposed benchmark are grouped into 27 clusters representing different subjects. The inter-annotator agreement measures show a moderate agreement for the annotations of the graduate students and a poor reliability for the annotations of the undergraduate students.  © 2021 Association for Computing Machinery.",Arabic benchmark; Arabic paraphrasing benchmark; HCL Clustering; inter-annotator agreement; K-means; Paraphrasing; semantic similarity; transformation rules,K-means clustering; Metadata; Quality control; Semantics; Students; Arabic benchmark; Arabic paraphrasing benchmark; Clusterings; Hier-archical clustering; Hierarchical Clustering; Hierarchical clustering clustering; Inter-annotator agreement; K-means; Paraphrasing; Semantic similarity; Transformation rules; Natural language processing systems
Sentiment Analysis of Sinhala News Comments,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115206483&doi=10.1145%2f3445035&partnerID=40&md5=a537d80d01359529f03379e42c0132a8,"Sinhala is a low-resource language, for which basic language and linguistic tools have not been properly defined. This affects the development of NLP-based end-user applications for Sinhala. Thus, when implementing NLP tools such as sentiment analyzers, we have to rely only on language-independent techniques. This article presents the use of such language-independent techniques in implementing a sentiment analysis system for Sinhala news comments. We demonstrate that for low-resource languages such as Sinhala, the use of recently introduced word embedding models as semantic features can compensate for the lack of well-developed language-specific linguistic or language resources, and text classification with acceptable accuracy is indeed possible using both traditional statistical classifiers and Deep Learning models. The developed classification models, a corpus of 8.9 million tokens extracted from Sinhala news articles and user comments, and Sinhala Word2Vec and fastText word embedding models are now available for public use; 9,048 news comments annotated with POSITIVE/NEGATIVE/NEUTRAL polarities have also been released.  © 2021 Association for Computing Machinery.",news comments; sentiment analysis; Sinhala; text classification,Classification (of information); Deep learning; Embeddings; Semantics; Analysis system; Embeddings; End-user applications; Language independents; Low resource languages; News comment; NLP tools; Semantic features; Sentiment analysis; Sinhalum; Sentiment analysis
Dependency Parsing-based Entity Relation Extraction over Chinese Complex Text,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120336982&doi=10.1145%2f3450273&partnerID=40&md5=60f063777a1400484fbacc28429d8666,"Open Relation Extraction (ORE) plays a significant role in the field of Information Extraction. It breaks the limitation that traditional relation extraction must pre-define relational types in the annotated corpus and specific domains restrictions, to realize the goal of extracting entities and the relation between entities in the open domain. However, with the increase of sentence complexity, the precision and recall of Entity Relation Extraction will be significantly reduced. To solve this problem, we present an unsupervised Clause_CORE method based on Chinese grammar and dependency parsing features. Clause_CORE is used for complex sentences processing, including decomposing complex sentence and dynamically complementing sentence components, which can reduce sentences complexity and maintain the integrity of sentences at the same time. Then, we perform dependency parsing for complete sentences and implement open entity relation extraction based on the model constructed by Chinese grammar rules. The experimental results show that the performance of Clause_CORE method is better than that of other advanced Chinese ORE systems on Wikipedia and Sina news datasets, which proves the correctness and effectiveness of the method. The results on mixed datasets of news data and encyclopedia data prove the generalization and portability of the method.  © 2020 Association for Computing Machinery.",Chinese grammar rules; complex sentences processed; dependency parsing; Open entity relation extraction; unsupervised,Formal languages; Chinese grammar rule; Complex sentence processed; Complex sentences; Dependency parsing; Entity relation extractions; Grammar rules; Open entity relation extraction; Precision and recall; Relation extraction; Unsupervised; Syntactics
An Improved English-to-Mizo Neural Machine Translation,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120322147&doi=10.1145%2f3445974&partnerID=40&md5=76110840f02a4a1dc09f6ecb66e941f5,"Machine Translation is an effort to bridge language barriers and misinterpretations, making communication more convenient through the automatic translation of languages. The quality of translations produced by corpus-based approaches predominantly depends on the availability of a large parallel corpus. Although machine translation of many Indian languages has progressively gained attention, there is very limited research on machine translation and the challenges of using various machine translation techniques for a low-resource language such as Mizo. In this article, we have implemented and compared statistical-based approaches with modern neural-based approaches for the English-Mizo language pair. We have experimented with different tokenization methods, architectures, and configurations. The performance of translations predicted by the trained models has been evaluated using automatic and human evaluation measures. Furthermore, we have analyzed the prediction errors of the models and the quality of predictions based on variations in sentence length and compared the model performance with the existing baselines.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",BLEU; low-resource language; METEOR; Mizo; Neural machine translation; TER; transformer,Computational linguistics; Computer aided language translation; Automatic translation; BLEU; Corpus-based approaches; Language barriers; Low resource languages; Machine translations; METEOR; Mizo; TER; Transformer; Neural machine translation
Developing the Persian Wordnet of Verbs Using Supervised Learning,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120309226&doi=10.1145%2f3450969&partnerID=40&md5=b62492b3f67c5925138fb2861b6021eb,"Nowadays, wordnets are extensively used as a major resource in natural language processing and information retrieval tasks. Therefore, the accuracy of wordnets has a direct influence on the performance of the involved applications. This paper presents a fully-automated method for extending a previously developed Persian wordnet to cover more comprehensive and accurate verbal entries. At first, by using a bilingual dictionary, some Persian verbs are linked to Princeton WordNet synsets. A feature set related to the semantic behavior of compound verbs as the majority of Persian verbs is proposed. This feature set is employed in a supervised classification system to select the proper links for inclusion in the wordnet. We also benefit from a pre-existing Persian wordnet, FarsNet, and a similarity-based method to produce a training set. This is the largest automatically developed Persian wordnet with more than 27,000 words, 28,000 PWN synsets and 67,000 word-sense pairs that substantially outperforms the previous Persian wordnet with about 16,000 words, 22,000 PWN synsets and 38,000 word-sense pairs.  © 2021 Association for Computing Machinery.",Ontology; Persian language; verbs; wordnet,Learning algorithms; Natural language processing systems; Semantics; Supervised learning; Features sets; Language informations; Ontology's; Performance; Persian languages; Persians; Synsets; Verb; Word sense; Wordnet; Ontology
Facebook Tells Me Your Gender: An Exploratory Study of Gender Prediction for Turkish Facebook Users,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120314166&doi=10.1145%2f3448253&partnerID=40&md5=3684263661fcf77509c07542140d4548,"Online Social Networks (OSNs) are very popular platforms for social interaction. Data posted publicly over OSNs pose various threats against the individual privacy of OSN users. Adversaries can try to predict private attribute values, such as gender, as well as links/connections. Quantifying an adversary's capacity in inferring the gender of an OSN user is an important first step towards privacy protection. Numerous studies have been made on the problem of predicting the gender of an author/user, especially in the context of the English language. Conversely, studies in this field are quite limited for the Turkish language and specifically in the domain of OSNs. Previous studies for gender prediction of Turkish OSN users have mostly been performed by using the content of tweets and Facebook comments. In this article, we propose using various features, not just user comments, for the gender prediction problem over the Facebook OSN. Unlike existing studies, we exploited features extracted from profile, wall content, and network structure, as well as wall interactions of the user. Therefore, our study differs from the existing work in the broadness of the features considered, machine learning and deep learning methods applied, and the size of the OSN dataset used in the experimental evaluation. Our results indicate that basic profile information provides better results; moreover, using this information together with wall interactions improves prediction quality. We measured the best accuracy value as 0.982, which was obtained by combining profile data and wall interactions of Turkish OSN users. In the wall interactions model, we introduced 34 different features that provide better results than the existing content-based studies for Turkish.  © 2021 Association for Computing Machinery.",attribute inference; Facebook; gender detection; online social networks; text categorization,Deep learning; Forecasting; Text processing; User profile; Attribute inference; Exploratory studies; Facebook; Gender detection; Gender predictions; Network users; Popular platform; Social interactions; Text categorization; Turkishs; Social networking (online)
Learning Syllables Using Conv-LSTM Model for Swahili Word Representation and Part-of-speech Tagging,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120346864&doi=10.1145%2f3445975&partnerID=40&md5=57445e1dccbd8b00768a963be766f30f,"The need to capture intra-word information in natural language processing (NLP) tasks has inspired research in learning various word representations at word, character, or morpheme levels, but little attention has been given to syllables from a syllabic alphabet. Motivated by the success of compositional models in morphological languages, we present a Convolutional-long short term memory (Conv-LSTM) model for constructing Swahili word representation vectors from syllables. The unified architecture addresses the word agglutination and polysemous nature of Swahili by extracting high-level syllable features using a convolutional neural network (CNN) and then composes quality word embeddings with a long short term memory (LSTM). The word embeddings are then validated using a syllable-aware language model (31.267) and a part-of-speech (POS) tagging task (98.78), both yielding very competitive results to the state-of-art models in their respective domains. We further validate the language model using Xhosa and Shona, which are syllabic-based languages. The novelty of the study is in its capability to construct quality word embeddings from syllables using a hybrid model that does not use max-over-pool common in CNN and then the exploitation of these embeddings in POS tagging. Therefore, the study plays a crucial role in the processing of agglutinative and syllabic-based languages by contributing quality word embeddings from syllable embeddings, a robust Conv-LSTM model that learns syllables for not only language modeling and POS tagging, but also for other downstream NLP tasks.  © 2021 Association for Computing Machinery.",Deep learning; language modeling; part-of-speech tagging; syllabic alphabet; word representation,Antigen-antibody reactions; Brain; Computational linguistics; Convolution; Convolutional neural networks; Embeddings; Modeling languages; Natural language processing systems; Compositional models; Convolutional neural network; Deep learning; Embeddings; Language model; Memory modeling; Part of speech tagging; Parts-of-speech tagging; Syllabic alphabet; Word representations; Long short-term memory
"A Neural Joint Model with BERT for Burmese Syllable Segmentation, Word Segmentation, and POS Tagging",2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120328955&doi=10.1145%2f3436818&partnerID=40&md5=e9353916280cfb3c990ab8598151399e,"The smallest semantic unit of the Burmese language is called the syllable. In the present study, it is intended to propose the first neural joint learning model for Burmese syllable segmentation, word segmentation, and part-of-speech (POS) tagging with the BERT. The proposed model alleviates the error propagation problem of the syllable segmentation. More specifically, it extends the neural joint model for Vietnamese word segmentation, POS tagging, and dependency parsing [28] with the pre-training method of the Burmese character, syllable, and word embedding with BiLSTM-CRF-based neural layers. In order to evaluate the performance of the proposed model, experiments are carried out on Burmese benchmark datasets, and we fine-tune the model of multilingual BERT. Obtained results show that the proposed joint model can result in an excellent performance.  © 2021 Association for Computing Machinery.",BERT; BiLSTM-CRF; Burmese; joint training; POS tagging; word segmentation,Backpropagation; Benchmarking; Computational linguistics; Multilayer neural networks; Natural language processing systems; BERT; BiLSTM-CRF; Burmese; Joint models; Joint training; Part of speech tagging; Parts-of-speech tagging; Performance; Syllable segmentation; Word segmentation; Semantics
Query Expansion for Transliterated Text Retrieval,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120345151&doi=10.1145%2f3447649&partnerID=40&md5=55599fdfb0bc33bab1b9d3a204be1ed0,"With Web 2.0, there has been exponential growth in the number of Web users and the volume of Web content. Most of these users are not only consumers of the information but also generators of it. People express themselves here in colloquial languages, but using Roman script (transliteration). These texts are mostly informal and casual, and therefore seldom follow grammar rules. Also, there does not exist any prescribed set of spelling rules in transliterated text. This freedom leads to large-scale spelling variations, which is a major challenge in mixed script information processing. This article studies different existing phonetic algorithms to handle the issue of spelling variation, points out the limitations of them, and proposes a novel phonetic encoding approach with two different flavors in the light of Hindi transliteration. Experiments performed over Hindi song lyrics retrieval in mixed script domain with three different retrieval models show that proposed approaches outperform the existing techniques in a majority of the cases (sometimes statistically significantly) for a number of metrics like nDCG@1, nDCG@5, nDCG@10, MAP, MRR, and Recall.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Mixed script information retrieval; phonetics; query expansion; transliteration,Expansion; Linguistics; Websites; Exponential growth; Grammar rules; Large-scales; Mixed script information retrieval; Query expansion; Text retrieval; Transliteration; Web 2.0; Web content; Web users; Information retrieval
Cross-lingual Adaptation Using Universal Dependencies,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111026083&doi=10.1145%2f3448251&partnerID=40&md5=21524a1eb6906275462c0d6d94f2c7da,"We describe a cross-lingual adaptation method based on syntactic parse trees obtained from the Universal Dependencies (UD), which are consistent across languages, to develop classifiers in low-resource languages. The idea of UD parsing is to capture similarities as well as idiosyncrasies among typologically different languages. In this article, we show that models trained using UD parse trees for complex NLP tasks can characterize very different languages. We study two tasks of paraphrase identification and relation extraction as case studies. Based on UD parse trees, we develop several models using tree kernels and show that these models trained on the English dataset can correctly classify data of other languages, e.g., French, Farsi, and Arabic. The proposed approach opens up avenues for exploiting UD parsing in solving similar cross-lingual tasks, which is very useful for languages for which no labeled data is available.  © 2021 Association for Computing Machinery.",cross-lingual learning; low resource language; tree kernel; Universal dependencies,Classification (of information); Forestry; Trees (mathematics); Adaptation methods; Cross-lingual; Cross-lingual learning; Dependency parser; Dependency parsing; Low resource languages; Parse trees; Syntactic parse tree; Tree kernels; Universal dependency; Syntactics
An Embedding-Based Topic Model for Document Classification,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119338785&doi=10.1145%2f3431728&partnerID=40&md5=e2cc01a646ed17a8f873d47f28e1f85d,"Topic modeling is an unsupervised learning task that discovers the hidden topics in a collection of documents. In turn, the discovered topics can be used for summarizing, organizing, and understanding the documents in the collection. Most of the existing techniques for topic modeling are derivatives of the Latent Dirichlet Allocation which uses a bag-of-word assumption for the documents. However, bag-of-words models completely dismiss the relationships between the words. For this reason, this article presents a two-stage algorithm for topic modelling that leverages word embeddings and word co-occurrence. In the first stage, we determine the topic-word distributions by soft-clustering a random set of embedded n-grams from the documents. In the second stage, we determine the document-topic distributions by sampling the topics of each document from the topic-word distributions. This approach leverages the distributional properties of word embeddings instead of using the bag-of-words assumption. Experimental results on various data sets from an Australian compensation organization show the remarkable comparative effectiveness of the proposed algorithm in a task of document classification. © 2021 Association for Computing Machinery.",clustering; document classification; Topic modelling; word embedding,Classification (of information); Information retrieval; Information retrieval systems; Statistics; Bag of words; Bag-of-words models; Clusterings; Collection of documents; Document Classification; Embeddings; Latent Dirichlet allocation; Topic Modeling; Topic words; Word embedding; Embeddings
A Framework for Extractive Text Summarization Based on Deep Learning Modified Neural Network Classifier,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119378931&doi=10.1145%2f3392048&partnerID=40&md5=6b19fb467b52da7a7da8f11d7210a3c4,"There is an exponential growth of text data over the internet, and it is expected to gain significant growth and attention in the coming years. Extracting meaningful insights from text data is crucially important as it offers value-added solutions to business organizations and end-users. Automatic text summarization (ATS) automates text summarization by reducing the initial size of the text without the loss of key information elements. In this article, we propose a novel text summarization algorithm for documents using Deep Learning Modifier Neural Network (DLMNN) classifier. It generates an informative summary of the documents based on the entropy values. The proposed DLMNN framework comprises six phases. In the initial phase, the input document is pre-processed. Subsequently, the features are extracted using pre-processed data. Next, the most appropriate features are selected using the improved fruit fly optimization algorithm (IFFOA). The entropy value for every chosen feature is computed. These values are then classified into two classes, (a) highest entropy values and (b) lowest entropy values. Finally, the class that holds the highest entropy values is chosen, representing the informative sentences that form the last summary. The results observed from the experiment indicate that the DLMNN classifier gives 81.56, 91.21, and 83.53 of sensitivity, accuracy, specificity, precision, and f-measure. Whereas the existing schemes such as ANN relatively provide lesser value in contrast to DLMNN. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automatic Text Summarization (ATS); Deep Learning Modified Neural Network (DLMNN); Extractive Summarization; Improved Fruit Fly Optimization Algorithm (IFFOA); Krill Herd Optimization Algorithm (KHOA); Single Document Summarization,Data mining; Deep learning; Entropy; Fruits; Learning algorithms; Text processing; Automatic text summarization; Deep learning modified neural network; Extractive summarizations; Fly optimization algorithms; Fruitflies; Improved fruit fly optimization algorithm; Krill herd optimization algorithm; Modified neural networks; Optimization algorithms; Single-document summarizations; Optimization
A Deep Learning-based Approach for Emotions Classification in Big Corpus of Imbalanced Tweets,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119331042&doi=10.1145%2f3410570&partnerID=40&md5=3342abbcb1faf81177a5a3adaabe82cf,"Emotions detection in natural languages is very effective in analyzing the user's mood about a concerned product, news, topic, and so on. However, it is really a challenging task to extract important features from a burst of raw social text, as emotions are subjective with limited fuzzy boundaries. These subjective features can be conveyed in various perceptions and terminologies. In this article, we proposed an IoT-based framework for emotions classification of tweets using a hybrid approach of Term Frequency Inverse Document Frequency (TFIDF) and deep learning model. First, the raw tweets are filtered using the tokenization method for capturing useful features without noisy information. Second, the TFIDF statistical technique is applied to estimate the importance of features locally as well as globally. Third, the Adaptive Synthetic (ADASYN) class balancing technique is applied to solve the imbalance class issue among different classes of emotions. Finally, a deep learning model is designed to predict the emotions with dynamic epoch curves. The proposed methodology is analyzed on two different Twitter emotions datasets. The dynamic epoch curves are shown to show the behavior of test and train data points. It is proved that this methodology outperformed the popular state-of-the-art methods. © 2021 Association for Computing Machinery.",Data mining; deep learning; emotions analysis; NLP; TFIDF,Deep learning; Information filtering; Information retrieval systems; Inverse problems; Natural language processing systems; Text processing; Deep learning; Emotion analysis; Emotion classification; Emotion detection; Important features; Learning models; Learning-based approach; Natural languages; News topics; Term frequencyinverse document frequency (TF-IDF); Data mining
Deep Structured Learning for Natural Language Processing,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119371017&doi=10.1145%2f3433538&partnerID=40&md5=d5ff34d5dc1e07bb49186a46f003ff7f,"The real-time and dissemination characteristics of network information make net-mediated public opinion become more and more important food safety early warning resources, but the data of petabyte (PB) scale growth also bring great difficulties to the research and judgment of network public opinion, especially how to extract the event role of network public opinion from these data and analyze the sentiment tendency of public opinion comment. First, this article takes the public opinion of food safety network as the research point, and a BLSTM-CRF model for automatically marking the role of event is proposed by combining BLSTM and conditional random field organically. Second, the Attention mechanism based on vocabulary in the field of food safety is introduced, the distance-related sequence semantic features are extracted by BLSTM, and the emotional classification of sequence semantic features is realized by using CNN. A kind of Att-BLSTM-CNN model for the analysis of public opinion and emotional tendency in the field of food safety is proposed. Finally, based on the time series, this article combines the role extraction of food safety events and the analysis of emotional tendency and constructs a net-mediated public opinion early warning model in the field of food safety according to the heat of the event and the emotional intensity of the public to food safety public opinion events. © 2021 Association for Computing Machinery.",event role extraction; Food safety; public opinion analysis and early warning; sentiment orientation analysis,Deep learning; Food safety; Natural language processing systems; Random processes; Semantics; Social aspects; Time series analysis; Early warning; Event role extraction; Food-safety; Network public opinions; Opinion analysis; Public opinion analyse and early warning; Public opinions; Real- time; Semantic features; Sentiment orientation analysis; Extraction
Transfer Learning Based Recurrent Neural Network Algorithm for Linguistic Analysis,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119321986&doi=10.1145%2f3406204&partnerID=40&md5=767667addd1b96cfd9ff6102ffa407a6,"Each language is a system of understanding and skills that allows language users to interact, express thoughts, hypotheses, feelings, wishes, and all that needs to be expressed. Linguistics is the research of these structures in all respects: the composition, usage, and sociology of language, in particular, are the core of linguistics. Machine Learning is the research area that allows machines to learn without being specifically scheduled. In linguistics, the design of writing is understood to be a foundation for many distinct company apps and probably the most useful if incorporated with machine learning methods. Research shows that besides text tagging and algorithm training, there are major problems in the field of Big Data. This article provides a collaborative effort (transfer learning integrated into Recurrent Neural Network) to analyze the distinct kinds of writing between the language's linear and non-computational sides, and to enhance granularity. The outcome demonstrates stronger incorporation of granularity into the language from both sides. Comparative results of machine learning algorithms are used to determine the best way to analyze and interpret the structure of the language. © 2021 Association for Computing Machinery.",Linguistics; machine learning algorithm; part-of-speech; sentence analysis; text analytics,Learning algorithms; Linguistics; Learn+; Linguistic analysis; Machine learning algorithms; Machine-learning; Neural networks algorithms; Part Of Speech; Research areas; Sentence analysis; Text analytics; Transfer learning; Recurrent neural networks
"The Effects of Negative Online Reviews on Consumer Perception, Attitude and Purchase Intention: Experimental Investigation of the Amount, Quality, and Presentation Order of eWOM",2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119340241&doi=10.1145%2f3426883&partnerID=40&md5=04ce6d5d45060694ee6dc832093ca581,"The quick growth and fast spread of electronic word-of-mouth (eWOM) have created a new threat to Internet merchants and marketers through paid online reviewers flooding sites with product and service reviews that could confuse and deter customers. This study examined the effects of the posts by paid reviewers-specifically, the negative reviews-on consumers' risk perception, product attitude, and purchase intention. While extant research examined negative eWOM as an information source, little attention has been paid to the role of a hired reviewer's post aimed at destroying the reputation of certain targets (Internet Water Army Attack [IWAA]). To gain a better understanding of this phenomenon, three experiments were conducted to investigate the effects of the amount, quality, and presentation order of negative eWOM on consumers' perception change and decision making. We tested the hypothesis using a test environment that mimicked a PTT online forum (https://www.ptt.cc/) in Taiwan. Three simulation cases (smartphone, digital camera, and tablet) based on real-world events were used. A total of 193 participants completed all three experiments and provided valid responses. The results of this study are mostly consistent with previous research findings that online marketing is greatly threatened by negative eWOM. Nevertheless, it was also found that the effects of the amount, quality, and presentation order of negative eWOM are more complicated than we have anticipated. The findings revealed that IWAA can effectively increase customers' risk perception toward a product and change their attitude and purchase intention. © 2021 Association for Computing Machinery.",Information amount; information quality; negative reviews; presentation order; word-of-mouth,Consumer behavior; Electronic commerce; HTTP; Risk perception; Sales; Consumer attitudes; Consumer perception; Consumer purchase; Experimental investigations; Information amount; Information quality; Negative review; Online reviews; Presentation order; Purchase intention; Purchasing
Design and Development of Heuristic Utility Management Algorithm for Chinese Library Management System,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119381038&doi=10.1145%2f3397968&partnerID=40&md5=c4850c10623e228bbb10da3222f0b446,"Utility Management in a library is the programmatic tool with the synthetic mental program ability, along with Artificial Intelligence capacities, headed to manage a high volume of books, articles, and assignments, which help to ease the manual significance of librarians. This computerized machine code helps librarians to deal with various databases of the library management system. This framework keeps the records of all the resource details in an optimized manner. It uses a utility management software code with an optimized search classifier that helps to deal with the resource of the library. In this work, the Heuristic Utility Management Algorithm (HUMA) has been used to keep track of resources in the library using mathematical modeling and standardized programmatic computation on tags, which relates the decode scanner to parse the input information. HUMA helps to reduce the manual routine work done by the librarians, and it has been analyzed in this research with prominent survey outcomes based on experimental validation. © 2021 Association for Computing Machinery.",artificial intelligence; database; heuristic approach; library management; Utility management,Artificial intelligence; Information management; Libraries; Optimization; Design and Development; Heuristics approaches; High volumes; Library management; Machine codes; Management software; Management systems; Programmatics; Software codes; Utility management; Heuristic methods
TAMIZHI: Historical Tamil-Brahmi Script Recognition Using CNN and MobileNet,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119365923&doi=10.1145%2f3402891&partnerID=40&md5=ec44f58d13e2160fca9420ea5c9e4794,"Computational epigraphy is the study of an ancient script where the computer science and mathematical model is relatively built for epigraphy. The Tamil-Brahmi inscriptions are the most ancient of the extant written of the Tamil. The inscriptions furnish valuable information on many aspects of life in the ancient Tamil country from a period anterior to the literary age of Sangam. The recognition of the script and systematic analysis of the script is required. The recognition of this script is complex, containing various curves for a single character and the style of writing overlap with curves and lines. Generating corpus of the script is necessary, since it is the initial step for computational epigraphy. The archaeological department has supported the raw data that helped to develop a corpus of Tamizhi. In this article, we have implemented a convolution neural network in various ways, i.e., (i) Training the CNN model from scratch a Softmax classifier in a sequential model (ii) using MobileNet: Transfer learning paradigm from a pre-trained model on a Tamizhi dataset (iii) Building Model with CNN and SVM (iv) SVM for evaluation of best accuracy to recognize handwritten Brahmi characters. To train the CNN Model an extensive TAMIZHI handwritten Brahmi Dataset of 1lakh and 90,000 isolated samples for the character has been created and deployed. The designed dataset consists of 9 vowels and 18 consonants and 209 class so researchers can use machine learning. MobileNet outperformed among all the models implemented with the accuracy of 68.3%, whereas other algorithm ranges from 58% to 67% with respect to the Tamizhi dataset. MobileNet model is trained and tested for the dataset of vowels (8 class), consonants (18 class), and consonants vowels (26 class) with the accuracy of 98.1%, 97.7%, 97.5%, respectively. © 2021 Association for Computing Machinery.",Brahmi; convolutional neural network (CNN); handwritten script recognition; MobileNet; TAMIZHI,Character recognition; Classification (of information); Convolution; Convolutional neural networks; Support vector machines; 'Brahmi'; Convolution neural network; Convolutional neural network; Handwritten script recognition; Mobilenet; Neural network model; Sequential modeling; Systematic analysis; TAMIZHI; Linguistics
A Two-stage Text Feature Selection Algorithm for Improving Text Classification,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119376094&doi=10.1145%2f3425781&partnerID=40&md5=873efb9ab2e5ced124595f4a19e243d8,"As the number of digital text documents increases on a daily basis, the classification of text is becoming a challenging task. Each text document consists of a large number of words (or features) that drive down the efficiency of a classification algorithm. This article presents an optimized feature selection algorithm designed to reduce a large number of features to improve the accuracy of the text classification algorithm. The proposed algorithm uses noun-based filtering, a word ranking that enhances the performance of the text classification algorithm. Experiments are carried out on three benchmark datasets, and the results show that the proposed classification algorithm has achieved the maximum accuracy when compared to the existing algorithms. The proposed algorithm is compared to Term Frequency-Inverse Document Frequency, Balanced Accuracy Measure, GINI Index, Information Gain, and Chi-Square. The experimental results clearly show the strength of the proposed algorithm. © 2021 Association for Computing Machinery.",Feature selection; text classification; text feature extraction; text feature optimization,Classification (of information); Information retrieval systems; Text processing; Classification algorithm; Feature selection algorithm; Features extraction; Features optimizations; Features selection; Text document; Text feature; Text feature extraction; Text feature optimization; Feature extraction
Study on Automated Approach to Recognize Characters for Handwritten and Historical Document,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119351706&doi=10.1145%2f3396167&partnerID=40&md5=c093842c46a8d866fb6d4e7b700eb277,"Script recognition is the mechanism of automatic script analysis and recognition whereby intensive study has been carried out and a significant amount of papers on this problem have been released over the past. But there are still a few issues to be solved, particularly in Indian historical manuscripts. This literature examines the Script recognition with reference to multi-script document and different historical scripts such as Kurdish-Latin, Devanagari, Grantha, Arabic handwritten characters, Bangladesh, Devanagari and Gurumukhi, ancient Chinese, Arabic, Nam Character, Greek, Nastalique Urdu, Georgian handwritten, Nandinagari, and Hebrew, which provide the course of study that focuses on the framework for script recognition. This review concentrates on scope of prediction, dataset type, the methods used for data preprocessing, and measures of performance used for analysis. On the basis of this survey, Current research constraints have been recognized and future study specifications are emphasized in the area of modeling historical manuscripts.CCS Concepts:  © 2021 Association for Computing Machinery.",Historical manuscripts; multi-script document; script recognition,Character recognition; 'current; Automated approach; Bangladesh; Data preprocessing; Hand-written characters; Handwritten document; Historical documents; Measure of performance; Multi-script document; Script recognition; History
SACNN: Self-attentive Convolutional Neural Network Model for Natural Language Inference,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113150053&doi=10.1145%2f3426884&partnerID=40&md5=8af85b461ab9b4e097a1a1d890e46078,"Inference has been central problem for understanding and reasoning in artificial intelligence. Especially, Natural Language Inference is an interesting problem that has attracted the attention of many researchers. Natural language inference intends to predict whether a hypothesis sentence can be inferred from the premise sentence. Most prior works rely on a simplistic association between the premise and hypothesis sentence pairs, which is not sufficient for learning complex relationships between them. The strategy also fails to exploit local context information fully. Long Short Term Memory (LSTM) or gated recurrent units networks (GRU) are not effective in modeling long-term dependencies, and their schemes are far more complex as compared to Convolutional Neural Networks (CNN). To address this problem of long-term dependency, and to involve context for modeling better representation of a sentence, in this article, a general Self-Attentive Convolution Neural Network (SACNN) is presented for natural language inference and sentence pair modeling tasks. The proposed model uses CNNs to integrate mutual interactions between sentences, and each sentence with their counterparts is taken into consideration for the formulation of their representation. Moreover, the self-attention mechanism helps fully exploit the context semantics and long-term dependencies within a sentence. Experimental results proved that SACNN was able to outperform strong baselines and achieved an accuracy of 89.7% on the stanford natural language inference (SNLI) dataset. © 2021 Association for Computing Machinery.",attention; convolutional neural network; machine learning; Natural language inference,Complex networks; Convolution; Convolutional neural networks; Modeling languages; Natural language processing systems; Semantics; Attention; Central problems; Complex relationships; Convolution neural network; Convolutional neural network; Language inference; Long-term dependencies; Natural language inference; Natural languages; Neural network model; Long short-term memory
Heuristic Bilingual Graph Corpus Network to Improve English Instruction Methodology Based on Statistical Translation Approach,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119323076&doi=10.1145%2f3406205&partnerID=40&md5=c8203db43465bd174baafb3dec704e50,"The number of sentence pairs in the bilingual corpus is a key to translation accuracy in computational machine translations. However, if the amount goes beyond a certain degree, the increasing number of cases has less impact on the translation while the construction of translation systems requires a considerable amount of time and energy, thus preventing the development of a statistical translation by the computer. This article offers a number of classifications for measuring the amount of information for each pair of sentences, using the Heuristic Bilingual Graph Corpus Network (HBGCN) to form an improved method of corpus selection that takes the difference between the first amount of information between the pairs of sentences into account. Using a graphic-based selector method as a training set, they achieve a close translation result through our experiments with the whole body and achieve better results than basic results for the following based on the Document Inverse Frequency (DIF) ranking approach. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",corpus selection method; document inverse frequency; Heuristic bilingual graph; machine translation,Classification (of information); Computational linguistics; Heuristic methods; Inverse problems; Machine translation; Amount of information; Bilinguals; Corpus networks; Corpus selection method; Corpus selections; Document inverse frequency; Heuristic bilingual graph; Machine translations; Selection methods; Statistical translation; Computer aided language translation
An Analysis for Elements of Affecting the Establishment and Promotion of Micro-business Trust in C2C Model under WeChat Circumstance,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119326989&doi=10.1145%2f3398011&partnerID=40&md5=bbe38313974cedd99d0ef6029853616d,"The core of micro-business and consumer transactions is trust. Based on the Theory of Reasoned Action and Technology Acceptance Model, this article discusses the factors of the establishment and promotion of micro-business trust from the trust orientation of consumer, the trust of WeChat businesses, and the trust of WeChat platform. Data were obtained by questionnaire, and SPSS software was used for data reliability and multiple regression analysis. It is concluded that all three levels have a significant positive impact on the establishment and promotion of C2C mode micro-business trust. The trust of WeChat businesses and the trust of WeChat platform have a greater influence on the establishment of micro-business trust. The trust orientation of consumer and the trust of WeChat businesses have a greater impact on the promotion of micro-business trust. Among them, the WeChat business trust level is the most important factor. © 2021 Association for Computing Machinery.",establishment and promotion of trust; micro-business; WeChat circumstance,Reliability analysis; Software reliability; Data reliability; Establishment and promotion of trust; Micro-business; Multiple regression analysis; Technology acceptance model; Theory of reasoned action; Three-level; Trust level; Wechat circumstance; Regression analysis
Semantic Graphical Dependence Parsing Model in Improving English Teaching Abilities,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119351731&doi=10.1145%2f3425633&partnerID=40&md5=8113b4948bc9ccdec59a830fe4dc80ea,"It is a very difficult problem to achieve high-order functionality for graphical dependency parsing without growing decoding difficulties. To solve this problem, this article offers a way for Semantic Graphical Dependence Parsing Model (SGDPM) with a language-dependency model and a beam search to represent high-order functions for computer applications. The first approach is to scan a large amount of unnoticed data using a baseline parser. It will build auto-parsed data to create the Language-dependence Model (LDM). The LDM is based on a set of new features during beam search decoding, where it will incorporate the LDM features into the parsing model and utilize the features in parsing models of bilingual text. Our approach has main benefits, which include rich high-order features that are described given the large size and the additional large crude corpus for increasing the difficulty of decoding. Further, SGDPM has been evaluated using the suggested method for parsing tasks of mono-parsing text and bi-parsing text to carry out experiments on the English and Chinese data in the mono-parsing text function using computer applications. Experimental results show that the most accurate Chinese data is obtained with the best known English data systems and their comparable accuracy. Furthermore, the lab-scale experiments on the Chinese/General bilingual information in the bitext parsing process outperform the best recorded existing solutions. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",and Bi-Parsing Text; Dependence Parsing; Mono-parsing Text; Semantic Graphical,Decoding; And bi-parsing text; Beam search; Dependence model; Dependence parsing; Dependency parsing; English teaching; High-order; Higher-order; Mono-parsing text; Semantic graphical; Semantics
Introduction to the Special Issue on Deep Structured Learning for Natural Language Processing,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119383532&doi=10.1145%2f3474087&partnerID=40&md5=f3f295fc9dd2665c28b596d126284bc4,[No abstract available],,
Using Sub-character Level Information for Neural Machine Translation of Logographic Languages,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105742442&doi=10.1145%2f3431727&partnerID=40&md5=de43d858a813666feb373c14957e9bba,"Logographic and alphabetic languages (e.g., Chinese vs. English) have different writing systems linguistically. Languages belonging to the same writing system usually exhibit more sharing information, which can be used to facilitate natural language processing tasks such as neural machine translation (NMT). This article takes advantage of the logographic characters in Chinese and Japanese by decomposing them into smaller units, thus more optimally utilizing the information these characters share in the training of NMT systems in both encoding and decoding processes. Experiments show that the proposed method can robustly improve the NMT performance of both ""logographic""language pairs (JA-ZH) and ""logographic + alphabetic""(JA-EN and ZH-EN) language pairs in both supervised and unsupervised NMT scenarios. Moreover, as the decomposed sequences are usually very long, extra position features for the transformer encoder can help with the modeling of these long sequences. The results also indicate that, theoretically, linguistic features can be manipulated to obtain higher share token rates and further improve the performance of natural language processing systems. © 2021 Copyright held by the owner/author(s).",logographic languages; Neural machine translation; shared information; unsupervised NMT,Computational linguistics; Computer aided language translation; Information dissemination; Signal encoding; Character level; Encoding and decoding; Linguistic features; Long sequences; Machine translations; NAtural language processing; Sharing information; Writing systems; Natural language processing systems
A Hindi Image Caption Generation Framework Using Deep Learning,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105719699&doi=10.1145%2f3432246&partnerID=40&md5=cc5e1575bd8f9dc9b5f512312d2d6096,"Image captioning is the process of generating a textual description of an image that aims to describe the salient parts of the given image. It is an important problem, as it involves computer vision and natural language processing, where computer vision is used for understanding images, and natural language processing is used for language modeling. A lot of works have been done for image captioning for the English language. In this article, we have developed a model for image captioning in the Hindi language. Hindi is the official language of India, and it is the fourth most spoken language in the world, spoken in India and South Asia. To the best of our knowledge, this is the first attempt to generate image captions in the Hindi language. A dataset is manually created by translating well known MSCOCO dataset from English to Hindi. Finally, different types of attention-based architectures are developed for image captioning in the Hindi language. These attention mechanisms are new for the Hindi language, as those have never been used for the Hindi language. The obtained results of the proposed model are compared with several baselines in terms of BLEU scores, and the results show that our model performs better than others. Manual evaluation of the obtained captions in terms of adequacy and fluency also reveals the effectiveness of our proposed approach.Availability of resources: The codes of the article are available at https://github.com/santosh1821cs03/Image_Captioning_Hindi_Language; The dataset will be made available: http://www.iitp.ac.in/g1/4ai-nlp-ml/resources.html. © 2021 Association for Computing Machinery.",attention; deep-learning; hindi; Image captioning,Computer vision; Modeling languages; Natural language processing systems; Attention mechanisms; English languages; Image captioning; Language model; NAtural language processing; Official languages; Spoken languages; Textual description; Deep learning
Arabic Diacritic Recovery Using a Feature-rich biLSTM Model,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105740810&doi=10.1145%2f3434235&partnerID=40&md5=2c5e3c5dc8cbdbdfeb443df7353e507b,"Diacritics (short vowels) are typically omitted when writing Arabic text, and readers have to reintroduce them to correctly pronounce words. There are two types of Arabic diacritics: The first are core-word diacritics (CW), which specify the lexical selection, and the second are case endings (CE), which typically appear at the end of word stems and generally specify their syntactic roles. Recovering CEs is relatively harder than recovering core-word diacritics due to inter-word dependencies, which are often distant. In this article, we use feature-rich recurrent neural network model that use a variety of linguistic and surface-level features to recover both core word diacritics and case endings. Our model surpasses all previous state-of-The-Art systems with a CW error rate (CWER) of 2.9% and a CE error rate (CEER) of 3.7% for Modern Standard Arabic (MSA) and CWER of 2.2% and CEER of 2.5% for Classical Arabic (CA). When combining diacritized word cores with case endings, the resultant word error rates are 6.0% and 4.3% for MSA and CA, respectively. This highlights the effectiveness of feature engineering for such deep neural models. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Arabic; diacritization; text tagging,Errors; Recovery; Arabic texts; Error rate; Feature engineerings; Modern standards; Neural models; Recurrent neural network model; State-of-the-art system; Word error rate; Recurrent neural networks
Multi-level Chunk-based Constituent-To-Dependency Treebank Transformation for Tibetan Dependency Parsing,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105725378&doi=10.1145%2f3424247&partnerID=40&md5=32f95e8279628a966b9e882d6e1f5fbd,"Dependency parsing is an important task for Natural Language Processing (NLP). However, a mature parser requires a large treebank for training, which is still extremely costly to create. Tibetan is a kind of extremely low-resource language for NLP, there is no available Tibetan dependency treebank, which is currently obtained by manual annotation. Furthermore, there are few related kinds of research on the construction of treebank. We propose a novel method of multi-level chunk-based syntactic parsing to complete constituent-To-dependency treebank conversion for Tibetan under scarce conditions. Our method mines more dependencies of Tibetan sentences, builds a high-quality Tibetan dependency tree corpus, and makes fuller use of the inherent laws of the language itself. We train the dependency parsing models on the dependency treebank obtained by the preliminary transformation. The model achieves 86.5% accuracy, 96% LAS, and 97.85% UAS, which exceeds the optimal results of existing conversion methods. The experimental results show that our method has the potential to use a low-resource setting, which means we not only solve the problem of scarce Tibetan dependency treebank but also avoid needless manual annotation. The method embodies the regularity of strong knowledge-guided linguistic analysis methods, which is of great significance to promote the research of Tibetan information processing. © 2021 Association for Computing Machinery.",Knowledge-driven; Low-resource dependency parsing; Multi-level chunk mechanism; Tibetan dependency trees,Natural language processing systems; Syntactics; Conversion methods; Dependency parsing; Linguistic analysis; Low resource languages; Low-resource settings; Manual annotation; NAtural language processing; Tibetan information processing; Forestry
"Taming the Wild Etext: Managing, Annotating, and Sharing Tibetan Corpora in Open Spaces",2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105713104&doi=10.1145%2f3418060&partnerID=40&md5=451e3c595667c9174c043283f570d7b2,"Digital text is quickly becoming essential to modern daily life. The article you are reading right now is born digital; unlike texts of the not-so-distant past, it may never be printed at all. Worldwide, the trend is clear: Digital text is on the way in, and print is on its way out. Year-by-year, more and more readers are turning to ebooks, internet news, and other forms of ereading, while generation by generation, print is becoming less and less relevant.11Pew research shows 50% of Americans have a dedicated ereading device, with yearly gains in ereadership [1]; industry research, too, shows a definite trend toward ereading and non-Traditional publishing, with ebooks making up 50% of fiction reading in 2016 [2], while journalism is also trending online [3].These trends are not unique to English-to meet the demands and expectations of today's readers, Tibetan texts, too, are being digitized by many organizations and institutions with a shared appreciation for the Tibetan literary heritage. They include a variety of secular publishers, monastic institutions, and Buddhist foundations, among others. But while these organizations share common goals for common texts, their work is all too frequently completely disconnected from the community at large.This situation negatively impacts what is already a minoritized and under-resourced language. While competition-from other languages, as well as other publishers in the Tibetan etext world-has been a driver of innovation in the adoption of ereading technology, we believe that a rich, shared data source is not only in everyone's best interest but also the only practical way forward when we consider the time, effort, expertise, and money that quality digitization takes.That is why we have designed OpenPecha to be a public, open platform for collaborative etext curation and annotation sharing. Its aim is providing a wide range of users with the latest version of the exact ""view""of any text needed, while maintaining the integrity of the text and its annotations and simultaneously allowing for community improvements and additions. In this article, we explore the details of how the project came to be, what it is, and how it works, while also presenting a few common use cases. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",annotation transfer; annotations; corpora; data; data management; Etext; format; Tibetan; toolkit,Electronic publishing; Curation; Daily lives; Digital text; Industry research; Non-traditional; Open platforms; Shared data; Under-resourced languages; Data Sharing
Plan Optimization to Bilingual Dictionary Induction for Low-resource Language Families,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105732473&doi=10.1145%2f3448215&partnerID=40&md5=ce057d06370887af9873c37649035723,"Creating bilingual dictionary is the first crucial step in enriching low-resource languages. Especially for the closely related ones, it has been shown that the constraint-based approach is useful for inducing bilingual lexicons from two bilingual dictionaries via the pivot language. However, if there are no available machine-readable dictionaries as input, we need to consider manual creation by bilingual native speakers. To reach a goal of comprehensively create multiple bilingual dictionaries, even if we already have several existing machine-readable bilingual dictionaries, it is still difficult to determine the execution order of the constraint-based approach to reducing the total cost. Plan optimization is crucial in composing the order of bilingual dictionaries creation with the consideration of the methods and their costs. We formalize the plan optimization for creating bilingual dictionaries by utilizing Markov Decision Process (MDP) with the goal to get a more accurate estimation of the most feasible optimal plan with the least total cost before fully implementing the constraint-based bilingual lexicon induction. We model a prior beta distribution of bilingual lexicon induction precision with language similarity and polysemy of the topology as and parameters. It is further used to model cost function and state transition probability. We estimated the cost of all investment plans as a baseline for evaluating the proposed MDP-based approach with total cost as an evaluation metric. After utilizing the posterior beta distribution in the first batch of experiments to construct the prior beta distribution in the second batch of experiments, the result shows 61.5% of cost reduction compared to the estimated all investment plans and 39.4% of cost reduction compared to the estimated MDP optimal plan. The MDP-based proposal outperformed the baseline on the total cost. © 2021 Association for Computing Machinery.",closely related languages; low-resource languages; pivot-based bilingual lexicon induction; Plan optimization,Cost estimating; Cost functions; Investments; Markov processes; Natural language processing systems; Topology; Accurate estimation; Beta distributions; Bilingual dictionary; Bilingual lexicons; Low resource languages; Machine-readable dictionaries; Markov Decision Processes; State transition probabilities; Cost reduction
Finding Better Subwords for Tibetan Neural Machine Translation,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105760121&doi=10.1145%2f3448216&partnerID=40&md5=5a0e7361cc1ffe5c498f930987011ba8,"Subword segmentation plays an important role in Tibetan neural machine translation (NMT). The structure of Tibetan words consists of two levels. First, words consist of a sequence of syllables, and then a syllable consists of a sequence of characters. According to this special word structure, we propose two methods for Tibetan subword segmentation, namely syllable-based and character-based methods. The former generates subwords based on the Tibetan syllables, and the latter is based on Tibetan characters. In addition, we carry out experiments with these two subword segmentation methods on low-resource Tibetan-To-Chinese NMT, respectively. The experimental results show that both of them can improve translation performance, in which the subword segmentation based on character sequences can achieve better results. Overall, our proposed character-based subword segmentation is more simple and effective. Moreover, it can achieve better experimental results without paying much attention to the linguistic features of Tibetan. © 2021 Copyright held by the owner/author(s).",low resource; Neural machine translation; subword; Tibetan,Computer aided language translation; Character-based methods; Linguistic features; Machine translations; Segmentation methods; Special word; Sub words; Tibetans; Computational linguistics
Sanskrit Parsing following Indian Theories of Verbal Cognition,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105734587&doi=10.1145%2f3418061&partnerID=40&md5=cee3b6c4e56cc1418c3f334bc0895b51,"Pini's grammar is an important milestone in the Indian grammatical tradition. Unlike grammars of other languages, it is almost exhaustive and together with the theories of śAbdabodha (verbal cognition), this grammar provides a system for language analysis as well as generation. The theories of śAbdabodha describe three conditions necessary for verbal cognition. They are £A (expectancy), yogyatA (meaning congruity), and sannidhi (proximity). We examine them from a computational viewpoint and provide appropriate computational models for their representation. Next, we describe the design of a parser following the theories of śAbdabodha and present three algorithms for solving the constraints imposed by the theories of śAbdabodha. The first algorithm is modeled as a constraint satisfaction problem, the second one as a vertex-centric graph traversal, and the third one as an edge-centric binary join, each one being an improvement over the previous one. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",constraint programming; dependency parser; expectancy; free word order; graph traversal; Indian theories of verbal cognition; projectivity; sannidhi; Sanskrit; selectional restriction; yogyatA; £A; śAbdabodha,Constraint satisfaction problems; Graph algorithms; Computational model; Graph traversals; Language analysis; Computation theory
A Hybrid Model for Named Entity Recognition on Chinese Electronic Medical Records,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105759882&doi=10.1145%2f3436819&partnerID=40&md5=40a3530920018364656ab5498e280fdd,"Electronic medical records (EMRs) contain valuable information about the patients, such as clinical symptoms, diagnostic results, and medications. Named entity recognition (NER) aims to recognize entities from unstructured text, which is the initial step toward the semantic understanding of the EMRs. Extracting medical information from Chinese EMRs could be a more complicated task because of the difference between English and Chinese. Some researchers have noticed the importance of Chinese NER and used the recurrent neural network or convolutional neural network (CNN) to deal with this task. However, it is interesting to know whether the performance could be improved if the advantages of the RNN and CNN can be both utilized. Moreover, RoBERTa-WWM, as a pre-Training model, can generate the embeddings with word-level features, which is more suitable for Chinese NER compared with Word2Vec. In this article, we propose a hybrid model. This model first obtains the entities identified by bidirectional long short-Term memory and CNN, respectively, and then uses two hybrid strategies to output the final results relying on these entities. We also conduct experiments on raw medical records from real hospitals. This dataset is provided by the China Conference on Knowledge Graph and Semantic Computing in 2019 (CCKS 2019). Results demonstrate that the hybrid model can improve performance significantly. © 2021 Association for Computing Machinery.",Chinese electronic medical records; hybrid models; Named entity recognition; neural networks,Character recognition; Convolutional neural networks; Diagnosis; Knowledge representation; Medical computing; Semantics; Tantalum compounds; Electronic medical record; Electronic medical records (EMRs); Improve performance; Medical information; Named entity recognition; Semantic Computing; Semantic understanding; Unstructured texts; Recurrent neural networks
Applying Text Analytics to the Mind-section Literature of the Tibetan Tradition of the Great Perfection,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105746600&doi=10.1145%2f3392047&partnerID=40&md5=fbaef8dac1817bb52f94de449a468fde,"Over the past decade, through a mixture of optical character recognition and manual input, there is now a growing corpus of Tibetan literature available as e-Texts in Unicode format. With the creation of such a corpus, the techniques of text analytics that have been applied in the analysis of English and other modern languages may now be applied to Tibetan. In this work, we narrow our focus to examine a modest portion of that literature, the Mind-section portion of the literature of the Tibetan tradition of the Great Perfection. Here, we will use the lens of text analytics tools based on machine learning techniques to investigate a number of questions of interest to scholars of this and related traditions of the Great Perfection. It has been necessary for us to participate in all portions of this process: corpora identification and text edition selection, rendering the text as e-Texts in Unicode using both Optical Character Recognition and manual entry, data cleaning and transformation, implementation of software for text analysis, and interpretation of results. For this reason, we hope this study can serve as a model for other low-resource languages that are just beginning to approach the problem of providing text analytics for their language. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Digital humanities; natural language processing for low-resource languages; text analytics,Learning systems; Metadata; Optical character recognition; Data cleaning; Low resource languages; Modern languages; On-machines; Text analysis; Text analytics; Tibetans; Unicodes; Text mining
Toward a Sustainable Handling of Interlinear-Glossed Text in Language Documentation,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105737286&doi=10.1145%2f3389010&partnerID=40&md5=ef40ea194ac17d434a8f605b9b8751be,"While the amount of digitally available data on the worlds' languages is steadily increasing, with more and more languages being documented, only a small proportion of the language resources produced are sustainable. Data reuse is often difficult due to idiosyncratic formats and a negligence of standards that could help to increase the comparability of linguistic data. The sustainability problem is nicely reflected in the current practice of handling interlinear-glossed text, one of the crucial resources produced in language documentation. Although large collections of glossed texts have been produced so far, the current practice of data handling makes data reuse difficult. In order to address this problem, we propose a first framework for the computer-Assisted, sustainable handling of interlinear-glossed text resources. Building on recent standardization proposals for word lists and structural datasets, combined with state-of-The-Art methods for automated sequence comparison in historical linguistics, we show how our workflow can be used to lift a collection of interlinear-glossed Qiang texts (an endangered language spoken in Sichuan, China), and how the lifted data can assist linguists in their research. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",computer-Assisted language comparison; interlinear-glossed text; qiang; Sino-Tibetan; standardization,"Linguistics; Computer assisted; Current practices; Endangered languages; Language resources; Linguistic data; Sequence comparisons; Sichuan , China; State-of-the-art methods; Data handling"
Recognition of Tibetan Maximal-length Noun Phrases Based on Syntax Tree,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105713269&doi=10.1145%2f3423324&partnerID=40&md5=0e590b16439cfd331ba92ef73904faee,"Frequently corresponding to syntactic components, the Maximal-length Noun Phrase (MNP) possesses abundant syntactic and semantic information and acts a certain semantic role in sentences. Recognition of MNP plays an important role in Natural Language Processing and lays the foundation for analyzing and understanding sentence structure and semantics. By comparing the essence of different MNPs, this article defines the MNP in the Tibetan language from the perspective of syntax tree. A total of 6,038 sentences are extracted from the syntax tree corpus, the structure type, boundary feature, and frequency of MNPs are analyzed, and the MNPs are recognized by applying the sequence tagging model and the syntactic analysis model. The accuracy, recall, and F1 score of the recognition results of applying sequence tagging model are 87.14%, 84.72%, and 85.92%, respectively. The accuracy, recall, and F1 score of the recognition results of applying syntactic analysis model are 87.66%, 87.63%, and 87.65%, respectively. © 2021 Association for Computing Machinery.",the Maximal-length Noun Phrase; Tibetan syntax tree; type of noun phrase,Computational grammars; Natural language processing systems; Semantics; Trees (mathematics); NAtural language processing; Noun phrase; Semantic information; Semantic roles; Sentence structures; Structure type; Syntactic analysis; Tagging models; Syntactics
A Hierarchical Sequence-To-Sequence Model for Korean POS Tagging,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105727255&doi=10.1145%2f3421762&partnerID=40&md5=6ba072d9bd134bb3ef1ebbb222993a82,"Part-of-speech (POS) tagging is a fundamental task in natural language processing. Korean POS tagging consists of two subtasks: morphological analysis and POS tagging. In recent years, scholars have tended to use the seq2seq model to solve this problem. The full context of a sentence is considered in these seq2seq-based Korean POS tagging methods. However, Korean morphological analysis relies more on local contextual information, and in many cases, there exists one-To-one matching between morpheme surface form and base form. To make better use of these characteristics, we propose a hierarchical seq2seq model. In our model, the low-level Bi-LSTM encodes the syllable sequence, whereas the high-level Bi-LSTM models the context information of the whole sentence, and the decoder generates the morpheme base form syllables as well as the POS tags. To improve the accuracy of the morpheme base form recovery, we introduced the convolution layer and the attention mechanism to our model. The experimental results on the Sejong corpus show that our model outperforms strong baseline systems in both morpheme-level F1-score and eojeol-level accuracy, achieving state-of-The-Art performance. © 2021 Association for Computing Machinery.",convolution; hierarchical; Korean POS tagging; sequence-To-sequence,Computational linguistics; Natural language processing systems; Attention mechanisms; Context information; Hierarchical sequences; Local contextual information; Morphological analysis; NAtural language processing; Part of speech tagging; State-of-the-art performance; Long short-term memory
Chinese Spelling Error Detection Using a Fusion Lattice LSTM,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105756101&doi=10.1145%2f3426882&partnerID=40&md5=fd5755e401db44f982aa32465fbfde28,"Spelling error detection serves as a crucial preprocessing in many natural language processing applications. Unlike English, where every single word is directly typed by keyboard, we have to use an input method to input Chinese characters. The pinyin input method is the most widely used. By intuition, pinyin should be helpful in detecting spelling errors. However, when detect spelling errors, most of the current methods ignore the pinyin information and adopt a pipeline framework that leads to error propagation. In this article, we propose a fusion lattice-LSTM model under the end-To-end framework to integrate character, word, and pinyin features for error detection. Experiments on the SIGHAN Bake-off-2015 dataset show that pinyin is a discriminating feature, and our end-To-end model outperforms the baseline models obviously. © 2021 Association for Computing Machinery.",Chinese spelling error; neural networks; spelling check,Error detection; Natural language processing systems; Baseline models; Chinese characters; End-to-end models; Error propagation; Input methods; Natural language processing applications; Single words; Spelling errors; Long short-term memory
A Joint Model for Representation Learning of Tibetan Knowledge Graph Based on Encyclopedia,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105736079&doi=10.1145%2f3447248&partnerID=40&md5=ec1145a61467f2bf4e7a4f8ccd6e87ad,"Learning the representation of a knowledge graph is critical to the field of natural language processing. There is a lot of research for English knowledge graph representation. However, for the low-resource languages, such as Tibetan, how to represent sparse knowledge graphs is a key problem. In this article, aiming at scarcity of Tibetan knowledge graphs, we extend the Tibetan knowledge graph by using the triples of the high-resource language knowledge graphs and Point of Information map information. To improve the representation learning of the Tibetan knowledge graph, we propose a joint model to merge structure and entity description information based on the Translating Embeddings and Convolution Neural Networks models. In addition, to solve the segmentation errors, we use character and word embedding to learn more complex information in Tibetan. Finally, the experimental results show that our model can make a better representation of the Tibetan knowledge graph than the baseline. © 2021 Association for Computing Machinery.",encyclopedia; joint model; knowledge graph; representation learning; Tibetan,Embeddings; Graphic methods; Learning systems; Natural language processing systems; Complex information; Convolution neural network; Description information; Information map; Knowledge graphs; Low resource languages; NAtural language processing; Segmentation error; Knowledge representation
Recent Developments in Tibetan NLP,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105739864&doi=10.1145%2f3453692&partnerID=40&md5=11275abb9b86b1d39dfae685ca7a0d56,[No abstract available],,
A Systematic Review on Hadith Authentication and Classification Methods,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105708542&doi=10.1145%2f3434236&partnerID=40&md5=14983e6b1564af12ba5dc44b6b0f3d56,"Background: A hadith refers to sayings, actions, and characteristics of the Prophet Muhammad peace be upon him. The authenticity of hadiths is crucial, because they constitute the source of legislation for Muslims with the Holy Quran. Classifying hadiths into groups is a matter of importance as well, to make them easy to search and recognize.Objective: To report the results of a systematic review concerning hadith authentication and classification methods.Data sources: Original articles found in ACM, IEEE Xplore, ScienceDirect, Scopus, Web of Science, Springer Link, and Wiley Online Library.Study selection criteria: Only original articles written in English and dealing with hadith authentication and classification. Reviews, editorial, letters, grey literature, and restricted or incomplete articles are excluded.Data extraction: Two authors were assigned to extract data using a predefined data extraction form to answer research questions and assess studies quality.Results: A total of 27 studies were included in this review. There are 14 studies in authentication and 13 studies in classification. Most of the selected studies (17 of 27) were published in conferences, while the others (10 of 27) were published in scientific journals. Research in the area of hadith authentication and classification has received more attention in recent years (2016-2019).Conclusions: Hadith authentication methods are classified into machine learning, rule-based, and a hybrid of rule-based and machine learning and rule-based and statistical methods. Hadith classification methods are classified into machine learning and rule-based. All classification studies used Matn, while the majority of authentication studies used isnad. As a dataset source, Sahih Al-Bukhari was used by most studies. None of the used datasets is publicly available as a benchmark dataset, either in hadith authentication or classification. Recall and Precision are the most frequent evaluation metrics used by the selected studies. © 2021 Association for Computing Machinery.",authentication; classification; Hadith; systematic review,Authentication; Extraction; Machine learning; Authentication methods; Benchmark datasets; Classification methods; Evaluation metrics; Recall and precision; Research questions; Scientific journals; Selection criteria; Classification (of information)
Neural Dependency Parser for Tibetan Sentences,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105699925&doi=10.1145%2f3429456&partnerID=40&md5=51988e8fc11b772a07388747ce093dfc,"The research of Tibetan dependency analysis is mainly limited to two challenges: lack of a dataset and reliance on expert knowledge. To resolve the preceding challenges, we first introduce a new Tibetan dependency analysis dataset, and then propose a neural-based framework that resolves the reliance on the expert knowledge issue by automatically extracting feature vectors of words and predicts their head words and type of dependency arcs. Specifically, we convert the words in the sentence into distributional vectors and employ a sequence to vector network to extract feature words. Furthermore, we introduce a head classifier and type classifier to predict the head word and type of dependency arc, respectively. Experiments demonstrate that our model achieves promising performance on the Tibetan dependency analysis task. © 2021 Association for Computing Machinery.",neural networks; recurrent neural network; Tibetan dependency analysis,Agricultural engineering; Natural resources; Dependency analysis; Dependency parser; Expert knowledge; Extracting features; Feature words; Tibetans; Vector networks; Syntactics
Special Issue on Deep Structured Learning for Natural Language Processing,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104173581&doi=10.1145%2f3436206&partnerID=40&md5=4166d82a28b1ccc95a223c661f79a4a5,[No abstract available],,
A Cascaded Unsupervised Model for PoS Tagging,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104199857&doi=10.1145%2f3447759&partnerID=40&md5=019d9b7cae071d1bc86e5713750f7813,"Part of speech (PoS) tagging is one of the fundamental syntactic tasks in Natural Language Processing, as it assigns a syntactic category to each word within a given sentence or context (such as noun, verb, adjective, etc.). Those syntactic categories could be used to further analyze the sentence-level syntax (e.g., dependency parsing) and thereby extract the meaning of the sentence (e.g., semantic parsing). Various methods have been proposed for learning PoS tags in an unsupervised setting without using any annotated corpora. One of the widely used methods for the tagging problem is log-linear models. Initialization of the parameters in a log-linear model is very crucial for the inference. Different initialization techniques have been used so far. In this work, we present a log-linear model for PoS tagging that uses another fully unsupervised Bayesian model to initialize the parameters of the model in a cascaded framework. Therefore, we transfer some knowledge between two different unsupervised models to leverage the PoS tagging results, where a log-linear model benefits from a Bayesian model's expertise. We present results for Turkish as a morphologically rich language and for English as a comparably morphologically poor language in a fully unsupervised framework. The results show that our framework outperforms other unsupervised models proposed for PoS tagging. © 2021 ACM.",Bayesian learning; log-linear model; part-of-speech tagging (PoS tagging); Unsupervised learning,Bayesian networks; Context free grammars; Knowledge management; Natural language processing systems; Regression analysis; Semantics; Dependency parsing; Initialization technique; Loglinear model; NAtural language processing; Part of speech tagging; Semantic parsing; Sentence level; Tagging problem; Syntactics
Unsupervised Neural Machine Translation for Similar and Distant Language Pairs : An Empirical Study,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104172896&doi=10.1145%2f3418059&partnerID=40&md5=b8ce8e89e856758f957c786fff267d4c,"Unsupervised neural machine translation (UNMT) has achieved remarkable results for several language pairs, such as French-English and German-English. Most previous studies have focused on modeling UNMT systems; few studies have investigated the effect of UNMT on specific languages. In this article, we first empirically investigate UNMT for four diverse language pairs (French/German/Chinese/Japanese-English). We confirm that the performance of UNMT in translation tasks for similar language pairs (French/German-English) is dramatically better than for distant language pairs (Chinese/Japanese-English). We empirically show that the lack of shared words and different word orderings are the main reasons that lead UNMT to underperform in Chinese/Japanese-English. Based on these findings, we propose several methods, including artificial shared words and pre-ordering, to improve the performance of UNMT for distant language pairs. Moreover, we propose a simple general method to improve translation performance for all these four language pairs. The existing UNMT model can generate a translation of a reasonable quality after a few training epochs owing to a denoising mechanism and shared latent representations. However, learning shared latent representations restricts the performance of translation in both directions, particularly for distant language pairs, while denoising dramatically delays convergence by continuously modifying the training data. To avoid these problems, we propose a simple, yet effective and efficient, approach that (like UNMT) relies solely on monolingual corpora: pseudo-data-based unsupervised neural machine translation. Experimental results for these four language pairs show that our proposed methods significantly outperform UNMT baselines. © 2021 ACM.",pseudo-data-based unsupervised neural machine translation; similar and distant language pairs; Unsupervised neural machine translation,Computational linguistics; Computer aided language translation; Empirical studies; French-english; General method; Language pairs; Machine translations; Pre orderings; Specific languages; Training epochs; Modeling languages
"Cyberbullying Detection, Based on the FastText and Word Similarity Schemes",2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104186090&doi=10.1145%2f3398191&partnerID=40&md5=962d13e9145a143a4f3c75f5a0380c33,"With recent developments in online social networks (OSNs), these services are widely applied in daily lives. On the other hand, cyberbullying, which is a relatively new type of harassment through the internet-based electronic devices, is rising in online social networks. Accordingly, scholars are attracted to investigating cyberbullying behaviors. Studies show that cyberbullying has a devastating effect on mental health, especially for teenagers. In order to reduce or even stop cyberbullying, different machine learning techniques are applied and numerous studies have been conducted so far. However, conventional detection schemes still have challenges, such as low accuracy. Therefore, it is of significant importance to find an efficient detection solution in the natural language processing and machine learning communities. In the present study, characteristics of cyberbullying are initially analyzed from vocabulary and syntax points of view. Then a new detection algorithm is proposed based on FastText and word similarity schemes. Finally, experiments are carried out to evaluate the effectiveness and performance of the proposed method. Obtained results show that the proposed algorithm can effectively improve the detection accuracy and recall rate of cyberbullying detection. © 2020 ACM.",abusive language; cyberbullying; FastText; natural language processing; Online social networks; word similarity,Computer crime; Machine learning; Natural language processing systems; Conventional detection; Detection algorithm; Efficient detection; Machine learning communities; Machine learning techniques; NAtural language processing; On-line social networks; Online social networks (OSNs); Social networking (online)
A Survey of Offensive Language Detection for the Arabic Language,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104179281&doi=10.1145%2f3421504&partnerID=40&md5=b7ee20ace4551e9921727efb306d15db,"The use of offensive language in user-generated content is a serious problem that needs to be addressed with the latest technology. The field of Natural Language Processing (NLP) can support the automatic detection of offensive language. In this survey, we review previous NLP studies that cover Arabic offensive language detection. This survey investigates the state-of-The-Art in offensive language detection for the Arabic language, providing a structured overview of previous approaches, including core techniques, tools, resources, methods, and main features used. This work also discusses the limitations and gaps of the previous studies. Findings from this survey emphasize the importance of investing further effort in detecting Arabic offensive language, including the development of benchmark resources and the invention of novel preprocessing and feature extraction techniques. © 2021 ACM.",Arabic language; deep learning; literature review; machine learning; natural language processing; Offensive language,Natural language processing systems; Surveys; Arabic languages; Automatic Detection; Feature extraction techniques; Latest technology; NAtural language processing; Offensive languages; State of the art; User-generated content; Feature extraction
Venue Topic Model-enhanced Joint Graph Modelling for Citation Recommendation in Scholarly Big Data,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104194103&doi=10.1145%2f3404995&partnerID=40&md5=8cc0bf628f12e5c5b7f89b50ee013201,"Natural language processing technologies, such as topic models, have been proven to be effective for scholarly recommendation tasks with the ability to deal with content information. Recently, venue recommendation is becoming an increasingly important research task due to the unprecedented number of publication venues. However, traditional methods focus on either the author's local network or author-venue similarity, where the multiple relationships between scholars and venues are overlooked, especially the venue-venue interaction. To solve this problem, we propose an author topic model-enhanced joint graph modeling approach that consists of venue topic modeling, venue-specific topic influence modeling, and scholar preference modeling. We first model the venue topic with Latent Dirichlet Allocation. Then, we model the venue-specific topic influence in an asymmetric and low-dimensional way by considering the topic similarity between venues, the top-influence of venues, and the top-susceptibility of venues. The top-influence characterizes venues' capacity of exerting topic influence on other venues. The top-susceptibility captures venues' propensity of being topically influenced by other venues. Extensive experiments on two real-world datasets show that our proposed joint graph modeling approach outperforms the state-of-The-Art methods. © 2020 ACM.",academic information retrieval; natural language processing; Network embedding; scientific collaboration,Big data; Natural language processing systems; Statistics; Content information; Latent Dirichlet allocation; Low dimensional; NAtural language processing; Preference modeling; Real-world datasets; State-of-the-art methods; Topic similarity; Graph theory
Multimodal News Feed Evaluation System with Deep Reinforcement Learning Approaches,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104208255&doi=10.1145%2f3414523&partnerID=40&md5=b495e4bbd97286fc35fce80662c03197,"Multilingual and multimodal data analysis is the emerging news feed evaluation system. News feed analysis and evaluations are interrelated processes, which are useful in understanding the news factors. The news feed evaluation system can be implemented for single or multilingual language models. Classification techniques used on multilingual news analysis require deep layered learning techniques rather than conventional approaches. In this proposed work, a hierarchical structure of deep learning algorithms is implemented for making an effective complex news evaluation system. Deep learning techniques such as the Deep Cooperative Multilingual Reinforcement Learning Model, the Multidimensional Genetic Algorithm, and the Multilingual Generative Adversarial Network are developed to evaluate a vast number of news feeds. The proposed tech-niques collaborate in a pipeline order to build a deep news feed evaluation system. The implementation details project that the newly proposed system performs 5% to 12% better than the other news evaluation systems. © 2021 ACM.",DL techniques; multilingual news and analysis; News feeds; RL techniques,Genetic algorithms; Learning algorithms; Learning systems; Reinforcement learning; Adversarial networks; Analysis and evaluation; Classification technique; Conventional approach; Hierarchical structures; Multimodal data analysis; Reinforcement learning approach; Reinforcement learning models; Deep learning
Real-Time Assistive Reader Pen for Arabic Language,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104179091&doi=10.1145%2f3423133&partnerID=40&md5=82635022c7385c39dca3ea5fa0ecf539,"Disability is an impairment affecting an individual's livelihood and independence. Assistive technology enables the disabled cohort of the community to break the barriers to learning, access information, contribute to the community, and live independently. This article proposes an assistive device to enable people with visual disabilities and learning disabilities to access printed Arabic material in real-Time, and to help them participate in the education system and the professional workforce. This proposed assistive device employs Optical Character Recognition (OCR) and Text To Speech (TTS) conversion, using concatenation synthesis. OCR is achieved using image processing, character extraction, and classification, while Arabic speech synthesis is achieved through concatenation synthesis, followed by Multi Band Re-synthesis Overlap-Add (MBROLA). Waveform generation in the second phase produces vocal output for the disabled user to hear. OCR character and word accuracy tests were conducted for nine Arabic fonts. The results show that six fonts were recognized with over 60% character accuracy and two fonts were recognized with over 88% accuracy. A Mean Opinion Score (MOS) test for speech quality was conducted. The results showed an overall MOS score of 3.53/5 and indicated that users were able to understand the speech. A real-Time usability testing was conducted with 10 subjects. The results showed an overall average of agreements scores of 3.9/5 and indicated that the proposed Arabic reader pen meets the real-Time constraints and is pleasant and satisfying to use and can contribute to make printed Arabic material accessible to visually impaired persons and people with learning disabilities. © 2021 ACM.",Arabic language; assistive embedded systems; optical character recognition; reader pen; real-Time systems; text-To-speech,Image processing; Optical character recognition; Optical data processing; Speech synthesis; Assistive technology; Character extraction; Concatenation synthesis; Learning disabilities; Optical character recognition (OCR); Real time constraints; Visually impaired persons; Waveform generation; Speech recognition
KArSL: Arabic Sign Language Database,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104184271&doi=10.1145%2f3423420&partnerID=40&md5=93e108f232c6983d8914d608096dd3e8,"Sign language is the major means of communication for the deaf community. It uses body language and gestures such as hand shapes, lib patterns, and facial expressions to convey a message. Sign language is geography-specific, as it differs from one country to another. Arabic Sign language is used in all Arab countries. The availability of a comprehensive benchmarking database for ArSL is one of the challenges of the automatic recognition of Arabic Sign language. This article introduces KArSL database for ArSL, consisting of 502 signs that cover 11 chapters of ArSL dictionary. Signs in KArSL database are performed by three professional signers, and each sign is repeated 50 times by each signer. The database is recorded using state-of-Art multi-modal Microsoft Kinect V2. We also propose three approaches for sign language recognition using this database. The proposed systems are Hidden Markov Models, deep learning images' classification model applied on an image composed of shots of the video of the sign, and attention-based deep learning captioning system. Recognition accuracies of these systems indicate their suitability for such a large number of Arabic signs. The techniques are also tested on a publicly available database. KArSL database will be made freely available for interested researchers. © 2021 ACM.",Arabic sign language database; gesture recognition; HMM; human computer interaction; sign language recognition; sign language translation,Deep learning; Hidden Markov models; Arab countries; Arabic sign language; Automatic recognition; Classification models; Facial Expressions; Microsoft kinect; Recognition accuracy; Sign Language recognition; Database systems
Improving Semantic Coherence of Gujarati Text Topic Model Using Inflectional Forms Reduction and Single-letter Words Removal,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104172169&doi=10.1145%2f3447760&partnerID=40&md5=36f4b452458f0d28f46b562fa167509e,"A topic model is one of the best stochastic models for summarizing an extensive collection of text. It has accomplished an inordinate achievement in text analysis as well as text summarization. It can be employed to the set of documents that are represented as a bag-of-words, without considering grammar and order of the words. We modeled the topics for Gujarati news articles corpus. As the Gujarati language has a diverse morphological structure and inflectionally rich, Gujarati text processing finds more complexity. The size of the vocabulary plays an important role in the inference process and quality of topics. As the vocabulary size increases, the inference process becomes slower and topic semantic coherence decreases. If the vocabulary size is diminished, then the topic inference process can be accelerated. It may also improve the quality of topics. In this work, the list of suffixes has been prepared that encounters too frequently with words in Gujarati text. The inflectional forms have been reduced to the root words concerning the suffixes in the list. Moreover, Gujarati single-letter words have been eliminated for faster inference and better quality of topics. Experimentally, it has been proved that if inflectional forms are reduced to their root words, then vocabulary length is shrunk to a significant extent. It also caused the topic formation process quicker. Moreover, the inflectional forms reduction and single-letter word removal enhanced the interpretability of topics. The interpretability of topics has been assessed on semantic coherence, word length, and topic size. The experimental results showed improvements in the topical semantic coherence score. Also, the topic size grew notably as the number of tokens assigned to the topics increased. © 2021 ACM.",inflectional forms reduction; Latent Dirichlet allocation; morphological analysis; text summarization,Semantics; Stochastic systems; Text processing; Formation process; Inference process; Interpretability; Morphological structures; Text summarization; Topic Modeling; Vocabulary size; Word removals; Stochastic models
TransBERT: A Three-Stage Pre-Training Technology for Story-Ending Prediction,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104202642&doi=10.1145%2f3427669&partnerID=40&md5=80660a57843b4f73cbc999bb25b20629,"Recent advances, such as GPT, BERT, and RoBERTa, have shown success in incorporating a pre-Trained transformer language model and fine-Tuning operations to improve downstream NLP systems. However, this framework still has some fundamental problems in effectively incorporating supervised knowledge from other related tasks. In this study, we investigate a transferable BERT (TransBERT) training framework, which can transfer not only general language knowledge from large-scale unlabeled data but also specific kinds of knowledge from various semantically related supervised tasks, for a target task. Particularly, we propose utilizing three kinds of transfer tasks, including natural language inference, sentiment classification, and next action prediction, to further train BERT based on a pre-Trained model. This enables the model to get a better initialization for the target task. We take story-ending prediction as the target task to conduct experiments. The final results of 96.0% and 95.0% accuracy on two versions of Story Cloze Test datasets dramatically outperform previous state-of-The-Art baseline methods. Several comparative experiments give some helpful suggestions on how to select transfer tasks to improve BERT. Furthermore, experiments on six English and three Chinese datasets show that TransBERT generalizes well to other tasks, languages, and pre-Trained models. © 2021 ACM.",Natural language processing; pre-Trained models; story-ending prediction; transfer learning,Agricultural engineering; Natural resources; Action prediction; Baseline methods; Comparative experiments; Language model; Natural languages; Sentiment classification; State of the art; Training framework; Forecasting
Automatic Indonesian Sentiment Lexicon Curation with Sentiment Valence Tuning for Social Media Sentiment Analysis,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104194650&doi=10.1145%2f3425632&partnerID=40&md5=7eddb6d7f245c6fe81985fd21600368c,"A novel Indonesian sentiment lexicon (SentIL-Sentiment Indonesian Lexicon) is created with an automatic pipeline; from creating sentiment seed words, adding new words with slang words, emoticons, and from the given dictionary and sentiment corpus, until tuning sentiment value with tagged sentiment corpus. It begins by taking seed words from WordNet Bahasa that mapped with sentiment value from English SentiWordNet. The seed words are enriched by combining the dictionary-based method with words' synonyms and antonyms, and corpus-based methods with word embedding for word similarity that trained in positive and negative sentiment corpus from online marketplaces review and Twitter data. The valence score of each lexicon is recalculated based on its relative occurrence in the corpus. We also add some famous slang words and emoticons to enrich the lexicon. Our experiment shows that the proposed method can provide an increase of 3.5 times lexicon number as well as improve the accuracy of 80.9% for online review and 95.7% for Twitter data, and they are better than other published and available Indonesian sentiment lexicons. © 2021 ACM.",lexicon; Sentiment analysis; sentiment valence; social media; word embedding,Electronic commerce; Sentiment analysis; Corpus-based methods; Negative sentiments; On-line marketplaces; Online reviews; Sentiment lexicons; SentiWordNet; Social media; Word similarity; Social networking (online)
Knowledge Discovery of News Text Based on Artificial Intelligence,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104197810&doi=10.1145%2f3418062&partnerID=40&md5=e07e540f5a389cbde2a50a5fadd59ee4,"The explosion of news text and the development of artificial intelligence provide a new opportunity and challenge to provide high-quality media monitoring service. In this article, we propose a semantic analysis approach based on the Latent Dirichlet Allocation (LDA) and Apriori algorithm, and we realize application to improve media monitoring reports by mining large-scale news text. First, we propose to use LDA model to mine news text topic words and reducing news dimensionality. Then, we propose to use Apriori algorithm to discovering the relationship of topic words. Finally, we discovery the relevance of news text topic words and show the intensity and dependency among topic words through drawing. This application can realize to extract the news topics and discover the correlation and dependency among news topics in mass news text. The results show that the method based on LDA and Apriori can help the media monitoring staff to better understand the hidden knowledge in the news text and improve the media analysis report. © 2020 ACM.",association rules; knowledge discovery; LDA; news text,Learning algorithms; Semantics; Statistics; Text mining; Apriori algorithms; Hidden knowledge; High quality; Latent dirichlet allocations; Media analysis; Media monitoring; Semantic analysis; Topic words; Artificial intelligence
"Optimisation of the Largest Annotated Tibetan Corpus Combining Rule-based, Memory-based, and Deep-learning Methods",2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104201248&doi=10.1145%2f3409488&partnerID=40&md5=a25ec86c099a2df51857a910533008f2,"This article presents a pipeline that converts collections of Tibetan documents in plain text or XML into a fully segmented and POS-Tagged corpus. We apply the pipeline to the large extent collection of the Buddhist Digital Resource Center. The semi-supervised methods presented here not only result in a new and improved version of the largest annotated Tibetan corpus to date, the integration of rule-based, memory-based, and neural-network methods also serves as a good example of how to overcome challenges of under-researched languages. The end-To-end accuracy of our entire automatic pipeline of 91.99% is high enough to make the resulting corpus a useful resource for both linguists and scholars of Tibetan studies. © 2021 Owner/Author.",historical treebanks; NLP; POS tagging; Tibetan,Learning systems; Pipelines; Semi-supervised learning; Combining rules; Digital resources; Learning methods; Neural network method; Optimisations; Plain text; Rule based; Semi-supervised method; Deep learning
Hate Speech Detection in Roman Urdu,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104203945&doi=10.1145%2f3414524&partnerID=40&md5=a20133c96e6eb223947a3be0e6f340a9,"Hate speech is a specific type of controversial content that is widely legislated as a crime that must be identified and blocked. However, due to the sheer volume and velocity of the Twitter data stream, hate speech detection cannot be performed manually. To address this issue, several studies have been conducted for hate speech detection in European languages, whereas little attention has been paid to low-resource South Asian languages, making the social media vulnerable for millions of users. In particular, to the best of our knowledge, no study has been conducted for hate speech detection in Roman Urdu text, which is widely used in the sub-continent. In this study, we have scrapped more than 90,000 tweets and manually parsed them to identify 5,000 Roman Urdu tweets. Subsequently, we have employed an iterative approach to develop guidelines and used them for generating the Hate Speech Roman Urdu 2020 corpus. The tweets in the this corpus are classified at three levels: Neutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another contribution, we have used five supervised learning techniques, including a deep learning technique, to evaluate and compare their effectiveness for hate speech detection. The results show that Logistic Regression outperformed all other techniques, including deep learning techniques for the two levels of classification, by achieved an F1 score of 0.906 for distinguishing between Neutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate speech tweets. © 2021 ACM.",Hate speech detection; Low-resource languages; Roman Urdu; South Asian Languages,Data streams; Deep learning; Iterative methods; Learning systems; Logistic regression; Social networking (online); Speech; Supervised learning; European languages; F1 scores; Iterative approach; Learning techniques; Social media; South Asian languages; Speech detection; Sub-continents; Speech recognition
The Transnational Happiness Study with Big Data Technology,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104174613&doi=10.1145%2f3412497&partnerID=40&md5=4a38bf96c99fe302a4d7437bb3893419,"Happiness is a hot topic in academic circles. The study of happiness involves many disciplines, such as philosophy, psychology, sociology, and economics. However, there are few studies on the quantitative analysis of the factors affecting happiness. In this article, we used the well-known World Values Survey Wave 6 (WV6) dataset to quantitatively analyze the happiness of 57 countries with Big Data techniques. First, we obtained the seven most important factors by constructing happiness decision trees for each country. Calculating the frequencies of these factors, we obtained the 17 most important indicators for the prediction of happiness in the world. Then, we selected five representative countries, namely, Sweden, Japan, India, China, and the USA, and analyzed the indicators with the random forest method. We identified different patterns of factors that influence happiness in different countries. This study is a successful attempt to apply data mining technology in the social sciences, and the results are of practical significance. © 2020 ACM.",Big Data; decision tree; feature selection; happiness,Decision trees; Economics; Large dataset; Social sciences computing; Sociology; Data mining technology; Data technologies; Hot topics; Random forest methods; Data mining
Deep Interactive Memory Network for Aspect-Level Sentiment Analysis,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104176905&doi=10.1145%2f3402886&partnerID=40&md5=68d176daac241a1bc321f50a981729b3,"The goal of aspect-level sentiment analysis is to identify the sentiment polarity of a specific opinion target expressed; it is a fine-grained sentiment analysis task. Most of the existing works study how to better use the target information to model the sentence without using the interactive information between the sentence and target. In this article, we argue that the prediction of aspect-level sentiment polarity depends on both context and target. First, we propose a new model based on LSTM and the attention mechanism to predict the sentiment of each target in the review, the matrix-interactive attention network (M-IAN) that models target and context, respectively. M-IAN use an attention matrix to learn the interactive attention of context and target and generates the final representations of target and context. Then we introduce two gate networks based on M-IAN to build a deep interactive memory network to capture multiple interactions of target and context. The deep interactive memory network can excellently formulate specific memory for different targets, which is helpful in sentiment analysis. The experimental results of Restaurant and Laptop datasets of SemEval 2014 validate the effectiveness of our model. © 2020 ACM.",Aspect-level sentiment; matrix-interactive memory network; multiple attention,Sentiment analysis; Attention mechanisms; Fine grained; Interactive informations; Memory network; Model-based OPC; Multiple interactions; Opinion targets; Target information; Long short-term memory
Hybridization between Neural Computing and Nature-Inspired Algorithms for a Sentence Similarity Model Based on the Attention Mechanism,2021,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104186264&doi=10.1145%2f3447756&partnerID=40&md5=c12a4f5936d5780b6c6e12acbe00ef93,"Sentence similarity analysis has been applied in many fields, such as machine translation, the question answering system, and voice customer service. As a basic task of natural language processing, sentence similarity analysis plays an important role in many fields. The task of sentence similarity analysis is to establish a sentence similarity scoring model through multi-features. In previous work, researchers proposed a variety of models to deal with the calculation of sentence similarity. But these models do not consider the association information of sentence pairs, but only input sentence pairs into the model. In this article, we propose a sentence feature extraction model based on multi-feature attention. In addition, with the development of deep learning and the application of nature-inspired algorithms, researchers have proposed various hybrid algorithms that combine nature-inspired algorithms with neural networks. The hybrid algorithms not only solve the problem of decision-making based on multiple features but also improve the performance of the model. In the model, we use the attention mechanism to extract sentence features and assign weight. Then, the convolutional neural network is used to reduce the dimension of the matrix. In the training process, we integrate the firefly algorithm in the neural networks. The experimental results show that the accuracy of our model is 74.21%. © 2021 ACM.",attention mechanism; convolutional neural network; feature extraction; nature-inspired algorithms; Sentence similarity analysis,Computer aided language translation; Convolutional neural networks; Decision making; Deep learning; Natural language processing systems; Optimization; Attention mechanisms; Firefly algorithms; Hybrid algorithms; Machine translations; NAtural language processing; Nature inspired algorithms; Question answering systems; Sentence similarity; Biomimetics
An Extensible Framework of Leveraging Syntactic Skeleton for Semantic Relation Classification,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097247957&doi=10.1145%2f3402885&partnerID=40&md5=3f01aaadf297c79746476ff865cad04d,"Relation classification is one of the most fundamental upstream tasks in natural language processing and information extraction. State-of-the-art approaches make use of various deep neural networks (DNNs) to extract higher-level features directly. They can easily access to accurate classification results by taking advantage of both local entity features and global sentential features. Recent works on relation classification devote efforts to modify these neural networks, but less attention has been paid to the feature design concerning syntax. However, from a linguistic perspective, syntactic features are essential for relation classification. In this article, we present a novel linguistically motivated approach that enhances relation classification by imposing additional syntactic constraints. We investigate to leverage syntactic skeletons along with the sentential contexts to identify hidden relation types. The syntactic skeletons are extracted under the guidance of prior syntax knowledge. During extraction, the input sentences are recursively decomposed into syntactically shorter and simpler chunks. Experimental results on the SemEval-2010 Task 8 benchmark show that incorporating syntactic skeletons into current DNN models enhances the task of relation classification. Our systems significantly surpass two strong baseline systems. One of the substantial advantages of our proposal is that this framework is extensible for most current DNN models. © 2020 ACM.",Neural network; relation classification; syntactic skeleton,Deep neural networks; Musculoskeletal system; Natural language processing systems; Neural networks; Semantics; Syntactics; Baseline systems; Classification results; Extensible framework; NAtural language processing; Relation classifications; Semantic relations; State-of-the-art approach; Syntactic features; Classification (of information)
Global Encoding for Long Chinese Text Summarization,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097250183&doi=10.1145%2f3407911&partnerID=40&md5=e3021421607eabd1e8b4ba55d5ce045f,"Text summarization is one of the significant tasks of natural language processing, which automatically converts text into a summary. Some summarization systems, for short/long English, and short Chinese text, benefit from advances in the neural encoder-decoder model because of the availability of large datasets. However, the long Chinese text summarization research has been limited to datasets of a couple of hundred instances. This article aims to explore the long Chinese text summarization task. To begin with, we construct a first large-scale, long Chinese text summarization corpus, the Long Chinese Summarization of Police Inquiry Record Text (LCSPIRT). Based on this corpus, we propose a sequence-to-sequence (Seq2Seq) model that incorporates a global encoding process with an attention mechanism. Our model achieves a competitive result on the LCSPIRT corpus compared with several benchmark methods. © 2020 Owner/Author.",corpus building; long Chinese text; Text summarization,Encoding (symbols); Large dataset; Linguistics; Natural language processing systems; Signal encoding; Attention mechanisms; Encoder-decoder; Encoding process; Large datasets; NAtural language processing; Short chinese texts; Summarization systems; Text summarization; Text processing
Detecting Entities of Works for Chinese Chatbot,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097244683&doi=10.1145%2f3414901&partnerID=40&md5=d0b08412084fe30c180b4297fe304eb1,"Chatbots such as Xiaoice have gained huge popularity in recent years. Users frequently mention their favorite works such as songs and movies in conversations with chatbots. Detecting these entities can help design better chat strategies and improve user experience. Existing named entity recognition methods are mainly designed for formal texts, and their performance on the informal chatbot conversation texts may not be optimal. In addition, these methods rely on massive manually annotated data for model training. In this article, we propose a neural approach to detect entities of works for Chinese chatbot. Our approach is based on a language model (LM) long-short term memory (LSTM) convolutional neural network (CNN) conditional random value (CRF), or LM-LSTM-CNN-CRF, framework, which contains a language model to generate context-aware character embeddings, a Bi-LSTM network to learn contextual character representations from global contexts, a CNN to learn character representations from local contexts, and a CRF layer to jointly decode the character label sequence. In addition, we propose an automatic text annotation method via quote marks to reduce the effort of manual annotation. Besides, we propose an iterative data purification method to improve the quality of the automatically constructed labeled data. Massive experiments on a real-world dataset validate that our approach can achieve good performance on entity detection for Chinese chatbots. © 2020 ACM.",chatbot; Named entity recognition; neural network,Computational linguistics; Convolutional neural networks; Iterative methods; Multilayer neural networks; User experience; Character representations; Entity detection; Global context; Manual annotation; Model training; Named entity recognition; Purification method; Text annotations; Long short-term memory
Condition-Transforming Variational Autoencoder for Generating Diverse Short Text Conversations,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097244909&doi=10.1145%2f3402884&partnerID=40&md5=6aac05b89561c7a8ebd954ee624d7000,"In this article, conditional-transforming variational autoencoders (CTVAEs) are proposed for generating diverse short text conversations. In conditional variational autoencoders (CVAEs), the prior distribution of latent variable z follows a multivariate Gaussian distribution with mean and variance modulated by the input conditions. Previous work found that this distribution tended to become condition-independent in practical applications. Thus, this article designs CTVAEs to enhance the influence of conditions in CVAEs. In a CTVAE model, the latent variable z is sampled by performing a non-linear transformation on the combination of the input conditions and the samples from a condition-independent prior distribution N (0, I). In our experiments using a Chinese Sina Weibo dataset, the CTVAE model derives z samples for decoding with better condition-dependency than that of the CVAE model. The earth mover's distance (EMD) between the distributions of the latent variable z at the training stage, and the testing stage is also reduced by using the CTVAE model. In subjective preference tests, our proposed CTVAE model performs significantly better than CVAE and sequence-to-sequence (Seq2Seq) models on generating diverse, informative, and topic-relevant responses. © 2020 ACM.",conversation; Neural network; text generation; variational autoencoder,Linear transformations; Mathematical transformations; Autoencoders; Earth Mover's distance; Latent variable; Multivariate Gaussian Distributions; Non-linear transformations; Prior distribution; Short texts; Subjective preference test; Learning systems
On the Construction of Web NER Model Training Tool based on Distant Supervision,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097242385&doi=10.1145%2f3422817&partnerID=40&md5=6376eac00e958381e3e9e00ca264979e,"Named entity recognition (NER) is an important task in natural language understanding, as it extracts the key entities (person, organization, location, date, number, etc.) and objects (product, song, movie, activity name, etc.) mentioned in texts. However, existing natural language processing (NLP) tools (such as Stanford NER) recognize only general named entities or require annotated training examples and feature engineering for supervised model construction. Since not all languages or entities have public NER support, constructing a tool for NER model training is essential for low-resource language or entity information extraction. In this article, we study the problem of developing a tool to prepare training corpus from the Web with known seed entities for custom NER model training via distant supervision. The major challenge of automatic labeling lies in the long labeling time due to large corpus and seed entities as well as the concern to avoid false positive and false negative examples due to short and long seeds. To solve this problem, we adopt locality-sensitive hashing (LSH) for various length of seed entities. We conduct experiments on five types of entity recognition tasks, including Chinese person names, food names, locations, points of interest (POIs), and activity names to demonstrate the improvements with the proposed Web NER model construction tool. Because the training corpus is obtained by automatic labeling of the seed entity-related sentences, one could use either the entire corpus or the positive only sentences for model training. Based on the experimental results, we found the decision should depend on whether traditional linear chained conditional random fields (CRF) or deep neural network-based CRF is used for model training as well as the completeness of the provided seed list. © 2020 ACM.",distant supervision; Information extraction; locality-sensitive hashing (LSH); named entity recognition; scalable automatic labeling,Deep neural networks; Linguistics; Random processes; Conditional random field; False positive and false negatives; Feature engineerings; Locality sensitive hashing; Low resource languages; Named entity recognition; NAtural language processing; Natural language understanding; Natural language processing systems
AyaTEC: Building a Reusable Verse-Based Test Collection for Arabic Question Answering on the Holy Qur'an,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097222525&doi=10.1145%2f3400396&partnerID=40&md5=786cea00335a2d4d1cf215110a681c19,"The absence of publicly available reusable test collections for Arabic question answering on the Holy Qur'an has impeded the possibility of fairly comparing the performance of systems in that domain. In this article, we introduce AyaTEC, a reusable test collection for verse-based question answering on the Holy Qur'an, which serves as a common experimental testbed for this task. AyaTEC includes 207 questions (with their corresponding 1,762 answers) covering 11 topic categories of the Holy Qur'an that target the information needs of both curious and skeptical users. To the best of our effort, the answers to the questions (each represented as a sequence of verses) in AyaTEC were exhaustive-that is, all qur'anic verses that directly answered the questions were exhaustively extracted and annotated. To facilitate the use of AyaTEC in evaluating the systems designed for that task, we propose several evaluation measures to support the different types of questions and the nature of verse-based answers while integrating the concept of partial matching of answers in the evaluation. © 2020 ACM.",Classical Arabic; evaluation,Natural resources; Evaluation measures; Experimental testbed; Partial matching; Performance of systems; Question Answering; Test Collection; Agricultural engineering
A Technique to Calculate National Happiness Index by Analyzing Roman Urdu Messages Posted on Social Media,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097216029&doi=10.1145%2f3400712&partnerID=40&md5=da8bbf2b1e85bd501e2c1119378fe581,"National Happiness Index (NHI) is a national indicator of development that estimates the economic and social well-being of the nation's individuals. With the proliferation of the internet, people share a significant amount of data on social media websites. We can process the data with different sentiment analysis techniques to calculate the NHI. In the literature, different approaches have been used to calculate NHI, which include the lexicon-based approach and machine learning approach. All of these existing approaches are proposed to calculate NHI for the sentiments written in the English language. However, these methods fail for complex Roman Urdu tweets that contain more than two sub-opinions. There are three primary objectives of the research: (1) to investigate current sentiment analysis techniques are sufficient for the classification of complex Roman Urdu sentiments; (2) to propose rule-based classifier for the classification of Roman Urdu sentiments comprising multiple sub-opinions; (3) to calculate NHI using Roman Urdu sentiments. For this purpose, we proposed the discourse information extractor, the rule-based method (3-RBC), and the machine learning classifier. The experimental results show that 3-RBC is efficient for feature identification, and it is more statistically significant than the baseline classifiers. The 3-RBC has successfully increased the accuracy by 7% and precision by 8%, which provides evidence that the proposed technique significantly increased the calculation of NHI. © 2020 ACM.",discourse information; lexicon-based approach; National happiness index; rule-based classifier; sentiment analysis; support vector machines,Machine learning; Sentiment analysis; Social networking (online); Analysis techniques; English languages; Feature identification; Machine learning approaches; Primary objective; Rule-based classifier; Rule-based method; Social media websites; Classification (of information)
Translating Morphologically Rich Indian Languages under Zero-Resource Conditions,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097239414&doi=10.1145%2f3407912&partnerID=40&md5=b430ed5a530b35a3dcde6f9cc30759e1,"This work presents an in-depth analysis of machine translations of morphologically-rich Indo-Aryan and Dravidian languages under zero-resource conditions. It focuses on Zero-Shot Systems for these languages and leverages transfer-learning by exploiting target-side monolingual corpora and parallel translations from other languages. These systems are compared with direct translations using the BLEU and TER metrics. Further, Zero-Shot Systems are used as pre-trained models for fine-tuning with real human-generated data taken in different proportions that range from 100 sentences to the entire training set. Performances of the Indo-Aryan and Dravidian languages are compared with a focus on their morphological complexity. The systems with a Dravidian source language performed much better and reached very near to the level of direct translations. This is observed likely due to morphological richness and complexity in the language, which in turn provided more room for transfer-learning in this case. A comparative analysis based on language families has been done. These systems were fine-tuned further, which in turn outperformed direct translations with just 500 parallel sentences for a Dravidian source language. However, systems with an Indo-Aryan source language showed similar performance after getting fine-tuned with 10,000 sentences. © 2020 ACM.",Dravidian; Indo-Aryan; low-resource; morphological complexity; transfer-learning; Zero-Shot Systems,Transfer learning; Comparative analysis; Different proportions; In-depth analysis; Indian languages; Machine translations; Morphological complexity; Resource conditions; Source language; Translation (languages)
Attention Mechanism for Uyghur Personal Pronouns Resolution,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097242759&doi=10.1145%2f3412323&partnerID=40&md5=b748a183fd8e5c6d1091c6085fd4b0a4,"Deep neural network models for Uyghur personal pronoun resolution learn semantic information for personal pronoun and antecedents, but tend to be short-sighted-they ignore the importance of each feature. In this article, we propose a Uyghur personal pronoun resolution model based on Attention mechanism, Convolutional neural networks and Gated recurrent unit (ATCG). Our model studies the grammatical structure and semantic features of Uyghur, and extracts 11 key features for Uyghur resolution task. Attention mechanism can focus on the importance of words in sentences. Gated Recurrent Unit (GRU) is applied in this model to achieve the interdependent features with long distance. The ATCG model effectively makes up for the shortcomings of relying only on the features of the content level and achieves better classification performance. Experimental results on Uyghur resolution dataset show that our model surpasses the state-of-the-art models. © 2020 ACM.",anaphora resolution; Attention mechanism; CNN; GRU; Uyghur,Convolutional neural networks; Deep neural networks; Semantics; Attention mechanisms; Classification performance; Grammatical structure; Neural network model; Pronoun resolution; Semantic features; Semantic information; State of the art; Recurrent neural networks
A Link Prediction Approach for Accurately Mapping a Large-scale Arabic Lexical Resource to English WordNet,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097218076&doi=10.1145%2f3404854&partnerID=40&md5=3fdf01a6de5a7fc75a5f6fc298823496,"Success of Natural Language Processing (NLP) models, just like all advanced machine learning models, rely heavily on large-scale lexical resources. For English, English WordNet (EWN) is a leading example of a large-scale resource that has enabled advances in Natural Language Understanding (NLU) tasks such as word sense disambiguation, question answering, sentiment analysis, and emotion recognition. EWN includes sets of cognitive synonyms called synsets, which are interlinked by means of conceptual-semantic and lexical relations and where each synset expresses a distinct concept. However, other languages are still lagging behind in having large-scale and rich lexical resources similar to EWN. In this article, we focus on enabling the development of such resources for Arabic. While there have been efforts in developing an Arabic WordNet (AWN), the current version of AWN has its limitations in size and in lacking transliteration standards, which are important for compatibility with Arabic NLP tools. Previous efforts for extending AWN resulted in a lexicon, called ArSenL, that overcame the size and the transliteration standard limitation but was limited in accuracy due to the heuristic approach that only considered surface matching between the English definitions from the Standard Arabic Morphological Analyzer (SAMA) and EWN synset terms, and that resulted in inaccurate mapping of Arabic lemmas to EWN's synsets. Furthermore, there has been limited exploration of other expansion methods due to expensive manual validation needed. To address these limitations of simultaneously having large-scale size with high accuracy and standard representations, the mapping problem is formulated as a link prediction problem between a large-scale Arabic lexicon and EWN, where a word in one lexicon is linked to a word in another lexicon if the two words are semantically related. We use a semi-supervised approach to create a training dataset by finding common terms in the large-scale Arabic resource and AWN. This set of data becomes implicitly linked to EWN and can be used for training and evaluating prediction models. We propose the use of a two-step Boosting method, where the first step aims at linking English translations of SAMA's terms to EWN's synsets. The second step uses surface similarity between SAMA's glosses and EWN's synsets. The method results in a new large-scale Arabic lexicon that we call ArSenL 2.0 as a sequel to the previously developed sentiment lexicon ArSenL. A comprehensive study covering both intrinsic and extrinsic evaluations shows the superiority of the method compared to several baseline and state-of-the-art link prediction methods. Compared to previously developed ArSenL, ArSenL 2.0 included a larger set of sentimentally charged adjectives and verbs. It also showed higher linking accuracy on the ground truth data compared to previous ArSenL. For extrinsic evaluation, ArSenL 2.0 was used for sentiment analysis and showed, here, too, higher accuracy compared to previous ArSenL. © 2020 ACM.",arabic natural language processing; arabic sentiment lexicon; arabic wordnet expansion; lexical resources; Link prediction; wordnet,Forecasting; Heuristic methods; Mapping; Ontology; Predictive analytics; Semantics; Sentiment analysis; Emotion recognition; Heuristic approach; Machine learning models; Morphological analyzer; NAtural language processing; Natural language understanding; Question Answering; Word Sense Disambiguation; Large dataset
Disambiguating Arabic Words According to Their Historical Appearance in the Document Based on Recurrent Neural Networks,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097238876&doi=10.1145%2f3410569&partnerID=40&md5=7298f81d837b57c111267d85cdb453c0,"How can we determine the semantic meaning of a word in relation to its context of appearance? We eventually have to grabble with this difficult question, as one of the paramount problems of Natural Language Processing (NLP). In other words, this issue is commonly defined as Word Sense Disambiguation (WSD). The latter is one of the crucial difficulties within the NLP field. In this respect, word vectors extracted from a neural network model have been successfully applied for resolving the WSD problem. Accordingly, this article presents an unprecedented method to disambiguate Arabic words according to both their contextual appearance in a source text and the era in which they emerged. In fact, in the few previous decades, many researchers have been grabbling with Arabic Word Sense Disambiguation. It should be noted that the Arabic language can be divided into three major historical periods: Old Arabic, middle-age Arabic, and contemporary Arabic. Actually, contemporary Arabic has proved to be the greatest concern of many researchers. The main gist of our work is to disambiguate Arabic words according to the historical period in which they appeared. To perform such a task, we suggest a method that deploys contextualized word embeddings to better gather valid syntactic and semantic information of the same word by taking into account its contextual uses. The preponderant thing is to convert both the senses and the contextual uses of an ambiguous item to vectors, then determine which of the possible conceptual meanings of the target word is closer to the given context. © 2020 ACM.",contemporary arabic; contextualized word embeddings; historical dictionary; middle-age arabic; Natural language processing; old arabic; recurrent neural networks; word sense disambiguation,Natural language processing systems; Semantics; Arabic languages; Document-based; Historical periods; NAtural language processing; Neural network model; Semantic information; Word Sense Disambiguation; Word vectors; Recurrent neural networks
"Grading Tibetan Children's Literature: A Test Case Using the NLP Readability Tool ""dakje""",2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097230910&doi=10.1145%2f3392046&partnerID=40&md5=8691ee00f5357f45c305ca2d3a97f673,"Worldwide, literacy is on the rise. This historically unprecedented surge-especially over the past 200 years-has changed nearly everything about the ancient technology of reading. Who reads is changing: Literacy is no longer just for elite, professional readers, but for anyone and everyone. What and why we read is changing: We do not just read difficult texts for academic, religious, legal, or record-keeping purposes; we also read easy texts to be entertained, to access information, and to communicate with each other on a daily basis. And how we read is changing: Memorization, recitation, and oral performance has given way to a rapid, silent, individual activity. Many of these democratizing changes have been made possible by technology. This has included advances in methods and materials that have made reading and writing easy, cheap, and widely available-like paper, the printing press, and the digital revolution. But perhaps the biggest reason literacy has become so widespread has been its ability to reach people in their own natural languages. More recently, this progress has been enhanced by NLP tools, like readability editors, that have helped authors, journalists, and other writing professionals make simple, clear content suitable for both beginning readers and widespread audiences. To that end, this article introduces a new readability tool, ""Dakje,""alongside a specific use case, and demonstrates how it can help benefit literacy in the Tibetan languages. This NLP software works by word-splitting Tibetan text and analyzing those words using level lists that are based on frequency analysis from corpora. Users then have instant access to statistics on the readability of their word choices so they can make edits for easy-to-read text. In our test-case, Dakje helped us reduce sentence complexity by 34%, total word count by 10%, and non-level vocabulary use from 16% to 1% when compared to an original English-to-Tibetan translation. © 2020 ACM.",children's literature; corpus linguistics; diglossia; graded readers; literacy; readability; speech corpus; Tibetan,Grading; Records management; Ancient technologies; Digital revolution; Frequency Analysis; Methods and materials; Natural languages; NLP tools; Record keeping; Word choices; Natural language processing systems
The Impact of Weighting Schemes and Stemming Process on Topic Modeling of Arabic Long and Short Texts,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097221930&doi=10.1145%2f3405843&partnerID=40&md5=f0107167f05e1b2f69699d7ec9221b25,"In this article, first a comprehensive study of the impact of term weighting schemes on the topic modeling performance (i.e., LDA and DMM) on Arabic long and short texts is presented. We investigate six term weighting methods including Word count method (standard topic models), TFIDF, PMI, BDC, CLPB, and CEW. Moreover, we propose a novel combination term weighting scheme, namely, CmTLB. We utilize the mTFIDF that takes into account the missing terms and the number of the documents in which the term appears when calculating the term weight. For further robust term weight, we combine mTFIDF with two weighting methods. We evaluate CmTLB against the studied weighting schemes by the quality of the learned topics (topic visualization and topic coherence), classification, and clustering tasks. We applied weighting schemes to Latent Dirichlet allocation (LDA) and Dirichlet multinomial mixture (DMM) on eight Arabic long and short document datasets, respectively. The experiment results outline that appropriate weighting schemes can effectively improve topic modeling performance on Arabic texts. More importantly, our proposed CmTLB significantly outperforms the other weighting schemes. Secondly, we investigate whether the Arabic stemming process can improve topic modeling performance. We study the three approaches of Arabic stemming including root-based, stem-based, and statistical approaches. We also train topic models with weighting schemes on documents after applying four stemmers related to different stemming approaches. The results outline that applying the stemming process not only reduces the dimensionality of term-document matrix leading to fast estimation process, but also show enhancement of topic modeling performance both on short and long Arabic documents. Moreover, Farasa stemmer achieves the highest performance in most cases, since it prevents the ambiguity that may happen because of the blind removal of the affixes such as in root-based or stem-based stemmers. © 2020 ACM.",arabic text; Dirichlet multinomial mixture (DMM); Latent Dirichlet allocation (LDA); natural language processing (NLP); stemming process; term weighting schemes,Statistics; Document datasets; Document matrices; Latent dirichlet allocations; Statistical approach; Term weighting scheme; Topic visualizations; Weighting methods; Weighting scheme; Natural language processing systems
Classification of Ancient Handwritten Tamil Characters on Palm Leaf Inscription Using Modified Adaptive Backpropagation Neural Network with GLCM Features,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097218650&doi=10.1145%2f3406209&partnerID=40&md5=9e5d88454210ee77c3b510e585e75790,"The core aspiration of this proposed work is to classify Tamil characters inscribed in the palm leaf manuscript using an Artificial Neural Network. Tamil palm leaf manuscript characters in the form of images were processed and segmented using contour-based convex hull bounding box segmentation. The segmented characters were transformed into two forms: Binary Coded Value and the Gray-Level Co-occurrence Matrix (GLCM) feature. The features extracted from the segmented characters were trained by the proposed method of the Modified Adaptive Backpropagation Network (MABPN) algorithm with Shannon activation function. Weight initialization plays an important role in the Backpropagation Neural Network, and hence Nguyen-Widrow weight initialization was introduced to initialize the weights instead of random weight initialization in the proposed method. The models evaluated are MABPN with Shannon activation function using Nguyen-Widrow weight initialization in two forms of input: Binary Coded Value and GLCM feature extracted values. The proposed method with GLCM features as input gave a promising result over binary coded transform. © 2020 ACM.",ABPN; GLCM; machine learning; Nguyen-Widrow; Shannon,Backpropagation; Chemical activation; Image segmentation; Activation functions; Adaptive back propagation; Adaptive back-propagation networks; Back propagation neural networks; GLCM feature; Gray level co occurrence matrix(GLCM); Random weight; Weight initialization; Neural networks
CESS-A system to categorize bangla web text documents,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091650649&doi=10.1145%2f3398070&partnerID=40&md5=658c79cad7000864a634733167f7036c,"Technology has evolved remarkably, which has led to an exponential increase in the availability of digital text documents of disparate domains over the Internet. This makes the retrieval of the information a very much time- and resource-consuming task. Thus, a system that can categorize such documents based on their domains can truly help the users in obtaining the required information with relative ease and also reduce the workload of the search engines. This article presents a text categorization system (CESS) that categorizes text document using newly proposed hybrid features that combines term frequency-inverse document frequency-inverse class frequency and modified chi-square methods. Experiments were performed on real-world Bangla documents from eight domains comprises of 24,29,857 tokens, and the highest accuracy of 99.91% has been obtained with multilayer perceptron-based classification. Also, the experiments were tested on Reuters-21578 and 20 Newsgroups datasets and obtained accuracies of 97.29% and 94.67%, respectively, to show the language-independent nature of the system. © 2020 Association for Computing Machinery.",Bangla texts; Modified chi-square; Multilayer perceptron; Text categorization; Tf-idf-icf,Information retrieval systems; Inverse problems; Multilayer neural networks; Text processing; Exponential increase; Hybrid features; Inverse class frequency; Language independents; Reuters-21578; Term frequency-inverse document frequencies; Text categorization systems; Text document; Search engines
Speech-driven end-to-end language discrimination toward Chinese dialects,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091652704&doi=10.1145%2f3389021&partnerID=40&md5=715df2c8cc6391a19c106ca740d07d53,"Language discrimination among similar languages, varieties, and dialects is a challenging natural language processing task. The traditional text-driven focus leads to poor results. In this article, we explore the effectiveness of speech-driven features toward language discrimination among Chinese dialects. First, we systematically explore the appropriateness of speech-driven MFCC features toward CNN-based language discrimination. Then, we design an end-to-end speech recognition model based on HMM-DNN to predict Chinese dialect words. We adopt attention mechanism to extract the discriminative words related to different Chinese dialects. Finally, through a CNN, we combine the word-level embedding and the MFCC-based features. Evaluation of two benchmark Chinese dialect corpora shows the appropriateness and effectiveness of the proposed speech-driven approach to fine-grained Chinese dialect discrimination compared to the state-ofthe-art methods. © 2020 Association for Computing Machinery.",Attention; Chinese dialect; Language discrimination; Speech-driven features; Text-driven features,Natural language processing systems; Speech; Art methods; Attention mechanisms; Chinese dialects; Fine grained; Language discrimination; NAtural language processing; Recognition models; Word level; Speech recognition
Iterative training of unsupervised neural and statistical machine translation systems,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091624123&doi=10.1145%2f3389790&partnerID=40&md5=9c078045e84d853deff1336b5842eda6,"Recent work achieved remarkable results in training neural machine translation (NMT) systems in a fully unsupervised way, with new and dedicated architectures that only rely on monolingual corpora. However, previous work also showed that unsupervised statistical machine translation (USMT) performs better than unsupervised NMT (UNMT), especially for distant language pairs. To take advantage of the superiority of USMT over UNMT, and considering that SMT suffers from well-known limitations overcome by NMT, we propose to define UNMT as NMT trained with the supervision of synthetic parallel data generated by USMT. This way we can exploit USMT up to its limits while ultimately relying on full-fledged NMT models to generate translations. We show significant improvements in translation quality over previous work and also that further improvements can be obtained by alternatively and iteratively training USMT and UNMT. Without the need of a dedicated architecture for UNMT, our simple approach can straightforwardly benefit from any recent and future advances in supervised NMT. Our systems achieve a new state-of-the-art for unsupervised machine translation in all of our six translation tasks for five diverse language pairs, surpassing even supervised SMT or NMT in some tasks. Furthermore, our analysis shows how crucial the comparability between the monolingual corpora used for unsupervised training is in improving translation quality. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Knowledge acquisition; Low-resource language pair; Machine translation; Phrase table induction; Semantic similarity,Computational linguistics; Speech transmission; Language pairs; Machine translations; Simple approach; State of the art; Statistical machine translation; Statistical machine translation system; Translation quality; Unsupervised training; Computer aided language translation
Inside importance factors of graph-based keyword extraction on Chinese short text,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091656005&doi=10.1145%2f3388971&partnerID=40&md5=f9ae422a5e215cd3e879dc4d551c872f,"Keywords are considered to be important words in the text and can provide a concise representation of the text. With the surge of unlabeled short text on the Internet, automatic keyword extraction task has proven useful in other information processing applications. Graph-based approaches are prevalent unsupervised models for this task. However, most of these methods emphasize the importance of the relation between words without considering other importance factors. Furthermore, when measuring the importance of a word in a text, the damping factor is set to 0.85 following PageRank. To the best of our knowledge, there is no existing work investigating the impact of the damping factor on the keyword extraction task. In addition, there are few publicly available labeled Chinese short text datasets for this task. In this article, we investigate the importance parts of words in a given document and propose an improved graph-based method for keyword extraction from short documents. Moreover, we analyze the impact of importance factors on performance. We also provide annotated long and short Chinese datasets for this task. The model is performed on Chinese and English datasets, and results show that our model obtains improvements in performance over the previous unsupervised models on short documents. Comparative experiments show that the damping factor is related to the text length, which is neglected in traditional methods. © 2020 Association for Computing Machinery.",Extraction; Importance rank; Short text,Damping; Extraction; Chinese short-text; Comparative experiments; Concise representations; Damping factors; Graph-based methods; Importance factors; Keyword extraction; Processing applications; Graphic methods
Robust Arabic text categorization by combining convolutional and recurrent neural networks,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091666137&doi=10.1145%2f3390092&partnerID=40&md5=1c5e805c751c151f3c30a5665e565146,"Text Categorization is an important task in the area of Natural Language Processing (NLP). Its goal is to learn a model that can accurately classify any textual document for a given language into one of a set of predefined categories. In the context of the Arabic language, several approaches have been proposed to tackle this problem, many of which are based on the bag-of-words assumption. Even though these methods usually produce good results for the classification task, they often fail to capture contextual dependencies from textual data. On the other hand, deep learning architectures that are usually based on Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs) do not suffer from such a limitation and have recently shown very promising results in various NLP applications. In this work, we use deep learning models that combine RNN and CNN for the task of Arabic text categorization using static, dynamic, and fine-tuned word embeddings. The experimental results reported on the Open Source Arabic Corpora (OSAC) dataset have shown the effectiveness and high performance of our proposed models. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Arabic language; Arabic text categorization; Arabic text classification; Convolutional neural networks; Deep learning; Natural language processing; Pretrained word embeddings; Recurrent neural networks,Convolution; Convolutional neural networks; Natural language processing systems; Text processing; Arabic languages; Classification tasks; Learning architectures; Learning models; NAtural language processing; Recurrent neural network (RNNs); Text categorization; Textual documents; Recurrent neural networks
Deep learning for Arabic error detection and correction,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091652810&doi=10.1145%2f3373266&partnerID=40&md5=591ff9fc157cab1a2ae63bbe9c86c6c9,"Research on tools for automating the proofreading of Arabic text has received much attention in recent years. There is an increasing demand for applications that can detect and correct Arabic spelling and grammatical errors to improve the quality of Arabic text content and application input. Our review of previous studies indicates that few Arabic spell-checking research efforts appropriately address the detection and correction of ill-formed words that do not conform to the Arabic morphology system. Even fewer systems address the detection and correction of erroneous well-formed Arabic words that are either contextually or semantically inconsistent within the text. We introduce an approach that investigates employing deep neural network technology for error detection in Arabic text. We have developed a systematic framework for spelling and grammar error detection, as well as correction at the word level, based on a bidirectional long short-term memory mechanism and word embedding, in which a polynomial network classifier is at the top of the system. To get conclusive results, we have developed the most significant gold standard annotated corpus to date, containing 15 million fully inflected Arabic words. The data were collected from diverse text sources and genres, in which every erroneous and ill-formed word has been annotated, validated, and manually revised by Arabic specialists. This valuable asset is available for the Arabic natural language processing research community. The experimental results confirm that our proposed system significantly outperforms the performance of Microsoft Word 2013 and Open Office Ayaspell 3.4, which have been used in the literature for evaluating similar research. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bidirectional long short-term memory; Error correction; Error detection; Polynomial network classifier; Word embedding,Deep neural networks; Error detection; Natural language processing systems; Arabic natural language processing; Error detection and correction; Grammatical errors; Polynomial networks; Research communities; Research efforts; Short term memory; Systematic framework; Deep learning
Morphological segmentation to improve crosslingual word embeddings for low resource languages,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091657798&doi=10.1145%2f3390298&partnerID=40&md5=8fcc503697526126a02b2ca77eec78a5,"Crosslingual word embeddings developed from multiple parallel corpora help in understanding the relationships between languages and improving the prediction quality of machine translation. However, in low resource languages with complex and agglutinative morphologies, inducing good-quality crosslingual embeddings becomes challenging due to the problem of complex morphological forms and rare words. This is true even for languages that share common linguistic structure. In our work, we have shown that performing a simple morphological segmentation upon the corpora prior to the generation of crosslingual word embeddings for both roots and suffixes greatly improves the prediction quality and captures semantic similarities more effectively. To exhibit this, we have chosen two related languages: Telugu and Kannada of the Dravidian language family. We have also tested our method upon a widely spoken North Indian language, Hindi, belonging to the Indo-European language family, and have observed encouraging results. © 2020 Association for Computing Machinery.",Bilingual embeddings; Crosslingual embeddings; Linear transformation; Machine translation; Morphologically rich languages; Morphology; Supervised learning; Word embeddings; Word2vec,Computer aided language translation; Semantics; Agglutinative morphology; European languages; Linguistic structure; Low resource languages; Machine translations; Morphological forms; Morphological segmentation; Semantic similarity; Embeddings
Neural co-training for sentiment classification with product attributes,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091674806&doi=10.1145%2f3394113&partnerID=40&md5=57b1313f7846fe3bca8f7aa2ed0f9895,"Sentiment classification aims to detect polarity from a piece of text. The polarity is usually positive or negative, and the text genre is usually product review. The challenges of sentiment classification are that it is hard to capture semantic of reviews, and the labeled data is hard to annotate. Therefore, we propose neural co-training to learn the semantic representation of each review using the neural network model, and learn the information from unlabeled data using a co-training framework. In particular, we use the attention-based bi-directional Gated Recurrent Unit (Att-BiGRU) to model the semantic content of each review and regard different categories of the target product as different views. We then use a co-training framework to learn and predict the unlabeled reviews with different views. Experiment results with the Yelp dataset demonstrate the effectiveness of our approach. © 2020 Association for Computing Machinery.",Co-training; Product attributes; Semi-supervised sentiment classification,Semantics; Text processing; Bi-directional; Neural network model; Product attributes; Product reviews; Semantic content; Semantic representation; Sentiment classification; Unlabeled data; Recurrent neural networks
Chinese short text classification with mutual-attention convolutional neural networks,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090295007&doi=10.1145%2f3388970&partnerID=40&md5=43a9ed13fce6e78ecb3a0f9a609189b3,"The methods based on the combination of word-level and character-level features can effectively boost performance on Chinese short text classification. A lot of works concatenate two-level features with little processing, which leads to losing feature information. In this work, we propose a novel framework called Mutual-Attention Convolutional Neural Networks, which integrates word and character-level features without losing too much feature information. We first generate two matrices with aligned information of two-level features by multiplying word and character features with a trainable matrix. Then, we stack them as a three-dimensional tensor. Finally, we generate the integrated features using a convolutional neural network. Extensive experiments on six public datasets demonstrate improved performance of our new framework over current methods. © 2020 Association for Computing Machinery.",Convolutional neural networks; Feature integration; Mutual-attention; Short text classification; Word-level and character-level,Classification (of information); Convolution; Text processing; Character level; Chinese short-text; Feature information; Over current; Word and characters; Word level; Convolutional neural networks
A survey of the model transfer approaches to cross-lingual dependency parsing,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091649996&doi=10.1145%2f3383772&partnerID=40&md5=624dc44e1e2dd16c3f3ebc72df8c0a5c,"Cross-lingual dependency parsing approaches have been employed to develop dependency parsers for the languages for which little or no treebanks are available using the treebanks of other languages. A language for which the cross-lingual parser is developed is usually referred to as the target language and the language whose treebank is used to train the cross-lingual parser model is referred to as the source language. The crosslingual parsing approaches for dependency parsing may be broadly classified into three categories: model transfer, annotation projection, and treebank translation. This survey provides an overview of the various aspects of the model transfer approach of cross-lingual dependency parsing. In this survey, we present a classification of the model transfer approaches based on the different aspects of the method. We discuss some of the challenges associated with cross-lingual parsing and the techniques used to address these challenges. In order to address the difference in vocabulary between two languages, some approaches use only non-lexical features of the words to train the models while others use shared representations of the words. Some approaches address the morphological differences by chunk-level transfer rather than word-level transfer. The syntactic differences between the source and target languages are sometimes addressed by transforming the source language treebanks or by combining the resources of multiple source languages. Besides crosslingual transfer parser models may be developed for a specific target language or it may be trained to parse sentences of multiple languages. With respect to the above-mentioned aspects, we look at the different ways in which the methods can be classified. We further classify and discuss the different approaches from the perspective of the corresponding aspects. We also demonstrate the performance of the transferred models under different settings corresponding to the classification aspects on a common dataset. © 2020 Association for Computing Machinery.",Cross-lingual dependency parsing; Cross-lingual model transfer; Delexicalization; Dependency parsing; Direct model transfer; Multi-source dependency parsing; Treebank transformation,Classification (of information); Forestry; Natural language processing systems; Surveys; Syntactics; Dependency parser; Dependency parsing; Lexical features; Multiple languages; Shared representations; Source language; Target language; Three categories; Computational linguistics
Personalized query auto-completion for large-scale POI search at baidu maps,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090402927&doi=10.1145%2f3394137&partnerID=40&md5=8bd4fd74f5dec5497d224db5fb40f500,"Query auto-completion (QAC) is a featured function that has been widely adopted by many sub-domains of search. It can dramatically reduce the number of typed characters and avoid spelling mistakes. These merits of QAC are highlighted to improve user satisfaction, especially when users intend to type in a query on mobile devices. In this article, we will present our industrial solution to the personalized QAC for the point of interest (POI) search at Baidu Maps, a well-known Web mapping service on mobiles in China. The industrial solution makes a good tradeoff between the offline effectiveness of a novel neural learning model that we devised for feature generation and the online efficiency of an off-the-shelf learning to rank (LTR) approach for the real-time suggestion. Besides some practical lessons from how a real-world QAC system is built and deployed in Baidu Maps to facilitate a large number of users in searching tens of millions of POIs, we mainly explore two specific features for the personalized QAC function of the POI search engine: the spatial-temporal characteristics of POIs and the historically queried POIs of individual users. We leverage the large-volume POI search logs in Baidu Maps to conduct offline evaluations of our personalized QAC model measured by multiple metrics, including Mean Reciprocal Rank (MRR), Success Rate (SR), and normalized Discounted Cumulative Gain (nDCG). Extensive experimental results demonstrate that the personalized model enhanced by the proposed features can achieve substantial improvements (i.e., +3.29% MRR, +3.78% SR@1, +5.17% SR@3, +1.96% SR@5, and +3.62% nDCG@5). After deploying this upgraded model into the POI search engine at Baidu Maps for A/B testing online, we observe that some other critical indicators, such as the average number of keystrokes and the average typing speed at keystrokes in a QAC session, which are also related to user satisfaction, decrease as well by 1.37% and 1.69%, respectively. So the conclusion is that the two kinds of features contributed by us are quite helpful in personalized mapping services for industrial practice. © 2020 Association for Computing Machinery.",Baidu Maps; Large-scale point of interest (POI) search; Learning to rank (LTR); Neural networks; Personalized query auto-completion (QAC),Mapping; Search engines; Well testing; Feature generation; Industrial practices; Industrial solutions; Mean reciprocal ranks; Offline evaluation; Personalized model; Spatial-temporal characteristics; Web mapping services; Learning to rank
Emoji-based sentiment analysis using attention networks,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091623863&doi=10.1145%2f3389035&partnerID=40&md5=fd4ff5a42b83f2ad52f748229a7a7ec2,"Emojis are frequently used to express moods, emotions, and feelings in social media. There has been much research on emojis and sentiments. However, existing methods mainly face two limitations. First, they treat emojis as binary indicator features and rely on handcrafted features for emoji-based sentiment analysis. Second, they consider the sentiment of emojis and texts separately, not fully exploring the impact of emojis on the sentiment polarity of texts. In this article, we investigate a sentiment analysis model based on bidirectional long short-term memory, and the model has two advantages compared with the existing work. First, it does not need feature engineering. Second, it utilizes the attention approach to model the impact of emojis on text. An evaluation on 10,042 manually labeled Sina Weibo showed that our model achieves much better performance compared with several strong baselines. To facilitate the related research, our corpus will be publicly available at https://github.com/yx100/emoji. © 2020 Association for Computing Machinery.",Attention; Deep learning; Emoji; Sentiment analysis; Social media,Social networking (online); Analysis models; Feature engineerings; Sina-weibo; Social media; Sentiment analysis
Learning word-vector quantization: A case study in morphological disambiguation,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091642203&doi=10.1145%2f3397967&partnerID=40&md5=2601b13aa7c17280dbbc2b72bee001d3,"We introduced a new classifier named Learning Word-vector Quantization (LWQ) to solve morphological ambiguities in Turkish, which is an agglutinative language. First, a new and morphologically annotated corpus, and then its datasets are prepared with a series of processes. According to datasets, LWQ finds optimal word-vectors positions by moving them in the Euclidean space. LWQ does morphological disambiguation in two steps: First, it defines all solution candidates of an ambiguous word using a morphological analyzer; second, it chooses the best candidate according to its total distances to neighbor words that are not ambiguous. To show LWQ's performance, we have conducted many tests on the corpus by considering the consistency of classification. In the experiments, we achieve 98.4% correct classification ratio to choose correct parse output, which is an excellent level for the literature. © 2020 Association for Computing Machinery.",Classification; Learning vector quantization; Learning word-vector quantization; Turkish morphological disambiguation,Linguistics; Text processing; Vector spaces; Agglutinative language; All solutions; Correct classification ratios; Euclidean spaces; Morphological analyzer; Morphological disambiguation; Total distances; Word vectors; Vector quantization
Deep neural network-based machine translation system combination,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091656416&doi=10.1145%2f3389791&partnerID=40&md5=27d94898b45ae49bf1c408aee50216c3,"Deep neural networks (DNNs) have provably enhanced the state-of-the-art natural language process (NLP) with their capability of feature learning and representation. As one of the more challenging NLP tasks, neural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy and word coverage. It is therefore a promising direction to combine the advantages of both NMT and SMT. In this article, we propose a deep neural network-based system combination framework leveraging both minimum Bayes-risk decoding and multi-source NMT, which take as input the N-best outputs of NMT and SMT systems and produce the final translation. In particular, we apply the proposed model to both RNN and self-attention networks with different segmentation granularity. We verify our approach empirically through a series of experiments on resource-rich Chinese=English and low-resource English=Vietnamese translation tasks. Experimental results demonstrate the effectiveness and universality of our proposed approach, which significantly outperforms the conventional system combination methods and the best individual system output. © 2020 Association for Computing Machinery.",DNN; Low-resource translation; Minimal Bayes-risk decoding; NMT; SMT; System combination,Computational linguistics; Computer aided language translation; Natural language processing systems; Recurrent neural networks; Speech transmission; Conventional systems; Individual systems; Machine translation systems; Machine translations; Minimum bayes risk; Natural language process; Network based systems; Statistical machine translation; Deep neural networks
Native Language Identification of Fluent and Advanced Non-Native Writers,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091855716&doi=10.1145%2f3383202&partnerID=40&md5=f76652042dedf1e1796db5a357f1dbd8,"Native Language Identification (NLI) aims at identifying the native languages of authors by analyzing their text samples written in a non-native language. Most existing studies investigate this task for educational applications such as second language acquisition and require the learner corpora. This article performs NLI in a challenging context of the user-generated-content (UGC) where authors are fluent and advanced non-native speakers of a second language. Existing NLI studies with UGC (i) rely on the content-specific/social-network features and may not be generalizable to other domains and datasets, (ii) are unable to capture the variations of the language-usage-patterns within a text sample, and (iii) are not associated with any outlier handling mechanism. Moreover, since there is a sizable number of people who have acquired non-English second languages due to the economic and immigration policies, there is a need to gauge the applicability of NLI with UGC to other languages. Unlike existing solutions, we define a topic-independent feature space, which makes our solution generalizable to other domains and datasets. Based on our feature space, we present a solution that mitigates the effect of outliers in the data and helps capture the variations of the language-usage-patterns within a text sample. Specifically, we represent each text sample as a point set and identify the top-k stylistically similar text samples (SSTs) from the corpus. We then apply the probabilistic k nearest neighbors' classifier on the identified top-k SSTs to predict the native languages of the authors. To conduct experiments, we create three new corpora where each corpus is written in a different language, namely, English, French, and German. Our experimental studies show that our solution outperforms competitive methods and reports more than 80% accuracy across languages.  © 2020 ACM.",Author profiling; forensic investigation; native language identification; stylometry; text classification,Natural language processing systems; Nearest neighbor search; Pattern recognition; Statistics; Educational Applications; K-nearest neighbors; Native language; Non-native language; Non-native speakers; Number of peoples; Second language acquisition; User generated content (UGC); Text processing
Context-Dependent Sequence-to-Sequence Turkish Spelling Correction,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091826044&doi=10.1145%2f3383200&partnerID=40&md5=f8f99bfe3ca576315cde887a4ed4e850,"In this article, we make use of sequence-to-sequence (seq2seq) models for spelling correction in the agglutinative Turkish language. In the baseline system, misspelled and target words are split into their letters and the letter sequences are fed into the seq2seq model. We prefer letters as the unit of the model due to the agglutinative nature of Turkish, which results in an impractical dictionary size when words are used as a dictionary unit. In order to improve the baseline performance, we incorporate right and left context of the misspelled words. All context words are represented with their first three consonants in the context-dependent model. We train the seq2seq models using a large text corpus collected automatically from the Internet. The corpus contains approximately 4 million sentences. We randomly introduce substitution, deletion, and insertion spelling errors to the words in the corpus. We test the performance of the proposed context-dependent seq2seq model using synthetic and realistic test sets. The synthetic test set is constructed similar to the training set. The realistic test set contains human-made misspellings from Twitter messages. In the experiments, we observed that the proposed context-dependent model performs significantly better than the baseline system. Its correction accuracy reaches 94% on the synthetic dataset. Additionally, the proposed method provides 2.1% absolute improvement over a state-of-the-art Turkish spelling correction system on the Twitter test set.  © 2020 ACM.",agglutinative language; sequence-to-sequence models; Spelling correction,Errors; Natural language processing systems; Social networking (online); Base-line performance; Baseline systems; Context dependent; Context dependent modeling; Correction accuracy; Spelling correction; State of the art; Turkish language; Testing
Structurally Comparative Hinge Loss for Dependency-Based Neural Text Representation,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091821607&doi=10.1145%2f3387633&partnerID=40&md5=442cf3e14faf2f8f7cb2bbdf657c474c,"Dependency-based graph convolutional networks (DepGCNs) are proven helpful for text representation to handle many natural language tasks. Almost all previous models are trained with cross-entropy (CE) loss, which maximizes the posterior likelihood directly. However, the contribution of dependency structures is not well considered by CE loss. As a result, the performance improvement gained by using the structure information can be narrow due to the failure in learning to rely on this structure information. To face the challenge, we propose the novel structurally comparative hinge (SCH) loss function for DepGCNs. SCH loss aims at enlarging the margin gained by structural representations over non-structural ones. From the perspective of information theory, this is equivalent to improving the conditional mutual information of model decision and structure information given text. Our experimental results on both English and Chinese datasets show that by substituting SCH loss for CE loss on various tasks, for both induced structures and structures from an external parser, performance is improved without additional learnable parameters. Furthermore, the extent to which certain types of examples rely on the dependency structure can be measured directly by the learned margin, which results in better interpretability. In addition, through detailed analysis, we show that this structure margin has a positive correlation with task performance and structure induction of DepGCNs, and SCH loss can help model focus more on the shortest dependency path between entities. We achieve the new state-of-the-art results on TACRED, IMDB, and Zh. Literature datasets, even compared with ensemble and BERT baselines.  © 2020 ACM.",graph convolutional networks; loss function; Text representation,Convolutional neural networks; Hinges; Information theory; Conditional mutual information; Convolutional networks; Dependency structures; Positive correlations; Posterior likelihood; Structural representation; Structure information; Text representation; Decision theory
Extracting Arabic Composite Names Using Genitive Principles of Arabic Grammar,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091840598&doi=10.1145%2f3382187&partnerID=40&md5=3241e1eec7ea5ed8ad94ac67370d8888,"Named Entity Recognition (NER) is a basic prerequisite of using Natural Language Processing (NLP) for information retrieval. Arabic NER is especially challenging as the language is morphologically rich and has short vowels with no capitalisation convention. This article presents a novel rule-based approach that uses linguistic grammar-based techniques to extract Arabic composite names from Arabic text. Our approach uniquely exploits the genitive Arabic grammar rules; in particular, the rules regarding the identification of definite nouns (-) and indefinite nouns (-) to support the process of extracting composite names. Based on domain knowledge and Arabic Genitive Rules (AGR), the developed approach formalises a set of syntactical rules and linguistic patterns that initially use genitive patterns to classify definiteness within phrases and then extracts proper composite names from the unstructured text. The developed novel approach does not place any constraints on the length of the Arabic composite name and our initial experimentation demonstrated high recall and precision results when the NER algorithm was applied to a financial domain corpus.  © 2020 ACM.",Arabic language grammar; Arabic named entity recognition; domain knowledge; natural language processing,Linguistics; Text processing; Domain knowledge; Financial domains; Linguistic patterns; Named entity recognition; NAtural language processing; Recall and precision; Rule-based approach; Unstructured texts; Natural language processing systems
Machine Normalization: Bringing Social Media Text from Non-Standard to Standard Form,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091824969&doi=10.1145%2f3378414&partnerID=40&md5=befab00bedbf4006096d2daae9ca5b58,"User-generated text in social media communication (SMC) is mainly characterized by non-standard form. It may contain code switching (CS) text, a widespread phenomenon in SMC, in addition to noisy elements used, especially in written conversations (use of abbreviations, symbols, emoticons) or misspelled words. All of these factors constitute a wall in front of text mining applications. Common text mining tools are dedicated to standard use of standard languages but cannot deal with other forms, especially written text in social media. To overcome these problems, in this work we present our solution for the normalization of non-standard use of standard and non-standard languages (dialects) in SMC text with the use of existent resources and tools. The main processing in our solution consists of CS normalization from multiple to one language by the use of a machine translation - like approach. This processing relies on a linguistic approach of CS, which aims at identifying automatically the translation source and target languages (without human intervention). The remaining processing operations concern the normalization of SMC special expressions and spelling correction of out-of-vocabulary words. To preserve the coded-switched sentence meaning across translation, we adopt a knowledge-based approach for word sense translation disambiguation reinforced with a multi-lingual vertical context. All of these processes are embedded in what we refer to as the machine normalization system. Our solution can be used as a front-end of text mining processing, enabling the analysis of SMC noisy text. The conducted experiments show that our system performs better than considered baselines.  © 2020 ACM.",automatic language identification; code switching normalization; dialects; matrix language; multilingual vertical context; social media; standard languages; Text normalization; word sense disambiguation,Computer aided language translation; Knowledge based systems; Linguistics; Social networking (online); Turing machines; Human intervention; Knowledge-based approach; Linguistic approach; Machine translations; Out of vocabulary words; Processing operations; Spelling correction; Translation disambiguation; Text mining
Lipi Gnani: A Versatile OCR for Documents in any Language Printed in Kannada Script,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091868342&doi=10.1145%2f3387632&partnerID=40&md5=fcafdff095460291f0d25a5feb9c2f40,"A Kannada OCR, called Lipi Gnani, has been designed and developed from scratch, with the motivation of it being able to convert printed text or poetry in Kannada script, without any restriction on vocabulary. The training and test sets have been collected from more than 35 books published from 1970 to 2002, and this includes books written in Halegannada and pages containing Sanskrit slokas written in Kannada script. The coverage of the OCR is nearly complete in the sense that it recognizes all punctuation marks, special symbols, and Indo-Arabic and Kannada numerals. Several minor and major original contributions have been done in developing this OCR at different processing stages, such as binarization, character segmentation, recognition, and Unicode mapping. This has created a Kannada OCR that performs as good as, and in some cases better than, Google's Tesseract OCR, as shown by the results. To the best of our knowledge, this is the maiden report of a complete Kannada OCR, handling all issues involved. Currently, there is no dictionary-based postprocessing, and the obtained results are due solely to the recognition process. Four benchmark test databases containing scanned pages from books in Kannada, Sanskrit, Konkani, and Tulu languages, but all of them printed in Kannada script, have been created. The word-level recognition accuracy of Lipi Gnani is 5.3% higher on the Kannada dataset than that of Google's Tesseract OCR, 8.5% higher on the Sanskrit dataset, and 23.4% higher on the datasets of Konkani and Tulu.  © 2020 ACM.",Halegannada; Kannada; Konkani; OCR; Sanskrit; segmentation; SVM; Tulu,Agricultural engineering; Natural resources; Benchmark tests; Character segmentation; Kannada scripts; Processing stage; Punctuation marks; Recognition accuracy; Recognition process; Special symbols; Benchmarking
Editorial from the New Editor-in-Chief,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091849963&doi=10.1145%2f3397501&partnerID=40&md5=c866680194eccce76e1e51bcc663eaf0,[No abstract available],,
Sign Language Generation System Based on Indian Sign Language Grammar,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091881375&doi=10.1145%2f3384202&partnerID=40&md5=f715177af32fdfbb13f7f5a13373a495,"Sign Language (SL), also known as gesture-based language, is used by people with hearing loss to convey their messages. SL interpreters are required for people who do not have the knowledge of SL, but interpreters are not readily available. Thus, a machine-based translation system is required to translate the text into SL. In this article, a system is implemented for translating English text into Indian Sign Language (ISL). It acts as a tool for human-computer interaction and eliminates the need for an ISL human interpreter for communicating with people who have hearing loss. The system features a rich corpus of English words and commonly used sentences. It consists of components such as an ISL parser, the Hamburg Notation System, the Signing Gesture Mark-up Language, and 3D avatar animation for generating SL according to ISL grammar. The proposed system has been tested rigorously by SL users. The results proved that the proposed system is highly efficient and achieves an average score of accuracy (i.e., 4.2 for English words and 3.8 for sentences on a scale from 1 to 5). The performance of proposed system has also been evaluated using the BiLingual Evaluation Understudy score, which results in 0.95 accuracy. The proposed system and mobile application together has the potential to bring individuals with hearing loss and their entourage together.  © 2020 ACM.",communication; hearing loss people; human-computer interaction; Indian Sign Language (ISL); Sign Language (SL); speech to sign; text to sign,Audition; Human computer interaction; Syntactics; Three dimensional computer graphics; Turing machines; 3D avatar animation; Indian sign languages; Indian sign languages (ISL); Mobile applications; Notation systems; Sign language; System features; Translation systems; Translation (languages)
Adversarial Evaluation of Robust Neural Sequential Tagging Methods for Thai Language,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091874042&doi=10.1145%2f3383201&partnerID=40&md5=1c59388824b08e46025c8e820db81aa2,"Sequential tagging tasks, such as Part-Of-Speech (POS) tagging and Named-Entity Recognition, are the building blocks of many natural language processing applications. Although prior works have reported promising results in standard settings, they often underperform on non-standard text, such as microblogs and social media. In this article, we introduce an adversarial evaluation scheme for the Thai language by creating adversarial examples based on known spelling errors. Furthermore, we propose novel methods including UNK masking, condition initialization with affixation embeddings, and untied-directional self-attention mechanism to enhance robustness and interpretability of the neural networks. We conducted experiments on two Thai corpora: BEST2010 and ORCHID. Our adversarial evaluation schemes reveal that bidirectional LSTM (BiLSTM) do not perform well on adversarial examples. Our best methods match the performance of the BiLSTM baseline model and outperform it on adversarial examples.  © 2020 ACM.",named-entity recognition; Neural networks; part-of-speech tagging,Natural language processing systems; Petroleum reservoir evaluation; Speech recognition; Attention mechanisms; Building blockes; Evaluation scheme; Interpretability; Named entity recognition; Natural language processing applications; Part of speech tagging; Standard setting; Long short-term memory
Joint Model of Entity Recognition and Relation Extraction with Self-attention Mechanism,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091861652&doi=10.1145%2f3387634&partnerID=40&md5=7381a3b82b405d888a42b0ab689cb90e,"In recent years, the joint model of entity recognition (ER) and relation extraction (RE) has attracted more and more attention in the healthcare and medical domains. However, there are some problems with the prior work. The joint model cannot extract all the relations for a specific entity, and the majority of joint models heavily rely on complex artificial features or professional natural language processing (NLP) tools. In this article, we construct a novel joint model that can simultaneously extract all medical entities and relations from medicine Chinese instructions. Moreover, the self-attention mechanism is introduced to the joint model to learn word intra-sentence dependencies. The proposed model is evaluated using a medicine Chinese instruction dataset that we collect and an open dataset provided in CoNLL-2004. Experimental results show that the model with self-attention achieves the state-of-the-art performance.  © 2020 ACM.",entity recognition; Joint model; medicine chinese instruction; relation extraction; self-attention,Extraction; Attention mechanisms; Entity recognition; Joint modeling; Joint models; Medical domains; NAtural language processing; Relation extraction; State-of-the-art performance; Natural language processing systems
Hindi EmotionNet: A Scalable Emotion Lexicon for Sentiment Classification of Hindi Text,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091828830&doi=10.1145%2f3383330&partnerID=40&md5=bd208988eac0fd4c92bf8ba3fe6b0168,"In this study, we create an emotion lexicon for the Hindi language called Hindi EmotionNet. It can assign emotional affinity to words in IndoWordNet. This lexicon contains 3,839 emotion words, with 1,246 positive and 2,399 negative words. We also introduce ambiguous (217 words) and neutral (95 words) emotions to Hindi. Positive emotion words covered nine types of positive emotions, negative emotion words covered eleven types of negative emotions, ambiguous emotion words covered seven types of ambiguous emotions, and neutral emotion words covered two neutral emotions. The proposed Hindi EmotionNet was then applied to opinion classification and emotion classification. We introduce a centrality-based approach for emotion classification that uses degree, closeness, betweenness, and page rank as centrality measures. We also created a dataset of Hindi based on screenplays, stories, and blogs in the language. We translated emotion data from SemEval 2017 into Hindi for further comparison. The proposed approach delivered promising results on opinion and emotion classification, with an accuracy of 85.78% for the former and 75.91% for the latter.  © 2020 ACM.",centrality; emotion classification; Hindi; sentiment analysis,Classification (of information); Betweenness; Centrality measures; Emotion classification; Page ranks; Positive emotions; Sentiment classification; Text processing
Improving Code-mixed POS Tagging Using Code-mixed Embeddings,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091870984&doi=10.1145%2f3380967&partnerID=40&md5=9c58432d2cb02eeef891dda2c5d5ee5e,"Social media data has become invaluable component of business analytics. A multitude of nuances of social media text make the job of conventional text analytical tools difficult. Code-mixing of text is a phenomenon prevalent among social media users, wherein words used are borrowed from multiple languages, though written in the commonly understood roman script. All the existing supervised learning methods for tasks such as Parts Of Speech (POS) tagging for code-mixed social media (CMSM) text typically depend on a large amount of training data. Preparation of such large training data is resource-intensive, requiring expertise in multiple languages. Though the preparation of small dataset is possible, the out of vocabulary (OOV) words pose major difficulty, while learning models from CMSM text as the number of different ways of writing non-native words in roman script is huge. POS tagging for code-mixed text is non-trivial, as tagging should deal with syntactic rules of multiple languages. The important research question addressed by this article is whether abundantly available unlabeled data can help in resolving the difficulties posed by code-mixed text for POS tagging. We develop an approach for scraping and building word embeddings for code-mixed text illustrating it for Bengali-English, Hindi-English, and Telugu-English code-mixing scenarios. We used a hierarchical deep recurrent neural network with linear-chain CRF layer on top of it to improve the performance of POS tagging in CMSM text by capturing contextual word features and character-sequence-based information. We prepared a labeled resource for POS tagging of CMSM text by correcting 19% of labels from an existing resource. A detailed analysis of the performance of our approach with varying levels of code-mixing is provided. The results indicate that the F1-score of our approach with custom embeddings is better than the CRF-based baseline by 5.81%, 5.69%, and 6.3% in Bengali, Hindi, and Telugu languages, respectively.  © 2020 ACM.",code-mixed (multi-lingual) text; deep neural networks; Parts of speech tagging,Deep neural networks; Embeddings; Learning systems; Mixing; Multilayer neural networks; Recurrent neural networks; Social networking (online); Syntactics; Business analytics; Contextual words; Linear chain crf; Multiple languages; Out of vocabulary words; Research questions; Social media datum; Supervised learning methods; Computational linguistics
Outline Extraction with Question-Specific Memory Cells,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091830882&doi=10.1145%2f3377707&partnerID=40&md5=63884a24d969cd446d8ee79b70930822,"Outline extraction has been widely applied in online consultation to help experts quickly understand individual cases. Given a specific case described as unstructured plain text, outline extraction aims to make a summary for this case by answering a set of questions, which in fact is a new type of machine reading comprehension task. Inspired by a recently popular memory network, we propose a novel question-specific memory cell network (QSMCN) to extract information related to multiple questions on-the-fly as it reads texts. QSMCN constructs a specific memory cell for each question, which is sequentially expanded in recurrent neural network style. Each cell contains three specific vectors to first identify whether current input is related to corresponding question and then update question-specific case representation. We add a penalization term in the loss function to make extracted knowledge more reasonable and interpretable. To support this study, we construct a new outline extraction corpus, InjuryCase,1 which is composed of 3,995 real Chinese occupational injury cases. Experimental results show that our method makes a significant improvement. We further apply the proposed framework on two multi-aspect extraction tasks and find that the proposed model also remarkably outperforms existing state-of-the-art methods of the aspect extraction task.  © 2020 ACM.",aspect extraction; memory cell network; Outline extraction,Cells; Cytology; Extraction; Memory architecture; Occupational risks; Semiconductor storage; Case representation; Extract informations; Memory network; Occupational injury; Outline extractions; Reading comprehension; Set of questions; State-of-the-art methods; Recurrent neural networks
Named Entity Recognition and Classification for Punjabi Shahmukhi,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087945794&doi=10.1145%2f3383306&partnerID=40&md5=cd33a423f5ead70fbeafd9a3f4055e33,"Named entity recognition (NER) refers to the identification of proper nouns from natural language text and classifying them into named entity types, such as person, location, and organization. Due to the widespread applications of NER, numerous NER techniques and benchmark datasets have been developed for both Western and Asian languages. Even though Shahmukhi script of the Punjabi language has been used by nearly three fourths of the Punjabi speakers worldwide, Gurmukhi has been the main focus of research activities. Specifically, a benchmark NER corpus for Shahmukhi is non-existent, which has thwarted the commencement of NER research for the Shahmukhi script. To this end, this article presents the development and specifications of the first-ever NER corpus for Shahmukhi. The newly developed corpus is composed of 318,275 tokens and 16,300 named entities, including 11,147 persons, 3,140 locations, and 2,013 organizations. To establish the strength of our corpus, we have compared the specifications of our corpus with its Gurmukhi counterparts. Furthermore, we have demonstrated the usability of our corpus using five supervised learning techniques, including two state-of-the-art deep learning techniques. The results are compared, and valuable insights about the behaviors of the most effective technique are discussed.  © 2020 ACM.",Asian languages; Low-resource languages; named entity recognition; Punjabi; Shahmukhi,Benchmarking; Character recognition; Deep learning; Learning systems; Specifications; Supervised learning; Asian languages; Benchmark datasets; Focus of researches; Learning techniques; Named entities; Named entity recognition; Natural language text; Proper nouns; Natural language processing systems
Towards Integrated Classification Lexicon for Handling Unknown Words in Chinese-Vietnamese Neural Machine Translation,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083468979&doi=10.1145%2f3373267&partnerID=40&md5=e32784f9ef4792648a08ce80be595604,"In Neural Machine Translation (NMT), due to the limitations of the vocabulary, unknown words cannot be translated properly, which brings suboptimal performance of the translation system. For resource-scarce NMT that have small-scale training corpus, the effect is amplified. The traditional approach of amplifying the scale of the corpus is not applicable, because the parallel corpus is difficult to obtain in a resource-scarce setting; however, it is easy to obtain and utilize external knowledge, bilingual lexicon, and other resources. Therefore, we propose classification lexicon approach for processing unknown words in the Chinese-Vietnamese NMT task. Specifically, three types of unknown Chinese-Vietnamese words are classified and their corresponding classification lexicon are constructed by word alignment, Wikipedia extraction, and rule-based methods, respectively. After translation, the unknown words are restored by lexicon for post-processing. Experiment results on Chinese-Vietnamese, English-Vietnamese, and Mongolian-Chinese translations show that our approach significantly improves the accuracy and the performance of NMT especially in a resource-scarce setting. © 2020 ACM.",classification lexicon; Neural machine translation; resource-scarce; unknown words,Computer aided language translation; Bilingual lexicons; External knowledge; Integrated classification; Machine translations; Rule-based method; Sub-optimal performance; Traditional approaches; Translation systems; Computational linguistics
Subword Attentive Model for Arabic Sentiment Analysis: A Deep Learning Approach,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081131636&doi=10.1145%2f3360016&partnerID=40&md5=d311b815ef24d883ca02809ce8252c57,"Social media data is unstructured data where these big data are exponentially increasing day to day in many different disciplines. Analysis and understanding the semantics of these data are a big challenge due to its variety and huge volume. To address this gap, unstructured Arabic texts have been studied in this work owing to their abundant appearance in social media Web sites. This work addresses the difficulty of handling unstructured social media texts, particularly when the data at hand is very limited. This intelligent data augmentation technique that handles the problem of less availability of data are used. This article has proposed a novel architecture for hand Arabic words classification and understands based on convolutional neural networks (CNNs) and recurrent neural networks. Moreover, the CNN technique is the most powerful for the analysis of Arabic tweets and social network analysis. The main technique used in this work is character-level CNN and a recurrent neural network stacked on top of one another as the classification architecture. These two techniques give 95% accuracy in the Arabic texts dataset. © 2020 Royal Society of Chemistry. All rights reserved.",Arabic; convolutional neural network; data augmentation; gated recurrent unit; sentiment evaluation; unstructured texts,Convolution; Convolutional neural networks; Network architecture; Semantics; Sentiment analysis; Social networking (online); Arabic; Data augmentation; gated recurrent unit; sentiment evaluation; Unstructured texts; Recurrent neural networks
Loanword Identification in Low-Resource Languages with Minimal Supervision,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083706507&doi=10.1145%2f3374212&partnerID=40&md5=003c5ff4ec88f4d30134c4fac5c989dd,"Bilingual resources play a very important role in many natural language processing tasks, especially the tasks in cross-lingual scenarios. However, it is expensive and time consuming to build such resources. Lexical borrowing happens in almost every language. This inspires us to detect these loanwords effectively, and to use the ""loanword (in receipt language)""-""donor word (in donor language)"" to extend the bilingual resource for NLP tasks in low-resource languages. In this article, we propose a novel method to identify loanwords in Uyghur. The most important advantage of this method is that the model only relies on large amount of monolingual corpora and only a small scale of annotated data. Our loanword identification model includes two parts: loanword candidate generation and loanword prediction. In the first part, we use two large-scale monolingual corpora and a small bilingual dictionary to train a cross-lingual embedding model. Since semantic unrelated words often cannot be treated as loanword pairs, a loanword candidate list will be generated according to this model and a word list in Uyghur. In the second part, we predict from the preceding candidates based on a log-linear model that integrates several features such as pronunciation similarity, part-of-speech tags, and hybrid language modeling. To evaluate the effectiveness of our proposed method, we conduct two types of experiments: loanword identification and OOV translation. Experimental results showed that (1) our proposed method achieved significant F1 improvements compared to other models in all four loanword identification tasks in Uyghur, and (2) after extending the existing translation models with loanword identification results, OOV rates in several language pairs reduced significantly and the translation performance improved. © 2020 ACM.",cross-lingual embedding; loanword identification; low-resource neural machine translation; out-of-vocabulary; pronunciation similarity; Uyghur loanword,Learning algorithms; Modeling languages; Natural language processing systems; Regression analysis; Semantics; Bilingual dictionary; Bilingual resources; Candidate generation; Hybrid language model; Identification model; Low resource languages; NAtural language processing; Part-of-speech tags; Translation (languages)
Dynamic Updating of the Knowledge Base for a Large-Scale Question Answering System,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083309513&doi=10.1145%2f3377708&partnerID=40&md5=b07c90638556ea0f896dfd2aff484c9a,"Today, the knowledge base question answering (KB-QA) system is promising to achieve a large-scale high-quality reply in the e-commerce industry. However, there exist two major challenges to efficiently support large-scale KB-QA systems. On the one hand, it is difficult to serve tens of thousands of online stores (i.e., constrained by the tuning and deployment time), and it would perform poorly if the systems start without a sufficient number of chat records. On the other hand, current KB-QA systems cannot be updated in an efficient way due to the high cost of knowledge base (KB) updating. In this article, we propose an automatic learning scheme for KB-QA systems, called ALKB-QA, using a vector modeling method to address the preceding two main challenges. The ALKB-QA system provides online stores with basic KB templates that are suitable for many common occasions, and this feature enables the ability to deploy chatbots for a large number of online stores in a short time. Then, the KBs are further updated automatically to adapt to their own businesses (meet different specific needs), leading to increased reply accuracy. Our work has three main contributions. First, the proposed ALKB-QA system has a good business model in the e-commerce industry (serving tens of thousands of online stores with low cost), breaking the scalability limitations of existing KB-QA systems. Second, we assess the reply accuracy of the proposed ALKB-QA system using human evaluations, and the results show that it outperforms human annotation-base approaches. Third, we launched our ALKB-QA system as a real-world business application, and it supports tens of thousands of online stores. © 2020 ACM.",automatic learning; knowledge base; Knowledge base question answering,Costs; Electronic commerce; Knowledge based systems; Natural language processing systems; Automatic-learning; Business applications; Business modeling; Dynamic updating; Human annotations; Human evaluation; Question Answering; Question answering systems; Online systems
S3-NET: SRU-Based Sentence and Self-Matching Networks for Machine Reading Comprehension,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098990952&doi=10.1145%2f3365679&partnerID=40&md5=9c53944678b45540e0b078afbf498ca5,"Machine reading comprehension question answering (MRC-QA) is the task of understanding the context of a given passage to find a correct answer within it. A passage is composed of several sentences; therefore, the length of the input sentence becomes longer, leading to diminished performance. In this article, we propose S3-NET, which adds sentence-based encoding to solve this problem. S3-NET, which is based on a simple recurrent unit architecture, is a deep learning model that solves the MRC-QA by applying matching network to sentence-level encoding. In addition, S3-NET utilizes self-matching networks to compute attention weight for its own recurrent neural network sequences. We perform MRC-QA for the SQuAD dataset of English and MindsMRC dataset of Korean. The experimental results show that for SQuAD, the S3-NET model proposed in this article produces 71.91% and 74.12% exact match and 81.02% and 82.34% F1 in single and ensemble models, respectively, and for MindsMRC, our model achieves 69.43% and 71.28% exact match and 81.53% and 82.77% F1 in single and ensemble models, respectively.  © 2020 ACM.",hierarchical model; Korean MRC-QA; Machine reading comprehension; question answering; sentence and self-matching network; simple recurrent unit,Encoding (symbols); Hierarchical systems; Natural language processing systems; Signal encoding; Hierarchical model; Korean machine reading comprehension question answering; Machine reading comprehension; Matching networks; Question Answering; Reading comprehension; Sentence and self-matching network; Simple recurrent unit; Simple++; Recurrent neural networks
Conducting Natural Language Inference with Word-Pair-Dependency and Local Context,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083320865&doi=10.1145%2f3377704&partnerID=40&md5=c9c009a8d7d40f4a2eac731815b049ea,"This article proposes to conduct natural language inference with novel Enhanced-Relation-Head-Dependent triplets (RHD triplets), which are constructed via enhancing each word in the RHD triplet with its associated local context. Most previous approaches based on deep neural network (DNN) for this task either perform token alignment without considering syntactic dependency among words, or directly use tree- LSTM to generate passage representation with irrelevant information. To improve token alignment and inference judgment with word-pair-dependency, the RHD triplet structure is first proposed. To avoid incorporating irrelevant information, this proposed approach performs comparison directly on each triplet-pair of the given passage-pair (instead of comparing each triplet in a passage with the content merged from the whole opposite passage). Furthermore, to take local context into consideration while conducting token alignment and inference judgment, we also enhance the words of the triplets with their associated local context to improve the performance. Experimental results show that the proposed approach is better than most previous approaches that adopt tree structures, and its performance is comparable to other state-of-the-art approaches (however, our approach is more human comprehensible). © 2020 ACM.",local context; Natural language inference; relation triplet; syntactic relation; textual entailment; word-pair dependency,Alignment; Deep neural networks; Forestry; Trees (mathematics); Local contexts; Natural languages; State-of-the-art approach; Syntactic dependencies; Tree structures; Triplet structures; Word-pairs; Long short-term memory
Isarn DharmaWord Segmentation Using a Statistical Approach with Named Entity Recognition,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081136570&doi=10.1145%2f3359990&partnerID=40&md5=c13c6ef8f88007db81f453bb6dff693d,"In this study, we developed an Isarn Dharma word segmentation system. We mainly focused on solving the word ambiguity and unknown word problems in unsegmented Isarn Dharma text. Ambiguous Isarn Dharma words occur frequently in word construction due to the writing style without tone markers. Thus, words can be interpreted as having different tones and meanings in the same writing text. To overcome these problems, we developed an Isarn Dharma character cluster-(IDCC) based statistical model and affixation and integrated it with the named entity recognition method (IDCC-C-based statistical model and affixation with named entity recognition (NER)). This method integrates the IDCC-based and character-based statistical models to distinguish the word boundaries. The IDCC-based statistical model utilizes the IDCC feature to disambiguate any ambiguous words. The unknown words are handled using the character-based statistical model, based on the character features. In addition, linguistic knowledge is employed to detect the boundaries of a new word based on the construction morphology and NER. In evaluations, we compared the proposed method with various word segmentation methods. The experimental results showed that the proposed method performed slightly better than the othermethods when the corpus size increased. Using the test set, the proposed method obtained the best F-measure of 92.19, an F-measure that was better than the IDCC longest matching grouping at 2.85. © 2020 Royal Society of Chemistry. All rights reserved.",Isarn Dharma; Isarn Tham; statistical model; word segmentation approach,Computational linguistics; Isarn Dharma; Isarn Tham; Linguistic knowledge; Named entity recognition; Statistical approach; Statistical modeling; Word segmentation; Word segmentation systems; Statistics
Enhanced Language Modeling with Proximity and Sentence Relatedness Information for Extractive Broadcast News Summarization,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083318082&doi=10.1145%2f3377407&partnerID=40&md5=9f1d68bfcf01c3fec58d49fba52c0531,"The primary task of extractive summarization is to automatically select a set of representative sentences from a text or spoken document that can concisely express the most important theme of the original document. Recently, language modeling (LM) has been proven to be a promising modeling framework for performing this task in an unsupervised manner. However, there still remain three fundamental challenges facing the existing LM-based methods, which we set out to tackle in this article. The first one is how to construct a more accurate sentence model in this framework without resorting to external sources of information. The second is how to take into account sentence-level structural relationships, in addition to word-level information within a document, for important sentence selection. The last one is how to exploit the proximity cues inherent in sentences to obtain a more accurate estimation of respective sentence models. Specifically, for the first and second challenges, we explore a novel, principled approach that generates overlapped clusters to extract sentence relatedness information from the document to be summarized, which can be used not only to enhance the estimation of various sentence models but also to render sentence-level structural relationships within the document, leading to better summarization effectiveness. For the third challenge, we investigate several formulations of proximity cues for use in sentence modeling involved in the LM-based summarization framework, free of the strict bag-of-words assumption. Furthermore, we also present various ensemble methods that seamlessly integrate proximity and sentence relatedness information into sentence modeling. Extensive experiments conducted on a Mandarin broadcast news summarization task show that such integration of proximity and sentence relatedness information is indeed beneficial for speech summarization. Our proposed summarization methods can significantly boost the performance of an LM-based strong baseline (e.g., with a maximum ROUGE-2 improvement of 26.7% relative) and also outperform several state-of-the-art unsupervised methods compared in the article. © 2020 ACM.",Extractive summarization; language modeling; overlapped clustering; proximity information,Computational linguistics; Natural language processing systems; Accurate estimation; Ensemble methods; External sources; Extractive summarizations; Sentence selection; Speech summarization; Structural relationship; Unsupervised method; Modeling languages
Punjabi to ISO 15919 and Roman Transliteration with Phonetic Rectification,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081118285&doi=10.1145%2f3359991&partnerID=40&md5=7a62adbe05594a7befb5f2d9182f4bab,"Transliteration removes the script barriers. Unfortunately, Punjabi is written in four different scripts, i.e., Gurmukhi, Shahmukhi, Devnagri, and Latin. The Latin script is understandable for nearly all factions of the Punjabi community. The objective of ourwork is to transliterate the Punjabi Gurmukhi script into Latin script. There has been considerable progress in Punjabi to Latin transliteration, but the accuracy of present-day systems is less than 50% (Google Translator has approximately 45% accuracy). We do not have the facility of a rich parallel corpus for Punjabi, so we cannot use the corpus-based techniques ofmachine learning that are in vogue these days. The existing systems of transliteration follow grapheme-based approach. The graphemebased transliteration is unable to handlemany scenarios such as tones, inherent schwa, glottal stops, nasalization, and gemination. In this article, the grapheme-based transliteration has been augmented with phonetic rectification where the Punjabi script is rectified phonetically before applying character-to-character mapping. Handling the inherent short vowel schwa was the major challenge in phonetic rectification. Instead of following the fixed syllabic pattern, we devised a generic finite state transducer to insert schwa. The accuracy of our transliteration system is approximately 96.82%. © 2020 Royal Society of Chemistry. All rights reserved.",computational linguistics; natural language processing; phonology; Punjabi; Transliteration,Computational linguistics; Natural language processing systems; Existing systems; Finite state transducers; NAtural language processing; Parallel corpora; phonology; Punjabi; Transliteration; Transliteration system; Linguistics
Improving Neural Machine Translation with Linear Interpolation of a Short-Path Unit,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083312366&doi=10.1145%2f3377851&partnerID=40&md5=0a0d2fcd9c7d9157bed4b54a5a574826,"In neural machine translation (NMT), the source and target words are at the two ends of a large deep neural network, normally mediated by a series of non-linear activations. The problem with such consequent non-linear activations is that they significantly decrease the magnitude of the gradient in a deep neural network, and thus gradually loosen the interaction between source words and their translations. As a result, a source word may be incorrectly translated into a target word out of its translational equivalents. In this article, we propose short-path units (SPUs) to strengthen the association of source and target words by allowing information flow over adjacent layers effectively via linear interpolation. In particular, we enrich three critical NMT components with SPUs: (1) an enriched encoding model with SPU, which interpolates source word embeddings linearly into source annotations; (2) an enriched decoding model with SPU, which enables the source context linearly flow to target-side hidden states; and (3) an enriched output model with SPU, which further allows linear interpolation of target-side hidden states into output states. Experimentation on Chinese-to-English, English-to-German, and low-resource Tibetan-to-Chinese translation tasks demonstrates that the linear interpolation of SPUs significantly improves the overall translation quality by 1.88, 1.43, and 3.75 BLEU, respectively. Moreover, detailed analysis shows that our approaches much strengthen the association of source and target words. From the preceding, we can see that our proposed model is effective both in rich- and low-resource scenarios. © 2020 ACM.",low resource; Neural machine translation; neural networks; short-path units,Chemical activation; Computational linguistics; Computer aided language translation; Interpolation; Adjacent layers; Encoding models; Hidden state; Information flows; Linear Interpolation; Machine translations; Non-linear activation; Translation quality; Deep neural networks
Uniformly interpolated balancing for robust prediction in translation quality estimation: A case study of English-Korean translation,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078253410&doi=10.1145%2f3365916&partnerID=40&md5=3c16051973dc9f4bd91e37917373aec2,"There has been growing interest among researchers in quality estimation (QE), which attempts to automatically predict the quality of machine translation (MT) outputs. Most existing works on QE are based on supervised approaches using quality-annotated training data. However, QE training data quality scores readily become imbalanced or skewed: QE data are mostly composed of high translation quality sentence pairs but the data lack low translation quality sentence pairs. The use of imbalanced data with an induced quality estimator tends to produce biased translation quality scores with “high” translation quality scores assigned even to poorly translated sentences. To address the data imbalance, this article proposes a simple, efficient procedure called uniformly interpolated balancing to construct more balanced QE training data by inserting greater uniformness to training data. The proposed uniformly interpolated balancing procedure is based on the preparation of two different types of manually annotated QE data: (1) default skewed data and (2) near-uniform data. First, we obtain default skewed data in a naive manner without considering the imbalance by manually annotating qualities on MT outputs. Second, we obtain near-uniform data in a selective manner by manually annotating a subset only, which is selected from the automatically quality-estimated sentence pairs. Finally, we create uniformly interpolated balanced data by combining these two types of data, where one half originates from the default skewed data and the other half originates from the near-uniform data. We expect that uniformly interpolated balancing reflects the intrinsic skewness of the true quality distribution and manages the imbalance problem. Experimental results on an English-Korean quality estimation task show that the proposed uniformly interpolated balancing leads to robustness on both skewed and uniformly distributed quality test sets when compared to the test sets of other non-balanced datasets. © 2020 Association for Computing Machinery.",Imbalanced data; Predictor-Estimator; Translation quality estimation; Uniformly interpolated balancing,Quality control; Annotated training data; Imbalanced data; Machine translations; Predictor-Estimator; Quality distribution; Quality estimation; Robust predictions; Translation quality; Balancing
Efficient low-resource neural machine translation with reread and feedback mechanism,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078309769&doi=10.1145%2f3365244&partnerID=40&md5=4a0e6a8b730c97ac467768178c2323f3,"How to utilize information sufficiently is a key problem in neural machine translation (NMT), which is effectively improved in rich-resource NMT by leveraging large-scale bilingual sentence pairs. However, for low-resource NMT, lack of bilingual sentence pairs results in poor translation performance; therefore, taking full advantage of global information in the encoding-decoding process is effective for low-resource NMT. In this article, we propose a novel reread-feedback NMT architecture (RFNMT) for using global information. Our architecture builds upon the improved sequence-to-sequence neural network and consists of a doubledeck attention-based encoder-decoder framework. In our proposed architecture, the information generated by the first-pass encoding and decoding process flows to the second-pass encoding process for more sufficient parameters initialization and information use. Specifically, we first propose a ""reread"" mechanism to transfer the outputs of the first-pass encoder to the second-pass encoder, and then the output is used for the initialization of the second-pass encoder. Second, we propose a ""feedback"" mechanism that transfers the first-pass decoder's outputs to a second-pass encoder via an important weight model and an improved gated recurrent unit (GRU). Experiments on multiple datasets show that our approach achieves significant improvements over state-of-the-art NMT systems, especially in low-resource settings. © 2020 Association for Computing Machinery. All rights reserved.",Feedback; Low-resource; Neural machine translation; Reread,Computational linguistics; Computer aided language translation; Decoding; Encoding (symbols); Feedback; Network architecture; Signal encoding; Encoding and decoding; Feedback mechanisms; Global informations; Low-resource; Low-resource settings; Machine translations; Proposed architectures; Reread; Information use
Extracting polarity shifting patterns from any corpus based on natural annotation,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078543886&doi=10.1145%2f3345518&partnerID=40&md5=41c263706b3156de3d6773dc9be0e2dc,"In recent years, online sentiment texts are generated by users in various domains and in different languages. Binary polarity classification (positive or negative) on business sentiment texts can help both companies and customers to evaluate products or services. Sometimes, the polarity of sentiment texts can be modified, making the polarity classification difficult. In sentiment analysis, such modification of polarity is termed as polarity shifting, which shifts the polarity of a sentiment clue (emotion, evaluation, etc.). It is well known that detection of polarity shifting can help improve sentiment analysis in texts. However, to detect polarity shifting in corpora is challenging: (1) polarity shifting is normally sparse in texts, making human annotation difficult; (2) corpora with dense polarity shifting are few; we may need polarity shifting patterns from various corpora. In this article, an approach is presented to extract polarity shifting patterns from any text corpus. For the first time, we proposed to select texts rich in polarity shifting by the idea of natural annotation, which is used to replace human annotation.With a sequence mining algorithm, the selected texts are used to generate polarity shifting pattern candidates, and then we rank them by C-value before human annotation. The approach is tested on different corpora and different languages. The results show that our approach can capture various types of polarity shifting patterns, and some patterns are unique to specific corpora. Therefore, for better performance, it is reasonable to construct polarity shifting patterns directly from the given corpus. © 2020 Association for Computing Machinery.",Natural annotation; Polarity shifting; Prior polarity; Sentiment analysis; Sequence mining,Data mining; Corpus-based; Human annotations; Natural annotation; Polarity classification; Polarity shifting; Prior polarity; Sequence mining; Shifting patterns; Sentiment analysis
Korean part-of-speech tagging based on morpheme generation,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078256794&doi=10.1145%2f3373608&partnerID=40&md5=d0c8b7d8f5857880fab617440d4cfc24,"Two major problems of Korean part-of-speech (POS) tagging are that the word-spacing unit is not mapped one-to-one to a POS tag and that morphemes should be recovered during POS tagging. Therefore, this article proposes a novel two-step Korean POS tagger that solves the problems. This tagger first generates a sequence of lemmatized and recovered morphemes that can be mapped one-to-one to a POS tag using an encoder-decoder architecture derived from a POS-tagged corpus. Then, the POS tag of each morpheme in the generated sequence is finally determined by a standard sequence labeling method. Since the knowledge for segmenting and recovering morphemes is extracted automatically from a POS-tagged corpus by an encoderdecoder architecture, the POS tagger is constructed without a dictionary nor handcrafted linguistic rules. The experimental results on a standard dataset show that the proposed method outperforms existing POS taggers with its state-of-the-art performance. © 2020 Association for Computing Machinery. All rights reserved.",Morpheme generation; Morphologically complex languages; Part-of-speech tagging,Industrial plants; Natural language processing systems; Recovery; Signal encoding; Syntactics; Encoder-decoder architecture; Linguistic rules; Morpheme generation; Part of speech tagging; PoS tagging; Sequence Labeling; State-of-the-art performance; Word-spacing; Computational linguistics
StyloThai: A scalable framework for stylometric authorship identification of Thai documents,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078247693&doi=10.1145%2f3365832&partnerID=40&md5=3042c093c3268058114b0a56e15240c3,"Authorship identification helps to identify the true author of a given anonymous document from a set of candidate authors. The applications of this task can be found in several domains, such as law enforcement agencies and information retrieval. These application domains are not limited to a specific language, community, or ethnicity. However, most of the existing solutions are designed for English, and a little attention has been paid to Thai. These existing solutions are not directly applicable to Thai due to the linguistic differences between these two languages. Moreover, the existing solution designed for Thai is unable to (i) handle outliers in the dataset, (ii) scale when the size of the candidate authors set increases, and (iii) perform well when the number of writing samples for each candidate author is low.We identify a stylometric feature space for the Thai authorship identification task. Based on our feature space, we present an authorship identification solution that uses the probabilistic k nearest neighbors classifier by transforming each document into a collection of point sets. Specifically, this document transformation allows us to (i) use set distance measures associated with an outlier handling mechanism, (ii) capture stylistic variations within a document, and (iii) produce multiple predictions for a query document. We create a new Thai authorship identification corpus containing 547 documents from 200 authors, which is significantly larger than the corpus used by the existing study (an increase of 32 folds in terms of the number of candidate authors). The experimental results show that our solution can overcome the limitations of the existing solution and outperforms all competitors with an accuracy level of 91.02%. Moreover, we investigate the effectiveness of each stylometric features category with the help of an ablation study. We found that combining all categories of the stylometric features outperforms the other combinations. Finally, we cross compare the feature spaces and classification methods of all solutions. We found that (i) our solution can scale as the number of candidate authors increases, (ii) our method outperforms all the competitors, and (iii) our feature space provides better performance than the feature space used by the existing study. © 2020 Association for Computing Machinery. All rights reserved.",Authorship analysis; Similarity search; Stylometry; Thai authorship identification,Classification (of information); Nearest neighbor search; Authorship analysis; Authorship identification; Classification methods; Document transformation; K-nearest neighbors classifiers; Law-enforcement agencies; Similarity search; Stylometry; Statistics
Learning and modeling unit embeddings using deep neural networks for unit-selection-based Mandarin speech synthesis,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078266098&doi=10.1145%2f3372244&partnerID=40&md5=7fbd460823ea188759c97d1375a06929,"Amethod of learning andmodeling unit embeddings using deep neutral networks (DNNs) is presented in this article for unit-selection-based Mandarin speech synthesis. Here, a unit embedding is defined as a fixed-length embedding vector for a phone-sized unit candidate in a corpus. Modeling phone-sized embedding vectors instead of frame-sized acoustic features can better measure the long-term dependencies among consecutive units in an utterance. First, a DNN with an embedding layer is built to learn the embedding vectors of all unit candidates in the corpus from scratch. In order to enable the extracted embedding vectors to carry both acoustic and linguistic information of unit candidates, a multitarget learning strategy is designed for the DNN. Its optional prediction targets include frame-level acoustic features, unit durations, monophone and tone identifiers, and context classes. Then, another two DNNs are constructed to map linguistic features toward the extracted embedding vectors. One of them employs the unit vectors of preceding phones besides the linguistic features of current phone as its input. At synthesis time, the distances between the unit vectors predicted by these two DNNs and the ones derived from unit candidates are used as a part of the target cost and a part of the concatenation cost, respectively. Our experiments on a Mandarin speech synthesis corpus demonstrate that learning and modeling unit embeddings improve the naturalness of hidden Markov model (HMM)-based unit selection speech synthesis. Furthermore, integrating multiple targets for learning unit embeddings achieves better performance than using only acoustic targets according to our subjective evaluation results. © 2020 Association for Computing Machinery. All rights reserved.",Deep neural network; Hidden Markov model; Multitarget learning; Speech synthesis; Unit embedding; Unit selection,Embeddings; Hidden Markov models; Linguistics; Speech synthesis; Telephone sets; Vectors; Linguistic features; Linguistic information; Long-term dependencies; Multitarget; Subjective evaluations; Unit embedding; Unit selection; Unit-selection speech synthesis; Deep neural networks
Semantic role labeling system for Persian language,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078267478&doi=10.1145%2f3372246&partnerID=40&md5=2bfb3e2ceda7452f6cc2c781fb6034c4,"In this article, we present an automatic semantic role labeling system in Persian consisting of two modules: Argument identification for specifying argument spans and argument classification for categorizing their semantic roles. Our modules have been trained on Persian Proposition Bank in which predicate-argument information is manually added as a layer on top of PersianDependency Treebankwith about 30,000 sentences. Therefore, our systemwas trained on 216,871 verbal predicates and 42,386 nonverbal ones consisting of 40,813 nouns and 1,573 adjectives with 33 semantic classes. As a supervised method, we used maximum entropy for building an argument identifier that results in human-level accuracy of 99% and support vector machine for an argument classifier with an F1 of 84. Regarding both verbal and nonverbal predicates with an expanded role set, we achieved reasonable results. © 2020 Association for Computing Machinery. All rights reserved.",Argument classification; Argument identification; Persian Proposition Bank; Semantic role labeling,Maximum entropy methods; Support vector machines; Argument identifications; Human levels; Persian languages; Persians; Semantic class; Semantic role labeling; Semantic roles; Supervised methods; Semantics
A Burmese (Myanmar) treebank: Guideline and analysis,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078312949&doi=10.1145%2f3373268&partnerID=40&md5=b7a2a6347ef71e1465731019fc0ea00e,"A 20,000-sentence Burmese (Myanmar) treebank on news articles has been released under a CC BY-NC-SA license. Complete phrase structure annotation was developed for each sentence from the morphologically annotated data prepared in previous work of Ding et al. [1]. As the final result of the Burmese component in the Asian Language Treebank Project, this is the first large-scale, open-access treebank for the Burmese language. The annotation details and features of this treebank are presented. © 2020 Association for Computing Machinery. All rights reserved.",Burmese (Myanmar); Phrase structure; Treebank,Agricultural engineering; Natural resources; Asian languages; Burmese; Myanmars; News articles; Phrase structure; Treebanks; Forestry
Enhanced double-carrier word embedding via phonetics and writing,2020,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078287335&doi=10.1145%2f3344920&partnerID=40&md5=d9565a3817b7f7970ce69c901de72236,"Word embeddings, which map words into a unified vector space, capture rich semantic information. From a linguistic point of view, words have two carriers, speech and writing. Yet the most recent word embedding models focus on only the writing carrier and ignore the role of the speech carrier in semantic expressions. However, in the development of language, speech appears before writing and plays an important role in the development of writing. For phonetic language systems, the written forms are secondary symbols of spoken ones. Based on this idea, we carried out our work and proposed double-carrier word embedding (DCWE). We used DCWE to conduct a simulation of the generation order of speech and writing. We trained written embedding based on phonetic embedding. The final word embedding fuses writing and phonetic embedding. To illustrate that our model can be applied to most languages, we selected Chinese, English, and Spanish as examples and evaluated these models through word similarity and text classification experiments. © 2020 Association for Computing Machinery.",Linguistic; Phonetic embedding; Word embedding,Classification (of information); Linguistics; Semantics; Text processing; Vector spaces; Phonetic embedding; Semantic information; Text classification; Word embedding; Word similarity; Embeddings
Fusion of spatio-temporal information for indic word recognition combining online and offline text data,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077786717&doi=10.1145%2f3364533&partnerID=40&md5=0f80d5c0c606372bed5067dd1b38ed6a,"We present a novel Indic handwritten word recognition scheme by fusion of spatio-temporal information extracted from handwritten images. The main challenge in Indic word recognition lies in its complexity because of modifiers, touching characters, and compound characters. Hidden Markov Models (HMMs) are being used to model such data due to their ability to learn sequential data, however, the recognition performance is not satisfactory. We propose here a Long Short-Term Memory (LSTM)-based architecture for offline Indic word recognition. Offline recognition methods usually involve spatial data, whereas it has been observed that online recognition schemes show better performance than the offline methodologies. Online information usually refers to the temporal information obtained from the strokes of the pen tip while writing, which is missing in offline word images. In this article, an effort has been made to extract the online temporal information from offline images using stroke recovery and later it is combined with spatial information in LSTM architecture. During recognition, the character models are trained using both offline and extracted pseudo-online handwritten data separately. Finally, a novel fusion scheme has been used to combine them together. From the experiment, it is noted that recognition performance of handwritten Indic words improves considerably due to the fusion scheme of spatial and temporal data. © 2019 Association for Computing Machinery.",Fusion; Indic word recognition; LSTM; Offline to online conversion,Character recognition; Fusion reactions; Hidden Markov models; Image processing; Vocabulary control; Handwritten word recognition; Hidden markov models (HMMs); LSTM; Offline; Spatial informations; Spatiotemporal information; Temporal information; Word recognition; Long short-term memory
WASF-VEC: Topology-based word embedding for modern standard Arabic and Iraqi dialect ontology,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077799973&doi=10.1145%2f3345517&partnerID=40&md5=09243fbb009b832b2bebfe2e01d159bc,"Word clustering is a serious challenge in low-resource languages. Since words that share semantics are expected to be clustered together, it is common to use a feature vector representation generated from a distributional theory-based word embedding method. The goal of this work is to utilize Modern Standard Arabic (MSA) for better clustering performance of the low-resource Iraqi vocabulary. We began with a new Dialect Fast Stemming Algorithm (DFSA) that utilizes the MSA data. The proposed algorithm achieved 0.85 accuracy measured by the F1 score. Then, the distributional theory-based word embedding method and a new simple, yet effective, feature vector named Wasf-Vec word embedding are tested. Wasf-Vec word representation utilizes a word’s topology features. The difference between Wasf-Vec and distributional theory-based word embedding is that Wasf-Vec captures relations that are not contextually based. The embedding is followed by an analysis of how the dialect words are clustered within other MSA words. The analysis is based on the word semantic relations that are well supported by solid linguistic theories to shed light on the strong and weak word relation representations identified by each embedding method. The analysis is handled by visualizing the feature vector in two-dimensional (2D) space. The feature vectors of the distributional theory-based word embedding method are plotted in 2D space using the t-sne algorithm, while the Wasf-Vec feature vectors are plotted directly in 2D space. A word’s nearest neighbors and the distance-histograms of the plotted words are examined. For validation purpose of the word classification used in this article, the produced classes are employed in Class-based Language Modeling (CBLM). Wasf-Vec CBLM achieved a 7% lower perplexity (pp) than the distributional theory-based word embedding method CBLM. This result is significant when working with low-resource languages. © 2019 Association for Computing Machinery.",2D visualizing; Arabic language; Class-based language modeling; Dialect; Morphology; Orthographic; Phonology; Topology; Word embedding; Words classification; Words features; Words ontology,Classification (of information); Computational linguistics; Modeling languages; Morphology; Natural language processing systems; Ontology; Semantics; Topology; Vector spaces; Vectors; 2D visualizing; Arabic languages; Class-based language model; Dialect; Orthographic; Phonology; Word embedding; Words features; Embeddings
Word reordering for translation into Korean sign language using syntactically-guided classification,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077801161&doi=10.1145%2f3357612&partnerID=40&md5=335c3f400004bc4e26bd45c21c4e8020,"Machine translation aims to break the language barrier that prevents communication with others and increase access to information. Deaf people face huge language barriers in their daily lives, including access to digital and spoken information. There are very few digital resources for sign language processing. In this article, we present a transfer-based machine translation system for translating Korean-to-Korean Sign Language (KSL) glosses, mainly composed of (1) dictionary-based lexical transfer and (2) a hybrid syntactic transfer based on a data-driven model. In particular, we formulate complicated word reordering problems in syntactic transfer as multi-class classification tasks and propose “syntactically guided” data-driven syntactic transfer. The core part of our study is a neural classification model for reordering order-important constituent pairs with a reordering task that is newly designed for Korean-to-KSL translation. The experiment results evaluated on news transcript data show that the proposed system achieves a BLEU score of 0.512 and a RIBES score of 0.425, significantly improving upon the baseline system performance. © 2019 Association for Computing Machinery.",Machine translation; Neural networks; Sentence embedding; Sign language; Word reordering,Computer aided language translation; Neural networks; Syntactics; Language barriers; Machine translation systems; Machine translations; Multi-class classification; Neural classification; Sentence embedding; Sign language; Word reordering; Computational linguistics
A deep neural network framework for English Hindi question answering,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077800105&doi=10.1145%2f3359988&partnerID=40&md5=e0e7e36bd1e0510b6c4055b770fb1b41,"In this article, we propose a unified deep neural network framework for multilingual question answering (QA). The proposed network deals with the multilingual questions and answers snippets. The input to the network is a pair of factoid question and snippet in the multilingual environment (English and Hindi), and output is the relevant answer from the snippet. We begin by generating the snippet using a graph-based language-independent algorithm, which exploits the lexico-semantic similarity between the sentences. The soft alignment of the question words from the English and Hindi languages has been used to learn the shared representation of the question. The learned shared representation of question and attention-based snippet representation are passed as an input to the answer extraction layer of the network, which extracts the answer span from the snippet. Evaluation on a standard multilingual QA dataset shows the state-of-the-art performance with 39.44 Exact Match (EM) and 44.97 F1 values. Similarly, we achieve the performance of 50.11 Exact Match (EM) and 53.77 F1 values on Translated SQuAD dataset. © 2019 Association for Computing Machinery.",Attention mechanism; Character embedding; Gated recurrent units; Low-resourced languages; Neural networks; Question answering; Snippet generation,Graphic methods; Natural language processing systems; Network layers; Neural networks; Recurrent neural networks; Semantics; Attention mechanisms; Character embedding; Gated recurrent units; Question Answering; Snippet generation; Deep neural networks
Children’s story classification in Indian languages using linguistic and keyword-based features,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077796777&doi=10.1145%2f3342356&partnerID=40&md5=8d1ff6816c8b22706a41745bb7a9148e,"The primary objective of this work is to classify Hindi and Telugu stories into three genres: fable, folk-tale, and legend. In this work, we are proposing a framework for story classification (SC) using keyword and part-of-speech (POS) features. For improving the performance of SC system, feature reduction techniques and combinations of various POS tags are explored. Further, we investigated the performance of SC by dividing the story into parts depending on its semantic structure. In this work, stories are (i) manually divided into parts based on their semantics as introduction, main, and climax; and (ii) automatically divided into equal parts based on number of sentences in a story as initial, middle, and end. We have also examined sentence increment model, which aims at determining an optimum number of sentences required to identify story genre by incremental selection of sentences in a story. Experiments are conducted on Hindi and Telugu story corpora consisting of 300 and 150 short stories, respectively. The performance of SC system is evaluated using different combinations of keyword and POS-based features, with three well-established machine learning classifiers: (i) Naive Bayes (NB), (ii) k-Nearest Neighbour (KNN), and (iii) Support Vector Machine (SVM). Performance of the classifier is evaluated using 10-fold cross-validation and effectiveness of classifier is measured using precision, recall, and F-measure. From the classification results, it is observed that adding linguistic information boosts the performance of story classification. In view of the structure of the story, main, and initial parts of the story have shown comparatively better performance. The results from the sentence incremental model have indicated that the first nine and seven sentences in Hindi and Telugu stories, respectively, are sufficient for better classification of stories. In most of the studies, SVM models outperformed the other models in classification accuracy. © 2019 Association for Computing Machinery.",K-nearest neighbour; Keyword features; Latent semantic analysis; Linear discriminant analysis; Linguistic features; Naive bayes; Sparse representation; Story classification; Support vector machines; Text-to-speech; Vector space model,Classifiers; Discriminant analysis; Nearest neighbor search; Semantics; Text processing; Vector spaces; K-nearest neighbours; Keyword features; Latent Semantic Analysis; Linear discriminant analysis; Linguistic features; Naive bayes; Sparse representation; Story classification; Text to speech; Vector space models; Support vector machines
Transliteration of Arabizi into Arabic script for Tunisian dialect,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077803095&doi=10.1145%2f3364319&partnerID=40&md5=1085d1d650cbe3f58ed9b8693e858f0d,"The evolution of information and communication technology has markedly influenced communication between correspondents. This evolution has facilitated the transmission of information and has engendered new forms of written communication (email, chat, SMS, comments, etc.). Most of these messages and comments are written in Latin script, also called Arabizi. Moreover, the language used in social media and SMS messaging is characterized by the use of informal and non-standard vocabulary, such as repeated letters for emphasis, typos, non-standard abbreviations, and nonlinguistic content like emoticons. Since the Tunisian dialect suffers from the unavailability of basic tools and linguistic resources compared to Modern Standard Arabic, we resort to the use of these written sources as a starting point to build large corpora automatically. In the context of natural language processing and to benefit from these networks’ data, transliterating from Arabizi to Arabic script is a necessary step because most recently available tools for processing the Tunisian dialect expect Arabic script input. Indeed, the transliteration task can help construct and enrich parallel corpora and dictionaries for the Tunisian dialect and can be useful for developing various natural language processing applications such as sentiment analysis, opinion mining, topic detection, and machine translation. In this article, we focus on converting the Tunisian dialect text that is written in Latin script to Arabic script following the Conventional Orthography for Dialectal Arabic. Then, we propose two models to transliterate Arabizi into Arabic script for the Tunisian dialect, namely a rule-based model and a discriminative model as a sequence classification task based on conditional random fields). In the first model, we use a set of transliteration rules to convert the Tunisian dialect Arabizi texts to Arabic script. In the second model, transliteration is performed both at word and character levels. In the end, our models got a character error rate of 10.47%. © 2019 Copyright held by the owner/author(s).",Arabizi corpus; CRF model; Diacritization; Natural language processing; Rule-based approach; Transliteration; Tunisian dialect,Classification (of information); Random processes; Sentiment analysis; Arabizi corpus; Crf models; Diacritization; NAtural language processing; Rule-based approach; Transliteration; Tunisian dialect; Linguistics
Filtered pseudo-parallel corpus improves low-resource neural machine translation,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075726590&doi=10.1145%2f3341726&partnerID=40&md5=42325923cda390ceb2acf4f855032aba,"Large-scale parallel corpora are essential for training high-quality machine translation systems; however, such corpora are not freely available for many language translation pairs. Previously, training data has been augmented by pseudo-parallel corpora obtained by using machine translation models to translate monolingual corpora into the source language. However, in low-resource language pairs, in which only low-accurate machine translation systems can be used, translation quality degrades when a pseudo-parallel corpus is naively used. To improve machine translation performance with low-resource language pairs, we propose a method to effectively expand the training data via filtering the pseudo-parallel corpus using quality estimation based on sentence-level round-trip translation. For experiments with three language pairs that utilized small, medium, and large size parallel corpora, BLEU scores significantly improved for low-resource language pairs. Additionally, the effects of iterative bootstrapping on translation performance quality is investigated; resultingly, it is confirmed that bootstrapping can further improve the translation performance. © 2019 Association for Computing Machinery.",Bootstrapping; Filtering; Low-resource language pairs; Pseudo-parallel corpus; Round-trip translation; Sentence-level similarity metrics,Computational linguistics; Filtration; Iterative methods; Bootstrapping; Low resource languages; Pseudo parallels; Round trip; Similarity metrics; Computer aided language translation
SentiFars: A persian polarity lexicon for sentiment analysis,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075667176&doi=10.1145%2f3345627&partnerID=40&md5=a95387dd4a08bb0e64e8975269ecfdea,"There is no doubt about the usefulness of public opinion toward different issues in social media and theWorld WideWeb. Extracting the feelings of people about an issue from text is not straightforward. Polarity lexicons that assign polarity tags or scores to words and phrases play an important role in sentiment analysis systems. As English is the richest language in this area, getting benefits from existing English resources in order to build new ones has attracted the interest of many researchers in recent years. In this article, we propose a new translation-based approach for building polarity resources in resource-lean languages such as Persian. The results of empirical evaluation of the proposed approach prove its effectiveness. The generated resource is the largest publicly available polarity lexicon for Persian. © 2019 Association for Computing Machinery.",Classifier combination; Polarity extraction; Polarity lexicon; Sentiment analysis; Translation,Sentiment analysis; Social aspects; Classifier combination; Empirical evaluations; Persians; Polarity lexicon; Public opinions; Social media; Translation (languages)
26Layer-wise de-Training and re-Training for ConvS2S machine translation,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075715291&doi=10.1145%2f3358414&partnerID=40&md5=9d6de4743cbb5c4aedec3d59a0311e3a,"The convolutional sequence-To-sequence (ConvS2S) machine translation system is one of the typical neural machine translation (NMT) systems. Training the ConvS2S model tends to get stuck in a local optimum in our pre-studies. To overcome this inferior behavior, we propose to de-Train a trained ConvS2S model in a mild way and retrain to find a better solution globally. In particular, the trained parameters of one layer of the NMT network are abandoned by re-initialization while other layers' parameters are kept at the same time to kick off re-optimization from a new start point and safeguard the new start point not too far from the previous optimum. This procedure is executed layer by layer until all layers of the ConvS2S model are explored. Experiments show that when compared to various measures for escaping from the local optimum, including initialization with random seeds, adding perturbations to the baseline parameters, and continuing training (con-Training) with the baseline models, our method consistently improves the ConvS2S translation quality across various language pairs and achieves better performance. © 2019 Association for Computing Machinery.",ConvS2S; Local optimum; Neural machine translation,Computer aided language translation; Baseline models; ConvS2S; Layer by layer; Local optima; Machine translation systems; Machine translations; Reinitialization; Translation quality; Computational linguistics
Neural conversation generation with auxiliary emotional supervised models,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075732697&doi=10.1145%2f3344788&partnerID=40&md5=b835b31d17ef2f18c7469bf7a8b06688,"An important aspect of developing dialogue agents involves endowing a conversation system with emotion perception and interaction. Most existing emotion dialogue models lack the adaptability and extensibility of different scenes because of their limitation to require a specified emotion category or their reliance on a fixed emotional dictionary. To overcome these limitations, we propose a neural conversation generation with auxiliary emotional supervised model (nCG-ESM) comprising a sequence-to-sequence (Seq2Seq) generation model and an emotional classifier used as an auxiliary model. The emotional classifier was trained to predict the emotion distributions of the dialogues, which were then used as emotion supervised signals to guide the generation model to generate diverse emotional responses. The proposed nCG-ESM is flexible enough to generate responses with emotional diversity, including specified or unspecified emotions, which can be adapted and extended to different scenarios. We conducted extensive experiments on the popular dataset of Weibo post-response pairs. Experimental results showed that the proposed model was capable of producing more diverse, appropriate, and emotionally rich responses, yielding substantial gains in diversity scores and human evaluations. © 2019 Association for Computing Machinery.",Natural language processing; Neural conversation; Sequence-to-sequence model,Natural language processing systems; Auxiliary modeling; Conversation systems; Emotional Classifiers; Emotional response; Human evaluation; NAtural language processing; Neural conversation; Sequence modeling; Behavioral research
"Matching graph, a method for extracting parallel information from comparable corpora",2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074849943&doi=10.1145%2f3329713&partnerID=40&md5=3ce1d3884efea732dbb1db9339145e8b,"Comparable corpora are valuable alternatives for the expensive parallel corpora. They comprise informative parallel fragments that are useful resources for different natural language processing tasks. In this work, a generative model is proposed for efficient extraction of parallel fragments from a pair of comparable documents. The core of the proposed model is a graph called the Matching Graph. The ability of the Matching Graph to be trained on a small initial seed makes it a proper model for language pairs suffering from the scarce resource problem. Experiments show that the Matching Graph performs significantly better than other recently publishedmodels.According to the experiments on English-Persian and Arabic-Persian language pairs, the extracted parallel fragments can be used instead of parallel data for training statistical machine translation systems. Results reveal that the extracted fragments in the best case are able to retrieve about 90% of the information of a statistical machine translation system that is trained on a parallel corpus. Moreover, it is shown that using the extracted fragments as additional information for training statistical machine translation systems leads to an improvement of about 2% for English-Persian and about 1% for Arabic-Persian translation on BLEU score. © 2019 Association for Computing Machinery.",And Arabic languages; Comparable corpora; English; Generative model; Information extraction; Natural language processing; Parallel fragments; Persian; Statistical machine translation,Computational linguistics; Information retrieval; Natural language processing systems; Arabic languages; Comparable corpora; English; Generative model; NAtural language processing; Parallel fragments; Persians; Statistical machine translation; Computer aided language translation
Deep contextualized word embeddings for universal dependency parsing,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074871229&doi=10.1145%2f3326497&partnerID=40&md5=09ccf9da4bb6b0ef21e1bedcd2cadc41,"Deep contextualized word embeddings (Embeddings from Language Model, short for ELMo), as an emerging and effective replacement for the static word embeddings, have achieved success on a bunch of syntactic and semantic NLP problems. However, little is known about what is responsible for the improvements. In this article, we focus on the effect of ELMo for a typical syntax problem-universal POS tagging and dependency parsing. We incorporate ELMo as additional word embeddings into the state-of-the-art POS tagger and dependency parser, and it leads to consistent performance improvements. Experimental results show the model using ELMo outperforms the state-of-the-art baseline by an average of 0.91 for POS tagging and 1.11 for dependency parsing. Further analysis reveals that the improvements mainly result from the ELMo's better abstraction ability on the out-of-vocabulary (OOV) words, and the character-level word representation in ELMo contributes a lot to the abstraction. Based on ELMo's advantage on OOV, experiments that simulate low-resource settings are conducted and the results show that deep contextualized word embeddings are effective for data-insufficient tasks where the OOV problem is severe. © 2019 Association for Computing Machinery.",Deep contextualized word embeddings; Natural language processing; Out-of-vocabulary words; POS tagging; Universal dependency parsing; Visualization,Abstracting; Embeddings; Flow visualization; Natural language processing systems; Semantics; Syntactics; Consistent performance; Dependency parser; Dependency parsing; Low-resource settings; NAtural language processing; Out of vocabulary words; PoS tagging; Word representations; Computational linguistics
μ-forcing: Training variational recurrent autoencoders for text generation,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074852292&doi=10.1145%2f3341110&partnerID=40&md5=65bf9b43393cf8cadddae11bc5913764,"It has been previously observed that training Variational Recurrent Autoencoders (VRAE) for text generation suffers from serious uninformative latent variables problems. The model would collapse into a plain language model that totally ignores the latent variables and can only generate repeating and dull samples. In this article, we explore the reason behind this issue and propose an effective regularizer-based approach to address it. The proposed method directly injects extra constraints on the posteriors of latent variables into the learning process of VRAE, which can flexibly and stably control the tradeoff between the Kullback-Leibler (KL) term and the reconstruction term, making the model learn dense and meaningful latent representations. The experimental results show that the proposed method outperforms several strong baselines and can make the model learn interpretable latent variables and generate diverse meaningful sentences. Furthermore, the proposed method can perform well without using other strategies, such as KL annealing. © 2019 Association for Computing Machinery.",Uninformative latent variables issues; Variational autoencoders; Variational recurrent autoencoders,Agricultural engineering; Natural resources; Autoencoders; Kullback-Leibler; Language model; Latent variable; Learning process; Regularizer; Text generations; Learning systems
An automatic and a machine-assisted method to clean Bilingual corpus,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074878485&doi=10.1145%2f3342351&partnerID=40&md5=a1c05d641533bd0028074ca35c440a0c,"Two different methods of corpus cleaning are presented in this article. One is a machine-assisted technique, which is good to clean small-sized parallel corpus, and the other is an automatic method, which is suitable for cleaning large-sized parallel corpus. A baseline SMT (MOSES) system is used to evaluate these methods. The machine-assisted technique used two features: word alignment and length of the source and target language sentence. These features are used to detect mistranslations in the corpus, which are then handled by a human translator. Experiments of this method are conducted on the English-to-Indian Language Machine Translation (EILMT) corpus (English-Hindi). The Bilingual Evaluation Understudy (BLEU) score is improved by 0.47% for the clean corpus. Automatic method of corpus cleaning uses a combination of two features. One feature is length of source and target language sentence and the second feature is Viterbi alignment score generated by Hidden Markov Model for each sentence pair. Two different threshold values are used for these two features. These values are decided by using a small-sized manually annotated parallel corpus of 206 sentence pairs. Experiments of this method are conducted on the HindEnCorp corpus, released in the workshop of the Association of Computational Linguistics (ACL 2014). The BLEU score is improved by 0.6% on clean corpus. A comparison of the two methods is also presented on EILMT corpus. © 2019 Association for Computing Machinery.",Bilingual corpus cleaning; Statistical machine translation,Computational linguistics; Computer aided language translation; Hidden Markov models; Automatic method; Bilingual corpora; Indian languages; Machine translations; Parallel corpora; Statistical machine translation; Target language; Word alignment; Cleaning
Chinese zero pronoun resolution: A chain-to-chain approach,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074868687&doi=10.1145%2f3321129&partnerID=40&md5=aedcec1c314887221e0445847abcd449,"Chinese zero pronoun (ZP) resolution plays a critical role in discourse analysis. Different from traditional mention-to-mention approaches, this article proposes a chain-to-chain approach to improve the performance of ZP resolution in three aspects. First, consecutive ZPs are clustered into coreferential chains, each working as one independent anaphor as a whole. In this way, those ZPs far away from their overt antecedents can be bridged via other consecutive ZPs in the same coreferential chains and thus better resolved. Second, common noun phrases (NPs) are automatically grouped into coreferential chains using traditional approaches, each working as one independent antecedent candidate as a whole. That is, those NPs occurring in the same coreferential chain are viewed as one antecedent candidate as a whole, and ZP resolution is made between ZP coreferential chains and common NP coreferential chains. In this way, the performance can be much improved due to the effective reduction of the search space by pruning singletons and negative instances. Third and finally, additional features from ZP and common NP coreferential chains are employed to better represent anaphors and their antecedent candidates, respectively. Comprehensive experiments on the OntoNotes V5.0 corpus show that our chain-to-chain approach significantly outperforms the state-of-the-art mentionto- mention approaches. To our knowledge, this is the first work to resolve zero pronouns in a chain-to-chain way. © 2019 Association for Computing Machinery.",Chain-level features; Chain-to-chain approach; Chinese zero pronoun resolution; Zero pronoun coreferential chains,Natural resources; Anaphors; Discourse analysis; Negative instances; Noun phrase; Pronoun resolution; Search spaces; State of the art; Traditional approaches; Agricultural engineering
NeuMorph: Neural morphological tagging for low-resource languages - An experimental study for indic languages,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074939650&doi=10.1145%2f3342354&partnerID=40&md5=1d17c4c70144b23af58041b09917349f,"This article deals with morphological tagging for low-resource languages. For this purpose, five Indic languages are taken as reference. In addition, two severely resource-poor languages, Coptic and Kurmanji, are also considered. The task entails prediction of the morphological tag (case, degree, gender, etc.) of an incontext word. We hypothesize that to predict the tag of a word, considering its longer context such as the entire sentence is not always necessary. In this light, the usefulness of convolution operation is studied resulting in a convolutional neural network (CNN) based morphological tagger. Our proposedmodel (BLSTM-CNN) achieves insightful results in comparison to the present state-of-the-art. Following the recent trend, the task is carried out under three different settings: Single language, across languages, and across keys. Whereas the previous models used only character-level features, we show that the addition of word vectors along with character-level embedding significantly improves the performance of all the models. Since obtaining high-quality word vectors for resource-poor languages remains a challenge, in that scenario, the proposed character-level BLSTM-CNN proves to be most effective. © 2019 Association for Computing Machinery.",Convolutional neural network; Indic languages; Multitask learning; Recurrent neural network; Tagging,Convolution; Recurrent neural networks; Character level; Convolutional neural network; Low resource languages; Morphological tagger; Morphological tagging; Multitask learning; State of the art; Tagging; High level languages
Chinese syntax parsing based on sliding match of semantic string,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074864079&doi=10.1145%2f3329707&partnerID=40&md5=132a9da1d80e1cfc42146d97e1cef1f0,"Different from the current syntax parsing based on deep learning,we present a novel Chinese parsing method, which is based on Sliding Match of Semantic String (SMOSS). (1) Training stage: In a treebank, headwords of tree nodes are represented by semantic codes given in the Synonym Dictionary (Tongyici Cilin). N-gram semantic templates are extracted from every layer of a syntax tree by means of sliding windowto establish one N-gram semantic template library. (2) Parsing stage:Words of a sentence, including headwords of chunks, are represented by the semantic codes from Tongyici Cilin. With the sliding window method, N-gram semantic code strings are extracted tomatch with the templates in the N-gram semantic template library; subsequently, the mapping information of the matched templates is employed to guide the chunking of semantic code strings. The Chinese syntax parsing is completed through continuous matching and chunking. On the same training scale, N-gram semantic template can create favorable conditions for flexible matching and improve the syntax parsing performance. With train and test sets from the Tsinghua Chinese Treebank (TCT), the results are F1-score 99.71% (closed test) and F1-score 70.43% (open test), respectively. © 2019 Association for Computing Machinery.",Chinese parsing; Semantic code template; Sliding match of semantic string; SMOSS,Codes (symbols); Deep learning; Semantics; Template matching; Trees (mathematics); Chinese parsing; Favorable conditions; Mapping information; Semantic codes; Semantic templates; Sliding match of semantic string; Sliding window methods; SMOSS; Syntactics
Importance of signal processing cues in transcription correction for low-resource Indian languages,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074822344&doi=10.1145%2f3342352&partnerID=40&md5=3b06858329335399a1b206d46ee72428,"Accurate phonetic transcriptions are crucial for building robust acoustic models for speech recognition as well as speech synthesis applications. Phonetic transcriptions are not usually provided with speech corpora. A lexicon is used to generate phone-level transcriptions of speech corpora with sentence-level transcriptions. When lexical entries are not available, letter-to-sound (LTS) rules are used. Whether it is a lexicon or LTS, the rules for pronunciation are generic and may not match the spoken utterance. This can lead to transcription errors. The objective of this study is to address the issue of mismatch between the transcription and its acoustic realisation. In particular, the issue of vowel deletions is studied. Group-delay-based segmentation is used to determine insertion/deletion of vowels in the speech utterance. The transcriptions are corrected in the training data based on this. The corrected data are used in automatic speech recognition (ASR) and text to speech synthesis (TTS) systems. ASR and TTS systems built with the corrected transcriptions show improvements in the performance. © 2019 Association for Computing Machinery.",Automatic speech recognition; Hidden Markov model-forced Viterbi alignment; Signal processing cue based on group delay; Text-to-speech synthesis; Transcription mismatch errors,Character recognition; Group delay; Hidden Markov models; Linguistics; Signal processing; Speech synthesis; Automatic speech recognition; Indian languages; Lexical entries; Mismatch errors; Phonetic transcriptions; Sentence level; Speech utterance; Viterbi; Speech recognition
Chinese zero pronoun resolution: A collaborative filtering-based approach,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074814720&doi=10.1145%2f3325884&partnerID=40&md5=56d9ed123d6a933daa7119d5e3f88953,"Semantic information that has been proven to be necessary to the resolution of common noun phrases is typically ignored by most existing Chinese zero pronoun resolvers. This is because that zero pronouns convey no descriptive information, which makes it almost impossible to calculate semantic similarities between the zero pronoun and its candidate antecedents. Moreover, most of traditional approaches are based on the single-candidate model, which considers the candidate antecedents of a zero pronoun in isolation and thus overlooks their reciprocities. To address these problems, we first propose a neural-network-based zero pronoun resolver (NZR) that is capable of generating vector-space semantics of zero pronouns and candidate antecedents. On the basis of NZR, we develop the collaborative filtering-based framework for Chinese zero pronoun resolution task, exploring the reciprocities between the candidate antecedents of a zero pronoun to more rationally re-estimate their importance. Experimental results on the Chinese portion of the OntoNotes 5.0 corpus are encouraging: Our proposed model substantially surpasses the Chinese zero pronoun resolution baseline systems. © 2019 Association for Computing Machinery.",Collaborative filtering; Deep neural network; Zero pronoun resolution,Deep neural networks; Semantics; Vector spaces; Baseline systems; Candidate models; Descriptive information; Pronoun resolution; Semantic information; Semantic similarity; Traditional approaches; Vector space semantics; Collaborative filtering
Urdu named entity recognition: Corpus generation and deep learning applications,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074827193&doi=10.1145%2f3329710&partnerID=40&md5=2a8524548c8bd5f3020b9433d315a77e,"Named Entity Recognition (NER) plays a pivotal role in various natural language processing tasks, such as machine translation and automatic question-answering systems. Recognizing the importance of NER, a plethora of NER techniques forWestern and Asian languages have been developed. However, despite having over 490 million Urdu language speakers worldwide, NER resources for Urdu are either non-existent or inadequate. To fill this gap, this article makes four key contributions. First, we have developed the largest Urdu NER corpus, which contains 926,776 tokens and 99,718 carefully annotated NEs. The developed corpus has at least doubled the number of manually tagged NEs as compared to any of the existing Urdu NER corpora. Second, we have generated six new word embeddings using three different techniques, fastText, Word2vec, and Glove, on two corpora of Urdu text. These are the only publicly available embeddings for the Urdu language, besides the recently released Urdu word embeddings by Facebook. Third, we have pioneered in the application of deep learning techniques, NN and RNN, for Urdu named entity recognition. Finally, we have performed 10-folds of 32 different experiments using the combinations of a traditional supervised learning and deep learning techniques, seven types of word embeddings, and two different Urdu NER datasets. Based on the analysis of the results, several valuable insights are provided about the effectiveness of deep learning techniques, the impact of word embeddings, and variations of datasets. © 2019 Association for Computing Machinery.",Deep learning; FastText; Resource poor languages; Urdu NER corpus; Word embeddings; Word2vec,Embeddings; Learning algorithms; Linguistics; Natural language processing systems; Automatic question answering; FastText; Machine translations; Named entity recognition; NAtural language processing; Three different techniques; Urdu NER corpus; Word2vec; Deep learning
"Transform, combine, and transfer: Delexicalized transfer parser for low-resource languages",2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074811302&doi=10.1145%2f3325886&partnerID=40&md5=7edca2b7903409a76e381aace1be706e,"Transfer parsing has been used for developing dependency parsers for languages with no treebank by using transfer from treebanks of other languages (source languages). In delexicalized transfer, parsed words are replaced by their part-of-speech tags. Transfer parsing may not work well if a language does not follow uniform syntactic structure with respect to its different constituent patterns. Earlier work has used information derived from linguistic databases to transform a source language treebank to reduce the syntactic differences between the source and the target languages. We propose a transformation method where a source language pattern is transformed stochastically to one of the multiple possible patterns followed in the target language. The transformed source language treebank can be used to train a delexicalized parser in the target language. We show that this method significantly improves the average performance of single-source delexicalized transfer parsers. We also show that, in the multi-source settings, parsers trained using a concatenation of transformed source language treebanks work better when a subset of the source language treebanks is used rather than concatenating all of them or only one. However, the problem of selecting the subset of treebanks whose combination gives the best-performing parser from the set of all the available treebanks is hard. We propose a greedy selection heuristic based on the labelled attachment scores of the corresponding single-source parsers trained using the treebanks after transformation. © 2019 Association for Computing Machinery.",Cross-lingual transfer parsing; Delexicalization; Dependency parsing; Syntax; Transfer parsing,Forestry; Natural language processing systems; Syntactics; Cross-lingual; Delexicalization; Dependency parsing; Syntax; Transfer parsing; Computational linguistics
Sentiment analysis for a resource poor language-Roman Urdu,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074842394&doi=10.1145%2f3329709&partnerID=40&md5=f3bbca7bd95441988fc21bdcf3ce88f2,"Sentiment analysis is an important sub-task of Natural Language Processing that aims to determine the polarity of a review. Most of the work done on sentiment analysis is for the resource-rich languages of the world, but very limited work has been done on resource-poor languages. In this work, we focus on developing a Sentiment Analysis System for Roman Urdu, which is a resource-poor language. To this end, a dataset of 11,000 reviews has been gathered from six different domains. Comprehensive annotation guidelines were defined and the dataset was annotated using the multi-annotator methodology. Using the annotated dataset, state-of-the-art algorithms were used to build a sentiment analysis system. To improve the results of these algorithms, four different studies were carried out based on: word-level features, character level features, and feature union. The best results showed that we could reduce the error rate by 12% from the baseline (80.07%). Also, to see if the improvements are statistically significant, we applied t-test and Confidence Interval on the obtained results and found that the best results of each study are statistically significant from the baseline. © 2019 Association for Computing Machinery.",Resource poor language; Roman Urdu; Roman Urdu sentiment analysis,Agricultural engineering; Natural resources; Character level; Confidence interval; Different domains; NAtural language processing; Resource poor language; Resource-Rich; Roman Urdu; State-of-the-art algorithms; Sentiment analysis
Explicitly modeling word translations in neural machine translation,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074843516&doi=10.1145%2f3342353&partnerID=40&md5=9ebb98f223afc25094ae0d5d76427afb,"In this article, we show that word translations can be explicitly incorporated into NMT effectively to avoid wrong translations. Specifically, we propose three cross-lingual encoders to explicitly incorporateword translations into NMT: (1) Factored encoder, which encodes a word and its translation in a vertical way; (2) Gated encoder, which uses a gated mechanism to selectively control the amount of word translations moving forward; and (3) Mixed encoder, which stitchingly learns a word and its translation annotations over sequences where words and their translations are alternatively mixed. Besides, we first use a simple word dictionary approach and then a word sense disambiguation (WSD) approach to effectively model the word context for better word translation. Experimentation on Chinese-to-English translation demonstrates that all proposed encoders are able to improve the translation accuracy for both traditional RNN-based NMT and recent selfattention- based NMT (hereafter referred to as Transformer). Specifically, Mixed encoder yields the most significant improvement of 2.0 in BLEU on the RNN-based NMT, while Gated encoder improves 1.2 in BLEU on Transformer. This indicates the usefulness of an WSD approach in modeling word context for better word translation. This also indicates the effectiveness of our proposed cross-lingual encoders in explicitly modeling word translations to avoid wrong translations in NMT. Finally, we discuss in depth how word translations benefit different NMT frameworks from several perspectives. © 2019 Association for Computing Machinery.",Cross-lingual encoder; Neuralmachine translation; Word sense disambiguation; Word translation,Natural language processing systems; Cross-lingual; Machine translations; Word contexts; Word Sense Disambiguation; Word translation; Signal encoding
Adversarial training for unknown word problems in neural machine translation,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074836850&doi=10.1145%2f3342482&partnerID=40&md5=d448567252d7fb04546d4c849177de2f,"Nearly all of the work in neural machine translation (NMT) is limited to a quite restricted vocabulary, crudely treating all other words the same as an < unk > symbol. For the translation of language with abundant morphology, unknown (UNK) words also come from the misunderstanding of the translation model to the morphological changes. In this study, we explore two ways to alleviate the UNK problem in NMT: a new generative adversarial network (added value constraints and semantic enhancement) and a preprocessing technique that mixes morphological noise. The training process is like a win-win game in which the players are three adversarial sub models (generator, filter, and discriminator). In this game, the filter is to emphasize the discriminator's attention to the negative generations that contain noise and improve the training efficiency. Finally, the discriminator cannot easily discriminate the negative samples generated by the generator with filter and human translations. The experimental results show that the proposed method significantly improves over several strong baseline models across various language pairs and the newly emerged Mongolian-Chinese task is state-of-the-art. © 2019 Association for Computing Machinery.",Generative adversarial network; Neural machine translation; UNK; Value iteration,Computational linguistics; Iterative methods; Semantics; Adversarial networks; Machine translations; Morphological changes; Preprocessing techniques; Semantic enhancements; Training efficiency; Translation models; Value iteration; Computer aided language translation
Machine translation evaluation metric based on dependency parsing model,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073250006&doi=10.1145%2f3312573&partnerID=40&md5=f1156ad5f28bb4721f056909f30e9c1f,"Most of the syntax-based metrics obtain the similarity by comparing the sub-structures extracted from the trees of hypothesis and reference. These sub-structures cannot represent all the information in the trees because their lengths are limited. To sufficiently use the reference syntax information, a new automatic evaluation metric is proposed based on the dependency parsing model. First, a dependency parsing model is trained using the reference dependency tree for each sentence. Then, the hypothesis is parsed by this dependency parsing model and the corresponding hypothesis dependency tree is generated. The quality of hypothesis can be judged by the quality of the hypothesis dependency tree. Unigram F-score is included in the new metric so that lexicon similarity is obtained. According to experimental results, the proposed metric can perform better than METEOR and BLEU on system level and get comparable results with METEOR on sentence level. To further improve the performance, we also propose a combined metric which gets the best performance on the sentence level and on the system level. © 2019 Association for Computing Machinery. All rights reserved.",Automatic evaluation metric; Dependency parsing model; Machine translation,Computer aided language translation; Context free grammars; Forestry; Automatic evaluation; Dependency parsing; Dependency trees; Machine translation evaluations; Machine translations; Sentence level; Sub-structures; System levels; Syntactics
Multi-channel embedding convolutional neural network model for Arabic sentiment classification,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073193822&doi=10.1145%2f3314941&partnerID=40&md5=a68e598ea2ee7f17f62f34547b4992ab,"With the advent of social network services, Arabs' opinions on the web have attracted many researchers in recent years toward detecting and classifying sentiments in Arabic tweets and reviews. However, the impact of word embeddings vectors (WEVs) initialization and dataset balance on Arabic sentiment classification using deep learning has not been thoroughly studied. In this article, a multi-channel embedding convolutional neural network (MCE-CNN) is proposed to improve Arabic sentiment classification by learning sentiment features from different text domains, word, and character n-grams levels. MCE-CNN encodes a combination of different pre-trained word embeddings into the embedding block at each embedding channel and trains these channels in parallel. Besides, a separate feature extraction module implemented in a CNN block is used to extract more relevant sentiment features. These channels and blocks help to start training on high-quality WEVs and fine-tuning them. The performance of MCE-CNN is evaluated on several standard balanced and imbalanced datasets to reflect real-world use cases. Experimental results show that MCE-CNN provides a high classification accuracy and benefits from the second embedding channel on both standard Arabic and dialectal Arabic text, which outperforms state-of-the-art methods. © 2019 Association for Computing Machinery. All rights reserved.",Arabic language; Arabic sentiment classification; Arabic word embeddings; Convolutional neural network; Deep learning; Multi-channel; Neural language models,Classification (of information); Convolution; Deep learning; Deep neural networks; Embeddings; Neural networks; Arabic languages; Convolutional neural network; Language model; Multi channel; Sentiment classification; Text processing
Role of discourse information in Urdu sentiment classification: A Rule-based Method and Machine-learning Technique,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073204237&doi=10.1145%2f3300050&partnerID=40&md5=1671a0a2de88625a2cedc02ae3599d3c,"In computational linguistics, sentiment analysis refers to the classification of opinions in a positive class or a negative class. There exist a lot of different methods for sentiment analysis of the English language, but the literature lacks the availability of methods and techniques for Urdu, which is the largely spoken language in the South Asian sub-continent and the national language of Pakistan. The currently available techniques, such as adjective count method known as Bag of Words (BoW), is not sufficient for classification of complex sentiment written in the Urdu language. Also, the performance of available machine-learning techniques (with legacy features), for classification of Urdu sentiments, are not comparable with the achieved accuracy of other languages. In the case of the English language, the discourse information (sub-sentence-level information) boosts the performance of both the BoW method and machine-learning techniques, but there are very few works available that have tested the context-level information for the sentiment analysis of the Urdu language. This research aims to extract the discourse information from the Urdu sentiments and utilise the discourse information to improve the performance and reduce the error rate of existing techniques for Urdu Sentiment classification. The proposed solution extracts the discourse information, suggests a new set of features for machine-learning techniques, and introduces a set of rules to extend the capabilities of the BoW model. The results show that the task has been enhanced significantly and the performance metrics such as recall, precision, and accuracy are increased by 31.25%, 8.46%, and 21.6%, respectively. In future, the proposed technique can be extended to sentiments with more than two sub-opinions, such as for blogs, reviews, and TV talk shows. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Discourse features; Sentiment feature extraction; Urdu natural language processing; Urdu sentiment classification,Learning algorithms; Machine learning; Sentiment analysis; Discourse features; English languages; Machine learning techniques; National language; NAtural language processing; Performance metrics; Rule-based method; Sentiment classification; Classification (of information)
Toward an effective Igbo part-of-speech tagger,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073211790&doi=10.1145%2f3314942&partnerID=40&md5=f0d558b37a81bee356e94804687831bf,"Part-of-speech (POS) tagging is a well-established technology for most Western European languages and a few other world languages, but it has not been evaluated on Igbo, an agglutinative African language. This article presents POS tagging experiments conducted using an Igbo corpus as a test bed for identifying the POS taggers and the Machine Learning (ML) methods that can achieve a good performance with the small dataset available for the language. Experiments have been conducted using different well-known POS taggers developed for English or European languages, and different training data styles and sizes. Igbo has a number of language-specific characteristics that present a challenge for effective POS tagging. One interesting case is the wide use of verbs (and nominalizations thereof) that have an inherent noun complement, which form “linked pairs” in the POS tagging scheme, but which may appear discontinuously. Another issue is Igbo's highly productive agglutinative morphology, which can produce many variant word forms from a given root. This productivity is a key cause of the out-of-vocabulary (OOV) words observed during Igbo tagging. We report results of experiments on a promising direction for improving tagging performance on such morphologically-inflected OOV words. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",African language; Corpora; Corpus annotation; Igbo; Language technology; Machine learning; Morphological analysis; Natural language processing (NLP); Part-of-speech (POS) tagging; POS tagger; Tagset; Text processing,Industrial plants; Learning algorithms; Learning systems; Machine learning; Natural language processing systems; Statistical tests; Syntactics; Tellurium compounds; Text processing; African languages; Corpora; Corpus annotations; Igbo; Language technology; Morphological analysis; NAtural language processing; Part of speech tagging; PoS taggers; Tagset; Computational linguistics
Multi-round transfer learning for low-resource NMT using multiple high-resource languages,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073231001&doi=10.1145%2f3314945&partnerID=40&md5=ef507d2a1affa4ecc50101a713d5e117,"Neural machine translation (NMT) has made remarkable progress in recent years, but the performance of NMT suffers from a data sparsity problem since large-scale parallel corpora are only readily available for high-resource languages (HRLs). In recent days, transfer learning (TL) has been used widely in low-resource languages (LRLs) machine translation, while TL is becoming one of the vital directions for addressing the data sparsity problem in low-resource NMT. As a solution, a transfer learning method in NMT is generally obtained via initializing the low-resource model (child) with the high-resource model (parent). However, leveraging the original TL to low-resource models is neither able to make full use of highly related multiple HRLs nor to receive different parameters from the same parents. In order to exploit multiple HRLs effectively, we present a language-independent and straightforward multi-round transfer learning (MRTL) approach to low-resource NMT. Besides, with the intention of reducing the differences between high-resource and low-resource languages at the character level, we introduce a unified transliteration method for various language families, which are both semantically and syntactically highly analogous with each other. Experiments on low-resource datasets show that our approaches are effective, significantly outperform the state-of-the-art methods, and yield improvements of up to 5.63 BLEU points. © 2019 Association for Computing Machinery. All rights reserved.",High-resource language; Low-resource language; Multi-round; Neural machine translation; Transfer learning; Transliteration,Computational linguistics; Computer aided language translation; High level languages; High-resource language; Low resource languages; Machine translations; Multi-round; Transfer learning; Transliteration; Learning systems
Syntax-based Chinese-Vietnamese tree-to-tree statistical machine translation with bilingual features,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073243452&doi=10.1145%2f3314938&partnerID=40&md5=66fef6d21aa6fe20d7e6ab323e3e0d4b,"Because of the scarcity of bilingual corpora, current Chinese-Vietnamese machine translation is far from satisfactory. Considering the differences between Chinese and Vietnamese, we investigate whether linguistic differences can be used to supervise machine translation and propose a method of syntax-based Chinese-Vietnamese tree-to-tree statistical machine translation with bilingual features. Analyzing the syntax differences between Chinese and Vietnamese, we define some linguistic difference-based rules, such as attributive position, time adverbial position, and locative adverbial position, and create rewards for similar rules. These rewards are integrated into the extraction of tree-to-tree translation rules, and we optimize the pruning of the search space during the decoding phase. The experiments on Chinese-Vietnamese bilingual sentence translation show that the proposed method performs better than several compared methods. Further, the results show that syntactic difference features, with search pruning, can improve the accuracy of machine translation without degrading the efficiency. © 2019 Association for Computing Machinery.",Chinese-Vietnamese; Linguistic features; Pruning optimization; Statistical machine translation; Tree-to-tree,Computational linguistics; Forestry; Syntactics; Bilingual corpora; Linguistic differences; Linguistic features; Machine translations; Statistical machine translation; Translation rules; Tree-to-tree; Vietnamese; Computer aided language translation
Experience-based Causality Learning for Intelligent Agents,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073188104&doi=10.1145%2f3314943&partnerID=40&md5=30051461b1ac6976697b5362050e17b6,"Understanding causality in text is crucial for intelligent agents. In this article, inspired by human causality learning, we propose an experience-based causality learning framework. Comparing to traditional approaches, which attempt to handle the causality problem relying on textual clues and linguistic resources, we are the first to use experience information for causality learning. Specifically, we first construct various scenarios for intelligent agents, thus, the agents can gain experience from interaction in these scenarios. Then, human participants build a number of training instances for agents of causality learning based on these scenarios. Each instance contains two sentences and a label. Each sentence describes an event that an agent experienced in a scenario, and the label indicates whether the sentence (event) pair has a causal relation. Accordingly, we propose a model that can infer the causality in text using experience by accessing the corresponding event information based on the input sentence pair. Experiment results show that our method can achieve impressive performance on the grounded causality corpus and significantly outperform the conventional approaches. Our work suggests that experience is very important for intelligent agents to understand causality. © 2019 Association for Computing Machinery.",Causality learning; Experience; Grounded language learning; Intelligent agent; Virtual environment,Virtual reality; Causality learning; Conventional approach; Experience; Experience informations; Language learning; Learning frameworks; Linguistic resources; Traditional approaches; Intelligent agents
Multi-task stack propagation for neural quality estimation,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073211352&doi=10.1145%2f3321127&partnerID=40&md5=a344dcf3be502869fc6a45cca8b14e22,"Quality estimation is an important task in machine translation that has attracted increased interest in recent years. A key problem in translation-quality estimation is the lack of a sufficient amount of the quality annotated training data. To address this shortcoming, the Predictor-Estimator was proposed recently by introducing “word prediction” as an additional pre-subtask that predicts a current target word with consideration of surrounding source and target contexts, resulting in a two-stage neural model composed of a predictor and an estimator. However, the original Predictor-Estimator is not trained on a continuous stacking model but instead in a cascaded manner that separately trains the predictor from the estimator. In addition, the Predictor-Estimator is trained based on single-task learning only, which uses target-specific quality-estimation data without using other training data that are available from other-level quality-estimation tasks. In this article, we thus propose a multi-task stack propagation, which extensively applies stack propagation to fully train the Predictor-Estimator on a continuous stacking architecture and multi-task learning to enhance the training data from related other-level quality-estimation tasks. Experimental results on WMT17 quality-estimation datasets show that the Predictor-Estimator trained with multi-task stack propagation provides statistically significant improvements over the baseline models. In particular, under an ensemble setting, the proposed multi-task stack propagation leads to state-of-the-art performance at all the sentence/word/phrase levels for WMT17 quality estimation tasks. © 2019 Association for Computing Machinery. All rights reserved.",Multi-task learning; Predictor-estimator; Stack propagation; Translation quality estimation,Agricultural engineering; Natural resources; Annotated training data; Machine translations; Multitask learning; Predictor-estimator; Single task learning; Stacking architecture; State-of-the-art performance; Translation quality; Learning systems
Identifying and analyzing different aspects of english-hindi code-switching in twitter,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075686891&doi=10.1145%2f3314935&partnerID=40&md5=e66e93d52de43408510b14c1533f25d7,"Code-switching or the juxtaposition of linguistic units from two or more languages in a single utterance, has, in recent times, become very common in text, thanks to social media and other computer mediated forms of communication. In this exploratory study of English-Hindi code-switching on Twitter, we automatically create a large corpus of code-switched tweets and devise techniques to identify the relationship between successive components in a code-switched tweet. More specifically, we identify pragmatic functions such as narrative-evaluative, negative reinforcement, translation or semantically equivalent statements, and so on characterizing the relation between successive components. We analyze the difference/similarity between switching patterns in code-switched and monolingual multi-component tweets. We observe strong dominance of narrative-evaluative (non-opinion to opinion or vice versa) switching in case of both code-switched and monolingualmulti-component tweets in around 40% of cases. Polarity switching appears to be a prevalent switching phenomenon (10%) specifically in code-switched tweets (three to four times higher than monolingual multi-component tweets) where preference of expressing negative sentiment in Hindi is approximately twice compared to English. Positive reinforcement appears to be an important pragmatic function for English multi-component tweets, whereas negative reinforcement plays a key role for Devanagari multi-component tweets. Our results also indicate that the extent and nature of code-switching also strongly depend on the topic (sports, politics, etc.) of discussion. © 2019 Association for Computing Machinery.",Code-switching; Multi-component tweets; Opinion and sentiment detection; Pragmatic function; Twitter,Linguistics; Reinforcement; Social networking (online); Code-switching; Exploratory studies; Multicomponents; Negative sentiments; Polarity switching; Switching patterns; Switching phenomenon; Twitter; Switching
Order-sensitive keywords based response generation in open-domain conversational systems,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071660976&doi=10.1145%2f3343258&partnerID=40&md5=77c708e2daf1e51b03094a3a6ad7ef23,"External keywords are crucial for response generation models to address the generic response problems in open-domain conversational systems. The occurrence of keywords in a response depends heavily on the order of the keywords as they are generated sequentially.Meanwhile, the order of keywords also affects the semantics of a response. Previous keywords based methods mainly focus on the composite of keywords, while the order of keywords has not been sufficiently discussed. In this work, we propose an order-sensitive keywords based model to explore the influence of the order of keywords in open-domain response generation. It automatically inferences the most suitable order that is optimized to generate a natural and relevant response, and subsequently generates the response using the ordered keywords as building blocks. We conducted experiments on a public Twitter dataset and the results show that our approach outperforms the state-of-the-art baselines in both automatic and human evaluations. © 2019 Copyright held by the owner/author(s).",Conversational system; Order sensitive; Response generation; Sequenceto-sequence,Agricultural engineering; Natural resources; Building blockes; Conversational systems; Human evaluation; Order sensitive; Response generation; Sequenceto-sequence; State of the art; Semantics
Automatic diacritics restoration for Tunisian dialect,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075691373&doi=10.1145%2f3297278&partnerID=40&md5=b9fd3596dbb7255af8d91062e56e6ef4,"Modern Standard Arabic, as well as Arabic dialect languages, are usually written without diacritics. The absence of these marks constitute a real problem in the automatic processing of these data by NLP tools. Indeed, writing Arabic without diacritics introduces several types of ambiguity. First, a word without diacratics could have many possible meanings depending on their diacritization. Second, undiacritized surface forms of an Arabic word might have as many as 200 readings depending on the complexity of its morphology [12]. In fact, the agglutination property of Arabic might produce a problem that can only be resolved using diacritics. Third, without diacritics a word could have many possible parts of speech (POS) instead of one. This is the case with the words that have the same spelling and POS tag but a different lexical sense, or words that have the same spelling but different POS tags and lexical senses [8]. Finally, there is ambiguity at the grammatical level (syntactic ambiguity). In this article, we propose the first work that investigates the automatic diacritization of Tunisian Dialect texts. We first describe our annotation guidelines and procedure. Then, we propose two major models, namely a statistical machine translation (SMT) and a discriminative model as a sequence classification task based on Conditional Random Fields (CRF). In the second approach, we integrate POS features to influence the generation of diacritics. Diacritics restoration was performed at both the word and the character levels. The results showed high scores of automatic diacritization based on the CRF system (Word Error Rate (WER) 21.44% for CRF and WER 34.6% for SMT). © 2019 Association for Computing Machinery.",CRF model; Diacritization; Natural language processing; POS tagging; SMT model; Tunisian dialect,Antigen-antibody reactions; Computational linguistics; Computer aided language translation; Data handling; Morphology; Natural language processing systems; Restoration; Speech transmission; Surface mount technology; Syntactics; Crf models; Diacritization; NAtural language processing; PoS tagging; Tunisian dialect; Random processes
Leveraging additional resources for improving statistical machine translation on asian low-resource languages,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075758647&doi=10.1145%2f3314936&partnerID=40&md5=aace4f12b003e0135cdd783a3019482e,"Phrase-based machine translation (MT) systems require large bilingual corpora for training. Nevertheless, such large bilingual corpora are unavailable for most language pairs in the world, causing a bottleneck for the development of MT. For the Asian language pairs-Japanese, Indonesian, Malay paired with Vietnamese-they are also not excluded from the case, in which there are no large bilingual corpora on these low-resource language pairs. Furthermore, although the languages are widely used in the world, there is no prior work on MT, which causes an issue for the development of MT on these languages. In this article, we conducted an empirical study of leveraging additional resources to improve MT for the Asian low-resource language pairs: Translation fromJapanese, Indonesian, andMalay to Vietnamese.We propose an innovative approach that lies in two strategies of building bilingual corpora from comparable data and phrase pivot translation on existing bilingual corpora of the languages paired with English. Bilingual corpora were built from Wikipedia bilingual titles to enhance bilingual data for the low-resource languages. Additionally,we introduced a combined model of the additional resources to create an effective solution to improveMT on the Asian low-resource languages. Experimental results show the effectiveness of our systems with the improvement of +2 to +7 BLEU points. This work contributes to the development of MT on low-resource languages, especially opening a promising direction for the progress of MT on the Asian language pairs. © 2019 Association for Computing Machinery.",Low-resource languages; Pivot methods; Semantic similarity; Sentence alignment; Statistical machine translation,Computational linguistics; Natural language processing systems; Semantics; Low resource languages; Pivot methods; Semantic similarity; Sentence alignment; Statistical machine translation; Computer aided language translation
A supplementary feature set for sentiment analysis in Japanese dialogues,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065790474&doi=10.1145%2f3310283&partnerID=40&md5=601c5ec2bd4e95250815f8026c484f4d,"Recently, real-time affect-awareness has been applied in several commercial systems, such as dialogue systems and computer games. Real-time recognition of affective states, however, requires the application of costly feature extraction methods and/or labor-intensive annotation of large datasets, especially in the case of Asian languages where large annotated datasets are seldom available. To improve recognition accuracy, we propose the use of cognitive context in the form of “emotion-sensitive” intentions. Intentions are often represented through dialogue acts and, as an emotion-sensitive model of dialogue acts, a tagset of interpersonal-relations-directing interpersonal acts (the IA model) is proposed. The model's adequacy is assessed using a sentiment classification task in comparison with two well-known dialogue act models, the SWBD-DAMSL and the DIT++. For the assessment, five Japanese in-game dialogues were annotated with labels of sentiments and the tags of all three dialogue act models which were used to enhance a baseline sentiment classifier system. The adequacy of the IA tagset is demonstrated by a 9% improvement to the baseline sentiment classifier's recognition accuracy, outperforming the other two models by more than 5%. © 2019 Association for Computing Machinery.",Affect-awareness; Dialogue acts; Gaming data; Japanese language; Sentiment recognition,Computer games; Large dataset; Sentiment analysis; Speech processing; Affect-awareness; Dialogue acts; Gaming data; Japanese language; Sentiment recognition; Real time systems
"A survey of opinion mining in Arabic: A comprehensive system perspective covering challenges and advances in tools, resources, models, applications, and visualizations",2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065785888&doi=10.1145%2f3295662&partnerID=40&md5=1bcc5dcd47bf08f8145c2ed36b106821,"Opinion-mining or sentiment analysis continues to gain interest in industry and academics. While there has been significant progress in developing models for sentiment analysis, the field remains an active area of research for many languages across the world, and in particular for the Arabic language, which is the fifth most-spoken language and has become the fourth most-used language on the Internet. With the flurry of research activity in Arabic opinion mining, several researchers have provided surveys to capture advances in the field. While these surveys capture a wealth of important progress in the field, the fast pace of advances in machine learning and natural language processing (NLP) necessitates a continuous need for a more up-to-date literature survey. The aim of this article is to provide a comprehensive literature survey for state-of-the-art advances in Arabic opinion mining. The survey goes beyond surveying previous works that were primarily focused on classification models. Instead, this article provides a comprehensive system perspective by covering advances in different aspects of an opinion-mining system, including advances in NLP software tools, lexical sentiment and corpora resources, classification models, and applications of opinion mining. It also presents future directions for opinion mining in Arabic. The survey also covers latest advances in the field, including deep learning advances in Arabic Opinion Mining. The article provides state-of-the-art information to help new or established researchers in the field as well as industry developers who aim to deploy an operational complete opinion-mining system. Key insights are captured at the end of each section for particular aspects of the opinion-mining system giving the reader a choice of focusing on particular aspects of interest. © 2019 Association for Computing Machinery.",,Application programs; Classification (of information); Data mining; Deep learning; Learning algorithms; Mining machinery; Surveys; Classification models; Comprehensive system; Industry developers; Literature survey; NAtural language processing; Research activities; Spoken languages; State of the art; Sentiment analysis
Handwritten manipuri meetei-mayek classification using convolutional neural network,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065788246&doi=10.1145%2f3309497&partnerID=40&md5=bb1217394de7ea2ca54e70ba3e11da3a,"A new technique for classifying all 56 different characters of the Manipuri Meetei-Mayek (MMM) is proposed herein. The characters are grouped under five categories, which are Eeyek Eepee (original alphabets), Lom Eeyek (additional letters), Cheising Eeyek (digits), Lonsum Eeyek (letters with short endings), and Cheitap Eeyek (vowel signs. Two related works proposed by previous researchers are studied for understanding the benefits claimed by the proposed deep learning approach in handwritten Manipuri Meetei-Mayek. (1) Histogram of Oriented (HOG) with SVM classifier is implemented for thoroughly understanding how HOG features can influence accuracy. (2) The handwritten samples are trained using simple Convolutional Neural Network (CNN) and compared with the proposed CNN-based architecture. Significant progress has been made in the field of Optical Character Recognition (OCR) for well-known Indian languages as well as globally popular languages. Our work is novel in the sense that there is no record of work available to date that is able to classify all 56 classes of the MMM. It will also serve as a pre-cursor for developing end-to-end OCR software for translating old manuscripts, newspaper archives, books, and so on. © 2019 Association for Computing Machinery.",Deep learning; Histogram of oriented gradient (HOG); Meetei-Mayek; Optical character recognition (OCR); Support vector machine (SVM),Convolution; Graphic methods; Neural networks; Optical character recognition; Support vector machines; CNN-based architecture; Convolutional neural network; Histogram of oriented gradients (HOG); Indian languages; Learning approach; Meetei-Mayek; Optical character recognition (OCR); SVM classifiers; Deep learning
"Multi-entity aspect-based sentiment analysis with context, entity, aspect memory and dependency information",2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065769295&doi=10.1145%2f3321125&partnerID=40&md5=50724baee7e8eedae5522dd86c8cc832,"Fine-grained sentiment analysis is a useful tool for producers to understand consumers' needs as well as complaints about products and related aspects from online platforms. In this article, we define a novel task named “Multi-Entity Aspect-Based Sentiment Analysis (ME-ABSA)”. It investigates the sentiment towards entities and their related aspects. It makes the well-studied aspect-based sentiment analysis a special case of this type, where the number of entities is limited to one. We contribute a new dataset for this task, with multi-entity Chinese posts in it. We propose to model context, entity, and aspect memory to address the task and incorporate dependency information for further improvement. Experiments show that our methods perform significantly better than baseline methods on datasets for both ME-ABSA task and ABSA task. The in-depth analysis further validates the effectiveness of our methods and shows that our methods are capable of generalizing to new (entity, aspect) combinations with little loss of accuracy. This observation indicates that data annotation in real applications can be largely simplified. © 2019 Association for Computing Machinery.",Aspect; Dependency; Entity; Sentiment analysis,Data mining; Aspect; Baseline methods; Dependency; Dependency informations; Entity; In-depth analysis; Loss of accuracy; Real applications; Sentiment analysis
Converting dependency structure into Persian phrase structure,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065800312&doi=10.1145%2f3314937&partnerID=40&md5=46311e3f1091602d69bea28ecf4fec89,"Treebank is one of the important and useful resources in natural language processing represented in two different annotated schemas: phrase and dependency structures. There are many works that convert a phrase structure into a dependency structure and vice versa. Most of them are based that exploit the handcrafted head percolation table and argument table in predefined deterministic ways. In this article, we propose a method to convert a dependency structure into a phrase structure by enriching a trainable model of former hybrid strategy approach. By adding a classifier to the algorithm and using postprocessing modification, the quality of conversion is increased. We evaluate our method in two different languages, English and Persian, and then analyze the errors. The results of our experiments show a 46.01% reduction of error rate in English and 76.50% for Persian compared to our baseline. We build a new phrase structure treebank by converting 10,000 sentences of Persian dependency treebank into corresponding phrase structures and correcting them manually. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Dependency structure; Machine learning; Parser; Persian; Phrase structure; Treebank,Forestry; Learning algorithms; Learning systems; Solvents; Syntactics; Dependency structures; Parser; Persians; Phrase structure; Treebanks; Natural language processing systems
A sense annotated corpus for all-words Urdu word sense disambiguation,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065797925&doi=10.1145%2f3314940&partnerID=40&md5=d0f4f0856192d45f1a519d55d65dafce,"Word Sense Disambiguation (WSD) aims to automatically predict the correct sense of a word used in a given context. All human languages exhibit word sense ambiguity, and resolving this ambiguity can be difficult. Standard benchmark resources are required to develop, compare, and evaluate WSD techniques. These are available for many languages, but not for Urdu, despite this being a language with more than 300 million speakers and large volumes of text available digitally. To fill this gap, this study proposes a novel benchmark corpus for the Urdu All-Words WSD task. The corpus contains 5,042 words of Urdu running text in which all ambiguous words (856 instances) are manually tagged with senses from the Urdu Lughat dictionary. A range of baseline WSD models based on n-gram are applied to the corpus, and the best performance (accuracy of 57.71%) is achieved using word 4-gram. The corpus is freely available to the research community to encourage further WSD research in Urdu. © 2019 Association for Computing Machinery.",All-words task; Sense tagged Urdu corpus; Word sense disambiguation,Agricultural engineering; Natural resources; All-words task; Human language; Large volumes; N-grams; Research communities; Sense tagged Urdu corpus; Word sense; Word Sense Disambiguation; Natural language processing systems
A neural semantic parser for math problems incorporating multi-sentence information,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065764177&doi=10.1145%2f3314939&partnerID=40&md5=5341e557ebb7e7fa313726117ab92841,"In this article, we study the problem of parsing a math problem into logical forms. It is an essential pre-processing step for automatically solving math problems. Most of the existing studies about semantic parsing mainly focused on the single-sentence level. However, for parsing math problems, we need to take the information of multiple sentences into consideration. To achieve the task, we formulate the task as a machine translation problem and extend the sequence-to-sequence model with a novel two-encoder architecture and a word-level selective mechanism. For training and evaluating the proposed method, we construct a large-scale dataset. Experimental results show that the proposed two-encoder architecture and word-level selective mechanism could bring significant improvement. The proposed method can achieve better performance than the state-of-the-art methods. © 2019 Association for Computing Machinery.",Math problem solving; Multi-sentence; Selective mechanism; Semantic parsing,Context free grammars; Large dataset; Semantics; Signal encoding; Syntactics; Encoder architecture; Large-scale dataset; Machine translations; Math problem solving; Multi-sentence; Pre-processing step; Semantic parsing; State-of-the-art methods; Problem solving
Regularizing output distribution of abstractive Chinese social media text summarization for improved semantic consistency,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065786104&doi=10.1145%2f3314934&partnerID=40&md5=6f0a9c88b98f65f02ee1bb3c2293c93b,"ive text summarization is a highly difficult problem, and the sequence-to-sequence model has shown success in improving the performance on the task. However, the generated summaries are often inconsistent with the source content in semantics. In such cases, when generating summaries, the model selects semantically unrelated words with respect to the source content as the most probable output. The problem can be attributed to heuristically constructed training data, where summaries can be unrelated to the source content, thus containing semantically unrelated words and spurious word correspondence. In this article, we propose a regularization approach for the sequence-to-sequence model and make use of what the model has learned to regularize the learning objective to alleviate the effect of the problem. In addition, we propose a practical human evaluation method to address the problem that the existing automatic evaluation method does not evaluate the semantic consistency with the source content properly. Experimental results demonstrate the effectiveness of the proposed approach, which outperforms almost all the existing models. Especially, the proposed approach improves the semantic consistency by 4% in terms of human evaluation. © 2019 Association for Computing Machinery.",Abstractive text summarization; Chinese social media text; Natural language processing; Semantic consistency,Semantics; Social networking (online); Text processing; Automatic evaluation; Learning objectives; NAtural language processing; Output distribution; Regularization approach; Semantic consistency; Social media; Text summarization; Natural language processing systems
A comparative analysis on Hindi and English extractive text summarization,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065792104&doi=10.1145%2f3308754&partnerID=40&md5=65c032888eb7b3efcffb56f6b0979643,"Text summarization is the process of transfiguring a large documental information into a clear and concise form. In this article, we present a detailed comparative study of various extractive methods for automatic text summarization on Hindi and English text datasets of news articles. We consider 13 different summarization techniques, namely, TextRank, LexRank, Luhn, LSA, Edmundson, ChunkRank, TGraph, UniRank, NN-ED, NN-SE, FE-SE, SummaRuNNer, and MMR-SE, and we evaluate their performance using various performance metrics, such as precision, recall, F1, cohesion, non-redundancy, readability, and significance. A thorough analysis is done in eight different parts that exhibits the strengths and limitations of these methods, effect of performance over the summary length, impact of language of a document, and other factors as well. A standard summary evaluation tool (ROUGE) and extensive programmatic evaluation using Python 3.5 in Anaconda environment are used to evaluate their outcome. © 2019 Association for Computing Machinery.",Graph-based techniques; Latent semantic analysis; Meta-heuristic-based techniques; Neural networks-based techniques; ROUGE; Text summarization,Graphic methods; Semantics; Graph-based techniques; Latent Semantic Analysis; Metaheuristic; ROUGE; Text summarization; Text processing
Multitask pointer network for Korean dependency parsing,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062335354&doi=10.1145%2f3282442&partnerID=40&md5=e3b9765eb815cc1bb79ba0a04eb6413a,"Dependency parsing is a fundamental problem in natural language processing. We introduce a novel dependency-parsing framework called head-pointing-based dependency parsing. In this framework, we cast the Korean dependency parsing problem as a statistical head-pointing and arc-labeling problem. To address this problem, a novel neural network called the multitask pointer network is devised for a neural sequential head-pointing and type-labeling architecture. Our approach does not require any handcrafted features or language-specific rules to parse dependency. Furthermore, it achieves state-of-the-art performance for Korean dependency parsing. © 2019 Association for Computing Machinery.",Deep learning; Dependency parsing; Head pointing; Multitask pointer networks,Deep learning; Neural networks; Dependency parsing; NAtural language processing; Novel neural network; State-of-the-art performance; Natural language processing systems
From genesis to Creole language: Transfer learning for singlish universal dependencies parsing and POS tagging,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066955141&doi=10.1145%2f3321128&partnerID=40&md5=d8ce88c1ffea8963644277953c74711e,"Singlish can be interesting to the computational linguistics community both linguistically, as a major low-resource creole based on English, and computationally, for information extraction and sentiment analysis of regional social media. In our conference paper, Wang et al. (2017), we investigated part-of-speech (POS) tagging and dependency parsing for Singlish by constructing a treebank under the Universal Dependencies scheme and successfully used neural stacking models to integrate English syntactic knowledge for boosting Singlish POS tagging and dependency parsing, achieving the state-of-the-art accuracies of 89.50% and 84.47% for Singlish POS tagging and dependency, respectively. In this work, we substantially extend Wang et al. (2017) by enlarging the Singlish treebank to more than triple the size and with much more diversity in topics, as well as further exploring neural multi-task models for integrating English syntactic knowledge. Results show that the enlarged treebank has achieved significant relative error reduction of 45.8% and 15.5% on the base model, 27% and 10% on the neural multi-task model, and 21% and 15% on the neural stacking model for POS tagging and dependency parsing, respectively. Moreover, the state-of-the-art Singlish POS tagging and dependency parsing accuracies have been improved to 91.16% and 85.57%, respectively. We make our treebanks and models available for further research. © 2019 Association for Computing Machinery.",Creole language; Dependency parsing; Multi-task network; Neural stacking; Part-of-speech tagging; Singlish; Transfer learning; Universal dependencies,Computational linguistics; Forestry; Sentiment analysis; Creole language; Dependency parsing; Neural stacking; Part of speech tagging; Singlish; Transfer learning; Universal dependencies; Syntactics
Ancient-modern Chinese translation with a new large training dataset,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066938237&doi=10.1145%2f3325887&partnerID=40&md5=1ae70a24879ebd46f34f4ad37e9e8d05,"Ancient Chinese brings the wisdom and spirit culture of the Chinese nation. Automatic translation from ancient Chinese to modern Chinese helps to inherit and carry forward the quintessence of the ancients. However, the lack of large-scale parallel corpus limits the study of machine translation in ancient-modern Chinese. In this article, we propose an ancient-modern Chinese clause alignment approach based on the characteristics of these two languages. This method combines both lexical-based information and statistical-based information, which achieves 94.2 F1-score on our manual annotation Test set. We use this method to create a new large-scale ancient-modern Chinese parallel corpus that contains 1.24M bilingual pairs. To our best knowledge, this is the first large high-quality ancient-modern Chinese dataset. Furthermore, we analyzed and compared the performance of the SMT and various NMT models on this dataset and provided a strong baseline for this task. © 2019 Association for Computing Machinery.",Ancient-Modern Chinese parallel corpus; Bilingual text alignment; Neural machine translation,Computational linguistics; Computer aided language translation; Automatic translation; Bilingual texts; F1 scores; High quality; Machine translations; Manual annotation; Parallel corpora; Training dataset; Large dataset
Towards Burmese (Myanmar) morphological analysis: Syllable-based Tokenization and Part-of-speech Tagging,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066950468&doi=10.1145%2f3325885&partnerID=40&md5=367665cdbab447ce9667f1f8a66c10d2,"This article presents a comprehensive study on two primary tasks in Burmese (Myanmar) morphological analysis: tokenization and part-of-speech (POS) tagging. Twenty thousand Burmese sentences of newswire are annotated with two-layer tokenization and POS-tagging information, as one component of the Asian Language Treebank Project. The annotated corpus has been released under a CC BY-NC-SA license, and it is the largest open-access database of annotated Burmese when this manuscript was prepared in 2017. Detailed descriptions of the preparation, refinement, and features of the annotated corpus are provided in the first half of the article. Facilitated by the annotated corpus, experiment-based investigations are presented in the second half of the article, wherein the standard sequence-labeling approach of conditional random fields and a long short-term memory (LSTM)-based recurrent neural network (RNN) are applied and discussed. We obtained several general conclusions, covering the effect of joint tokenization and POS-tagging and importance of ensemble from the viewpoint of stabilizing the performance of LSTM-based RNN. This study provides a solid basis for further studies on Burmese processing. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Annotated corpus; Burmese (Myanmar); CRF; LSTM-based RNN; Morphological analysis; POS-tagging; Tokenization,Long short-term memory; Open access; Random processes; Syntactics; Annotated corpus; LSTM-based RNN; Morphological analysis; Myanmars; PoS tagging; Tokenization; Computational linguistics
Incorporating multi-level user preference into document-level sentiment classification,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061206364&doi=10.1145%2f3234512&partnerID=40&md5=6ecfb5fb00363088c6e966f9d254efef,"Document-level sentiment classification aims to predict a user's sentiment polarity in a document about a product. Most existing methods only focus on review contents and ignore users who post reviews. In fact, when reviewing a product, different users have different word-using habits to express opinions (i.e., wordlevel user preference), care about different attributes of the product (i.e., aspect-level user preference), and have different characteristics to score the review (i.e., polarity-level user preference). These preferences have great influence on interpreting the sentiment of text. To address this issue, we propose a model called Hierarchical User Attention Network (HUAN), which incorporates multi-level user preference into a hierarchical neural network to perform document-level sentiment classification. Specifically, HUAN encodes different kinds of information (word, sentence, aspect, and document) in a hierarchical structure and imports user embedding and user attention mechanism to model these preferences. Empirical results on two real-world datasets show that HUAN achieves state-of-the-art performance. Furthermore, HUAN can also mine important attributes of products for different users. © 2018 Association for Computing Machinery.",Deep learning; Hierarchical attention network; Sentiment classification; User preference,Deep learning; Hierarchical neural networks; Hierarchical structures; Multi-level users; Real-world datasets; Review contents; Sentiment classification; State-of-the-art performance; User preference; Information retrieval systems
"""UTTAM"": An efficient spelling correction system for Hindi language based on supervised learning",2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061209910&doi=10.1145%2f3264620&partnerID=40&md5=0240230db0f02704dc32d5f4a252d3fa,"In this article, we propose a system called ""UTTAM,"" for correcting spelling errors in Hindi language text using supervised learning. Unlike other languages, Hindi contains a large set of characters, words with inflections and complex characters, phonetically similar sets of characters, and so on. The complexity increases the possibility of confusion and occasionally leads to entering a wrong character in a word. The existence of spelling errors in text significantly decreases the accuracy of the available resources, like search engine, text editor, and so on. The proposed work is the first approach to correct non-word (Out of Vocabulary) errors as well as real-word errors simultaneously in a sentence of Hindi language. The proposed method investigates the human behavior, i.e., the type and frequency of spelling errors done by humans in Hindi text. Based on the type and frequency of spelling errors, the heterogeneous data is collected in matrices. This data in matrices is used to generate the suitable candidate words for an input word. After generating candidate words, the Viterbi algorithm is applied to perform the word correction. The Viterbi algorithm finds the best sequence of candidate words to correct the input sentence. For Hindi, this work is the first attempt for real-word error correction. For non-word errors, the experiments show that ""UTTAM"" performs better than the existing systems SpellGuru and Saksham. © 2018 Association for Computing Machinery.",Hindi language; Natural language processing; Spelling correction; Viterbi algorithm,Behavioral research; Error correction; Machine learning; Matrix algebra; Natural language processing systems; Search engines; Supervised learning; Complex character; Existing systems; Heterogeneous data; Hindi language; Human behaviors; NAtural language processing; Real-word errors; Spelling correction; Viterbi algorithm
POS Tag-enhanced Coarse-to-fine Attention for Neural Machine Translation,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065620270&doi=10.1145%2f3321124&partnerID=40&md5=8bc2212037b2b190f78edb843d154020,"Although neural machine translation (NMT) has certain capability to implicitly learn semantic information of sentences, we explore and show that Part-of-Speech (POS) tags can be explicitly incorporated into the attention mechanism of NMT effectively to yield further improvements. In this article, we propose an NMT model with tag-enhanced attention mechanism. In our model, NMT and POS tagging are jointly modeled via multi-task learning. Besides following common practice to enrich encoder annotations by introducing predicted source POS tags, we exploit predicted target POS tags to refine attention model in a coarse-to-fine manner. Specifically, we first implement a coarse attention operation solely on source annotations and target hidden state, where the produced context vector is applied to update target hidden state used for target POS tagging. Then, we perform a fine attention operation that extends the coarse one by further exploiting the predicted target POS tags. Finally, we facilitate word prediction by simultaneously utilizing the context vector from fine attention and the predicted target POS tags. Experimental results and further analyses on Chinese-English and Japanese-English translation tasks demonstrate the superiority of our proposed model over the conventional NMT models. We release our code at https://github.com/middlekisser/PEA-NMT.git. © 2019 Association for Computing Machinery.",Attention model; Neural machine translation; POS tags,Computer aided language translation; Semantics; Speech transmission; Attention mechanisms; Attention model; Context vector; Machine translations; Multitask learning; Part-of-speech tags; Semantic information; Word prediction; Computational linguistics
Chinese-Catalan: A neural machine translation approach based on pivoting and attention mechanisms,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065611448&doi=10.1145%2f3312575&partnerID=40&md5=c7124dca8dd154a9875fe80c48a1f256,"This article innovatively addresses machine translation from Chinese to Catalan using neural pivot strategies trained without any direct parallel data. The Catalan language is very similar to Spanish from a linguistic point of view, which motivates the use of Spanish as pivot language. Regarding neural architecture, we are using the latest state-of-the-art, which is the Transformer model, only based on attention mechanisms. Additionally, this work provides new resources to the community, which consists of a human-developed gold standard of 4,000 sentences between Catalan and Chinese and all the others United Nations official languages (Arabic, English, French, Russian, and Spanish). Results show that the standard pseudo-corpus or synthetic pivot approach performs better than cascade. © 2019 Copyright held by the owner/author(s).",Chinese-Catalan; Neural machine translation; Pivot approaches; Transformer,Computational linguistics; Attention mechanisms; Catalans; Machine translations; Neural architectures; Official languages; Pivot approaches; Transformer; Transformer modeling; Computer aided language translation
A rule-based Kurdish text transliteration system,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061180141&doi=10.1145%2f3278623&partnerID=40&md5=c7de75df4b1b212a6854fc766393c58c,"In this article, we present a rule-based approach for transliterating two of the most used orthographies in Sorani Kurdish. Our work consists of detecting a character in a word by removing the possible ambiguities and mapping it into the target orthography. We describe different challenges in Kurdish text mining and propose novel ideas concerning the transliteration task for Sorani Kurdish. Our transliteration system, named Wergor, achieves 82.79% overall precision and more than 99% in detecting the double-usage characters. We also present a manually transliterated corpus for Kurdish. © 2019 Association for Computing Machinery.",Kurdish; Less-resourced language processing; Rule-based approach; Transliteration,Agricultural engineering; Natural resources; Kurdish; Language processing; Rule based; Rule-based approach; Text mining; Transliteration; Transliteration system; Data mining
Co-occurrence Weight Selection in Generation of Word Embeddings for Low Resource Languages,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060008762&doi=10.1145%2f3282443&partnerID=40&md5=46d14830fa267c450576c35e0fffdbd7,"This study aims to increase the performance of word embeddings by proposing a new weighting scheme for co-occurrence counting. The idea behind this new family of weights is to overcome the disadvantage of distant appearing word pairs, which are indeed semantically close, while representing them in the co-occurrence counting. For high-resource languages, this disadvantage might not be effective due to the high frequency of co-occurrence. However, when there are not enough available resources, such pairs suffer from being distant. To favour such pairs, a weighting scheme based on a polynomial fitting procedure is proposed to shift the weights up for distant words while the weights of nearby words are left almost unchanged. The parameter optimization for new weights and the effects of the weighting scheme are analysed for the English, Italian, and Turkish languages. A small portion of English resources and a quarter of Italian resources are utilized for demonstration purposes, as if these languages are low-resource languages. Performance increase is observed in analogy tests when the proposed weighting scheme is applied to relatively small corpora (i.e., mimicking low-resource languages) of both English and Italian. To show the effectiveness of the proposed scheme in small corpora, it is also shown for a large English corpus that the performance of the proposed weighting scheme cannot outperform the original weights. Since Turkish is relatively a low-resource language, it is demonstrated that the proposed weighting scheme can increase the performance of both analogy and similarity tests when all Turkish Wikipedia pages are utilized as a corpus. The positive effect of the proposed scheme has also been demonstrated in a standard sentiment analysis task for the Turkish language. © 2019 Association for Computing Machinery.",Co-occurrence weighting; Computational linguistics; Word embeddings,Computational linguistics; Sentiment analysis; Co-occurrence; Embeddings; High frequency HF; Low resource languages; Parameter optimization; Polynomial fittings; Turkish language; Weighting scheme; Natural language processing systems
On the usage of a classical Arabic corpus as a language resource: Related research and key challenges,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060042253&doi=10.1145%2f3277591&partnerID=40&md5=333d226f34ba0c73940c52acda4e1cc9,"This article presents a literature review of computer-science-related research applied on hadith, a kind of Arabic narration which appeared in the 7th century. We study and compare existent works in several fields of Natural Language Processing (NLP), Information Retrieval (IR), and Knowledge Extraction (KE). Thus, we illicit their main drawbacks and identify some perspectives, which may be considered by the research community. We also study the characteristics of these types of documents, by enumerating the advantages/limits of using hadith as a language resource. Moreover, our study shows that previous studies used different collections of hadiths, thus making it hard to compare their results objectively. Besides, many preprocessing steps are recurrent through these applications, thus wasting a lot of time. Consequently, the key issues for building generic language resources from hadiths are discussed, taking into account the relevance of related literature and the wide community of researchers that are interested in these narrations. The ultimate goal is to structure hadith books for multiple usages, thus building common collections which may be exploited in future applications. © 2019 Association for Computing Machinery.",Hadith knowledge extraction; Hadith mining; Hadith processing; Hadith retrieval,Extraction; Natural language processing systems; Classical arabics; Future applications; Hadith retrieval; Knowledge extraction; Language resources; Literature reviews; Pre-processing step; Research communities; Data mining
Unsupervised joint PoS tagging and stemming for agglutinative languages,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061191879&doi=10.1145%2f3292398&partnerID=40&md5=165b3ed4e65eea7244339de89d09c81c,"The number of possible word forms is theoretically infinite in agglutinative languages. This brings up the out-of-vocabulary (OOV) issue for part-of-speech (PoS) tagging in agglutinative languages. Since inflectional morphology does not change the PoS tag of a word, we propose to learn stems along with PoS tags simultaneously. Therefore, we aim to overcome the sparsity problem by reducing word forms into their stems. We adopt a Bayesian model that is fully unsupervised. We build a Hidden Markov Model for PoS tagging where the stems are emitted through hidden states. Several versions of the model are introduced in order to observe the effects of different dependencies throughout the corpus, such as the dependency between stems and PoS tags or between PoS tags and affixes. Additionally, we use neural word embeddings to estimate the semantic similarity between the word form and stem.We use the semantic similarity as prior information to discover the actual stem of a word since inflection does not change the meaning of a word. We compare our models with other unsupervised stemming and PoS tagging models on Turkish, Hungarian, Finnish, Basque, and English. The results show that a joint model for PoS tagging and stemming improves on an independent PoS tagger and stemmer in agglutinative languages. © 2019 Association for Computing Machinery.",Hidden Markov models (HMM); Joint learning; Neural word embeddings; Part-of-speech (PoS) tagging; Stemming; Unsupervised learning,Bayesian networks; Computational linguistics; Embeddings; Natural language processing systems; Semantics; Trellis codes; Unsupervised learning; Agglutinative language; Bayesian model; Joint learning; Part of speech tagging; Prior information; Semantic similarity; Sparsity problems; Stemming; Hidden Markov models
Low-resource machine transliteration using recurrent neural networks,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061177862&doi=10.1145%2f3265752&partnerID=40&md5=b47a435e30f34316501d285eb9eea9a4,"Grapheme-to-phoneme models are key components in automatic speech recognition and text-to-speech systems. With low-resource language pairs that do not have available and well-developed pronunciation lexicons, grapheme-to-phoneme models are particularly useful. These models are based on initial alignments between grapheme source and phoneme target sequences. Inspired by sequence-to-sequence recurrent neural network-based translation methods, the current research presents an approach that applies an alignment representation for input sequences and pretrained source and target embeddings to overcome the transliteration problem for a low-resource languages pair. Evaluation and experiments involving French and Vietnamese showed that with only a small bilingual pronunciation dictionary available for training the transliteration models, promising results were obtained with a large increase in BLEU scores and a reduction in Translation Error Rate (TER) and Phoneme Error Rate (PER).Moreover,we compared our proposed neural network-based transliteration approach with a statistical one. © 2019 Association for Computing Machinery.",Alignment; Embeddings; French-Vietnamese; Grapheme-to-phoneme; Low-resource language; Machine transliteration; Recurrent neural networks,Alignment; Character recognition; Embeddings; Speech recognition; Translation (languages); Automatic speech recognition; Grapheme to phonemes; Low resource languages; Machine transliteration; Pronunciation dictionaries; Pronunciation lexicon; Transliteration models; Vietnamese; Recurrent neural networks
Online handwritten gurmukhi words recognition: An inclusive study,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060030407&doi=10.1145%2f3282441&partnerID=40&md5=a277b0e7dc7c1fa8a04af4b8594c6a7e,"Identification of offline and online handwritten words is a challenging and complex task. In comparison to Latin and Oriental scripts, the research and study of handwriting recognition at word level in Indic scripts is at its initial phases. The two main methods of handwriting recognition are global and analytical. The present work introduces a novel analytical approach for online handwritten Gurmukhi word recognition based on a minimal set of words and recognizes an input Gurmukhi word as a sequence of characters. We employed a sequential step-by-step approach to recognize online handwritten Gurmukhi words. Considering the massive variability in online Gurmukhi handwriting, the present work employs the completely linked non-homogeneous hidden Markov model. In the present study, we considered the dependent, major-dependent, and super-dependent nature of strokes to form Gurmukhi characters in words. On test sets of online handwritten Gurmukhi datasets, the word-level accuracy rates are 85.98%, 84.80%, 82.40%, and 82.20% in four different modes. Besides the online Gurmukhi word recognition, the present work also provides Gurmukhi handwriting analysis study for varying writing styles and proposes novel techniques for zone detection and rearrangement of strokes. Our proposed algorithms have been successfully employed to online handwritten Gurmukhi word recognition in dependent and independent modes of handwriting. © 2019 Association for Computing Machinery.",Boxed rearrangement; Gurmukhi; Hidden Markov models; Online handwriting recognition; Word recognition; Zone detection,Hidden Markov models; Trellis codes; Vocabulary control; Boxed rearrangement; Gurmukhi; Online handwriting recognition; Word recognition; Zone detection; Character recognition
A survey of discourse representations for Chinese discourse annotation,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061182239&doi=10.1145%2f3293442&partnerID=40&md5=ad180f393567a77ad8a71395f2f66311,"A key element in computational discourse analysis is the design of a formal representation for the discourse structure of a text.With machine learning being the dominant method, it is important to identify a discourse representation that can be used to perform large-scale annotation. This survey provides a systematic analysis of existing discourse representation theories to evaluate whether they are suitable for annotation of Chinese text. Specifically, the two properties, expressiveness and practicality, are introduced to compare the representations of theories based on rhetorical relations and the representations of theories based on entity relations. The comparison systematically reveals linguistic and computational characteristics of the theories. After that, we conclude that none of the existing theories are quite suitable for scalable Chinese discourse annotation because they are not both expressive and practical. Therefore, a new discourse representation needs to be proposed, which should balance the expressiveness and practicality, and cover rhetorical relations and entity relations. Inspired by the conclusions, this survey discusses some preliminary proposals on how to represent the discourse structure that are worth pursuing. © 2019 Association for Computing Machinery.",Discourse analysis; Discourse representation; Discourse structure; Discourse theory,Learning systems; Semantics; Surveys; Discourse analysis; Discourse representation; Discourse representation theory; Discourse structure; Discourse theory; Formal representations; Rhetorical relations; Systematic analysis; Computation theory
Sentiment analysis of Iraqi Arabic dialect on Facebook based on distributed representations of documents,2019,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060003443&doi=10.1145%2f3278605&partnerID=40&md5=2baac174058101f3c710339d86363b2a,"Nowadays, social media is used by many people to express their opinions about a variety of topics. Opinion Mining or Sentiment Analysis techniques extract opinions from user generated contents. Over the years, a multitude of Sentiment Analysis studies has been done about the English language with deficiencies of research in all other languages. Unfortunately, Arabic is one of the languages that seems to lack substantial research, despite the rapid growth of its use on social media outlets. Furthermore, specific Arabic dialects should be studied, not just Modern Standard Arabic. In this paper, we experiment sentiments analysis of Iraqi Arabic dialect using word embedding. First, we made a large corpus from previous works to learn word representations. Second, we generated word embedding model by training corpus using Doc2Vec representations based on Paragraph and Distributed Memory Model of Paragraph Vectors (DM-PV) architecture. Lastly, the represented feature used for training four binary classifiers (Logistic Regression, Decision Tree, Support Vector Machine and Naive Bayes) to detect sentiment. We also experimented different values of parameters (window size, dimension and negative samples). In the light of the experiments, it can be concluded that our approach achieves a better performance for Logistic Regression and Support Vector Machine than the other classifiers. © 2019 Association for Computing Machinery.",Doc2Vec; Facebook; Iraqi Arabic Dialect; Sentiments analysis; Word embedding,Binary trees; Decision trees; Memory architecture; Regression analysis; Sentiment analysis; Social networking (online); Support vector machines; Arabic dialects; Doc2Vec; Facebook; Sentiments analysis; Word embedding; Data mining
Wikipedia-based relatedness measurements for multilingual short text clustering,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058793589&doi=10.1145%2f3276473&partnerID=40&md5=32a35d2e53903cca2d310f205b30baf1,"Throughout the world, people can post information about their local area in their own languages using social networking services. Multilingual short text clustering is an important task to organize such information, and it can be applied to various applications, such as event detection and summarization. However, measuring the relatedness between short texts written in various languages is a challenging problem. In addition to handling multiple languages, the semantic gaps among all languages must be considered. In this article, we propose two Wikipedia-based semantic relatedness measurement methods for multilingual short text clustering. The proposed methods solve the semantic gap problem by incorporating the inter-language links of Wikipedia into Extended Naive Bayes (ENB), a probabilistic method that can be applied to measure semantic relatedness among monolingual short texts. The proposed methods represent a multilingual short text as a vector of the English version of Wikipedia articles (entities). By transferring texts to a unified vector space, the relatedness between texts in different languages with similar meanings can be increased. We also propose an approach that can improve clustering performance and reduce the processing time by eliminating language-specific entities in the unified vector space. Experimental results on multilingual Twitter message clustering revealed that the proposed methods outperformed cross-lingual explicit semantic analysis, a previously proposed method to measure relatedness between texts in different languages. Moreover, the proposed methods were comparable to ENB applied to texts translated into English using a proprietary translation service. The proposed methods enabled relatedness measurements for multilingual short text clustering without requiring machine translation processes. © 2018 Association for Computing Machinery.",Multilingual text analysis; Semantic relatedness; Short text analysis; Text clustering,Cluster analysis; Natural language processing systems; Semantics; Vector spaces; Explicit semantic analysis; Multilingual texts; Probabilistic methods; Semantic relatedness; Short texts; Social networking services; Text Clustering; Translation services; Translation (languages)
TempO-HindiWordNet: A lexical knowledge-base for temporal information processing,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058801351&doi=10.1145%2f3277504&partnerID=40&md5=570846aa8abf0e974618eea86bec549a,"Temporality has significantly contributed to various Natural Language Processing and Information Retrieval applications. In this article, we first create a lexical knowledge-base in Hindi by identifying the temporal orientation of word senses based on their definition and then use this resource to detect underlying temporal orientation of the sentences. To create the resource, we propose a semi-supervised learning framework, where each synset of the Hindi WordNet is classified into one of the five categories, namely, past, present, future, neutral, and atemporal. The algorithm initiates learning with a set of seed synsets and then iterates following different expansion strategies, viz. probabilistic expansion based on classifier's confidence and semantic distance based measures. We manifest the usefulness of the resource that we build on an external task, viz. sentence-level temporal classification. The underlying idea is that a temporal knowledge-base can help in classifying the sentences according to their inherent temporal properties. Experiments on two different domains, viz. general and Twitter, show interesting results. © 2018 Association for Computing Machinery.",Hindi; Semi-supervised machine learning; Sentence-level temporality detection; Temporal sense detection,Expansion; Knowledge based systems; Learning algorithms; Natural language processing systems; Semantics; Hindi; Lexical knowledge base; Retrieval applications; Semi- supervised learning; Semi-supervised; Sentence level; Temporal classification; Temporal information processing; Supervised learning
Pause-based phrase extraction and effective OOV handling for low-resource machine translation systems,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058805822&doi=10.1145%2f3265751&partnerID=40&md5=8ae5d7fc50247c13953d39810e147cf5,"Machine translation is the core problem for several natural language processing research across the globe. However, building a translation system involving low-resource languages remains a challenge with respect to statistical machine translation (SMT). This work proposes and studies the effect of a phrase-induced hybrid machine translation system for translation from English to Tamil, under a low-resource setting. Unlike conventional hybrid MT systems, the free-word ordering feature of the target language Tamil is exploited to form a re-ordered target language model and to extend the parallel text corpus for training the SMT. In the current work, a novel rule-based phrase-extraction method, implemented using parts-of-speech (POS) and place-of-pause in both languages is proposed, which is used to pre-process the training corpus for developing the back-off phrase-induced SMT. Further, out-of-vocabulary (OOV) words are handled using speech-based transliteration and two-level thesaurus intersection techniques based on the POS tag of the OOV word. To ensure that the input with OOV words does not skip phrase-level translation in the hierarchical model, a phrase-level example-based machine translation approach is adopted to find the closest matching phrase and perform translation followed by OOV replacement. The proposed system results in a bilingual evaluation understudy score of 84.78 and a translation edit rate of 19.12. The performance of the system is compared in terms of adequacy and fluency, with existing translation systems for this specific language pair, and it is observed that the proposed system outperforms its counterparts. © 2018 Association for Computing Machinery.",Low-resource machine translation; PL-EBMT; Place-of-pause based phrase extraction; POS; Thesaurus intersection,Computer aided language translation; Extraction; Hierarchical systems; Natural language processing systems; Polonium; Speech transmission; Syntactics; Thesauri; Example based machine translations; Hybrid machine translation; Machine translation systems; Machine translations; Out of vocabulary words; Phrase extraction; PL-EBMT; Statistical machine translation; Computational linguistics
Diacritic-based matching of Arabic words,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058794589&doi=10.1145%2f3242177&partnerID=40&md5=c361f46682b5504e999a7156ee7a72ba,"Words in Arabic consist of letters and short vowel symbols called diacritics inscribed atop regular letters. Changing diacritics may change the syntax and semantics of a word; turning it into another. This results in difficulties when comparing words based solely on string matching. Typically, Arabic NLP applications resort to morphological analysis to battle ambiguity originating from this and other challenges. In this article, we introduce three alternative algorithms to compare two words with possibly different diacritics. We propose the Subsume knowledge-based algorithm, the Imply rule-based algorithm, and the Alike machine-learning-based algorithm. We evaluated the soundness, completeness, and accuracy of the algorithms against a large dataset of 86,886 word pairs. Our evaluation shows that the accuracy of Subsume (100%), Imply (99.32%), and Alike (99.53%). Although accurate, Subsume was able to judge only 75% of the data. Both Subsume and Imply are sound, while Alike is not. We demonstrate the utility of the algorithms using a real-life use case - in lemma disambiguation and in linking hundreds of Arabic dictionaries. © 2018 Association for Computing Machinery.",Arabic; Diacritics; Disambiguation,Learning systems; Semantics; Alternative algorithms; Arabic; Diacritics; Disambiguation; Knowledge-based algorithms; Morphological analysis; Rule based algorithms; String matching; Knowledge based systems
Sub-stroke-wise relative feature for online indic handwriting recognition,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058792344&doi=10.1145%2f3264735&partnerID=40&md5=b590ce8516f78c9f84ef52a966fa0950,"The main problem of Bangla (Bengali) and Devanagari handwriting recognition is the shape similarity of characters. There are only a few pieces of work on writer-independent cursive online Indian text recognition, and the shape similarity problem needs more attention from the researchers. To handle the shape similarity problem of cursive characters of Bangla and Devanagari scripts, in this article, we propose a new category of features called 'sub-stroke-wise relative feature' (SRF) which are based on relative information of the constituent parts of the handwritten strokes. Relative information among some of the parts within a character can be a distinctive feature as it scales up small dissimilarities and enhances discrimination among similar-looking shapes. Also, contextual anticipatory phenomena are automatically modeled by this type of feature, as it takes into account the influence of previous and forthcoming strokes. We have tested popular state-of-the-art feature sets as well as proposed SRF using various (up to 20,000-word) lexicons and noticed that SRF significantly outperforms the state-of-the-art feature sets for online Bangla and Devanagari cursive word recognition. © 2018 Association for Computing Machinery.",Cursive text recognition; Indic script; Lexicon driven recognition; Online handwriting recognition,Agricultural engineering; Natural resources; Handwriting recognition; Handwritten strokes; Indic script; Lexicon driven; Online handwriting recognition; Relative information; Shape similarity; Text recognition; Character recognition
Improving NER tagging performance in low-resource languages via multilingual learning,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058809485&doi=10.1145%2f3238797&partnerID=40&md5=9231c4d6b66a322b83991fdab645ce35,"Existing supervised solutions for Named Entity Recognition (NER) typically rely on a large annotated corpus. Collecting large amounts of NER annotated corpus is time-consuming and requires considerable human effort. However, collecting small amounts of annotated corpus for any language is feasible, but the performance degrades due to data sparsity. We address the data sparsity by borrowing features from the data of a closely related language. We use hierarchical neural networks to train a supervised NER system. The feature borrowing from a closely related language happens via the shared layers of the network. The neural network is trained on the combined dataset of the low-resource language and a closely related language, also termed Multilingual Learning. Unlike existing systems, we share all layers of the network between the two languages. We apply multilingual learning for NER in Indian languages and empirically show the benefits over a monolingual deep learning system and a traditional machine-learning system with some feature engineering. Using multilingual learning, we show that the low-resource language NER performance increases mainly due to (1) increased named entity vocabulary, (2) cross-lingual subword features, and (3) multilingual learning playing the role of regularization. © 2018 Association for Computing Machinery.",Deep learning; Indian languages; Low-resource languages; Multilingual learning; Named entity recognition,Learning algorithms; Network layers; Existing systems; Feature engineerings; Hierarchical neural networks; Indian languages; Low resource languages; Multilingual learning; Named entities; Named entity recognition; Deep learning
Improving word embedding coverage in less-resourced languages through multi-linguality and cross-linguality: A case study with aspect-based sentiment analysis,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058812683&doi=10.1145%2f3273931&partnerID=40&md5=826f68d97f323dd3022bde0c451358fc,"In the era of deep learning-based systems, efficient input representation is one of the primary requisites in solving various problems related to Natural Language Processing (NLP), data mining, text mining, and the like. Absence of adequate representation for an input introduces the problem of data sparsity, and it poses a great challenge to solve the underlying problem. The problem is more intensified with resource-poor languages due to the absence of a sufficiently large corpus required to train a word embedding model. In this work, we propose an effective method to improve the word embedding coverage in less-resourced languages by leveraging bilingual word embeddings learned from different corpora. We train and evaluate deep Long Short Term Memory (LSTM)-based architecture and show the effectiveness of the proposed approach for two aspect-level sentiment analysis tasks (i.e., aspect term extraction and sentiment classification). The neural network architecture is further assisted by hand-crafted features for prediction. We apply the proposed model in two experimental setups: multi-lingual and cross-lingual. Experimental results show the effectiveness of the proposed approach against the state-of-the-art methods. © 2018 Association for Computing Machinery.",Aspect-Based Sentiment Analysis (ABSA); Bilingual word embeddings; Cross-lingual sentiment analysis; Data sparsity; Deep learning; Indian languages; Long Short Term Memory (LSTM); Low-resourced languages; Sentiment analysis,Brain; Data mining; Deep learning; Linguistics; Memory architecture; Natural language processing systems; Network architecture; Problem solving; Sentiment analysis; Text processing; Aspect-Based Sentiment Analysis (ABSA); Cross-lingual; Data sparsity; Embeddings; Indian languages; Long short-term memory
Transition-based Korean dependency parsing using hybrid word representations of syllables and morphemes with LSTMs,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058817704&doi=10.1145%2f3241745&partnerID=40&md5=1c2ff448270099b2b7b1672e8998cd90,"Recently, neural approaches for transition-based dependency parsing have become one of the state-of-the art methods for performing dependency parsing tasks in many languages. In neural transition-based parsing, a parser state representation is first computed from the configuration of a stack and a buffer, which is then fed into a feed-forward neural network model that predicts the next transition action. Given that words are basic elements of a stack and buffer, a parser state representation is considerably affected by how a word representation is defined. In particular, word representation issues become more critical in morphologically rich languages such as Korean, as the set of potential words is not bound but introduce the second-order vocabulary complexity, called the phrase vocabulary complexity due to the agglutinative characteristics of the language. In this article, we propose a hybrid word representation that combines two compositional word representations, each of which is derived from representations of syllables and morphemes, respectively. Our underlying assumption for this hybrid word representation is that, because both syllables and morphemes are two common ways of decomposing Korean words, it is expected that their effects in inducing word representation are complementary to one another. Experimental results carried on Sejong and SPMRL 2014 datasets show that our proposed hybrid word representation leads to the state-of-the-art performance. © 2018 Association for Computing Machinery.",Compositional model; Dependency parsing; Hybrid approach; Morphemes; Stack LSTM; Syllables; Word representation,Complex networks; Computational linguistics; Compositional modeling; Dependency parsing; Hybrid approach; Morphemes; Stack LSTM; Syllables; Word representations; Long short-term memory
Nova: A feasible and flexible annotation system for joint tokenization and part-of-speech tagging,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058809420&doi=10.1145%2f3276773&partnerID=40&md5=45ebac52c301e87d950bc064499a578d,"A feasible and flexible annotation system is designed for joint tokenization and part-of-speech (POS) tagging to annotate those languages without natural definitions of words. This design was motivated by the fact that word separators are not used in many highly analytic East and Southeast Asian languages. Although several of the languages are well-studied, e.g., Chinese and Japanese, many are understudied with low resources, e.g., Burmese (Myanmar) and Khmer. In the first part of the article, the proposed annotation system, named nova, is introduced. nova contains only four basic tags (n, v, a, and o); these tags can be further modified and combined to adapt complex linguistic phenomena in tokenization and POS tagging. In the second part of the article, the feasibility and flexibility of nova is illustrated from the annotation practice on Burmese and Khmer. The relation between nova and two universal POS tagsets is discussed in the final part of the article. © 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Annotation; Asian languages; Part-of-speech tagging; Tokenization,Syntactics; Annotation; Annotation systems; Asian languages; Linguistic phenomena; Myanmars; Part of speech tagging; PoS tagging; Tokenization; Computational linguistics
Response selection and automatic message-response expansion in retrieval-based QA systems using semantic dependency pair model,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056766752&doi=10.1145%2f3229184&partnerID=40&md5=e90f0fe0f5246d09652ca6cf5d65fddf,"This article presents an approach to response selection and message-response (MR) database expansion from the unstructured data on the psychological consultation websites for a retrieval-based question answering (QA) system in a constrained domain for emotional support and comforting. First, we manually construct an initial MR database based on the articles collected from the psychological consultation websites. The Chinese Knowledge and Information Processing probabilistic context-free grammar is adopted to obtain the semantic dependency graphs (SDGs) of all the messages and responses in the initial MR database. For each sentence in the MR database, all the semantic dependencies, each composed of two words and their semantic relation, are extracted from the SDG of the sentence to form a semantic dependency set. Finally, a matrix with the element representing the correlation between the semantic dependencies of the messages and their corresponding responses is constructed as a semantic dependency pair model (SDPM) for response selection. Moreover, as the number of MR pairs in the psychological consultation websites is increasing day by day, the MR database in the QA system should be expanded to meet the needs of the users. For MR database expansion, the unstructured data from the message board are automatically collected. For the collected data, the supervised latent Dirichlet allocation is adopted for event detection and then the event-based delta Bayesian Information Criterion is used for message and response article segmentation. Each extracted message segment is then fed to the constructed retrieval-based QA system to find the best matched response segment and the matching score is also estimated to verify if the new MR pair is suitable to be included in the expanded MR database. Fivefold cross validation was employed to evaluate the performance of the proposed retrieval-based QA system over the expanded MR database based on SDPM. Compared to the vector space model-based method, the Okapi BM25 model, and the deep learning-based sequence-to-sequence with attention model, the proposed approach achieved a more favorable performance according to a statistical significance test. The retrieval accuracy based on MR expansion was also evaluated and a satisfactory result was obtained confirming the effectiveness of the expanded MR database. In addition, the user's satisfaction score of the proposed system was evaluated using the Cronbach's alpha value and the satisfaction score of the proposed SDPM was higher than those of the methods for comparison. © 2018 ACM",Event detection; Message and response expansion; Retrieval-based QA system; Semantic dependency,Context free grammars; Database systems; Deep learning; Natural language processing systems; Semantics; Statistics; Vector spaces; Websites; Bayesian information criterion; Event detection; Knowledge and information processing; Latent Dirichlet allocation; Probabilistic context free grammars; QA system; Semantic dependency; Statistical significance test; Search engines
Word segmentation for Burmese based on dual-layer CRFs,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056750914&doi=10.1145%2f3232537&partnerID=40&md5=5d054afffe064b1b5ec2c7eff0b53ed7,"Burmese is an isolated language, in which the syllable is the smallest unit. Syllable segmentation methods based on matching lead to performance subject to the syllable segmentation effect. This article proposes a word segmentation method with fusion conditions of double syllable features. It combines word segmentation and segmentation of syllables into one process, thus reducing the impact of errors on the syllable segmentation of Burmese. In the first layer of the conditional random fields (CRF) model, Burmese characters as atomic features are integrated into the Burma section of the Barkis Speech Paradigm (Backus normal form) features to realize the Burma syllable sequence tags. In the second layer of the CRFs model, with the syllable marked as input, it realizes the sequence markers through building a feature template with syllables as atomic features. The experimental results show that the proposed method has a better effect compared with the method based on the matching of syllables. © 2018 ACM",BNF; Burmese; CRFs; Syllable segmentation; Word segmentation,Random processes; Burmese; Conditional random field; CRFs; Feature template; Sequence markers; Smallest unit; Syllable segmentation; Word segmentation; Computational linguistics
Optimizing automatic evaluation of machine translation with the LISTMLE approach,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056774782&doi=10.1145%2f3226045&partnerID=40&md5=df098309f232f968f1335d765a5ed4c4,"Automatic evaluation of machine translation is critical for the evaluation and development of machine translation systems. In this study, we propose a new model for automatic evaluation of machine translation. The proposed model combines standard n-gram precision features and sentence semantic mapping features with neural features, including neural language model probabilities and the embedding distances between translation outputs and their reference translations. We optimize the model with a representative list-wise learning to rank approach, ListMLE, in terms of human ranking assessments. The experimental results on WMT'2015 Metrics task indicated that the proposed approach yields significantly better correlations with human assessments than several state-of-the-art baseline approaches. In particular, the results confirmed that the proposed list-wise learning to rank approach is useful and powerful for optimizing automatic evaluation metrics in terms of human ranking assessments. Deep analysis also demonstrated that optimizing automatic metrics with the ListMLE approach is a reasonable method and adding the neural features can gain considerable improvements compared with the traditional features. © 2018 ACM",Automatic evaluation of machine translation; Learning to rank; Recurrent neural network language model; Segment-level consistency; System-level correlation; Word embedding,Computer aided language translation; Recurrent neural networks; Semantics; Learning to rank; Machine translations; Segment-level consistency; System levels; Word embedding; Computational linguistics
Input method for human translators: A novel approach to integrate machine translation effectively and imperceptibly,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056765848&doi=10.1145%2f3230638&partnerID=40&md5=faa3c39b1b4fa5bea507a674cff2ff63,"Computer-aided translation (CAT) systems are the most popular tool for helping human translators efficiently perform language translation. To further improve the translation efficiency, there is an increasing interest in applying machine translation (MT) technology to upgrade CAT. To thoroughly integrate MT into CAT systems, in this article, we propose a novel approach: a new input method that makes full use of the knowledge adopted by MT systems, such as translation rules, decoding hypotheses, and n-best translation lists. The proposed input method contains two parts: a phrase generation model, allowing human translators to type target sentences quickly, and an n-gram prediction model, helping users choose perfect MT fragments smoothly. In addition, to tune the underlying MT system to generate the input method preferable results, we design a new evaluation metric for the MT system. The proposed input method integrates MT effectively and imperceptibly, and it is particularly suitable for many target languages with complex characters, such as Chinese and Japanese. The extensive experiments demonstrate that our method saves more than 23% in time and over 42% in keystrokes, and it also improves the translation quality by more than 5 absolute BLEU scores compared with the strong baseline, i.e., post-editing using Google Pinyin. © 2018 ACM",Computer-aided translation; Evaluation metric; Input method; Machine translation,Computational linguistics; Complex character; Computer-aided translations; Evaluation metrics; Input methods; Language translation; Machine translations; Translation quality; Translation rules; Computer aided language translation
Arabic authorship attribution: An extensive study on twitter posts,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056768969&doi=10.1145%2f3236391&partnerID=40&md5=d1f7dfde942a66c2e53106e670d026c8,"Law enforcement faces problems in tracing the true identity of offenders in cybercrime investigations. Most offenders mask their true identity, impersonate people of high authority, or use identity deception and obfuscation tactics to avoid detection and traceability. To address the problem of anonymity, authorship analysis is used to identify individuals by their writing styles without knowing their actual identities. Most authorship studies are dedicated to English due to its widespread use over the Internet, but recent cyber-attacks such as the distribution of Stuxnet indicate that Internet crimes are not limited to a certain community, language, culture, ideology, or ethnicity. To effectively investigate cybercrime and to address the problem of anonymity in online communication, there is a pressing need to study authorship analysis of languages such as Arabic, Chinese, Turkish, and so on. Arabic, the focus of this study, is the fourth most widely used language on the Internet. This study investigates authorship of Arabic discourse/text, especially tiny text, Twitter posts. We benchmark the performance of a profile-based approach that uses n-grams as features and compare it with state-of-the-art instance-based classification techniques. Then we adapt an event-visualization tool that is developed for English to accommodate both Arabic and English languages and visualize the result of the attribution evidence. In addition, we investigate the relative effect of the training set, the length of tweets, and the number of authors on authorship classification accuracy. Finally, we show that diacritics have an insignificant effect on the attribution process and part-of-speech tags are less effective than character-level and word-level n-grams. © 2018 ACM",Authorship attribution; Short text; Social media; Twitter; Visualization,Benchmarking; Classification (of information); Computational linguistics; Flow visualization; Information dissemination; Network security; Social networking (online); Visualization; Authorship attribution; Classification accuracy; Classification technique; Cybercrime investigations; On-line communication; Short texts; Social media; Twitter; Computer crime
CLASENTI: A class-specific sentiment analysis framework,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052944884&doi=10.1145%2f3209885&partnerID=40&md5=03f4db1f14a152b6b1b3a9975f32221b,"Arabic text sentiment analysis suffers from low accuracy due to Arabic-specific challenges (e.g., limited resources, morphological complexity, and dialects) and general linguistic issues (e.g., fuzziness, implicit sentiment, sarcasm, and spam). The limited resources problem requires efforts to build new and improved Arabic corpora and lexica. We propose a class-specific sentiment analysis (CLASENTI) framework. The framework includes a new annotation approach to build multi-faceted Arabic corpus and lexicon allowing for simultaneous annotation of different facets, including domains, dialects, linguistic issues, and polarity strengths. Each of these facets has multiple classes (e.g., the nine classes representing dialects found in the Arab world). The new corpus and lexicon annotations facilitate the development of new class-specific classification models and polarity strength calculation. For the new sentiment classification models, we propose a hybrid model combining corpus-based and lexicon-based models. The corpus-based model has two interrelated phases to build; (1) full-corpus classification models for all facets; and (2) class-specific models trained on filtered subsets of the corpus according to the performances of the full-corpus models. To calculate polarity strengths, the lexicon-based model filters the annotated lexicon based on the specific classes of the domain and dialect. As a case study, we collect and annotate 15274 reviews from various sources, including surveys, Facebook comments, and Twitter posts, pertaining to governmental services. In addition, we develop a new web-based application to apply the proposed framework on the case study. CLASENTI framework reaches up to 95% accuracy and 93% F1-Score surpassing the best-known sentiment classifiers implemented in Scikit-learn library that achieve 82% accuracy and 81% F1-Score for Arabic when tested on the same dataset. © 2018 ACM.",Arabic language processing; Opinion mining; Sentiment analysis; Text mining,Data mining; Filtration; Linguistics; Natural language processing systems; Sentiment analysis; Social networking (online); Text processing; Arabic language processing; Classification models; Governmental services; Morphological complexity; Sentiment classification; Strength calculation; Text mining; Web-based applications; Classification (of information)
The rule-based sundanese stemmer,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053415002&doi=10.1145%2f3195634&partnerID=40&md5=5470443a081dc357f66c268631f3304d,"Our research proposed an iterative Sundanese stemmer by removing the derivational affixes prior to the inflexional. This scheme was chosen because, in the Sundanese affixation, a confix (one of derivational affix) is applied in the last phase of a morphological process. Moreover, most of Sundanese affixes are derivational, so removing the derivational affix as the first step is reasonable. To handle ambiguity, the last recognized affix was returned as the result. As the baseline, a Confix-Stripping Approach that applies Porter Stemmer for the Indonesian language was used. This stemmer shares similarities in terms of affix type, but uses a different stemming order. To observe whether the baseline stems the Sundanese affixed word properly, some features that were not covered by the baseline, such as the infix and allomorph removal, were added. The evaluation was done using 4,453 unique affixed words collected from Sundanese online magazines. The experiment shows that, as a whole, our stemmer outperforms the modified baseline in terms of recognized affixed type accuracy and properly stemmed affixed words. Our stemmer recognized 68.87% of the Sundanese affixed types and produced 96.79% of the correctly affixed words; the modified baseline resulted in 21.70% and 71.59%, respectively. © 2018 ACM.",Affixes; Mis-stemming; Over-stemming; Stem-word; Stemming; Under-stemming,Agricultural engineering; Natural resources; Affixes; Indonesian languages; Morphological process; Over-stemming; Rule based; Stemming; Under-stemming; Natural language processing systems
Comparison of methods to annotate named entity corpora,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053410101&doi=10.1145%2f3218820&partnerID=40&md5=f4c73cec0d947cd1792b5a3bd24e9b6b,"The authors compared two methods for annotating a corpus for the named entity (NE) recognition task using non-expert annotators: (i) revising the results of an existing NE recognizer and (ii) manually annotating the NEs completely. The annotation time, degree of agreement, and performance were evaluated based on the gold standard. Because there were two annotators for one text for each method, two performances were evaluated: the average performance of both annotators and the performance when at least one annotator is correct. The experiments reveal that semi-automatic annotation is faster, achieves better agreement, and performs better on average. However, they also indicate that sometimes, fully manual annotation should be used for some texts whose document types are substantially different from the training data document types. In addition, the machine learning experiments using semi-automatic and fully manually annotated corpora as training data indicate that the F-measures could be better for some texts when manual instead of semiautomatic annotation was used. Finally, experiments using the annotated corpora for training as additional corpora show that (i) the NE recognition performance does not always correspond to the performance of the NE tag annotation and (ii) the system trained with the manually annotated corpus outperforms the system trained with the semi-automatically annotated corpus with respect to newswires, even though the existing NE recognizer was mainly trained with newswires. © 2018 ACM.",Annotation; Named entity extraction; Non-expert annotator,Learning systems; Annotation; Comparison of methods; Manual annotation; Named entities; Named entity extraction; Non-experts; Semi-automatic annotation; Semi-automatics; Automation
Domain-specific named entity recognition with document-level optimization,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053426743&doi=10.1145%2f3213544&partnerID=40&md5=3f358b88db3d930fc042f74999b9582b,"Previous studies normally formulate named entity recognition (NER) as a sequence labeling task and optimize the solution in the sentence level. In this article, we propose a document-level optimization approach to NER and apply it in a domain-specific document-level NER task. As a baseline, we apply a state-of-the-art approach, i.e., long-short-term memory (LSTM), to perform word classification. On this basis, we define a global objective function with the obtained word classification results and achieve global optimization via Integer Linear Programming (ILP). Specifically, in the ILP-based approach, we propose four kinds of constraints, i.e., label transition, entity length, label consistency, and domain-specific regulation constraints, to incorporate various entity recognition knowledge in the document level. Empirical studies demonstrate the effectiveness of the proposed approach to domain-specific document-level NER. © 2018 ACM.",Chinese language processing; Integer linear programming; Named entity recognition,Global optimization; Long short-term memory; Natural language processing systems; Chinese language processing; Entity recognition; Global objective functions; Integer Linear Programming; Named entity recognition; Optimization approach; State-of-the-art approach; Word classification; Integer programming
Words are important: Improving sentiment analysis in the Persian language by lexicon refining,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053434408&doi=10.1145%2f3195633&partnerID=40&md5=d0a7c3c929975aaeb626f397c8b279eb,"Lexicon-based sentiment analysis (SA) aims to address the problem of extracting people's opinions from their comments on the Web using a predefined lexicon of opinionated words. In contrast to the machine learning (ML) approach, lexicon-based methods are domain-independent methods that do not need a large annotated training corpus and hence are faster. This makes the lexicon-based approach prevalent in the SA community. However, the story is different for the Persian language. In contrast to English, using the lexicon-based method in Persian is a new discipline. There are rather limited resources available for SA in Persian, making the accuracy of the existing lexicon-based methods lower than other languages. In the current study, first an exhaustive investigation of the lexicon-based method is performed. Then two new resources are introduced to address the problem of resource scarcity for SA in Persian: a carefully labeled lexicon of sentiment words, PerLex, and a new handmade dataset of about 16,000 rated documents, PerView. Moreover, a new hybrid method using both ML and the lexicon-based approach is presented in which PerLex words are used to train the ML algorithm. Experiments are carried out on our new PerView dataset. Results indicate that the accuracy of PerLex is higher than the existing CNRC, Adjectives, SentiStrength, PerSent, and LexiPers lexicons. In addition, the results show that using PerLex significantly decreases the execution time of the proposed system in comparison to the above-mentioned lexicons. Moreover, the results demonstrate the excellence of using opinionated lexicon terms followed by bigrams as the features employed in the ML method. © 2018 ACM.",Lexicon-based approach; Machine learning; Opinion mining; Persian language; PerView dataset; Sentiment analysis,Artificial intelligence; Data mining; Learning systems; Natural language processing systems; Domain independents; Execution time; Lexicon-based; Persian languages; PerView dataset; Resource scarcity; Sentiment Words; Training corpus; Sentiment analysis
A dependency parser for spontaneous Chinese spoken language,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053373843&doi=10.1145%2f3196278&partnerID=40&md5=883cf41c4b07d67193037269855ad4e1,"Dependency analysis is vital for spoken language understanding in spoken dialogue systems. However, existing research has mainly focused on western spoken languages, Japanese, and so on. Little research has been done for spoken Chinese in terms of dependency parsing. Therefore, the new spoken corpus, D-ESCSC (Dependency-Expressive Speech Corpus of Standard Chinese) is built by adding new dependency relations special to spoken Chinese based on a written Chinese annotation scheme. Since spoken Chinese contains typical ill-grammatical phenomena, e.g., translocation, repetition, duplication, and omission, the new atom feature related to punctuation and three feature templates are proposed to improve a graph-based dependency parser. Experimental results on spoken Chinese corpus show that the atom feature and three templates really work and the new parser outperforms the baseline parser. To our best knowledge, it is the first work to report dependency parsing results of spoken Chinese. © 2018 ACM.",Dependency parsing; Graph-based model; Spoken language; Spontaneous Chinese,Graphic methods; Natural language processing systems; Speech processing; Speech recognition; Dependency analysis; Dependency parsing; Dependency relation; Graph-based modeling; Spoken dialogue system; Spoken language understanding; Spoken languages; Spontaneous Chinese; Computational linguistics
Improving vector space word representations via Kernel canonical correlation analysis,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053393910&doi=10.1145%2f3197566&partnerID=40&md5=62e8cf673a5603b88477752ffcb679f0,"Cross-lingual word embeddings are representations for vocabularies of two or more languages in one common continuous vector space and are widely used in various natural language processing tasks. A state-ofthe-art way to generate cross-lingual word embeddings is to learn a linear mapping, with an assumption that the vector representations of similar words in different languages are related by a linear relationship. However, this assumption does not always hold true, especially for substantially different languages. We therefore propose to use kernel canonical correlation analysis to capture a non-linear relationship between word embeddings of two languages. By extensively evaluating the learned word embeddings on three tasks (word similarity, cross-lingual dictionary induction, and cross-lingual document classification) across five language pairs, we demonstrate that our proposed approach achieves essentially better performances than previous linear methods on all of the three tasks, especially for language pairs with substantial typological difference. © 2018 ACM.",Cross-lingual word representation; Kernel canonical correlation analysis (KCCA); Word embedding evaluation,Correlation methods; Information retrieval systems; Natural language processing systems; Document Classification; Kernel canonical correlation analysis; Linear relationships; Non-linear relationships; Vector representations; Word embedding; Word representations; Word similarity; Vector spaces
Graph-based bilingual word embedding for statistical machine translation,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053425845&doi=10.1145%2f3203078&partnerID=40&md5=7684c0ce8f53d9d0347883b20b4d544a,"Bilingual word embedding has been shown to be helpful for Statistical Machine Translation (SMT). However, most existing methods suffer from two obvious drawbacks. First, they only focus on simple contexts such as an entire document or a fixed-sized sliding window to build word embedding and ignore latent useful information from the selected context. Second, the word sense but not the word should be the minimal semantic unit; however, most existing methods still use word representation. To overcome these drawbacks, this article presents a novel Graph-Based Bilingual Word Embedding (GBWE) method that projects bilingual word senses into a multidimensional semantic space. First, a bilingual word co-occurrence graph is constructed using the co-occurrence and pointwise mutual information between the words. Then, maximum complete subgraphs (cliques), which play the role of a minimal unit for bilingual sense representation, are dynamically extracted according to the contextual information. Consequently, correspondence analysis, principal component analyses, and neural networks are used to summarize the clique-word matrix into lower dimensions to build the embedding model. © 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM..",,Computational linguistics; Graphic methods; Principal component analysis; Semantics; Contextual information; Correspondence analysis; Pointwise mutual information; Semantic Space; Sliding Window; Statistical machine translation; Word co-occurrence; Word representations; Computer aided language translation
Novel character identification utilizing semantic relation with animate nouns in Korean,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053370807&doi=10.1145%2f3197657&partnerID=40&md5=510c21d57ce39c5a9328c3b3f37c1cae,"For identifying speakers of quoted speech or extracting social networks from literature, it is indispensable to extract character names and nominals. However, detecting proper nouns in the novels translated into or written in Korean is harder than in English because Korean does not have a capitalization feature. In addition, it is almost impossible for any proper noun dictionary to include all kinds of character names that have been created or will be created by authors. Fortunately, a previous study shows that utilizing postpositions for animate nouns is a simple and effective tool for character identification in Korean novels without a proper noun dictionary and a training corpus. In this article, we propose a character identification method utilizing the semantic relation with known animate nouns. For 80 novels in Korean, the proposed method increases the micro- and macro-average recall by 13.68% and 11.86%, respectively, while decreasing the micro-average precision by 0.28% and increasing the macro-average precision by 0.07% compared to the previous study. If we focus on characters that are responsible for more than 1% of the character name mentions in each novel, the micro- and macro-average F-measure of the proposed method are 96.98% and 97.32%, respectively. © 2018 ACM.",Animate nouns; Character identification; Novels translated into or written in Korean; Semantic relation,Agricultural engineering; Natural resources; Animate nouns; Character identification; Effective tool; Micro and macro; Novels translated into or written in Korean; Proper nouns; Semantic relations; Training corpus; Semantics
Weakly supervised POS tagging without disambiguation,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053383706&doi=10.1145%2f3214707&partnerID=40&md5=9e47a94ad9a0aee7af98eec20b2a75f3,"Weakly supervised part-of-speech (POS) tagging is to learn to predict the POS tag for a given word in context by making use of partial annotated data instead of the fully tagged corpora. Weakly supervised POS tagging would benefit various natural language processing applications in such languages where tagged corpora are mostly unavailable. In this article, we propose a novel framework for weakly supervised POS tagging based on a dictionary of words with their possible POS tags. In the constrained error-correcting output codes (ECOC)-based approach, a unique L-bit vector is assigned to each POS tag. The set of bitvectors is referred to as a coding matrix with value {1, −1}. Each column of the coding matrix specifies a dichotomy over the tag space to learn a binary classifier. For each binary classifier, its training data is generated in the following way: each pair of words and its possible POS tags are considered as a positive training example only if the whole set of its possible tags falls into the positive dichotomy specified by the column coding and similarly for negative training examples. Given a word in context, its POS tag is predicted by concatenating the predictive outputs of the L binary classifiers and choosing the tag with the closest distance according to some measure. By incorporating the ECOC strategy, the set of all possible tags for each word is treated as an entirety without the need of performing disambiguation. Moreover, instead of manual feature engineering employed in most previous POS tagging approaches, features for training and testing in the proposed framework are automatically generated using neural language modeling. The proposed framework has been evaluated on three corpora for English, Italian, and Malagasy POS tagging, achieving accuracies of 93.21%, 90.9%, and 84.5% individually, which shows a significant improvement compared to the state-of-the-art approaches. © 2018 ACM.",Disambiguation; Error-correcting output codes; POS tagging; Weakly supervised,Classification (of information); Codes (symbols); Matrix algebra; Modeling languages; Natural language processing systems; Automatically generated; Disambiguation; Error correcting output code; Part of speech tagging; PoS tagging; State-of-the-art approach; Training and testing; Weakly supervised; Computational linguistics
Learning to Recommend Related Entities with Serendipity for Web Search Users,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091027799&doi=10.1145%2f3185663&partnerID=40&md5=222c2ea49592494b6258d7b1f4908900,"Entity recommendation, providing entity suggestions to assist users in discovering interesting information, has become an indispensable feature of today's Web search engine. However, the majority of existing entity recommendation methods are not designed to boost the performance in terms of serendipity, which also plays an important role in the appreciation of users for a recommendation system. To keep users engaged, it is important to take into account serendipity when building an entity recommendation system. In this article, we propose a learning to recommend framework that consists of two components: related entity finding and candidate entity ranking. To boost serendipity performance, three different sets of features that correlate with the three aspects of serendipity are employed in the proposed framework. Extensive experiments are conducted on large-scale, real-world datasets collected from a widely used commercial Web search engine. The experiments show that our method significantly outperforms several strong baseline methods. An analysis on the impact of features reveals that the set of interestingness features is the most powerful feature set, and the set of unexpectedness features can significantly contribute to recommendation effectiveness. In addition, online controlled experiments conducted on a commercial Web search engine demonstrate that our method can significantly improve user engagement against multiple baseline methods. This further confirms the effectiveness of the proposed framework. © 2018 ACM.",entity recommendation; recommender system; serendipitous entities; serendipitous recommendations; Serendipity; Web search,Large dataset; Search engines; Websites; Baseline methods; Interesting information; Online controlled experiments; Real-world datasets; Recommendation methods; Related entities; Related entity findings; Sets of features; Recommender systems
Constructing a WordNet for Turkish Using Manual and Automatic Annotation,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060634808&doi=10.1145%2f3185664&partnerID=40&md5=c9b2b4c389f58ca9191b94e2a1b15d68,"In this article, we summarize the methodology and the results of our 2-year-long efforts to construct a comprehensive WordNet for Turkish. In our approach, we mine a dictionary for synonym candidate pairs and manually mark the senses in which the candidates are synonymous. We marked every pair twice by different human annotators. We derive the synsets by finding the connected components of the graph whose edges are synonym senses. We also mined Turkish Wikipedia for hypernym relations among the senses. We analyzed the resulting WordNet to highlight the difficulties brought about by the dictionary construction methods of lexicographers. After splitting the unusually large synsets, we used random walk-based clustering that resulted in a Zipfian distribution of synset sizes. We compared our results to BalkaNet and automatic thesaurus construction methods using variation of information metric. Our Turkish WordNet is available online. © 2018 ACM.",synset; Turkish; WordNet,Graph algorithms; Graph structures; Thesauri; Automatic annotation; Automatic thesaurus constructions; Based clustering; Connected component; Dictionary constructions; Random Walk; Variation of informations; Wikipedia; Ontology
Incorporating Prior Knowledge into Word Embedding for Chinese Word Similarity Measurement,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089348418&doi=10.1145%2f3182622&partnerID=40&md5=636fb68455d120f53d4937f0bd43b5a7,"Word embedding-based methods have received increasing attention for their flexibility and effectiveness in many natural language-processing (NLP) tasks, including Word Similarity (WS). However, these approaches rely on high-quality corpus and neglect prior knowledge. Lexicon-based methods concentrate on human's intelligence contained in semantic resources, e.g., Tongyici Cilin, HowNet, and Chinese WordNet, but they have the drawback of being unable to deal with unknown words. This article proposes a three-stage framework for measuring the Chinese word similarity by incorporating prior knowledge obtained from lexicons and statistics into word embedding: in the first stage, we utilize retrieval techniques to crawl the contexts of word pairs from web resources to extend context corpus. In the next stage, we investigate three types of single similarity measurements, including lexicon similarities, statistical similarities, and embedding-based similarities. Finally, we exploit simple combination strategies with math operations and the counter-fitting combination strategy using optimization method. To demonstrate our system's efficiency, comparable experiments are conducted on the PKU-500 dataset. Our final results are 0.561/0.516 of Spearman/Pearson rank correlation coefficient, which outperform the state-of-the-art performance to the best of our knowledge. Experiment results on Chinese MC-30 and SemEval-2012 datasets show that our system also performs well on other Chinese datasets, which proves its transferability. Besides, our system is not language-specific and can be applied to other languages, e.g., English. © 2018 ACM.",Chinese word similarity; prior knowledge; word embedding,Natural language processing systems; Semantics; Combination strategies; Incorporating prior knowledge; NAtural language processing; Optimization method; Rank correlation coefficient; Retrieval techniques; Similarity measurements; State-of-the-art performance; Embeddings
Morphological Segmentation and Part-of-Speech Tagging for the Arabic Heritage,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061324279&doi=10.1145%2f3178459&partnerID=40&md5=2301a9623acba44c3fabe601f4f7cdb7,"We annotate 60,000 words of Classical Arabic (CA) with topics in philosophy, religion, literature, and law with fine-grain segment-based morphological descriptions. We use these annotations for building a morphological segmenter and part-of-speech (POS) tagger for CA. With character-level classification and features from the word and its lexical context, the segmenter achieves a word accuracy of 96.8% with the main issue being a high rate of out-of-vocabulary words. A token-based POS tagger achieves an accuracy of 96.22% with 97.72% on known tokens despite the small size of the corpus. An error analysis shows that most of the tagging errors are results of segmentation and that quality improves with more data being added. The morphological segmenter and tagger have a wide range of potential applications in processing CA, a low-resource variety of the language. © 2018 ACM.",Arabic; heritage; morphological analysis; part-of-speech tagging; segmentation,Error analysis; Character level; Lexical contexts; Morphological description; Morphological segmentation; Out of vocabulary words; Part Of Speech; Part of speech tagging; Word accuracies; Computational linguistics
Application of structural and topological features to recognize online handwriten bangla characters,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042535455&doi=10.1145%2f3178457&partnerID=40&md5=57b26cd61db2125af00a5e2f75340dac,"This article presents a set of novel features for robust online Bangla handwritten character recognition. Two feature extraction methods are presented here. The first describes the transition from background to foreground pixels and vice versa. The second uses a combination of topological features and centre-of-gravity-(CG) based circular features where global information, local information, and Circular Quadrant Mass Distribution information have been extracted. The impact of each along with their combination have also been analyzed. A total of 15,000 isolated online Bangla character samples have been collected and used for the evaluation. A Support Vector Machine classifier records the best recognition rate when the transition count feature, CG-based circular features, and topological features are combined. © 2018 ACM.",Bangla script; Cg-based circle; Circular quadrant mass distribution; Global information; Local information; Online handwriting recognition; Transition count,Gravitation; Topology; Bangla scripts; Cg-based circle; Global informations; Local information; Mass distribution; Online handwriting recognition; Transition count; Character recognition
Leveraging hierarchical deep semantics to classify implicit discourse relations via a mutual learning method,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042540127&doi=10.1145%2f3178456&partnerID=40&md5=219d44338f6a07fa9fa4a75ceffc99db,"This article presents a mutual learning method using hierarchical deep semantics for the classification of implicit discourse relations in English. With the absence of explicit discourse markers, traditional discourse techniques mainly concentrate on discrete linguistic features in this task, which always leads to a data sparseness problem. Torelieve this problem, wepropose amutual learning neuralmodel that makes use of multilevel semantic information together, including the distribution of implicit discourse relations, the semantics of arguments, and the co-occurrence of phrases and words. During the training process, the predicting targets of the model, which are the probability of the discourse relation type and the distributed representation of semantic components, are learned jointly and optimized mutually. The experimental results show that this method outperforms the previous works, especially in multiclass identification attributed to the hierarchical semantic representations and the mutual learning strategy. © 2018 ACM.",Hierarchical deep semantics; Implicit discourse relation classification; Mutual learning neural network,Classification (of information); Learning systems; Linguistics; Natural language processing systems; Data sparseness problem; Distributed representation; Linguistic features; Mutual learning; Relation classifications; Semantic components; Semantic information; Semantic representation; Semantics
Arabic speech act recognition techniques,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042491385&doi=10.1145%2f3170576&partnerID=40&md5=6cc2fc6f4d6d2af8dee7e1165368b743,"This article presents rule-based and statistical-based techniques for Arabic speech act recognition. The proposed techniques classify an utterance into Arabic speech act categories based on three criteria: surface features, cue words, and contextual information. A rule-based expert system has been developed in a bootstrapping manner based on the fact thatArabic language syntax is inherently rule-based. Various machine-learning algorithms have been used to detect Arabic speech act categories: Decision Tree, Naïve Bayes, Neural Network, and SVM.We compare the experimental results for both techniques (machine-learning and rule-based expert systems). Using a corpus of 1,500 sentences, the rule-based expert system achieved an accuracy rate of 98.92%, while the Decision Tree, Naïve Bayes, Neural Network, and SVM achieved an accuracy rate of 97.09%, 96.48%, 93.50%, and 93.70%, respectively. © 2018 ACM.",Bootstrapping; Corpus annotation; Grammatical classification; Sentence type recognition; Speech act,Artificial intelligence; Classification (of information); Data mining; Decision trees; Expert systems; Learning algorithms; Learning systems; Linguistics; Sodium; Speech; Bootstrapping; Contextual information; Corpus annotations; Language syntax; Rule based expert systems; Sentence type recognition; Speech acts; Surface feature; Speech recognition
Chinese open relation extraction and knowledge base establishment,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042520593&doi=10.1145%2f3162077&partnerID=40&md5=32d231395c2f85d7ee49ff50c783d9a9,"Named entity relation extraction is an important subject in the field of information extraction. Although many English extractors have achieved reasonable performance, an effective system for Chinese relation extraction remains undeveloped due to the lack of Chinese annotation corpora and the specificity of Chinese linguistics. Here, we summarize three kinds of unique but common phenomena in Chinese linguistics. In this article, we investigate unsupervised linguistics-based Chinese open relation extraction (ORE), which can automatically discover arbitrary relations without any manually labeled datasets, and research the establishment of a large-scale corpus. By mapping the entity relations into dependency-trees and considering the unique Chinese linguistic characteristics, we propose a novel unsupervised Chinese ORE model based on Dependency Semantic Normal Forms (DSNFs). This model imposes no restrictions on the relative positions among entities and relationships and achieves a high yield by extracting relationsmediated by verbs or nouns and processing the parallel clauses. Empirical results from our model demonstrate the effectiveness of this method, which obtains stable performance on four heterogeneous datasets and achieves better precision and recall in comparison with several Chinese ORE systems. Furthermore, a large-scale knowledge base of entity and relation, called COER, is established and published by applying our method to web text, which conquers the trouble of lack of Chinese corpora. © 2018 ACM.",And knowledge base; Chinese entity relation extraction; Dependency parsing; Linguistics; Open; Unsupervised,Extraction; Knowledge based systems; Linguistics; Natural language processing systems; Ore treatment; Semantics; Dependency parsing; Entity relation extractions; Knowledge base; Open; Unsupervised; Data mining
Improved discourse parsing with two-step neural transition-based model,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042479567&doi=10.1145%2f3152537&partnerID=40&md5=590cca9157b25ec062598953a82fd778,"Discourse parsing aims to identify structures and relationships between different discourse units. Most existing approaches analyze a whole discourse at once, which often fails in distinguishing long-span relations and properly representing discourse units. In this article, we propose a novel parsing model to analyze discourse in a two-step fashion with different feature representations to characterize intra sentence and inter sentence discourse structures, respectively. Our model works in a transition-based framework and benefits from a stack long short-term memory neural network model. Experiments on benchmark tree banks show that our method outperforms traditional 1-step parsing methods in both English and Chinese. © 2018 ACM.",Dependency parsing; Discourse parsing; LSTM; Transition-based system,Context free grammars; Syntactics; Dependency parsing; Discourse parsing; Discourse structure; Feature representation; LSTM; Neural network model; Parsing methods; Transition-based system; Long short-term memory
Phrase table induction using monolingual data for low-resource statistical machine translation,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042472898&doi=10.1145%2f3168054&partnerID=40&md5=5af201f61f601490665745536c4ab3eb,"We propose a new method for inducing a phrase-based translation model from a pair of unrelated monolingual corpora. Our method is able to deal with phrases of arbitrary length and to find phrase pairs that are useful for statistical machine translation, without requiring large parallel or comparable corpora. First, our method generates phrase pairs through coupling source and target phrases separately collected from respective monolingual data. Then, for each phrase pair, we compute features using the monolingual data and a small quantity of parallel sentences. Finally, incorrect phrase pairs are pruned, and a phrase table is made using the remaining phrase pairs. In our experiments on French-Japanese and Spanish-Japanese translation tasks under low-resource conditions, we observe that incorporating a phrase table induced by our method to the machine translation system leads to large improvements in translation quality. Furthermore, we show that a phrase table induced by our method can also be useful in a wide range of configurations, including configurations where we have already access to large parallel corpora and configurations where only small monolingual corpora are available. © 2018 Association for Computing Machinery. All rights reserved.",Knowledge acquisition; Low-resourced language pairs; Machine translation; Phrase table induction; Semantic similarity,Computational linguistics; Couplings; Knowledge acquisition; Linguistics; Semantics; Language pairs; Machine translation systems; Machine translations; Phrase table induction; Resource conditions; Semantic similarity; Statistical machine translation; Translation quality; Computer aided language translation
Expanding paraphrase lexicons by exploiting generalities,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042487102&doi=10.1145%2f3160488&partnerID=40&md5=7defcb4f580e40707843a74823a5199c,"Techniques for generating and recognizing paraphrases, i.e., semantically equivalent expressions, play an important role in a wide range of natural language processing tasks. In the last decade, the task of automatic acquisition of subsentential paraphrases, i.e., words and phrases with (approximately) the same meaning, has been drawing much attention in the research community. The core problem is to obtain paraphrases of high quality in large quantity. This article presents a method for tackling this issue by systematically expanding an initial seed lexicon made up of high-quality paraphrases. This involves automatically capturing morphosemantic and syntactic generalizations within the lexicon and using them to leverage the power of large-scale monolingual data. Given an input set of paraphrases, our method starts by inducing paraphrase patterns that constitute generalizations over corresponding pairs of lexical variants, such as ""amending"" and ""amendment,"" in a fully empirical way. It then searches large-scale monolingual data for new paraphrases matching those patterns. The results of our experiments on English, French, and Japanese demonstrate that our method manages to expand seed lexicons by a large multiple. Human evaluation based on paraphrase substitution tests reveals that the automatically acquired paraphrases are also of high quality. © 2018 Association for Computing Machinery. All rights reserved.",Knowledge acquisition; Lexical variants; Paraphrase; Semantic similarity,Knowledge acquisition; Learning algorithms; Semantics; Automatic acquisition; Core problems; Equivalent expression; Human evaluation; Lexical variants; Paraphrase; Research communities; Semantic similarity; Natural language processing systems
The contribution of stemming and semantics in Arabic topic segmentation,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042493259&doi=10.1145%2f3152464&partnerID=40&md5=57a691aa4846c0f40e0fe74d9aaf9017,"Topic Segmentation is one of the pillars of Natural Language Processing. Yet there is a remarkable research gap in this field, as far as the Arabic language is concerned. The purpose of this article is to improve Arabic Topic Segmentation (ATS) by inquiring into two segmenters: ArabC99 and ArabTextTiling. This study is carried out on two independent levels: the pre-processing level and the segmentation level. These levels represent the basic steps of topic segmentation. On the pre-processing level, we examine the effect of using different Arabic stemming algorithms on ATS. We find out that Light10 is more appropriate for the pre-processing step. Based on this conclusion, we proceed to the second level by proposing two Arabic segmenters called ArabC99-LS-LSA and ArabTextTiling-LS-LSA. These latter use external semantic knowledge related to the Latent Semantic Analysis (LSA). Based on the evaluation results, we notice that LSA provides improvements in this field. Hence, the main outcome of this article emphasizes the multilevel improvement of ATS based on Light10 and LSA. © 2018 ACM.",Arab-TextTiling; Arabc99; Arabic stemming algorithms; Arabic topic segmentation,Semantics; Arabc99; Arabic stemming; Latent Semantic Analysis; Pre-processing step; Segmentation levels; Semantic knowledge; Texttiling; Topic segmentations; Natural language processing systems
Integrating shallow syntactic labels in the phrase-boundary translation model,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042491513&doi=10.1145%2f3178460&partnerID=40&md5=f1d08b89f3058b3a7554b22a00461ef4,"Using a novel rule labeling method, this article proposes a hierarchical model for statistical machine translation. The proposed model labels translation rules by matching the boundaries of target side phrases with the shallow syntactic labels including POS tags and chunk labels on the target side of the training corpus. The boundary labels are concatenated if there is no label for the whole target span. Labeling with the classes of boundary words on the target side phrases has been previously proposed as a phrase-boundary model which can be considered as the base form of our model. In the extended model, the labeler uses a POS tag if there is no chunk label in one boundary. Using chunks as phrase labels, the proposed model generalizes the rules to decrease the model sparseness. The sparseness is a more important issue in the language pairs with a lot of differences in the word order because they have less number of aligned phrase pairs for extraction of rules. The extended phrase-boundary model is also applicable for low-resource languages having no syntactic parser. Some experiments are performed with the proposed model, the base phrase-boundary model, and variants of Syntax Augmented Machine Translation (SAMT) in translation from Persian and German to English as source and target languages with different word orders. According to the results, the proposed model improves the translation performance in the quality and decoding time aspects. Using BLEU as our metric, the proposed model has achieved a statistically significant improvement of about 0.5 point over the base phrase-boundary model. © 2018 ACM.",Chunk label; Hierarchical models; POS tag; Statistical machine translation,Computational linguistics; Computer aided language translation; Hierarchical systems; Linguistics; Translation (languages); Hierarchical model; Labeling methods; Low resource languages; Machine translations; Statistical machine translation; Syntactic parsers; Translation models; Translation rules; Syntactics
A basic language resource kit implementation for the IgboNLP project,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042519794&doi=10.1145%2f3146387&partnerID=40&md5=db30ef0ecdce57122660f1b813140c1f,"Igbo, an African language with around 32 million speakers worldwide, is one of the many languages having few or none of the language processing resources needed for advanced language technology applications. In this article, we describe the approach taken to creating an initial set of resources for Igbo, including an electronic text corpus, a part-of-speech (POS) tagset, and a POS-tagged subcorpus. We discuss the approach taken in gathering texts, the preprocessing of these texts, and the development of the POS tagged corpus. We also discuss some of the problems encountered during corpus and tagset development and the solutions arrived at for these problems. © 2017 ACM.",African language; Corpora; Corpus annotation; Human annotator; Igbo; Interannotation agreement; Language technology; Morphology; Natural language processing (NLP); Normalization; Part-of-speech (POS) tagging; Segmentation; Tagset; Text processing; Tokenization,Image segmentation; Morphology; Natural language processing systems; Tellurium compounds; Text processing; African languages; Corpora; Corpus annotations; Human annotator; Igbo; Language technology; Normalization; Part of speech tagging; Tagset; Tokenization; Linguistics
Using communities ofwords derived from multilingual word vectors for cross-language information retrieval in Indian languages,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061199937&doi=10.1145%2f3208358&partnerID=40&md5=422729b1bf06cc6d40c81358feeb8ad0,"We investigate the use of word embeddings for query translation to improve precision in cross-language information retrieval (CLIR). Word vectors represent words in a distributional space such that syntactically or semantically similar words are close to each other in this space. Multilingual word embeddings are constructed in such a way that similar words across languages have similar vector representations. We explore the effective use of bilingual and multilingual word embeddings learned from comparable corpora of Indic languages to the task of CLIR. We propose a clustering method based on the multilingual word vectors to group similar words across languages. For thiswe construct a graphwithwords from multiple languages as nodes andwith edges connecting words with similar vectors.We use the Louvain method for community detection to find communities in this graph.We show that choosing target language words as query translations from the clusters or communities containing the query terms helps in improving CLIR. We also find that better-quality query translations are obtained when words from more languages are used to do the clustering even when the additional languages are neither the source nor the target languages. This is probably because having more similar words across multiple languages helps define well-defined dense subclusters that help us obtain precise query translations. In this article, we demonstrate the use of multilingual word embeddings and word clusters for CLIR involving Indic languages. We also make available a tool for obtaining related words and the visualizations of the multilingual word vectors for English, Hindi, Bengali, Marathi, Gujarati, and Tamil. © 2018 held by the owner/author(s). Publication rights licensed to ACM.",Clusters; Cross-language information retrieval; Multilingual visualization; Multilingual word embeddings,Computational linguistics; Embeddings; Information retrieval; Query processing; Vector spaces; Vectors; Visualization; Clustering methods; Clusters; Community detection; Comparable corpora; Cross language information retrieval; Multiple languages; Query translations; Vector representations; Translation (languages)
End-to-end Korean part-of-speech tagging using copying mechanism,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042531949&doi=10.1145%2f3178458&partnerID=40&md5=96d8fbb7cea93ece4d1f24e58d3612f7,"In this article, we introduce a novel neural architecture for the end-to-end Korean Part-of-Speech (POS) tagging problem. To address the problem, we extend the present recurrent neural network-based sequence-to-sequence models to deal with the key challenges in this task: rare word generation and POS tagging. To overcome these issues, Input-Feeding and Copying mechanism are adopted. Although our approach does not require any manual features or preprocessed pattern matching dictionaries, our best single model achieves an F-score of 97.08. This is competitive with the current state-of-the-art model (F-score 98.03), which requires extensive manual feature processing. © 2018 ACM.",Copying mechanism; Deep learning; Part-of-speech tagging,Deep learning; Pattern matching; Recurrent neural networks; Syntactics; End to end; Feature processing; Neural architectures; Part of speech tagging; PoS tagging; Sequence models; Single models; State of the art; Computational linguistics
Empirical Exploring Word-Character Relationship for Chinese Sentence Representation,2018,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041707155&doi=10.1145%2f3156778&partnerID=40&md5=edbf99628fd39e07f8bfcb167431390f,"This article addresses the problem of learning compositional Chinese sentence representations, which represent the meaning of a sentence by composing the meanings of its constituent words. In contrast to English, a Chinese word is composed of characters,which contain rich semantic information. However, this information has not been fully exploited by existing methods. In this work, we introduce a novel, mixed character-word architecture to improve the Chinese sentence representations by utilizing rich semantic information of innerword characters.We propose two novel strategies to reach this purpose. The first one is to use a mask gate on characters, learning the relation among characters in a word. The second one is to use a max-pooling operation on words to adaptively find the optimal mixture of the atomic and compositional word representations. Finally, the proposed architecture is applied to various sentence compositionmodels, which achieves substantial performance gains over baseline models on sentence similarity task. To further verify the generalization ability of our model, we employ the learned sentence representations as features in sentence classification task, question classification task, and sentence entailment task. Results have shown that the proposed mixed character-word sentence representation models outperform both the character-based andword-based models. © 2018 ACM.",Compositionmodel; Inner-word character; Mask gate; Max pooling; Mixed character-word representation; Sentence representation,Agricultural engineering; Natural resources; Compositionmodel; Inner-word character; Max-pooling; Sentence representation; Word representations; Semantics
A Generalized Constraint Approach to Bilingual Dictionary Induction for Low-Resource Language Families,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034666604&doi=10.1145%2f3138815&partnerID=40&md5=692d01df96383e7033f99d4047738779,"The lack or absence of parallel and comparable corpora makes bilingual lexicon extraction a difficult task for low-resource languages. The pivot language and cognate recognition approaches have been proven useful for inducing bilingual lexicons for such languages. We propose constraint-based bilingual lexicon induction for closely related languages by extending constraints from the recent pivot-based induction technique and further enabling multiple symmetry assumption cycle to reach many more cognates in the transgraph. We further identify cognate synonyms to obtain many-to-many translation pairs. This article utilizes four datasets: one Austronesian low-resource language and three Indo-European high-resource languages. We use three constraint-based methods from our previous work, the Inverse Consultation method and translation pairs generated from Cartesian product of input dictionaries as baselines. We evaluate our result using the metrics of precision, recall, and F-score. Our customizable approach allows the user to conduct cross validation to predict the optimal hyperparameters (cognate threshold and cognate synonym threshold) with various combination of heuristics and number of symmetry assumption cycles to gain the highest F-score. Our proposed methods have statistically significant improvement of precision and F-score compared to our previous constraint-based methods. The results show that our method demonstrates the potential to complement other bilingual dictionary creation methods like word alignment models using parallel corpora for high-resource languages while well handling low-resource languages. © 2017 ACM.",Closely-related languages; Cognate recognition; Constraint satisfaction problem; Low-resource languages; Pivot-based bilingual lexicon induction,Constraint satisfaction problems; Inverse problems; Semantics; Bilingual dictionary; Bilingual dictionary creations; Bilingual lexicon extractions; Bilingual lexicons; Cognate recognition; Constraint based method; Generalized constraint; Low resource languages; Translation (languages)
Linguistic-relationships-based approach for improving word alignment,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032904312&doi=10.1145%2f3133323&partnerID=40&md5=47b0593a300f5e5668f0dc380bb0a2e4,"The unsupervised word alignments (such as GIZA++) are widely used in the phrase-based statistical machine translation. The quality of the model is proportional to the size and the quality of the bilingual corpus. However, for low-resource language pairs such as Chinese and Vietnamese, a result of unsupervised word alignment sometimes is of low quality due to the sparse data. In addition, this model does not take advantage of the linguistic relationships to improve performance of word alignment. Chinese and Vietnamese have the same language type and have close linguistic relationships. In this article, we integrate the characteristics of linguistic relationships into the word alignment model to enhance the quality of Chinese-Vietnamese word alignment. These linguistic relationships are Sino-Vietnamese and content word. The experimental results showed that our method improved the performance of word alignment as well as the quality of machine translation. © 2017 ACM.",,Alignment; Computational linguistics; Computer aided language translation; Linguistics; Bilingual corpora; Improve performance; Low qualities; Low resource languages; Machine translations; Phrase-based statistical machine translation; Sparse data; Word alignment; Translation (languages)
Development and analysis of speech recognition systems for assamese language using HTK,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032893738&doi=10.1145%2f3137055&partnerID=40&md5=695fc8063afa2640ccb7f6998e049803,"Language analysis is very important for the native speaker to connect with the digital world. Assamese is a relatively unexplored language. In this report, we analyze different aspects of speech-to-text processing, starting from building a speech corpus, defining syllable rules, and finally developing a speech search engine of Assamese. We have collected about 20 hours of speech in three (viz., read, extempore, and conversation) modes and transcribed it. We also discuss some issues and challenges faced during development of the corpus. We have developed an automatic syllabification model with 11 rules for the Assamese language and found an accuracy of more than 95% in our result. We found 12 different syllable patterns where 5 are found most frequent. The maximum length of a syllable found is four letters. With the help of Hidden Markov Model Toolkit (HTK) 3.5, we used deep learning based neural network for our speech recognition model, where we obtained 78.05% accuracy for automatic transcription of Assamese speech. © 2017 ACM.",Assamese; Automatic transcription; HTK; Speech corpus; Speech search engine; Syllabification,Deep neural networks; Hidden Markov models; Markov processes; Search engines; Speech; Text processing; Transcription; Assamese; Automatic transcription; Hidden Markov model toolkits; Issues and challenges; Speech corpora; Speech recognition systems; Speech-to-text processing; Syllabification; Speech recognition
Cleaning of online bangla free-form handwriten text,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033785781&doi=10.1145%2f3145538&partnerID=40&md5=81a9951b3892446329bcdb4ad9e988d2,"In the normal free-form handwritten text, repetition (repeated writing of the same stroke several times in the same place), over-writing, and crossing out are very common. In this article, we call the presence of these three types of writing as ""noise."" Cleaning to extract useful text from such types of noisy text is an important task for robust recognition. To the best of our knowledge, no work has been reported on cleaning of such noise from online text in any scripts and hence, in this article, we propose an automatic text-cleaning approach for online handwriting recognition. Here, at first, crossing out noise with straight strike-through lines is detected using the straightness criteria of online strokes. Next, regions containing repetition, over-writing, and other types of crossing out are located using the positional information of the overlapping strokes. Stroke density, self-intersections of strokes etc. are computed from the strokes of located regions to predict the type of noise and this type of information is used as follows for their cleaning. For cleaning of crossing outs, all strokes of the crossing-out region are removed. For cleaning repetition and over-writing, strokes written earlier are removed, keeping the latest strokes. Finally, delayed strokes are properly arranged and word is passed to online recognizer. Though recognition of free-form handwriting is quite difficult, in this attempt, we obtained up to 70.71% improvement in word-recognition accuracy after noise cleaning. © 2017 ACM.",Bangla script; Free-form handwriting; Indic script; Lexicon driven approach; Strike-through text detection,Cleaning; Bangla scripts; Freeforms; Indic script; Lexicon driven; Text detection; Character recognition
An emotion cause corpus for Chinese Microblogs with multiple-user structures,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033787740&doi=10.1145%2f3132684&partnerID=40&md5=3ca3e16912fa262babc07f06c6318694,"A notably challenging problem in emotion analysis is recognizing the cause of an emotion. Although there have been a few studies on emotion cause detection, most of them work on news reports or a few of them focus on microblogs using a single-user structure (i.e., all texts in a microblog are written by the same user). In this article, we focus on emotion cause detection for Chinese microblogs using a multiple-user structure (i.e., texts in a microblog are successively written by several users). First, based on the fact that the causes of an emotion of a focused user may be provided by other users in a microblog with the multiple-user structure, we design an emotion cause annotation scheme which can deal with such a complicated case, and then provide an emotion cause corpus using the annotation scheme. Second, based on the analysis of the emotion cause corpus, we formalize two emotion cause detection tasks for microblogs (current-subtweet-based emotion cause detection and original-subtweet-based emotion cause detection). Furthermore, in order to examine the difficulty of the two emotion cause detection tasks and the contributions of texts written by different users in a microblog with the multiple-user structure, we choose two popular classification methods (SVM and LSTM) to do emotion cause detection. Our experiments show that the current-subtweet-based emotion cause detection is much more difficult than the original-subtweet-based emotion cause detection, and texts written by different users are very helpful for both emotion cause detection tasks. This study presents a pilot study of emotion cause detection which deals with Chinese microblogs using a complicated structure. © 2017 ACM.",Emotion cause annotation; Emotion cause corpus; Emotion cause detection,Natural resources; Annotation scheme; Classification methods; Complicated structures; Detection tasks; Emotion analysis; Emotion cause annotation; Emotion cause corpus; Pilot studies; Agricultural engineering
Automatically building VoIP speech parallel corpora for Arabic dialects,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032970832&doi=10.1145%2f3132708&partnerID=40&md5=809e33fbea5cf7b637d3a3e0b1fbcb68,"This article discusses the process of automatically building Arabic multi-dialect speech corpora using Voice over Internet Protocol (VoIP). The Asterisk frameworkwas adopted to act as the main connection between the parties, for which two virtual machines were created: a sender and a receiver. The sender makes a VoIP call to the receiver using the Asterisk framework, while the receiver records the call automatically, a process that is repeated for all the audio files involved in the corpora. In this work, more than 67,000 automatic calls were made between the sender and receiver machines, generating VoIP Arabic corpora for four Arabic dialects. The resulting corpora can be considered the first Arabic VoIP parallel speech corpora and will be made freely available to researchers in Arabic NLP and speech recognition research. © 2017 ACM.",Arabic multi-dialect; Arabic speech recognition; Asterisk; VoIP corpora,Internet telephony; Speech; Voice/data communication systems; Arabic dialects; Arabic multi-dialect; Arabic speech recognition; Asterisk; Parallel corpora; Sender and receivers; Voice over Internet protocol; VoIP corpora; Speech recognition
Translating low-resource languages by vocabulary adaptation from close counterparts,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029821969&doi=10.1145%2f3099556&partnerID=40&md5=3824887e000358b5aeda1304df6150bf,"Some natural languages belong to the same family or share similar syntactic and/or semantic regularities. This property persuades researchers to share computational models across languages and benefit from high-quality models to boost existing low-performance counterparts. In this article, we follow a similar idea, whereby we develop statistical and neural machine translation (MT) engines that are trained on one language pair but are used to translate another language. First we train a reliable model for a highresource language, and then we exploit cross-lingual similarities and adapt the model to work for a close language with almost zero resources. We chose Turkish (Tr) and Azeri or Azerbaijani (Az) as the proposed pair in our experiments. Azeri suffers from lack of resources as there is almost no bilingual corpus for this language. Via our techniques, we are able to train an engine for the Az→English (En) direction, which is able to outperform all other existing models. © 2017 ACM.",Lowresource languages; Neural machine translation; Statistical machine translation,Computational linguistics; Computer aided language translation; Engines; Semantics; Bilingual corpora; Computational model; Language pairs; Low resource languages; Machine translations; Natural languages; Reliable models; Statistical machine translation; Translation (languages)
Role of morphology injection in SMT: A case study from indian language perspective,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029797554&doi=10.1145%2f3129208&partnerID=40&md5=6f2acdb86d00626eb1d078e6bf4ceeca,"Phrase-based StatisticalMachine Translation (PBSMT) is commonly used for automatic translation. However, PBSMT runs into difficulty when either or both of the source and target languages are morphologically rich. Factored models are found to be useful for such cases, as they consider word as a vector of factors. These factors can contain any information about the surface word and use it while translating. The objective of the current work is to handle morphological inflections in Hindi, Marathi, and Malayalam using Factored translation models when translating from English. Statistical MT approaches face the problem of data sparsity when translating to a morphologically rich language. It is very unlikely for a parallel corpus to contain all morphological forms of words. We propose a solution to generate these unseen morphological forms and inject them into the original training corpus. We propose a simple and effective solution based on enriching the input with various morphological forms of words. We observe that morphology injection improves the quality of translation in terms of both adequacy and fluency. We verify this with experiments on three morphologically rich languages when translating from English. From the detailed evaluations, we observed an order of magnitude improvement in translation quality. © 2017 ACM.",Factored statistical machine translation models; Morphology injection; Statistical machine translation,Computational linguistics; Computer aided language translation; Linguistics; Morphology; Surface mount technology; Automatic translation; Effective solution; Indian languages; Morphological forms; Parallel corpora; Statistical machine translation; Translation models; Translation quality; Translation (languages)
Predictor-estimator: Neuralquality estimation based on targetword prediction for machine translation,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029814545&doi=10.1145%2f3109480&partnerID=40&md5=e1d649d7d1018c4520e17eb7aa9dff8c,"Recently, quality estimation has been attracting increasing interest from machine translation researchers, aiming at finding a good estimator for the ""quality"" of machine translation output. The common approach for quality estimation is to treat the problem as a supervised regression/classification task using a qualityannotated noisy parallel corpus, called quality estimation data, as training data. However, the available size of quality estimation data remains small, due to the too-expensive cost of creating such data. In addition, most conventional quality estimation approaches rely on manually designed features to model nonlinear relationships between feature vectors and corresponding quality labels. To overcome these problems, this article proposes a novel neural network architecture for quality estimation task-called the predictor-estimator-that considersword prediction as an additional pre-task. The major component of the proposed neural architecture is a word prediction model based on a modified neural machine translation model-a probabilistic model for predicting a targetword conditioned on all the other source and target contexts. The underlying assumption is that the word prediction model is highly related to quality estimation models and is therefore able to transfer useful knowledge to quality estimation tasks. Our proposed quality estimation method sequentially trains the following two types of neural models: (1) Predictor: a neural word prediction model trained from parallel corpora and (2) Estimator: a neural quality estimation model trained fromquality estimation data. To transferword a prediction task to a quality estimation task, we generate quality estimation feature vectors from theword prediction model and feed them into the quality estimation model. The experimental results on WMT15 and 16 quality estimation datasets show that our proposed method has great potential in the various sub-challenges. © 2017 ACM.",Bidirectional language model; Feature extraction; Machine translation; Neural networks; Quality estimation; Word prediction,Computational linguistics; Computer aided language translation; Estimation; Feature extraction; Network architecture; Neural networks; Language model; Machine translation models; Machine translations; Neural architectures; Non-linear relationships; Probabilistic modeling; Quality estimation; Word prediction; Forecasting
Urdu Named Entity Recognition and Classification system using Artificial Neural Network,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029814687&doi=10.1145%2f3129290&partnerID=40&md5=838e2fa2f6d3e55af7ee42d2ef664ebf,"Named Entity Recognition and Classification (NERC) is a process of identifying words and classifying them into person names, location names, organization names, and so on. In this article, we discuss the development of an Urdu Named Entity (NE) corpus, called the Kamran-PU-NE (KPU-NE) corpus, for three entity types, that is, Person, Organization, and Location, and marking the remaining tokens as Others (O). We use two supervised learning algorithms, Hidden Markov Model (HMM) and Artificial Neural Network (ANN), for the development of the Urdu NERC system.We annotate the 652852-token corpus taken from 15 different genres with a total of 44480 NEs. The inter-annotator agreement between the two annotators in terms of Kappa k statistic is 73.41%.With HMM, the highest recorded precision, recall, and f-measure values are 55.98%, 83.11%, and 66.90%, respectively, and with ANN, they are 81.05%, 87.54%, and 84.17%, respectively. © 2017 ACM.",Deep learning; Ner data; Ner using deep learning; Resource poor languages; Urdu pos tagged data; Urdu word2vec,Deep learning; Learning algorithms; Markov processes; Neural networks; Classification system; Entity-types; F measure; Named entities; Named entity recognition; Ner data; Tagged data; Urdu word2vec; Hidden Markov models
A supervised learning approach for Authorship attribution of Bengali literary texts,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028576640&doi=10.1145%2f3099473&partnerID=40&md5=4ab7b57f0380e2d860c561a0fc3e8d7e,"Authorship Attribution is a long-standing problem in Natural Language Processing. Several statistical and computational methods have been used to find a solution to this problem. In this article, we have proposed methods to deal with the authorship attribution problem in Bengali. More specifically, we proposed a supervised framework consisting of lexical and shallow features and investigated the possibility of using topic-modeling-inspired features, to classify documents according to their authors. We have created a corpus from nearly all the literary works of three eminent Bengali authors, consisting of 3,000 disjoint samples. Our models showed better performance than the state-of-the-art, with more than 98% test accuracy for the shallow features and 100% test accuracy for the topic-based features. Further experiments with GloVe vectors [Pennington et al. 2014] showed comparable results, but flexible patterns based on content words and high-frequency words [Schwartz et al. 2013] failed to perform as well as expected. © 2017 ACM.",Authorship Attribution; Lexical features; Machine learning; Naive bayes; Topic model,Learning algorithms; Learning systems; Authorship attribution; Flexible patterns; High-frequency words; Lexical features; Naive bayes; Standing problems; Supervised learning approaches; Topic Modeling; Natural language processing systems
A position-aware language modeling framework for Extractive broadcast news speech summarization,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028536651&doi=10.1145%2f3099472&partnerID=40&md5=5c0fb59e161977c8de32789ef7e2cc49,"Extractive summarization, a process that automatically picks exemplary sentences from a text (or spoken) document with the goal of concisely conveying key information therein, has seen a surge of attention from scholars and practitioners recently. Using a language modeling (LM) approach for sentence selection has been proven effective for performing unsupervised extractive summarization. However, one of the major difficulties facing the LM approach is to model sentences and estimate their parameters more accurately for each text (or spoken) document. We extend this line of research and make the following contributions in this work. First, we propose a position-aware language modeling framework using various granularities of position-specific information to better estimate the sentence models involved in the summarization process. Second, we explore disparate ways to integrate the positional cues into relevance models through a pseudo-relevance feedback procedure. Third, we extensively evaluate various models originated from our proposed framework and several well-established unsupervised methods. Empirical evaluation conducted on a broadcast news summarization task further demonstrates performance merits of the proposed summarization methods. © 2017 ACM.",Extractive summarization; Positional language modeling; Relevance modeling; Speech information,Computational linguistics; Natural language processing systems; Empirical evaluations; Extractive summarizations; Language model; Position-specific information; Pseudo relevance feedback; Relevance models; Speech information; Speech summarization; Modeling languages
Animation of fingerspelled words and number signs of the Sinhala Sign language,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028544363&doi=10.1145%2f3092743&partnerID=40&md5=b92b7f2e5d4c2ffd0051cfb7e3f49a6b,"Sign language is the primary communication medium of the aurally handicapped community. Often, a sign gesture is mapped to a word or a phrase in a spoken language and named as a conversational sign. A fingerspelling sign is a special sign derived to show a single character that matches a character in the alphabet of a given language. This enables the deaf community to express words that do not have a conversational sign, such as a name, using a letter-by-letter technique. Sinhala Sign Language (SSL) uses a phonetic pronunciation mechanism to decode such words due to the presence of one or more modifiers after a consonant. Expressing numbers also have a similar notation, and it is broken down into parts before interpretation in sign gestures. This article presents the variations implemented to make the 3D avatar-based interpreter system look similar to an actual fingerspelled SSL by a human interpreter. To accomplish the task, a phonetic English-based 3D avatar animation system is developed with Blender animation software. The conversion of Sinhala Unicode text to phonetic English and numbers written in digits to sign gestures is done with a Visual Basic.NET (VB.NET) application. The presented application has 61 SSL fingerspelling signs and 40 SSL number signs. It is capable of interpreting any word written using the modern Sinhala alphabet without conversational signs and interprets the numbers that go up to the billions. This is a helpful tool in teaching SSL fingerspelling and number signs of SSL to deaf children. © 2017 ACM.",3D signing avatar; Number gesture animation; Sinhala fingerspelling; Sinhala Sign language,Animation; Blending; Education; Linguistics; 3D avatar animation; Animation softwares; Communication medium; Gesture animation; Interpreter systems; Sign language; Signing avatars; Sinhala fingerspelling; Three dimensional computer graphics
AROMA: A recursive deep learning model for opinion mining in Arabic as a low resource language,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026649439&doi=10.1145%2f3086575&partnerID=40&md5=005d216b9e996733759b66a1a5b8e0cc,"While research on English opinion mining has already achieved significant progress and success, work on Arabic opinion mining is still lagging. This is mainly due to the relative recency of research efforts in developing natural language processing (NLP) methods for Arabic, handling its morphological complexity, and the lack of large-scale opinion resources for Arabic. To close this gap, we examine the class of models used for English and that do not require extensive use of NLP or opinion resources. In particular, we consider the Recursive Auto Encoder (RAE). However, RAE models are not as successful in Arabic as they are in English, due to their limitations in handling the morphological complexity of Arabic, providing a more complete and comprehensive input features for the auto encoder, and performing semantic composition following the natural way constituents are combined to express the overall meaning. In this article, we propose A Recursive Deep Learning Model for Opinion Mining in Arabic (AROMA) that addresses these limitations. AROMA was evaluated on three Arabic corpora representing different genres and writing styles. Results show that AROMA achieved significant performance improvements compared to the baseline RAE. It also outperformed several well-known approaches in the literature. © 2017 ACM.",Deep Learning; Opinion mining in Arabic; Recursive Auto Encoder; Recursive Neural Networks,Complex networks; Deep learning; Learning systems; Natural language processing systems; Neural networks; Odors; Semantics; Signal encoding; Auto encoders; Learning models; Low resource languages; Morphological complexity; Opinion mining; Recursive neural networks; Research efforts; Semantic composition; Data mining
A CDT-styled end-to-end Chinese discourse parser,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026675242&doi=10.1145%2f3099557&partnerID=40&md5=b41b04970b95b4937c1d436d1726d030,"Discourse parsing is a challenging task and plays a critical role in discourse analysis. Since the release of the Rhetorical Structure Theory Discourse Treebank and the Penn Discourse Treebank, the research on English discourse parsing has attracted increasing attention and achieved considerable success in recent years. At the same time, some preliminary research on certain subtasks about discourse parsing for other languages, such as Chinese, has been conducted. In this article, we present an end-to-end Chinese discourse parser with the Connective-Driven Dependency Tree scheme, which consists of multiple components in a pipeline architecture, such as the elementary discourse unit (EDU) detector, discourse relation recognizer, discourse parse tree generator, and attribution labeler. In particular, the attribution labeler determines two attributions (i.e., sense and centering) for every nonterminal node (i.e., discourse relation) in the discourse parse trees. Systematically, our parser detects all EDUs in a free text, generates the discourse parse tree in a bottom-up way, and determines the sense and centering attributions for all nonterminal nodes by traversing the discourse parse tree. Comprehensive evaluation on the Connective-Driven Dependency Treebank corpus from both component-wise and error-cascading perspectives is conducted to illustrate how each component performs in isolation, and how the pipeline performs with error propagation. Finally, it shows that our end-to-end Chinese discourse parser achieves an overall F1 score of 20% with full automation. © 2017 ACM",Connective-Driven Dependency Tree; Discourse parse tree; Discourse parsing; Elementary discourse unit; End-to-end Chinese discourse parser,Computational linguistics; Text processing; Dependency trees; Discourse parsers; Discourse parsing; Elementary discourse unit; Parse trees; Forestry
A sentiment treebank and morphologically enriched recursive deep models for effective sentiment analysis in Arabic,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026660253&doi=10.1145%2f3086576&partnerID=40&md5=23f25c774d4649562c15e10a8f5ccb51,"Accurate sentiment analysis models encode the sentiment of words and their combinations to predict the overall sentiment of a sentence. This task becomes challenging when applied to morphologically rich languages (MRL). In this article, we evaluate the use of deep learning advances, namely the Recursive Neural Tensor Networks (RNTN), for sentiment analysis in Arabic as a case study of MRLs. While Arabic may not be considered the only representative of all MRLs, the challenges faced and proposed solutions in Arabic are common to many other MRLs. We identify, illustrate, and address MRL-related challenges and show how RNTN is affected by the morphological richness and orthographic ambiguity of the Arabic language. To address the challenges with sentiment extraction from text in MRL, we propose to explore different orthographic features as well as different morphological features at multiple levels of abstraction ranging from raw words to roots. A key requirement for RNTN is the availability of a sentiment treebank; a collection of syntactic parse trees annotated for sentiment at all levels of constituency and that currently only exists in English. Therefore, our contribution also includes the creation of the first Arabic Sentiment Treebank (ARSENTB) that is morphologically and orthographically enriched. Experimental results show that, compared to the basic RNTN proposed for English, our solution achieves significant improvements up to 8% absolute at the phrase level and 10.8% absolute at the sentence level, measured by average F1 score. It also outperforms well-known classifiers including Support Vector Machines, Recursive Auto Encoders, and Long Short-Term Memory by 7.6%, 3.2%, and 1.6% absolute respectively, all models being trained with similar morphological considerations. © 2017 ACM",,Forestry; Long short-term memory; Arabic languages; Auto encoders; F1 scores; Morphological features; Multiple levels; Sentence level; Sentiment analysis; Syntactic parse tree; Data mining
Evaluating the content of LMF standardized dictionaries: A practical experiment on Arabic language,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019682677&doi=10.1145%2f3047406&partnerID=40&md5=120b77b9e6e5ffbadd3924459095f0f9,"Since the age of paper versions, dictionaries are often published with anomalies in their content resulting from lexicographer's mistakes or from the lack of efficiency of automatic enrichment systems. Many of these anomalies are expensive to manually detect and difficult to automatically control, notably with lightly structured models of dictionaries. In this article, we take advantage of the fine structure proposed by the Lexical Markup Framework (LMF) norm to investigate the detection of anomalies in the content of LMF normalized dictionaries. First, we give a theoretical study on the plausible anomalies, such as inconsistency, incoherence, redundancy, and incompleteness. Second, we detail the approach that we propose for the automatic detection of such anomalies. Finally, we report on an experiment carried out on an available normalized dictionary of the Arabic language. The experiment has shown that the proposed approach gives reasonable results in terms of precision and recall. © 2017 ACM.",Anomalies' detection; Arabic language; LMF standardized dictionaries; LMF-ISO 24613,Natural resources; Arabic languages; Automatic Detection; Enrichment system; Fine structures; LMF-ISO 24613; Precision and recall; Structured model; Theoretical study; Agricultural engineering
Corpus-based translation induction in indian languages using auxiliary language corpora from wikipedia,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017107260&doi=10.1145%2f3038295&partnerID=40&md5=38f6b4a1693909b87f57092286c6127c,"Identifying translations from comparable corpora is awell-known problem with several applications. Existing methods rely on linguistic tools or high-quality corpora. Absence of such resources, especially in Indian languages, makes this problem hard; for example, state-of-The-Art techniques achieve a mean reciprocal rank of 0.66 for English-Italian, and a mere 0.187 for Telugu-Kannada. In this work, we address the problem of comparable corpora-based translation correspondence induction (CC-TCI) when the only resources available are small noisy comparable corpora extracted from Wikipedia. We observe that translations in the source and target languages have many topically related words in common in other ""auxiliary"" languages. To model this, we define the notion of a translingual theme, a set of topically related words from auxiliary language corpora, and present a probabilistic framework for CC-TCI. Extensive experiments on 35 comparable corpora showed dramatic improvements in performance. We extend these ideas to propose a method for measuring cross-lingual semantic relatedness (CLSR) between words. To stimulate further research in this area, we make publicly available two new high-quality human-Annotated datasets for CLSR. Experiments on the CLSR datasets show more than 200% improvement in correlation on the CLSR task. We apply the method to the real-world problem of cross-lingual Wikipedia title suggestion and build the WikiTSu system. A user study on WikiTSu shows a 20% improvement in the quality of titles suggested. © 2017 ACM.",Auxiliary language; Bilingual lexicon; Comparable corpora; Cross-lingual semantic relatedness; Translation correspondence induction; Wikipedia title suggestion,Semantics; Auxiliary language; Bilingual lexicons; Comparable corpora; Semantic relatedness; Wikipedia; Translation (languages)
Inducing a bilingual lexicon from short parallel multiword sequences,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017121491&doi=10.1145%2f3003726&partnerID=40&md5=158907fe04218edb3d7a0a67789ee249,"This article proposes a technique for mining bilingual lexicons from pairs of parallel short word sequences. The technique builds a generative model from a corpus of training data consisting of such pairs. The model is a hierarchical nonparametric Bayesian model that directly induces a bilingual lexicon while training. The model learns in an unsupervised manner and is designed to exploit characteristics of the language pairs being mined. The proposed model is capable of utilizing commonly used word-pair frequency information and additionally can employ the internal character alignments within the words themselves. It is thereby capable of mining transliterations and can use reliably aligned transliteration pairs to support the mining of other words in their context. The model is also capable of performing word reordering and word deletion during the alignment process, and it is furthermore capable of operating in the absence of full segmentation information. In this work, we study two mining tasks based on English-Japanese and English-Chinese language pairs, and compare the proposed approach to baselines based on a simpler models that use only word-pair frequency information. Our results show that the proposed method is able to mine bilingual word pairs at higher levels of precision and recall than the baselines. © 2017 ACM.",Alignment; Bilingual lexicon; Mining,Agricultural engineering; Alignment; Mining; Natural resources; Bilingual lexicons; Bilingual word pairs; Frequency information; Generative model; Non-parametric Bayesian modeling; Precision and recall; Segmentation informations; Transliteration pairs; Bayesian networks
Implicit discourse relation recognition for English and Chinese with multiview modeling and effective representation learning,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017114315&doi=10.1145%2f3028772&partnerID=40&md5=18dd30a1b4a843074db160aa35dd2f9f,"Discourse relations between two text segments play an important role in many Natural Language Processing (NLP) tasks. The connectives strongly indicate the sense of discourse relations, while in fact, there are no connectives in a large proportion of discourse relations, that is, implicit discourse relations. Compared with explicit relations, implicit relations are much harder to detect and have drawn significant attention. Until now, there have been many studies focusing on English implicit discourse relations, and few studies address implicit relation recognition in Chinese even though the implicit discourse relations in Chinese are more common than those in English. In our work, both the English and Chinese languages are our focus. The key to implicit relation prediction is to properly model the semantics of the two discourse arguments, as well as the contextual interaction between them. To achieve this goal, we propose a neural network based framework that consists of two hierarchies. The first one is the model hierarchy, in which we propose a maxmargin learning method to explore the implicit discourse relation from multiple views. The second one is the feature hierarchy, in which we learn multilevel distributed representations from words, arguments, and syntactic structures to sentences. We have conducted experiments on the standard benchmarks of English and Chinese, and the results show that compared with several methods our proposed method can achieve the best performance in most cases. © 2017 ACM.",Implicit discourse relation; Maxmargin learning; Multilevel features; Neural network,Benchmarking; Neural networks; Semantics; Syntactics; Distributed representation; Implicit discourse relation; Maxmargin learning; Multi-view modeling; Multilevel features; NAtural language processing; Network-based framework; Syntactic structure; Natural language processing systems
A hybrid model for Chinese Spelling check,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017161889&doi=10.1145%2f3047405&partnerID=40&md5=53874921fcf29638484e80cf70716b16,"Spelling check for Chinese has more challenging difficulties than that for other languages. A hybrid model for Chinese spelling check is presented in this article. The hybrid model consists of three components: one graph-based model for generic errors and two independently trained models for specific errors. In the graph model, a directed acyclic graph is generated for each sentence, and the single-source shortest-path algorithm is performed on the graph to detect and correct general spelling errors at the same time. Prior to that, two types of errors over functional words (characters) are first solved by conditional random fields: the confusion of (at) (pinyin is zai in Chinese), (again, more, then) (pinyin: zai) and (of) (pinyin: de), (-ly, adverb-forming particle) (pinyin: de), and (so that, have to) (pinyin: de). Finally, a rule-based model is exploited to distinguish pronoun usage confusion: (she) (pinyin: ta), (he) (pinyin: ta), and some other common collocation errors. The proposed model is evaluated on the standard datasets released by the SIGHAN Bake-off shared tasks, giving state-of-the-art results. © 2017 ACM 2375-4699/2017/03-ART21 $15.00.",Chinese spelling check; Conditional random field; Graph model; Hybrid model; Rule-based model,Directed graphs; Errors; Graphic methods; Image segmentation; Random errors; Random processes; Conditional random field; Graph model; Hybrid model; Rule-based models; Spelling checks; Graph theory
Named entity recognition with word embeddings and wikipedia categories for a low-resource language,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010807453&doi=10.1145%2f3015467&partnerID=40&md5=635d64f9b277790690ea54ee95e3132f,"In this article, we propose a word embedding-based named entity recognition (NER) approach. NER is commonly approached as a sequence labeling task with the application of methods such as conditional random field (CRF). However, for low-resource languages without the presence of sufficiently large training data, methods such as CRF do not perform well. In our work, we make use of the proximity of the vector embeddings of words to approach the NER problem. The hypothesis is that word vectors belonging to the same name category, such as a person's name, occur in close vicinity in the abstract vector space of the embedded words. Assuming that this clustering hypothesis is true, we apply a standard classification approach on the vectors of words to learn a decision boundary between the NER classes. Our NER experiments are conducted on a morphologically rich and low-resource language, namely Bengali. Our approach significantly outperforms standard baseline CRF approaches that use cluster labels of word embeddings and gazetteers constructed from Wikipedia. Further, we propose an unsupervised approach (that uses an automatically created named entity (NE) gazetteer from Wikipedia in the absence of training data). For a low-resource language, the word vectors obtained from Wikipedia are not sufficient to train a classifier. As a result, we propose to make use of the distance measure between the vector embeddings of words to expand the set of Wikipedia training examples with additional NEs extracted from a monolingual corpus that yield significant improvement in the unsupervised NER performance. In fact, our expansion method performs better than the traditional CRF-based (supervised) approach (i.e., F-score of 65.4% vs. 64.2%). Finally, we compare our proposed approach to the official submission for the IJCNLP-2008 Bengali NER shared task and achieve an overall improvement of F-score 11.26% with respect to the best official system. © 2017 ACM.",Classifier; CRF-based NER; Language-independent NER; Unsupervised NER; Wikipedia-based NER; Word embedding,Abstracting; Classifiers; Vector spaces; Vectors; CRF-based NER; Language independents; Unsupervised NER; Wikipedia; Word embedding; Random processes
Comparison study on critical components in composition model for phrase representation,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010737763&doi=10.1145%2f3010088&partnerID=40&md5=29059f2cd0004de11fd1a8954a596ce8,"Phrase representation, an important step in many NLP tasks, involves representing phrases as continuousvalued vectors. This article presents detailed comparisons concerning the effects of word vectors, training data, and the composition and objective function used in a composition model for phrase representation. Specifically, we first discuss how the augmented word representations affect the performance of the composition model. Then, we investigate whether different types of training data influence the performance of the composition model and, if so, how they influence it. Finally, we evaluate combinations of different composition and objective functions and discuss the factors related to composition model performance. All evaluations were conducted in both English and Chinese. Our main findings are as follows: (1) The Additive model with semantic enhanced word vectors performs comparably to the state-of-the-art model; (2) The Additive model which updates augmented word vectors and the Matrix model with semantic enhanced word vectors systematically outperforms the state-of-the-art model in bigram and multi-word phrase similarity task, respectively; (3) Representing the high frequency phrases by estimating their surrounding contexts is a good training objective for bigram phrase similarity tasks; and (4) The performance gain of composition model with semantic enhanced word vectors is due to the composition function and the greater weight attached to important words. Previous works focus on the composition function; however, our findings indicate that other components in the composition model (especially word representation) make a critical difference in phrase representation. © 2017 ACM.",Composition model; Max-margin; Mean square error; Phrase representation; Retrofitting; Word paraphrasing,Function evaluation; Mean square error; Retrofitting; Semantics; Composition functions; Composition model; Critical component; Max-margin; Objective functions; Phrase representation; Word paraphrasing; Word representations; Vectors
Improving transition-based dependency parsing of hindi and Urdu by modeling syntactically relevant phenomena,2017,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010756800&doi=10.1145%2f3005447&partnerID=40&md5=a2c2039bae0eb1b68ac088ab8f606cdf,"In recent years, transition-based parsers have shown promise in terms of efficiency and accuracy. Though these parsers have been extensively explored for multiple Indian languages, there is still considerable scope for improvement by properly incorporating syntactically relevant information. In this article, we enhance transition-based parsing of Hindi and Urdu by redefining the features and feature extraction procedures that have been previously proposed in the parsing literature of Indian languages. We propose and empirically show that properly incorporating syntactically relevant information like case marking, complex predication and grammatical agreement in an arc-eager parsing model can significantly improve parsing accuracy. Our experiments show an absolute improvement of ~2% LAS for parsing of both Hindi and Urdu over a competitive baseline which uses rich features like part-of-speech (POS) tags, chunk tags, cluster ids and lemmas. We also propose some heuristics to identify ezafe constructions in Urdu texts which show promising results in parsing these constructions. © 2017 ACM.",Averaged perceptron; Dependency parsing; Normalized pointwise mutual information; Shift-reduce parsing; Treebanks,Computational linguistics; Feature extraction; Averaged perceptron; Dependency parsing; Pointwise mutual information; Shift-reduce parsing; Treebanks; Syntactics
Modeling monolingual character alignment for automatic evaluation of Chinese translation,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020216171&doi=10.1145%2f2815619&partnerID=40&md5=33acf04b3df712e5b0a30aea3f578434,"Automatic evaluation of machine translations is an important task. Most existing evaluation metrics rely on matching the same word or letter n-grams. This strategy leads to poor results on Chinese translations because one has to rely merely on matching identical characters. In this article, we propose a new evaluation metric that allows different characters with the same or similar meaning to match. An Indirect Hidden Markov Model (IHMM) is proposed to align the Chinese translation with human references at the character level. In the model, the emission probabilities are estimated by character similarity, including character semantic similarity and character surface similarity, and transition probabilities are estimated by a heuristic distance-based distortion model. When evaluating the submitted output of English-to-Chinese translation systems in the IWSLT’08 CT-EC and NIST’08 EC tasks, the experimental results indicate that the proposed metric has a significantly better correlation with human evaluation than the state-of-the-art machine translation metrics (i.e., BLEU, Meteor Universal, and TESLA-CELAB). This study shows that it is important to allow different characters to match in the evaluation of Chinese translations and that the IHMM is a reasonable approach for the alignment of Chinese characters. © 2015 ACM",And Phrases: Automatic evaluation; Chinese character; Chinese translation; IHMM; Monolingual character alignment; Segment-level consistency; Synonym matching; System-level correlation; Word order,Computational linguistics; Computer aided language translation; Hidden Markov models; Semantics; Automatic evaluation; Chinese characters; IHMM; Segment-level consistency; Synonym matching; System levels; Word orders; Natural language processing systems
Bilingual semantic role labeling inference via dual decomposition,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026616501&doi=10.1145%2f2835493&partnerID=40&md5=6d2ea866621b5a479636e778819b879d,"This article focuses on bilingual Semantic Role Labeling (SRL); its goal is to annotate semantic roles on both sides of the parallel bilingual texts (bi-texts). Since rich bilingual information is encoded, bilingual SRL has been applied in many natural-language processing (NLP) tasks such as machine translation (MT), cross-lingual information retrieval (IR), and the like. A feasible way of performing bilingual SRL is using monolingual SRL systems to perform SRL on each side of bi-texts separately. However, it is difficult to obtain consistent SRL results on both sides of bi-texts in this way. Some works have tried to jointly infer bilingual SRL because there are many complementary language cues on both sides of bi-texts and they reported better performance than monolingual systems. However, there are two limits in the existing methods. First, the existing methods often require high inference costs due to the complex objective function. Second, the existing methods fully adopt the candidates generated by monolingual SRL systems, but many candidates are discarded in the argument pruning or identification stage of monolingual systems. In this article, we propose two strategies to overcome these limits. We utilize a simple but efficient technique: Dual Decomposition to search for consistent results for both sides of bi-texts. On the other hand, we propose a method called Bi-Directional Projection (BDP) to recover arguments discarded in monolingual SRL systems. We evaluate our method on a standard parallel benchmark: the OntoNotes dataset. The experimental results show that our method yields significant improvements over the state-of-the-art monolingual systems. In addition, our approach is also better and faster than existing methods due to BDP and Dual Decomposition. © 2015 ACM",Bi-directional projection; Bi-texts; Lagrange dual decomposition; Semantic role labeling,Natural language processing systems; Semantics; Bi-directional; Bi-lingual information; Cross-lingual information retrieval; Lagrange dual decompositions; Machine translations; Monolingual systems; Objective functions; Semantic role labeling; Learning algorithms
Using bisect k-means clustering technique in the analysis of Arabic documents,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050505616&doi=10.1145%2f2812809&partnerID=40&md5=c109a35a7681dbcd110891d3c15661c7,"In this article, I have investigated the performance of the bisect K-means clustering algorithm compared to the standard K-means algorithm in the analysis of Arabic documents. The experiments included five commonly used similarity and distance functions (Pearson correlation coefficient, cosine, Jaccard coefficient, Euclidean distance, and averaged Kullback-Leibler divergence) and three leading stemmers. Using the purity measure, the bisect K-means clearly outperformed the standard K-means in all settings with varying margins. For the bisect K-means, the best purity reached 0.927 when using the Pearson correlation coefficient function, while for the standard K-means, the best purity reached 0.884 when using the Jaccard coefficient function. Removing stop words significantly improved the results of the bisect K-means but produced minor improvements in the results of the standard K-means. Stemming provided additional minor improvement in all settings except the combination of the averaged Kullback-Leibler divergence function and the root-based stemmer, where the purity was deteriorated by more than 10%. These experiments were conducted using a dataset with nine categories, each of which contains 300 documents. © 2015 ACM",And Phrases: Information retrieval; Arabic stemmers; Bisect K-means; K-means; Similarity measures,Correlation methods; Natural language processing systems; Arabic stemmers; Jaccard coefficients; K-means; K-Means clustering algorithm; K-means clustering techniques; Kullback Leibler divergence; Pearson correlation coefficients; Similarity measure; Clustering algorithms
Adaptation of language models for SMT using neural networks with topic information,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057550206&doi=10.1145%2f2816816&partnerID=40&md5=be1950ede1dde8361ef149f6433dee29,"Neural network language models (LMs) are shown to be effective in improving the performance of statistical machine translation (SMT) systems. However, state-of-the-art neural network LMs usually use words before the current position as context and neglect global topic information, which can help machine translation (MT) systems to select better translation candidates from a higher perspective. In this work, we propose improvement of the state-of-the-art feedforward neural language model with topic information. Two main issues need to be tackled when adding topics into neural network LMs for SMT: one is how to incorporate topics to the neural network; the other is how to get target-side topic distribution before translation. We incorporate topics by appending topic distribution to the input layer of a feedforward LM. We adopt a multinomial logistic-regression (MLR) model to predict the target-side topic distribution based on source side information. Moreover, we propose a feedforward neural network model to learn joint representations on the source side for topic prediction. LM experiments demonstrate that the perplexity on validation set can be greatly reduced by the topic-enhanced feedforward LM, and the prediction of target-side topics can be improved dramatically with the MLR model equipped with the joint source representations. A final MT experiment, conducted on a large-scale Chinese–English dataset, shows that our feedforward LM with predicted topics improves the translation performance against a strong baseline. © 2015 ACM",Feedforward neural network language model; Joint representation; Multinomial logistic regression; Statistical machine translation; Topic model,Computer aided language translation; Feedforward neural networks; Forecasting; Regression analysis; Speech transmission; Machine translation systems; Multinomial logistic regression; Network language; Side information; State of the art; Statistical machine translation; Topic distributions; Topic Modeling; Computational linguistics
"Inter-, intra-, and extra-chunk pre-ordering for statistical Japanese-to-English machine translation",2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057568128&doi=10.1145%2f2818381&partnerID=40&md5=7102912d9a22d16eaa9bc920c6bdb5c1,"A rule-based pre-ordering approach is proposed for statistical Japanese-to-English machine translation using the dependency structure of source-side sentences. A Japanese sentence is pre-ordered to an English-like order at the morpheme level for a statistical machine translation system during the training and decoding phase to resolve the reordering problem. In this article, extra-chunk pre-ordering of morphemes is proposed, which allows Japanese functional morphemes to move across chunk boundaries. This contrasts with the intra-chunk reordering used in previous approaches, which restricts the reordering of morphemes within a chunk. Linguistically oriented discussions show that correct pre-ordering cannot be realized without extra-chunk movement of morphemes. The proposed approach is compared with five rule-based pre-ordering approaches designed for Japanese-to-English translation and with a language independent statistical preordering approach on a standard patent dataset and on a news dataset obtained by crawling Internet news sites. Two state-of-the-art statistical machine translation systems, one phrase-based and the other hierarchical phrase-based, are used in experiments. Experimental results show that the proposed approach outperforms the compared approaches on automatic reordering measures (Kendall’s τ, Spearman’s ρ, fuzzy reordering score, and test set RIBES) and on the automatic translation precision measure of test set BLEU score. © 2015 ACM",Chunk; Dependency structure; English; Japanese; Morpheme; Pre-ordering; Rule-based,Computational linguistics; Hierarchical systems; Statistics; Chunk; Dependency structures; English; Japanese; Morpheme; Pre orderings; Rule based; Computer aided language translation
Enhancing shift-reduce constituent parsing with action n-gram model,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057536343&doi=10.1145%2f2820902&partnerID=40&md5=850c889f5f971580fff7fc2f2456e6d4,"Current shift-reduce parsers “understand” the context by embodying a large number of binary indicator features with a discriminative model. In this article, we propose the action n-gram model, which utilizes the action sequence to help parsing disambiguation. The action n-gram model is trained on action sequences produced by parsers with the n-gram estimation method, which gives a smoothed maximum likelihood estimation of the action probability given a specific action history. We show that incorporating action n-gram models into a state-of-the-art parsing framework could achieve parsing accuracy improvements on three datasets across two languages. © 2016 ACM",Action history; Action n-gram model; And Phrases: Shift-reduce constituent parsing,Maximum likelihood estimation; Accuracy Improvement; Action sequences; And Phrases: Shift-reduce constituent parsing; Discriminative models; Estimation methods; N-gram modeling; N-gram models; State of the art; Computational linguistics
Arabic cross-language information retrieval: A review,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021742491&doi=10.1145%2f2789210&partnerID=40&md5=37ad4a88ad546644b6b810e36fc51763,"Cross-language information retrieval (CLIR) deals with retrieving relevant documents in one language using queries expressed in another language. As CLIR tools rely on translation techniques, they are challenged by the properties of highly derivational and flexional languages like Arabic. Much work has been done on CLIR for different languages including Arabic. In this article, we introduce the reader to the motivations for solving some problems related to Arabic CLIR approaches. The evaluation of these approaches is discussed starting from the 2001 and 2002 TREC Arabic CLIR tracks, which aim to objectively evaluate CLIR systems. We also study many other research works to highlight the unresolved problems or those that require further investigation. These works are discussed in the light of a deep study of the specificities and the tasks of Arabic information retrieval (IR). Particular attention is given to translation techniques and CLIR resources, which are key issues challenging Arabic CLIR. To push research in this field, we discuss how a new standard collection can improve Arabic IR and CLIR tracks. © 2015 ACM",Arabic cross-language information retrieval; Arabic translation knowledge; Complex morphology; Stemming; Stopwords removal; Tokenization; Transliteration,Computational linguistics; Information retrieval; Complex morphology; Cross language information retrieval; Stemming; Tokenization; Translation knowledge; Transliteration; Translation (languages)
Speech act identification using semantic dependency graphs with probabilistic context-free grammars,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046905425&doi=10.1145%2f2786978&partnerID=40&md5=496e41abbed2160cf8e86d21ae1aaf38,"We propose an approach for identifying the speech acts of speakers’ utterances in conversational spoken dialogue that involves using semantic dependency graphs with probabilistic context-free grammars (PCFGs). The semantic dependency graph based on the HowNet knowledge base is adopted to model the relationships between words in an utterance parsed by PCFG. Dependency relationships between words within the utterance are extracted by decomposing the semantic dependency graph according to predefined events. The corresponding values of semantic slots are subsequently extracted from the speaker’s utterances according to the corresponding identified speech act. The experimental results obtained when using the proposed approach indicated that the accuracy rates of speech act detection and task completion were 95.6% and 77.4% for human-generated transcription (REF) and speech-to-text recognition output (STT), respectively, and the average numbers of turns of each dialogue were 8.3 and 11.8 for REF and STT, respectively. Compared with Bayes classifier, partial pattern tree, and Bayesian-network-based approaches, we obtained 14.1%, 9.2%, and 3% improvements in the accuracy of speech act identification, respectively. © 2015 ACM",Conversational dialogue systems; Probabilistic context-free grammars; Semantic dependency graph; Speech act identification; Spoken language processing,Bayesian networks; Character recognition; Context free grammars; Context free languages; Graphic methods; Knowledge based systems; Semantics; Speech communication; Speech processing; Dialogue systems; Probabilistic context free grammars; Semantic dependency; Speech acts; Spoken language processing; Speech recognition
Converting Continuous-Space Language Models into N-gram Language Models with Efficient Bilingual Pruning for Statistical Machine Translation,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992511776&doi=10.1145%2f2843942&partnerID=40&md5=6ad62c7fd5b515fdca55009d1908a540,"The Language Model (LM) is an essential component of Statistical Machine Translation (SMT). In this article, we focus on developing efficient methods for LM construction. Our main contribution is that we propose a Natural N-grams based Converting (NNGC) method for transforming a Continuous-Space Language Model (CSLM) to a Back-off N-gram Language Model (BNLM). Furthermore, a Bilingual LM Pruning (BLMP) approach is developed for enhancing LMs in SMT decoding and speeding up CSLM converting. The proposed pruning and converting methods can convert a large LM efficiently by working jointly. That is, a LM can be effectively pruned before it is converted from CSLM without sacrificing performance, and further improved if an additional corpus contains out-of-domain information. For different SMT tasks, our experimental results indicate that the proposed NNGC and BLMP methods outperform the existing counterpart approaches significantly in BLEU and computational cost. 2016 Copyright is held by the owner/author(s).",,Computer aided language translation; Speech transmission; Backoffs; Computational costs; Continuous spaces; Domain informations; Language model; N-gram language models; N-grams; Statistical machine translation; Computational linguistics
A discourse-based approach for arabic question answering,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997418735&doi=10.1145%2f2988238&partnerID=40&md5=90628b1adcdd506fed8f87cbc182182b,"The treatment of complex questions with explanatory answers involves searching for arguments in texts. Because of the prominent role that discourse relations play in reflecting text producers' intentions, capturing the underlying structure of text constitutes a good instructor in this issue. From our extensive review, a system for automatic discourse analysis that creates full rhetorical structures in large-scale Arabic texts is currently unavailable. This is due to the high computational complexity involved in processing a large number of hypothesized relations associated with large texts. Therefore, more practical approaches should be investigated. This article presents a new Arabic Text Parser oriented for question-answering systems dealing with 'Arabic passage' ""why"" and 'Arabic passage' ""how to"" questions. The Text Parser presented here considers the sentence as the basic unit of text and incorporates a set of heuristics to avoid computational explosion. With this approach, the developed question-answering system reached a significant improvement over the baseline with a Recall of 68% and MRR of 0.62. © 2016 ACM.",Arabic question answering; Discourse analysis; Information extraction,Artificial intelligence; Information retrieval; Semantics; Syntactics; Text processing; Arabic texts; Basic units; Complex questions; Discourse analysis; Question Answering; Question answering systems; Rhetorical structure; Structure of text; Natural language processing systems
Word re-segmentation in Chinese-Vietnamese machine translation,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997521996&doi=10.1145%2f2988237&partnerID=40&md5=ece93b7425ced42ace6fceec1e2a4ff9,"In isolated languages, such as Chinese and Vietnamese, words are not separated by spaces, and a word may be formed by one or more syllables. Therefore, word segmentation (WS) is usually the first process that is implemented in the machine translation process. WS in the source and target languages is based on different training corpora, and WS approaches may not be the same. Therefore, the WS that results in these two languages are not often homologous, and thus word alignment results in many 1-n and n-1 alignment pairs in statistical machine translation, which degrades the performance of machine translation. In this article, we will adjust the WS for both Chinese and Vietnamese in particular and for isolated language pairs in general and make the word boundary of the two languages more symmetric in order to strengthen 1-1 alignments and enhance machine translation performance. We have tested this method on the Computational Linguistics Center's corpus, which consists of 35,623 sentence pairs. The experimental results show that our method has significantly improved the performance of machine translation compared to the baseline translation system, WS translation system, and anchor language-based WS translation systems. © 2016 ACM.",Character; Chinese-vietnamese machine translation; Isolated language; Word boundary; Word re-segmentation; Word segmentation,Alignment; Computational linguistics; Computer aided language translation; Linguistics; Character; Isolated language; Machine translations; Word boundary; Word segmentation; Translation (languages)
Improving semantic parsing with enriched synchronous context-free grammars in statistical machine translation,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997343283&doi=10.1145%2f2963099&partnerID=40&md5=b02495dd363b430b886aca3ebc8d7d6f,"Semantic parsing maps a sentence in natural language into a structured meaning representation. Previous studies show that semantic parsing with synchronous context-free grammars (SCFGs) achieves favorable performance over most other alternatives. Motivated by the observation that the performance of semantic parsing with SCFGs is closely tied to the translation rules, this article explores to extend translation rules with high quality and increased coverage in three ways. First, we examine the difference between word alignments for semantic parsing and statistical machine translation (SMT) to better adapt word alignment in SMT to semantic parsing. Second, we introduce both structure and syntax informed nonterminals, better guiding the parsing in favor of well-formed structure, instead of using a uninformed nonterminal in SCFGs. Third, we address the unknown word translation issue via synthetic translation rules. Last but not least, we use a filtering approach to improve performance via predicting answer type. Evaluation on the standard GeoQuery benchmark dataset shows that our approach greatly outperforms the state of the art across various languages, including English, Chinese, Thai, German, and Greek. © 2016 ACM.",Enriched synchronous context-free grammars; Semantic parsing; Statistical machine translation; Word alignment,Alignment; Computer aided language translation; Linguistics; Semantics; Speech transmission; Syntactics; Translation (languages); Benchmark datasets; Improve performance; Semantic parsing; Statistical machine translation; Structured meanings; Synchronous context-free grammars; Translation rules; Word alignment; Context free grammars
Understanding document semantics from summaries: A case study on Hindi texts,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997294901&doi=10.1145%2f2956236&partnerID=40&md5=57d07c8fd4df79cf159885b17f8b1119,"Summary of a document contains words that actually contribute to the semantics of the document. Latent Semantic Analysis (LSA) is a mathematical model that is used to understand document semantics by deriving a semantic structure based on patterns of word correlations in the document. When using LSA to capture semantics from summaries, it is observed that LSA performs quite well despite being completely independent of any external sources of semantics. However, LSA can be remodeled to enhance its capability to analyze correlations within texts. By taking advantage of the model being language independent, this article presents two stages of LSA remodeling to understand document semantics in the Indian context, specifically from Hindi text summaries. One stage of remodeling is done by providing supplementary information, such as document category and domain information. The second stage of remodeling is done by using a supervised term weighting measure in the process. The remodeled LSA's performance is empirically evaluated in a document classification application by comparing the accuracies of classification to plain LSA. An improvement in the performance of LSA in the range of 4.7% to 6.2% is achieved from the remodel when compared to the plain model. The results suggest that summaries of documents efficiently capture the semantic structure of documents and is an alternative to full-length documents for understanding document semantics. © 2016 ACM.",Dimensionality reduction; Document classification; Experimentation; Extractive summary; H.3.3 [information storage and retrieval]: information search and retrieval - retrieval models; I.2.7 [artificial intelligence]: natural language processing - language models; I.5.1 [pattern recognition]: models - statistical; Performance; Semantic structure; Singular value decomposition; Supervised term weighting; Supplemented latent semantic analysis; Text analysis,Artificial intelligence; Character recognition; Classification (of information); Computer keyboards; Information retrieval systems; Natural language processing systems; Pattern recognition; Semantics; Singular value decomposition; Text processing; Dimensionality reduction; Document Classification; Experimentation; Extractive summary; I.5.1 [pattern recognition]: models - statistical; Latent Semantic Analysis; NAtural language processing; Performance; Retrieval models; Semantic structures; Term weighting; Text analysis; Information retrieval
An approach to construct a named entity annotated English-Vietnamese bilingual corpus,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057489504&doi=10.1145%2f2990191&partnerID=40&md5=188db4fd22968c7f498d254a58aa6f96,"Manually constructing an annotated Named Entity (NE) in a bilingual corpus is a time-consuming, labor–intensive, and expensive process, but this is necessary for natural language processing (NLP) tasks such as cross-lingual information retrieval, cross-lingual information extraction, machine translation, etc. In this article, we present an automatic approach to construct an annotated NE in English-Vietnamese bilingual corpus from a bilingual parallel corpus by proposing an aligned NE method. Basing this corpus on a bilingual corpus in which the initial NEs are extracted from its own language separately, the approach tries to correct unrecognized NEs or incorrectly recognized NEs before aligning the NEs by using a variety of bilingual constraints. The generated corpus not only improves the NE recognition results but also creates alignments between English NEs and Vietnamese NEs, which are necessary for training NE translation models. The experimental results show that the approach outperforms the baseline methods effectively. In the English-Vietnamese NE alignment task, the F-measure increases from 68.58% to 79.77%. Thanks to the improvement of the NE recognition quality, the proposed method also increases significantly: the F-measure goes from 84.85% to 88.66% for the English side and from 75.71% to 85.55% for the Vietnamese side. By providing the additional semantic information for the machine translation systems, the BLEU score increases from 33.04% to 45.11%. © 2016 ACM",Annotated bilingual corpus; English-Vietnamese; Named entity translation; Word alignment constraint,Alignment; Computational linguistics; Computer aided language translation; Semantics; Bilingual corpora; Cross-lingual information; Cross-lingual information retrieval; Machine translation systems; Machine translations; Named entity translation; Vietnamese; Word alignment; Natural language processing systems
Minimally supervised Chinese event extraction from multiple views,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997428781&doi=10.1145%2f2994600&partnerID=40&md5=0a268ad8990ef235826bbd7d1d56693b,"Although several semi-supervised learning models have been proposed for English event extraction, there are few successful stories in Chinese due to its special characteristics. In this article, we propose a novel minimally supervised model for Chinese event extraction from multiple views. Besides the traditional pattern similarity view (PSV), a semantic relationship view (SRV) is introduced to capture the relevant event mentions from relevant documents. Moreover, a morphological structure view (MSV) is incorporated to both infer more positive patterns and help filter negative patterns via morphological structure similarity. An evaluation of the ACE 2005 Chinese corpus shows that our minimally supervised model significantly outperforms several strong baselines. © 2016 ACM.",Chinese event extraction; Minimally supervised model; Morphological structure view; Semantic relationship view,Data mining; Extraction; Semantics; Supervised learning; Chinese corpus; Event extraction; Morphological structures; Multiple views; Pattern similarity; Relevant documents; Semantic relationships; Semi- supervised learning; Information analysis
A semisupervised tag-transition-based markovian model for Uyghur morphology analysis,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997079132&doi=10.1145%2f2968410&partnerID=40&md5=db00c5c84f2228db1d13149001917e43,"Morphological analysis, which includes analysis of part-of-speech (POS) tagging, stemming, and morpheme segmentation, is one of the key components in natural language processing (NLP), particularly for agglutinative languages. In this article, we investigate the morphological analysis of the Uyghur language, which is the native language of the people in the Xinjiang Uyghur autonomous region of western China. Morphological analysis of Uyghur is challenging primarily because of factors such as (1) ambiguities arising due to the likelihood of association of a multiple number of POS tags with a word stem or a multiple number of functional tags with a word suffix, (2) ambiguous morpheme boundaries, and (3) complex morphopholonogy of the language. Further, the unavailability of a manually annotated training set in the Uyghur language for the purpose of word segmentation makes Uyghur morphological analysis more difficult. In our proposed work, we address these challenges by undertaking a semisupervised approach of learning a Markov model with the help of a manually constructed dictionary of ""suffix to tag"" mappings in order to predict the most likely tag transitions in the Uyghur morpheme sequence. Due to the linguistic characteristics of Uyghur, we incorporate a prior belief in our model for favoring word segmentations with a lower number of morpheme units. Empirical evaluation of our proposed model shows an accuracy of about 82%. We further improve the effectiveness of the tag transition model with an active learning paradigm. In particular, we manually investigated a subset of words for which the model prediction ambiguity was within the top 20%. Manually incorporating rules to handle these erroneous cases resulted in an overall accuracy of 93.81%. © 2016 ACM.",Markov model; Uyghur morphological analysis,Artificial intelligence; Computational linguistics; Markov processes; Morphology; Natural language processing systems; Agglutinative language; Empirical evaluations; Markov model; Morphological analysis; Morphology analysis; NAtural language processing; Overall accuracies; Part of speech tagging; Linguistics
Query expansion in resource-scarce languages: A multilingual framework utilizing document structure,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997170523&doi=10.1145%2f2997643&partnerID=40&md5=039ee5c76e22426db23b4fb55afb3206,"Retrievals in response to queries to search engines in resource-scarce languages often produce no results, which annoys the user. In such cases, at least partially relevant documents must be retrieved. We propose a novel multilingual framework, MultiStructPRF, which expands the query with related terms by (i) using a resource-rich assisting language and (ii) giving varied importance to the expansion terms depending on their position of occurrence in the document. Our system uses the help of an assisting language to expand the query in order to improve system recall. We propose a systematic expansion model for weighting the expansion terms coming from different parts of the document. To combine the expansion terms from query language and assisting language, we propose a heuristics-based fusion model. Our experimental results show an improvement over other PRF techniques in both precision and recall for multiple resource-scarce languages like Marathi, Bengali, Odia, Finnish, and the like. We study the effect of different assisting languages on precision and recall for multiple query languages. Our experiments reveal an interesting fact: Precision is positively correlated with the typological closeness of query language and assisting language, whereas recall is positively correlated with the resource richness of the assisting language. © 2016 ACM.",Multilingual retrieval; Query expansion; Resource scarce languages,Query languages; Search engines; Document structure; Multilingual retrieval; Multiple resources; Precision and recall; Query expansion; Relevant documents; Resource-Rich; Systematic expansion; Expansion
Boosted web named entity recognition via tri-training,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994495946&doi=10.1145%2f2963100&partnerID=40&md5=b76b37028a5905b3de805e119783c09c,"Named entity extraction is a fundamental task for many natural language processing applications on the web. Existing studies rely on annotated training data, which is quite expensive to obtain large datasets, limiting the effectiveness of recognition. In this research, we propose a semisupervised learning approach for web named entity recognition (NER) model construction via automatic labeling and tri-training. The former utilizes structured resources containing known named entities for automatic labeling, while the latter makes use of unlabeled examples to improve the extraction performance. Since this automatically labeled training data may contain noise, a self-testing procedure is used as a follow-up to remove low-confidence annotation and prepare higher-quality training data. Furthermore, we modify tri-training for sequence labeling and derive a proper initialization for large dataset training to improve entity recognition. Finally, we apply this semisupervised learning framework for person name recognition, business organization name recognition, and location name extraction. In the task of Chinese NER, an F-measure of 0.911, 0.849, and 0.845 can be achieved, for person, business organization, and location NER, respectively. The same framework is also applied for English and Japanese business organization name recognition and obtains models with performance of a 0.832 and 0.803 F-measure. © 2016 ACM.",Named entity recognition; Semisupervised learning; Tri-training for sequence labeling; Tri-training initialization,Extraction; Learning algorithms; Speech recognition; Supervised learning; Annotated training data; Business organizations; Named entity extraction; Named entity recognition; Natural language processing applications; Semi- supervised learning; Sequence Labeling; Tri-training; Natural language processing systems
A seed-based method for generating Chinese confusion sets,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057866182&doi=10.1145%2f2933396&partnerID=40&md5=a6ce848576cd8ecf2973a6b8b3b4287e,"In natural language, people often misuse a word (called a “confused word”) in place of other words (called “confusing words”). In misspelling corrections, many approaches to finding and correcting misspelling errors are based on a simple notion called a “confusion set.” The confusion set of a confused word consists of confusing words. In this article, we propose a new method of building Chinese character confusion sets. Our method is composed of two major phases. In the first phase, we build a list of seed confusion sets for each Chinese character, which is based on measuring similarity in character pinyin or similarity in character shape. In this phase, all confusion sets are constructed manually, and the confusion sets are organized into a graph, called a “seed confusion graph” (SCG), in which vertices denote characters and edges are pairs of characters in the form (confused character, confusing character). In the second phase, we extend the SCG by acquiring more pairs of (confused character, confusing character) from a large Chinese corpus. For this, we use several word patterns (or patterns) to generate new confusion pairs and then verify the pairs before adding them into a SCG. Comprehensive experiments show that our method of extending confusion sets is effective. Also, we shall use the confusion sets in Chinese misspelling corrections to show the utility of our method. © 2016 ACM.",And Phrases: Confusion set; Context probability; Pattern matching; Pinyin similarity; Shape similarity,Natural language processing systems; Pattern matching; And Phrases: Confusion set; Chinese characters; Chinese corpus; Confusion sets; Measuring similarities; Natural languages; Pinyin similarities; Shape similarity; Character sets
Boosting neural Pos tagger for farsi using morphological information,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055383319&doi=10.1145%2f2934676&partnerID=40&md5=d3031f189fdb7b6699f33ef9ed1f41fe,"Farsi (Persian) is a low-resource language that suffers from the data sparsity problem and a lack of efficient processing tools. Due to their broad application in natural language processing tasks, part-of-speech (POS) taggers are one of those important tools that should be considered in this respect. Despite recent work on Farsi tagging, there is still room for improvement. The best reported accuracy so far is 96%, which in special cases can rise to 96.9%. The main problem with existing taggers is their inefficiency in coping with out-of-vocabulary (OOV) words. Addressing both problems of accuracy and OOV words, we developed a neural network-based POS tagger (NPT) that performs efficiently on Farsi. Despite using less data, NPT provides better results in comparison to state-of-the-art systems. Our proposed tagger performs with an accuracy of 97.4%, with performance highly influenced by morphological features. We carry out a shallow morphological analysis and show considerable improvement over the baseline configuration. © 2016 ACM.",And Phrases: POS tagging; Farsi; Morphological analysis,Industrial plants; Morphology; Natural language processing systems; Baseline configurations; Farsi; Low resource languages; Morphological analysis; Morphological information; Out of vocabulary words; PoS tagging; State-of-the-art system; Computational linguistics
Printed text image database for sindhi OCR,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046635054&doi=10.1145%2f2846093&partnerID=40&md5=d2e2dbaa780d6006233f646d494bc507,"Document Image Understanding (DIU) and Electronic Document Management are active fields of research involving image understanding, interpretation, efficient handling, and routing of documents as well as their retrieval. Research on most of the noncursive scripts (Latin) has matured, whereas research on the cursive (connected) scripts is still moving toward perfection. Many researchers are currently working on the cursive scripts (Arabic and other scripts adopting it) around the world so that the difficulties and challenges in document understanding and handling of these scripts can be overcome. Sindhi script has the largest extension of the original Arabic alphabet among languages adopting the Arabic script; it contains 52 characters, compared to 28 characters in the original Arabic alphabet, in order to accommodate more sounds for the language. There are 24 differentiating characters with some possessing four dots. For Sindhi OCR research and development, a database is needed for training and testing of Sindhi text images. We have developed a large database containing over 4 billion words and 15 billion characters in 150 various fonts in four font weights and four styles. The database contents were collected from various sources including websites, books, and theses. A custom-built application was also developed to create a text image from a text document that supports various fonts and sizes. The database considers words, characters, characters with spaces, and lines. The database is freely available as a partial or full database by sending an email to one of the authors. © 2016 ACM.",Sindhi optical character recognition; Text image database,Image understanding; Information services; Optical character recognition; Word processing; Arabic scripts; Database contents; Document understanding; Electronic document management; Large database; Research and development; Text images; Training and testing; Database systems
Online handwritten gurmukhi strokes dataset based on minimal set of words,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013624031&doi=10.1145%2f2896318&partnerID=40&md5=31d73ca0c1c548d91a99ff27954b49aa,"The online handwriting data are an integral part of data analysis and classification research, as collected handwritten data offers many challenges to group handwritten stroke classes. The present work has been done for grouping handwritten strokes from the Indic script Gurmukhi. Gurmukhi is the script of the popular and widely spoken language Punjabi. The present work includes development of the dataset of Gurmukhi words in the context of online handwriting recognition for real-life use applications, such as maps navigation. We have collected the data of 100 writers from the largest cities in the Punjab region. The writers’ variations, such as writing skill level (beginner, moderate, and expert), gender, right or left handedness, and their adaptability to digital handwriting, have been considered in dataset development. We have introduced a novel technique to form handwritten stroke classes based on a limited set of words. The presence of all alphabets including vowels of Gurmukhi script has been considered before selection of a word. The developed dataset includes 39,411 strokes from handwritten words and forms 72 classes of strokes after using a k-means clustering technique and manual verification through expert and moderate writers. We have achieved recognition results using the Hidden Markov Model as 87.10%, 85.43%, and 84.33% for middle zone strokes when using training data as 66%, 50%, and 80% of the developed dataset. The present work is a step in a direction to find groups for unknown handwriting strokes with reasonably higher levels of accuracy. © 2016 ACM",And Phrases: Online handwriting recognition; Classification; Clustering; Data collection; Digital handwriting; HMM; K-means,Classification (of information); Hidden Markov models; Clustering; Data collection; Digital handwriting; K-means; Online handwriting recognition; Character recognition
From image to translation: Processing the endangered Nyushu script,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057551705&doi=10.1145%2f2857052&partnerID=40&md5=eed8850c0ec058cb7cc6c2856f21eedf,"The lack of computational support has significantly slowed down automatic understanding of endangered languages. In this paper, we take Nyushu (simplified Chinese:; literally: “women’s writing”) as a case study to present the first computational approach that combines Computer Vision and Natural Language Processing techniques to deeply understand an endangered language. We developed an end-to-end system to read a scanned hand-written Nyushu article, segment it into characters, link them to standard characters, and then translate the article into Mandarin Chinese. We propose several novel methods to address the new challenges introduced by noisy input and low resources, including Nyushu-specific feature selection for character segmentation and linking, and character linking lattice based Machine Translation. The end-to-end system performance indicates that the system is a promising approach and can serve as a standard benchmark. © 2016 ACM",Endangered languages; Nyushu; Recognition; Translation,Benchmarking; Natural language processing systems; Automatic understanding; Character segmentation; Computational approach; End-to-end systems; Endangered languages; Machine translations; Nyushu; Recognition; Translation (languages)
Word segmentation for Burmese (Myanmar),2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009487612&doi=10.1145%2f2846095&partnerID=40&md5=bc3d8c39201c560212ff3bb6491d05ca,"Experiments on various word segmentation approaches for the Burmese language are conducted and discussed in this note. Specifically, dictionary-based, statistical, and machine learning approaches are tested. Experimental results demonstrate that statistical and machine learning approaches perform significantly better than dictionary-based approaches. We believe that this note, based on an annotated corpus of relatively considerable size (containing approximately a half million words), is the first systematic comparison of word segmentation approaches for Burmese. This work aims to discover the properties and proper approaches to Burmese textual processing and to promote further researches on this understudied language.",Algorithm; Burmese; Myanmar; Syllable; Word segmentation,Algorithms; Artificial intelligence; Learning systems; Burmese; Machine learning approaches; Myanmars; Syllable; Word segmentation; Computational linguistics
A “suggested” picture of web search in Turkish,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057567686&doi=10.1145%2f2891105&partnerID=40&md5=c15c16ab18554c0b6293cb9ca456b6b4,"Although query log analysis provides crucial insights about Web users’ search interests, conducting such analyses is almost impossible for some languages, as large-scale and public query logs are quite scarce. In this study, we first survey the existing query collections in Turkish and discuss their limitations. Next, we adopt a novel strategy to obtain a set of Turkish queries using the query autocompletion services from the four major search engines and provide the first large-scale analysis of Web queries and their results in Turkish. © 2016 ACM",And Phrases: Turkish query characteristics; query spelling correction,Search engines; Websites; Large-scale analysis; Novel strategies; Query log analysis; Query logs; Spelling correction; Turkishs; Web searches; Web users; Information retrieval
A four-tier annotated Urdu handwritten text image dataset for multidisciplinary research on Urdu script,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046657590&doi=10.1145%2f2857053&partnerID=40&md5=91707a824be84057b7f1d9e00db3fa81,"This article introduces a large handwritten text document image corpus dataset for Urdu script named CALAM (Cursive And Language Adaptive Methodologies). The database contains unconstrained handwritten sentences along with their structural annotations for the offline handwritten text images with their XML representation. Urdu is the fourth most frequently used language in the world, but due to its complex cursive writing script and low resources, it is still a thrust area for document image analysis. Here, a unified approach is applied in the development of an Urdu corpus by collecting printed texts, handwritten texts, and demographic information of writers on a single form. CALAM contains 1,200 handwritten text images, 3,043 lines, 46,664 words, and 101,181 ligatures. For capturing maximum variance among the words and handwritten styles, data collection is distributed among six categories and 14 subcategories. Handwritten forms were filled out by 725 different writers belonging to different geographical regions, ages, and genders with diverse educational backgrounds. A structure has been designed to annotate handwritten Urdu script images at line, word, and ligature levels with an XML standard to provide a ground truth of each image at different levels of annotation. This corpus would be very useful for linguistic research in benchmarking and providing a testbed for evaluation of handwritten text recognition techniques for Urdu script, signature verification, writer identification, digital forensics, classification of printed and handwritten text, categorization of texts as per use, and so on. The experimental results of some recently developed handwritten text line segmentation techniques experimented on the proposed dataset are also presented in the article for asserting its viability and usability. © 2016 ACM",And Phrases: Urdu handwritten text; Annotation; Corpus; OCR algorithms benchmarking,Benchmarking; Classification (of information); Digital forensics; Electronic document identification systems; Geographical regions; Optical character recognition; Text processing; XML; Annotation; Corpus; Demographic information; Document image analysis; Hand-written text recognition; Handwritten texts; Multi-disciplinary research; OCR algorithms; Image annotation
A fast and compact language model implementation using double-array structures,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046714339&doi=10.1145%2f2873068&partnerID=40&md5=530a9eac9edd43c9cdc473db9f4215bf,"The language model is a widely used component in fields such as natural language processing, automatic speech recognition, and optical character recognition. In particular, statistical machine translation uses language models, and the translation speed and the amount of memory required are greatly affected by the performance of the language model implementation. We propose a fast and compact implementation of n-gram language models that increases query speed and reduces memory usage by using a double-array structure, which is known to be a fast and compact trie data structure. We propose two types of implementation: one for backward suffix trees and the other for reverse tries. The data structure is optimized for space efficiency by embedding model parameters into otherwise unused spaces in the double-array structure. We show that the reverse trie version of our method is among the smallest state-of-the-art implementations in terms of model size with almost the same speed as the implementation that performs fastest on perplexity calculation tasks. Similarly, we achieve faster decoding while keeping compact model sizes, and we confirm that our method can utilize the efficiency of the double-array structure to achieve a balance between speed and size on translation tasks. © 2016 ACM",Double array; Language model; Machine translation; N-gram model; Trie,Computer aided language translation; Data structures; Efficiency; Natural language processing systems; Optical character recognition; Speech recognition; Double arrays; Language model; Machine translations; N-gram modeling; Trie; Computational linguistics
Classification of printed Gujarati characters using low-level stroke features,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053500189&doi=10.1145%2f2856105&partnerID=40&md5=359af8dd63816b7c8c88f8d3f7497035,"This article presents an elegant technique for extracting the low-level stroke features, such as endpoints, junction points, line elements, and curve elements, from offline printed text using a template matching approach. The proposed features are used to classify a subset of characters from Gujarati script. The database consists of approximately 16,782 samples of 42 middle-zone symbols from the Gujarati character set collected from three different sources: machine printed books, newspapers, and laser printed documents. The purpose of this division is to add variety in terms of size, font type, style, ink variation, and boundary deformation. The experiments are performed on the database using a k-nearest neighbor (kNN) classifier and results are compared with other widely used structural features, namely Chain Codes (CC), Directional Element Features (DEF), and Histogram of Oriented Gradients (HoG). The results show that the features are quite robust against the variations and give comparable performance with other existing works. © 2016 ACM",Characters classification; Gujarati characters; Stroke features,Character recognition; Character sets; Nearest neighbor search; Template matching; Directional element feature; Gujarati characters; Gujarati scripts; Histogram of oriented gradients (HOG); K-nearest neighbor classifiers (KNN); Printed documents; Stroke features; Structural feature; Classification (of information)
Bangla handwritten character segmentation using structural features: A supervised and bootstrapping approach,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057567135&doi=10.1145%2f2890497&partnerID=40&md5=0a5c9603c142a37f23f6ecfba6133d4c,"In this article, we propose a new framework for segmentation of Bangla handwritten word images into meaningful individual symbols or pseudo-characters. Existing segmentation algorithms are not usually treated as a classification problem. However, in the present study, the segmentation algorithm is looked upon as a two-class supervised classification problem. The method employs an SVM classifier to select the segmentation points on the word image on the basis of various structural features. For training of the SVM classifier, an unannotated training set is prepared first using candidate segmenting points. The training set is then clustered, and each cluster is labeled manually with minimal manual intervention. A semi-automatic bootstrapping technique is also employed to enlarge the training set from new samples. The overall architecture describes a basic step toward building an annotation system for the segmentation problem, which has not so far been investigated. The experimental results show that our segmentation method is quite efficient in segmenting not only word images but also handwritten texts. As a part of this work, a database of Bangla handwritten word images has also been developed. Considering our data collection method and a statistical analysis of our lexicon set, we claim that the relevant characteristics of an ideal lexicon set are present in our handwritten word image database. © 2016 ACM",And Phrases: Supervised classification based segmentation; Annotation; Bangla handwriting database; Bootstrapping; Handwriting segmentation; Structural features; SVM classifier,Classification (of information); Database systems; Statistical methods; Supervised learning; Annotation; Bootstrapping; Handwriting segmentation; Structural feature; Supervised classification; SVM classifiers; Image segmentation
Learning generalized features for semantic role labeling,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044983384&doi=10.1145%2f2890496&partnerID=40&md5=66763bc63c70321a00280d2b63a269b5,"This article makes an effort to improve Semantic Role Labeling (SRL) through learning generalized features. The SRL task is usually treated as a supervised problem. Therefore, a huge set of features are crucial to the performance of SRL systems. But these features often lack generalization powers when predicting an unseen argument. This article proposes a simple approach to relieve the issue. A strong intuition is that arguments occurring in similar syntactic positions are likely to bear the same semantic role, and, analogously, arguments that are lexically similar are likely to represent the same semantic role. Therefore, it will be informative to SRL if syntactic or lexical similar arguments can activate the same feature. Inspired by this, we embed the information of lexicalization and syntax into a feature vector for each argument and then use K-means to make clustering for all feature vectors of training set. For an unseen argument to be predicted, it will belong to the same cluster as its similar arguments of training set. Therefore, the clusters can be thought of as a kind of generalized feature. We evaluate our method on several benchmarks. The experimental results show that our approach can significantly improve the SRL performance. © 2016 ACM",And Phrases: Semantic role labeling; Generalized features; K-means; Similar arguments,Syntactics; Feature vectors; Generalized features; K-means; Lexicalization; Semantic role labeling; Semantic roles; Similar arguments; Simple approach; Semantics
BenLem (A Bengali lemmatizer) and its role in WSD,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037071361&doi=10.1145%2f2835494&partnerID=40&md5=f17d1c6d956ac08220b30cae131e5df1,"A lemmatization algorithm for Bengali has been developed and evaluated. Its effectiveness for word sense disambiguation (WSD) is also investigated. One of the key challenges for computer processing of highly inflected languages is to deal with the frequent morphological variations of the root words appearing in the text. Therefore, a lemmatizer is essential for developing natural language processing (NLP) tools for such languages. In this experiment, Bengali, which is the national language of Bangladesh and the second most popular language in the Indian subcontinent, has been taken as a reference. In order to design the Bengali lemmatizer (named as BenLem), possible transformations through which surface words are formed from lemmas are studied so that appropriate reverse transformations can be applied on a surface word to get the corresponding lemma back. BenLem is found to be capable of handling both inflectional and derivational morphology in Bengali. It is evaluated on a set of 18 news articles taken from the FIRE Bengali News Corpus consisting of 3,342 surface words (excluding proper nouns) and found to be 81.95% accurate. The role of the lemmatizer is then investigated for Bengali WSD. Ten highly polysemous Bengali words are considered for sense disambiguation. The FIRE corpus and a collection of Tagore's short stories are considered for creating the WSD dataset. Different WSD systems are considered for this experiment, and it is noticed that BenLem improves the performance of all the WSD systems and the improvements are statistically significant. © 2016 ACM.",Bengali; Evaluation; Indic languages; Lemmatizer; Word sense disambiguation(WSD),Linguistics; Bengalis; Evaluation; Indian subcontinents; Lemmatization algorithms; Lemmatizer; Morphological variation; Reverse Transformation; Word-sense disambiguation; Natural language processing systems
Extracting Arabic causal relations using linguistic patterns,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997451496&doi=10.1145%2f2800786&partnerID=40&md5=7a77b9b270070dc178d0e0979d4bfc7e,"Identifying semantic relations is a crucial step in discourse analysis and is useful for many applications in both language and speech technology. Automatic detection of Causal relations therefore has gained popularity in the literature within different frameworks. The aim of this article is the automatic detection and extraction of Causal relations that are explicitly expressed in Arabic texts. To fulfill this goal, a Pattern Recognizer model was developed to signal the presence of cause–effect information within sentences from nonspecific domain texts. This model incorporates approximately 700 linguistic patterns so that parts of the sentence representing the cause and those representing the effect can be distinguished. The patterns were constructed based on different sets of syntactic features by analyzing a large untagged Arabic corpus. In addition, the model was boosted with three independent algorithms to deal with certain types of grammatical particles that indicate causation. With this approach, the proposed model achieved an overall recall of 81% and a precision of 78%. Evaluation results revealed that the justification particles play a key role in detecting Causal relations. To the best of our knowledge, no previous studies have been dedicated to dealing with this type of relation in the Arabic language. © 2016 ACM",And Phrases: Patterns matching; Arabic discourse relations; Causal relations; Information extraction,Agricultural engineering; Information retrieval; Natural resources; Arabic discourse relations; Automatic Detection; Causal relations; Effect information; Linguistic patterns; Pattern recognizers; Patterns matching; Syntactic features; Semantics
Constructing complex search tasks with coherent subtask search goals,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057770220&doi=10.1145%2f2742547&partnerID=40&md5=dec7785b9d9a032175ed1a196f60314b,"Nowadays, due to the explosive growth of web content and usage, users deal with their complex search tasks by web search engines. However, conventional search engines consider a search query corresponding only to a simple search task. In order to accomplish a complex search task, which consists of multiple subtask search goals, users usually have to issue a series of queries. For example, the complex search task ""travel to Dubai"" may involve several subtask search goals, including reserving hotel room, surveying Dubai landmarks, booking flights, and so forth. Therefore, a user can efficiently accomplish his or her complex search task if search engines can predict the complex search task with a variety of subtask search goals. In this work, we propose a complex search task model (CSTM) to deal with this problem. The CSTM first groups queries into complex search task clusters, and then generates subtask search goals from each complex search task cluster. To raise the performance of CSTM, we exploit four web resources including community question answering, query logs, search engine result pages, and clicked pages. Experimental results show that our CSTM is effective in identifying the comprehensive subtask search goals of a complex search task. 2015 Copyright is held by the owner/author(s). © 2016 Association for Computing Machinery. All Rights Reserved.",Algorithms; Human Factors; Languages,Algorithms; Human engineering; Query languages; Search engines; Community question answering; Complex searches; Explosive growth; Search engine results; Search queries; Search tasks; Web content; Web resources; Information retrieval
Pairwise comparative classification for translator stylometric analysis,2016,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997146385&doi=10.1145%2f2898997&partnerID=40&md5=c74196bb0b0f83a7735518786d468629,"In this article, we present a new type of classification problem, which we call Comparative Classification Problem (CCP), where we use the term data record to refer to a block of instances. Given a single data record with n instances for n classes, the CCP problem is to map each instance to a unique class. This problem occurs in a wide range of applications where the independent and identically distributed assumption is broken down. The primary difference between CCP and classical classification is that in the latter, the assignment of a translator to one record is independent of the assignment of a translator to a different record. In CCP, however, the assignment of a translator to one record within a block excludes this translator from further assignments to any other record in that block. The interdependency in the data poses challenges for techniques relying on the independent and identically distributed (iid) assumption. In the Pairwise CCP (PWCCP), a pair of records is grouped together. The key difference between PWCCP and classical binary classification problems is that hidden patterns can only be unmasked by comparing the instances as pairs. In this article, we introduce a new algorithm, PWC4.5, which is based on C4.5, to manage PWCCP. We first show that a simple transformation-that we call Gradient-Based Transformation (GBT)-can fix the problem of iid in C4.5. We then evaluate PWC4.5 using two real-world corpora to distinguish between translators on Arabic-English and French-English translations. While the traditional C4.5 failed to distinguish between different translators, GBT demonstrated better performance. Meanwhile, PWC4.5 consistently provided the best results over C4.5 and GBT. © 2016 ACM",Arabic translation; Classification; Translator stylometry,Classification (of information); Natural resources; Arabic-English; Binary classification problems; Broken down; French-english; Gradient based; Hidden patterns; Real-world; Stylometry; Agricultural engineering
Effectiveness Analysis of Entrepreneurial Legal Risk Prevention Based on Multi-Modal Deep Learning Model,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197357277&doi=10.1145%2f3622937&partnerID=40&md5=eb6f10e311223609add96ad8efee6690,"With the emergence of the upsurge of entrepreneurship, entrepreneurs are increasingly concerned about legal risks. In the process of entrepreneurship, legal risk is the biggest hidden danger of entrepreneurial enterprises. The prevention and avoidance of legal risks is a long and arduous process, and not all risks can be identified and avoided in time. The deep learning method has brought great changes to the fields of speech recognition, image recognition and natural language processing. The tasks in these fields only involve single-mode input, but more recent applications need to involve multi-mode intelligence. Multimodal deep learning mainly includes three aspects: multimodal learning representation, multimodal signal fusion and multimodal application. Through the research on the legal risks in recent years, this paper believed that there are many legal problems in the development of entrepreneurial enterprises, including contract disputes, trade secret disputes, trademark infringement disputes, copyright infringement disputes, and so on. This paper aimed to study the problem of Legal Risks of Entrepreneurship (LRE) that entrepreneurs are concerned about, and proposed a new solution from the perspective of image recognition technology. 486 entrepreneurs were investigated by questionnaire. In the process of entrepreneurship, 87.04% of entrepreneurs encountered legal risks, and they would turn to lawyers for help, which is a better way. However, in most cases, entrepreneurs are exposed to LRE because of their insufficient understanding of economic law, so they solve it through lawyers. However, if a lawyer is hired, the cost would be very high, which would bring great economic pressure to the enterprise. Only 17.90% of entrepreneurs would safeguard their legitimate rights and interests through their own knowledge and legal weapons without resorting to lawyers. It can be seen that entrepreneurs have relatively low practical ability in the use of LRE, and their legal practical ability is obviously insufficient. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesVenture Legal Risk; artificial neural network; image recognition technology; multimodal deep learning model; risk prevention,Commerce; Copyrights; Deep learning; Image recognition; Laws and legislation; Learning systems; Natural language processing systems; Risk assessment; Speech recognition; Additional key word and phrasesventure legal risk; Effectiveness analysis; Image recognition technology; Key words; Learning methods; Learning models; Legal risks; Multi-modal; Multimodal deep learning model; Risk prevention; Neural networks
Application of 3D Image Technology in Rural Planning,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197412400&doi=10.1145%2f3628448&partnerID=40&md5=e8e30ac88953f3c37aed22488f3112ba,"The well-being of villages and villagers is directly related to the development of urban-rural relations. Rural development is an important part of poverty alleviation, as well as the main goal and means of rural rejuvenation, because rural planning affects rural economic development and rural revitalization. Electronic imaging can improve the speed of rural area planning, and can also model the planned scheme. However, the current rural planning still lacks top-level design, which cannot improve the overall structural design of rural planning, and the data resources of rural planning are not perfect. Therefore, this article studied the direction of rural planning and design by analyzing the focus, problems, and external environment characteristics of rural planning, and then analyzed the application characteristics in rural planning according to the process of 3D image technology. By reducing the complex design links in rural planning, the office efficiency of planning and the 3D visualization of planning model can be promoted, so as to improve the effect of rural planning and the construction service of rural planning. The simulated annealing algorithm showed that the planning efficiency and average planning speed of 3D image application in rural planning were gradually increasing; the average planning efficiency was about 1.80, and the average planning speed was about 1.05. The planning efficiency increased by 1.20 in the whole process, while the average planning speed increased by 0.33 in the whole process. Through comparison, it can be seen that the planning rationality of rural planning under 3D image was 12.16% higher than the original one, while the measurement error rate of some teachers was 47.28% lower. In a word, 3D imaging and electronic imaging can improve the architectural design and layout planning of rural planning. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D imaging; electronic imaging; planning process analysis; Rural planning,Economic and social effects; Efficiency; Imaging systems; Regional planning; Simulated annealing; Structural design; Three dimensional computer graphics; 3-D image; 3D imaging; 3D-images; Electronic imaging; Image technology; Planning process; Planning process analyse; Process analysis; Rural planning; Whole process; Rural areas
Intelligent Multimedia Network Security and PBL Teaching Mode in the Basic Course Teaching of College Design Major,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197456471&doi=10.1145%2f3597429&partnerID=40&md5=41ac7b8b80e4ed3dca4f9cf638a332ca,"At this stage, there are problems of disconnection between theoretical knowledge and practice and lack of coherence and progression in the teaching of basic courses for design majors. The teaching of basic courses for design majors is an important part of design disciplines. Improving the teaching effect of basic courses of design majors is conducive to cultivating students’ design thinking and improving students’ creative ability. Under the traditional teaching mode, the teaching of the basic courses of design majors excessively pursues the cultivation of skills, ignoring the cultivation of students’ imagination and creativity. Students are not very motivated to study the basic courses of design majors. Therefore, this article studied the application of intelligent multimedia Project-based Learning (PBL) teaching mode in the teaching of basic courses of design majors and analyzed the intelligent multimedia PBL teaching mode from five aspects: students’ interest in learning, teamwork ability, learning efficiency, academic level, and satisfaction. Research showed that under the intelligent multimedia PBL teaching mode, students’ interest in learning has increased by 4.37%, teamwork ability has increased by 5.07%, learning efficiency has increased by 4.65%, academic performance has increased by 5.85%, and student satisfaction has also increased. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",basic courses for design majors; design majors; Intelligent multimedia PBL teaching mode; traditional teaching mode,Curricula; Efficiency; Network security; Teaching; Basic course; Basic course for design major; Design major; Intelligent multimedia; Intelligent multimedium project-based learning teaching mode; Multimedia projects; Project based learning; Teaching modes; Traditional teaching mode; Students
Research on Recognition Method of Social Robot Based on T-A-GCNIIT in the Metaverse,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197422585&doi=10.1145%2f3624014&partnerID=40&md5=1f829066a432a7b5100354863679f937,"Social robots are used in intelligent customer service, intelligent chat, intelligent shopping guides, and more because of emotion recognition studies in cognitive psychology. However, determining the user’s purpose quickly and precisely has proved challenging. Domestic researchers proposed the A-GCNII model to address missing feature information; however, it needs a lot of math. This research offers a social robot recognition approach using the T-A-GCNIIT model and cognitive psychology to optimize computing complexity and performance. The T-A-GCNIIT algorithm processes social network data, and the Viola–Jones algorithm improves social robot intelligence to represent social robots in the meta-universe. The model performs well in node classification, link prediction, community discovery, and other tasks, with enhanced accuracy, recall, F1 score value, and other metrics. The model can also better comprehend the user’s emotional state using cognitive psychology to better recognize their purpose and propose a fresh notion for enhancing social robots’ cognitive psychology. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cognitive psychology; metaverse; Social robot; T-A-GCNIIT algorithm; Viola–Jones algorithm,Intelligent robots; Cognitive psychology; Customer-service; Emotion recognition; Metaverses; Missing features; Recognition methods; Shopping guides; Social robots; T-A-GCNIIT algorithm; Viola - Jones algorithms; Emotion Recognition
Virtual Sound Image Reconstruction Method for Multi-objective Optimization of Folk Music Based on Evolutionary Algorithm,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197374448&doi=10.1145%2f3604614&partnerID=40&md5=d8e365d10d0c3ae607bfc0daf50779e5,"At present, the exchanges in various fields such as culture, economy, and politics are becoming more and more close in the world. From the perspective of the relationship between national music and cultural development, the development of national music has also received more and more attention, and it has become an inevitable trend in the development of today’s era. The purpose of this paper was to perform virtual reconstruction of the sound image of folk music through multiple objective optimizations, and to model the virtual sound image of folk music as a multi-objective optimization problem. According to the research on sound image positioning, the relevant noise factors were removed, so as to achieve the playback of ethnic music that enhanced the surround effect and visual enjoyment. The evolutionary algorithm in this paper was mainly based on the multi-objective optimized sound image localization technology. For the music virtual sound image, an improved FCM algorithm and an evolutionary multi-objective optimization algorithm combining local and non-local information of the sound image were proposed, respectively. Through the analysis of the traditional sound image algorithm method, the accuracy of the sound image localization on the horizontal plane could be effectively improved. After the conversion, the audience could feel the better stereo image and surround feeling of the folk music. Through experimental analysis, it can be seen that the system can not only perform virtual conversion of audio-visual signals of different frequencies, but also provide data for different audio playback systems. The feature point registration error is low, and the reconstruction effect is good, especially the random delay processing in the range of 0∼20m, and the performance is better than the traditional method. Finally, virtual left and right surround sound image signals were obtained, which effectively improved the three-dimensional surround feeling of folk music. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",digital multimedia; evolutionary algorithm; Folk music; multiobjective optimization; traditional optimization method; virtual sound image reconstruction,Audio acoustics; Audio systems; Evolutionary algorithms; Image enhancement; Image reconstruction; Music; Stereo image processing; Digital multimedia; Folk music; Images reconstruction; Multi-objectives optimization; Optimization method; Sound image; Sound image localizations; Traditional optimization method; Virtual sound image reconstruction; Virtual sound images; Multiobjective optimization
Neural Machine Translation for Low-Resource Languages from a Chinese-centric Perspective: A Survey,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197468586&doi=10.1145%2f3665244&partnerID=40&md5=21b2b0856ec4c49170d301bfbc32f476,"Machine translation-the automatic transformation of one natural language (source language) into another (target language) through computational means-occupies a central role in computational linguistics and stands as a cornerstone of research within the field of Natural Language Processing (NLP). In recent years, the prominence of Neural Machine Translation (NMT) has grown exponentially, offering an advanced framework for machine translation research. It is noted for its superior translation performance, especially when tackling the challenges posed by low-resource language pairs that suffer from a limited corpus of data resources. This article offers an exhaustive exploration of the historical trajectory and advancements in NMT, accompanied by an analysis of the underlying foundational concepts. It subsequently provides a concise demarcation of the unique characteristics associated with low-resource languages and presents a succinct review of pertinent translation models and their applications, specifically within the context of languages with low-resources. Moreover, this article delves deeply into machine translation techniques, highlighting approaches tailored for Chinese-centric low-resource languages. Ultimately, it anticipates upcoming research directions in the realm of low-resource language translation.  Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",Chinese-centric languages; large language models; Low-resource languages; multilingual translation; neural machine translation; transfer learning; unsupervised learning,Computational linguistics; Computer aided language translation; Deep learning; Learning algorithms; Natural language processing systems; Transfer learning; Automatic transformations; Chinese-centric language; Language model; Large language model; Low resource languages; Machine translations; Multilingual translations; Natural languages; Source language; Transfer learning; Neural machine translation
Investigation of Visual Language Landscape of Tourist Attractions from Multimodal Perspective,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197445996&doi=10.1145%2f3638049&partnerID=40&md5=c8703dd27bb56145b1351573437cf735,"With the development of economic globalization, the tourism industry has been welcomed by the public. The visual language landscape of tourist attractions can not only assist tourists to play and watch the project, but if it is properly planned, the language landscape can also become a major feature and highlight of the scenic spot. Therefore, how to set up and construct the visual language landscape of tourist attractions is a problem that needs to be considered in each region. In response to the above problems, on the basis of understanding the concept types of the visual language landscape of tourist attractions, this article conducts in-depth research and investigation on the visual language landscape of tourist attractions, combining the evaluation dataset in the multimodal perspective and the Convolutional Neural Network (CNN) –Recurrent Neural Network (RNN) model based on semantic regularization. This article conducted a comparative experiment on each model on the NUS-WIDE dataset and the MS-COCO dataset. The experimental results showed that it was crucial to give full play to the expressive power of the CNN. Compared to the NUS-WIDE dataset, the MS-COCO dataset brought less additional boost by leveraging social media tags. The CIDEr score of the CNN-RNN model based on semantic regularization was improved by 11.4%, which placed the foundation for the investigation and analysis of the linguistic landscape of tourist attractions. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",convolutional neural network; multimodal view; semantic regularization; tourist attractions; Visibility language landscape,Arts computing; Convolution; Convolutional neural networks; Recurrent neural networks; Semantic Web; Visual languages; Convolutional neural network; Economic globalization; Model-based OPC; Multi-modal; Multimodal view; Recurrent neural network model; Regularisation; Semantic regularization; Tourist attractions; Visibility language landscape; Semantics
Construction of Practical Teaching Mode of Law Course Based on Multi-Mode and Low-Resource Language Learning,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197430214&doi=10.1145%2f3622938&partnerID=40&md5=8b60133f0f63802a5d55bf07c9008f97,"China’s legal profession is a new profession emerging in the process of China’s rule of law. Its development and refinement are increasing in accordance with the rapid social, economic and political development. However, as a lawyer, people need to learn real kung fu. For complex and changeable legal practice problems, people should use the simplest and most effective way to solve them. Therefore, it is necessary to study the curriculum of law specialty to achieve its purpose. It is also necessary to reform the traditional education methods, introduce modern scientific and technological means, and improve their effectiveness, so as to construct a practical teaching mode of Professional Courses in Law (PCL). For practical legal courses, it can be conducted through multi-mode low-resource language learning method. This paper aimed to study how to study the ability of students to obtain information, process information and analyze and process information based on intelligent multi-image feature fusion. Based on the investigation of teaching objectives and teachers’ teaching requirements, and the analysis and arrangement of the investigation results, a multitask learning system model based on image intelligent features was established. In this experiment, 494 valid questionnaires were used to analyze the current situation of PCL practical teaching. Among them, 10.32% and 65.59% of the students were very satisfied with the results of PCL practical teaching, and 18.02% and 59.31% of the students liked PCL practical teaching very much. In the survey on the influencing factors of PCL practical teaching activities, 61.34% of the students thought that the students did not know enough and were not enthusiastic enough. On the whole, the students are satisfied with the practical teaching arrangement of PCL, but from the actual situation, the effect of practical teaching is not very good, and the expected goal has not been achieved. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",intelligent multi image feature fusion; multi-mode low-resource language learning; practical teaching; Professional Courses in Law; teaching model,Curricula; Education computing; Image analysis; Image fusion; Learning systems; Teaching; Features fusions; Image features; Intelligent multi image feature fusion; Language learning; Low resource languages; Multi-images; Multi-mode low-resource language learning; Multimodes; Practical teachings; Professional course in law; Teaching model; Students
Sentiment Analysis and Corpus: Cognitive Perspective and Overhead-accuracy Tradeoff,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197397501&doi=10.1145%2f3594537&partnerID=40&md5=777f74270eb801895cb52f78b32ea356,"Human logical thinking is in the form of natural language. With the development of computer science techniques, it becomes easier and more convenient for natural language processing. Therefore, various natural language processing applications have emerged. Sentiment analysis is one of these novel applications and has been applied in many areas. In Amazon.com, there are a large number of user comments and product discussions, which can help a person to decide whether to buy a product or not without asking the opinions from friends and family members. Therefore, sentiment analysis on user comments and product discussions, such as Amazon reviews, becomes increasingly useful and important. In this paper, the effect of corpus on sentiment analysis of the Amazon review dataset with the aid of support vector machine is studied. We generate eight different size datasets from the Amazon review dataset filtered by different word frequency in Corpus of Contemporary American and conduct some experiments on these eight datasets. According to the experimental result, we make some conclusions and give some suggestions to facilitate researchers to make a trade-off between accuracy and experimental cost. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Corpus of Contemporary American; natural language processing; sentiment analysis; support vector machine,Economic and social effects; Sentiment analysis; Cognitive perspectives; Corpus of contemporary american; Language processing; Logical thinking; Natural language processing; Natural language processing applications; Natural languages; Novel applications; Sentiment analysis; Support vectors machine; Support vector machines
Integrated End-to-End Automatic Speech Recognition for Languages for Agglutinative Languages,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197432635&doi=10.1145%2f3663568&partnerID=40&md5=bbbbfda75487cbac165deaec6aaaae67,"The relevance of the problem of automatic speech recognition lies in the lack of research for low-resource languages, stemming from limited training data and the necessity for new technologies to enhance efficiency and performance. The purpose of this work was to study the main aspects of integrated end-to-end speech recognition and the use of modern technologies in the natural processing of agglutinative languages, including Kazakh. In this article, the study of language models was carried out using comparative, graphic, statistical, and analytical-synthetic methods, which were used in combination. This article addresses automatic speech recognition (ASR) in agglutinative languages, particularly Kazakh, through a unified neural network model that integrates both acoustic and language modeling. Employing advanced techniques like connectionist temporal classification and attention mechanisms, the study focuses on effective speech-to-text transcription for languages with complex morphologies. Transfer learning from high-resource languages helps mitigate data scarcity in languages such as Kazakh, Kyrgyz, Uzbek, Turkish, and Azerbaijani. The research assesses model performance, underscores ASR challenges, and proposes advancements for these languages. It includes a comparative analysis of phonetic and word-formation features in agglutinative Turkic languages, using statistical data. The findings aid further research in linguistics and technology for enhancing speech recognition and synthesis, contributing to voice identification and automation processes. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data corpus; Language model; scarcity of resources; system learning,Classification (of information); Computational linguistics; Learning systems; Modeling languages; Natural language processing systems; Text processing; Agglutinative language; Automatic speech recognition; Data corpus; Efficiency and performance; End to end; Language model; Limited training data; Low resource languages; Scarcity of resource; System learning; Speech recognition
Research on the Implementation of Advertising Design Teaching Based on Unity3D Development Platform and Web3D Technology,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197402764&doi=10.1145%2f3595294&partnerID=40&md5=739149defe088908da53d05ade1844cd,"In this work, the Unity3D development platform and Web3D technology are integrated into the teaching method of advertising design to get rid of the issues due to lack of communication and efficacy through design of digital advertisement. Based on this, an approach for ingenious product design from nature is proposed, with an emphasis on attaining a functional interaction of aesthetic intent and geometric features and investigating the relationships among natural systems and designers in product design from nature. The ponderings and research findings for the methodologies associated with the proposed approach are presented. This methodology is thought to significantly bring down the delivery time of ground-breaking design and development of products, both economically and technologically. The findings in comprehensive experiments demonstrates that interactive virtual technology can significantly enhance the efficacy and interaction of the whole system in the process of digital advertising design. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Advertising design; product design from nature; Unity3D development platform; Web3D technology,Marketing; Advertizing; Advertizing design; Design teaching; Development platform; Platform technology; Product design from nature; Teaching methods; Unity3d; Unity3d development platform; Web3d technologies; Product design
Application of Psychological Effects in Intelligent Digital English Teaching Based on Multimodal Low Resource Language Information Processing from a Psychological Perspective,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197382675&doi=10.1145%2f3599726&partnerID=40&md5=a920ae5ff0a5afa3153e58efc59b9869,"With the development of information technology, intelligent digital education is gradually emerging. At the same time, with the closer economic and trade relations between regions, the demand for English talents is becoming more and more urgent. Applying the concept and method of intelligent digital education to English Teaching (ET) is of great significance for improving students’ learning efficiency and academic level, and enhancing students’ English application ability and cross-cultural communication ability. However, due to the limitations of the traditional teaching model, such as the single and outdated model, the low interest of students in learning, and the lack of emphasis on cultivating students’ English practical ability, it cannot effectively meet the requirements of improving students’ learning effect. Therefore, this article studies the intelligent digital ET mode based on multimodal low-resource language information processing, proposes the construction method of intelligent digital ET mode, and conducts experimental research on it. The research showed that the positive emotional input level index of Group X students was 8.47% higher than that of Group T students; the efficiency index of Group X students was 5.02% higher than that of Group T students; the autonomous learning ability and English learning level of Group X students were higher than those of Group T; the students in Group X had higher recognition of ET mode than those in Group T. The intelligent digital ET mode can effectively improve the learning effect. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",English teaching mode; intelligent digital education; learning efficiency; positive emotional input level; positive psychology,E-learning; Education computing; Efficiency; Learning systems; Teaching; English teaching; English teaching mode; Intelligent digital education; Language informations; Learning efficiency; Low resource languages; Multi-modal; Positive emotional input level; Positive psychology; Teaching modes; Students
Machine Learning and Natural Language Processing Algorithms in the Remote Mobile Medical Diagnosis System of Internet Hospitals,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197423110&doi=10.1145%2f3632172&partnerID=40&md5=7440bb72870169319a9db609a96e2241,"In order to alleviate the contradiction between supply and demand of professional pharmacists, integrate medical resources, and ensure the safety of patients’ medication, the telemedicine diagnosis system has played a great role. Today, in this “Internet +” era, all walks of life have begun to integrate with Internet technology. The purpose of this article was to discuss the practical utility of machine learning and natural language processing algorithms in the remote mobile medical diagnosis system of Internet hospitals, for which this article conducted in-depth discussion. This article first introduced the basic concepts, development, and characteristics of machine learning and natural language processing algorithms in detail, and carefully studied and analyzed the development and culture of traditional offline medical diagnosis models. Based on machine learning and natural language processing algorithms, a remote mobile medical diagnosis is designed. By combining with the medical diagnosis system of traditional hospitals, a new type of remote mobile medical diagnosis system for Internet hospitals was designed and developed, and the combination of traditional medical industry and Internet technology was deeply studied. According to each functional requirement, the image module, heart rate measurement module, and user setting module are designed, respectively. Compared to traditional medical diagnosis systems, the accuracy of the remote mobile medical diagnosis system based on machine learning applied in Internet hospital diagnosis in this article reached 80% or even higher. At the same time, it was found through experiments that when the evolution number was 3, the maximum fit value and average fit value were the same, both of which were 0.6. This indicates that the system can accommodate more than 10,000 people at the same time, and patients can receive good treatment plans, with a very broad application prospect. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",BP neural network; medical tesources; mobile Internet; natural language processing algorithm; Telemedicine diagnosis system,Diagnosis; Economics; Learning algorithms; Machine learning; Natural language processing systems; Neural networks; Patient treatment; Telemedicine; BP neural networks; Diagnose system; Language processing; Medical diagnosis system; Medical tesource; Mobile Internet; Natural language processing algorithm; Natural languages; Processing algorithms; Telemedicine diagnose system; Hospitals
An Interaction-Design Method Based upon a Modified Algorithm of Newton's Second Law of Motion,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197392365&doi=10.1145%2f3657634&partnerID=40&md5=23ce3cba671111bdf972adf1b735f1d4,"Newton’s Second Law of Motion algorithm is crucial to interactive visual effects and interactive behavior in interface design. Designers can only utilize simple algorithm templates in interface design since they lack organized mathematical science, especially programming. Directly using Newton’s Second Law of Motion algorithm introduces two interface design issues. First, the created picture has a simplistic impact, laborious interaction, too few interactive parts, and boring visual effects. Second, using this novel approach directly to interface design reduces creativity, originality, and cognitive inertia. This study suggests a Newton’s Second Law–based algorithm modification. It provides a novel algorithm application idea and a design strategy based on algorithm change to enable new interface design. Algorithm design gives interface design a new viewpoint and improves content production. In the arithmetic process of Newton’s Second Law of Motion algorithm, the introduction of repulsive force, reset force, shape, color, and other attributes of interactive objects, and the integration of other algorithms to transform its basic arithmetic logic, is conducive to the improvement of the visual effect of interaction design. It also improves users’ interaction experiences, sentiments, and desire to participate with design work. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",algorithm modification; interaction design; Newton’s Second Law of Motion,Computation theory; Algorithm modification; Interaction design; Interaction design methods; Interface designs; Law of motions; Modified algorithms; Motion algorithm; Newton’s second law of motion; Second law; Visual effects; Design
An adaptive dual graph convolution fusion network for aspect-based sentiment analysis,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197372612&doi=10.1145%2f3659579&partnerID=40&md5=18fe5a94e5b69bcc596ae0c29a3758a0,"Aspect-based Sentiment Analysis (ABSA), also known as fine-grained sentiment analysis, aims to predict the sentiment polarity of specific aspect words in the sentence. Some studies have explored the semantic correlation between words in sentences through attention-based methods. Other studies have learned syntactic knowledge by using graph convolution networks to introduce dependency relations. These methods have achieved satisfactory results in the ABSA tasks. However, due to the complexity of language, effectively capturing semantic and syntactic knowledge remains a challenging research question. Therefore, we propose an Adaptive Dual Graph Convolution Fusion Network (AD-GCFN) for aspect-based sentiment analysis. This model uses two graph convolution networks: one for the semantic layer to learn semantic correlations by an attention mechanism, and the other for the syntactic layer to learn syntactic structure by dependency parsing. To reduce the noise caused by the attention mechanism, we designed a module that dynamically updates the graph structure information for adaptively aggregating node information. To effectively fuse semantic and syntactic information, we propose a cross-fusion module that uses the double random similarity matrix to obtain the syntactic features in the semantic space and the semantic features in the syntactic space, respectively. Additionally, we employ two regularizers to further improve the ability to capture semantic correlations. The orthogonal regularizer encourages the semantic layer to learn word semantics without overlap, while the differential regularizer encourages the semantic and syntactic layers to learn different parts. Finally, the experimental results on three benchmark datasets show that the AD-GCFN model is superior to the contrast models in terms of accuracy and macro-F1.  Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",attention mechanism; double random matrix; Graph convolution; regularization,Semantics; Sentiment analysis; Syntactics; Attention mechanisms; Double random matrix; Dual graphs; Graph convolution; Learn+; Random Matrix; Regularisation; Regularizer; Semantic layer; Sentiment analysis; Convolution
Learning Domain Specific Sub-layer Latent Variable for Multi-Domain Adaptation Neural Machine Translation,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197368626&doi=10.1145%2f3661305&partnerID=40&md5=5c731b0e1d81876d3b19329b84ef5348,"Domain adaptation proves to be an effective solution for addressing inadequate translation performance within specific domains. However, the straightforward approach of mixing data from multiple domains to obtain the multi-domain neural machine translation (NMT) model can give rise to the parameter interference between domains problem, resulting in a degradation of overall performance. To address this, we introduce a multi-domain adaptive NMT method aimed at learning domain specific sub-layer latent variable and employ the Gumbel-Softmax reparameterization technique to concurrently train both model parameters and domain specific sub-layer latent variable. This approach facilitates learning private domain-specific knowledge while sharing common domain-invariant knowledge, effectively mitigating the parameter interference problem. The experimental results show that our proposed method significantly improved by up to 7.68 and 3.71 BLEU compared with the baseline model in English-German and Chinese-English public multi-domain datasets, respectively.  Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",multi-domain adaptation; Neural machine translation; parameter interference; sub-layer latent variable,Computational linguistics; Computer aided language translation; Domain Knowledge; Learning systems; Multilayer neural networks; Domain adaptation; Domain specific; Effective solution; Latent variable; Multi-domain adaptation; Multi-domains; Parameter interference; Performance; Sub-layer latent variable; Sub-layers; Neural machine translation
SUSTEM: An Improved Rule-based Sundanese Stemmer,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197404532&doi=10.1145%2f3656342&partnerID=40&md5=a1224861d280a8c7c788d628d871be5a,"Current Sundanese stemmers either ignore reduplication words or define rules to handle only affixes. There is a significant amount of reduplication words in the Sundanese language. Because of that, it is impossible to achieve superior stemming precision in the Sundanese language without addressing reduplication words. This article presents an improved stemmer for the Sundanese language, which handles affixed and reduplicated words. With a Sundanese root word list, we use a rules-based stemming technique. In our approach, all stems produced by the affixes removal or normalization processes are added to the stem list. Using a stem list can help increase stemmer accuracy by reducing stemming errors caused by affix removal sequence errors or morphological issues. The current Sundanese language stemmer, RBSS, was used as a comparison. Two datasets with 8,218 unique affixed words and reduplication words were evaluated. The results show that our stemmer’s strength and accuracy have improved noticeably. The use of stem list and word reduplication rules improved our stemmer’s affixed type recognition and allowed us to achieve up to 99.30% accuracy. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",affixed word; reduplication word; stemming; Sundanese language,'current; Affixed word; Normalization process; Reduplication word; Removal process; Rule based; Stemming; Sundanese language; Word lists
Collaborative Optimization of English Online Teaching Informatization Based on Intelligent Multimedia Image Technology,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197398523&doi=10.1145%2f3599725&partnerID=40&md5=c3fd22140b2409f10c30276bd0fbee37,"With the rapid development of computer technology and the deepening of educational concepts, the traditional English teaching methods have also changed. As a new teaching method, online teaching has received extensive attention. Online teaching has introduced intelligent multimedia image technology in English teaching, which has transformed the essence of English teaching by organically integrating traditional teaching and multimedia teaching. With the rapid growth of Internet and intelligent multimedia image technology, the number of digital images is growing exponentially. How to extract the required image from the large amount of multimedia image data has been an important topic at present. In this case, intelligent multimedia image technology emerges as the times require, which extracts the features of the image from the content of the image itself, and then retrieves it through the features. This article proposed a retrieval and classification method based on intelligent multimedia image technology. Among them, Support Vector Machine (SVM) was used to classify multimedia resources, which had a good effect. The results of this experiment showed that before the experiment, the listening score of the multimedia group was 65.5, and the speaking score was 59.7. After the experiment, the listening score of the multimedia group was 79.8, and the speaking score was 80.6. It can be found that the performance of the multimedia group has been greatly improved after the experiment, which shows that online English teaching based on intelligent multimedia image technology is more conducive to the improvement of students’ academic performance. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",English online teaching; image classification; intelligent multimedia image technology; Support Vector Machine,Data mining; E-learning; Image classification; Image enhancement; Informatization; Multimedia systems; Teaching; English online teaching; English teaching; Image technology; Images classification; Intelligent multimedia; Intelligent multimedium image technology; Multimedia images; Online teaching; Support vectors machine; Teaching methods; Support vector machines
Application of Intelligent Image Recognition and Digital Media Art in the Inheritance of Black Pottery Intangible Cultural Heritage,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197441817&doi=10.1145%2f3597430&partnerID=40&md5=b07a3d34458ea0e75c4bb8e140fad68f,"With the development of science and technology, intelligent image recognition and digital media art are gradually applied to the inheritance of intangible cultural heritage. In the inheritance of black pottery, intelligent image recognition can help identify the authenticity and age of black pottery, and digital media art can inject new vitality into black pottery culture. Digital media, which combines digitalization and mediaization, breaks down the limitations of traditional media and provides ideological and technical guarantees for the reconstruction, dissemination, and promotion of intangible cultural heritage protection. Image recognition technology is no longer limited to the auxiliary detection of images; on the contrary, it has become a recording and collection tool. With the promotion and development of science and technology, it can be more widely shared and disseminated in time and space. In this article, the traditional machine learning algorithm and the intelligent learning algorithm are compared in image recognition, and the image information recognition rate, the number of iterations, the training time, and the information loss rate are compared in four aspects. The results have found that, compared with the traditional learning algorithm, the number of iterations of the intelligent learning algorithm is reduced by 20%, the training time is reduced by 40%, the information loss rate is reduced by 6.3%, and the recognition accuracy rate is improved by 6.1%, which showed that under the current technology, intangible cultural heritage can be better inherited, and Chinese culture can be better promoted, so that more people can understand and join in the protection of intangible cultural heritage. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",convolutional neural network; cultural heritage; Cultural heritage; image recognition technology; intangible cultural heritage: digital media art,Arts computing; Convolutional neural networks; Image recognition; Learning algorithms; Machine learning; Convolutional neural network; Cultural heritages; Development of science and technologies; Image recognition technology; Intangible cultural heritage: digital medium art; Intangible cultural heritages; Intelligent learning; Media arts; Number of iterations; Digital storage
Research on Product Advertising Design Combining Feature Extraction Technology and Web3D Technology,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197389263&doi=10.1145%2f3608948&partnerID=40&md5=115d6d97e09532d756f9511bad0db824,"This work, built on the Unity3D development platform, presents a way for merging feature extraction technology and Web3D technology into advertising design to effectively address the issues of poor efficiency and distortion in the field. Using the candidate text layout generating technique of visual salience, we first build the vector function set based on the three main colors, then we produce the visual communication partition model of advertising design. Next, the number of feature parameters of the shape advertising design is obtained via the establishment of coding coefficient constraint features and the use of an upgraded neural network technique to extract local feature parameter information about the product. Finally, the product design model is brought to life using Web3D technology to boost advertising design’s productivity and accuracy. The experiments show that this method not only results in a high rate of correct product identification but also offers a fresh viewpoint on the visual communication of product advertising design by merging the two disciplines. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",feature extraction; product design from nature; Unity3D development platform; vector function; Web3D technology,Extraction; Feature extraction; Marketing; Merging; Visual communication; Advertizing; Development platform; Extraction technology; Feature parameters; Features extraction; Product design from nature; Unity3d; Unity3d development platform; Vector functions; Web3d technologies; Product design
More Than Syntaxes: Investigating Semantics to Zero-shot Cross-lingual Relation Extraction and Event Argument Role Labelling,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193513666&doi=10.1145%2f3582261&partnerID=40&md5=8e711916cbe80a9abb53567a6a4b72d6,"Syntactic dependency structures are commonly utilized as language-agnostic features to solve the word order difference issues in zero-shot cross-lingual relation and event extraction tasks. However, while sentences in multiple forms can be employed to express the same meaning, the syntactic structure may vary considerably in specific scenarios. To fix this problem, we find semantics are rarely considered, which could provide a more consistent semantic analysis of sentences and be served as another bridge between different languages. Therefore, in this article, we introduce Syntax and Semantic Driven Network (SSDN) to equip syntax and semantic knowledge across languages simultaneously. Specifically, predicate-argument structures from semantic role labelling are explicitly incorporated into word representations. Then, a semantic-aware relational graph convolutional network and a transformer-based encoder are utilized to model both semantic dependency and syntactic dependency structures, respectively. Finally, a fusion module is introduced to integrate output representations adaptively. We conduct experiments on the widely used Automatic Content Extraction 2005 English, Chinese, and Arabic datasets. The evaluation results demonstrate that the proposed method achieves the state-of-the-art performance. Further study also indicates SSDN could produce robust representations that facilitate the transfer operations across languages.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCross-lingual relation and event extraction; relational graph convolutional network; semantic parsing; zero-resource transfer,Convolution; Extraction; Semantic Web; Syntactics; Zero-shot learning; Additional key word and phrasescross-lingual relation and event extraction; Convolutional networks; Cross-lingual; Events extractions; Key words; Relation extraction; Relational graph; Relational graph convolutional network; Semantic parsing; Zero-resource transfer; Semantics
Multilingual Neural Machine Translation for Indic to Indic Languages,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193476432&doi=10.1145%2f3652026&partnerID=40&md5=c2e63e0ec45584c01ee1b156d663a24f,"The method of translation from one language to another without human intervention is known as Machine Translation (MT). Multilingual neural machine translation (MNMT) is a technique for MT that builds a single model for multiple languages. It is preferred over other approaches, since it decreases training time and improves translation in low-resource contexts, i.e., for languages that have insufficient corpus. However, good-quality MT models are yet to be built for many scenarios such as for Indic-to-Indic Languages (IL-IL). Hence, this article is an attempt to address and develop the baseline models for low-resource languages i.e., IL-IL (for 11 Indic Languages (ILs)) in a multilingual environment. The models are built on the Samanantar corpus and analyzed on the Flores-200 corpus. All the models are evaluated using standard evaluation metrics i.e., Bilingual Evaluation Understudy (BLEU) score (with the range of 0 to 100). This article examines the effect of the grouping of related languages, namely, East Indo-Aryan (EI), Dravidian (DR), and West Indo-Aryan (WI) on the MNMT model. From the experiments, the results reveal that related language grouping is beneficial for the WI group only while it is detrimental for the EI group and it shows an inconclusive effect on the DR group. The role of pivot-based MNMT models in enhancing translation quality is also investigated in this article. Owing to the presence of large good-quality corpora from English (EN) to ILs, MNMT IL-IL models using EN as a pivot are built and examined. To achieve this, English-Indic Language (EN-IL) models are developed with and without the usage of related languages. Results show that the use of related language grouping is advantageous specifically for EN to ILs. Thus, related language groups are used for the development of pivot MNMT models. It is also observed that the usage of pivot models greatly improves MNMT baselines. Furthermore, the effect of transliteration on ILs is also analyzed in this article. To explore transliteration, the best MNMT models from the previous approaches (in most of cases pivot model using related groups) are determined and built on corpus transliterated from the corresponding scripts to a modified Indian language Transliteration script (ITRANS). The outcome of the experiments indicates that transliteration helps the models built for lexically rich languages, with the best increment of BLEU scores observed in Malayalam (ML) and Tamil (TA), i.e., 6.74 and 4.72, respectively. The BLEU score using transliteration models ranges from 7.03 to 24.29. The best model obtained is the Punjabi (PA)-Hindi (HI) language pair trained on PA-WI transliterated corpus. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Indic languages; language relatedness; Multilingual neural machine translation; Pivot; transliteration,Computational linguistics; Computer aided language translation; Bilinguals; Human intervention; Indic language; Language model; Language relatedness; Machine translation models; Machine translations; Multilingual neural machine translation; Pivot; Transliteration; Neural machine translation
Multization: Multi-Modal Summarization Enhanced by Multi-Contextually Relevant and Irrelevant Attention Alignment,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193505922&doi=10.1145%2f3651983&partnerID=40&md5=957dbc7873f1553eed6d99f718cb6d96,"This article focuses on the task of Multi-Modal Summarization with Multi-Modal Output for China JD.COM e-commerce product description containing both source text and source images. In the context learning of multi-modal (text and image) input, there exists a semantic gap between text and image, especially in the cross-modal semantics of text and image. As a result, capturing shared cross-modal semantics earlier becomes crucial for multi-modal summarization. However, when generating the multi-modal summarization, based on the different contributions of input text and images, the relevance and irrelevance of multi-modal contexts to the target summary should be considered, so as to optimize the process of learning cross-modal context to guide the summary generation process and to emphasize the significant semantics within each modality. To address the aforementioned challenges, Multization has been proposed to enhance multi-modal semantic information by multi-contextually relevant and irrelevant attention alignment. Specifically, a Semantic Alignment Enhancement mechanism is employed to capture shared semantics between different modalities (text and image), so as to enhance the importance of crucial multi-modal information in the encoding stage. Additionally, the IR-Relevant Multi-Context Learning mechanism is utilized to observe the summary generation process from both relevant and irrelevant perspectives, so as to form a multi-modal context that incorporates both text and image semantic information. The experimental results in the China JD.COM e-commerce dataset demonstrate that the proposed Multization method effectively captures the shared semantics between the input source text and source images, and highlights essential semantics. It also successfully generates the multi-modal summary (including image and text) that comprehensively considers the semantics information of both text and image. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Business intelligence; multi-modal cross learning; multi-modal summarization; semantic enhancement and attention,Electronic commerce; Image enhancement; Signal encoding; Business-intelligence; Cross-modal; Modal semantics; Multi-modal; Multi-modal cross learning; Multi-modal summarization; Semantic enhancement and attention; Semantic enhancements; Semantics Information; Semantics
Knowledge-based Data Processing for Multilingual Natural Language Analysis,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191490667&doi=10.1145%2f3583686&partnerID=40&md5=e8e782b7b952398702b84f3d3068b928,"Natural Language Processing (NLP) aids the empowerment of intelligent machines by enhancing human language understanding for linguistic-based human-computer communication. Recent developments in processing power, as well as the availability of large volumes of linguistic data, have enhanced the demand for data-driven methods for automatic semantic analysis. This paper proposes multilingual data processing using feature extraction with classification using deep learning architectures. Here, the input text data has been collected based on various languages and processed to remove missing values and null values. The processed data has been extracted using Histogram Equalization based Global Local Entropy (HEGLE) and classified using Kernel-based Radial basis Function (Ker_Rad_BF). These architectures could be utilized to process natural language. We present solutions to the multilingual sentiment analysis issue in this research article by implementing algorithms, and we compare precision factors to discover the optimum option for multilingual sentiment analysis. For the HASOC dataset, the proposed HEGLE_ Ker_Rad_BF achieved an accuracy of 98%, a precision of 97%, a recall of 90.5%, an f-1 score of 85%, RMSE of 55.6%, and a loss curve analysis attained 44%. For the TRAC dataset, the accuracy of 98%, the precision attained is 97%, the Recall is 91%, the F-1 score is 87%, and the RMSE of the proposed neural network is 55%.  © 2024 Copyright held by the owner/author(s).",deep learning; Histogram Equalization based Global Local Entropy; Kernel-based Radial basis Function; multilingual data processing; Sentiment analysis,Classification (of information); Data handling; Deep learning; Entropy; Knowledge based systems; Network architecture; Radial basis function networks; Semantics; Sentiment analysis; Base function; Deep learning; Global-local; Histogram equalization based global local entropy; Histogram equalizations; Kernel-based radial base function; Local entropy; Multilingual data processing; Radial basis; Sentiment analysis; Graphic methods
Performance of Binarization Algorithms on Tamizhi Inscription Images: An Analysis,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193520971&doi=10.1145%2f3656583&partnerID=40&md5=2b3d2608c7fb7dfebbffbf9f135cb7f7,"Binarization of Tamizhi (Tamil-Brahmi) inscription images are highly challenging, as it is captured from very old stone inscriptions that exists around 3rd century BCE in India. The difficulty is due to the degradation of these inscriptions by environmental factors and human negligence over ages. Though many works have been carried out in the binarization of inscription images, very little research was performed for inscription images and no work has been reported for binarization of inscriptions inscribed on irregular medium. The findings of the analysis hold true to all writings that are carved in irregular background. This article reviews the performance of various binarization techniques on Tamizhi inscription images. Since no previous work was performed, we have applied the existing binarization algorithms on Tamizhi inscription images and analyzed the performance of these algorithms with proper reasoning. In the future, we believe that this reasoning on the results will help a new researcher to adapt or combine or devise new binarization techniques.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTamil-Brahmi; binarization; culture and heritage; inscription images; inscriptions; Tamizhi,'Brahmi'; Additional key word and phrasestamil-brahmi; Binarization algorithm; Binarizations; Culture and heritage; Inscription; Inscription image; Key words; Performance; Tamizhi; Image analysis
Automatically Temporal Labeled Data Generation Using Positional Lexicon Expansion for Focus Time Estimation of News Articles,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193544980&doi=10.1145%2f3568164&partnerID=40&md5=1c72a419bdc43aef6296a290e0d5966f,"Many facts change over time, which is a fundamental aspect of our physical environment. In the case of pandemic articles, the user is not interested in the creation date of the document but in the facts and the cause of the last pandemic. Fake news can be better combated by having a document with a temporal focus. Currently, neither the sequence of events nor the temporal focus is considered when obtaining news documents. Despite the limited number of temporal aspects in the available datasets, it is difficult to test and evaluate the temporal conclusions of the model. The goal of this work is to develop a temporal focus news article retrieval model based on co-training to advance research in semi-supervised learning. A mapping of the dataset is performed using (1) the evolving focus time of news articles and (2) the semi-supervised method based on coincidence contexts for learning low-dimensional continuous vectors for learning neural contrast embedding models generating focus time-based query in sequential news articles to facilitate temporal understanding by learning low-dimensional continuous vectors. A diverse dataset of news articles is used to evaluate the effectiveness of the proposed method. With semi-supervised learning and lexicon expansion, the result of the developed model can achieve 89%. The method performed better than previous baselines and traditional machine learning models with improvements of 12.65% and 4.7%, respectively.  © 2024 Association for Computing Machinery.",focus time; Information retrieval; inverted pyramid; news retrieval; temporal information retrieval,Learning algorithms; Learning systems; Supervised learning; Data generation; Focus time; Inverted pyramid; Labeled data; Lexicon expansions; Low dimensional; News articles; News retrievals; Semi-supervised learning; Temporal information retrievals; Information retrieval
Part-of-speech Tagging for Low-resource Languages: Activation Function for Deep Learning Network to Work with Minimal Training Data,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193540899&doi=10.1145%2f3655023&partnerID=40&md5=ae83ed02071b7851bc0a190cbe7fb321,"Numerous natural language processing (NLP) applications exist today, especially for the most commonly spoken languages such as English, Chinese, and Spanish. Popular traditional methods such as Rule based methods, Naive Bayes classifiers, Hidden Markov models, Conditional Random field-based classifiers, and other stochastic methods have contributed to this improvement in the past. Recently, deep learning has led to exciting breakthroughs in several areas of artificial intelligence, including image processing and natural language processing. It is important to label words as parts of speech to begin developing most of the NLP applications. A deep study in this area reveals that many popular approaches used for this purpose require massive training data. Therefore, these approaches have not been helpful for languages not rich in digital resources. Applying these methods with very little training data prompts the need for innovative problem-solving. This article describes our research, which examines the strengths and weaknesses of well-known approaches, such as conditional random fields and state-of-the-art deep learning models, when applied for part-of-speech tagging using minimal training data for Assamese and English. We also examine the factors affecting them. We discuss our deep learning architecture and the proposed activation function, which shows promise with little training data. The activation function categorizes words belonging to different classes with more confidence by using the outcomes of statistical methods with SMTaylor SoftMax in our deep learning model. With minimal training, our deep learning architecture using the proposed modification of SM-Taylor SoftMax improves accuracy upto 4%, for our small dataset. This technique is a combination of SMTaylor SoftMax and statistical probability distribution of words over tags.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",activation function; Additional Key Words and PhrasesLow-resource languages; Assamese; deep learning; minimal training; parts of speech tagging; SoftMax,Chemical activation; Computational linguistics; Deep learning; Image processing; Learning systems; Natural language processing systems; Network architecture; Probability distributions; Stochastic models; Stochastic systems; Syntactics; Activation functions; Additional key word and phraseslow-resource language; Assamese; Deep learning; Key words; Minimal training; Part of speech tagging; Parts-of-speech tagging; Softmax; Training data; Hidden Markov models
Crossing Linguistic Barriers: Authorship Attribution in Sinhala Texts,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193706562&doi=10.1145%2f3655620&partnerID=40&md5=12d75a8d1fb7ca3584184760b765f302,"Authorship attribution involves determining the original author of an anonymous text from a pool of potential authors. The author attribution task has applications in several domains, such as plagiarism detection, digital text forensics, and information retrieval. While these applications extend beyond any single language, existing research has predominantly centered on English, posing challenges for application in languages such as Sinhala due to linguistic disparities and a lack of language processing tools. We present the first comprehensive study on cross-topic authorship attribution for Sinhala texts and propose a solution that can effectively perform the authorship attribution task even if the topics within the test and training samples differ. Our solution consists of three main parts: (i) extraction of topic-independent stylometric features, (ii) generation of a small candidate author set with the help of similarity search, and (iii) identification of the true author. Several experimental studies were carried out to demonstrate that the proposed solution can effectively handle real-world scenarios involving a large number of candidate authors and a limited number of text samples for each candidate author.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAuthorship attribution; linguistic barriers; low-resource; Sinhala,Digital forensics; Information retrieval; Additional key word and phrasesauthorship attribution; Authorship attribution; Digital text; Key words; Language processing; Linguistic barrier; Low-resource; Plagiarism detection; Processing tools; Sinhalum; Linguistics
A Research on University Students' Behavioral Intention to Use New-generation Information Technology in Intelligent Foreign Language Learning,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193510518&doi=10.1145%2f3563774&partnerID=40&md5=ed6ab746576976915286831791584555,"A better understanding of how advancement in science and technology affect students' learning behavior in an academic setting can help all educators in higher education. With the advancement of science and technology, the new-generation information technology represented by ""Cloud Computing, Big Data, Internet of Things, Mobile Network, and Artificial Intelligence""has been profoundly changing the form of education, accelerating the transformation from Teaching 1.0 to Learning 2.0. To investigate the factors influencing university students using a new-generation information technology in intelligent foreign language learning, this study proposed a research model based on Technology Acceptance Model. The sample data were collected from a survey of 237 students at a university. The analysis of structural equation modeling indicated that perceived usefulness, perceived ease of use, construction of foreign language intelligence classroom, and computer self-efficacy all have a positive impact on learners' behavioral intention. These findings suggest that more emphasis should be laid on students' learning characteristics, advantages of a new generation of technology, as well as teachers' intelligent literacy, to further promote the deep integration of the new-generation information technology and intelligent foreign language learning, improving the learning effectiveness ultimately and effectively.  © 2024 Association for Computing Machinery.",Additional Key Words and PhrasesNew-generation information technology; behavioral intention; deep integration; intelligent foreign language learning; technology acceptance model,Deep learning; Education computing; Engineering education; Information use; Learning systems; Modeling languages; Students; Additional key word and phrasesnew-generation information technology; Behavioral intention; Deep integrations; Foreign language learning; Intelligent foreign language learning; Key words; Science and Technology; Student learning; Technology acceptance model; University students; Metadata
Supervised Contrast Learning Text Classification Model Based on Data Quality Augmentation,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193493758&doi=10.1145%2f3653300&partnerID=40&md5=b2781725103bf7f81d59b7e3fb5b1402,"Token-level data augmentation generates text samples by modifying the words of the sentences. However, data that are not easily classified can negatively affect the model. In particular, not considering the role of keywords when performing random augmentation operations on samples may lead to the generation of low-quality supplementary samples. Therefore, we propose a supervised contrast learning text classification model based on data quality augmentation. First, dynamic training is used to screen high-quality datasets containing beneficial information for model training. The selected data is then augmented with data based on important words with tag information. To obtain a better text representation to serve the downstream classification task, we employ a standard supervised contrast loss to train the model. Finally, we conduct experiments on five text classification datasets to validate the effectiveness of our model. In addition, ablation experiments are conducted to verify the impact of each module on classification.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrast learning; data quality; Text augmentation; text classification,Data reduction; Learning systems; Text processing; Classifieds; Contrast learning; Data augmentation; Data quality; Learning text; Low qualities; Model-based OPC; Text augmentation; Text classification; Text classification models; Classification (of information)
Knowledge-Enriched Prompt for Low-Resource Named Entity Recognition,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193496467&doi=10.1145%2f3659948&partnerID=40&md5=9c35b87b84a01efa6c12250493d2fbd0,"Named Entity Recognition (NER) in low-resource settings aims to identify and categorize entities in a sentence with limited labeled data. Although prompt-based methods have succeeded in low-resource perspectives, challenges persist in effectively harnessing information and optimizing computational efficiency. In this work, we present a novel prompt-based method to enhance low-resource NER without exhaustive template tuning. First, we construct knowledge-enriched prompts by integrating representative entities and background information to provide informative supervision tailored to each entity type. Then, we introduce an efficient reverse generative framework inspired by question answering (QA), which avoids redundant computations. Finally, we reduce costs by generating entities from their types while retaining model reasoning capacity. Experiment results demonstrate that our method outperforms other baselines on three datasets under few-shot settings.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLow-resource NER; Knowledge Injection; Prompt Engineering,Natural language processing systems; Additional key word and phraseslow-resource named entity recognition; Background information; Entity-types; Key words; Knowledge injection; Labeled data; Low-resource settings; Named entity recognition; Prompt engineering; Question Answering; Computational efficiency
Fast Recurrent Neural Network with Bi-LSTM for Handwritten Tamil Text Segmentation in NLP,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193510853&doi=10.1145%2f3643808&partnerID=40&md5=1705b5dde355079a68c5e80a9e6d78c1,"Tamil text segmentation is a long-standing test in language comprehension that entails separating a record into adjacent pieces based on its semantic design. Each segment is important in its own way. The segments are organised according to the purpose of the content examination as text groups, sentences, phrases, words, characters or any other data unit. That process has been portioned using rapid tangled neural organisation in this research, which presents content segmentation methods based on deep learning in natural language processing (NLP). This study proposes a bidirectional long short-term memory (Bi-LSTM) neural network prototype in which fast recurrent neural networks (FRNNs) are used to learn Tamil text group embedding and phrases are fragmented using text-oriented data. As a result, this prototype is capable of handling variable measured setting data and gives a vast new dataset for naturally segmenting text in Tamil. In addition, we develop a segmentation prototype and show how well it sums up to unnoticeable regular content using this dataset as a base. With Bi-LSTM, the segmentation precision of FRNN is superior to that of other segmentation approaches; however, it is still inferior to that of certain other techniques. Every content is scaled to the required size in the proposed framework, which is immediately accessible for the preparation. This means, each word in a scaled Tamil text is employed to prepare neural organisation as fragmented content. The results reveal that the proposed framework produces high rates of segmentation for manually authored material that are nearly equivalent to segmentation-based plans.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bidirectional LSTM; fast-RNN; natural language processing; offline handwriting; segmentation accuracy.; Tamil text segmentation,Image segmentation; Natural language processing systems; Semantics; Bidirectional LSTM; Fast-RNN; Language processing; Natural language processing; Natural languages; Offline handwriting; Segmentation accuracy; Segmentation accuracy.; Tamil text segmentation; Text segmentation; Long short-term memory
A Novel Pretrained General-purpose Vision Language Model for the Vietnamese Language,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193502419&doi=10.1145%2f3654796&partnerID=40&md5=b9ffaed02c140376ac2302b007795b0d,"Lying in the cross-section of computer vision and natural language processing, vision language models are capable of processing images and text at once. These models are helpful in various tasks: text generation from image and vice versa, image-text retrieval, or visual navigation. Besides building a model trained on a dataset for a task, people also study general-purpose models to utilize many datasets for multitasks. Their two primary applications are image captioning and visual question answering. For English, large datasets and foundation models are already abundant. However, for Vietnamese, they are still limited. To expand the language range, this work proposes a pretrained general-purpose image-text model named VisualRoBERTa. A dataset of 600k images with captions (translated MS COCO 2017 from English to Vietnamese) is introduced to pretrain VisualRoBERTa. The model's architecture is built using Convolutional Neural Network and Transformer blocks. Fine-tuning VisualRoBERTa shows promising results on the ViVQA dataset with 34.49% accuracy, 0.4173 BLEU 4, and 0.4390 RougeL (in visual question answering task), and best outcomes on the sViIC dataset with 0.6685 BLEU 4, 0.6320 RougeL (in image captioning task).  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesComputer vision; foundation; image text; machine learning; multi-modal; natural language processing; pretrain; Vietnamese; visual linguistic,Computational linguistics; Computer vision; Convolutional neural networks; Learning algorithms; Machine learning; Natural language processing systems; Visual languages; Additional key word and phrasescomputer vision; Image texts; Key words; Language processing; Machine-learning; Multi-modal; Natural language processing; Natural languages; Pretrain; Vietnamese; Visual linguistic; Large datasets
Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193737651&doi=10.1145%2f3654811&partnerID=40&md5=6b70cb41f7ca0788898118c7fe3033cd,"Scholars in the humanities heavily rely on ancient manuscripts to study history, religion, and socio-political structures of the past. Significant efforts have been devoted to digitizing these precious manuscripts using OCR technology. However, most manuscripts have been blemished over the centuries, making it unrealistic for OCR programs to accurately capture faded characters. This work presents the Transformer + Confidence Score mechanism architecture for post-processing Google's Tibetan OCR-ed outputs. According to the Loss and Character Error Rate metrics, our Transformer + Confidence Score mechanism architecture proves superior to the Transformer, LSTM-to-LSTM, and GRU-to-GRU architectures. Our method can be adapted to any language dealing with post-processing OCR outputs.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",neural networks; Post-processing OCR output; Transformer,Architecture; History; Network architecture; Optical character recognition; Confidence score; Correction models; Google+; Neural-networks; Political structure; Post-processing; Post-processing OCR output; Spelling correction; Tibetans; Transformer; Long short-term memory
MRMI-TTS: Multi-Reference Audios and Mutual Information Driven Zero-Shot Voice Cloning,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193514914&doi=10.1145%2f3649501&partnerID=40&md5=f58b7fff2c69a70af3513bd523330971,"Voice cloning in text-to-speech (TTS) is the process of replicating the voice of a target speaker with limited data. Among various voice cloning techniques, this article focuses on zero-shot voice cloning. Although existing TTS models can generate high-quality speech for seen speakers, cloning the voice of an unseen speaker remains a challenging task. The key aspect of zero-shot voice cloning is to obtain a speaker embedding from the target speaker. Previous works have used a speaker encoder to obtain a fixed-size speaker embedding from a single reference audio unsupervised, but they suffer from insufficient speaker information and content information leakage in speaker embedding. To address these issues, this article proposes MRMI-TTS, a FastSpeech2-based framework that uses speaker embedding as a conditioning variable to provide speaker information. The MRMI-TTS extracts speaker embedding and content embedding from multi-reference audios using a speaker encoder and a content encoder. To obtain sufficient speaker information, multi-reference audios are selected based on sentence similarity. The proposed model applies mutual information minimization on the two embeddings to remove entangled information within each embedding. Experiments on the public English dataset VCTK show that our method can improve synthesized speech in terms of both similarity and naturalness, even for unseen speakers. Compared to state-of-the-art reference embedding learned methods, our method achieves the best performance on the zero-shot voice cloning task. Furthermore, we demonstrate that the proposed method has a better capability of maintaining the speaker embedding in different languages. Sample outputs are available on the demo page.1 © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Multiple reference audios; Mutual information; Text-to-speech; Zero-shot,Cloning; Signal encoding; Zero-shot learning; Audio information; Embeddings; Limited data; Multi reference; Multiple reference audio; Multiple references; Mutual informations; Target speaker; Text to speech; Zero-shot; Embeddings
Multi-Lingual Representation of Natural Language Processing for Low Resource Asian Language Processing Systems,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193524433&doi=10.1145%2f3603169&partnerID=40&md5=99a27f2aba2a92822006749c82b34d06,[No abstract available],,
Neurocomputer System of Semantic Analysis of the Text in the Kazakh Language,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191592513&doi=10.1145%2f3652159&partnerID=40&md5=d1cc060a0619c0201dc0b3d6cf933c26,"The purpose of the study is to solve an extreme mathematical problem-semantic analysis of natural language, which can be used in various fields, including marketing research, online translators, and search engines. When training the neural network, data training methods based on the latent Dirichlet allocation model and vector representation of words were used. This study presents the development of a neurocomputer system used for the purpose of semantic analysis of the text in the Kazakh language, based on machine learning and the use of the latent Dirichlet allocation model. In the course of the study, the stages of system development were considered, regarding the text recognition algorithm. The Python programming language was used as a tool using libraries that greatly simplify the process of creating neural networks, including the Keras library. An experiment was conducted with the involvement of experts to test the effectiveness of the system, the results of which confirmed the reliability of the data provided by the system. The papers of modern computer linguists dealing with the problems of natural language processing using various technologies and methods are considered.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMachine learning; automatic recognition; Keras library; latent Dirichlet allocation; Python,Character recognition; Computational linguistics; High level languages; Learning systems; Marketing; Natural language processing systems; Neural networks; Search engines; Semantics; Statistics; Additional key word and phrasesmachine learning; Allocation model; Automatic recognition; Keras library; Key words; Latent Dirichlet allocation; Natural languages; Neural-networks; Neurocomputers; Semantic analysis; Python
Contrastive Language-knowledge Graph Pre-Training,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191603269&doi=10.1145%2f3644820&partnerID=40&md5=e88e2dad1b2e7bbe7001cc45767a095d,"Recent years have witnessed a surge of academic interest in knowledge-enhanced pre-Trained language models (PLMs) that incorporate factual knowledge to enhance knowledge-driven applications. Nevertheless, existing studies primarily focus on shallow, static, and separately pre-Trained entity embeddings, with few delving into the potential of deep contextualized knowledge representation for knowledge incorporation. Consequently, the performance gains of such models remain limited. In this article, we introduce a simple yet effective knowledge-enhanced model, College (Contrastive Language-Knowledge Graph Pre-Training), which leverages contrastive learning to incorporate factual knowledge into PLMs. This approach maintains the knowledge in its original graph structure to provide the most available information and circumvents the issue of heterogeneous embedding fusion. Experimental results demonstrate that our approach achieves more effective results on several knowledge-intensive tasks compared to previous state-of-The-Art methods. Our code and trained models are available at https://github.com/Stacy027/COLLEGE.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLanguage Model; Contrastive Learning; Knowledge Graph,Graph embeddings; Additional key word and phraseslanguage model; Contextualized knowledge; Contrastive learning; Embeddings; Factual knowledge; Key words; Knowledge graphs; Knowledge-representation; Language model; Pre-training; Knowledge graph
"Enriching Urdu NER with BERT Embedding, Data Augmentation, and Hybrid Encoder-CNN Architecture",2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191596051&doi=10.1145%2f3648362&partnerID=40&md5=5aa8f8b795eb444d2e7ddda4ecb1e62d,"Named Entity Recognition (NER) is an indispensable component of Natural Language Processing (NLP), which aims to identify and classify entities within text data. While Deep Learning (DL) models have excelled in NER for well-resourced languages such as English, Spanish, and Chinese, they face significant hurdles when dealing with low-resource languages such as Urdu. These challenges stem from the intricate linguistic characteristics of Urdu, including morphological diversity, a context-dependent lexicon, and the scarcity of training data. This study addresses these issues by focusing on Urdu Named Entity Recognition (U-NER) and introducing three key contributions. First, various pre-Trained embedding methods are employed, encompassing Word2vec (W2V), GloVe, FastText, Bidirectional Encoder Representations from Transformers (BERT), and Embeddings from language models (ELMo). In particular, fine-Tuning is performed on BERTBASE and ELMo using Urdu Wikipedia and news articles. Second, a novel generative Data Augmentation (DA) technique replaces Named Entities (NEs) with mask tokens, employing pre-Trained masked language models to predict masked tokens, effectively expanding the training dataset. Finally, the study introduces a novel hybrid model combining a Transformer Encoder with a Convolutional Neural Network (CNN) to capture the intricate morphology of Urdu. These modules enable the model to handle polysemy, extract short-and long-range dependencies, and enhance learning capacity. Empirical experiments demonstrate that the proposed model, incorporating BERT embeddings and an innovative DA approach, attains the highest F1-score of 93.99%, highlighting its efficacy for the U-NER task.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesUrdu; Asian languages; low-resource languages; Named Entity Recognition,Character recognition; Computational linguistics; Convolutional neural networks; Deep learning; Natural language processing systems; Signal encoding; Additional key word and phrasesurdu; Asian languages; Convolutional neural network; Data augmentation; Embeddings; Key words; Language model; Low resource languages; Named entity recognition; Neural network architecture; Embeddings
Unsupervised Multimodal Machine Translation for Low-resource Distant Language Pairs,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190500126&doi=10.1145%2f3652161&partnerID=40&md5=d553b883a239c8abab9d9b9eae4ccee7,"Unsupervised machine translation (UMT) has recently attracted more attention from researchers, enabling models to translate when languages lack parallel corpora. However, the current works mainly consider close language pairs (e.g., English-German and English-French), and the effectiveness of visual content for distant language pairs has yet to be investigated. This article proposes an unsupervised multimodal machine translation model for low-resource distant language pairs. Specifically, we first employ adequate measures such as transliteration and re-ordering to bring distant language pairs closer together. We then use visual content to extend masked language modeling and generate visual masked language modeling for UMT. Finally, empirical experiments are conducted on our distant language pair dataset and the public Multi30k dataset. Experimental results demonstrate the superior performance of our model, with BLEU score improvements of 2.5 and 2.6 on translation for distant language pairs English-Uyghur and Chinese-Uyghur. Moreover, our model also brings remarkable results for close language pairs, improving 2.3 BLEU compared with the existing models in English-German.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesVisual masked language modeling; distant language pair; image feature; unsupervised machine translation,Computational linguistics; Computer aided language translation; Modeling languages; Natural language processing systems; Visual languages; Additional key word and phrasesvisual masked language modeling; Distant language pair; Image features; Key words; Language model; Language pairs; Machine translations; Multi-modal; Unsupervised machine translation; Visual content; Machine translation
A Study for Enhancing Low-resource Thai-Myanmar-English Neural Machine Translation,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191607114&doi=10.1145%2f3645111&partnerID=40&md5=a6b0de1c7167896ea797a4bcdd7bbdc4,"Several methodologies have recently been proposed to enhance the performance of low-resource Neural Machine Translation (NMT). However, these techniques have yet to be explored thoroughly in the low-resource Thai and Myanmar languages. Therefore, we first applied augmentation techniques such as SwitchOut and Ciphertext Based Data Augmentation (CipherDAug) to improve NMT performance in these languages. Second, we enhanced the NMT performance by fine-Tuning the pre-Trained Multilingual Denoising BART model (mBART), where BART denotes Bidirectional and Auto-Regressive Transformer. We implemented three NMT systems: namely, Transformer+SwitchOut, Multi-Source Transformer+CipherDAug, and fine-Tuned mBART in the bidirectional translations of Thai-English-Myanmar language pairs from the ASEAN-MT corpus. Experimental results showed that Multi-Source Transformer+CipherDAug significantly improved Bilingual Evaluation Understudy (BLEU), Character n-gram F-score (ChrF), and Translation Error Rate (TER) scores over the first baseline Transformer and second baseline Edit-Based Transformer. The model achieved notable BLEU scores: 37.9 (English-To-Thai), 42.7 (Thai-To-English), 28.9 (English-To-Myanmar), 31.2 (Myanmar-To-English), 25.3 (Thai-To-Myanmar), and 25.5 (Myanmar-To-Thai). The fine-Tuned mBART model also considerably outperformed the two baselines, except for the Myanmar-To-English pair. SwitchOut improved over the second baseline in all pairs and performed similarly to the first baseline in most cases. Last, we performed detailed analyses verifying that the CipherDAug and mBART models potentially facilitate improving low-resource NMT performance in Thai and Myanmar languages.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLow-resource; data augmentation; fine-Tuning pre-Trained model; linguistic features; neural machine translation,Computational linguistics; Computer aided language translation; Additional key word and phraseslow-resource; Ciphertexts; Data augmentation; De-noising; Fine tuning; Fine-tuning pre-trained model; Key words; Linguistic features; Myanmars; Performance; Neural machine translation
Sentiment Analysis Method of Epidemic-related Microblog Based on Hesitation Theory,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191601891&doi=10.1145%2f3648360&partnerID=40&md5=7f4e8463a9a6c9571ba6d4ee96834d8f,"The COVID-19 pandemic in 2020 brought an unprecedented global crisis. After two years of control efforts, life gradually returned to the pre-pandemic state, but localized outbreaks continued to occur. Toward the end of 2022, COVID-19 resurged in China, leading to another disruption of people's lives and work. Many pieces of information on social media reflected people's views and emotions toward the second outbreak, which showed distinct differences compared to the first outbreak in 2020. To explore people's emotional attitudes toward the pandemic at different stages and the underlying reasons, this study collected microblog data from November 2022 to January 2023 and from January to June 2020, encompassing Chinese reactions to the COVID-19 pandemic. Based on hesitancy and the Fuzzy Intuition theory, we proposed a hypothesis: hesitancy can be integrated into machine learning models to select suitable corpora for training, which not only improves accuracy but also enhances model efficiency. Based on this hypothesis, we designed a hesitancy-integrated model. The experimental results demonstrated the model's positive performance on a self-constructed database. By applying this model to analyze people's attitudes toward the pandemic, we obtained their sentiments in different months. We found that the most negative emotions appeared at the beginning of the pandemic, followed by emotional fluctuations influenced by social events, ultimately showing an overall positive trend. Combining word cloud techniques and the Latent Dirichlet Allocation (LDA) model effectively helped explore the reasons behind the changes in pandemic attitude.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCOVID-19; fuzzy intuition theory; hesitancy; machine learning; sentiment analysis,Fuzzy sets; Machine learning; Sentiment analysis; Statistics; Additional key word and phrasescovid-19; Analysis method; Control effort; Fuzzy intuition theory; Hesitancy; Key words; Localised; Machine-learning; Micro-blog; Sentiment analysis; COVID-19
Cross-Domain Aspect-Based Sentiment Classification with a Pre-Training and Fine-Tuning Strategy for Low-Resource Domains,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191592431&doi=10.1145%2f3653299&partnerID=40&md5=f7338bb4f87bfc111a5fcffb33a758eb,"Aspect-based sentiment classification (ABSC) is a crucial sub-Task of fine-grained sentiment analysis, which aims to predict the sentiment polarity of the given aspects in a sentence as positive, negative, or neutral. Most existing ABSC methods are based on supervised learning. However, these methods rely heavily on fine-grained labeled training data, which can be scarce in low-resource domains, limiting their effectiveness. To overcome this challenge, we propose a low-resource cross-domain aspect-based sentiment classification (CDABSC) approach based on a pre-Training and fine-Tuning strategy. This approach applies the pre-Training and fine-Tuning strategy to an advanced deep learning method designed for ABSC, namely the attention-based encoding graph convolutional network (AEGCN) model. Specifically, a high-resource domain is selected as the source domain, and the AEGCN model is pre-Trained using a large amount of fine-grained annotated data from the source domain. The optimal parameters of the model are preserved. Subsequently, a low-resource domain is used as the target domain, and the pre-Trained model parameters are used as the initial parameters of the target domain model. The target domain is fine-Tuned using a small amount of annotated data to adapt the parameters to the target domain model, improving the accuracy of sentiment classification in the low-resource domain. Finally, experimental validation on two domain benchmark datasets, restaurant and laptop, demonstrates significant outperformance of our approach over the baselines in CDABSC Micro-F1.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCross-domain aspect-based sentiment classification; domain adaption; pre-Training and fine-Tuning; transfer learning,Deep learning; Learning systems; Transfer learning; Additional key word and phrasescross-domain aspect-based sentiment classification; Domain adaptions; Fine tuning; Key words; Pre-training; Pre-training and fine-tuning; Resource domains; Sentiment classification; Target domain; Transfer learning; Sentiment analysis
Medical Question Summarization with Entity-driven Contrastive Learning,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191607577&doi=10.1145%2f3652160&partnerID=40&md5=c26c3f9714af509042b9817a03de9f29,"By summarizing longer consumer health questions into shorter and essential ones, medical question-Answering systems can more accurately understand consumer intentions and retrieve suitable answers. However, medical question summarization is very challenging due to obvious distinctions in health trouble descriptions from patients and doctors. Although deep learning has been applied to successfully address the medical question summarization (MQS) task, two challenges remain: how to correctly capture question focus to model its semantic intention, and how to obtain reliable datasets to fairly evaluate performance. To address these challenges, this article proposes a novel medical question summarization framework based on entity-driven contrastive learning (ECL). ECL employs medical entities present in frequently asked questions (FAQs) as focuses and devises an effective mechanism to generate hard negative samples. This approach compels models to focus on essential information and consequently generate more accurate question summaries. Furthermore, we have discovered that some MQS datasets, such as the iCliniq dataset with a 33% duplicate rate, have significant data leakage issues. To ensure an impartial evaluation of the related methods, this article carefully examines leaked samples to reorganize more reasonable datasets. Extensive experiments demonstrate that our ECL method outperforms the existing methods and achieves new state-of-The-Art performance, i.e., 52.85, 43.16, 41.31, 43.52 in terms of ROUGE-1 metric on MeQSum, CHQ-Summ, iCliniq, HealthCareMagic dataset, respectively. The code and datasets are available at https://github.com/yrbobo/MQS-ECL  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMedical question summarization; contrastive learning; hard negative samples; medical entity; question focus,Deep learning; Additional key word and phrasesmedical question summarization; Consumer healths; Contrastive learning; Hard negative sample; Key words; Medical entity; Medical question answering; Negative samples; Question answering systems; Question focus; Semantics
Boundary-Aware Abstractive Summarization with Entity-Augmented Attention for Enhancing Faithfulness,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191610393&doi=10.1145%2f3641278&partnerID=40&md5=8555b3ad0a66672b16415ee61d2c960f,"With the successful application of deep learning, document summarization systems can produce more readable results. However, abstractive summarization still suffers from unfaithful outputs and factual errors, especially in named entities. Current approaches tend to employ external knowledge to improve model performance while neglecting the boundary information and the semantics of the entities. In this article, we propose an entity-augmented method (EAM) to encourage the model to make full use of the entity boundary information and pay more attention to the critical entities. Experimental results on three Chinese and English summarization datasets show that our method outperforms several strong baselines and achieves state-of-the-art performance on the CLTS dataset. Our method can also improve the faithfulness of the summary and generalize well to different pre-trained language models. Moreover, we propose a method to evaluate the integrity of generated entities. Besides, we adapt the data augmentation method in the FactCC model according to the difference between Chinese and English in grammar and train a new evaluation model for factual consistency evaluation in Chinese summarization.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Abstractive text summarization; entity-augmented; factual consistency,Deep learning; Natural language processing systems; 'current; Abstractive text summarization; Boundary information; Document summarization; Entity-augmented; External knowledge; Factual consistency; Named entities; Summarization systems; Text Summarisation; Semantics
Leveraging Bidirectionl LSTM with CRFs for Pashto Tagging,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191548490&doi=10.1145%2f3649456&partnerID=40&md5=70bbd7f068e12e8667183e92bfe1d7ce,"Part-of-speech tagging plays a vital role in text processing and natural language understanding. Very few attempts have been made in the past for tagging Pashto Part-of-Speech. In this work, we present a Long Short-Term Memory-based approach for Pashto part-of-speech tagging with special focus on ambiguity resolution. Initially, we created a corpus of Pashto sentences having words with multiple meanings and their tags. We introduce a powerful sentences representation and new architecture for Pashto text processing. The accuracy of the proposed approach is compared with state-of-The-Art Hidden Markov Model. Our Model shows 87.60% accuracy for all words excluding punctuation and 95.45% for ambiguous words; however, Hidden Markov Model shows 78.37% and 44.72% accuracy, respectively. Results show that our approach outperforms Hidden Markov Model in Part-of-Speech tagging for Pashto text.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLSTM; CRF; tagging; word embedding,Computational linguistics; Long short-term memory; Natural language processing systems; Syntactics; Text processing; Additional key word and phraseslstm; CRF; Embeddings; Hidden-Markov models; Key words; Part of speech tagging; Parts-of-speech tagging; Tagging; Text processing languages; Word embedding; Hidden Markov models
Towards Mental Health Analysis in Social Media for Low-resourced Languages,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188348643&doi=10.1145%2f3638761&partnerID=40&md5=b1aeca0e6e7115729f4a110e99b09563,"The surge in internet use for expression of personal thoughts and beliefs has made it increasingly feasible for the social Natural Language Processing (NLP) research community to find and validate associations between social media posts and mental health status. Cross-sectional and longitudinal studies of low-resourced social media data bring to fore the importance of real-time responsible Artificial Intelligence (AI) models for mental health analysis in native languages. Aiming at classifying research for social computing and tracking advances in the development of learning-based models, we propose a comprehensive survey on mental health analysis for social media and posit the need of analyzing low-resourced social media data for mental health. We first classify three components for computing on social media as: SM- data mining/natural language processing on social media, IA- integrated applications with social media data and user-network modeling, and NM- user and network modeling on social networks. To this end, we posit the need of mental health analysis in different languages of East Asia (e.g., Chinese, Japanese, Korean), South Asia (Hindi, Bengali, Tamil), Southeast Asia (Malay, Thai, Vietnamese), European languages (Spanish, French) and the Middle East (Arabic). Our comprehensive study examines available resources and recent advances in low-resourced languages for different aspects of SM, IA, and NM to discover new frontiers as potential field of research. © 2024 Copyright held by the owner/author(s).",Computational analysis; low-resourced languages; mental health; social media,Modeling languages; Natural language processing systems; Social networking (online); Social sciences computing; Computational analysis; Internet use; Language processing; Low-resourced language; Mental health; Natural languages; Network models; Research communities; Social media; Social media datum; Data mining
Disambiguation of Isolated Manipuri Tonal Contrast Word Pairs Using Acoustic Features,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188304149&doi=10.1145%2f3643830&partnerID=40&md5=030cad428a829270f39753cca2ce9d27,"Manipuri is a low-resource, Tibeto-Burman tonal language spoken mainly in Manipur, a northeastern state of India. Tone identification is crucial to speech comprehension for tonal languages, where tone defines the word’s meaning. Automatic Speech Recognition for those languages can perform better by including tonal information from a powerful tone detection system. While significant research has been conducted on tonal languages like Mandarin, Thai, Cantonese, and Vietnamese, a notable gap exists in exploring Manipuri within this context. To address this gap, this work expands our previously developed handcrafted speech corpus, ManiTo, which comprises isolated Manipuri tonal contrast word pairs to study the tones of Manipuri. This extension includes contributions from 20 native speakers. Preliminary findings have confirmed that Manipuri has two unique tones, Falling and Level. The study then conducts a comprehensive acoustic feature analysis. Two sets of features based on Pitch contours, Jitter, and Shimmer measurements are investigated to distinguish the two tones of Manipuri. Support Vector Machine, Long Short-term Memory, Random Forest, and k-Nearest Neighbors are the classifiers adopted to validate the selected feature sets. The results indicate that the second set of features consistently outperformed the first set, demonstrating higher accuracy, particularly when utilizing the Random Forest classifier, which provides valuable insights for further advancements in speech recognition technology for low-resource tonal language Manipuri. © 2024 Copyright held by the owner/author(s).",jitter; low-resource; manipuri; pitch; shimmer; Tonal language; tone detection,Forestry; Nearest neighbor search; Speech recognition; Support vector machines; Acoustic features; Automatic speech recognition; Low-resource; Manipuri; Pitch; Sets of features; Shimmer; Tonal languages; Tone detection; Word-pairs; Jitter
Arabic Sentiment Analysis for ChatGPT Using Machine Learning Classification Algorithms: A Hyperparameter Optimization Technique,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188317856&doi=10.1145%2f3638285&partnerID=40&md5=2d546fed8e45581e10ae5a2498a19774,"In the realm of ChatGPT’s language capabilities, exploring Arabic Sentiment Analysis emerges as a crucial research focus. This study centers on ChatGPT, a popular machine learning model engaging in dialogues with users, garnering attention for its exceptional performance and widespread impact, particularly in the Arab world. The objective is to assess people’s opinions about ChatGPT, categorizing them as positive or negative. Despite abundant research in English, there is a notable gap in Arabic studies. We assembled a dataset from X (formerly known as Twitter), comprising 2,247 tweets, classified by Arabic language specialists. Employing various machine learning algorithms, including Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF), and Naïve Bayes (NB), we implemented hyperparameter optimization techniques such as Bayesian optimization, Grid Search, and random search to select the best hyperparameters that contribute to achieving the best performance. Through training and testing, performance enhancements were observed with optimization algorithms. SVM exhibited superior performance, achieving 90% accuracy, 88% precision, 95% recall, and 91% F1 score with Grid Search. These findings contribute valuable insights into ChatGPT’s impact in the Arab world, offering a comprehensive understanding of sentiment analysis through machine learning methodologies. © 2024 Copyright held by the owner/author(s).",Arabic sentiment analysis; ChatGPT; hyperparameter tuning; machine learning; optimization,Forestry; Learning systems; Logistic regression; Random forests; Support vector regression; Arabic sentiment analyse; ChatGPT; Hyper-parameter; Hyper-parameter optimizations; Hyperparameter tuning; Machine-learning; Optimisations; Optimization techniques; Performance; Sentiment analysis; Sentiment analysis
An Expert System for Indian Sign Language Recognition Using Spatial Attention–based Feature and Temporal Feature,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188313842&doi=10.1145%2f3643824&partnerID=40&md5=903ade91395ca6c8e449847bd99ebb9a,"Sign Language (SL) is the only means of communication for the hearing-impaired people. Normal people have difficulty understanding SL, resulting in a communication barrier between hearing impaired people and hearing community. However, the Sign Language Recognition System (SLRS) has helped to bridge the communication gap. Many SLRs are proposed for recognizing SL; however, a limited number of works are reported for Indian Sign Language (ISL). Most of the existing SLRS focus on global features other than the Region of Interest (ROI). Focusing more on the hand region and extracting local features from the ROI improves system accuracy. The attention mechanism is a widely used technique for emphasizing the ROI. However, only a few SLRS used the attention method. They employed the Convolution Block Attention Module and temporal attention but Spatial Attention (SA) is not utilized in previous SLRS. Therefore, a novel SA based SLRS named Spatial Attention-based Sign Language Recognition Module (SASLRM) is proposed to recognize ISL words for emergency situations. SASLRM recognizes ISL words by combining convolution features from a pretrained VGG-19 model and attention features from a SA module. The proposed model accomplished an average accuracy of 95.627% on the ISL dataset. The proposed SASLRM is further validated on LSA64, WLASL, and Cambridge Hand Gesture Recognition datasets where, the proposed model reached an accuracy of 97.84%, 98.86%, and 98.22%, respectively. The results indicate the effectiveness of the proposed SLRS in comparison with the existing SLRS. © 2024 Copyright held by the owner/author(s).",dynamic gestures; Indian sign language; spatial attention; transfer learning,Audition; Expert systems; Image segmentation; Learning systems; Dynamic gesture; Hearing impaired; Indian sign languages; Recognition systems; Region-of-interest; Regions of interest; Sign language; Sign Language recognition; Spatial attention; Transfer learning; Convolution
Modeling a Novel Approach for Emotion Recognition Using Learning and Natural Language Processing,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188290201&doi=10.1145%2f3641851&partnerID=40&md5=4517c158171134e624d70aa482900575,"Various facts, including politics, entertainment, industry, and research fields, are connected to analyzing the audience’s emotions. Sentiment Analysis (SA) is a Natural Language Processing (NLP) concept that uses statistical and lexical forms as well as learning techniques to forecast how different types of content in social media will express the audience’s neutral, positive, and negative emotions. There is lack of an adequate tool to quantify the characteristics and independent text for assessing the primary audience emotion from the available online social media dataset. The focus of this research is on modeling a cutting-edge method for decoding the connectivity among social media texts and assessing audience emotions. Here, a novel dense layer graph model (DLG-TF) for textual feature analysis is used to analyze the relevant connectedness inside the complex media environment to forecast emotions. The information from the social media dataset is extracted using some popular convolution network models, and the predictions are made by examining the textual properties. The experimental results show that, when compared to different standard emotions, the proposed DLG-TF model accurately predicts a greater number of possible emotions. The macro-average of baseline is 58%, the affective is 55%, the crawl is 55%, and the ultra-dense is 59%, respectively. The feature analysis comparison of baseline, affective, crawl, ultra-dense and DLG-TF using the unsupervised model based on EmoTweet gives the precision, recall, and F1-score of the anticipated model are explained. The micro- and macro-average based on these parameters are compared and analyzed. The macro-average of baseline is 47%, the affective is 46%, the crawl is 50%, and the ultra-dense is 85%, respectively. It makes precise predictions using the social media dataset that is readily available. A few criteria, including accuracy, recall, precision, and F-measure, are assessed and contrasted with alternative methods. © 2024 Copyright held by the owner/author(s).",complex network; emotion recognition; feature representation; prediction; Social media,Complex networks; Emotion Recognition; Learning algorithms; Learning systems; Modeling languages; Social networking (online); Speech recognition; Emotion recognition; Entertainment industry; Feature analysis; Feature representation; Language processing; Learning languages; Natural languages; Research fields; Sentiment analysis; Social media; Forecasting
Improved Regression Analysis with Ensemble Pipeline Approach for Applications across Multiple Domains,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188351513&doi=10.1145%2f3645110&partnerID=40&md5=05d3988ed57ac46391320b3cca7af319,"In this research, we introduce two new machine learning regression methods: the Ensemble Average and the Pipelined Model. These methods aim to enhance traditional regression analysis for predictive tasks and have undergone thorough evaluation across three datasets, Kaggle House Price, Boston House Price, and California Housing, using various performance metrics. The results consistently show that our models outperform existing methods in terms of accuracy and reliability across all three datasets. The Pipelined Model, in particular, is notable for its ability to combine predictions from multiple models, leading to higher accuracy and impressive scalability. This scalability allows for their application in diverse fields like technology, finance, and healthcare. Furthermore, these models can be adapted for real-time and streaming data analysis, making them valuable for applications such as fraud detection, stock market prediction, and IoT sensor data analysis. Enhancements to the models also make them suitable for big data applications, ensuring their relevance for large datasets and distributed computing environments. It is important to acknowledge some limitations of our models, including potential data biases, specific assumptions, increased complexity, and challenges related to interpretability when using them in practical scenarios. Nevertheless, these innovations advance predictive modeling, and our comprehensive evaluation underscores their potential to provide increased accuracy and reliability across a wide range of applications. The results indicate that the proposed models outperform existing models in terms of accuracy and robustness for all three datasets. © 2024 Copyright held by the owner/author(s).",Ensemble average; light gradient boost; pipeline scheme; random forest; XgBoost,Data handling; Housing; Information analysis; Large datasets; Learning systems; Pipelines; Regression analysis; Ensemble averages; House's prices; Light gradient boost; Light gradients; Machine-learning; Multiple domains; Pipeline scheme; Random forests; Regression method; Xgboost; Scalability
PAMR: Persian Abstract Meaning Representation Corpus,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188280597&doi=10.1145%2f3638288&partnerID=40&md5=45e8e4d2993601eebb1bb18cc5a2cac1,"One of the most used and well-known semantic representation models is Abstract Meaning Representation (AMR). This representation has had numerous applications in natural language processing tasks in recent years. Currently, for English and Chinese languages, large annotated corpora are available. In addition, in some low-resource languages, related corpora have been generated with less size; although, until now, to the best of our knowledge, there is not any AMR corpus for the Persian/Farsi language. Therefore, the aim of this article is to create a Persian AMR (PAMR) corpus via translating English sentences and adjusting AMR guidelines and to solve the various challenges that are faced in this regard. The result of this research is a corpus, containing 1,020 Persian sentences and their related AMR that can be used in various natural language processing tasks. In this article, to investigate the feasibility of using the corpus, we have applied it to two natural language processing tasks: Sentiment Analysis and Text Summarization. © 2024 Copyright held by the owner/author(s).",Abstract Meaning Representation; corpus; low-resource language; natural language processing; Persian; text,Abstracting; Semantics; Abstract meaning representation; Corpus; Language processing; Low resource languages; Natural language processing; Natural languages; Persians; Representation model; Semantic representation; Text; Sentiment analysis
SEEUNRS: Semantically Enriched Entity-Based Urdu News Recommendation System,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188327563&doi=10.1145%2f3639049&partnerID=40&md5=145679ce5912a42bc731c06b7a89a924,"The advancement in the production, distribution, and consumption of news has fostered easy access to the news with fair challenges. The main challenge is to present the right news to the right audience. The news recommendation system is one of the technological solutions to this problem. Much work has been done on news recommendation systems for the major languages of the world, but trivial work has been done for resource-poor languages like Urdu. Another significant hurdle in the development of an efficient news recommendation system is the scarcity of an accessible and suitable Urdu dataset. To this end, an Urdu news mobile application was used to collect the news data and user feedback for 1 month. After refinement, the first-ever Urdu dataset of 100 users and 23,250 news was curated for the Urdu news recommendation system. In addition, SEEUNRS, a semantically enriched entity-based Urdu news recommendation system, is proposed. The proposed scheme exploits the hidden features of a news article and entities to suggest the right article to the right audience. Results have shown that the presented model has an improvement of 6.9% in the F1 measure from traditional recommendation system techniques. © 2024 Copyright held by the owner/author(s).",deep learning; named entity; Urdu dataset; Urdu news recommendation system,Deep learning; Deep learning; Mobile applications; Named entities; News articles; News recommendation; Production distribution; Technological solution; Urdu dataset; Urdu news recommendation system; User feedback; Recommender systems
Improved BIO-Based Chinese Automatic Abstract-Generation Model,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188288088&doi=10.1145%2f3643695&partnerID=40&md5=9f1d83362397610ac68abae9b73ac9e7,"With its unique information-filtering function, text summarization technology has become a significant aspect of search engines and question-and-answer systems. However, existing models that include the copy mechanism often lack the ability to extract important fragments, resulting in generated content that suffers from thematic deviation and insufficient generalization. Specifically, Chinese automatic summarization using traditional generation methods often loses semantics because of its reliance on word lists. To address these issues, we proposed the novel BioCopy mechanism for the summarization task. By training the tags of predictive words and reducing the probability distribution range on the glossary, we enhanced the ability to generate continuous segments, which effectively solves the above problems. Additionally, we applied reinforced canonicality to the inputs to obtain better model results, making the model share the sub-network weight parameters and sparsing the model output to reduce the search space for model prediction. To further improve the model’s performance, we calculated the bilingual evaluation understudy (BLEU) score on the English dataset CNN/DailyMail to filter the thresholds and reduce the difficulty of word separation and the dependence of the output on the word list. We fully fine-tuned the model using the LCSTS dataset for the Chinese summarization task and conducted small-sample experiments using the CSL dataset. We also conducted ablation experiments on the Chinese dataset. The experimental results demonstrate that the optimized model can learn the semantic representation of the original text better than other models and performs well with small sample sizes. © 2024 Copyright held by the owner/author(s).",Abstract summarization; copy mechanism; multilingual; pre-train model,Abstracting; Probability distributions; Search engines; Semantics; Abstract summarization; Copy mechanism; Filtering functions; Generalisation; Multilingual; Pre-train model; Question and answer system; Text Summarisation; Train model; Word lists; Information filtering
Multi-view Image Fusion Using Ensemble Deep Learning Algorithm For MRI And CT Images,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188347633&doi=10.1145%2f3640811&partnerID=40&md5=895bd700c0f37b9de8f8c15ae13ccf62,"Medical image fusions are crucial elements in image-based health care diagnostics or therapies and generic applications of computer visions. However, the majority of existing methods suffer from noise distortion that affects the overall output. When pictures are distorted by noises, classical fusion techniques perform badly. Hence, fusion techniques that properly maintain information comprehensively from multiple faulty pictures need to be created. This work presents Enhanced Lion Swarm Optimization (ESLO) with Ensemble Deep Learning (EDL) to address the aforementioned issues. The primary steps in this study include image fusions, segmentation, noise reduction, feature extraction, picture classification, and feature selection. Adaptive Median Filters are first used for noise removal in sequence to enhance image quality by eliminating noises. The MRIs and CT images are then segmented using the Region Growing–based k-Means Clustering (RKMC) algorithm to separate the images into their component regions or objects. Images in black and white are divided into image. In the white image, the RKMC algorithm successfully considered the earlier tumour probability. The next step is feature extraction, which is accomplished by using the Modified Principal Component Analysis (MPCA) to draw out the most informative aspects of the images. Then the ELSO algorithm is applied for optimal feature selection, which is computed by best fitness values. After that, multi-view image fusions of multi modal images derive lower-, middle-, and higher-level image contents. It is done by using Deep Convolution Neural Network (DCNN) and the Tissue-Aware Conditional Generative Adversarial Network (TAcGAN) algorithm, which fuses the multi-view features and relevant image features, and it is used for real-time applications. ELSO +EDL algorithm gives better results in terms of accuracy, Peak Signal-To-Noise Ratio (PSNR), and lower Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE) when compared to other existing algorithms. © 2024 Copyright held by the owner/author(s).",Computed Tomography (CT) images; Enhanced Lion Swarm Optimization (ELSO); Ensemble Deep Learning (EDL) algorithm; Magnetic Resonance Imaging (MRI) images; Multi-view image fusions,Adaptive filtering; Adaptive filters; Biomedical signal processing; Computerized tomography; Deep learning; Diagnosis; Extraction; Feature extraction; Generative adversarial networks; Image enhancement; Image fusion; Image segmentation; K-means clustering; Learning algorithms; Learning systems; Mean square error; Median filters; Medical imaging; Particle swarm optimization (PSO); Principal component analysis; Signal to noise ratio; Computed tomography images; Enhanced lion swarm optimization; Ensemble deep learning  algorithm; Features extraction; Fusion techniques; Magnetic resonance imaging  image; Multi-view image; Multi-view image fusion; Region growing; Swarm optimization; Magnetic resonance imaging
Consensus-Based Machine Translation for Code-Mixed Texts,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188328580&doi=10.1145%2f3628427&partnerID=40&md5=c75e2c50b0bd84d7945b68bbb8850ddd,"Multilingualism in India is widespread due to its long history of foreign acquaintances. This leads to the presence of an audience familiar with conversing using more than one language. Additionally, due to the social media boom, the usage of multiple languages to communicate has become extensive. Hence, the need for a translation system that can serve the novice and monolingual user is the need of the hour. Such translation systems can be developed by methods such as statistical machine translation and neural machine translation, where each approach has its advantages as well as disadvantages. In addition, the parallel corpus needed to build a translation system, with code-mixed data, is not readily available. In the present work, we present two translation frameworks that can leverage the individual advantages of these pre-existing approaches by building an ensemble model that takes a consensus of the final outputs of the preceding approaches and generates the target output. The developed models were used for translating English-Bengali code-mixed data (written in Roman script) into their equivalent monolingual Bengali instances. A code-mixed to monolingual parallel corpus was also developed to train the preceding systems. Empirical results show improved BLEU and TER scores of 17.23 and 53.18 and 19.12 and 51.29, respectively, for the developed frameworks. © 2024 Copyright held by the owner/author(s).",code-mixed; consensus; neural machine translation; neural network; parallel corpus; Phrase-based machine translation,Computational linguistics; Computer aided language translation; Network coding; Bengalis; Code-mixed; Consensus; Machine translations; Mixed data; Multilingualism; Neural-networks; Parallel corpora; Phrase-based machine translations; Translation systems; Neural machine translation
CodeKGC: Code Language Model for Generative Knowledge Graph Construction,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188341236&doi=10.1145%2f3641850&partnerID=40&md5=c5028943448e9842683253ad8d8fa3c1,"Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provide intermediate steps, thereby improving knowledge extraction abilities. Experimental results indicate that the proposed approach can obtain better performance on benchmark datasets compared with baselines. © 2024 Copyright held by the owner/author(s).",code; Knowledge graph construction; language model,Benchmarking; Computational linguistics; Semantics; Specification languages; 'current; Code; Code languages; Graph construction; Knowledge graph construction; Knowledge graphs; Language model; Natural languages; Performance; Structural knowledge; Knowledge graph
Gender Classification System Based on the Behavioral Biometric Modality: Application of Handwritten Text,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188352922&doi=10.1145%2f3626236&partnerID=40&md5=dd60f50b5ff91143f256941e07e4a990,"Forensic Science is a branch of science that deals with the discovery, examination, and analysis of strong elements or evidence involved in the criminal justice system. It involves the use of scientific methods to investigate crimes. The Gender Classification System is closely linked to forensic studies, specifically investigating individuals through their handwriting, known as Behavioral Biometrics. Biometric systems rely on behavioral and physiological traits such as brain-prints, fingerprints, handwritten text, speech, facial attributes, gait information, palm vein patterns, hand geometry, electrocardiograms (ECGs), and more. Gender classification is an intriguing and important aspect within the field of pattern recognition and machine learning. It involves a binary problem of classifying individuals as either male or female. Analyzing the differences in femininity and masculinity behaviors can contribute to the evaluation of biometric-based identification systems. Gender classification has numerous forensic applications, including crime identification, demographic research, forgery detection, security, and surveillance. The main objective of this article is to present the latest survey findings on the gender classification system based on handwritten text, specifically the behavioral biometric modality. It includes an overview of the state-of-the-art work, the general framework, approaches, biometric modalities, and critical analysis. The article concludes with a critical analysis, discussion of open issues, concluding remarks, and future perspectives. © 2024 Copyright held by the owner/author(s).",biometric system; Gender classification; handwriting recognition; machine learning,Biometrics; Character recognition; Crime; Crystallinity; Electrocardiograms; Forensic science; Text processing; Behavioural Biometric; Biometric systems; Classification system; Criminal justice system; Critical analysis; Gender classification; Handwriting recognition; Handwritten texts; Machine-learning; Scientific method; Machine learning
SCT: Summary Caption Technique for Retrieving Relevant Images in Alignment with Multimodal Abstractive Summary,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188355596&doi=10.1145%2f3645029&partnerID=40&md5=50395e963cd8a02a9a8a190e609c51af,"This work proposes an efficient Summary Caption Technique that considers the multimodal summary and image captions as input to retrieve the correspondence images from the captions that are highly influential to the multimodal summary. Matching a multimodal summary with an appropriate image is a challenging task in computer vision and natural language processing. Merging in these fields is tedious, though the research community has steadily focused on cross-modal retrieval. These issues include the visual question-answering, matching queries with the images, and semantic relationship matching between two modalities for retrieving the corresponding image. Relevant works consider questions to match the relationship of visual information and object detection and to match the text with visual information and employing structural-level representation to align the images with the text. However, these techniques are primarily focused on retrieving the images to text or for image captioning. But less effort has been spent on retrieving relevant images for the multimodal summary. Hence, our proposed technique extracts and merge features in the Hybrid Image Text layer and captions in the semantic embeddings with word2vec where the contextual features and semantic relationships are compared and matched with each vector between the modalities, with cosine semantic similarity. In cross-modal retrieval, we achieve top five related images and align the relevant images to the multimodal summary that achieves the highest cosine score among the retrieved images. The model has been trained with seq-to-seq modal with 100 epochs, besides reducing the information loss by the sparse categorical cross entropy. Further, experimenting with the multimodal summarization with multimodal output dataset, in cross-modal retrieval, helps to evaluate the quality of image alignment with an image-precision metric that demonstrate the best results. © 2024 Copyright held by the owner/author(s).",Cross-modal retrieval; embedding; fusion; global; local features,Natural language processing systems; Object detection; Semantics; Cross-modal; Cross-modal retrieval; Embeddings; Global; Image caption; Local feature; Matchings; Multi-modal; Semantic relationships; Visual information; Embeddings
Seq2Set2Seq: A Two-stage Disentangled Method for Reply Keyword Generation in Social Media,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188338881&doi=10.1145%2f3644074&partnerID=40&md5=55386569481377f1478474a8352bc51c,"Social media produces large amounts of content every day. How to predict the potential influences of the contents from a social reply feedback perspective is a key issue that has not been explored. Thus, we propose a novel task named reply keywords prediction in social media, which aims to predict the keywords in the potential replies in as many aspects as possible. One prerequisite challenge is that the accessible social media datasets labeling such keywords remain absent. To solve this issue, we propose a new dataset,1 to study the reply keywords prediction in social media. This task could be seen as a single-turn dialogue keywords prediction for open-domain dialogue system. However, existing methods for dialogue keywords prediction cannot be adopted directly, which has two main drawbacks. First, they do not provide an explicit mechanism to model topic complementarity between keywords which is crucial in social media to controllably model all aspects of replies. Second, the collocations of keywords are not explicitly modeled, which also makes it less controllable to optimize for fine-grained prediction since the context information is much less than that in dialogue. To address these issues, we propose a two-stage disentangled framework, which can optimize the complementarity and collocation explicitly in a disentangled fashion. In the first stage, we use a sequence-to-set paradigm via multi-label prediction and determinantal point processes, to generate a set of keyword seeds satisfying the complementarity. In the second stage, we adopt a set-to-sequence paradigm via seq2seq model with the keywords seeds guidance from the set, to generate the more-fine-grained keywords with collocation. Experiments show that this method can generate not only a more diverse set of keywords but also more relevant and consistent keywords. Furthermore, the keywords obtained based on this method can achieve better reply generation results in the retrieval-based system than others. © 2024 Copyright held by the owner/author(s).",determinantal point processes; keyword prediction; multi-label classification; Social media reply; text generation,Classification (of information); Social networking (online); Speech processing; Text processing; Determinantal point process; Key Issues; Keyword prediction; Large amounts; Multi-label classifications; Novel task; Point process; Social media; Social medium reply; Text generations; Forecasting
DSISA: A New Neural Machine Translation Combining Dependency Weight and Neighbors,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186209408&doi=10.1145%2f3638762&partnerID=40&md5=df8bfb6df5baec7c9c7d090d58c9b1d9,"Most of the previous neural machine translations (NMT) rely on parallel corpus. Integrating explicitly prior syntactic structure information can improve the neural machine translation. In this article, we propose a Syntax Induced Self-Attention (SISA) which explores the influence of dependence relation between words through the attention mechanism and fine-tunes the attention allocation of the sentence through the obtained dependency weight. We present a new model, Double Syntax Induced Self-Attention (DSISA), which fuses the features extracted by SISA and a compact convolution neural network (CNN). SISA can alleviate long dependency in sentence, while CNN captures the limited context based on neighbors. DSISA utilizes two different neural networks to extract different features for richer semantic representation and replaces the first layer of Transformer encoder. DSISA not only makes use of the global feature of tokens in sentences but also the local feature formed with adjacent tokens. Finally, we perform simulation experiments that verify the performance of the new model on standard corpora. © 2024 Association for Computing Machinery. All rights reserved.",convolution neural network; dependency relation; Neural machine translation; transformer,Computer aided language translation; Convolution; Multilayer neural networks; Neural machine translation; Semantics; Syntactics; Attention mechanisms; Context-based; Convolution neural network; Dependence relation; Dependency relation; Network capture; Parallel corpora; Structure information; Syntactic structure; Transformer; Computational linguistics
Hypergraph Neural Network for Emotion Recognition in Conversations,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186261545&doi=10.1145%2f3638760&partnerID=40&md5=437721a0d83b16801650fb3f3c9320a6,"Modeling conversational context is an essential step for emotion recognition in conversations. Existing works still suffer from insufficient utilization of local context information and remote context information. This article designs a hypergraph neural network, namely HNN-ERC, to better utilize local and remote contextual information. HNN-ERC combines the recurrent neural network with the conventional hypergraph neural network to strengthen connections between utterances and make each utterance receive information from other utterances better. The proposed model has empirically achieved state-of-the-art results on three benchmark datasets, demonstrating the effectiveness and superiority of the new model. © 2024 Association for Computing Machinery. All rights reserved.",emotion recognition in conversations; graph convolution neural network; Hypergraph convolution neural network,Convolution; Graph neural networks; Recurrent neural networks; Speech recognition; Context information; Convolution neural network; Emotion recognition; Emotion recognition in conversation; Graph convolution neural network; Hyper graph; Hypergraph convolution neural network; Local contexts; Neural-networks; Emotion Recognition
Few-shot Incremental Event Detection,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186203438&doi=10.1145%2f3634747&partnerID=40&md5=03bf48c7d47e4cd32bef0d850feed006,"Event detection tasks can enable the quick detection of events from texts and provide powerful support for downstream natural language processing tasks. Most such methods can only detect a fixed set of predefined event classes. To extend them to detect a new class without losing the ability to detect old classes requires costly retraining of the model from scratch. Incremental learning can effectively solve this problem, but it requires abundant data of new classes. In practice, however, the lack of high-quality labeled data of new event classes makes it difficult to obtain enough data for model training. To address the above mentioned issues, we define a new task, few-shot incremental event detection, which focuses on learning to detect a new event class with limited data, while retaining the ability to detect old classes to the extent possible. We created a benchmark dataset IFSED for the few-shot incremental event detection task based on FewEvent and propose two benchmarks, IFSED-K and IFSED-KP. Experimental results show that our approach has a higher F1-score than baseline methods and is more stable. © 2024 Association for Computing Machinery. All rights reserved.",Event detection; Few-shot; Incremental learning,Detection tasks; Down-stream; Event class; Events detection; Few-shot; Fixed sets; Incremental learning; Language processing; Natural languages; Quickest detection; Natural language processing systems
Dual-Branch Multitask Fusion Network for Offline Chinese Writer Identification,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186203058&doi=10.1145%2f3638554&partnerID=40&md5=d1c677cc66fff4e2481d3a7293489049,"Chinese characters are complex and contain discriminative information, meaning that their writers have the potential to be recognized using less text. In this study, offline Chinese writer identification based on a single character was investigated. To extract comprehensive features to model Chinese characters, explicit and implicit information as well as global and local features are of interest. A dual-branch multitask fusion network is proposed that contains two branches for global and local feature extraction simultaneously, and introduces auxiliary tasks to help the main task. Content recognition, stroke number estimation, and stroke recognition are considered as three auxiliary tasks for explicit information. The main task extracts implicit information of writer identity. The experimental results validated the positive influences of auxiliary tasks on the writer identification task, with the stroke number estimation task being most helpful. In-depth research was conducted to investigate the influencing factors in Chinese writer identification, with respect to character complexity, stroke importance, and character number, which provides a systematic reference for the actual application of neural networks in Chinese writer identification. © 2024 Association for Computing Machinery. All rights reserved.",Chinese language; Multitask learning; Writer identification,Complex networks; Learning systems; Chinese characters; Chinese language; Explicit information; Global feature; Implicit informations; Main tasks; Multitask learning; Offline; Strokes number; Writer identification; Character recognition
Leveraging Dual Gloss Encoders in Chinese Biomedical Entity Linking,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186244474&doi=10.1145%2f3638555&partnerID=40&md5=0ceb7e6d579d639e6a656d515351bee5,"Entity linking is the task of assigning a unique identity to named entities mentioned in a text, a sort of word sense disambiguation that focuses on automatically determining a pre-defined sense for a target entity to be disambiguated. This study proposes the DGE (Dual Gloss Encoders) model for Chinese entity linking in the biomedical domain. We separately model a dual encoder architecture, comprising a context-aware gloss encoder and a lexical gloss encoder, for contextualized embedding representations. DGE are then jointly optimized to assign the nearest gloss with the highest score for target entity disambiguation. The experimental datasets consist of a total of 10,218 sentences that were manually annotated with glosses defined in the BabelNet 5.0 across 40 distinct biomedical entities. Experimental results show that the DGE model achieved an F1-score of 97.81, outperforming other existing methods. A series of model analyses indicate that the proposed approach is effective for Chinese biomedical entity linking. © 2024 Association for Computing Machinery. All rights reserved.",biomedical informatics; language transformers; lexical semantics; natural language understanding; Word sense disambiguation,Bioinformatics; Medical informatics; Natural language processing systems; Semantics; Biomedical domain; Biomedical informatics; Context-Aware; Embeddings; Encoder architecture; Language transformer; Lexical semantics; Named entities; Natural language understanding; Word Sense Disambiguation; Signal encoding
A Relation Embedding Assistance Networks for Multi-hop Question Answering,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186218787&doi=10.1145%2f3635114&partnerID=40&md5=366ba16eada5bb47297f21e0fd23ebba,"Multi-hop Knowledge Graph Question Answering aims at finding an entity to answer natural language questions from knowledge graphs. When humans perform multi-hop reasoning, people tend to focus on specific relations across different hops and confirm the next entity. Therefore, most algorithms choose the wrong specific relation, which makes the system deviate from the correct reasoning path. The specific relation at each hop plays an important role in multi-hop question answering. Existing work mainly relies on the question representation as relation information, which cannot accurately calculate the specific relation distribution. In this article, we propose an interpretable assistance framework that fully utilizes the relation embeddings to assist in calculating relation distributions at each hop. Moreover, we employ the fusion attention mechanism to ensure the integrity of relation information and hence to enrich the relation embeddings. The experimental results on three English datasets and one Chinese dataset demonstrate that our method significantly outperforms all baselines. The source code of REAN will be available at https://github.com/2399240664/REAN © 2024 Association for Computing Machinery. All rights reserved.",interpretable framework; Knowledge graphs; multi-hop reasoning; question answering,Graph embeddings; Natural language processing systems; Attention mechanisms; Interpretable framework; Knowledge graphs; Multi-hop reasoning; Multi-hops; Natural language questions; Question Answering; Relation embedding; Relation information; Source codes; Knowledge graph
Multi-granularity Knowledge Sharing in Low-resource Neural Machine Translation,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186223235&doi=10.1145%2f3639930&partnerID=40&md5=e6db92facd1d382ee511a559a3240acb,"As the rapid development of deep learning methods, neural machine translation (NMT) has attracted more and more attention in recent years. However, lack of bilingual resources decreases the performance of the low-resource NMT model seriously. To overcome this problem, several studies put their efforts on knowledge transfer from high-resource language pairs to low-resource language pairs. However, these methods usually focus on one single granularity of language and the parameter sharing among different granularities in NMT is not well studied. In this article, we propose to improve the parameter sharing in low-resource NMT by introducing multi-granularity knowledge such as word, phrase and sentence. This knowledge can be monolingual and bilingual. We build the knowledge sharing model for low-resource NMT based on a multi-task learning framework, three auxiliary tasks such as syntax parsing, cross-lingual named entity recognition, and natural language generation are selected for the low-resource NMT. Experimental results show that the proposed method consistently outperforms six strong baseline systems on several low-resource language pairs. © 2024 Association for Computing Machinery. All rights reserved.","learning, parameter sharing; multi-granularity knowledge; multi-task; Neural machine translation","Computational linguistics; Computer aided language translation; Knowledge management; Learning algorithms; Learning systems; Natural language processing systems; Neural machine translation; Syntactics; Knowledge-sharing; Language pairs; Learning methods; Learning parameters; Learning, parameter sharing; Low resource languages; Multi tasks; Multi-granularity; Multi-granularity knowledge; Parameter sharing; Deep learning"
Autoregressive Feature Extraction with Topic Modeling for Aspect-based Sentiment Analysis of Arabic as a Low-resource Language,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186212972&doi=10.1145%2f3638050&partnerID=40&md5=64672b4560895cc95dc39b7e2d2e2b14,"This paper proposes an approach for aspect-based sentiment analysis of Arabic social data, especially the considerable text corpus generated through communications on X (formerly known as Twitter) for expressing opinions in Arabic-language tweets during the COVID-19 pandemic. The proposed approach examines the performance of several pre-trained predictive and autoregressive language models; namely, Bidirectional Encoder Representations from Transformers (BERT) and XLNet, along with topic modeling algorithms; namely, Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), for aspect-based sentiment analysis of online Arabic text. In addition, Bidirectional Long Short Term Memory (Bi-LSTM) deep learning model is used to classify the extracted aspects from online reviews. Obtained experimental results indicate that the combined XLNet-NMF model outperforms other implemented state-of-the-art methods through improving the feature extraction of unstructured social media text with achieving values of 0.946 and 0.938, for average sentiment classification accuracy and F-measure, respectively. © 2024 Association for Computing Machinery. All rights reserved.",Arabic; COVID-19; feature extraction; latent dirichlet allocation (LDA); low-resource language; non-negative matrix factorization (NMF); Sentiment analysis; topic modeling; X (formerly known as Twitter); XLNet,Extraction; Feature extraction; Long short-term memory; Matrix algebra; Modeling languages; Non-negative matrix factorization; Sentiment analysis; Social networking (online); Statistics; Arabic; Features extraction; Latent Dirichlet allocation; Low resource languages; Non-negative matrix factorization; Nonnegative matrix factorization; Sentiment analysis; Topic Modeling; X (formerly known as twitter); XLNet; COVID-19
Transliteration Characteristics in Romanized Assamese Language Social Media Text and Machine Transliteration,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186192401&doi=10.1145%2f3639565&partnerID=40&md5=05c10fd5f07edc9da61c4a1148eb97c1,"This article aims to understand different transliteration behaviors of Romanized Assamese text on social media. Assamese, a language that belongs to the Indo-Aryan language family, is also among the 22 scheduled languages in India. With the increasing popularity of social media in India and also the common use of the English Qwerty keyboard, Indian users on social media express themselves in their native languages, but using the Roman/Latin script. Unlike some other popular South Asian languages (say Pinyin for Chinese), Indian languages do not have a common standard romanization convention for writing on social media platforms. Assamese and English are two very different orthographical languages. Thus, considering both orthographic and phonemic characteristics of the language, this study tries to explain how Assamese vowels, vowel diacritics, and consonants are represented in Roman transliterated form. From a dataset of romanized Assamese social media texts collected from three popular social media sites: (Facebook, YouTube, and X (formerly known as Twitter)),1 we have manually labeled them with their native Assamese script. A comparison analysis is also carried out between the transliterated Assamese social media texts with six different Assamese romanization schemes that reflect how Assamese users on social media do not adhere to any fixed romanization scheme. We have built three separate character-level transliteration models from our dataset. One using a traditional phrase-based statistical machine transliteration model, (1) PBSMT model and two separate neural transliteration models, (2) BiLSTM neural seq2seq model with attention, and (3) Neural transformer model. A thorough error analysis has been performed on the transliteration result obtained from the three state-of-the-art models mentioned above. This may help to build a more robust machine transliteration system for the Assamese social media domain in the future. Finally, an attention analysis experiment is also carried out with the help of attention weight scores taken from the character-level BiLSTM neural seq2seq transliteration model built from our dataset. © 2024 Association for Computing Machinery. All rights reserved.",attention; BiLSTM; grapheme; PBSMT; phoneme; transformer; Transliteration,Linguistics; Attention; BiLSTM; Grapheme; Machine transliteration; PBSMT; Phoneme; Social media; Transformer; Transliteration; Transliteration models; Social networking (online)
Ibn-Ginni: An Improved Morphological Analyzer for Arabic,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186175007&doi=10.1145%2f3639050&partnerID=40&md5=52652ae59b6a28b0dfe7d15bcd69bff1,"Arabic is a morphologically rich language, which means that the Arabic language has a complicated system of word formation and structure. The affixes in the Arabic language (i.e., prefixes and suffixes) can be added to root words to generate different meanings and grammatical functions. These affixes can indicate aspects such as tense, gender, number, case, person, and more. In addition, the meaning and function of words can be modified in Arabic using an internal structure known as morphological patterns. Computational morphological analyzers of Arabic are vital to developing Arabic language processing toolkits. In this article, we introduce a new morphological analyzer (Ibn-Ginni) that inherits the speed and quality of the Buckwalter Arabic Morphological Analyzer (BAMA). The BAMA has poor coverage of the classical Arabic language. Hence, the coverage of classical Arabic is improved by using the Alkhalil analyzer. Although it is slow, it was used to generate a huge number of solutions for 3 million unique Arabic words collected from different resources. These word form-based solutions were converted to stem-based solutions, refined manually, and added to the database of BAMA, resulting in substantial improvements in the quality of the analysis. Hence, Ibn-Ginni is a hybrid system between BAMA and Alkhalil analyzers and may be considered an efficient large-scale analyzer. The Ibn-Ginni analyzer analyzed 0.6 million more words than the BAMA analyzer. Therefore, our analyzer significantly improves the coverage of the Arabic language. Besides, the Ibn-Ginni analyzer is high speed at providing solutions, the average time to analyze a word is 0.3 ms. Using a corpus designed for benchmarking Arabic morphological analyzers, our analyzer was able to find all solutions for 72.72% of the words. Moreover, the analyzer did not provide all possible morphological solutions for 24.24% of the words. The analyzer and its morphological database are publicly available on GitHub.1 © 2024 Association for Computing Machinery. All rights reserved.",affixation; alkhalil; arabic lemmatization; arabic morphology; arabic stemming; buckwalter morphological analyzer (BAMA); stem-based morphology; The arabic morphological analyzers; wordform-based morphology,Hybrid systems; Natural language processing systems; Quality control; Affixation; Alkhalil; Arabic lemmatization; Arabic morphology; Arabic stemming; Buckwalter morphological analyzer; Lemmatization; Morphological analyzer; Stem-based morphology; The arabic morphological analyzer; Wordform-based morphology; Morphology
Improving the Detection of Multilingual South African Abusive Language via Skip-gram Using Joint Multilevel Domain Adaptation,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186261974&doi=10.1145%2f3638759&partnerID=40&md5=f0b0dffc40b02eb8493e235536c50e79,"The distinctiveness and sparsity of low-resource multilingual South African abusive language necessitate the development of a novel solution to automatically detect different classes of abusive language instances using machine learning. Skip-gram has been used to address sparsity in machine learning classification problems but is inadequate in detecting South African abusive language due to the considerable amount of rare features and class imbalance. Joint Domain Adaptation has been used to enlarge features of a low-resource target domain for improved classification outcomes by jointly learning from the target domain and large-resource source domain. This article, therefore, builds a Skip-gram model based on Joint Domain Adaptation to improve the detection of multilingual South African abusive language. Contrary to the existing Joint Domain Adaptation approaches, a Joint Multilevel Domain Adaptation model involving adaptation of monolingual source domain instances and multilingual target domain instances with high frequency of rare features was executed at the first level and adaptation of target-domain features and first-level features at the next level. Both surface-level and embedding word features were used to evaluate the proposed model. In the evaluation of surface-level features, the Joint Multilevel Domain Adaptation model outperformed the state-of-the-art models with accuracy of 0.92 and F1-score of 0.68. In the evaluation of embedding features, the proposed model outperformed the state-of-the-art models with accuracy of 0.88 and F1-score of 0.64. The Joint Multilevel Domain Adaptation model significantly improved the average information gain of the rare features in different language categories and reduced class imbalance. © 2024 Association for Computing Machinery. All rights reserved.",Joint Domain Adaptation; Skip-gram; South African abusive language; Text classification,Classification (of information); Machine learning; Text processing; Adaptation models; Class imbalance; Domain adaptation; Embeddings; Joint domain adaptation; Multilevels; Skip-gram; South african abusive language; Target domain; Text classification; Embeddings
Deep Learning-based POS Tagger and Chunker for Odia Language Using Pre-trained Transformers,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186189973&doi=10.1145%2f3637877&partnerID=40&md5=6db320f73fa37083f54c5e0b488a534e,"Developing effective natural language processing (NLP) tools for low-resourced languages poses significant challenges. This article centers its attention on the task of Part-of-speech (POS) tagging and chunking, which pertains to the identification and categorization of linguistic units within sentences. POS tagging and Chunking have already produced positive results in English and other European languages. However, in Indian languages, particularly in Odia language, it is not yet well explored because of the lack of supporting tools, resources, and its complex linguistic morphology. This study presents the building of a manually annotated dataset for Odia phrase chunking task and the development of a deep learning-based model specifically tailored to accommodate the distinctive properties of the language. The process of annotating the Odia chunking corpus involved the utilization of inside-outside-begin labels, which were tagged by using designed Odia chunking tagset. We utilize the constructed Odia chunking dataset to build Odia chunker based on deep learning techniques, employing state-of-the-art architectures. Various techniques, such as Recurrent Neural Networks, Convolutional Neural Networks, and transformer-based models, are investigated to determine the most effective approach for Odia POS tagging and chunking. In addition, we conduct experiments utilizing diverse input representations, including Odia word embeddings, character-level representations, and sub-word units, to effectively capture the complex linguistic characteristics of the Odia language. Numerous experiments are conducted that evaluate the performance of our Odia POS tagger and chunker, employing standard evaluation metrics and making comparisons with existing approaches. The results demonstrate that our transformer-based tagger and chunker achieves superior accuracy and robustness in identifying and categorizing linguistic POS tags and chunks within Odia sentences. It outperforms existing work and exhibits consistent performance across diverse linguistic contexts and sentence structures. The developed Odia POS tagger and chunker have enormous potential for a variety of NLP applications, including information extraction, syntactic parsing, and machine translation, all of which are tailored to the low-resource Odia language. This work contributes to developing NLP tools and technologies for low-resource languages, thereby facilitating enhanced language processing capabilities in various linguistic contexts. © 2024 Association for Computing Machinery. All rights reserved.",chunking; deep learning; low-resource language; Part of speech (POS); transformers,Complex networks; Computational linguistics; Convolutional neural networks; Learning systems; Natural language processing systems; Recurrent neural networks; Chunking; Deep learning; Low resource languages; Part of speech; Part of speech tagging; Part-of-speech tagger; Part-of-speech tags; Parts-of-speech tagging; Transformer; Syntactics
An Ensemble Strategy with Gradient Conflict for Multi-Domain Neural Machine Translation,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186172507&doi=10.1145%2f3638248&partnerID=40&md5=175e633eddf60cca94b7582f29befb92,"Multi-domain neural machine translation aims to construct a unified neural machine translation model to translate sentences across various domains. Nevertheless, previous studies have one limitation is the incapacity to acquire both domain-general and domain-specific representations concurrently. To this end, we propose an ensemble strategy with gradient conflict for multi-domain neural machine translation that automatically learns model parameters by identifying both domain-shared and domain-specific features. Specifically, our approach consists of (1) a parameter-sharing framework, where the parameters of all the layers are originally shared and equivalent to each domain, and (2) ensemble strategy, in which we design an Extra Ensemble strategy via a piecewise condition function to learn direction and distance-based gradient conflict. In addition, we give a detailed theoretical analysis of the gradient conflict to further validate the effectiveness of our approach. Experimental results on two multi-domain datasets show the superior performance of our proposed model compared to previous work. © 2024 Association for Computing Machinery. All rights reserved.",domain-specific; gradient conflict; Multi-domain neural machine translation,Computational linguistics; Computer aided language translation; Domain specific; Ensemble strategies; Gradient conflict; Learn+; Machine translation models; Modeling parameters; Multi-domain neural machine translation; Multi-domains; Parameter sharing; Piece-wise; Neural machine translation
A Machine Learning–Based Readability Model for Gujarati Texts,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186225798&doi=10.1145%2f3637826&partnerID=40&md5=c760552e13226ba7ca8b09a3f1720c98,"This study aims to develop a machine learning–based model to predict the readability of Gujarati texts. The dataset was 50 prose passages from Gujarati literature. Fourteen lexical and syntactic readability text features were extracted from the dataset using a machine learning algorithm of the unigram parts of speech tagger and three Python programming scripts. Two samples of native Gujarati speaking secondary and higher education students rated the Gujarati texts for readability judgment on a 10-point scale of “easy” to “difficult” with the interrater agreement. After dimensionality reduction, seven text features as the independent variables and the mean readability rating as the dependent variable were used to train the readability model. As the students’ level of education and gender were related to their readability rating, four readability models for school students, university students, male students, and female students were trained with a backward stepwise multiple linear regression algorithm of supervised machine learning. The trained model is comparable across the raters’ groups. The best model is the university students’ readability rating model. The model is cross-validated. It explains 91% and 88% of the variance in readability ratings at training and cross-validation, respectively, and its effect size and power are large and high. © 2024 Association for Computing Machinery. All rights reserved.",Gujarati texts; interrater agreement; model comparison; Readability model; readability rating and level of education,Education computing; Learning algorithms; Learning systems; Students; Supervised learning; Gujarati text; Inter-rater agreements; Learning Based Models; Level of educations; Machine-learning; Models comparisons; Readability model; Readability rating and level of education; Text feature; University students; Linear regression
The Computational Method for Supporting Thai VerbNet Construction,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186177087&doi=10.1145%2f3638533&partnerID=40&md5=540c89705291617ae245a024ea61d900,"VerbNet is a lexical resource for verbs that has many applications in natural language processing tasks, especially ones that require information about both the syntactic behavior and the semantics of verbs. This article presents an attempt to construct the first version of a Thai VerbNet corpus via data enrichment of the existing lexical resource. This corpus contains the annotation at both the syntactic and semantic levels, where verbs are tagged with frames within the verb class hierarchy and their arguments are labeled with the semantic role. We discuss the technical aspect of the construction process of Thai VerbNet and survey different semantic role labeling methods to make this process fully automatic. We also investigate the linguistic aspect of the computed verb classes and the results show the potential in assisting semantic classification and analysis. At the current stage, we have built the verb class hierarchy consisting of 28 verb classes from 112 unique concept frames over 490 unique verbs using our association rule learning method on Thai verbs. © 2024 Association for Computing Machinery. All rights reserved.",association rule learning; semantic role classification; Thai verbnet; verb classification,Association rules; Classification (of information); Learning algorithms; Learning systems; Natural language processing systems; Semantics; Class hierarchies; Data enrichments; Language processing; Lexical resources; Natural languages; Semantic levels; Semantic role classifications; Thai verbnet; Verb classification; VerbNet; Syntactics
Explanation Guided Knowledge Distillation for Pre-trained Language Model Compression,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186258843&doi=10.1145%2f3639364&partnerID=40&md5=0e1e9a75b9713dd5c8a307df7acb5102,"Knowledge distillation is widely used in pre-trained language model compression, which can transfer knowledge from a cumbersome model to a lightweight one. Though knowledge distillation based model compression has achieved promising performance, we observe that explanations between the teacher model and the student model are not consistent. We argue that the student model should study not only the predictions of the teacher model but also the internal reasoning process. To this end, we propose Explanation Guided Knowledge Distillation (EGKD) in this article, which utilizes explanations to represent the thinking process and improve knowledge distillation. To obtain explanations in our distillation framework, we select three typical explanation methods rooted in different mechanisms, namely gradient-based, perturbation-based, and feature selection methods. Then, to improve computational efficiency, we propose different optimization strategies to utilize the explanations obtained by these three different explanation methods, which could provide the student model with better learning guidance. Experimental results on GLUE demonstrate that leveraging explanations can improve the performance of the student model. Moreover, our EGKD could also be applied to model compression with different architectures. © 2024 Association for Computing Machinery. All rights reserved.",Explanation; knowledge distillation; model compression,Computational efficiency; Computational linguistics; Students; Different mechanisms; Explanation; Internal reasoning process; Knowledge distillation; Language model; Model compression; Performance; Student Modeling; Teacher models; Thinking process; Distillation
User Interest Identification with Social Media Information using Natural Language and Meta-Heuristic Technique,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183770629&doi=10.1145%2f3579165&partnerID=40&md5=0a1536d2bc6eafd72f072374df3fdbd4,"As the number of Internet users and social networking apps has grown in recent years, interest-based recommendation systems have been more commonly used in practice. Given the vast quantity of data available from LinkedIn and Twitter, as well as the expanding number of users, it was critical to create a real-time framework for recommending and monitoring relevant tweets or posts based on the user's interests. Using association rules, the interests of social network users can be uncovered. A considerable number of association rules extracted from social networks were found to be mostly dependent on coverage requirements. After finding patterns of frequent and non-frequent patterns, a large number of non-frequent terms were eliminated in association rule mining. In order to reduce the complexity of the association rule mining process, the more relevant terms are selected by the Hybridized Competitive Swarm Optimizer and Gravitational Search Algorithm (CSO-GSA), which is utilized for association rule generation and classification using deep learning techniques. In this research, numerous relevant rules for human interest are identified. The numerical outcome of the proposed strategy is compared with existing state-of-the-art techniques. The proposed CSO with GSA outperforms the existing techniques. © 2024 Association for Computing Machinery. All rights reserved.",CSO and GSA; feature selection; Linkedin; natural language processing; optimization; Twitter; User interest; web crawling,Data mining; Deep learning; Feature Selection; Heuristic methods; Learning algorithms; Learning systems; Natural language processing systems; Optimization; Social networking (online); Web crawler; CSO and GSA; Features selection; Language processing; LinkedIn; Natural language processing; Natural languages; Optimisations; Twitter; Users' interests; Web Crawling; Association rules
Deep Ensemble Network for Sentiment Analysis in Bi-lingual Low-resource Languages,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180236639&doi=10.1145%2f3600229&partnerID=40&md5=62b6a01fbbf7607a303e51725654f1b4,"Sentiment analysis (SA) is the systematic identification, extraction, quantification, and study of affective states and subjective information using natural language processing. It is widely used for analyzing users' feedback, such as reviews or social posts. Recently, SA has been one of the favorite research domains in NLP due to their wide range of applications, including E-commerce, healthcare, hotel business, and others. Many machine learning and deep learning-based models exist to predict the sentiment of the user's post. However, the sentiment analysis in low-resource languages such as Kannada, Malayalam, Telugu, and Tamil received less attention due to language complexity and the low availability of required resources. This research fills the gap by proposing an ensemble model for predicting the sentiment of code-mixed Kannada and Malayalam languages. The ensemble of transformer-based models achieved a promising weighted F1-score of 0.66 for Kannada code-mixed language. In contrast, the ensemble model of the deep learning framework performed best by achieving a weighted F1-score of 0.72 for the Malayalam dataset, outperforming existing research. © 2024 Association for Computing Machinery. All rights reserved.",BERT; code-mixed; deep learning; ensemble learning; Kannada; machine learning; Malayalam; Sentiment analysis; transformer,Codes (symbols); Deep learning; Learning algorithms; Learning systems; Network coding; BERT; Code-mixed; Deep learning; Ensemble learning; Kannada; Low resource languages; Machine-learning; Malayalams; Sentiment analysis; Transformer; Sentiment analysis
Sentiment Analysis of Turkish Drug Reviews with Bidirectional Encoder Representations from Transformers,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183699701&doi=10.1145%2f3626523&partnerID=40&md5=897aae79531067b6a4392f03fa361292,"Sentiment analysis of user generated product or service reviews is significant to enhance quality. Healthcare related computational linguistics studies, particularly analysis of drug based user criticisms, have principal importance above all. Sentiment analysis of healthcare reviews reveal the relations between patients, doc- tors and healthcare services. More specifically, sentiment analysis of drug reviews may be used to acquire relations such as adverse drug reactions (ADRs), diagnosis-treatment assist, and personalized therapy recom- mendations. Most of the drug review sentiment studies are in English. Though Turkish is a widely spoken language, there is limited research conducted on medical domain and there is particularly no study related to drug review sentiment analysis. In this study, we generated a Turkish drug review dataset and we eval- uated the generated dataset in detail against (i) traditional machine learning algorithms with language pre- processing steps, stemming and feature selection, (ii) deep learning algorithms with word2vec embedding language model, and (iii) various bidirectional encoder representations from transformers (BERT) models in terms of sentiment analysis. The experiments show that neural transformers are promising in Turkish drug review sentiment identification. In particular, Turkish dedicated BERT (BERTurk) resulted in 95.1% weighted- F1 score as the best drug review sentiment prediction performance. © 2024 Association for Computing Machinery. All rights reserved.",bidirectional transformer; drug review; Turkish; word embedding,Deep learning; Diagnosis; Embeddings; Health care; Quality control; Signal encoding; Adverse drug reactions; Bidirectional transformer; Drug review; Embeddings; Healthcare services; Personalized therapies; Sentiment analysis; Turkishs; User-generated; Word embedding; Sentiment analysis
Semantic and Context Understanding for Sentiment Analysis in Hindi Handwritten Character Recognition Using a Multiresolution Technique,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183331132&doi=10.1145%2f3557895&partnerID=40&md5=195c02fbc0be58531c50765f5136f6b9,"The rapid growth ofWeb 2.0, which enables people to generate, communicate, and share information, has resulted in an increase in the total number of users. In developing countries, online users' sentiment influences decision-making, social views, individual consumption decisions, and entity quality monitoring. As a result, more accurate sentiment analysis, particularly in their native language such as Hindi, is preferred over crude binary categorization. This is because of the abundance of web-based data in Indian languages such as Hindi, Marathi, Kannada, Tamil, and so on. Analyzing this data and recovering valuable and relevant information from handwritten text has become extremely important. Despite years of research and development, no optical writing recognition (OCR) system has ever been certified as completely reliable. The first step in any pattern recognition system is feature selection. In many fields, feature selection is studied as a combinatorial optimization problem. The primary goal of feature selection is to reduce the number of redundant and ineffective traits in the recognition system. This feature selection is used to maintain or improve the performance of the classifier used by the recognition system: A support vectormachine (SVM) technique could be used to solve this character recognition problem. The Hindi character recognition system recognizes Hindi characters by employing morphological operations, edge detection, HOG feature extraction, and an SVM-based classifier. The proposed model outperformed the current state-of-the-art method, achieving an accuracy of 96.77%. © 2024 Association for Computing Machinery.",Datasets; gaze detection; neural networks; text tagging,Behavioral research; Decision making; Developing countries; Feature Selection; Mathematical morphology; Semantic Web; Support vector machines; Dataset; Features selection; Gaze detection; Handwritten character recognition; Multiresolution techniques; Neural-networks; Recognition systems; Sentiment analysis; SupportVector Machines; Text tagging; Sentiment analysis
A Systematic Review of Stemmers of Indian and Non-Indian Vernacular Languages,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183768786&doi=10.1145%2f3604612&partnerID=40&md5=02715b994d4493e3c78f9a61d0fec394,"The stemming process is crucial and significant in the pre-processing step of natural language processing. The stemmer oversees the stemming process. It facilitates the extraction of morphological variants of a root or base word from the provided word. Over the period, several stemmers for various vernacular languages have been proposed. However, very few research studies have comprehensively investigated these available stemmers. This article makes multifold contributions. First, we discuss the various stemmers of 15 Indian and 17 non-Indian languages describing their key points, benefits, and drawbacks. All the Indian languages for which stemmers have been built are covered in this study. For the non-Indian languages, stemmers of commonly spoken languages have been covered. Second, we present a language-wise comparative analysis of stemmers based on our identified parameters. Third, we discuss the wordnets and dictionaries available for different languages. Fourth, we provide details of the datasets available for various languages. Fifth, we also provide challenges in existing stemmers and future directions for future researchers. The study presented in this article reveals that significant research has been carried out for the stemmers of influential languages such as English, Arabic, and Urdu. On the other hand, languages with d resources, such as Farsi, Polish, Odia, Amharic, and others, have received the least attention for research. Moreover, rigorous analysis reveals that most of the stemmers suffer from over-stemming errors. With a complete catalogue of available stemmers, this study aims at assisting the researchers and professionals working in the areas such as information retrieval, semantic annotation, word meaning disambiguation, and ontology learning. © 2024 Association for Computing Machinery. All rights reserved.",dictionary-based stemmer; hybrid stemmer; Natural Language Processing (NLP); over-stemming error; rule-based stemmer; stemming; under-stemming error,Natural language processing systems; Ontology; Semantics; Dictionary-based stemmer; Hybrid stemmer; Language processing; Natural language processing; Natural languages; Over-stemming error; Rule based; Rule-based stemmer; Stemming; Under-stemming error; Errors
CodeFed: Federated Speech Recognition for Low-Resource Code-Switching Detection,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183331471&doi=10.1145%2f3571732&partnerID=40&md5=46868697ac5ae75122c39577eabe437b,"One common constraint in the practical application of speech recognition is Code Switching. The issue of code-switched languages is especially aggravated in the context of Indian languages -since most massively multilingual models are trained on corpora that are not representative of the diverse set of Indian languages. An associated constraint with such systems is the privacy-intrusive nature of the applications that aim to collate such representative data. To collectively mitigate both problems, this work presents CodeFed: A fed- erated learning-based code-switching detection model that can be deployed to collaboratively be trained by leveraging private data from multiple users, without compromising their privacy. Using a representative low- resource Indic dataset, we demonstrate the superior performance of a collaboratively trained global model that is trained using federated learning on three low-resource Indic languages -Gujarati, Tamil and Telugu and draw a comparison of the model with respect to the most current work in the field. Finally, to evaluate the practical realizability of the proposed system, CodeFed also discusses the system overview of the label generation architecture which may accompany CodeFed's possible real-time deployment.  © 2024 Association for Computing Machinery.",Code-switching; fed- erated learning; low resource Indian languages; mobile computing; speech processing,Data privacy; Learning systems; Speech processing; Code-switching; Detection models; Feed- erated learning; Global models; Indian languages; Low resource indian language; Mobile-computing; Multiple user; Performance; Private data; Speech recognition
Exploring Web-Based Translation Resources Applied to Hindi-English Cross-Lingual Information Retrieval,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158127573&doi=10.1145%2f3569010&partnerID=40&md5=7d1f7b83fa94b23c88346f8b97bacc6a,"Internet users perceive a multilingual web but are unfamiliar with it due to communication in their regional language called Cross-Lingual Information Retrieval (CLIR). In CLIR, a translation technique is used to translate the user queries into the target document's language. Conventional translation techniques are based on either a manual dictionary or a parallel corpus, whereas the trending Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) techniques are trained on a parallel corpus. NMT is not so mature for Hindi-English translation, according to the literature, and SMT performs better than the NMT. SMT provides a static translation due to the limited vocabularies in the available parallel corpus. It may not provide the translations for missing or unseen words, whereas the web provides a dynamic interface where multiple users are updating information at the same time. The web may provide the translations for missing or unseen words, and therefore the web is effectively used for technically developed languages like English, German, Spanish, Russian, and Chinese. In this article, different web resources such as Wikipedia, Hindi WordNet and Indo WordNet, ConceptNet, and online dictionary based translation techniques are proposed and applied to Hindi-English CLIR. Wikipedia-based translation approach incorporates three modules-exactly matched, partially matched, and disambiguation-to address the issues of wrong inter-wiki links, partially matched terms, and ambiguous articles. Hindi WordNet and Indo WorNet attribute ""English synset""and ConceptNet attributes ""Related term""& ""Synonymy""are used for obtaining translations. Further, WordNet path similarity is used to disambiguate translations. Various online dictionaries are available that return multiple relevant and irrelevant translations. The proposed approaches are compared to the SMT where the Wikipedia-based approach achieves approximately similar mean average precision to SMT. © 2024 Association for Computing Machinery.",ConceptNet; Hindi WordNet; Indo WordNet; online dictionary; Statistical Machine Translation; Web resources; Wikipedia,Computational linguistics; Information retrieval; Neural machine translation; Ontology; Speech transmission; ConceptNet; Cross-lingual information retrieval; Hindi wordnet; Indo wordnet; Online dictionaries; Statistical machine translation; Web resources; Wikipedia; Wordnet; Computer aided language translation
End-to-end Multi-modal Low-resourced Speech Keywords Recognition Using Sequential Conv2D Nets,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183695284&doi=10.1145%2f3606019&partnerID=40&md5=535a79fb60a078518e10d00eae3700d3,"Advanced Neural Networks are widely used to recognize multi-modal conversational speech with significant improvements in accuracy automatically. Significantly, Convolutional Neural sheets have retreated cuttingedge performance in Automatic Voice Recognition (AVR) recently more appropriately in English; however, the Hindi language has not been explored and examined well on AVR systems. The work in this article has exposed a three-layered two-dimensional Sequential Convolutional neural architecture. The Sequential Conv2D is an end-to-end system that can instantaneously exploit speech signal spectral and temporal structures. The network has been trained and tested on different cepstral features such as Frequency and Time variant-Mel-Filters, Gamma-tone Filter Cepstral Quantities, Bark-Filter band Coefficients, and Spectrogram features of speech structures. The experiment was performed on two low-resourced speech command datasets; Hindi with 27,145 Speech Keywords developed by TIFR and 23,664 (1-s utterances) of English speech commands by Google TensorFlow and AIY English Speech Commands. The experimental outcome showed that the model achieves significant performance of Convolutional layers trained on spectrograms with 91.60% accuracy, compared to that achieved in other cepstral feature labels for English speech. However, the model achieved an accuracy of 69.65% for Hindi audio words in which bark-frequency cepstral coefficients features outperformed spectrogram features. © 2024 Association for Computing Machinery. All rights reserved.",convolution layers; Neural networks; sequential; spectrogram; speech recognition,Multilayer neural networks; Network layers; Spectrographs; Speech recognition; Cepstral features; Conversational speech; Convolution layer; End to end; Multi-modal; Neural-networks; Performance; Sequential; Spectrograms; Speech commands; Convolution
Review of Machine and Deep Learning Techniques in Epileptic Seizure Detection using Physiological Signals and Sentiment Analysis,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183330509&doi=10.1145%2f3552512&partnerID=40&md5=6fe1325428aa82d4243e28c48c0d17c4,"Epilepsy is one of the significant neurological disorders affecting nearly 65 million people worldwide. The repeated seizure is characterized as epilepsy. Different algorithms were proposed for efficient seizure detection using intracranial and surface EEG signals. In the last decade, various machine learning techniques based on seizure detection approaches were proposed. This paper discusses different machine learning and deep learning techniques for seizure detection using intracranial and surface EEG signals. A wide range of machine learning techniques such as support vector machine (SVM) classifiers, artificial neural network (ANN) classifier, and deep learning techniques such as a convolutional neural network (CNN) classifier, and long-short term memory (LSTM) network for seizure detection are compared in this paper. The effectiveness of time-domain features, frequency domain features, and time-frequency domain features are discussed along with different machine learning techniques. Along with EEG, other physiological signals such as electrocardiogram are used to enhance seizure detection accuracy which are discussed in this paper. In recent years deep learning techniques based on seizure detection have found good classification accuracy. In this paper, an LSTM deep learning-network-based approach is implemented for seizure detection and compared with state-of-the-art methods. The LSTM based approach achieved 96.5% accuracy in seizure-nonseizure EEG signal classification. Apart from analyzing the physiological signals, sentiment analysis also has potential to detect seizures. Impact Statement- This review paper gives a summary of different research work related to epileptic seizure detection using machine learning and deep learning techniques. Manual seizure detection is time consuming and requires expertise. So the artificial intelligence techniques such as machine learning and deep learning techniques are used for automatic seizure detection. Different physiological signals are used for seizure detection. Different researchers are working on developing automatic seizure detection using EEG, ECG, accelerometer, and sentiment analysis. There is a need for a review paper that can discuss previous techniques and give further research direction. We have discussed different techniques for seizure detection with an accuracy comparison table. It can help the researcher to get an overview of both surface and intracranial EEG-based seizure detection approaches. The new researcher can easily compare different models and decide the model they want to start working on. A deep learning model is discussed to give a practical application of seizure detection. Sentiment analysis is another dimension of seizure detection and summarizing it will give a new prospective to the reader.  © 2023 Association for Computing Machinery.",ANN classifier; convolutional neural network classifier; EEG; epilepsy; LSTM network; seizure; sentiment analysis; SVM classifier,Biomedical signal processing; Convolution; Convolutional neural networks; Electrocardiography; Frequency domain analysis; Learning algorithms; Long short-term memory; Neurodegenerative diseases; Neurophysiology; Signal detection; Support vector machines; Artificial neural network classifiers; Convolutional neural network; Convolutional neural network classifier; Epilepsy; Long-short term memory network; Memory network; Neural networks classifiers; Seizure; Sentiment analysis; Support vector machine classifiers; Learning systems
Ontology-Based Natural Language Processing for Sentimental Knowledge Analysis Using Deep Learning Architectures,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183757563&doi=10.1145%2f3624012&partnerID=40&md5=d59dc2e7512a52100d1c97bd5b955f55,"When tested with popular datasets, sentiment categorization using deep learning (DL) algorithms will produce positive results. Building a corpus on novel themes to train machine learning methods in sentiment classification with high assurance, however, will be difficult. This study proposes a way for representing efficient features of a dataset into a word embedding layer of DL methods in sentiment classification known as KPRO (knowledge processing and representation based on ontology), a procedure to embed knowledge in the ontology of opinion datasets. This research proposes novel methods in ontology-based natural language processing utilizing feature extraction as well as classification by a DL technique. Here, input text has been taken as web ontology based text and is processed for word embedding. Then the feature mapping is carried out for this processed text using least square mapping in which the sentiment-based text has been mapped for feature extraction. The feature extraction is carried out using a Markov model based auto-feature encoder (MarMod-AuFeaEnCod). Extracted features are classified by utilizing hierarchical convolutional attention networks. Based on this classified output, the sentiment of the text obtained from web data has been analyzed. Results are carried out for Twitter and Facebook ontology-based sentimental analysis datasets in terms of accuracy, precision, recall, F-1 score, RMSE, and loss curve analysis. For the Twitter dataset, the proposed MarMod-AuFeaEnCod-HCAN attains an accuracy of 98%, precision of 95%, recall of 93%, F-1 score of 91%, RMSE of 88%, and loss curve of 70.2%. For Facebook, ontology web dataset analysis is also carried out with the same parameters in which the proposed MarMod-AuFeaEnCod-HCAN acquires accuracy of 96%, precision of 92%, recall of 94%, F-1 score of 91%, RMSE of 77%, and loss curve of 68.2%. © 2024 Association for Computing Machinery. All rights reserved.",classification; deep learning; feature extraction; KPRO; NLP; Ontology,Classification (of information); Data mining; Deep learning; Embeddings; Extraction; Mapping; Markov processes; Natural language processing systems; Ontology; Social networking (online); Deep learning; Embeddings; Features extraction; KPRO; Markov modeling; Model-based OPC; Ontology's; Ontology-based; Ontology-based natural language processing; Sentiment classification; Feature extraction
The Influence of Chinese Characters on Chinese Sign Language,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183756072&doi=10.1145%2f3591465&partnerID=40&md5=4598cf900f418c6dcad9c982b3023fdb,"Chinese Sign Language (CSL) and Chinese are languages used in the Chinese mainland. As a dominant language, Chinese has great influence on all levels of CSL. CSL, as a visual sign language, is fundamen- tally different from Chinese in linguistic structure. Unlike English, Chinese, as a pictograph, has influence on Chinese and CSL. This study explains in detail the influence of Chinese characters on CSL at the lexi- cal level, including many elements from Chinese, such as ""'Greek Text' fangzi"" (form imitating Chinese characters), ""'Greek Text' shukong"" (writing in the air with the index finger), loan translation, finger spelling, and mouthing patterns. This influence is not a simple borrowing of Chinese characters, but a creative imitation and adaptation according to the needs of sign language to express meaning. After a long period of evolution, the characteristics of Chinese characters are naturally integrated into CSL loanwords, which makes the relationship between sign language and Chinese characters closer. CSL borrows a large number of Chi- nese words, most of which are signs to express non-core concepts. These borrowed signs are an indis- pensable part of the CSL sign language family, enrich sign language vocabulary, improve the accuracy of sign language expression, and play a positive role in promoting the learning, work, and lives of deaf people. © 2024 Association for Computing Machinery. All rights reserved.",Chinese characters; Chinese Sign Language; fangzi; shukong,Linguistics; Chinese characters; Chinese mainland; Chinese sign language; Fangzi; Finger spelling; Index finger; Linguistic structure; Shukong; Sign language; Simple++; Visual languages
Emotional Intelligence Attention Unsupervised Learning Using Lexicon Analysis for Irony-based Advertising,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183728746&doi=10.1145%2f3580496&partnerID=40&md5=9fa593e7badc639c97456e0fdcc5586a,"Social media platforms have made increasing use of irony in recent years. Users can express their ironic thoughts with audio, video, and images attached to text content. When you use irony, you are making fun of a situation or trying to make a point. It can also express frustration or highlight the absurdity of a situation. The use of irony in social media is likely to continue to increase, no matter the reason. By using syntactic information in conjunction with semantic exploration, we show that attention networks can be enhanced. Using learned embedding, unsupervised learning encodes word order into a joint space. By evaluating the entropy of an example class and adding instances, the active learning method uses the shared representation as a query to retrieve semantically similar sentences from a knowledge base. In this way, the algorithm can identify the instance with the maximum uncertainty and extract the most informative example from the training set. An ironic network trained for each labelled record is used to train a classifier (model). The partial training model and the original labelled data generate pseudo-labels for the unlabeled data. To correctly predict the label of a dataset, a classifier (attention network) updates the pseudo-labels for the remaining datasets. After the experimental evaluation of the 1,021 annotated texts, the proposed model performed better than the baseline models, achieving an F1 score of 0.63 on ironic tasks and 0.59 on non-ironic tasks. We also found that the proposed model generalized well to new instances of datasets. © 2024 Association for Computing Machinery. All rights reserved.",Emotional Intelligence; Information retrieval; irony detection; Natural Language Processing; unsupervised learning,Classification (of information); Knowledge based systems; Learning algorithms; Learning systems; Natural language processing systems; Semantics; Social networking (online); Advertizing; Audio images; Audio videos; Emotional intelligence; Irony detection; Language processing; Lexicon analysis; Natural language processing; Natural languages; Social media platforms; Unsupervised learning
Depression Detection from Social Media Text Analysis using Natural Language Processing Techniques and Hybrid Deep Learning Model,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183321577&doi=10.1145%2f3569580&partnerID=40&md5=9406c82ab426713882dcc1e638f00ea3,"Depression is a kind of emotion that negatively impacts people's daily lives. The number of people suffering from long-term feelings is increasing every year across the globe. Depressed patients may engage in self-harm behaviors, which occasionally result in suicide. Many psychiatrists struggle to identify the presence of mental illness or negative emotion early to provide a better course of treatment before they reach a critical stage. One of the most challenging problems is detecting depression in people at the earliest possible stage. Researchers are using Natural Language Processing (NLP) techniques to analyze text content uploaded on social media, which helps to design approaches for detecting depression. This work analyses numerous prior studies that used learning techniques to identify depression. The existing methods suffer from better model representation problems to detect depression from the text with high accuracy. The present work addresses a solution to these problems by creating a new hybrid deep learning neural network design with better text representations called ""Fasttext Convolution Neural Network with Long Short-Term Memory (FCL).""In addition, this work utilizes the advantage of NLP to simplify the text analysis during the model development. The FCL model comprises fasttext embedding for better text representation considering out-of-vocabulary (OOV) with semantic information, a convolution neural network (CNN) architecture to extract global information, and Long Short-Term Memory (LSTM) architecture to extract local features with dependencies. The present work was implemented on real-world datasets utilized in the literature. The proposed technique provides better results than the state-of-the-art to detect depression with high accuracy.  © 2024 Association for Computing Machinery.",Deep learning; depression; Natural Language Processing; social media conversations,Brain; Convolution; Convolutional neural networks; Diseases; Learning algorithms; Learning systems; Memory architecture; Natural language processing systems; Network architecture; Semantics; Social networking (online); Deep learning; Depression; High-accuracy; Language processing; Language processing techniques; Natural language processing; Natural languages; Social media; Social medium conversation; Text analysis; Long short-term memory
Can You Understand Why I Am Crying? A Decision-making System for Classifying Infants' Cry Languages Based on DeepSVM Model,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183743904&doi=10.1145%2f3579032&partnerID=40&md5=63e5217a9c5a72fd38ba98f8f4187bef,"Scientific and therapeutic advances in perinatology and neonatology have improved the survival prospects of preterm and extremely-low-birth-weight infants. Infants' cries are a valuable noninvasive tool for monitoring their neurologic health, especially if they are premature. Automatic acoustic analysis and data mining are employed in this study to determine the discriminative features of preterm and full-term infant cries. The use of machine learning for recognizing sounds in a newborn's cry language has received less attention than previous methods for analyzing the sounds. Moreover, to extract appropriate features from infant cries, adequate knowledge and appropriate signal descriptors are required. Accordingly, to analyze infant cry language, we propose an approach that uses fractal descriptors to extract discriminant features from spectrograms of windowed signals, followed by iterative neighborhood component analysis (iNCA) to select appropriate features. Additionally, the improved deep support vector machine (DeepSVM) is used to classify the infants' crying types and their meanings. The proposed method is verified using a newborn sound dataset. According to the classification of five types of crying perception based on various characteristics, 98.34% of all crying perceptions have been recognized. Although there are many classes examined, the feature extraction method based on the fractal method and our optimal classification have a much higher diagnostic accuracy compared with similar methods for analyzing baby crying language. The proposed method can overcome many problems associated with analyzing babies' crying sounds and understanding their language, such as uncertainty and unusual errors in classification.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",DeepSVM; Infant's cry language; iterative neighborhood component analysis; signal descriptor; signal windowing; time-frequency domain,Data mining; Decision making; Feature Selection; Fractals; Frequency domain analysis; Iterative methods; Pediatrics; Uncertainty analysis; Decision-making systems; Deep support vector machine; Infant cry; Infant cry language; Iterative neighborhood component analyse; Neighborhood component analysis; Signal descriptors; Signal windowing; Support vectors machine; Time frequency domain; Support vector machines
Multi-Label Text Classification Model Based on Multi-Level Constraint Augmentation and Label Association Attention,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180422867&doi=10.1145%2f3586008&partnerID=40&md5=b92ceaf7d12e09aeba7a67418f272f6d,"In the multi-label text classification task, a text usually corresponds to multiple label categories, and the la- bels have correlation and hierarchical structure. However, when the label hierarchy is unknown, the number of various labels is not balanced, which makes it difficult for the model to classify low-frequency labels. In addition, labels have semantic similarities that make it difficult for the model to distinguish between them. In this article, we propose a multi-label text classification model based on multi-level constraint augmentation and label association attention. Compared with traditional methods, our method has two contributions: (1) In order to alleviate the problem of unbalanced number of different label categories and ensure the rationality of sample generation, we propose a data augmentation method based on multi-level constraints. In the process of sample generation, this method uses historical generation information, sample original text information, and sample topic to constrain the generated text. (2) In order to make the model recognize the associated labels accurately, we propose an interaction mechanism based on label association attention and filter gate. This method combines text information and label weight information. At the same time, our classification model considers the important weights of text sentences and effectively utilizes the co-occurrence relation- ship between labels. Experimental results on three benchmark datasets show that our model outperforms state-of-the-art methods on all main evaluation metrics, especially on low-frequency label prediction with sparse samples. © 2024 Association for Computing Machinery. All rights reserved.",data augmentation; label association attention; Multi-label text classification,Character recognition; Classification (of information); Semantics; Text processing; Classification tasks; Data augmentation; Label association attention; Lower frequencies; Model-based OPC; Multi-label text classification; Multilevels; Sample generations; Text classification models; Text information; Association reactions
Isolated Arabic Sign Language Recognition Using a Transformer-based Model and Landmark Keypoints,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183707176&doi=10.1145%2f3584984&partnerID=40&md5=223f258c16b2e4e0a4893c6e1bde3aa6,"Pose-based approaches for sign language recognition provide light-weight and fast models that can be adopted in real-time applications. This article presents a framework for isolated Arabic sign language recognition using hand and face keypoints. We employed MediaPipe pose estimator for extracting the keypoints of sign gestures in the video stream. Using the extracted keypoints, three models were proposed for sign language recognition: Long-Term Short Memory, Temporal Convolution Networks, and Transformer-based models. Moreover, we investigated the importance of non-manual features for sign language recognition systems and the obtained results showed that combining hand and face keypoints boosted the recognition accuracy by around 4% compared with only hand keypoints. The proposed models were evaluated on Arabic and Argentinian sign languages. Using the KArSL-100 dataset, the proposed pose-based Transformer achieved the highest accuracy of 99.74% and 68.2% in signer-dependent and -independent modes, respectively. Additionally, the Transformer was evaluated on the LSA64 dataset and obtained an accuracy of 98.25% and 91.09% in signer-dependent and -independent modes, respectively. Consequently, the pose-based Transformer outperformed the state-of-the-art techniques on both datasets using keypoints from the signer's hands and face. © 2024 Association for Computing Machinery. All rights reserved.",arabic sign language; gesture recognition; pose recognition; Sign language recognition; TCN; transformer,Palmprint recognition; Arabic sign language; FAST model; Gestures recognition; Independent mode; Keypoints; Light weight; Pose recognition; Sign Language recognition; TCN; Transformer; Gesture recognition
Ensemble Classifier for Hindi Hostile Content Detection,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183755960&doi=10.1145%2f3591353&partnerID=40&md5=d245ce063bf82ebaf135d873eb8f9cc0,"Detection of hostile content fromsocialmedia posts (Facebook, Twitter, etc.) is a demanding task in the field of Natural Language Processing. The increase of hostile content in different electronic media has opened up new challenges in language understanding. It becomes more difficult in regional languages. AI-based solutions are required to identify hostile content on a large scale. Although a satisfactory amount of research has been carried out in the English language, finding hostile content in regional languages is still under development due to the unavailability of suitable datasets and tools. In terms of the number of speakers, Hindi ranks third in the world and first on the Indian subcontinent. The objective of this article is to design a hostile content detection system in Hindi using coarse-grained (binary) classification and fine-grained (multi-class, multi-label) classification. We note that different baseline learning methods with different pre-trained language models perform differently. Using the Constraint 2021 Hindi Dataset, this research proposes a Bidirectional Encoder Representations from Transformers-(BERT) based contextual embedding technique with a concatenation of emoji2vec embeddings to classify social media posts in Hindi Devanagari script as hostile or non-hostile. Additionally, for the fine-grained tasks where hostile posts are sub-categorized as defamation, fake, hate, and offensive, we develop an ensemble classifier varying different learning methods and embedding models. With an F1-Score of 0.9721, it is found that our proposed Indic-BERT+emoji model outperforms the baseline model and other existing models for the coarse-grained task. We have also observed that our proposed ensemble method provides better results than the existing models and the baseline model for the fine-grained tasks with F1-Scores of 0.43, 0.82, 0.58, and 0.62 for the defamation, fake, hate, and offensive classes, respectively. © 2024 Association for Computing Machinery. All rights reserved.",BERT; defamation; fake; hate; Hindi; Hostility detection; NLP; offensive; social media,Classification (of information); Embeddings; Fake detection; Social networking (online); BERT; Defamation; Ensemble-classifier; Fake; Fine grained; Hate; Hindi; Hostility detection; Offensive; Social media; Natural language processing systems
Negative Stances Detection from Multilingual Data Streams in Low-Resource Languages on Social Media Using BERT and CNN-Based Transfer Learning Model,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183721290&doi=10.1145%2f3625821&partnerID=40&md5=48e4b1cc2b1d460e98f584b213526a85,"Online social media allows users to connect with a large number of people across the globe and facilitate the exchange of information efficiently. These platforms cater to many of our day-to-day needs. However, at the same time, social media have been increasingly used to transmit negative stances such as derogatory language, hate speech, and cyberbullying. The task of identifying the negative stances from social media posts or comments or tweets is termed negative stance detection. One of the major challenges associated with negative stance detection is that most of the content published on social media is often in a multilingual format. This work aims to identify negative stances from multilingual data streams in low-resource languages on social media using a hybrid transfer learning and deep convolutional neural network approach. The proposed work starts by preprocessing the multilingual datasets by removing irrelevant information such as special characters and hyperlinks. The processed dataset is then passed through a pretrained BERT (bidirectional encoder representations from Transformers) model to generate embeddings by fine-tuning the model as per the dataset under consideration. The generated word embeddings are then passed to a deep convolutional neural network for extracting the latent features from the texts and removing the unessential information. This helps our model to achieve robustness and effectiveness for efficient learning on the given dataset and make appropriate predictions on zero-shot data. The article utilizes several optimization strategies for examining the impact of fine-tuning different BERT layers on the model's performance. Intensive experiments on a variety of languages - namely, English, French, Italian, Danish, Arabic, Spanish, Indonesian, German, and Portuguese - are performed. The experimental results demonstrate the effectiveness and efficiency of the proposed framework. © 2024 Association for Computing Machinery. All rights reserved.",Bidirectional encoder representations from Transformers (BERT); convolutional neural network (CNN); deep transfer learning; low-resource languages; machine learning classifier; negative stances detection,Convolution; Deep neural networks; Embeddings; Graph neural networks; Hypertext systems; Long short-term memory; Network coding; Social networking (online); Transfer learning; Zero-shot learning; Bidirectional encoder representation from transformer; Convolutional neural network; Deep transfer learning; Learning classifiers; Low resource languages; Machine learning classifier; Machine-learning; Negative stance detection; Transfer learning; Convolutional neural networks
AlgBERT: Automatic Construction of Annotated Corpus for Sentiment Analysis in Algerian Dialect,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181439105&doi=10.1145%2f3632948&partnerID=40&md5=b69353840f1377116551921e6bd22ed5,"Nowadays, sentiment analysis is one of the most crucial research fields of Natural Language Processing (NLP), and it is widely applied in a variety of applications such as marketing and politics. However, the Arabic language still lacks sufficient language resources to enable the tasks of opinion and emotion analysis comparing to other language such as English. Additionally, manual annotation requires a lot of effort and time. In this article, we address this problem and propose a novel automated annotation platform for sentiment analysis called AlgBERT by providing annotated corpus and using deep learning technology that includes many automatic natural language processing algorithms, which is the basis for text classification and opinion analysis. We suggest using BERT model as a method; it is the abbreviation of Bidirectional Encoder Representations from Transformers, as it is one of the most effective technologies in terms of results in different world languages. We used around of 54K comments collected from social networking (Twitter, YouTube) written in Arabic and Algerian dialects. Our AlgBERT system obtained excellent results with an accuracy of 91.04%, and this is considered as one of the best results for opinion analysis in Algerian dialect. © 2023 Copyright held by the owner/author(s)",Annotated corpus; BERT; deep learning,Classification (of information); Deep learning; Linguistics; Annotated corpus; Arabic languages; Automatic construction; BERT; Deep learning; Language processing; Natural languages; Opinion analysis; Research fields; Sentiment analysis; Sentiment analysis
Speaker-Aware Interactive Graph Attention Network for Emotion Recognition in Conversation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181847037&doi=10.1145%2f3627806&partnerID=40&md5=140dd2226b5d1c7d030053544a65e05f,"Recently, Emotion Recognition in Conversation (ERC) has attracted much attention and has become a hot topic in the field of natural language processing. Conversation is conducted in chronological order; current utterance is more likely influenced by nearby utterances. At the same time, speaker dependency also plays a core role in the conversation dynamic. The combined effect of the sequence-aware information and the speaker-aware information makes the emotion’s dynamic change. However, past works used simple information fusion methods to model the two kinds of information but ignored their interactive influence. Thus, we propose a novel method entitled SIGAT (Speaker-aware Interactive Graph Attention Network) to solve the problem. The core module is a mutual interactive module in which a dual-connection (self-connection and interact-connection) graph attention network is constructed. The advantage of SIGAT is modeling the speaker-aware and sequence-aware information in a unified graph and updating them simultaneously. In this way, we model the interactive influence of them and obtain the final representations, which have richer contextual clues. Experimental results on the four public datasets demonstrate that SIGAT outperforms the state-of-the-art models. © 2023 Copyright held by the owner/author(s)",Emotion recognition in conversation; natural language processing; text classification,Character recognition; Classification (of information); Speech recognition; 'current; Chronological order; Combined effect; Emotion recognition; Emotion recognition in conversation; Hot topics; Language processing; Natural language processing; Natural languages; Text classification; Emotion Recognition
Named Entity Recognition in Persian Language based on Self-attention Mechanism with Weighted Relational Position Encoding,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181476098&doi=10.1145%2f3633513&partnerID=40&md5=c04ae2a34e20a401e2b29bb04f8b1490,"Named-entity Recognition (NER) is challenging for languages with low digital resources. The main difficulties arise from the scarcity of annotated corpora and the consequent problematic training of an effective NER Model. We propose a customized model based on linguistic properties to compensate for this lack of resources in low-resource languages like Persian. According to pronoun-dropping and subject-object-verb word order specifications of Persian, we propose new weighted relative positional encoding in the self-attention mechanism. Using the pointwise mutual information factor, we inject co-occurrence information into context representation. We trained and tested our model on three different datasets: Arman, Peyma, and ParsTwiNER, and our method achieved 94.16%, 93.36%, and 84.49% word-level F1 scores, respectively. The experiments showed that our proposed model performs better than other Persian NER models. Ablation Study and Case Study also showed that our method can converge faster and is less prone to overfitting. © 2023 Copyright held by the owner/author(s)",Named entity recognition; persian natural language processing; pointwise mutual information; transformer,Encoding (symbols); Linguistics; Natural language processing systems; Attention mechanisms; Encodings; Language processing; Named entity recognition; Natural languages; Persian natural language processing; Persians; Pointwise mutual information; Recognition models; Transformer; Signal encoding
Dependency-based BERT for Chinese Event Argument Extraction,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181468450&doi=10.1145%2f3633306&partnerID=40&md5=b10e94d44d91684978bfb60d665f311a,"Existing event extraction methods independently identify and classify each argument role separately, ignoring the interdependence between different parameter roles. Further, these methods rely on simple vectors to represent word embeddings. By embedding explicit syntactic constraints in the attention mechanism, we address these shortcomings by using dependency syntax to guide the text modeling. Specifically, we use dependency syntax to guide the BERT model for Chinese event argument role extraction, which mainly consists of three stages. First, the self-attention method guided by a dependency syntactic parsing tree is embedded in the Transformer computing framework of the BERT model. In addition to obtaining a deep two-way linguistic representation of a word according to its context information, this method also expresses the long-distance syntactic dependency relationships between words based on context information. Secondly, the designed conditional layer normalization method is applied to the event argument extraction model in order to integrate the semantic information of trigger words into the text, thereby improving the accuracy of the argument role extraction. Lastly, conditional random fields (CRFs) are used to determine the optimal sequence of labels at the sentence level based on the dependence relationships between adjacent labels. According to the experimental results, the constructed model outperforms several strong baselines in the Chinese event argument extraction task on the ACE2005 dataset and the iFLYTEKA.I.2020 dataset. © 2023 Copyright held by the owner/author(s)",argument role; BERT; conditional random field; dependency syntactic parsing; Insert event extraction,Embeddings; Natural language processing systems; Random processes; Semantics; Syntactics; Argument role; BERT; Conditional random field; Context information; Dependency syntactic parsing; Embeddings; Events extractions; Insert event extraction; Random fields; Syntactic parsing; Extraction
A Novel Knowledge-augmented Model Customization Approach for Arabic Offensive Language Detection,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181478888&doi=10.1145%2f3634702&partnerID=40&md5=27d2ebdcc5bc08ae771f2a1e2f2a4693,"Multiple attempts to develop systems for detecting online Arabic offensive language have been explored in previous studies. However, most of these attempts do not consider the variation of Arabic dialects, cultures, and offensive phrases. In contrast, this study aims to extract knowledge from multiple offensive language datasets to build a cross-dialect and culture knowledge-based repository. This knowledge-based repository is utilized to develop novel system architecture based on customizing the AraBERT model in a unique method to preserve dialectal knowledge and offensive cultural knowledge within the contextual word embedding of BERT architecture. Performance evaluation procedures consist of statistical evaluation metrics and a behavioral checklist. Results report more effective predictions by the customized model than the uncustomized one, particularly for offensive text. The customization process allows the model to gain more knowledge of informal text in general, and a better understanding of dialectal Arabic. © 2023 Copyright held by the owner/author(s)",collocations; knowledge-based; Natural Language Processing; neural networks; offensive Language detection,Knowledge based systems; Natural language processing systems; Neural networks; Online systems; Collocation; Customisation; Knowledge based; Language detection; Language processing; Natural language processing; Natural languages; Neural-networks; Offensive language detection; Offensive languages; Network architecture
Factorized Recurrent Neural Network with Attention for Language Identification and Content Detection,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181851208&doi=10.1145%2f3630607&partnerID=40&md5=4e284e3cb79f7c4fe9245848ad1fdea8,"Language identification and content detection are essential for ensuring effective digital communication, and content moderation. While extensive research has primarily focused on well-known and widely spoken languages, challenges persist when dealing with indigenous and resource-limited languages, especially between closely similar languages such as Ethiopian languages. This article aims to simultaneously identify the language of a given text and detect its content, and to achieve this, we propose a novel attention-based recurrent neural network framework. The proposed method has an attention-embedded Bidirectional-LSTM architecture with two classifiers that identify the language of a given text and content within the text. The two classifiers share a common feature space before they branched at their task-specific layers where both layers are assisted by attention mechanism. We use five different topics in Six Ethiopian Languages the dataset consists of nearly 22,624 sentences. We compared our result with the classical NLP techniques, the proposed method shortened the data prepossessing steps. We evaluated the model performance using the accuracy metric, achieving results of 98.88% for language identification and 96.5% for text content detection. The dataset, source code, and pretrained model are available at https://github.com/bdu-birhanu/LID_TCD. © 2023 Copyright held by the owner/author(s)",Attention mechanism; datasets; factorized-recurrent neural network; language identification; neural networks; resource-limited languages; text content detection,Digital communication systems; Long short-term memory; Attention mechanisms; Content detection; Dataset; Factorized-recurrent neural network; Language content; Language identification; Neural-networks; Resource-limited language; Text content; Text content detection; Natural language processing systems
A Comparative Study of Different Dimensionality Reduction Techniques for Arabic Machine Translation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181445492&doi=10.1145%2f3634681&partnerID=40&md5=35168c3306882df87d6d2f57be1f1151,"Word embeddings are widely deployed in a tremendous range of fundamental natural language processing applications and are also useful for generating representations of paragraphs, sentences, and documents. In some contexts involving constrained memory, it may be beneficial to reduce the size of word embeddings since they represent a core component of several natural language processing tasks. By reducing the dimensionality of word embeddings, their usefulness in memory-limited devices can be significantly improved, yielding gains in many real-world applications. This article aims to provide a comparative study of different dimensionality reduction techniques to generate efficient lower-dimensional word vectors. Based on empirical experiments carried out on the Arabic machine translation task, we found that the post-processing algorithm combined with independent component analysis provides optimal performance over the considered dimensionality reduction techniques. Therefore, we arrive at a new combination of the post-processing algorithm and dimensionality reduction (independent component analysis) techniques, which has not been investigated before. The latter was applied to both contextual and non-contextual word embeddings to reduce the size of the vectors while achieving a better translation quality than the original ones. © 2023 Copyright held by the owner/author(s)",Arabic machine translation; Dimensionality Reduction Techniques; post-processing algorithm; Transformer,Computational linguistics; Computer aided language translation; Embeddings; Machine translation; Natural language processing systems; Arabic machine translation; Comparatives studies; Core components; Dimensionality reduction techniques; Embeddings; Machine translations; Natural language processing applications; Natural languages; Postprocessing algorithms; Transformer; Independent component analysis
Cross-lingual Sentiment Analysis of Tamil Language Using a Multi-stage Deep Learning Architecture,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181474262&doi=10.1145%2f3631391&partnerID=40&md5=9350dc4ef291fbd6b48a948b7cee9f34,"In recent years, sentiment analysis has become a focal point in natural language processing. Cross-lingual sentiment analysis is a particularly demanding yet essential task that seeks to construct models capable of effectively analyzing sentiments across a variety of languages. The primary motivation behind this research is to bridge the gap in current techniques that often struggle to perform well with low-resource languages, due to the scarcity of large, annotated datasets, and their unique linguistic characteristics. In light of these challenges, we propose a novel Multi-Stage Deep Learning Architecture (MSDLA) for cross-lingual sentiment analysis of the Tamil language, a low-resource language. Our approach utilizes transfer learning from a source language with abundant resources to overcome data limitations. Our proposed model significantly outperforms existing methods on the Tamil Movie Review dataset, achieving an accuracy, precision, recall, and F1-score of 0.8772, 0.8614, 0.8825, and 0.8718, respectively. ANOVA statistical comparison demonstrates that the MSDLA’s improvements over other models, including mT5, XLM, mBERT, ULMFiT, BiLSTM, LSTM with Attention, and ALBERT with Hugging Face English Embedding are significant, with p-values all less than 0.005. Ablation studies confirm the importance of both cross-lingual semantic attention and domain adaptation in our architecture. Without these components, the model’s performance drops to 0.8342 and 0.8043 in accuracy, respectively. Furthermore, MSDLA demonstrates robust cross-domain performance on the Tamil News Classification and Thirukkural datasets, achieving an accuracy of 0.8551 and 0.8624, respectively, significantly outperforming the baseline models. These findings illustrate the robustness and efficacy of our approach, making a significant contribution to cross-lingual sentiment analysis techniques, especially for low-resource languages. © 2023 Copyright held by the owner/author(s)",Artificial intelligence; attention mechanism; cross-lingual sentiment analysis; tamil sentiment analysis,Architecture; Classification (of information); Large dataset; Long short-term memory; Semantics; Attention mechanisms; Cross-lingual; Cross-lingual sentiment analyse; Learning architectures; Low resource languages; Multi-stages; Sentiment analysis; Tamil language; Tamil sentiment analyse; Sentiment analysis
Adaptive Learning and Correlative Assessment of Differential Usage Patterns for Students with-or-without Learning Disabilities via Learning Analytics,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181454882&doi=10.1145%2f3632365&partnerID=40&md5=4b2e55a88d21a7770ff5a61c398a6eea,"Learning Disabilities (LD) can be categorized into logical, analytical, grammatical, vocabulary, sequential, and inference disabilities. Analysis of such disabilities assists students to identify and strengthen their weak areas. A wide variety of analysis models is proposed by researchers to perform such tasks, but most of these models are highly complex and cannot be scaled for multimodal parameter sets. To overcome these issues, this text proposes a model for correlative assessment of differential usage patterns in students with-or-without learning disabilities via multimodal analysis. The proposed model initially collects real-time inference sets for students with Learning Disabilities (LD) and without LDs. These sets consist of question-specific recorded responses for “Addition,” “Carry Propagation,” “Basic to Advanced Grammar,” “Direct, Inference and Vocabulary Comprehension,” “Finding odd-man-out,” “Sequencing,” and “Pseudo and Sight Spelling” for different question sets. Answers to these questions and their metadata were processed via a correlative engine that assisted in evaluation of correctness, time needed per question per category, number of skips, number of revisits, and unanswered ratio for different students. This evaluation was combined with temporal analysis to identify per-category progress of students. Based on this progress, students were either upgraded to next level or given lower-level questions, which assisted them to incrementally improve their grades. The model proved that the performance of LD students is 55% less than the non-LD students and an average of 18 LD students have achieved an average of 33% of improvement after having multiple attempts of the adaptive lessons. The model uses a correlation function, which enables to identify answering patterns of LD and non-LD students with 98.4% accuracy, thus can be used for clinical scenarios. © 2023 Copyright held by the owner/author(s)",correlation analysis; Learning disability; multimodal analysis; multiple attempt analysis,Learning systems; Modal analysis; Adaptive learning; Analysis models; Correlation analysis; Learning disabilities; Multi-modal; Multimodal analysis; Multiple attempt analyse; Parameter set; Real-time inference; Usage patterns; Students
Semantic Template-based Convolutional Neural Network for Text Classification,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179122571&doi=10.1145%2f3627820&partnerID=40&md5=360741a9e699b69df3170be5fa36cea1,"We propose a semantic template-based distributed representation for the convolutional neural network called Semantic Template-based Convolutional Neural Network (STCNN) for text categorization that imitates the perceptual behavior of human comprehension. STCNN is a highly automatic approach that learns semantic templates that characterize a domain from raw text and recognizes categories of documents using a semantic-infused convolutional neural network that allows a template to be partially matched through a statistical scoring system. Our experiment results show that STCNN effectively classifies documents in about 140,000 Chinese news articles into predefined categories by capturing the most prominent and expressive patterns and achieves the best performance among all compared methods for Chinese topic classification. Finally, the same knowledge can be directly used to perform a semantic analysis task.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Chinese text categorization; Natural language processing; semantic template; text representation; topic classification,Behavioral research; Classification (of information); Convolution; Convolutional neural networks; Information retrieval systems; Natural language processing systems; Semantic Web; Text processing; Chinese text categorization; Convolutional neural network; Language processing; Natural language processing; Natural languages; Semantic templates; Template-based; Text classification; Text representation; Topic Classification; Semantics
Fintech Key-Phrase: A New Chinese Financial High-Tech Dataset Accelerating Expression-Level Information Retrieval,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179124515&doi=10.1145%2f3627989&partnerID=40&md5=fbecd2726dd7cd26cdc740e68fd03dbd,"Expression-level information extraction is a challenging task in natural language processing (NLP), which aims to retrieve crucial semantic information from linguistic documents. However, there is a lack of up-to-date data resources for accelerating expression-level information extraction, particularly in the Chinese financial high technology field. To address this gap, we introduce Fintech Key-Phrase: a human-annotated key-phrase dataset for the Chinese financial high technology domain. This dataset comprises over 12K paragraphs along with annotated domain-specific key-phrases. We extract the publicly released reports, Chinese management's discussion and analysis (CMD&A), from the renowned Chinese research data services platform (CNRDS) and then filter the reports related to high technology. The high technology key-phrases are annotated following pre-defined philosophy guidelines to ensure annotation quality. In order to better understand the limitations and challenges in the purposed dataset, we conducted comprehensive noise evaluation experiments for the Fintech Key-Phrase, including annotation consistency assessment and absolute annotation quality evaluation. To demonstrate the usefulness of our released Fintech Key-Phrase in retrieving valuable information in the Chinese financial high technology field, we evaluate its significance using several superior information retrieval systems as representative baselines and report corresponding performance statistics. Additionally, we further applied ChatGPT to the text augmentation approach of the Fintech Key-Phrase dataset. Extensive comparative experiments demonstrate that the augmented Fintech Key-Phrase dataset significantly improved the coverage and accuracy of extracting key phrases in the finance and high-tech domains. We believe that this dataset can facilitate scientific research and exploration in the Chinese financial high technology field. We have made the Fintech Key-Phrase dataset and the experimental code of the adopted baselines accessible on Github: https://github.com/albert-jin/Fintech-Key-Phrase. To encourage newcomers to participate in the financial high-tech domain information retrieval research, we have developed a series of tools, including an open website1 and corresponding real-time information retrieval APIs.2 © 2023 held by the owner/author(s). Publication rights licensed to ACM.",ChatGPT-based data augment; Chinese management's discussion and analysis; expression-level information extraction; financial high technology field; Information retrieval,Data mining; Finance; Information retrieval systems; Natural language processing systems; Quality control; Search engines; Semantics; ChatGPT-based data augment; Chinese management discussion and analyse; Expression levels; Expression-level information extraction; Financial high technology field; High tech; High-technology; Key-phrase; Natural languages; Technology fields; Information retrieval
Stacking of BERT and CNN Models for Arabic Word Sense Disambiguation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179123677&doi=10.1145%2f3623379&partnerID=40&md5=28c2b076ceb2693d35210e61703831a5,"We propose a new approach for Arabic Word Sense Disambiguation (AWSD) by hybridization of single-layer Convolutional Neural Network (CNN) with contextual representation (BERT). WSD is the task of automatically detecting the correct meaning of a word used in a given context. WSD can be performed as a classification task, and the context is generally a short sentence. Kim [26] proved that combining a CNN with an RNN (recurrent neural network) provides a good result for text classification. Here, we use a concatenation of BERT models as a word embedding to get simultaneously the target and context representation. Our approach improves the performance of WSD in Arabic languages. The experimental results show that our model outperforms the state-of-the-art approaches and improves the accuracy of 96.42% on the Arabic WordNet dataset.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Arabic text; BERT; convolutional neural network; supervised approach; transformer; Word sense disambiguation,Classification (of information); Convolution; Convolutional neural networks; Multilayer neural networks; Natural language processing systems; Network layers; Text processing; Arabic texts; BERT; Convolutional neural network; Hybridisation; Neural network model; New approaches; Stackings; Supervised approach; Transformer; Word Sense Disambiguation; Recurrent neural networks
Shortcut Enhanced Syntactic and Semantic Dual-channel Network for Aspect-based Sentiment Analysis,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179137718&doi=10.1145%2f3629518&partnerID=40&md5=775a0c6c7a662e01baeae31fd0b6284b,"Aspect-based sentiment analysis (ABSA) is a fine-grained task that predicts the sentiment polarity of different aspects in the same sentence. The main challenge is how to build a strong dependency between aspects and sentiment. Recently, the graph neural network (GNN) has become the mainstream trend to extract syntactic dependency relations from the syntactic dependency tree. However, further improvements are hampered by the inherent mistake on the syntactic dependency tree. Consequently, this article presents a dual-channel model to investigate whether considering both syntactic dependency and semantic relevance can further improve the performance. Specifically, we propose a multi-head syntactic graph convolution network (MHGCN) module in the syntactic channel, focusing on different aspects of the syntactic flow in parallel. We also design a syntactic local attention mechanism (Syn-LFAM) and a semantic local attention mechanism (Sem-LFAM) to fully exploit the crucial local information, respectively. Moreover, we use the cross semantic-syntax interaction module and gate fusion mechanism to control the combination of semantic and syntax dynamically. The experimental results show that we utilize less resource consumption, and the final model outperforms the state-of-the-art methods on three of the four publicly available datasets. © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Aspect-based; graph convolution network; local attention mechanism; neural networks; semantic analysis; sentiment analysis; syntactic analysis,Flow graphs; Graph neural networks; Parallel flow; Semantic Web; Semantics; Sentiment analysis; Syntactics; Trees (mathematics); Aspect-based; Attention mechanisms; Dual channel; Graph convolution network; Local attention mechanism; Neural-networks; Semantic analysis; Sentiment analysis; Syntactic analysis; Syntactic dependencies; Convolution
Semi-Automatic Building and Learning of a Multilingual Ontology,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179126241&doi=10.1145%2f3615864&partnerID=40&md5=c53233cf99eadfeea13eb64ff1401aaa,"Most online platforms, applications, and Websites use a massive amount of heterogeneous evolving data. These data must be structured and normalized before integration to improve the search and increase the relevance of results. An ontology can address this critical task by efficiently managing data and providing structured formats through techniques such as the Web Ontology Language (OWL). However, building an ontology can be costly, primarily if conducted manually. In this context, we propose a new methodology for automatically building and learning a multilingual ontology using Arabic as the base language via a corpus collected from Wikipedia. Our proposed methodology relies on Finite-state transducers (FSTs). FSTs are regrouped into a cascade to reduce errors and minimize ambiguity. The produced ontology is extended to English and French and independent language images via a translator we developed using APIs. The rationale for starting with the Arabic corpus to extract terms is that entity linking is more convenient from Arabic to other languages. In addition, many Wikipedia articles in English and French (for instance) do not have associated Arabic articles, but the opposite is true. In addition, dealing with Arabic terms permits us to enrich the Arabic module of the free linguistic platform we use in dictionaries and graphs. To assess the efficiency of our proposed methodology, we conducted performance metrics. The reported results are encouraging and promising.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",API; Arabic NLP; finite sate transducer; Ontology building and learning; transducer cascade,Semantic Web; Transducers; API; Arabic NLP; Finite sate transducer; Finite state transducers; Multilingual ontologies; Ontology building; Ontology learning; Ontology's; Semi-automatics; Transducer cascade; Ontology
How a Deep Contextualized Representation and Attention Mechanism Justifies Explainable Cross-Lingual Sentiment Analysis,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179134777&doi=10.1145%2f3626094&partnerID=40&md5=9dc670a91f5fa1cb8f450fb73c3a114f,"The number of applications in sentiment analysis is growing daily, and research in this field is increasing. Despite the rapid growth of data sources in English, low-resource languages suffer from a lack of data for accurate training models. Moreover, users cannot trust such systems without explaining the output. In this study, we propose a cross-lingual deep neural model to improve the accuracy of sentiment analysis for low-resource languages while providing an explainable description of the predictions. The proposed model contains a word representation model where we use XLM-RoBERTa, a pre-trained contextualized transformer-based cross-lingual language model, and a long short-term memory network together with an attention mechanism that helps improve the explainability of the model and detect the informative words that impact text polarity. Our experiments show the superiority of the proposed model compared to the state-of-the-art mono-lingual techniques and cross-lingual models. The results show 0.55% improvement compared to the cross-lingual sentiment analysis proposed by Ghasemi et al. and 15.08% improvement compared to the mono-lingual contextualized sentiment analysis. Moreover, we achieve 0.54% further improvement when using attention mechanisms for enhancing the model with explainability.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Attention mechanism; cross-lingual representation; deep learning; low-resource languages; sentiment analysis; transformers,Deep learning; Attention mechanisms; Cross-lingual; Cross-lingual representation; Data-source; Deep learning; Low resource languages; Rapid growth; Representation mechanism; Sentiment analysis; Transformer; Sentiment analysis
Chinese Event Discourse Deixis Resolution: Design of the Dataset and Model,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179137922&doi=10.1145%2f3618109&partnerID=40&md5=2fb040f8a131097f90fd35d4dfca651e,"Anaphora resolution is a traditional task in the natural language processing community, defined as a cohesion phenomenon where one entity points back to a previous entity. Event discourse deixis (EDD) is a kind of more complex anaphora in which the anaphors refer to event descriptions such as sentences or clauses. Event discourse deixis resolution (EDDR) is able to help machines understand the richer linguistic and semantic information in the discourse. However, compared to anaphora resolution, EDDR has received relatively less research attention. In this work, we investigate the EDDR task by designing the corresponding dataset and model. First, we manually construct a high-quality Chinese corpus for EDDR, including 4,417 documents and 5,929 event chains that consist of event antecedents and anaphors. Second, we propose a deep neural network model for EDDR, which formulates the task into two subtasks, namely event anaphor recognition and event antecedent recognition. Our model is trained under the two subtasks jointly so that the EDDR task can be performed end to end. Besides our final model, we also build seven pipeline and joint models as baselines to build comprehensive benchmarks for follow-up research. Experimental results on our EDDR dataset show that our model outperforms all the baselines and achieves about 53%, 44%, 53%, and 63% F1s using standard anaphora resolution metrics such as CoNLL, MUC, B3, and Ceafe. The performances show that EDDR is a challenging task and worth researching in the future. Our dataset and model will be released to facilitate follow-up research.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",anaphora resolution; coreference resolution; Discourse deixis resolution; event chain extraction,Natural language processing systems; Neural network models; Semantics; Anaphora resolution; Anaphors; Coreference resolution; Discourse deixis resolution; Event chain extraction; Event description; Follow up; Language processing; Natural languages; Subtask; Deep neural networks
Task-based Meta Focal Loss for Multilingual Low-resource Speech Recognition,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179139641&doi=10.1145%2f3626187&partnerID=40&md5=8303f0ed7a8fd4ff3e29ea5926453939,"Low-resource automatic speech recognition is a challenging task due to a lack of labeled training data. To resolve this issue, multilingual meta-learning learns a better model initialization from many source language tasks for fast adaptation to unseen target languages. However, for diverse source languages, the quantity and difficulty vary greatly because of their different data scales and phonological systems. These differences lead to task-quantity and task-difficulty imbalance issues and thus a failure of multilingual meta-learning ASR. In this work, we propose a task-based meta focal loss (TMFL) approach to address this tough challenge. Specifically, we introduce a hard-task moderator and update the meta-parameters using gradients from both the support set and query set. Our proposed approach focuses more on hard tasks and makes full use of the data from hard tasks. Moreover, we analyze the significance of the hard task moderator and interpret its significance at the sample level. Experiment results show that the proposed method, TMFL, significantly outperforms the state-of-the-art multilingual meta-learning on all target languages for the IARPA BABEL and OpenSLR datasets, especially under very-low-resource conditions. In particular, it can reduce character error rate from 72% to 60% by fine-tuning the pre-trained model with about 22 hours of Vietnamese data.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",focal loss; IARPA-BABEL; low-resource; Meta learning; OpenSLR; speech recognition,Ductile fracture; Moderators; Focal loss; Hard task; IARPA-BABEL; Low-resource; Low-resource speech recognition; Metalearning; OpenSLR; Source language; Target language; Task-based; Speech recognition
Reading Scene Text with Aggregated Temporal Convolutional Encoder,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179122827&doi=10.1145%2f3625822&partnerID=40&md5=e2d91f6aa3c8fd7cc8859f2bac3d21f2,"Reading scene text in the natural image is of fundamental importance in many real-world problems. Text recognition has a profound effect on information processing by enabling automated extraction and interpretation. Recent scene text recognition methods employ the encoder-decoder framework, which constructs the encoder by obtaining the visual representations based on the last layer of the backbone network and then feeding them into a sequence model. In this article, we propose a novel encoder structure that performs the feature extractor and the sequence modeling within a unified framework. The introduced Aggregated Temporal Convolutional Encoder (ATCE) first incorporates the temporal convolutional layers to consider the long-term temporal relationship in the encoder stage. The aggregation of these temporal convolution modules is designed to utilize visual features from different levels, by augmenting the standard architecture with deeper aggregation to better fuse information across modules. We also study the impact of different attention modules in convolutional blocks for learning accurate text representations. We conduct comparisons on several scene text recognition benchmarks for both Chinese and English; the experiments demonstrate the complementary ability with different decoder variants and the effectiveness of our proposed approach.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",feature aggregation; Scene text recognition; temporal convolutional encoder,Character recognition; Decoding; Signal encoding; Automated extraction; Convolutional encoders; Feature aggregation; Natural images; Real-world problem; Scene Text; Scene text recognition; Sequence models; Temporal convolutional encoder; Text recognition; Convolution
A Synergistic Bidirectional LSTM and N-gram Multi-channel CNN Approach Based on BERT and FastText for Arabic Event Identification,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179125984&doi=10.1145%2f3626568&partnerID=40&md5=f1782c1ef3226502f43a92c39c6c8aff,"Event extraction from texts continues to pose a challenge for many NLP systems. This article presents a novel neural network architecture that can extract and classify events from Arabic sentences. The model combines word representations and Part-Of-Speech (POS) tags and uses a bidirectional LSTM layer and a dual combined convolutional neural network. The first layer of the network focuses on sentence representations, while the second layer focuses on POS representations. The model takes advantage of both N-gram character features from FastText and contextual representations from bidirectional encoder representations from transformers. This combination proves to be successful, as evidenced by the good results obtained from evaluating the model on the Arabic TimeML corpus. Our results show that combining both contextual and N-gram representations outperforms the traditional skip-gram model.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Arabic language; BERT representation; deep learning; events identification; FastText representation; TimeML standard,Computational linguistics; Convolutional neural networks; Multilayer neural networks; Network architecture; Network layers; Syntactics; Arabic languages; BERT representation; Deep learning; Event identification; Events extractions; Fasttext representation; Multi channel; N-grams; NLP systems; Timeml standard; Long short-term memory
Fine-Grained Domain Adaptation for Chinese Syntactic Processing,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179129157&doi=10.1145%2f3629519&partnerID=40&md5=9c5db92e0353a3d9773b23a5f546dee9,"Syntactic processing is fundamental to natural language processing. It provides rich and comprehensive syntax information in sentences that could be potentially beneficial for downstream tasks. Recently, pretrained language models have shown great success in Chinese syntactic processing, which typically involves word segmentation, POS tagging, and dependency parsing. However, the on-going research never ends since performance would be degraded drastically when tested on a highly-discrepant domain. This problem is widely accepted as domain adaptation, where the test domain differs from the training domain in supervised learning. Self-training is one promising solution for it, and straightforward source-to-target adaptation has already shown remarkable effectiveness in previous work. While this strategy ignores the fact that sentences of the target domain sentences may have very different gaps from the source training domain. More specifically, sentences with large gaps might fail by direct self-training adaptation. To this end, we propose fine-grained domain adaptation for Chinese syntactic processing in this work, aiming to model the gaps between the source and the target domains accurately and progressively. The key idea is to divide the target domain into fine-grained subdomains by using a specified domain distance metric, and then perform gradual self-training on the subdomains. We further offer an intuitive theoretical illustration based on the theory of Kumar et al. (2020) approximately. In addition, a novel representation learning framework is proposed to encode fine-grained subdomains effectively, aiming to utilize the above idea fully. Experimental results on benchmark datasets show that our method can achieve significant improvements over a variety of baselines. © 2023 held by the owner/author(s). Publication rights licensed to ACM.",chinese syntactic processing; chinese word segmentation; dependency parsing; Domain adaptation; POS tagging; self training,Computational linguistics; Learning algorithms; Natural language processing systems; Chinese syntactic processing; Chinese word segmentation; Dependency parsing; Domain adaptation; Fine grained; Natural languages; POS tagging; Self-training; Subdomain; Target domain; Syntactics
Multi-task Pre-training Language Model for Semantic Network Completion,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179131594&doi=10.1145%2f3627704&partnerID=40&md5=9ba5db82e76dc9fee84090e699296b97,"Semantic networks, exemplified by the knowledge graph, serve as a means to represent knowledge by leveraging the structure of a graph. While the knowledge graph exhibits promising potential in the field of natural language processing, it suffers from incompleteness. This article focuses on the task of completing knowledge graphs by predicting linkages between entities, which is fundamental yet critical. Traditional methods based on translational distance struggle when dealing with unseen entities. In contrast, semantic matching presents itself as a potential solution due to its ability to handle such cases. However, semantic matching-based approaches necessitate large-scale datasets for effective training, which are typically unavailable in practical scenarios, hindering their competitive performance. To address this challenge, we propose a novel architecture for knowledge graphs known as LP-BERT, which incorporates a language model. LP-BERT consists of two primary stages: multi-task pre-training and knowledge graph fine-tuning. During the pre-training phase, the model acquires relationship information from triples by predicting either entities or relations through three distinct tasks. In the fine-tuning phase, we introduce a batch-based triple-style negative sampling technique inspired by contrastive learning. This method significantly increases the proportion of negative sampling while maintaining a nearly unchanged training time. Furthermore, we propose a novel data augmentation approach that leverages the inverse relationship of triples to enhance both the performance and robustness of the model. To demonstrate the effectiveness of our proposed framework, we conduct extensive experiments on three widely used knowledge graph datasets: WN18RR, FB15k-237, and UMLS. The experimental results showcase the superiority of our methods, with LP-BERT achieving state-of-the-art performance on the WN18RR and FB15k-237 datasets.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Knowledge graph; link prediction; multi-task learning; semantic matching; translational distance,Computational linguistics; Forecasting; Knowledge graph; Knowledge management; Large dataset; Learning systems; Natural language processing systems; Semantic Web; Semantics; Translation (languages); Fine tuning; Knowledge graphs; Language model; Link prediction; Multi tasks; Multitask learning; Pre-training; Semantic matching; Semantics networks; Translational distance; Inverse problems
MHG-ERC: Multi-hypergraph Feature Aggregation Network for Emotion Recognition in Conversations,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176767965&doi=10.1145%2f3622935&partnerID=40&md5=3b00076e18c169f39aad880f910395d1,"The modeling of conversational context is an essential step in Emotion Recognition in Conversations (ERC). To maintain high performance and a low GPU memory consumption, this article proposes a new idea of using multiple hypergraphs to model the conversational context and designs a multi-hypergraph feature aggregation network for ERC. We use context window, speaker information, position information between utterances, and specific step size to construct different hyperedges. Then, various hypergraphs generated by different hyperedges are used to aggregate local and remote context information in turn. Experiments on two dialogue emotion datasets, IEMOCAP and MELD, demonstrate the effectiveness and superiority of this new model. In addition, our model requires only relatively low GPU memory consumption.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dialogue systems; Emotion recognition; hypergraph convolution; multi-hypergraph learning,Convolution; Graph theory; Speech processing; Speech recognition; Aggregation network; Dialogue systems; Emotion recognition; Feature aggregation; Hyper graph; Hyperedges; Hypergraph convolution; Memory consumption; Multi-hypergraph learning; Emotion Recognition
Sentiment Analysis of Code-Mixed Telugu-English Data Leveraging Syllable and Word Embeddings,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176749733&doi=10.1145%2f3620670&partnerID=40&md5=1ce1cc762492155532ddee0826d6aada,"Learning the inherent meaning of a word in Natural Language Processing (NLP) has motivated researchers to represent a word at various levels of abstraction, namely character-level, morpheme-level, and subword-level vector representations. Syllable-Aware Word Embedding (SAWE) can effectively handle agglutinative and fusion-based NLP tasks. However, research attempts on assessing the SAWE on such extrinsic NLP tasks has been scanty, especially for low-resource languages in the context of code-mixing with English. A model to learn SAWE to extract semantics at fine-grained subunits of a word is proposed in this article, and the representative ability of the embeddings is assessed through sentiment analysis of code-mixed Telugu-English review corpora. Multilingual societies and advancements in communication technologies have accounted for the prolific usage of mixed data, which renders the State-of-the-Art (SOTA) sentiment analysis models developed based on monolingual data ineffective. Social media users in the Indian subcontinent exhibit a tendency to mix English and their respective native language (using the phonetic form of English) in expressing their opinions or sentiments. A code-mixing scenario provides flexibility to borrow words from a foreign language, usage of shorthand notations, elongation of vowels, and usage of words without following syntactic/grammatical rules, which renders the sentiment analysis of code-mixed data challenging to perform. Deep neural architectures like Long Short-Term Memory and Gated Recurrent Unit networks have been shown to be effective in solving several NLP tasks, such as sequence labeling, named entity recognition, and machine translation. In this article, a framework to perform sentiment analysis on a code-mixed Telugu-English review corpus is implemented. Both word embedding and SAWE are input to a unified deep neural network that contains a two-level Bidirectional Long Short-Term Memory/Gated Recurrent Unit network with Softmax as the output layer. The proposed model leverages the advantages of both word embedding and SAWE, which enable the proposed model to outperform existing SOTA code-mixed sentiment analysis models on a Telugu-English code-mixed dataset of the International Institute of Information Technology-Hyderabad and a dataset curated by the authors. The improvement realized by the proposed model on these datasets is [3% increase in F1-score and 2% increase in accuracy] and [7% increase in F1-score and 5% in accuracy], respectively, in comparison with the best-performing SOTA model. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCode-mixing; bidirectional networks; deep neural networks; gated recurrent unit; long short-term memory; sentiment analysis; syllable-aware embeddings; transliterated text; word embeddings,Brain; Embeddings; Long short-term memory; Mixing; Multilayer neural networks; Network coding; Network layers; Semantics; Sentiment analysis; Additional key word and phrasescode-mixing; Bidirectional networks; Embeddings; Gated recurrent unit; Key words; Sentiment analysis; Syllable-aware embedding; Transliterated text; Word embedding; Deep neural networks
An Arabic Probabilistic Parser Based on a Property Grammar,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178970422&doi=10.1145%2f3612921&partnerID=40&md5=e92002e96dabfd2c382243ac47d1d0dc,"The specificities of Arabic parsing, such as agglutination, vocalization, and the relatively order-free words in Arabic sentences, remain major issues to consider. To promote its robustness, such parseing should define different types of constraints. Property Grammar (PG) formalism verifies the satisfiability of the constraints directly on the units of the structure, thanks to its properties (or relations). In this context, we propose to build a probabilistic parser with syntactic properties, using a PG, and we measure the production rules in terms of different implicit information and in particular the syntactic properties. We experimented with our parser on the treebank ATB, using the parsing algorithm CYK, and we obtained encouraging results. Our method is also automatic for implementation of most property types. Its generalization for other languages or corpus domains (using treebanks) could be a good perspective. Its combination with pre-trained models of BERT may also make our parser faster.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesProbabilistic parser; Arabic language; lexicalized grammar; property grammar formalism,Antigen-antibody reactions; Context free grammars; Formal languages; Additional key word and phrasesprobabilistic parse; Arabic languages; Grammar formalisms; Key words; Lexicalized grammar; Probabilistics; Property; Property grammar formalism; Property grammars; Syntactic properties; Syntactics
Part-of-speech Tagger for Assamese Using Ensembling Approach,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178915557&doi=10.1145%2f3617653&partnerID=40&md5=388ff0e1148d7884858b591bd493bd6e,"Ensemble system for part-of-speech (POS) tagging is beneficial for many resource-poor languages that do not have enough annotated training data to train Deep Learning (DL, also named Deep Neural Network)-based POS taggers. An Ensemble system is a better choice to incorporate the linguistic features of a language and leverage the benefits of various types of POS taggers. In this work, we present our experiment of developing an ensemble tagger for Assamese, a low-resource, morphologically rich scheduled language of India, spoken by more than 15 million people. Despite the success of modern neural-network-based models in sequence tagging tasks, it has yet to receive attention in developing tasks such as POS in a resource-poor language such as Assamese. We develop a POS tagging model based on the BiLSTM-CRF architecture with a corpus of 404k tokens. We cover several word embeddings during training. Among all the experiments, the top two POS tagging models achieve tagging F1 scores of 0.746 and 0.745. We observe that the DL-based taggers are not able to achieve decent accuracy. It may be due to the inability to capture the linguistic features of the language or due to comparatively less annotated data. So, we build another POS tagger using a rule-based approach considering several morphological phenomena of the language and get an F1 score of 0.85. Subsequently, we integrate the top two DL-based taggers with the rule-based ones and develop a new POS tagger using an ensemble approach, of which we get an improved F1 score of 0.925. Performance improvement of our new ensemble POS taggers over the baseline taggers suggests that integration of the taggers combines the qualities of all taggers in the new tagger. Therefore, this study also states ensemble taggers are more suitable for highly inflectional, morphologically rich resource-poor languages.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPart of speech (POS) tagger; Assamese language; Deep Learning; ensemble POS tagger; language model; sequence labeling model; word embedding,Deep neural networks; Embeddings; Industrial plants; Syntactics; Additional key word and phrasespart of speech  tag; Assamese language; Deep learning; Embeddings; Ensemble phrasespart of speech tag; Key words; Language model; Sequence Labeling; Sequence labeling model; Word embedding; Computational linguistics
Assessing Urdu Language Processing Tools via Statistical and Outlier Detection Methods on Urdu Tweets,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176795008&doi=10.1145%2f3622939&partnerID=40&md5=a4f2c653e013789a11a3ca5a0f532d99,"Text pre-processing is a crucial step in Natural Language Processing (NLP) applications, particularly for handling informal and noisy content on social media. Word-level tokenization plays a vital role in text pre-processing by removing stop words, filtering irrelevant characters, and retaining relevant tokens. These tokens are essential for constructing meaningful n-grams within advanced NLP frameworks used for data modeling. However, tokenization in low-resource languages like Urdu presents challenges due to language complexity and limited resources. Conventional space-based methods and direct application of language-specific tools often result in erroneous tokens in Urdu Language Processing (ULP). This hinders language models from effectively learning language-specific and domain-specific tokens, leading to sub-optimal results for downstream tasks such as aspect mining, topic modeling, and Named Entity Recognition (NER). To address this issue for Urdu, we have proposed a data pre-processing technique that detects outliers using the Inter-Quartile-Range (IQR) method and proposed normalization algorithms for creating useful lexicons in conjunction with existing technologies. We have collected approximately 50 million Urdu tweets using the Twitter API and conducted the performance analysis of existing language-specific tokenizers (Urduhack and Space-based tokenizer). Dataset variants were created based on the language-specific tokenizers, and we performed statistical analysis tests and visualization techniques to compare tokenization results before and after applying the proposed outlier detection and normalization method. Our findings highlighted the noticeable improvement in token size distributions, handling of informal language tokens, and misspelled and lengthy tokens. The Urduhack tokenizer combined with the proposed outlier detection and normalization yielded tokens with the best-fitted distribution in ULP. Its effectiveness has been evaluated through the task of topic modeling using Non-negative Matrix Factorization (NMF) and Latent Dirichlet allocation (LDA). The results demonstrated new and distinct topics using unigram features while achieving highly coherent topics when utilizing bigram features. For the traditional space-based method, the results consistently demonstrated improved coherence and precision scores. However, the NMF topic modeling with bigram features outperformed LDA topic modeling with bigram features. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesUrdu language processing; outlier detection and removal; Urdu language processing tools; Urdu text tokenization; Urdu tweets,Data handling; Learning algorithms; Matrix algebra; Modeling languages; Natural language processing systems; Non-negative matrix factorization; Social networking (online); Statistical tests; Statistics; Additional key word and phrasesurdu language processing; Key words; Language processing; Outlier Detection; Outlier removals; Processing tools; Tokenization; Urdu language processing tool; Urdu text tokenization; Urdu tweet; Anomaly detection
Hindi Text Summarization Using Sequence to Sequence Neural Network,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178915450&doi=10.1145%2f3624013&partnerID=40&md5=d15e30cb3843750a7ad4d6cef85fce63,"Text summarizing reduces a large block of text data to a precise, short, and intelligible text that conveys the whole meaning of the actual text in a few words while maintaining the original context. Due to a lack of relevant summaries, it is hard to understand the main idea of the document. Text summarization using the abstractive technique is well-studied in English, although it is still in its infancy in Indian regional languages. In this study, we investigate the effectiveness of using a sequence-to-sequence (Seq2Seq) neural network based on attention and its optimization for text summarization for the Hindi language (HiATS), explicitly comparing the Adam and RMSprop optimizers. Our method allows the model to take the Hindi language dataset and, as output, provides a concise summary that accurately reflects the gist of the original text. The performance of the models will be evaluated using Rouge-1 and Rouge-2 metrics.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAbstractive text summarization; neural network; optimizers; word embedding,Additional key word and phrasesabstractive text summarization; Embeddings; Key words; Large blocks; Neural-networks; Optimizers; Text data; Text Summarisation; Text summarizing; Word embedding; Text processing
GAGPT-2: A Geometric Attention-based GPT-2 Framework for Image Captioning in Hindi,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176793258&doi=10.1145%2f3622936&partnerID=40&md5=8a236729433d87556dc611d33141c44d,"Image captioning frameworks usually employ an encoder-decoder paradigm, with the encoder receiving abstract image feature vectors as input and the decoder for language modeling. Nowadays, most prominent architectures employ features from region proposals derived from object detection modules. In this work, we propose a novel architecture for image captioning. We employ the object detection module integrated with transformer architecture as an encoder and GPT-2 (Generative Pre-trained Transformer) as a decoder. The encoder utilizes the information of the spatial relationships among detected objects. We introduce a unique methodology for image caption generation in Hindi, which is widely spoken in South Asia and India and is the world's third most spoken language as well as India's official language. In terms of BLEU scores, the proposed approach's performance is comparable to those of other baselines, and the results illustrate that the proposed approach outperforms the other baselines. The efficacy of the proposed approach in generating correct captions is further determined by human assessment in terms of adequacy and fluency. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDeep learning; attention; GPT-2; Hindi,Architecture; Decoding; Modeling languages; Object recognition; Signal encoding; Additional key word and phrasesdeep learning; Attention; Detection modules; Encoder-decoder; GPT-2; Hindi; Image captioning; Image features; Key words; Objects detection; Object detection
DIEU: A Dynamic Interaction Emotion Unit for Emotion Recognition in Conversation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176747593&doi=10.1145%2f3616493&partnerID=40&md5=af2d35cdc025631a2b48e3b02959a32e,"Emotion recognition in conversation (ERC) is challenging because the conversation takes place in real time and the speakers interact with each other. However, existing methods ignore the dynamic characteristics of interaction between speakers, and the problem of long-range context propagation still exists. In this article, we propose a dynamic interaction emotion unit to solve the preceding problems on the transcription of the conversation. First, we propose a main influence interval search algorithm to provide a dynamic interaction interval for each utterance. Then, we utilize the speaker-aware influence module and the two-stream context module to capture the dynamic interaction and the contextual information from this interval. Furthermore, to obtain the speaker state representation rich in emotional information, we propose a novel dynamic routing algorithm to fuse the preceding information. These well-integrated state representations also enable our model to capture contextual information at a longer distance. Experiments on multiple datasets demonstrate the effectiveness of the proposed method.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesText classification; dynamic routing; emotion recognition in conversation; interactive dynamics,Classification (of information); Speech recognition; Additional key word and phrasestext classification; Contextual information; Dynamic interaction; Dynamic routing; Emotion recognition; Emotion recognition in conversation; Interactive dynamic; Key words; Real- time; State representation; Emotion Recognition
"Fact-checking Vietnamese Information Using Knowledge Graph, Datalog, and KG-BERT",2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176789789&doi=10.1145%2f3624557&partnerID=40&md5=a704ed9382e26c92c01c23f2de24ba31,"In the era of digital information, ensuring the accuracy and reliability of information is crucial, making fact-checking a vital process. Currently, English fact-checking has thrived due to various language processing tools and ample datasets. However, the same cannot be said for Vietnamese fact-checking, which faces significant challenges due to the lack of such resources. To address these challenges, we propose a model for checking Vietnamese facts by synthesizing three popular technologies: Knowledge Graph (KG), Datalog, and KG-BERT. The KG serves as the foundation for the fact-checking process, containing a dataset of Vietnamese information. Datalog, a logical programming language, is used with inference rules to complete the knowledge within the Vietnamese KG. KG-BERT, a Deep Learning (DL) model, is then trained on this KG to rapidly and accurately classify information that needs fact-checking. Furthermore, to put Vietnamese complex sentences into the fact-checking model, we present a solution for extracting triples from these sentences. This approach also contributes significantly to the ease of constructing foundational datasets for the Vietnamese KG. To evaluate the model's performance, we create a Vietnamese dataset comprising 130,190 samples to populate the KG. Using Datalog, we enrich this graph with additional knowledge. The KG is then utilized to train the KG-BERT model, achieving an impressive accuracy of 95%. Our proposed solution shows great promise for fact-checking Vietnamese information and has the potential to contribute to the development of fact-checking tools and techniques for other languages. Overall, this research makes a significant contribution to the field of data science by providing an accurate solution for fact-checking information in Vietnamese language contexts.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",datalog; Fact checking; inference rule; KG-BERT; knowledge graph,Classification (of information); Deep learning; Accuracy of information; Datalog; Digital information; Fact checking; Inference rules; Knowledge graph-BERT; Knowledge graphs; Reliability of information; Vietnamese; Knowledge graph
MahaEmoSen: Towards Emotion-aware Multimodal Marathi Sentiment Analysis,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173207896&doi=10.1145%2f3618057&partnerID=40&md5=cdeb7beb7d31922b1c7f29436d8e3df7,"With the advent of the Internet, social media platforms have witnessed an enormous increase in user-generated textual and visual content. Microblogs on platforms such as Twitter are extremely useful for comprehending how individuals feel about a specific issue through their posted texts, images, and videos. Owing to the plethora of content generated, it is necessary to derive an insight of its emotional and sentimental inclination. Individuals express themselves in a variety of languages and, lately, the number of people preferring native languages has been consistently increasing. Marathi language is predominantly spoken in the Indian state of Maharashtra. However, sentiment analysis in Marathi has rarely been addressed. In light of the above, we propose an emotion-aware multimodal Marathi sentiment analysis method (MahaEmoSen). Unlike the existing studies, we leverage emotions embedded in tweets besides assimilating the content-based information from the textual and visual modalities of social media posts to perform a sentiment classification. We mitigate the problem of small training sets by implementing data augmentation techniques. A word-level attention mechanism is applied on the textual modality for contextual inference and filtering out noisy words from tweets. Experimental outcomes on real-world social media datasets demonstrate that our proposed method outperforms the existing methods for Marathi sentiment analysis in resource-constrained circumstances.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",emotions; Marathi; multimodal data classification; Sentiment analysis,Classification (of information); Modal analysis; Social networking (online); Data classification; Emotion; Marathi; Multi-modal; Multi-modal data; Multimodal data classification; Sentiment analysis; Social media; Social media platforms; User-generated; Sentiment analysis
Morpheme-Based Neural Machine Translation Models for Low-Resource Fusion Languages,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173578876&doi=10.1145%2f3610773&partnerID=40&md5=f06deabc7d54528d79048845fc51e8dd,"Neural approaches, which are currently state-of-the-art in many areas, have contributed significantly to the exciting advancements in machine translation. However, Neural Machine Translation (NMT) requires a substantial quantity and good quality parallel training data to train the best model. A large amount of training data, in turn, increases the underlying vocabulary exponentially. Therefore, several proposed methods have been devised for relatively limited vocabulary due to constraints of computing resources such as system memory. Encoding words as sequences of subword units for so-called open-vocabulary translation is an effective strategy for solving this problem. However, the conventional methods for splitting words into subwords focus on statistics-based approaches that mainly conform to agglutinative languages. In these languages, the morphemes have relatively clean boundaries. These methods still need to be thoroughly investigated for their applicability to fusion languages, which is the main focus of this article. Phonological and orthographic processes alter the borders of constituent morphemes of a word in fusion languages. Therefore, it makes it difficult to distinguish the actual morphemes that carry syntactic or semantic information from the word's surface form, the form of the word as it appears in the text. We, thus, resorted to a word segmentation method that segments words by restoring the altered morphemes. We also compared conventional and morpheme-based NMT subword models. We could prove that morpheme-based models outperform conventional subword models on a benchmark dataset. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",fusion languages; low-resource languages; morpheme-based word segmentation; Neural machine translation; transformers,Computational linguistics; Computer aided language translation; Semantics; Fusion language; Low resource languages; Machine translation models; Machine translations; Morpheme-based word segmentation; State of the art; Subword modeling; Training data; Transformer; Word segmentation; Neural machine translation
Detection of Hateful Social Media Content for Arabic Language,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173281941&doi=10.1145%2f3592792&partnerID=40&md5=39b69c140708234f9e602a8e187bfc7e,"Social media is a common medium for expression of views, discussion, sharing of content, and promotion of products and ideas. These views are either polite or obscene. The growth of hate speech is one of the negative aspects of the medium and its emergence poses risk factors for society at various levels. Although there are rules and laws for these platforms, they cannot oversee and control all types of content. Thus, there is an urgent need to develop modern algorithms to automatically detect hateful content on social media. Arab society is not isolated from the world, and the usage of social media by its members has highlighted the importance of automated systems that help build an electronic society free of hate and aggression. This article aims to detect hate speech based on Arabic context over the Twitter platform by proposing different novel deep learning architectures in order to provide a thorough analytical study. Also, a comparative study is presented with a different well-known machine learning algorithm, as well as other state-of-the-art algorithms from the literature to be used as a beacon for interested researchers. These models have been applied to the Arabic tweets dataset, which included 15K tweets and 14 features. After training these models, the results obtained for the top two models included an improved bidirectional long short-term memory with an accuracy of 92.20% and a macro F1-score of 92% and a modified convolutional neural network with an accuracy of 92.10% and a macro F1-score of 91%. The results also showed the superiority of the performance of the deep learning models over other models in terms of accuracy.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Arabic language; Arabic tweets; classification; deep learning; Hate speech; machine learning,Automation; Convolutional neural networks; Learning algorithms; Learning systems; Social networking (online); Speech recognition; Arabic languages; Arabic tweet; Automated systems; Deep learning; F1 scores; Hate speech; Machine-learning; Media content; Risk factors; Social media; Deep learning
Improving Sequence-to-sequence Tibetan Speech Synthesis with Prosodic Information,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173245392&doi=10.1145%2f3616012&partnerID=40&md5=d2c6467767f7799b851e4014ddc95cf3,"There are about 6,000 languages worldwide, most of which are low-resource languages. Although the current speech synthesis (or text-to-speech, TTS) for major languages (e.g., Mandarin, English, French) has achieved good results, the voice quality of TTS for low-resource languages (e.g., Tibetan) still needs to be further improved. Because prosody plays a significant role in natural speech, the article proposes two sequence-to-sequence (seq2seq) Tibetan TTS models with prosodic information fusion to improve the voice quality of synthesized Tibetan speech. We first constructed a large-scale Tibetan corpus for seq2seq TTS. Then we designed a prosody generator to extract prosodic information from the Tibetan sentences. Finally, we trained two seq2seq Tibetan TTS models by fusing prosodic information, including feature-level and model-level prosodic information fusion. The experimental results showed that the proposed two seq2seq Tibetan TTS models, which fuse prosodic information, could effectively improve the voice quality of synthesized speech. Furthermore, the model-level prosodic information fusion only needs 60% ∼ 70% of the training data to synthesize a voice similar to the baseline seq2seq Tibetan TTS. Therefore, the proposed prosodic information fusion methods can improve the voice quality of synthesized speech for low-resource languages.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",low-resource language; prosodic information fusion; Sequence-to-sequence speech synthesis; Tibetan speech synthesis,Information fusion; Low resource languages; Prosodic information fusion; Prosodics; Sequence-to-sequence speech synthesis; Speech models; Synthesized speech; Text to speech; Tibetan speech synthesis; Tibetans; Voice quality; Speech synthesis
WAD-X: Improving Zero-shot Cross-lingual Transfer via Adapter-based Word Alignment,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173272816&doi=10.1145%2f3610289&partnerID=40&md5=78b496a3ee687cfdf185227bd3153ff1,"Multilingual pre-trained language models (mPLMs) have achieved remarkable performance on zero-shot cross-lingual transfer learning. However, most mPLMs implicitly encourage cross-lingual alignment in pre-training stage, making it hard to capture accurate word alignment across languages. In this paper, we propose Word-align ADapters for Cross-lingual transfer (WAD-X) to explicitly align word representations of mPLMs via language-specific subspace. Taking a mPLM as the backbone model, WAD-X constructs subspace for each source-target language pair via adapters. The adapters use statistical alignment as the prior knowledge to guide word-level aligning in the corresponding bilingual semantic subspace. We evaluate our model across a set of target languages on three zero-shot cross-lingual transfer tasks: part-of-speech tagging (POS), dependency parsing (DP), and sentiment analysis (SA). Experimental results demonstrate that our proposed model improves zero-shot cross-lingual transfer on three benchmarks, with improvements of 2.19, 2.50, and 1.61 points in POS, DP, and SA tasks over strong baselines.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adapter; Cross-lingual transfer; low-resource languages; word alignment,Computational linguistics; Semantics; Transfer learning; Zero-shot learning; Adapter; Cross-lingual; Cross-lingual transfer; Dependency parsing; Language model; Low resource languages; Part of speech tagging; Parts-of-speech tagging; Target language; Word alignment; Sentiment analysis
Extractive Summarization of Telugu Text Using Modified Text Rank and Maximum Marginal Relevance,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173213840&doi=10.1145%2f3600224&partnerID=40&md5=082225194b80c054daf67f04680fc08b,"With the rapid growth of digital content, there is a need for an automatic text summarizer to provide short text from a long text document. Many research works have been presented for extractive text summarization (ETS). This article mainly focuses on the graph-based ETS approach for multiple Telugu text documents. A modified Text-Rank algorithm is employed with the noun and verb count of each sentence in the text as the initial score of each node. To get the optimal features, a novel feature selection algorithm called improved Flamingo Search Algorithm is proposed in this article. Though graph-based ETS is an important approach, the generated summaries are redundant. To reduce the redundancy in the generated summary, maximum marginal relevance is combined with the modified Text-Rank. Different word-embedding techniques such as Fast-Text, Word2vec, TF-IDF, and one-hot encoding are utilized to experiment with the proposed approach. The performance of the proposed text summarization approach is evaluated with BLEU and ROUGE in terms of F-measure, precision, and recall.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; encoding technique; improved flamingo search algorithm; Telugu text summarizer,Deep learning; Graphic methods; Learning algorithms; Signal encoding; Text processing; Deep learning; Encoding techniques; Extractive summarizations; Graph-based; Improved flamingo search algorithm; Rapid growth; Search Algorithms; Telugu text summarizer; Text document; Text Summarisation; Encoding (symbols)
Human-Computer Collaborative Visual Design Creation Assisted by Artificial Intelligence,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173222826&doi=10.1145%2f3554735&partnerID=40&md5=f48b126c0c0a243abd6a8b09745bed60,"With the support and promotion of big data and cloud computing, AI has penetrated into every field of people's lives more and more deeply, with its characteristics of sustainable work, extremely fast computing speed, and a certain intelligence. This is an effective way to solve the general lack of demand and productivity of visual design, and relieve the pressure off designers to deal with relatively low-quality and high-demand designs. Therefore, the combination of design and artificial intelligence technology is a necessity. Research on the application of artificial intelligence technology for visual design is also in full swing at home and abroad However, at present, teams at home and abroad are in the exploratory stage. This paper considers whether it is possible to build an intelligent visual design and creation system by using artificial intelligence technology to help graphic communication designers achieve high-quality, high-efficiency, and high-quantity design output. Additionally, this paperexplores how to combine artificial intelligence technology with designers' design workflow so as to form a complementary human-computer cooperation mode. We will explore how to integrate AI technology with designers' design workflow and then create a human-machine collaboration model with complementary advantages to achieve the high quality, high efficiency, and high quantity of design output required by the intelligent visual design creation system being built. Finally, a basic framework of a generative smart human-computer collaborative visual design creation system based on a subset of neural network expert systems in multiple domains and an aggregate of different modules supported by the system is formed, and the working principle and usage process of the system are further elaborated with the example of packaging design.  © 2023 Association for Computing Machinery.",Artificial intelligence; collaboration model; cooperation mode; creation system; human-computer collaboration; packaging design; visual design creation,Expert systems; Visual communication; Artificial intelligence technologies; Collaboration models; Cooperation mode; Creation system; High quality; Higher efficiency; Human-computer collaboration; Packaging designs; Visual design; Visual design creation; Efficiency
North Korean Neural Machine Translation through South Korean Resources,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173229113&doi=10.1145%2f3608947&partnerID=40&md5=47b371138b2174f8d39e588b59df1a72,"South and North Korea both use the Korean language. However, Korean natural language processing (NLP) research has mostly focused on South Korean language. Therefore, existing NLP systems in the Korean language, such as neural machine translation (NMT) systems, cannot properly process North Korean inputs. Training a model using North Korean data is the most straightforward approach to solving this problem, but the data to train NMT models are insufficient. To solve this problem, we constructed a parallel corpus to develop a North Korean NMT model using a comparable corpus. We manually aligned parallel sentences to create evaluation data and automatically aligned the remaining sentences to create training data. We trained a North Korean NMT model using our North Korean parallel data and improved North Korean translation quality using South Korean resources such as parallel data and a pre-trained model. In addition, we propose Korean-specific pre-processing methods, character tokenization, and phoneme decomposition to use the South Korean resources more efficiently. We demonstrate that the phoneme decomposition consistently improves the North Korean translation accuracy compared to other pre-processing methods.  © 2023 Copyright held by the owner/author(s).",Low resource; north korean machine translation; parallel data construction; pre-process,Computational linguistics; Computer aided language translation; Natural language processing systems; Data construction; Korean language; Low resource; Machine translation models; Machine translations; North Korean; North korean machine translation; Parallel data; Parallel data construction; Pre-process; Neural machine translation
Improving Generative Adversarial Network-based Vocoding through Multi-scale Convolution,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173582249&doi=10.1145%2f3610532&partnerID=40&md5=9db88e1ae240639698f3fca29713a3a9,"Vocoding is a sub-process of text-to-speech task, which aims at generating audios from intermediate representations between text and audio. Several recent works have shown that generative adversarial network- (GAN) based vocoders can generate audios with high quality. While GAN-based neural vocoders have shown higher efficiency in generating speed than autoregressive vocoders, the audio fidelity still cannot compete with ground-truth samples. One major cause of the degradation in audio quality and spectrogram vague comes from the average pooling layers in discriminator. As the multi-scale discriminator commonly used by recent GAN-based vocoders applies several average pooling layers to capture different-frequency bands, we believe it is crucial to prevent the high-frequency information from leakage in the average pooling process. This article proposes MSCGAN, which solves the above-mentioned problem and achieves higher-fidelity speech synthesis. We demonstrate that substituting the average pooling process with a multi-scale convolution architecture effectively retains high-frequency features and thus forces the generator to recover audio details in time and frequency domain. Compared with other state-of-the-art GAN-based vocoders, MSCGAN can produce competitive audio with a higher spectrogram clarity and mean opinion score score in subjective human evaluation. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",neural vocoder; Speech generation,Convolution; Frequency domain analysis; Spectrographs; Speech synthesis; Vocoders; High quality; Higher efficiency; Intermediate representations; Multi-scales; Network-based; Neural vocod; Spectrograms; Speech generation; Sub process; Text to speech; Generative adversarial networks
EnML: Multi-label Ensemble Learning for Urdu Text Classification,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173253160&doi=10.1145%2f3616111&partnerID=40&md5=0a4d0dd9b814926082d9a8c914757919,"Exponential growth of electronic data requires advanced multi-label classification approaches for the development of natural language processing (NLP) applications such as recommendation systems, drug reaction detection, hate speech detection, and opinion recognition/mining. To date, several machine and deep learning-based multi-label classification methodologies have been proposed for English, French, German, Chinese, Arabic, and other developed languages. Urdu is the 11th largest language in the world and has no computer-aided multi-label textual news classification approach. Unlike other languages, Urdu is lacking multi-label text classification datasets that can be used to benchmark the performance of existing machine and deep learning methodologies. With an aim to accelerate and expedite research for the development of Urdu multi-label text classification-based applications, this article provides multiple contributions as follows: First, it provides a manually annotated multi-label textual news classification dataset for the Urdu language. Second, it benchmarks the performance of traditional machine learning approaches particularly by adapting three data transformation approaches along with three top-performing machine learning classifiers and four algorithm adaptation-based approaches. Third, it benchmarks performance of 16 existing deep learning approaches and the four most widely used language models. Finally, it provides an ensemble approach that reaps the benefits of three different deep learning architectures to precisely predict different classes associated with a particular Urdu textual document. Experimental results reveal that proposed ensemble approach performance values (87% accuracy, 92% F1-score, and 8% hamming loss) are significantly higher than adapted machine and deep learning-based approaches.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data transformation methods; deep learning; language models; multi-label ensemble learning; multi-label Urdu news dataset; Multi-label Urdu text classification; traditional machine learning,Classification (of information); Computational linguistics; Deep learning; Learning algorithms; Learning systems; Metadata; Sentiment analysis; Speech recognition; Data transformation method; Datum transformation; Deep learning; Ensemble learning; Language model; Machine-learning; Multi-label ensemble learning; Multi-label urdu news dataset; Multi-label urdu text classification; Multi-labels; Text classification; Traditional machine learning; Transformation methods; Benchmarking
Can Same-right-and-different-left Gestures Be Recognized with Only Right-hand Signals?,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173286982&doi=10.1145%2f3617370&partnerID=40&md5=63eaebdd2eccaf50f32930ae93940771,"Sign language serves as a bridge between the hearing-impaired and other people. Existing sensor-based approaches tend to only collect data from the dominant hand. Does this signal collection method affect the accuracy of gesture recognition, especially gestures where the dominant hand has the same movement while the non-dominant hand has different movements? The specific gestures are called same-right-and-different-left (SRDL) where the right hand is dominant. This article is the first to propose an SRDL-aware sign language recognition system. First, an SRDL discriminator based on an autoencoder and range classifier is designed to determine whether the gesture is SRDL. Second, an SRDL feature selector based on clustering relationship is presented. Multivariate variational mode decomposition and fast fourier transform are used to obtain the feature expression. Moreover, a clustering relationship algorithm is proposed to dynamically select features for every group of SRDL gestures in the feature expression. Finally, the experimental results show that the average word error rate is 14.3% and decreases by 8.5% and 12.1% compared with Signspeaker and MyoSign, respectively.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",autoencoder; clustering relationship; Same-right-and-different-left; scattered point; sign language,Audition; Clustering algorithms; Fast Fourier transforms; Fourier series; Gesture recognition; Learning systems; Palmprint recognition; Speech recognition; Auto encoders; Clustering relationship; Clusterings; Collection methods; Feature expression; Hearing impaired; Same-right-and-different-leave; Scattered points; Sign language; Signal collection; Variational mode decomposition
Query Context Expansion for Open-Domain Question Answering,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170203040&doi=10.1145%2f3603498&partnerID=40&md5=c6f278099923c40edd098fbdd166acb5,"Humans are accustomed to autonomously associating prior knowledge with the text in a query when answering questions. However, for machines lacking cognition and common sense, a query is merely a combination of some words. Although we can enrich the semantic information of the given query through language representation or query expansion (QE), the information contained in the query is still insufficient. In this paper, we propose an effective passage retrieval method named query context expansion-based retrieval (QCER) for open-domain question answering (OpenQA). QCER associates a query with domain information by adding contextual association information based on the pseudo-relevance feedback (PRF). QCER uses a dense reader to select top-n expansion terms for QE. We implement QCER by appending reader predictions, theoretically present in candidate passages, as contextual information to the initial query to form the new query. QCER with sparse representations (BM25) can improve retrieval efficiency and accelerate query convergence so that the reader can find the desired answer using fewer relevant passages, e.g., 10 passages, as soon as possible. Moreover, QCER can be easily combined with dense passage retrieval (DPR) to achieve even better performance, as sparse and dense representations are often complementary. Remarkably, we demonstrate that QCER achieves state-of-The-Art performance in three tasks, passage retrieval, passage reading, and passage reranking, on the Natural Questions (NQ) and TriviaQA (Trivia) datasets under an extractive QA setup.  © 2023 Copyright held by the owner/author(s).",Open-domain question answering; passage retrieval; pseudo-relevance feedback; query expansion,Information retrieval; Natural language processing systems; Semantics; Common sense; Domain informations; Open domain question answering; Passage retrieval; Prior-knowledge; Pseudo-relevance feedbacks; Query context; Query expansion; Retrieval methods; Semantics Information; Query processing
Construction of Mizo: English Parallel Corpus for Machine Translation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170648773&doi=10.1145%2f3610404&partnerID=40&md5=000482c5004c635298295c6573594e34,"Parallel corpus is a key component of statistical and Neural Machine Translation (NMT). While most research focuses on machine translation, corpus creation studies are limited for many languages, and no research paper on a Mizo-English corpus exists yet. A high-quality parallel corpus is required for Natural Language Processing activities including machine translation, Chatbots, Transliteration, and Cross-Language Information Retrieval. This work aims to investigate parallel corpus creation techniques and apply them to the Mizo-English language pair. Another goal is to test machine translation on the newly constructed corpus. We contributed to LF Aligner tool to support Mizo language for Mizo sentence alignment in corpus development. Our effort created the first large-scale Mizo-English parallel corpus with over 529K sentences. The pre-processed corpus was used for Mizo-To-English NMT. It was evaluated using BLEU, Character F1 Score (ChrF), and Translation Edit Rate (TER) scores. Our system achieved BLEU 45.08, ChrF 65.36, and TER 41.16, setting a new benchmark for Mizo-To-English translation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bilingual corpus; corpus construction; machine translation; Mizo; parallel text,Computer aided language translation; Natural language processing systems; Neural machine translation; Bilingual corpora; Corpus construction; F1 scores; Machine translations; Mizo; On-machines; Parallel corpora; Parallel text; Research focus; Research papers; Computational linguistics
ICON: A Linguistically-Motivated Large-Scale Benchmark Indonesian Constituency Treebank,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170647040&doi=10.1145%2f3609798&partnerID=40&md5=0143e045627d0684e43de4941fc28161,"Constituency parsing is an important task of informing how words are combined to form sentences. While constituency parsing in English has seen significant progress in the last few years, tools for constituency parsing in Indonesian remain few and far between. In this work, we publish ICON (Indonesian CONstituency treebank), the hitherto largest publicly available manually-Annotated benchmark Indonesian constituency treebank with a size of 10,000 sentences and approximately 124,000 constituents and 182,000 tokens, which can support the training of state-of-The-Art transformer-based models. As part of the process of building the treebank, we review and revamp the constituent and POS tagsets in use in existing treebanks to ensure that the labels are relevant and suitable for the grammatical features of Indonesian. We establish strong baselines on the ICON dataset using the Berkeley Neural Parser with transformer-based pre-Trained embeddings, with the best performance of 88.85% F1 score coming from our own version of SpanBERT (IndoSpanBERT). We further analyze the predictions made by our best-performing model to reveal certain idiosyncrasies in Indonesian that pose challenges for constituency parsing.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Constituency parsing; corpus annotation; deep learning; Indonesian corpus; neural parser; treebank,Computational linguistics; Deep learning; Constituency parsing; Corpus annotations; Deep learning; Embeddings; Indonesian corpus; Large-scales; Neural parse; Performance; State of the art; Treebanks; Syntactics
Speech Feature Enhancement based on Time-frequency Analysis,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170638584&doi=10.1145%2f3605549&partnerID=40&md5=4c8016fcec5bd87f560747e5542bb1f3,"Time-frequency analysis (TFA) is a powerful method to exploit the hidden information of signals, including speech signals. Many techniques in this group were invented and developed to capture the most crucial stationary feature. However, human speech is not stable, and it contains some non-stationary elements. This work aims to design a new algorithm via the TFA technique to extract the trends and changes inside the speech signal in the time-frequency (TF) plane. We design a new algorithm to create a set of atoms for the signal transform, which can analyze the signal in many different view directions via Poly-Linear Chirplet Transform (PLCT). After processing the signal, the proposed method returns a multichannel output in which each channel results from a particular Linear Chirplet Transform (LCT). The feature then is combined with the MFCC feature to form the final representation. Although the size for speech representation rises, our extracted feature contains rich-meaning information to improve the recognition results compared to other features in gender recognition, dialect recognition, and speaker recognition.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Chirplet Transform; instantaneous frequency; multichannel representation; poly-linear chirplet transform; Speech feature; time-frequency analysis,Speech communication; Chirplet transforms; Instantaneous frequency; Multi channel; Multichannel representation; Poly-linear chirplet transform; Speech feature enhancement; Speech features; Speech signals; Time-frequency Analysis; Speech recognition
Investigating Unsupervised Neural Machine Translation for Low-resource Language Pair English-Mizo via Lexically Enhanced Pre-Trained Language Models,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170648022&doi=10.1145%2f3609222&partnerID=40&md5=d85994e7f84a60c059116a8edb4a0fd2,"The vast majority of languages in the world at present are considered to be low-resource languages. Since the availability of large parallel data is crucial for the success of most modern machine translation approaches, improving machine translation for low-resource languages is a key challenge. Most unsupervised techniques for translation benefit closely related languages with monolingual data of substantial quantity. To facilitate research in this direction for the extremely low resource language pair English (en) and Mizo (lus), we have developed a parallel and monolingual corpus for the Mizo language from various news websites. We explore Unsupervised Neural Machine Translation (UNMT) based on the developed monolingual data. We observe that cross-lingual embedding (CLWE) initializations on subword segmented data during pre-Training, based on both masked language modelling and sequence-To-sequence generation tasks, improve translation performance. We experiment with cross-lingual alignment and combined alignment and joint training for learning the cross-lingual embedding representations. We also report baseline performances and the impact of CLWE initialization using semi-supervised and supervised neural machine translation. Empirical results show that both CLWE initializations work well for the distant pair English-Mizo compared to the baselines.  © 2023 Copyright held by the owner/author(s).",cross-lingual word embeddings; low resource languages; Mizo; Unsupervised neural machine translation,Computational linguistics; Computer aided language translation; Embeddings; Modeling languages; Cross-lingual; Cross-lingual word embedding; Embeddings; Language model; Language pairs; Low resource languages; Machine translations; Mizo; Parallel data; Unsupervised neural machine translation; Neural machine translation
Deep Learning-based Sequence Labeling Tools for Nepali,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170644843&doi=10.1145%2f3606696&partnerID=40&md5=06a6a5a0c049de7135045691e919e98e,"A Part-of-Speech (POS) tagger and Chunker (or shallow parser) are sequence labeling tools, crucial for improving the accuracy of Natural Language Processing (NLP) tasks like parsing, named entity recognition, sentiment analysis, information extraction, and so on. Developing such tools for a low-resource language is an arduous task. Nepali is a relatively resource-poor Indian language and has not been able to evolve from a computational perspective. Therefore, we present effective part-of-speech tagging and chunking tools for the Nepali text using sequential deep learning models-Bidirectional Long Short-Term Memory Network with a Conditional Random Field Layer (BI-LSTM-CRF) and other LSTM-based models exploring both character and word embeddings of the Nepali texts. Word Embedding has been used to capture syntactic as well as semantic information whereas character embedding has been applied to capture the morphological as well as shape information of words and also to handle the out-of-vocabulary problem. The developed chunker is the first statistical chunker for the Nepali language. A baseline model with a Conditional Random Field has also been developed to identify the optimum feature set for the aforementioned tasks. The BI-LSTM-CRF model produced an accuracy of 99.20% and 98.40%, for Nepali POS tagging and chunking, respectively. This is the highest-ever accuracy for Nepali. Thorough error analysis and observations have also been reported with examples. The developed tools can help advance research in Nepali language processing, improve the accuracy of language technology applications, and contribute to the preservation and promotion of the Nepali language.  © 2023 Copyright held by the owner/author(s).",BI-LSTM-CRF neural network; Deep learning-based Nepali tools; Nepali chunker; Nepali optimum feature set; Nepali sequence labeling tools; Nepali text feature selection,Computational linguistics; Embeddings; Feature extraction; Random processes; Semantics; Sentiment analysis; Speech recognition; Syntactics; Bidirectional long short-term memory network with a conditional random field layer neural network; Deep learning-based nepali tool; Features sets; Labeling tools; Memory network; Nepali chunker; Nepali optimum feature set; Nepali sequence labeling tool; Nepali text feature selection; Neural-networks; Random fields; Sequence Labeling; Text feature selections; Long short-term memory
Mexican Sign Language Corpus: Towards an Automatic Translator,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170639496&doi=10.1145%2f3591471&partnerID=40&md5=1b698d24f8b1b555c8ee8674c2602f68,"The development of the Sign Language Corpus has been motivated by its great utility and application to various purposes and research areas. However, some countries do not have their own Sign Language Corpus. Developing a corpus thereby benefits the community of people with speech disabilities in diverse areas such as education. Thus, the motivation to develop this work is to present an advance toward constructing an RGB-D corpus of Mexican Sign Language captured by a Kinect sensor. A total of 90,000 samples of 570 words and 30 phrases interpreted by 150 people who commonly use Mexican Sign Language were collected. Of the participants, 86 were women and 64 were men, aged between 12 and 60 years old. The Mexican Sign Language Corpus was recorded by signers from three different regions of the south of Mexico. The constructed corpus contains depth, color, point clouds, and human skeleton positions. Six hundred of the most used words were selected from 17 semantic fields, considering the variability in the movement of both hands. After training a neural network, the performance developed by the recognition system was 98.62%.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Applied linguistics; corpus; Mexican Sign Language; sign recognition system,Pattern recognition; Applied linguistic; Automatic translators; Corpus; Mexican sign language; Recognition systems; Research areas; Sign language; Sign recognition; Sign recognition system; Speech disabilities; Semantics
Automatic Idiom Identification Model for Amharic Language,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170648803&doi=10.1145%2f3606864&partnerID=40&md5=05ccbb65587ea232ca9b00389db57548,"Idiomatic expressions are important natural parts of all languages and prominent parts of our daily speech. Idioms cannot be interpreted from the words that they are formed with directly and people may not understand the meaning. From past literature, it was noted that idiom affects Natural Language Processing research like machine translation, semantic analysis, and sentiment analysis. Other languages like English, Chinese, and Indian idioms are recognized through different methods in different research. As there is no standard method and research to identify Amharic idioms, this study is aimed to build a model to identify idioms for the Amharic language using a supervised machine learning approach. The study used 800 labeled expressions for training and 200 expressions for testing from Amharic idiom books and different Amharic documents. To measure the performance of the model, we used accuracy, precision, recall, and F-score. Finally, a 97.5% accuracy result was achieved from the testing dataset showing a promising result. The study contributes to the information systems discourse about improving the awareness and knowledge of researchers on Amharic idioms.  © 2023 Copyright held by the owner/author(s).",Amharic idioms; Idiom recognition; Natural Language Processing; supervised machine learning,Semantics; Statistical tests; Supervised learning; Amharic idiom; Identification modeling; Idiom recognition; Idiomatics; Language processing; Machine translations; Natural language processing; Natural languages; Semantic analysis; Supervised machine learning; Sentiment analysis
Arabic ChatGPT Tweets Classification Using RoBERTa and BERT Ensemble Model,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170642788&doi=10.1145%2f3605889&partnerID=40&md5=4c41a7e90ee65dcf08ec4131417a6692,"ChatGPT OpenAI, a large-language chatbot model, has gained a lot of attention due to its popularity and impressive performance in many natural language processing tasks. ChatGPT produces superior answers to a wide range of real-world human questions and generates human-like text. The new OpenAI ChatGPT technology may have some strengths and weaknesses at this early stage. Users have reported early opinions about the ChatGPT features, and their feedback is essential to recognize and fix its shortcomings and issues. This study uses the ChatGPT tweets Arabic dataset to automatically find user opinions and sentiments about ChatGPT technology. The dataset is preprocessed and labeled using the TextBlob Arabic Python library into positive, negative, and neutral tweets. Despite extensive works for the English language, languages like Arabic are less studied regarding tweet analysis. Existing literature about Arabic tweet sentiment analysis has mainly focused on machine learning and deep learning models. We collected a total of 27,780 unstructured tweets from Twitter using the Tweepy SNscrape Python library using various hash-Tags such as # Chat-GPT, #OpenAI, #Chatbot, Chat-GPT3, and so on. To enhance the model's performance and reduce computational complexity, unstructured tweets are converted into structured and normalized forms. Tweets contain missing values, URL and HTML tags, stop words, punctuation, diacritics, elongations, and numeric values that have no impact on the model performance; hence, these increase the computational cost. So, these steps are removed with the help of Python preprocessing libraries to enhance text quality and consistency. This study adopts Transformer-based models such as RoBERTa, XLNet, and DistilBERT that automatically classify the tweets. Additionally, a hybrid transformer-based model is proposed to obtain better results. The proposed hybrid model is developed by combining the hidden outputs of the RoBERTA and BERT models using a concatenation layer, then adding dense layers with ""Relu""activation employed as a hidden layer to create non-linearity and a ""softmax""activation function for multiclass classification. They differ from existing state-of-The-Art models due to the enhanced capabilities of both models in text classification. Hybrid models combine the different models to make accurate predictions and reduce bias and enhanced the overall results, while state-of-The-Art models are incapable of making accurate predictions. Experiments show that the proposed hybrid model achieves 96.02% accuracy, 100% precision on negative tweets, and 99% recall for neutral tweets. The performance of the proposed model is far better than existing state-of-The-Art models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Arabic tweets; BERT; ChatGPT; low-resource language; OpenAI; sentiment analysis; transformer models,Chemical activation; Classification (of information); Deep learning; High level languages; Learning algorithms; Learning systems; Sentiment analysis; Arabic tweet; ART model; BERT; ChatGPT; Hybrid model; Low resource languages; Openai; Sentiment analysis; State of the art; Transformer modeling; Python
So2al-wa-Gwab: A New Arabic Question-Answering Dataset Trained on Answer Extraction Models,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170639137&doi=10.1145%2f3605550&partnerID=40&md5=051e39f1c286e6ce523ca95121744728,"Question answering (QA) is the task of responding to questions posed by users automatically. A question-Answering system is divided into three main components: question analysis, information retrieval, and answer extraction. This paper has focused only on the answer extraction part. In the past couple of years, many QA systems have been developed and become mature and ready for use in different languages. Nevertheless, the advancement of Arabic QA systems still faces different obstacles and a lack of relevant resources and tools for researchers. This paper presents the So2al-wa-Gwab dataset since the publicly available datasets include various faults, such as the use of machine translation to build the data, a short context size, and a small number of question-Answer pairings. Thus, this new dataset avoids the aforementioned drawbacks. Furthermore, in this paper, we have trained three deep learning models, namely, Bi-Directional flow network (BiDAF), QA Network (QANet), and BERT model, and tested them on seven different datasets, thus providing a comprehensive comparison between existing Arabic QA datasets. The obtained results emphasize that machine-Translated datasets fall back when compared with human-Annotated data. Also, the QA task becomes harder as the context, from which to extract the answer, becomes larger.  © 2023 Copyright held by the owner/author(s).",Arabic question answering; BERT; BiDAF; neural networks; QANet; Question answering datasets,Computational linguistics; Computer aided language translation; Deep learning; Extraction; Information retrieval; Machine translation; Arabic question answering; BERT; Bi-directional flow network; Bi-directional flows; Flow network; Neural-networks; QA network; Question Answering; Question answering dataset; Search engines
The Contribution of Selected Linguistic Markers for Unsupervised Arabic Verb Sense Disambiguation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170649336&doi=10.1145%2f3605777&partnerID=40&md5=dbda732c0c6b8071f9cffdfcc3b05eab,"Word sense disambiguation (WSD) is the task of automatically determining the meaning of a polysemous word in a specific context. Word sense induction is the unsupervised clustering of word usages in a different context to distinguish senses and perform unsupervised WSD. Most studies consider function words as stop words and delete them in the pre-processing step. However, function words can encode meaningful information that can help to improve the performance ofWSD approaches.We propose in this work a novel approach to solve Arabic verb sense disambiguation that is based on a preposition-based classification that is used in an automatic word sense induction step to build sense inventories to disambiguate Arabic verbs. However, in the wake of the success of neural language models, recent works obtained encouraging results using BERT pre-trained models for English-language WSD approaches. Hence, we use contextualized word embeddings for an unsupervised Arabic WSD that is based on linguistic markers and uses sentence-BERT Transformer pre-trained models, which yields encouraging results that outperform other existing unsupervised neural AWSD approaches. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Linguistics; Natural language processing systems; Function words; Language model; Performance; Polysemous word; Pre-processing step; Sense inventories; Stop word; Unsupervised clustering; Word Sense Disambiguation; Word sense inductions; Classification (of information)
A Systematic Literature Review on Vietnamese Aspect-based Sentiment Analysis,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170642875&doi=10.1145%2f3610226&partnerID=40&md5=67df0d39c1a93e8ffabcd2b79927e4dd,"Aspect-based sentiment analysis (ABSA) is one of the principal tasks in the automatic deep understanding of texts, widely applied in a broad range of real-world applications. Many studies have been performed on different tasks and datasets for other languages (e.g., English, Chinese) to address this topic. For Vietnamese language, this topic has been attracting considerable interest in recent years. However, we found that many studies tend to repeat the research instead of inheriting and extending the previous works. Moreover, previous studies' methods of comparison or evaluation metrics have not shown consistency and connection. This might restrict the development of future studies on this research topic. To the best of our knowledge, no research has been conducted to overview the existing studies for the ABSA research in Vietnamese language. The primary objective of this study is to provide a systematic and comprehensive review of the current Vietnamese ABSA research. More specifically, we analyze the early approaches, evaluation metrics, and available published benchmark datasets used in the Vietnamese ABSA task. We also discuss the challenge and recommend potential future directions for Vietnamese ABSA. This work is expected to provide readers with a wealth of knowledge, the research gap, and the challenges in the Vietnamese ABSA field.  © 2023 Copyright held by the owner/author(s).",Aspect-based sentiment analysis; datasets; methods; systematic literature review; Vietnamese language,Aspect-based sentiment analyse; Dataset; Evaluation metrics; Method; Real-world; Sentiment analysis; Study methods; Systematic literature review; Vietnamese; Vietnamese language; Sentiment analysis
Exploiting Functional Discourse Grammar to Enhance Complex Arabic Relation Extraction using a Hybrid Semantic Knowledge Base-Machine Learning Approach,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170645743&doi=10.1145%2f3610581&partnerID=40&md5=cf56d47c58c05a1a155da06d674bfd36,"Relation extraction from unstructured Arabic text is especially challenging due to the Arabic language complex morphology and the variation in word semantics and lexical categories. The research documented in this paper presents a hybrid Semantic Knowledge base-Machine Learning (SKML) approach for extracting complex Arabic relations from unstructured Arabic documents; the proposed approach exploits the principles of Functional Discourse Grammar (FDG) to emphasise the semantic and pragmatic properties of the language and facilitate the identification of relation elements. At the initial phase, the novel FDG-SKML relation extraction approach deploys a lexical-based mechanism that utilises a purposely built domain-specific Semantic Knowledge to encode the semantic association between the identified relations' elements. The evaluation of the initial stage evidenced improved accuracy for extracting most complex Arabic relations. The initial relation extraction mechanism was further extended by integrating its output into a Machine Learning classifier that facilitated extracting especially complex relations with significant disparity in the relation elements' presence, order, and correlation. Using Economics as the problem domain, experimental evaluation evidenced the high accuracy of our FDG-SKML approach in complex Arabic relation extraction task and demonstrated its further improvement upon integration with machine learning classifiers.  © 2023 Copyright held by the owner/author(s).",Arabic relation extraction; Functional Discourse Grammar; hybrid knowledge-based machine learning classification; Natural Language Processing; semantic web base,Extraction; Knowledge based systems; Learning algorithms; Natural language processing systems; Semantic Web; Arabic relation extraction; Functional discourse grammar; Hybrid knowledge; Hybrid knowledge-based machine learning classification; Knowledge based; Language processing; Machine learning classification; Natural language processing; Natural languages; Relation extraction; Semantic web base; Semantic-Web; Machine learning
Prompt-based for Low-Resource Tibetan Text Classification,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170651504&doi=10.1145%2f3603168&partnerID=40&md5=9c1ac47ba01936e311a6dbff669361c7,"Text classification is a critical and foundational task in Tibetan natural language processing, it plays a crucial role in various applications, such as sentiment analysis and information extraction. However, the limited availability of annotated data poses a significant challenge to Tibetan natural language processing. This paper proposes a prompt learning-based method for low-resource Tibetan text classification to overcome this challenge. This method utilizes pre-Trained language models to learn text representation and generation capabilities on a large-scale unsupervised Tibetan corpus, enabling few-shot Tibetan text classification. Experimental results demonstrate that the proposed method significantly improves the performance of Tibetan text classification in low-resource scenarios. This work provides a new research idea and method for low-resource language processing, such as Tibetan natural language processing. Hopefully, it will inspire subsequent work on low-resource language processing.  © 2023 Copyright held by the owner/author(s).",deep learning; pre-Trained language model; prompt learning; Tibetan text classification,Classification (of information); Computational linguistics; Deep learning; Learning systems; Deep learning; Language model; Language processing; Low resource languages; Natural languages; Pre-trained language model; Prompt learning; Text classification; Tibetan text classification; Tibetans; Sentiment analysis
Online Reviews Sentiment Analysis and Product Feature Improvement with Deep Learning,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170561309&doi=10.1145%2f3522575&partnerID=40&md5=ecd119f58190347b28fe5017ff923700,"The text mining of online reviews is currently a popular research direction of e-commerce and is considered the next blue ocean. Online reviews can dig out consumer preferences and provide theoretical guidance for the improvement of product features. However, current research mostly focuses on sentiment analysis methods and rarely involves feature extraction and large-scale data recognition. This article uses word segmentation technology to create a new feature extraction method. With the long short-Term memory neural network and latent Dirichlet allocation topic model, we propose a product feature improvement model-CESC (Consumer online reviews-Extract short text-Sentiment analysis-Cluster feature). The model can derive the product features and attitudes that consumers prefer based on consumer online reviews and use it to improve product features. According to the experimental results of three electronic products sold on the e-commerce platform, the model can effectively dig out consumer preferences for online reviews. Enterprises can improve the quality of products and services, better meet the needs of consumers, promote consumers' consumption, and achieve the enterprises' goals and values. © 2023 Association for Computing Machinery.",consumer prefer; deep learning; Online reviews; product feature improvement; sentiment analysis; text mining,Data mining; Deep learning; E-learning; Electronic commerce; Extraction; Feature extraction; Statistics; Blue oceans; Consumer prefer; Consumers' preferences; Deep learning; E- commerces; Online reviews; Product feature; Product feature improvement; Sentiment analysis; Text-mining; Sentiment analysis
Comprehending the Gossips: Meme Explanation in Time-Sync Video Comment via Multimodal Cues,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170636648&doi=10.1145%2f3612920&partnerID=40&md5=d000eca0ada133d9bb65f6cd75fab998,"Recent years have witnessed the booming of online social media platforms with embracing the popular service called ""Time-Sync Comment"", which supports the viewers to share their time-sync opinions along with video content. In this way, we observe that numerous semantically-Altered terms, or ""Memes"", were created by niche users to express their unique ideas and emotions, and further attracted a large group of viewers with better activity and enthusiasm. Unfortunately, since the memes were created based on domain-specific knowledge and semantically varied depending on the multimodal context in videos, newcomers may fail to comprehend the semantic connotation of memes, which may severely impair their user-experiences. To deal with this issue, in this article, we propose a novel meme explanation framework, called ProMDE, to automatically capture and comprehend the memes in time-sync comments, which could further benefit the viewers with meme explanation service. Specifically, we first iteratively reconstruct the original time-sync comments compared with visual embedding to detect the semantically-Altered terms as meme candidates. Afterward, based on the guides from the domain-specific corpus, visual and textual features will be fused to represent the context-Aware multimodal cues. Moreover, to accurately describe the commonly-seen homophones in memes, i.e., they have the same pronunciation but different word-spelling expressions, we integrate the phonetic symbols as an additional modality to enhance the framework. Finally, we utilize a Transformer-based decoder to generate the natural language explanation for captured memes. Extensive experiments on a large real-world dataset prove that our framework could significantly outperform several state-of-The-Art baseline methods, demonstrating the efficacy of modeling multimodal context and pronunciation for meme detection and explanation.  © 2023 Copyright held by the owner/author(s).",Meme explanation; multimodal analysis; time-sync comment,Domain Knowledge; Iterative methods; Large dataset; Modal analysis; Natural language processing systems; Social networking (online); User profile; Domain-specific knowledge; Large groups; Meme explanation; Multi-modal; Multimodal analysis; Multimodal cues; Online social medias; Social media platforms; Time-sync comment; Video contents; Semantics
DZ-SMS: An Authentic Corpus of Algerian SMS,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170648955&doi=10.1145%2f3610522&partnerID=40&md5=0555e8dbb6a1d639baf50fb2101eaea9,"In this article, a complete methodology of a corpus realization of authentic Short Message Service (SMS) from Algerian dialect and which are transcribed in Latin characters or symbols is presented. A linguistic material constituted by 6,000 SMS coming from the different geographical regions of Algeria (Middle, East, and West) corresponding to 42 administrative and geographical departments, have been collected. The coexistence of several dialects through these three regions simultaneously has obliged us to consider and operate a classification of the data for each dialect. This data classification has yielded three extracted regional dialectic corpora, each of them covering a specific number of administrative departments. These treatments are based on the so-called Data-n-gram tokenization targeting the suppression of the stop words, the stemming and the imbalance of the classes linked to the nature of the SMS. Consequently, three text classifiers based on three linear classifiers, namely, Stochastic Gradient Descent (SGD), The Ridge Regression (RDG), and Linear Support Vector Machines, to find out the number of significant corpora to extract from the collected data. A deep analysis of the results has shown that the 5-grams data representation is more representative whereas the stop-words removal and stemming process has generated an information loss that has subsequently inferred an alteration of the recognition rate of about 2%. The emerging problem of classes imbalance has been treated by using three techniques: Random Oversampling, Synthetic Minorities Oversampling Technique (SMOTE), and Adaptive Synthetic (ADASYN). This treatment produced interesting results and enhancements; particularly, the classification by region with the oversampling process SMOTE by using the RDG technique has reached a better percentage of 55.93% whereas the classification by department with the oversampling process ADASYN associated with the SGD has only yielded a maximum score of about 17.11%. The results, which undoubtedly are in favor of the classification by region, have compelled us to create three Subdialectal regional corpora, each, covering a certain number of Algerian departments.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Algerian dialects; classification; corpus; oversampling; SMS,Classification (of information); Data mining; Gradient methods; Natural language processing systems; Regression analysis; Stochastic systems; Support vector machines; Text processing; Algeria; Algerian dialect; Corpus; Middle East; Over sampling; Ridge regression; Short message services; Stochastic gradient descent; Stop word; Synthetic minority over-sampling techniques; Geographical regions
SelfSeg: A Self-supervised Sub-word Segmentation Method for Neural Machine Translation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170640028&doi=10.1145%2f3610611&partnerID=40&md5=bf578d99421e0b5edbeaf1b5b0a7e231,"Sub-word segmentation is an essential pre-processing step for Neural Machine Translation (NMT). Existing work has shown that neural sub-word segmenters are better than Byte-Pair Encoding (BPE), however, they are inefficient, as they require parallel corpora, days to train, and hours to decode. This article introduces SelfSeg, a self-supervised neural sub-word segmentation method that is much faster to train/decode and requires only monolingual dictionaries instead of parallel corpora. SelfSeg takes as input a word in the form of a partially masked character sequence, optimizes the word generation probability, and generates the segmentation with the maximum posterior probability, which is calculated using a dynamic programming algorithm. The training time of SelfSeg depends on word frequencies, and we explore several word frequency normalization strategies to accelerate the training phase. Additionally, we propose a regularization mechanism that allows the segmenter to generate various segmentations for one word. To show the effectiveness of our approach, we conduct MT experiments in low-, middle-, and high-resource scenarios, where we compare the performance of using different segmentation methods. The experimental results demonstrate that, on the low-resource ALT dataset, our method achieves more than 1.2 BLEU score improvement compared with BPE and SentencePiece, and a 1.1 score improvement over Dynamic Programming Encoding (DPE) and Vocabulary Learning via Optimal Transport (VOLT), on average. The regularization method achieves approximately a 4.3 BLEU score improvement over BPE and a 1.2 BLEU score improvement over BPE-dropout, the regularized version of BPE. We also observed significant improvements on IWSLT15 Vi→En, WMT16 Ro→En, and WMT15 Fi→En datasets and competitive results on the WMT14 De→En and WMT14 Fr→En datasets. Furthermore, our method is 17.8× faster during training and up to 36.8× faster during decoding in a high-resource scenario compared to DPE. We provide extensive analysis, including why monolingual word-level data is enough to train SelfSeg.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",efficient NLP; machine translation; self-supervised learning; subword regularization; Subword segmentation,Computational linguistics; Computer aided language translation; Decoding; Dynamic programming; Encoding (symbols); Learning systems; Signal encoding; Byte-pair encoding; Efficient NLP; Machine translations; Regularisation; Self-supervised learning; Sub words; Subword regularization; Subword segmentation; Word segmentation; Neural machine translation
Urdu Speech Emotion Recognition: A Systematic Literature Review,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167692227&doi=10.1145%2f3595377&partnerID=40&md5=22f10d2d7649a4adad3628e8a2a611d6,"Research on Speech Emotion Recognition is becoming more mature day by day, and a lot of research is being carried out on Speech Emotion Recognition in resource-rich languages like English, German, French, and Chinese. Urdu is among the top 10 languages spoken worldwide. Despite its importance, few studies have worked on Urdu Speech emotion as Urdu is recognized as a resource-poor language. The Urdu language lacks publicly available datasets, and for this reason, few researchers have worked on Urdu Speech Emotion Recognition. To the best of our knowledge, no review has been found on Urdu Speech Emotion recognition. This study is the first systematic literature review on Urdu Speech Emotion Recognition, and the primary goal of this study is to provide a detailed analysis of the literature on Urdu Speech Emotion Recognition which includes the datasets, features, pre-processing, approaches, performance metrics, and validation methods used for Urdu Speech Emotion Recognition. This study also highlights the challenges and future directions for Urdu Speech Emotion Recognition. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSpeech Emotion Recognition; deep learning; low resource language; machine learning; Urdu Speech,Deep learning; Speech recognition; Additional key word and phrasesspeech emotion recognition; Deep learning; Emotion recognition; Key words; Low resource languages; Machine-learning; Resource-Rich; Speech emotion recognition; Systematic literature review; Urdu speech; Emotion Recognition
MABERT: Mask-Attention-Based BERT for Chinese Event Extraction,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167671193&doi=10.1145%2f3597455&partnerID=40&md5=eaecba27b8f6d435682e9e45177f936b,"Event extraction is an essential but challenging task in information extraction. This task has considerably benefited from pre-trained language models, such as BERT. However, when it comes to the trigger-word mismatch problem in languages without natural delimiters, existing methods ignore the complement of lexical information to BERT. In addition, the inherent multi-role noise problem could limit the performance of methods when one sentence contains multiple events. In this article, we propose a Mask-Attention-based BERT (MABERT) framework for Chinese event extraction to address the above problems. Firstly, in order to avoid trigger-word mismatch and integrate lexical features into BERT layers directly, a mask-attention-based transformer augmented with two mask matrices is devised to replace the original one in BERT. By the mask-attention-based transformer, the character sequence interacts with external lexical semantics sufficiently and keeps its structure information at the same time. Moreover, against the multi-role noise problem, we make use of event type information from representation and classification, two aspects to enrich entity features, where type markers and event-schema-based mask matrix are proposed. Experimental results on the widely used ACE2005 dataset show the effectiveness of our proposed MABERT on Chinese event extraction task compared with other state-of-the-art methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEvent extraction; event ontology; event type markers; mask-attention-based transformer,Classification (of information); Semantics; Additional key word and phrasesevent extraction; Event ontology; Event type marker; Event Types; Events extractions; Key words; Language model; Mask-attention-based transformer; matrix; Noise problems; Noise pollution
Bagging: An Ensemble Approach for Recognition of Handwritten Place Names in Gurumukhi Script,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167704300&doi=10.1145%2f3593024&partnerID=40&md5=9e64e99f399a5463ba185fff61badcca,"In this article, the authors present an effort to recognize handwritten Gurumukhi place names for use in postal automation. Five feature extraction techniques (zoning, horizontal peak extent, vertical peak extent, diagonal, and centroid) have been analyzed and optimized using Principal Component Analysis (PCA). Four classification methods (k-Nearest Neighbor (k-NN), decision tree, random forest, and Convolutional Neural Network (CNN)) have been utilized to classify the handwritten word images. To enhance the recognition results, the authors have employed Bootstrap Aggregation (Bagging) with a majority voting scheme. The authors used a public benchmark dataset of 40,000 handwritten place-name samples in the Punjabi language for their experimental work. The experiments were conducted using a 70:30 partitioning approach, where 70% of the data was utilized for training and the remaining 30% for testing. The system achieved a maximum recognition accuracy of 96.98% by utilizing a combination of zoning, vertical peak extent, and diagonal features, and a minimum Mean Squared Error (MSE) of 0.86% based on a combination of zoning and horizontal peak extent features with a majority voting scheme through ensemble (Bagging) methodology. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPostal automation; Bagging; classification; feature extraction; feature selection; Gurumukhi words; place names,Character recognition; Convolutional neural networks; Decision trees; Extraction; Mean square error; Nearest neighbor search; Principal component analysis; Zoning; Additional key word and phrasespostal automation; Bagging; Ensemble approaches; Feature extraction techniques; Features extraction; Features selection; Gurumukhi word; Key words; Place name; Voting schemes; Feature extraction
Low-resource Multilingual Neural Translation Using Linguistic Feature-based Relevance Mechanisms,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167700700&doi=10.1145%2f3594631&partnerID=40&md5=042d96bb26f6260dff9c814d5c3e141e,"This article investigates approaches to effectively harness source-side linguistic features for low-resource multilingual neural machine translation (MNMT). Previous works focus on using various features of a word such as lemma, part-of-speech tag, dependency label, and so on, to improve translation quality in a low-resource scenario. However, these studies deal with bilingual translation and do not focus on using features in multilingual training setups. Our work focuses on this particular point and experiments with low-resource multilingual models incorporating source-side linguistic features. Although techniques for integrating features into an NMT model such as concatenation and feature relevance perform quite well in bilingual settings, they do not work well in multilingual settings. To remedy this, we propose the use of dummy features and language indicator features in MNMT models. Experiments are conducted on English to Asian language translation on a multilingual, multi-parallel corpus spanning English and eight Asian languages where for each language pair, the training data size does not exceed 20,000 parallel sentences. After establishing strong bilingual baselines using feature relevance mechanisms and multilingual baselines without any features, we show that our proposed dummy features and language indicator features, in combination with feature relevance mechanisms, yield significant improvements in BLEU points for all language pairs. We then analyze our models from the perspectives of model sizes, the impact of individual linguistic features, validation perplexity computed during training, visualization of the activations of the relevance mechanisms, and exhaustive tuning of hyperparameters. We also report preliminary results for multilingual multi-way models using linguistic features. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLinguistic features; morphology; neural networks,Computational linguistics; Computer aided language translation; Neural machine translation; Additional key word and phraseslinguistic feature; Asian languages; Bilinguals; Feature relevance; Feature-based; Key words; Language pairs; Linguistic features; Neural-networks; Part-of-speech tags; Morphology
Tokenization of Tunisian Arabic: A Comparison between Three Machine Learning Models,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167679612&doi=10.1145%2f3599234&partnerID=40&md5=73c0dd92299e647e8d858ec9f7a42c58,"Tokenization represents the way of segmenting a piece of text into smaller units called tokens. Since Arabic is an agglutinating language by nature, this treatment becomes a crucial preprocessing step for many Natural Language Processing (NLP) applications such as morphological analysis, parsing, machine translation, information extraction, and so on. In this article, we investigate word tokenization task with a rewriting process to rewrite the orthography of the stem. For this task, we are using Tunisian Arabic (TA) text. To the best of the researchers' knowledge, this is the first study that uses TA for word tokenization. Therefore, we start by collecting and preparing various TA corpora from different sources. Then, we present a comparison of three character-based tokenizers based on Conditional Random Fields (CRF), Support Vector Machines (SVM) and Deep Neural Networks (DNN). The best proposed model using CRF achieved an F-measure result of 88.9%. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesWord tokenization; Arabic dialect; CRF; deep learning; SVM; Tunisian Arabic,Deep neural networks; Image segmentation; Learning algorithms; Learning systems; Natural language processing systems; Random processes; Additional key word and phrasesword tokenization; Arabic dialects; Conditional random field; Deep learning; Key words; Machine learning models; Random fields; Support vectors machine; Tokenization; Tunisian arabic; Support vector machines
MSA Speech Rhythm Pattern in a Multilingual Setting,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167721133&doi=10.1145%2f3593295&partnerID=40&md5=b6cba7a557df6ee48dd2f7dfcc9e1eb7,"This study examines variation in rhythm metrics in a multilingual setting by focusing on between-speaker differences. The investigation analyzes speech rhythm patterns of segmental durations in the speech of 77 Algerian speakers belonging to three educational background classes and three age groups. The experiment focuses on speech rhythm variability according to the level of educational background of the speakers and the language used in daily life. The gender and age of speakers are also analyzed. Results show that five vocalic rhythm metrics reflect the contrast between long and short vowels that was observed from the acoustic measurements. The statistical analysis reveals that rhythm metrics are sensitive to differences between groups of speakers, such as age and educational background. The outcomes also show that the lack of practice of Modern Standard Arabic by some speaker groups considerably affects vowel quantity. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRhythm metrics; age; Algerian speakers; educational background; French; gender; Modern Standard Arabic,Speech synthesis; Additional key word and phrasesrhythm metric; Age; Algerian speaker; Educational background; French; Gender; Key words; Modern standard arabic; Modern standards; Standard arabics; Linguistics
Adversarial Multi-task Learning for Efficient Chinese Named Entity Recognition,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167661487&doi=10.1145%2f3603626&partnerID=40&md5=bcb9464dd4006c1a9543d9b18b21c5f8,"Named entity recognition (NER) is a fundamental task for information extraction applications. NER is challenging because of semantic ambiguities in academic literature, especially for non-Latin languages. Besides word semantic information, recognizing Chinese named entities needs to consider word boundary information, as words contained in Chinese texts are not separated with spaces. Leveraging word boundary information could help to determine entity boundaries and thus improve entity recognition performance. In this article, we propose to combine word boundary information and semantic information for named entity recognition based on multi-task adversarial learning. Specifically, we learn commonly shared boundary information of entities from multiple kinds of tasks, including Chinese word segmentation (CWS), part-of-speech (POS) tagging, and entity recognition, with adversarial learning. We learn task-specific semantic information of words from these tasks and combine the learned boundary information with the semantic information to improve entity recognition with multi-task learning. We then propose a compression method based on improved clustering to accelerate the proposed model. We conduct extensive experiments on four public benchmark datasets and two private datasets, compared with state-of-the-art baseline models, and the experimental results demonstrate that our model achieves considerable performance improvements on various evaluation datasets. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNamed entity recognition; adversarial learning; Chinese word segmentation; multi-task learning; part-of-speech tagging,Computational linguistics; Learning systems; Natural language processing systems; Semantics; Speech recognition; Syntactics; Additional key word and phrasesnamed entity recognition; Adversarial learning; Boundary information; Chinese word segmentation; Entity recognition; Key words; Multitask learning; Part of speech tagging; Parts-of-speech tagging; Semantics Information; Benchmarking
Analysis of Cursive Text Recognition Systems: A Systematic Literature Review,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167725839&doi=10.1145%2f3592600&partnerID=40&md5=bd2a595a4a3f8ecc36b5dc5846efdd8a,"Regional and cultural diversities around the world have given birth to a large number of writing systems and scripts, which consist of varying character sets. Developing an optimal character recognition for such a varying and large character set is a challenging task. Unlimited variations in handwritten text due to mood swings, varying writing styles, changes in medium of writing, and many more puzzle the research community. To overcome this problem, researchers have proposed various techniques for the automatic recognition of cursive languages like Urdu, Pashto, and Arabic. With the passage of time, the field of text recognition matured, and the number of publications exponentially increased in the targeted field. It is very difficult to find all the techniques developed, calculate the time and resource consumptions, and understand the cost-benefit tradeoffs among these techniques. These tradeoffs resist making this technology able for practical use. To address these tradeoffs, this article systematic analysis to identify gaps in the literature and suggest new enhanced solution accordingly. A total of 153 of the most relevant articles from 2008 to 2022 are analyzed in this systematic literature review (SLR) work. This systematic review process shows (1) the list of techniques suggested for cursive text recognition purposes and its capabilities, (2) set of feature extraction techniques proposed, and (3) implementation tools used to design and simulate the empirical studies in this specialized field. We have also discussed the emerging trends and described their implications for the research community in this specialized domain. This systematic assessment will ultimately help researchers to perform an overview of the existing character/text recognition approaches, recognition capabilities, and time consumption and subsequently identify the areas that requires a significant attention in the near future. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCursive languages; feature techniques; recognition algorithms; systematic literature review,Character sets; Commerce; Cost benefit analysis; Additional key word and phrasescursive language; Cultural diversity; Feature technique; Key words; Recognition algorithm; Recognition systems; Research communities; Systematic literature review; Text recognition; Time consumption; Character recognition
Retrospective Multi-granularity Fusion Network for Chinese Idiom Cloze-style Reading Comprehension,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167730622&doi=10.1145%2f3603370&partnerID=40&md5=99e3389b03a19106fe6ffb2649707690,"Chinese idiom cloze-style reading comprehension task is of great significance for improving the machine's ability to understand Chinese idioms, which is one of the essential application requirements in advanced artificial intelligence. Existing methods suffer from an insufficient deep semantic understanding of the text. To solve this problem, this paper proposes a novel Retrospective Multi-granularity Fusion Network (RMFNet) for Chinese idiom cloze-style reading comprehension. Our RMFNet is equipped with two novel modules to model deeper contextual information of passage and Chinese idioms, respectively. First, we propose a novel Multi-granularity Passage Fusion (MgPF) module, which enhances the passage representation by integrating different semantic perspectives. Second, we propose a Retrospective Reading (Re) module that implements a back-and-forth reading mechanism to concentrate on critical Chinese idioms, thereby generating an ultimate memory for the whole text. Notably, the intuition of the MgPF module and the Re module is based on human reading strategies in the real world. The strategies in these modules are similar to how humans perceive the text. Extensive experiments are conducted on Chinese benchmark datasets to evaluate the effectiveness and superiority of the proposed method. Our RMFNet achieves state-of-the-art performance and in-depth analysis verifies its capability for understanding the deep semantics of the text. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDeep learning; Chinese idiom understanding; human reading strategy; information fusion; machine reading comprehension,Information fusion; Additional key word and phrasesdeep learning; Chinese idiom understanding; Comprehension tasks; Fusion modules; Human reading strategy; Key words; Machine reading comprehension; Multi-granularity; Reading comprehension; Reading strategies; Semantics
Instance-Aware Prompt Learning for Language Understanding and Generation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167681485&doi=10.1145%2f3604613&partnerID=40&md5=c841afa6e9724fc2ba960e67c748e755,"Prompt learning has emerged as a new paradigm for leveraging pre-trained language models (PLMs) and has shown promising results in downstream tasks with only a slight increase in parameters. However, the current usage of fixed prompts, whether discrete or continuous, assumes that all samples within a task share the same prompt. This assumption may not hold for tasks with diverse samples that require different prompt information. To address this issue, we propose an instance-aware prompt learning method that learns a different prompt for each instance. Specifically, we suppose that each learnable prompt token has a different contribution to different instances, and we learn the contribution by calculating the relevance score between an instance and each prompt token. The contribution-weighted prompt would be instance aware. We apply our method to both unidirectional and bidirectional PLMs on both language understanding and generation tasks. Extensive experiments demonstrate that our method achieves comparable results using as few as 1.5% of the parameters of PLMs tuned and obtains considerable improvements compared with strong baselines. In particular, our method achieves state-of-the-art results using ALBERT-xxlarge-v2 on the SuperGLUE few-shot learning benchmark.1 © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPre-trained language model; few-shot learning; parameter-efficient tuning; prompt learning,Computational linguistics; Additional key word and phrasespre-trained language model; Down-stream; Few-shot learning; Key words; Language generation; Language model; Language understanding; Learn+; Parameter-efficient tuning; Prompt learning; Learning systems
A Study on Corpus-based Stopword Lists in Indian Language IR,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167422874&doi=10.1145%2f3606262&partnerID=40&md5=bf5ff70463c230ffd2424849b8d17014,"We explore and evaluate the effect of different stopword lists (non-corpus-based and corpus-based) in the information retrieval (IR) tasks with different Indian languages such as Bengali, Marathi, Gujarati, Hindi, and English. The issue was investigated from three viewpoints. Is there any performance difference between non-corpus-based and corpus-based stopword removal in chosen Indian languages? Can corpus-based stopword lists improve performance in Indian languages IR? If yes, to what extent? Among the different corpus-based stopword lists, which stopword list provides the best IR performance? Does the length of a corpus-based stopword list affect the retrieval performance in Indian languages? If yes, to what extent? It was observed that a corpus-based stopword list provides better retrieval performance than a non-corpus-based stopword list in different Indian languages. Among the different corpus-based stopword lists generated and experimented with, Zipf's law-based stopword list (idf-based one) provides the best retrieval performance in various Indian languages. The aggregation1-based stopword list provides better retrieval than the aggregation2-based list in Indian languages, but in English, the aggregation2-based stopword list performs better than the aggregation1-based list. The best performing idf-based stopword list improves MAP score by 5.43% in Bengali, 1.91% in Marathi, 5.4% in Gujarati, 1.5% in Hindi, and 2.12% in English, respectively, over their baseline counterparts. The probabilistic retrieval models (BM25 and TF-IDF) perform best in different Indian languages. A smaller length of corpus-based stopword lists performs better than a larger length of non-corpus-based stopword lists for all the Indian languages considered. The proposed schemes demonstrate that a stopword list can be heuristically generated in a language-independent statistical method and effectively used for IR tasks with performance comparable, to or even better than non-corpus-based stopword lists.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesIndian languages; evaluation; stopword,Additional key word and phrasesindian language; Bengalis; Corpus-based; Evaluation; Improve performance; Indian languages; Key words; Performance; Retrieval performance; Stopword; Information retrieval
"Semantic Relation Extraction: A Review of Approaches, Datasets, and Evaluation Methods With Looking at the Methods and Datasets in the Persian Language",2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167715160&doi=10.1145%2f3592601&partnerID=40&md5=daa2da4f364e620fca8d263c1286b890,"A large volume of unstructured data, especially text data, is generated and exchanged daily. Consequently, the importance of extracting patterns and discovering knowledge from textual data is significantly increasing. As the task of automatically recognizing the relations between two or more entities, semantic relation extraction has a prominent role in the exploitation of raw text. This article surveys different approaches and types of relation extraction in English and the most prominent proposed methods in Persian. We also introduce, analyze, and compare the most important datasets available for relation extraction in Persian and English. Furthermore, traditional and emerging evaluation metrics for supervised, semi-supervised, and unsupervised methods are described, along with pointers to commonly used performance evaluation datasets. Finally, we briefly describe challenges in extracting relationships in Persian and English and dataset creation challenges. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSemantic relations; automatic extraction; dataset; evaluation methods; information extraction; linguistics; Natural Language Processing (NLP); Persian text processing; relation extraction,Character recognition; Data mining; Information retrieval; Natural language processing systems; Semantics; Additional key word and phrasessemantic relation; Automatic extraction; Dataset; Evaluation methods; Information extraction; Key words; Language processing; Natural language processing; Natural languages; Persian text processing; Persians; Relation extraction; Text-processing; Text processing
Swahili Speech Dataset Development and Improved Pre-training Method for Spoken Digit Recognition,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167738320&doi=10.1145%2f3597494&partnerID=40&md5=ca0bcf7ee19e9bee22bfab2f20db7644,"Speech dataset is an essential component in building commercial speech applications. However, low-resource languages such as Swahili lack such a resource that is vital for spoken digit recognition. For languages where such resources exist, they are usually insufficient. Thus, pre-training methods have been used with external resources to improve continuous speech recognition. However, to the best of our knowledge, no study has investigated the effect of pre-training methods specifically for spoken digit recognition. This study aimed at addressing these problems. First, we developed a Swahili spoken digit dataset for Swahili spoken digit recognition. Then, we investigated the effect of cross-lingual and multi-lingual pre-training methods on spoken digit recognition. Finally, we proposed an effective language-independent pre-training method for spoken digit recognition. The proposed method has the advantage of incorporating target language data during the pre-training stage that leads to an optimal solution when using less training data. Experiments on Swahili (being developed), English, and Gujarati datasets show that our method achieves better performance compared with all the baselines listed in this study. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSwahili language; convolutional neural network; cross-lingual; low-resource language; multi-lingual; pre-training; spoken digit recognition,Convolutional neural networks; Linguistics; Additional key word and phrasesswahilus language; Convolutional neural network; Cross-lingual; Digit recognition; Key words; Low resource languages; Multi-lingual; Pre-training; Spoken digit recognition; Training methods; Speech recognition
An Exhaustive Literature Review of Hadith Text Mining,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167652737&doi=10.1145%2f3588315&partnerID=40&md5=b3a06b9e18508046910540719a0b06c4,"The Quran and the hadith of the Prophet are the two sources of legislation for Muslims. Sharia rulings and laws are not only derived from the Quran but also the bulk of them come through hadith. Understanding the hadith, its classification, and verification of its authenticity is vital to reach detailed rulings, as the volume of the hadith is many times greater than the volume of the Quran. As a result, mining in the hadith text is one of the things that has attracted the attention of researchers in the past few years. In this study, we conducted a survey of all the techniques and systems related to the mining of the hadith in its two parts, the Al-Matn and the Al-Sanad. On the other hand, the challenges and obstacles which confronted researchers have been shown; in addition, some suggested tips were highlighted to overcome those challenges. Furthermore, the most essential modern techniques used in the classification of Arabic texts, which gave a high degree of efficiency, were highlighted as milestones for future studies. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words PhrasesHadith text mining; authentication; classification; deep learning; machine learning; natural language processing,Classification (of information); Data mining; Deep learning; Learning algorithms; Learning systems; Natural language processing systems; Text processing; Additional key word phraseshadith text mining; Deep learning; Key words; Language processing; Literature reviews; Machine-learning; Natural language processing; Natural languages; Text-mining; Two sources; Authentication
Aspect-Based Sentiment Analysis for Arabic Food Delivery Reviews,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167446677&doi=10.1145%2f3605146&partnerID=40&md5=6b1fae25e1f2db95313bf73238124439,"Business customers and consumers share their reviews online on social platforms such as Twitter. Therefore, Twitter data sentiment analysis is extremely useful for both research and commercial purposes. Manually analyzing reviews takes a long time and effort, hence, automatic sentiment analysis is required. In this article, we address aspect-based sentiment analysis for Arabic food delivery reviews using several deep learning approaches. In particular, we propose to use Transformer-based models (GigaBERT and AraBERT), Bi-LSTM-CRF, and LSTM, as well as a classical machine learning algorithm (SVM). We also present our dataset of food delivery service reviews, which we collected from Twitter. We annotated them and used them for training and evaluating our approaches.The experiments show that both GigaBERT and AraBERT outperformed the other models in all the tasks. The Transformer-based models received F1-scores of 77% in the aspect terms detection task, 82% in the Aspect category detection task, and 81% in the aspect polarity detection task, gaining 2%, 4%, and 4% over Bi-LSTM-CRF and LSTM in the first, second, and third tasks, respectively.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAspect-based sentiment classification; BERT; deep learning,Learning algorithms; Learning systems; Long short-term memory; Social networking (online); Support vector machines; Additional key word and phrasesaspect-based sentiment classification; BERT; Business customers; Deep learning; Detection tasks; Food delivery; Key words; Learning approach; Sentiment analysis; Sentiment classification; Sentiment analysis
Experiments of Supervised Learning and Semi-Supervised Learning in Thai Financial News Sentiment: A Comparative Study,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167702535&doi=10.1145%2f3603499&partnerID=40&md5=4b291b5038c08a8704f5ac598da8b0d9,"Sentiment classification is an instrument of natural language processing tasks in text analysis to measure customer feedback from given documents such as product reviews, news, and texts. This research aims to experiment with Thai financial news sentiment classification and evaluate sentiment classification performance. In this research, we show financial news sentiment classification experimental results when comparing supervised and semi-supervised methods. In the research methodology, we use PyThaiNLP to tokenize and remove stopwords and split datasets into 85% of the training set and 15% of the testing set. Next, we classify sentiment using machine learning and deep learning approaches with feature extraction such as bag-of-words, term frequency-inverse document frequency, and word embedding (Word2Vec and Bidirectional Encoder Representations from Transformers (BERT)) in given texts. The results show that support vector machine with the BERT model yields the best performance at 83.38%; in contrast, the random forest classifier with bag-of-words yields the worst performance at 54.10% in the machine learning approach. Another experiment reveals that long short-term memory with the BERT model yields the best performance at 84.07% in contrast to the convolutional neural network with bag-of-words, which yields the worst performance at 69.80% in the deep learning approach. The results imply that support vector machine, convolutional neural network, and long short-term memory are suitable for classifying sentiment in complex structure language. From this study, we observe the importance of sentiment classification tools between supervised and semi-supervised learning, and we look forward to furthering this work. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNatural language processing; semi-supervised learning; sentiment classification; supervised learning; Thai language,Brain; Classification (of information); Convolution; Convolutional neural networks; Finance; Information retrieval systems; Learning algorithms; Learning systems; Long short-term memory; Text processing; Additional key word and phrasesnatural language processing; Bag of words; Financial news; Key words; Language processing; Learning approach; Performance; Semi-supervised learning; Sentiment classification; Thai language; Support vector machines
IIITH-CSTD Corpus: Crowdsourced Strategies for the Collection of a Large-scale Telugu Speech Corpus,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167738708&doi=10.1145%2f3600228&partnerID=40&md5=74a5204b144fa84a4a07f7edb4256f8b,"Due to the lack of a large annotated speech corpus, many low-resource Indian languages struggle to utilize recent advancements in deep neural network architectures for Automatic Speech Recognition (ASR) tasks. Collecting large-scale databases is an expensive and time-consuming task. Current approaches lack extensive traditional expert-based data acquisition guidelines, as they are tedious and complex. In this work, we present the International Institute of Information Technology Hyderabad-Crowd Sourced Telugu Database (IIITH-CSTD), a Telugu corpus collected through crowdsourcing. In particular, our main objective is to mitigate the low-resource problem for Telugu. We also present the sources, crowdsourcing pipeline, and the protocols used to collect the corpus for a low-resource language, namely, Telugu. Data of approximately 2,000 hours of transcribed audio is presented and released in this article, covering three major regional dialects of the Telugu language in three different (i.e., read, conversational and spontaneous) speaking styles on topics such as politics, sports, and arts, science, and so on.1 We also present the experimental results of the collected corpus on ASR tasks. We hope this work will motivate researchers to curate large-scale annotated speech data for other low-resource Indic languages. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSpeech recognition; dialects; End-to-End; low-resource languages; resource creation; TDNN,Data acquisition; Deep neural networks; Network architecture; Speech recognition; Additional key word and phrasesspeech recognition; Dialect; End to end; Hyderabad; Key words; Large-scales; Low resource languages; Resource creation; Speech corpora; TDNN; Crowdsourcing
A New English/Arabic Parallel Corpus for Phishing Emails,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167685006&doi=10.1145%2f3606031&partnerID=40&md5=bcd4cdab6a97432c75eba5b2aa6c866f,"Phishing involves malicious activity whereby phishers, in the disguise of legitimate entities, obtain illegitimate access to the victims' personal and private information, usually through emails. Currently, phishing attacks and threats are being handled effectively through the use of the latest phishing email detection solutions. Most current phishing detection systems assume phishing attacks to be in English, though attacks in other languages are growing. In particular, Arabic is a widely used language and therefore represents a vulnerable target. However, there is a significant shortage of corpora that can be used to develop Arabic phishing detection systems. This article presents the development of a new English-Arabic parallel phishing email corpus that has been developed from the anti-phishing share task text (IWSPA-AP 2018). The email content was to be translated, and the task had been allotted to 10 volunteers who had a university background and were English and Arabic language experts. To evaluate the effectiveness of the new corpus, we develop phishing email detection models using Term Frequency-Inverse Document Frequency and Multilayer Perceptron using 1,258 emails in Arabic and English that have equal ratios of legitimate and phishing emails. The experimental findings show that the accuracy reaches 96.82% for the Arabic dataset and 94.63% for the emails in English, providing some assurance of the potential value of the parallel corpus developed. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEnglish-Arabic Parallel Corpus; frequency-inverse document frequency; Multilayer Perceptron; phishing emails,Computer crime; Multilayers; Text processing; Additional key word and phrasesenglish-arabic parallel corpus; Email Detection; Frequency-inverse document frequency; Inverse Document Frequency; Key words; Multilayers perceptrons; Parallel corpora; Phishing; Phishing attacks; Phishing email; Electronic mail
Using Data Augmentation and Bidirectional Encoder Representations from Transformers for Improving Punjabi Named Entity Recognition,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164243545&doi=10.1145%2f3595861&partnerID=40&md5=fdf072035071ebb9cd4dddd389f6b52e,"Named entity recognition (NER) is a task of proper noun identification from natural language text and classification into various types such as location, person, and organization. Due to NER's applications in different natural language processing (NLP) tasks, numerous NER approaches and benchmark datasets have been proposed. However, developing NER techniques for low-resource languages is still limited due to the absence of substantial training datasets. Punjabi is a classic example of low resource language. Although various researchers have worked on Punjabi, they focused on the Gurmukhi script. To overcome the challenges in developing NER for the Shahmukhi script, we present an improved technique for Punjabi NER for the Shahmukhi script in this paper. We firstly extend the existing dataset by adding new NER classes by leveraging a novel Pool of Words data augmentation strategy. Our extended dataset has 11,31,509 tokens and 1,25,789 labeled entities with more named entities (NEs) than the older dataset. In the next step, we fine-tuned a transformer model known as Bidirectional Encoder Representations from Transformers (BERT) for the NER task. We performed experiments using the proposed approach on a new and older dataset version, showing that our method achieved competitive results.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and phrasesPunjabi; Asian languages; low-resource languages; named entity recognition; Shahmukhi,Benchmarking; Classification (of information); Natural language processing systems; Text processing; Additional key word and phrasespunjabi; Asian languages; Data augmentation; Key words; Low resource languages; Named entity recognition; Natural languages texts; Persons and organizations; Proper nouns; Shahmukhi; Signal encoding
Dataset Enhancement and Multilingual Transfer for Named Entity Recognition in the Indonesian Language,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164241991&doi=10.1145%2f3592854&partnerID=40&md5=53f3104627522f1fe83d00ff665c72d4,"Named entity recognition in the Indonesian language has significantly developed in recent years. However, it still lacks standardized publicly available corpora; a small dataset is available but suffers from inconsistent annotations. Therefore, we re-annotated the dataset to improve its consistency and benefit the community. Our re-annotation led to better training results from an effective baseline model consisting of bidirectional long short-term memory and conditional random fields. To fully utilize the limited available data, we utilized better contextualization and transferred external knowledge by exploiting monolingual and multilingual pre-trained language models, such as IndoBERT and XLM-RoBERTa. In addition to the general improvement from the language models, we observed that the monolingual model is more sensitive, while the multilingual ones show advantages in rich morphological knowledge. We also applied cross-lingual transfer learning to utilize high-resource corpora in other languages. We adopted English, Spanish, Dutch, and German as the source languages for the target Indonesian language and found that Dutch plays a special role in the data transfer method due to morphological similarity attributable to historical reasons.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesDataset enhancement; cross-lingual transfer learning; Indonesian language; low-resource language; multilingual learning; named entity recognition,Computational linguistics; Transfer learning; Additional key word and phrasesdataset enhancement; Cross-lingual; Cross-lingual transfer learning; Indonesian languages; Key words; Language model; Low resource languages; Multilingual learning; Named entity recognition; Transfer learning; Data transfer
Multilingual BERT-based Word Alignment By Incorporating Common Chinese Characters,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164241072&doi=10.1145%2f3594634&partnerID=40&md5=be35f261945e1a37c6fd5963145f5a1e,"Word alignment is an important task of detecting translation equivalents between a sentence pair. Although word alignment is no longer necessarily needed for neural machine translation, it's still useful in a wealth of applications, e.g., bilingual lexicon induction, constraint decoding, and so on. However, the most well-known word aligners are still Giza++ and fastAlign, both of which are implementations of traditional IBM models. To keep pace with the advance in NMT, there has been a surge of interest in replacing the IBM models with neural models. We follow this trend but aim to boost performance of word alignment between Japanese and Chinese, which share a large portion of Chinese characters. Our key idea is to leverage these common Chinese characters in both languages as an indicator for inferring alignment; i.e., the source and target words with the common Chinese characters should be most likely aligned. Following this idea, we propose three methods that leverage common Chinese characters to boost the mBERT-based word alignment, including reward factor, representation alignment, and contrastive training. Furthermore, we annotate and release a golden dataset for Japanese-Chinese word alignment. Experiments on the dataset show that our methods outperform several strong baselines in terms of AER score and verify the effectiveness of exploiting common Chinese characters.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesWord alignment; common Chinese character; Japanese-Chinese; mBERT,Computational linguistics; Computer aided language translation; Additional key word and phrasesword alignment; Aligners; Bilingual lexicons; Chinese characters; Common chinese character; IBM Models; Japanese-chinese; Key words; MBERT; Word alignment; Neural machine translation
Integrating Reconstructor and Post-Editor into Neural Machine Translation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164243314&doi=10.1145%2f3588766&partnerID=40&md5=271981c0270ae75db7c3228eec4f233c,"Neural machine translation (NMT) mainly comprises the encoder and decoder. The encoder is mainly used to extract the feature vector of the source language sentence. The decoder predicts the next token according to the feature vector extracted by the encoder and the information of the current moment. In this process, there is no guarantee that the features extracted by the encoder are indistinguishable from the meaning of the sentences in the source language. There is also no guarantee that the decoder can accurately predict the corresponding character. These issues can lead to over-translation and under-translation issues in the translated results. Previous researchers alleviated this problem by calculating the gap between the reconstructed source-language sentences and the source-language sentences. Inspired by this method, we propose to integrate a reconstructor and a post-editor into NMT during the training. The reconstructor takes the translation of NMT as input to reconstruct the source sentence, and the post-editor takes the translation as input and post-edits it to predict the target sentence. Through the training of the reconstructor and the post-editor, the semantics of the translation are forced to follow the source sentence and the target sentence. Experimental results show that our approach can effectively improve the performance of NMT on multiple translation tasks.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesNeural network; loss function; neural machine translation; post-editor; reconstructor,Computational linguistics; Computer aided language translation; Decoding; Semantics; Signal encoding; 'current; Additional key word and phrasesneural network; Encoders and decoders; Features vector; Key words; Loss functions; Performance; Post editors; Reconstructors; Source language; Neural machine translation
BayesKGR: Bayesian Few-Shot Learning for Knowledge Graph Reasoning,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235922&doi=10.1145%2f3589183&partnerID=40&md5=213ab5db789c030fe53d55fb9914d2b0,"Reasoning over knowledge graphs (KGs) has received increasing attention recently due to its promising applications in many areas, such as semantic search and recommendation systems. Subsequently, most reasoning models are inherently transductive and ignore uncertainties of KGs, making it difficult to generalize to unseen entities. Moreover, existing approaches usually require each entity in the KG to have sufficient training samples, which leads to the overfitting of the entity having few instances. In fact, long-tail distributions are quite widespread in KGs, and newly emerging entities will tend to have only a few related triples. In this work, we aim at studying knowledge graph reasoning under a challenging setting where only limited training samples are available. Specifically, we propose a Bayesian inductive reasoning method and incorporate meta-learning techniques in few-shot learning to solve data deficiency and uncertainties. We design a Bayesian graph neural network as a meta-learner to achieve Bayesian inference, which can extrapolate meta-knowledge from observed KG to emerging entities. We conduct extensive experiments on two large-scale benchmark datasets, and the results demonstrate considerable performance improvement with the proposed approach over other baselines.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesKnowledge graph; few-shot learning; meta-learning; uncertainty,Bayesian networks; Benchmarking; Graph neural networks; Knowledge graph; Large dataset; Learning systems; Sampling; Semantic Web; Semantics; Additional key word and phrasesknowledge graph; Bayesian; Few-shot learning; Key words; Knowledge graphs; Metalearning; Semantic recommendations; Semantic search; Training sample; Uncertainty; Inference engines
Alabib-65: A Realistic Dataset for Algerian Sign Language Recognition,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164244430&doi=10.1145%2f3596909&partnerID=40&md5=08e642beee9685f685d7cbf9396d08cb,"Sign language recognition (SLR) is a promising research field that aims to blur boundaries between Deaf and hearing people by creating a system that can transcribe signs into a written or vocal language. There is a growing body of literature that investigates the recognition of different sign languages, especially American sign language. So far, to the best of our knowledge, no study has considered the Algerian SLR. This is mainly due to the lack of datasets. To address this issue, we created the Alabib-65, the first Algerian Sign Language dataset. It consists of up to 6,238 Videos recorded from 41 native signers under realistic settings. This dataset is challenging due to a variety of reasons. First, there is a little inter-class variability. The 65 sign classes are similar in terms of hands' configuration, placement, or movement and can share the same sub-parts. Second, there is a large intra-class variability. Furthermore, compared to other SL datasets that were collected from an indoor environment with a static and simple background, our videos were recorded from both indoor and outdoor environments with 22 backgrounds varying from simple to cluttered, and from static to dynamic. To underpin future research, we provided baseline results on this new dataset using state-of-the-art machine learning methods, namely: IDTFs with Fisher vector and SVM-classifier, VGG16-GRU, I3D, I3D-GRU, and I3D-GRU-Attention. The results show the validity and the challenges of our dataset.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesBenchmark dataset; deep learning features; gesture dataset; hand-crafted features; Sign Language dataset; subtle classes; videos in the wild,Classification (of information); Deep learning; Learning systems; Support vector machines; Additional key word and phrasesbenchmark dataset; Deep learning feature; Gesture dataset; Hand-crafted feature; Key words; Sign language; Sign language dataset; Sign Language recognition; Subtle class; Video in the wild; Audition
TPoet: Topic-Enhanced Chinese Poetry Generation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164242090&doi=10.1145%2f3593805&partnerID=40&md5=9ae01a5e2eddb4e80b6f99627c12d7eb,"Chinese poetry generation has been a challenging part of natural language processing due to the unique literariness and aesthetics of poetry. In most cases, the content of poetry is topic related. In other words, specific thoughts or emotions are usually expressed regarding given topics. However, topic information is rarely taken into consideration in current studies about poetry generation models. In this article, we propose a topic-enhanced Chinese poetry generation model called TPoet in which the topic model is integrated into the Transformer-based auto-regressive text generation model. By feeding topic information to the input layer and heterogeneous attention mechanism, TPoet can implicitly learn the latent information of topic distribution. In addition, by setting multiple identifiers such as segment, rhyme, and tone, the model can explicitly learn the constraints of generated poems. Extensive experimental results show that the quality of TPoet-generated poems outperforms the current advanced models or systems, and the topic consistency and diversity in generated poems have been significantly improved as well.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDeep learning; poetry generation; topic model,'current; Additional key word and phrasesdeep learning; Auto-regressive; Key words; Language processing; Learn+; Natural languages; Poetry generation; Text generations; Topic Modeling; Natural language processing systems
Detection of Offensive Language and ITS Severity for Low Resource Language,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164245888&doi=10.1145%2f3580476&partnerID=40&md5=1841037d66a36f068fb15f65022182c1,"Continuous proliferation of hate speech in different languages on social media has drawn significant attention from researchers in the past decade. Detecting hate speech is indispensable irrespective of the scale of use of language, as it inflicts huge harm on society. This work presents a first resource for classifying the severity of hate speech in addition to classifying offensive and hate speech content. Current research mostly limits hate speech classification to its primary categories, such as racism, sexism, and hatred of religions. However, hate speech targeted at different protected characteristics also manifests in different forms and intensities. It is important to understand varying severity levels of hate speech so that the most harmful cases of hate speech may be identified and dealt with earlier than the less harmful ones. In this work, we focus on detecting offensive speech, hate speech, and multiple levels of hate speech in the Urdu language. We investigate three primary target categories of hate speech: religion, racism, and national origin. We further divide these categories into levels based on the severity of hate conveyed. The severity levels are referred to as symbolization, insult, and attribution. A corpus comprising more than 20,000 tweets against the corresponding hate speech categories and severity levels is collected and annotated. A comprehensive experimentation scheme is applied using traditional as well as deep learning-based models to examine their impact on hate speech detection. The highest macro-averaged F-score yielded for detecting offensive speech is 86% while the highest F-scores for detecting hate speech with respect to ethnicity, national origin, and religious affiliation are 80%, 81%, and 72%, respectively. This shows that results are very encouraging and would provide a lead towards further investigation in this domain.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesHate speech; BERT; convolutional neural network; long short-term memory; Urdu NLP,Convolutional neural networks; Deep learning; Additional key word and phraseshate speech; BERT; Convolutional neural network; F-score; Key words; Low resource languages; Offensive languages; Social media; Speech content; Urdu NLP; Speech recognition
Semantic Tagging for the Urdu Language: Annotated Corpus and Multi-Target Classification Methods,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164239630&doi=10.1145%2f3582496&partnerID=40&md5=71508aacdd4b176828a17f03b8d03889,"Extracting and analysing meaning-related information from natural language data has attracted the attention of researchers in various fields, such as natural language processing, corpus linguistics, information retrieval, and data science. An important aspect of such automatic information extraction and analysis is the annotation of language data using semantic tagging tools. Different semantic tagging tools have been designed to carry out various levels of semantic analysis, for instance, named entity recognition and disambiguation, sentiment analysis, word sense disambiguation, content analysis, and semantic role labelling. Common to all of these tasks, in the supervised setting, is the requirement for a manually semantically annotated corpus, which acts as a knowledge base from which to train and test potential word and phrase-level sense annotations. Many benchmark corpora have been developed for various semantic tagging tasks, but most are for English and other European languages. There is a dearth of semantically annotated corpora for the Urdu language, which is widely spoken and used around the world. To fill this gap, this study presents a large benchmark corpus and methods for the semantic tagging task for the Urdu language. The proposed corpus contains 8,000 tokens in the following domains or genres: news, social media, Wikipedia, and historical text (each domain having 2K tokens). The corpus has been manually annotated with 21 major semantic fields and 232 sub-fields with the USAS (UCREL Semantic Analysis System) semantic taxonomy which provides a comprehensive set of semantic fields for coarse-grained annotation. Each word in our proposed corpus has been annotated with at least one and up to nine semantic field tags to provide a detailed semantic analysis of the language data, which allowed us to treat the problem of semantic tagging as a supervised multi-target classification task. To demonstrate how our proposed corpus can be used for the development and evaluation of Urdu semantic tagging methods, we extracted local, topical and semantic features from the proposed corpus and applied seven different supervised multi-target classifiers to them. Results show an accuracy of 94% on our proposed corpus which is free and publicly available to download.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesUrdu corpus annotation; multi-target classifiers; semantic annotation; semantic tagger,Classification (of information); Computational linguistics; Information retrieval; Knowledge based systems; Semantics; Additional key word and phrasesurdu corpus annotation; Corpus annotations; Key words; Multi-target classifier; Multi-targets; Semantic analysis; Semantic annotations; Semantic fields; Semantic tagging; Semantic tags; Sentiment analysis
"Text Polishing with Chinese Idiom: Task, Datasets and Pre-trained Baselines",2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164238423&doi=10.1145%2f3593806&partnerID=40&md5=4c6d6ecfabe5cf799b19f2773c17e49b,"This work presents the task of text polishing, which generates a sentence that is more graceful than the input sentence while retaining its semantic meaning. Text polishing has great value in real usage and is an important component in modern writing assistance systems. However, the task is still not well studied in the literature. Further research in this important direction requires more formal task definitions, benchmark datasets, and powerful baseline models. In this work, we formulate the task as a context-dependent text generation problem and conduct a case study on the text polishing with Chinese idiom. To circumvent the difficulties of task data annotation, we propose a semi-automatic data construction pipeline based on human-machine collaboration, and establish a large-scale text polishing dataset consisting of 1.5 million instances. We propose two types of task-specific pre-training objectives for the text polishing task and implement a series of Transformer-based models pre-trained on a massive Chinese corpus as baselines. We conduct extensive experiments with the baseline models on the constructed text polishing datasets and have some major findings. The human evaluation further reveals the polishing ability of the final system.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesIntelligent writing assistance; back-translation; Chinese idiom; pre-trained language model; text polishing,Large dataset; Semantics; Additional key word and phrasesintelligent writing assistance; Assistance system; Back translations; Baseline models; Benchmark datasets; Chinese idiom; Key words; Language model; Pre-trained language model; Text polishing; Polishing
The Impact of Arabic Diacritization on Word Embeddings,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164245264&doi=10.1145%2f3592603&partnerID=40&md5=37bf2c0be839904cb7a020b4d03ac35d,"Word embedding is used to represent words for text analysis. It plays an essential role in many Natural Language Processing (NLP) studies and has hugely contributed to the extraordinary developments in the field in the last few years. In Arabic, diacritic marks are a vital feature for the readability and understandability of the language. Current Arabic word embeddings are non-diacritized. In this article, we aim to develop and compare word embedding models based on diacritized and non-diacritized corpora to study the impact of Arabic diacritization on word embeddings. We propose evaluating the models in four different ways: clustering of the nearest words; morphological semantic analysis; part-of-speech tagging; and semantic analysis. For a better evaluation, we took the challenge to create three new datasets from scratch for the three downstream tasks. We conducted the downstream tasks with eight machine learning algorithms and two deep learning algorithms. Experimental results show that the diacritized model exhibits a better ability to capture syntactic and semantic relations and in clustering words of similar categories. Overall, the diacritized model outperforms the non-diacritized model. We obtained some more interesting findings. For example, from the morphological semantics analysis, we found that with the increase in the number of target words, the advantages of the diacritized model are also more obvious, and the diacritic marks have more significance in POS tagging than in other tasks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesArabic NLP; diacritization; morphological semantics; semantic analysis; word embeddings,Computational linguistics; Deep learning; Embeddings; Natural language processing systems; Syntactics; Additional key word and phrasesarabic natural language processing; Clusterings; Diacritization; Embeddings; Key words; Language processing; Morphological semantic; Natural languages; Semantic analysis; Word embedding; Semantics
An Efficient and Accurate Detection of Fake News Using Capsule Transient Auto Encoder,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164243849&doi=10.1145%2f3589184&partnerID=40&md5=d78e6aeee50956fd821419128b3bddaa,"Fake news is ""news reports that are deliberatively and indisputably fake.""News that uses fake information is becoming a threat. It becomes challenging for humans to distinguish between fake and actual news. It has become necessary to detect fake news, which seeks to determine whether a news document can be believed. Detection of fake news faces challenges in accurate classification, making existing detection algorithms ineffective. In these issues, this article uses a novel Adaptive Capsule Transient Auto Encoder (ACTAE) for effectively detecting fake news. ACTAE is a combined approach of a classifier named Capsule Auto Encoder and an algorithm called Adaptive Transient Search Optimization Algorithm. The overall detection process is performed in various stages, including preprocessing, feature withdrawal, feature selection, and classification and optimization of weight parameters of the classifier for better results. The overall process is executed in Python, proving that ACTAE detects fake news with higher accuracy (99%) and lower error rate.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adaptive Transient Search Optimization Algorithm; Additional Key Words and PhrasesFake news; anomaly detection; Capsule Auto Encoder; Feature hashing; Machine Learning; social media,Classification (of information); Fake detection; Feature extraction; Optimization; Signal encoding; Social networking (online); Adaptive transient search optimization algorithm; Additional key word and phrasesfake news; Anomaly detection; Auto encoders; Capsule auto encoder; Feature hashing; Key words; Machine-learning; Optimization algorithms; Search optimization; Social media; Anomaly detection
Prose2Poem: The Blessing of Transformers in Translating Prose to Persian Poetry,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164246452&doi=10.1145%2f3592791&partnerID=40&md5=a567e604abd16c8c0f979f6163eedb2f,"Persian poetry has consistently expressed its philosophy, wisdom, speech, and rationale based on its couplets, making it an enigmatic language on its own to both native and non-native speakers. Nevertheless, the noticeable gap between Persian prose and poems has left the two pieces of literature mediumless. Having curated a parallel corpus of prose and their equivalent poems, we introduce a novel Neural Machine Translation approach for translating prose to ancient Persian poetry using transformer-based language models in an exceptionally low-resource setting. Translating input prose into ancient Persian poetry presents two primary challenges: In addition to being reasonable in conveying the same context as the input prose, the translation must also satisfy poetic standards. Hence, we designed our method consisting of three stages. First, we trained a transformer model from scratch to obtain an initial translations of the input prose. Next, we designed a set of heuristics to leverage contextually rich initial translations and produced a poetic masked template. In the last stage, we pretrained different variations of BERT on a poetry corpus to use the masked language modelling technique to obtain final translations. During the evaluation process, we considered both automatic and human assessment. The final results demonstrate the eligibility and creativity of our novel heuristically aided approach among Literature professionals and non-professionals in generating novel Persian poems.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMachine translation; low-resource language; Persian poetry; transformers,Computational linguistics; Computer aided language translation; Modeling languages; Neural machine translation; Additional key word and phrasesmachine translation; Key words; Language model; Low resource languages; Low-resource settings; Non-native speakers; Parallel corpora; Persian poetry; Persians; Transformer; Conveying
Semi-Supervised Semantic Role Labeling with Bidirectional Language Models,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235356&doi=10.1145%2f3587160&partnerID=40&md5=8ccbedd07eab02f64c0f0ce2d1e74f8a,"The recent success of neural networks in NLP applications has provided a strong impetus to develop supervised models for semantic role labeling (SRL) that forego the requirement for extensive feature engineering. Recent state-of-the-art approaches require high-quality annotated datasets that are costly to obtain and almost unavailable for low-resource languages. We present a semi-supervised approach that utilizes both labeled and unlabeled data to provide performance improvement over a mere supervised SRL model. We show that our proposed semi-supervised SRL model provides larger improvement over a supervised model in the scenario where labeled training data size is small. Our SRL system leverages unlabeled data under the language modeling paradigm. We demonstrate that the incorporation of a self pre-trained bidirectional language model (S-PrLM) into a SRL system can help in SRL performance improvement by learning composition functions from the unlabeled data. Previous researches have concluded that syntax information is very useful for high-performing SRL systems, so we incorporate syntax information by employing an unsupervised approach to leverage dependency path information to connect argument candidates in vector space, which helps in distinguishing arguments with similar contexts but different syntactic functions. The basic idea is to connect predicate (wp) with argument candidate (wa) with the dependency path (r) between them in the embedding space. Experiments on the CoNLL-2008 and CoNLL-2009 datasets confirm that our full SRL model outperforms previous best models in terms of F1 score.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSemantic role labeling; CoNLL-2008; CoNLL-2009; contextualized representations; dependency; language models; path embedding; semantic parsing; semi-supervised; syntax; unsupervised,Computational linguistics; Embeddings; Modeling languages; Natural language processing systems; Semantics; Syntactics; Additional key word and phrasessemantic role labeling; CoNLL-2008; CoNLL-2009; Contextualized representation; Dependency; Key words; Labelings; Language model; Path embedding; Semantic parsing; Semi-supervised; Syntax; Unsupervised; Vector spaces
Metadial: A Meta-learning Approach for Arabic Dialogue Generation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164239556&doi=10.1145%2f3590960&partnerID=40&md5=18bf1c94dfec16d43a125134b58dcf23,"Dialogue generation is the automatic generation of a text response, given a user's input. Dialogue generation for low-resource languages has been a challenging tasks for researchers. However, the advancements in deep learning models have made developing conversational agents that perform the tasks of dialogue generation not only possible, but also effective and helpful in many applications spanning a variety of domains. Nevertheless, work on conversational bots for low-resource languages such as the Arabic language is still limited due to various challenges, including the language structure, vocabulary, and the scarcity of its data resources. Meta-learning has been introduced before in the natural language processing (NLP) realm and showed significant improvements in many tasks; however, it has rarely been used in natural language generation (NLG) tasks and never in Arabic NLG. In this work, we propose a meta-learning approach for Arabic dialogue generation for fast adaptation on low-resource domains, namely, Arabic. We start by using existing pre-trained models; we then meta-learn the initial parameters on high-resource dataset before finetuning the parameters on the target tasks. We prove that the proposed model that employs meta-learning techniques improves generalization and enables fast adaptation of the transformer model on low-resource NLG tasks. We report gains in the BLEU-4 and improvements in Semantic textual Similarity (STS) metrics when compared to the existing state-of-the-art approach. We also do a further study on the effectiveness of the meta-learning algorithms on the response generation of the models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTransformers; Arabic Natural Language Generation; model-agnostic meta-learning; Reptile,Deep learning; Learning systems; Natural language processing systems; Semantics; Additional key word and phrasestransformer; Arabic natural language generation; Dialogue generations; Key words; Low resource languages; Meta-learning approach; Metalearning; Model-agnostic meta-learning; Natural language generation; Reptile; Learning algorithms
Think More Ambiguity Less: A Novel Dual Interactive Model with Local and Global Semantics for Chinese Named Entity Recognition,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235588&doi=10.1145%2f3583685&partnerID=40&md5=9d73b539b9ab15ce05b13177b5caa959,"Chinese is a representative East Asian language. Chinese Named Entity Recognition (CNER) aims to recognize various entities. It is significant for other NLP tasks to utilize CNER. Recent research to develop CNER systems has been dedicated to either considering word enhancement or capturing global information to strengthen local composition and alleviate word ambiguity in the meanings of words. However, information on words acquired from external lexicons is often confused, and this has led to incorrect judgments regarding the boundaries of words. Moreover, relevant studies typically use excessively complex models to capture the global semantics of sentences. To solve these two problems, we incorporate a global representation into the procedure of local word enhancement. We propose an intuitive and effective dual-module interactive network that can enhance the boundaries of words and extract the global semantics by using a rethinking mechanism to refine the importance of local composition and global information. The results of experiments on four CNER datasets showed that the proposed model can outperform other baselines in terms of the F1 score.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAsian languages; Chinese Named Entity Recognition; using global information; word ambiguity; word enhancement,Natural language processing systems; Additional key word and phrasesasian language; Asian languages; Chinese named entity recognition; Global informations; Interactive models; Key words; Local compositions; Using global information; Word ambiguity; Word enhancement; Semantics
Vietnamese Sentiment Analysis: An Overview and Comparative Study of Fine-tuning Pretrained Language Models,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164244337&doi=10.1145%2f3589131&partnerID=40&md5=c6e299b762bc4f32434e34dc5a34af31,"Sentiment Analysis (SA) is one of the most active research areas in the Natural Language Processing (NLP) field due to its potential for business and society. With the development of language representation models, numerous methods have shown promising efficiency in fine-tuning pre-trained language models in NLP downstream tasks. For Vietnamese, many available pre-trained language models were also released, including the monolingual and multilingual language models. Unfortunately, all of these models were trained on different architectures, pre-trained data, and pre-processing steps; consequently, fine-tuning these models can be expected to yield different effectiveness. In addition, there is no study focusing on evaluating the performance of these models on the same datasets for the SA task up to now. This article presents a fine-tuning approach to investigate the performance of different pre-trained language models for the Vietnamese SA task. The experimental results show the superior performance of the monolingual PhoBERT model and ViT5 model in comparison with previous studies and provide new state-of-the-art performances on five benchmark Vietnamese SA datasets. To the best of our knowledge, our study is the first attempt to investigate the performance of fine-tuning Transformer-based models on five datasets with different domains and sizes for the Vietnamese SA task.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesVietnamese Sentiment Analysis; fine-tuning language models; monolingual BERT model; multilingual BERT model; T5 architecture,Benchmarking; Computational linguistics; Additional key word and phrasesvietnamese sentiment analyse; Fine tuning; Fine-tuning language model; Key words; Language model; Monolingual BERT model; Multilingual BERT model; Sentiment analysis; T5 architecture; Vietnamese; Sentiment analysis
Komala and Kathora: A Novel Approach Towards Classification of Hindi Poetry,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164242679&doi=10.1145%2f3589249&partnerID=40&md5=38f3347216687c1c87cc99d021531717,"Literary compositions are very often analyzed using various constituent units like words, phrases, sentences, and paragraphs. Unlike the conventional research that focuses on the aforementioned constituent units, our task is a statistical effort carried out on the most fundamental unit of any literary composition called varna, or character, followed by automated classification using learning algorithms. This article is a case study on the Hindi adaptations of two significant literary pieces, namely, Jana-Gana-Mana and Vande-Mataram, and acknowledging that the two songs being studied belong to different classes based on their bhava, i.e., the inherent emotion of the poem. The present task is the first of its kind that uses the concept of komala and kathora varna to establish diversity between the two. The two-proportion Z-test is successfully applied to statistical data pertaining to the candidate songs, thereby reestablishing the theoretical assertions by investigating real pieces of literature. Taking the statistical verification as ground, a learning-based classification system is designed to yield the best accuracy of 85%, which further compliments the theory reestablished statistically.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFeature engineering; arts; Hindi; machine learning; mathematics; personality; poem; statistical analysis; two-proportion Z-test,Machine learning; Additional key word and phrasesfeature engineering; Art; Automated classification; Fundamental units; Hindi; Key words; Machine-learning; Personality; Poem; Two-proportion Z-test; Learning algorithms
Development of a Benchmark Odia Handwritten Character Database for an Efficient Offline Handwritten Character Recognition with a Chronological Survey,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162930076&doi=10.1145%2f3583988&partnerID=40&md5=ebaee82150ebeca7167d874bc0004c5b,"A good benchmark dataset is a primary requirement in the offline handwritten character recognition (HCR) process. Only three handwritten numerals and alphabet datasets from Odia are publicly accessible for study, although many writers have used several datasets in their experiments. In this article, two tasks are done to address this issue. Those are the following: First, an extensive survey focused on various datasets is provided with the methodologies used in chronological order. The second factor is a solution to the lack of publicly available handwritten characters and numeral datasets. A new dataset of handwritten Odia characters with numerals has been developed. Anyone can access this dataset by sending an email to the authors of the article. This dataset was created with the help of 150 volunteers of various age groups, races, and qualifications. Some homogeneous experiments are conducted using deep learning models to evaluate the consistency of the dataset. One heterogeneous trial has also been performed to estimate the complexities of the characters present in the dataset by comparing them with the existing benchmark datasets.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",""" Odia scripts; ""matras"" and ""juktas; Additional Key Words and PhrasesBounding box; benchmark evaluation; convolutional neural network (CNN); median blur filtering; pre-trained deep learning networks; preprocessing; threshold","Character recognition; Convolutional neural networks; Deep learning; Learning systems; "" odia script; ""matra"" and ""juktas; Additional key word and phrasesbounding box; Benchmark evaluation; Convolutional neural network; Key words; Learning network; Median blur filtering; Pre-trained deep learning network; Preprocessing; Threshold; Median filters"
From Softmax to Nucleusmax: A Novel Sparse Language Model for Chinese Radiology Report Summarization,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235257&doi=10.1145%2f3596219&partnerID=40&md5=1b2232b3a03135bf89ec7674adfd0468,"The Chinese radiology report summarization is a crucial component in smart healthcare that employs language models to summarize key findings in radiology reports and communicate these findings to physicians. However, most language models for radiology report summarization utilize a softmax transformation in their output layer, leading to dense alignments and strictly positive output probabilities. This density is inefficient, reducing model interpretability and giving probability mass to many unrealistic outputs. To tackle this issue, we propose a novel approach named nucleusmax. Nucleusmax is able to mitigate dense outputs and improve model interpretability by truncating the unreliable tail of the probability distribution. In addition, we incorporate nucleusmax with a copy mechanism, a useful technique to avoid professional errors in the generated diagnostic opinions. To further promote the research of radiology report summarization, we also have created a Chinese radiology report summarization dataset, which is freely available. Experimental results showed via both automatic and human evaluation that the proposed approach substantially improves the sparsity and overall quality of outputs over competitive softmax models, producing radiology summaries that approach the quality of those authored by physicians. In general, our work demonstrates the feasibility and prospect of the language model to the domain of radiology and smart healthcare.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",abstractive summarization; Additional Key Words and PhrasesChinese radiology report summarization; language model; softmax,Computational linguistics; Health care; Probability distributions; Abstractive summarization; Additional key word and phraseschinese radiology report summarization; Dense output; Interpretability; Key words; Language model; Output layer; Radiology reports; Softmax; Sparse languages; Radiology
Image-Text Multimodal Sentiment Analysis Framework of Assamese News Articles Using Late Fusion,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235139&doi=10.1145%2f3584861&partnerID=40&md5=4160f0d111153412c823d3608eff3d94,"Before the arrival of the web as a corpus, people detected positive and negative news based on the understanding of the textual content from physical newspaper rather than an automatic identification approach from readily available e-newspapers. Thus, the earlier sentiment analysis approach is based on unimodal data, and less effort is paid to the multimodal data. However, the presence of multimodal information helps us to get a clearer understanding of the sentiment. To the best of our knowledge, less work has been introduced on the image-text multimodal sentiment analysis framework of Assamese, a low-resource Indian language mostly spoken in the northeast part of India. We built an Assamese news articles dataset consisting of news text and associated images and one image caption to conduct an experimental study. Focusing on important words and discriminative regions of the images mostly related to sentiment, two individual unimodal such as textual and visual models are proposed. The visual model is developed using an encoder-decoder-based image caption generation system. An image-text multimodal approach is proposed to explore the internal correlation between textual and visual features for joint sentiment classification. Finally, we propose the multimodal sentiment analysis framework, i.e., Textual Visual Multimodal Fusion, by employing a late fusion scheme to merge the three different modalities for the final sentiment prediction. Experimental results conducted on the Assamese dataset built in-house demonstrate that the contextual integration of multimodal features delivers better performance than unimodal features.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMultimodal sentiment analysis; caption generation; late fusion; low resource language; machine learning classifier,Automation; Image analysis; Image classification; Image fusion; Modal analysis; Newsprint; Sentiment analysis; Additional key word and phrasesmultimodal sentiment analyse; Caption generation; Image texts; Key words; Late fusion; Learning classifiers; Low resource languages; Machine learning classifier; Machine-learning; Sentiment analysis; Machine learning
Cross-lingual Text Reuse Detection at Document Level for English-Urdu Language Pair,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164242224&doi=10.1145%2f3592761&partnerID=40&md5=6a07783ac832055d6c08b282f10e4b65,"In recent years, the problem of Cross-Lingual Text Reuse Detection (CLTRD) has gained the interest of the research community due to the availability of large digital repositories and automatic Machine Translation (MT) systems. These systems are readily available and openly accessible, which makes it easier to reuse text across languages but hard to detect. In previous studies, different corpora and methods have been developed for CLTRD at the sentence/passage level for the English-Urdu language pair. However, there is a lack of large standard corpora and methods for CLTRD for the English-Urdu language pair at the document level. To overcome this limitation, the significant contribution of this study is the development of a large benchmark cross-lingual (English-Urdu) text reuse corpus, called the TREU (Text Reuse for English-Urdu) corpus. It contains English to Urdu real cases of text reuse at the document level. The corpus is manually labelled into three categories (Wholly Derived = 672, Partially Derived = 888, and Non Derived = 697) with the source text in English and the derived text in the Urdu language. Another contribution of this study is the evaluation of the TREU corpus using a diversified range of methods to show its usefulness and how it can be utilized in the development of automatic methods for measuring cross-lingual (English-Urdu) text reuse at the document level. The best evaluation results, for both binary (F1 = 0.78) and ternary (F1 = 0.66) classification tasks, are obtained using a combination of all Translation plus Mono-lingual Analysis (T+MA) based methods. The TREU corpus is publicly available to promote CLTRD research in an under-resourced language, i.e., Urdu.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCross-lingual text reuse; cross-lingual sentence embedding; cross-lingual text reuse detection; English-Urdu language pair; Translation Plus Mono-lingual Analysis,Translation (languages); Additional key word and phrasescross-lingual text reuse; Cross-lingual; Cross-lingual sentence embedding; Cross-lingual text reuse detection; Embeddings; English-urdu language pair; Key words; Language pairs; Reuse; Translation plus mono-lingual analyse; Computational linguistics
LFWE: Linguistic Feature Based Word Embedding for Hindi Fake News Detection,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164240285&doi=10.1145%2f3589764&partnerID=40&md5=49c894a058508ea515ab34972351eb39,"It is essential for research communities to investigate ways for authenticating news. The use of linguistic feature based analysis to automatically detect false news is gaining popularity among the scientific community. However, such techniques are exclusively created for English, leaving low-resource languages like Hindi behind. To address this issue, we constructed a novel annotated Hindi Fake News (HinFakeNews) dataset of roughly 33,300 articles that can be utilized to develop autonomous fake news detection systems. This work provides a two-stage benchmark model for identifying fake news in Hindi using machine learning. The proposed model, LFWE (Linguistic Feature Based Word Embedding), generates word embedding over linguistic features. This article focuses on 23 key linguistic features (15 extracted and 08 derived) for successful detection of Hindi fake news. These features are grouped as lexical, semantic, syntactic, psycho-linguistic, readability, and quantity features. The contribution is twofold. In the first phase, the dataset is preprocessed and linguistic features are extracted. In the second phase, feature sets are generated as word embeddings, and an Ensemble voting classification is carried out on the feature sets. According to experimental findings, the LFWE model accurately detects and classifies fake news in Hindi with an accuracy of 98.49%.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFake news; linguistic features; machine learning (ML); text classification; voting classifier; word embedding (WE),Classification (of information); Fake detection; Feature extraction; Semantics; Text processing; Additional key word and phrasesfake news; Embeddings; Feature-based; Key words; Linguistic features; Machine learning; Machine-learning; Text classification; Voting classifiers; Word embedding; Embeddings
Part-of-Speech Tagging of Odia Language Using Statistical and Deep Learning Based Approaches,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164237824&doi=10.1145%2f3588900&partnerID=40&md5=320153a08893f9a497203024ab7f1904,"Automatic part-of-speech (POS) tagging is a preprocessing step of many natural language processing tasks, such as named entity recognition, speech processing, information extraction, word sense disambiguation, and machine translation. It has already gained promising results in English and European languages. However, in Indian languages, particularly in the Odia language, it is not yet well explored because of the lack of supporting tools, resources, and morphological richness of the language. Unfortunately, we were unable to locate an open source POS tagger for the Odia language, and only a handful of attempts have been made to develop POS taggers for the Odia language. The main contribution of this research work is to present statistical approaches such as the maximum entropy Markov model and conditional random field (CRF), as well as deep learning based approaches, including the convolutional neural network (CNN) and bidirectional long short-term memory (Bi-LSTM) to develop the Odia POS tagger. A publicly accessible corpus annotated with the Bureau of Indian Standards (BIS) tagset is used in our work. However, most of the languages around the globe have used the dataset annotated with the Universal Dependencies (UD) tagset. Hence, to maintain uniformity, the Odia dataset should use the same tagset. Thus, following the BIS and UD guidelines, we constructed a mapping from the BIS tagset to the UD tagset. The maximum entropy Markov model, CRF, Bi-LSTM, and CNN models are trained using the Indian Languages Corpora Initiative corpus with the BIS and UD tagsets. We have experimented with various feature sets as input to the statistical models to prepare a baseline system and observed the impact of constructed feature sets. The deep learning based model includes the Bi-LSTM network, the CNN network, the CRF layer, character sequence information, and a pre-trained word vector. Seven different combinations of neural sequence labeling models are implemented, and their performance measures are investigated. It has been observed that the Bi-LSTM model with the character sequence feature and pre-trained word vector achieved a result with 94.58% accuracy.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPart of speech (POS); conditional random field (CRF); deep learning; word embedding,Convolutional neural networks; Long short-term memory; Markov processes; Multilayer neural networks; Natural language processing systems; Speech recognition; Speech transmission; Syntactics; Additional key word and phrasespart of speech; Conditional random field; Convolutional neural network; Deep learning; Embeddings; Indian standards; Key words; Parts-of-speech tagging; Random fields; Word embedding; Computational linguistics
Improving Multilingual Neural Machine Translation System for Indic Languages,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235681&doi=10.1145%2f3587932&partnerID=40&md5=93a5704b8eb0aaa989b8e7985fa2a1b2,"The Machine Translation System (MTS) serves as effective tool for communication by translating text or speech from one language to another language. Recently, neural machine translation (NMT) has become popular for its performance and cost-effectiveness. However, NMT systems are restricted in translating low-resource languages as a huge quantity of data is required to learn useful mappings across languages. The need for an efficient translation system becomes obvious in a large multilingual environment like India. Indian languages (ILs) are still treated as low-resource languages due to unavailability of corpora. In order to address such an asymmetric nature, the multilingual neural machine translation (MNMT) system evolves as an ideal approach in this direction. The MNMT converts many languages using a single model, which is extremely useful in terms of training process and lowering online maintenance costs. It is also helpful for improving low-resource translation. In this article, we propose an MNMT system to address the issues related to low-resource language translation. Our model comprises two MNMT systems, i.e., for English-Indic (one-to-many) and for Indic-English (many-to-one) with a shared encoder-decoder containing 15 language pairs (30 translation directions). Since most of IL pairs have a scanty amount of parallel corpora, not sufficient for training any machine translation model, we explore various augmentation strategies to improve overall translation quality through the proposed model. A state-of-the-art transformer architecture is used to realize the proposed model. In addition, the article addresses the use of language relationships (in terms of dialect, script, etc.), particularly about the role of high-resource languages of the same family in boosting the performance of low-resource languages. Moreover, the experimental results also show the advantage of back-translation and domain adaptation for ILs to enhance the translation quality of both source and target languages. Using all these key approaches, our proposed model emerges to be more efficient than the baseline model in terms of evaluation metrics, i.e., BLEU (BiLingual Evaluation Understudy) score for a set of ILs.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMultilingual neural machine translation system (MNMT); BLEU score; corpus; Indic languages (ILs); low resource language,Computational linguistics; Computer aided language translation; Cost effectiveness; Speech communication; Speech transmission; Additional key word and phrasesmultilingual neural machine translation system (multilingual neural machine translation); Bilingual evaluation understudy score; Bilinguals; Corpus; Indic language; Key words; Language pairs; Low resource languages; Machine translation systems; Performance; Neural machine translation
Enhancing RDF Verbalization with Descriptive and Relational Knowledge,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164234697&doi=10.1145%2f3595293&partnerID=40&md5=8a6ed2dfae68aa265769e2c437831282,"RDF verbalization has received increasing interest, which aims to generate a natural language description of the knowledge base. Sequence-to-sequence models based on Transformer are able to obtain strong performance equipped with pre-trained language models such as BART and T5. However, in spite of the general performance gain introduced by the pre-trained models, the performance of the task is still limited by the small scale of the training dataset. To address the problem, we propose two orthogonal strategies to enhance the representation learning of RDF triples. Concretely, two types of knowledge are introduced, i.e., descriptive knowledge and relational knowledge, respectively. The descriptive knowledge indicates the semantic information of self definition, and the relational knowledge indicates the semantic information learned from the structural context. We further combine the descriptive and relational knowledge together to enhance the representation learning. Experimental results on the WebNLG and SemEval-2010 datasets show that the two types of knowledge can both enhance the model performance, and their combination is able to obtain further improvements in most cases, providing new state-of-the-art results.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRDF Verbalization; Descriptive Knowledge; Pre-trained Language Models; Relational Knowledge,Computational linguistics; Resource Description Framework (RDF); Semantics; Additional key word and phrasesrdf verbalization; Descriptive knowledge; Key words; Language description; Language model; Natural languages; Performance; Pre-trained language model; Relational knowledge; Semantics Information; Knowledge based systems
Contrastive Adversarial Training for Multi-Modal Machine Translation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164244138&doi=10.1145%2f3587267&partnerID=40&md5=bcefdbafcebd3ee2d0b9f61201827714,"The multi-modal machine translation task is to improve translation quality with the help of additional visual input. It is expected to disambiguate or complement semantics while there are ambiguous words or incomplete expressions in the sentences. Existing methods have tried many ways to fuse visual information into text representations. However, only a minority of sentences need extra visual information as complementary. Without guidance, models tend to learn text-only translation from the major well-aligned translation pairs. In this article, we propose a contrastive adversarial training approach to enhance visual participation in semantic representation learning. By contrasting multi-modal input with the adversarial samples, the model learns to identify the most informed sample that is coupled with a congruent image and several visual objects extracted from it. This approach can prevent the visual information from being ignored and further fuse cross-modal information. We examine our method in three multi-modal language pairs. Experimental results show that our model is capable of improving translation accuracy. Further analysis shows that our model is more sensitive to visual information.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesContrastive Learning; adversarial training; multi-modal machine translation,Computational linguistics; Computer aided language translation; Learning systems; Semantics; Visual languages; Additional key word and phrasescontrastive learning; Adversarial training; Key words; Learn+; Machine translations; Multi-modal; Multi-modal machine translation; Text representation; Translation quality; Visual information; Machine translation
Robust Multi-task Learning-based Korean POS Tagging to Overcome Word Spacing Errors,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235460&doi=10.1145%2f3591206&partnerID=40&md5=28d1e18d631e300dd93fc376274aa72e,"End-to-end neural network-based approaches have recently demonstrated significant improvements in natural language processing (NLP). However, in the NLP application such as assistant systems, NLP components are still processed to extract results using a pipeline paradigm. The pipeline-based concept has issues with error propagation. In Korean, morphological analysis and part-of-speech (POS) tagging step, incorrectly analyzing POS tags for a sentence containing spacing errors negatively affects other modules behind the POS module. Hence, we present a multi-task learning-based POS tagging neural model for Korean with word spacing challenges. When we apply this model to the Korean morphological analysis and POS tagging, we get findings that are robust to word spacing errors. We adopt syllable-level input and output formats, as well as a simple structure for ELECTRA and RNN-CRF models for multi-task learning, and we achieve a good performance 98.30 of F1, better than previous studies on the Sejong corpus test set.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMorphological analysis; multi-task learning; part-of-speech tagging; word spacing,Backpropagation; Computational linguistics; Errors; Learning systems; Natural language processing systems; Syntactics; Additional key word and phrasesmorphological analyse; End to end; Key words; Morphological analysis; Multitask learning; Neural-networks; Part of speech tagging; Parts-of-speech tagging; Spacing errors; Word-spacing; Pipelines
Cross-lingual Sentence Embedding for Low-resource Chinese-Vietnamese Based on Contrastive Learning,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164236307&doi=10.1145%2f3589341&partnerID=40&md5=e6a254a7edd6916c60632d0c8f221d33,"Cross-lingual sentence embedding's goal is mapping sentences with similar semantics but in different languages close together and dissimilar sentences farther apart in the representation space. It is the basis of many downstream tasks such as cross-lingual document matching and cross-lingual summary extraction. At present, the works of cross-lingual sentence embedding tasks mainly focus on languages with large-scale corpus. But low-resource languages such as Chinese-Vietnamese are short of sentence-level parallel corpora and clear cross-lingual monitoring signals, and these works on low-resource languages have poor performances. Therefore, we propose a cross-lingual sentence embedding method based on contrastive learning and effectively fine-tune powerful pretraining mode by constructing sentence-level positive and negative samples to avoid the catastrophic forgetting problem of the traditional fine-tuning pre-trained model based only on small-scale aligned positive samples. First, we construct positive and negative examples by taking parallel Chinese Vietnamese sentences as positive examples and non-parallel sentences as negative examples. Second, we construct a siamese network to get contrastive loss by inputting positive and negative samples and fine-tuning our model. The experimental results show that our method can effectively improve the semantic alignment accuracy of cross-lingual sentence embedding in Chinese and Vietnamese contexts.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesChinese-Vietnamese; cross-lingual sentence embedding; low-resource language; mBERT; siamese network,Semantics; Tuning; Additional key word and phraseschinese-vietnamese; Cross-lingual; Cross-lingual sentence embedding; Embeddings; Key words; Low resource languages; MBERT; Sentence level; Siamese network; Vietnamese; Embeddings
Knowledge-enhanced Prompt-tuning for Stance Detection,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164238045&doi=10.1145%2f3588767&partnerID=40&md5=a148c8d43eea427317f57d6966c60708,"Investigating public attitudes on social media is important in opinion mining systems. Stance detection aims to analyze the attitude of an opinionated text (e.g., favor, neutral, or against) toward a given target. Existing methods mainly address this problem from the perspective of fine-tuning. Recently, prompt-tuning has achieved success in natural language processing tasks. However, conducting prompt-tuning methods for stance detection in real-world remains a challenge for several reasons: (1) The text form of stance detection is usually short and informal, which makes it difficult to design label words for the verbalizer. (2) The tweet text may not explicitly give the attitude. Instead, users may use various hashtags or background knowledge to express stance-aware perspectives. In this article, we first propose a prompt-tuning-based framework that performs stance detection in a cloze question manner. Specifically, a knowledge-enhanced prompt-tuning framework (KEprompt) method is designed, which consists of an automatic verbalizer (AutoV) and background knowledge injection (BKI). Specifically, in AutoV, we introduce a semantic graph to build a better mapping from the predicted word of the pretrained language model and detection labels. In BKI, we first propose a topic model for learning hashtag representation and introduce ConceptGraph as the supplement of the target. At last, we present a challenging dataset for stance detection, where all stance categories are expressed in an implicit manner. Extensive experiments on a large real-world dataset demonstrate the superiority of KEprompt over state-of-the-art methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesStance detection; deep learning; prompt-tuning framework,Deep learning; Large dataset; Semantics; Additional key word and phrasesstance detection; Background knowledge; Deep learning; Hashtags; Key words; Mining systems; Opinion mining; Prompt-tuning framework; Public attitudes; Social media; Sentiment analysis
Rule Based Fuzzy Computing Approach on Self-Supervised Sentiment Polarity Classification with Word Sense Disambiguation in Machine Translation for Hindi Language,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162176319&doi=10.1145%2f3574130&partnerID=40&md5=723835642e0a8f75d1700b2ccb79a024,"With increasing globalization, communication among people of diverse cultural backgrounds is also taking place to a very large extent in the present era. Issues like language diversity in various parts of the world can lead to hindrance in communication. The usage of social media and user-generated material has grown at an exponential rate and existing supervised sentiment polarity classification techniques need labelling for the training dataset. In this study, two problems have been analyzed. First, sentiment analysis of the Twitter dataset and sense disambiguation of morphologically rich Hindi language. A rule-based fuzzy logics-based system for self-supervised sentiment classification was used to compute and analyze the self-supervised or completely unsupervised sentiment categorization of a social-media dataset using three types of lexicons. The combination of fuzzy with three different types of lexicons gives sentiment analysis a new path. The unsupervised fuzzy rules integrate the fuzziness of both negative as well as positive scores, and fuzzy logic-based systems can cope with ambiguity and vagueness. The fuzzy-system uses an unsupervised/self-supervised fuzzy rule-based technique to identify text using natural language processing (NLP) and sense of word. We compared the results of fuzzy rule based self-supervised sentiment classification by using three types of lexicons on five different datasets, with unsupervised as well as supervised sentiment classification techniques. Second, using cross-lingual sense embedding rather than cross-lingual word embedding resolves the ambiguity issue. The word sense embeddings are produced for the source languages to learn multiple or various senses of the words. Different evaluation metrics depict an improved performance for English-Hindi language.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",fuzzy sets; lexicon; self-learning; Sentiment analysis; unsupervised sentiment classification,Classification (of information); Computational linguistics; Embeddings; Fuzzy inference; Fuzzy rules; Social networking (online); Classification technique; Embeddings; Lexicon; Polarity classification; Rule based; Self-learning; Sentiment analysis; Sentiment classification; Social media; Unsupervised sentiment classification; Sentiment analysis
Personality Detection using Kernel-based Ensemble Model for Leveraging Social Psychology in Online Networks,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162142808&doi=10.1145%2f3571584&partnerID=40&md5=659f82791711330a14143d89a012f5df,"The Asian social networking market dominates the world landscape with the highest consumer penetration rate. Businesses and investors often look for winning strategies to attract consumers to increase revenues from sales, advertisements, and other services offered on social media platforms. Social media engagement and online relational cohesion have often been defined within the frameworks of social psychology and personality identification is a possible way in which social psychology can inform, engage, and learn from social media. Personality profiling has many real-world applications, including preference-based recommendation systems, relationship building, and career counseling. This research puts forward a novel kernel-based soft-voting ensemble model for personality detection from natural language, KBSVE-P. The KBSVE-P model is built by first evaluating the performance of various Support Vector Machine (SVM) kernels, namely radial basis function (RBF), linear, sigmoidal, and polynomial, to find the best-suited kernel for automatic personality detection in natural language text. Next, an ensemble of SVM kernels is implemented with a variety of voting techniques, such as soft voting, hard voting, and weighted hard voting. The model is evaluated on the publicly available Kaggle_MBTI dataset and a novel South Asian, Indian, low-resource Hindi language _MBTI (pronounced as vishesh charitr, meaning personality in Hindi) dataset for detecting a user's personality across four personality traits, namely introvert/extrovert (IE), thinking/feeling (TF), sensing/intuitive (SI), and judging/perceiving (JP). The proposed kernel-based ensemble with soft voting, KBSVE-P, outperforms the existing models on English Kaggle-MBTI dataset with an average F-score of 85.677 and achieves an accuracy of 66.89 for the Hindi _MBTI dataset.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",kernel-based methods; natural language; personality psychology; Social Networks,Social networking (online); Social psychology; Support vector machines; Ensemble models; Kernel based methods; Natural languages; On-line network; Personality detections; Personality psychology; Social media; Social network; Social psychology; Soft voting; Radial basis function networks
Generation of Voice Signal Tone Sandhi and Melody Based on Convolutional Neural Network,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162187718&doi=10.1145%2f3545569&partnerID=40&md5=dadf73fc1927be11ed524a07d401b832,"There is a need to prevent the use of modulated voice signals to conduct criminal activities. Voice signal change detection based on convolutional neural networks is proposed. We use three commonly used voice processing software (Audacity, CoolEdit, and RTISI) to change tones in voice libraries. The research further raises each voice by five semitones and are recorded at different levels (+4, +5, +6, +7, and +8, respectively). Simultaneously, every voice is lowered by five halftones, represented as -4, -5, -6, -7, and -8, respectively. The convolution neural network corresponding to network b-3 is determined as the final classifier in this article through experiments. The average accuracy A1 of its three categories has reached more than 97%, the detection accuracy A2 of electronic tone sandhi speech has reached more than 97%, and the false alarm rate of the original speech is less than 1.9%. The outcomes obtained shows that the detection algorithm in this article is effective, and it has good generalization ability.  © 2023 Association for Computing Machinery.",Detection algorithm,Convolutional neural networks; Signal detection; Speech recognition; Change detection; Convolution neural network; Convolutional neural network; Criminal activities; Detection algorithm; Level 4; Processing software; Signal changes; Three categories; Voice signals; Convolution
Opinion Leader Detection in Asian Social Networks using Modified Spider Monkey Optimization,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162171419&doi=10.1145%2f3555311&partnerID=40&md5=da1c2ddb692c60f1658e96a5dcba2eb6,"The Asian social networks are dominated by the society's collectivist culture, and this interestingly introduces an influence mechanism aided by word-of-mouth and opinion leaders. An opinion leader can help to generate and shape other people's opinion and achieve a high information spread on any topic. In this work, a modified spider monkey optimization based opinion leader detection approach is proposed. Firstly, we employ the modified node2vec graph embedding to generate the lower dimensional vectors which act as the initial features for the nodes in a typical Asian social network. Next, the entire population is broken down into several groups using the k-means++ algorithm where the number of clusters is equal to the number of opinion leaders to be selected. The local and global leaders are chosen by using the coordinates of the cluster centres of these clusters. The coordinates of the centroids of the clusters are then used to detect the local and global leaders in the network. The local leaders then form the seed set of opinion leaders for the network. The positions of the nodes in the network, including the local and global leaders, are updated over a number of iterations. At the end of these iterations, the seed set generating the maximum influence forms the set of opinion leaders in the network. We test our proposed approach using the popular information diffusion and cognitive opinion dynamics (COD) models. We perform intensive experiments on several real-life social networks based on various performance metrics. The results obtained reveal that the proposed approach outperforms several existing techniques of opinion leader detection.  © 2023 Association for Computing Machinery.",Asian social networks (ASN); k-means clustering; opinion leader; spider monkey optimization,Social networking (online); Asian social network; Detection approach; Graph embeddings; Influence mechanism; K-means++ clustering; Leader detections; Opinion leaders; Optimisations; Seed-set; Spider monkey optimization; K-means clustering
Fast and Accurate Framework for Ontology Matching in Web of Things,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162144185&doi=10.1145%2f3578708&partnerID=40&md5=ebc311db726dbea23a4bd36e3aa8ee75,"The Web of Things (WoT) can help with knowledge discovery and interoperability issues in many Internet of Things (IoT) applications. This article focuses on semantic modeling of WoT and proposes a new approach called Decomposition for Ontology Matching (DOM) to discover relevant knowledge by exploring correlations between WoT data using decomposition strategies. The DOM technique adopts several decomposition techniques to order highly linked ontologies of WoT data into similar groups. The main idea is to decompose the instances of each ontology into similar groups and then match instances of similar groups instead of entire instances of two ontologies. Three main algorithms for decomposition have been developed. The first algorithm is based on radar scanning, which determines the distribution of distances between each instance and all other instances to determine the cluster centroid. The second algorithm is based on adaptive grid clustering, where it focuses on distribution information and the construction of spanning trees. The third algorithm is based on split index clustering, where instances are divided into groups of cells from which noise is removed during the merging process. Several studies were conducted with different ontology databases to illustrate the use of the DOM technique. The results show that DOM outperforms state-of-The-Art ontology matching models in terms of computational cost while maintaining the quality of the matching. Moreover, these results demonstrate that DOM is capable of handling various large datasets in WoT contexts.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",decomposition; Ontology matching; Web of Things,Clustering algorithms; Large dataset; Ontology; Semantics; Trees (mathematics); Adaptive grids; Cluster centroids; Decomposition strategy; Decomposition technique; Grid clustering; Matching techniques; New approaches; Ontology matching; Ontology's; Semantic modelling; Internet of things
Improving Readability for Automatic Speech Recognition Transcription,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162088232&doi=10.1145%2f3557894&partnerID=40&md5=2195810cbb037f6b57bcee3b004b807b,"Modern Automatic Speech Recognition (ASR) systems can achieve high performance in terms of recognition accuracy. However, a perfectly accurate transcript still can be challenging to read due to grammatical errors, disfluency, and other noises common in spoken communication. These readable issues introduced by speakers and ASR systems will impair the performance of downstream tasks and the understanding of human readers. In this work, we present a task called ASR post-processing for readability (APR) and formulate it as a sequence-to-sequence text generation problem. The APR task aims to transform the noisy ASR output into a readable text for humans and downstream tasks while maintaining the semantic meaning of speakers. We further study the APR task from the benchmark dataset, evaluation metrics, and baseline models: First, to address the lack of task-specific data, we propose a method to construct a dataset for the APR task by using the data collected for grammatical error correction. Second, we utilize metrics adapted or borrowed from similar tasks to evaluate model performance on the APR task. Lastly, we use several typical or adapted pre-trained models as the baseline models for the APR task. Furthermore, we fine-tune the baseline models on the constructed dataset and compare their performance with a traditional pipeline method in terms of proposed evaluation metrics. Experimental results show that all the fine-tuned baseline models perform better than the traditional pipeline method, and our adapted RoBERTa model outperforms the pipeline method by 4.95 and 6.63 BLEU points on two test sets, respectively. The human evaluation and case study further reveal the ability of the proposed model to improve the readability of ASR transcripts.  © 2023 Association for Computing Machinery.",Automatic speech recognition; data synthesis; post-processing for readability; pre-trained model,Error correction; Pipelines; Speech recognition; Automatic speech recognition; Automatic speech recognition system; Baseline models; Data synthesis; Grammatical errors; Performance; Pipeline methods; Post-processing; Post-processing for readability; Pre-trained model; Semantics
A Context-focused Attention Evolution Model for Aspect-based Sentiment Classification,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162106107&doi=10.1145%2f3587465&partnerID=40&md5=8e2d6c99729f8639c0178ce49de6e102,"Due to their inherent capability in the semantic alignment of aspects and their context words, Attention and Long-Short-Term-Memory (LSTM) mechanisms are widely adopted for Aspect-Based Sentiment Classification (ABSC) tasks. Instead, it is challenging to handle long-range word dependencies on multiple entities due to the deficiency in attention mechanisms. To solve this problem, we propose a Context-Focused Aspect-Based Network to align attention before LSTM, making the model focus more on aspect-related words and ignore irrelevant words, improving the accuracy of final classification. This can either alleviate attention distraction or reinforce the text representation ability. Experiments on two benchmark datasets show that the results achieve respectable performance compared to the state-of-the-art methods available in ABSC. Our approach has the potential to improve classification accuracy by adaptively adjusting the focus on context.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Aspect-Based Sentiment Classification; Bert; masked language model,Benchmarking; Semantics; Aspect-based sentiment classification; Bert; Classification tasks; Context-word; Evolution modeling; Language model; Masked language model; Memory mechanism; Semantic alignments; Sentiment classification; Long short-term memory
Combination of Loss-based Active Learning and Semi-supervised Learning for Recognizing Entities in Chinese Electronic Medical Records,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162099095&doi=10.1145%2f3588314&partnerID=40&md5=06bfe9df0d958f5ccfb77c307a5b79fc,"The recognition of entities in an electronic medical record (EMR) is especially important to downstream tasks, such as clinical entity normalization and medical dialogue understanding. However, in the medical professional field, training a high-quality named entity recognition system always requires large-scale annotated datasets, which are highly expensive to obtain. In this article, to lower the cost of data annotation and maximizing the use of unlabeled data, we propose a hybrid approach to recognizing the entities in Chinese electronic medical record, which is in combination of loss-based active learning and semi-supervised learning. Specifically, we adopted a dynamic balance strategy to dynamically balance the minimum loss predicted by a named entity recognition decoder and a loss prediction module at different stages in the process. Experimental results demonstrated our proposed framework's effectiveness and efficiency, achieving higher performances than existing approaches on Chinese EMR entity recognition datasets under limited labeling resources.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dynamic balance strategy; Electronic medical record; loss-based active learning; semi-supervised learning,E-learning; Learning algorithms; Medical computing; Active Learning; Balance strategies; Down-stream; Dynamic balance; Dynamic balance strategy; Electronic medical record; Loss-based active learning; Medical record; Named entity recognition; Semi-supervised learning; Large dataset
Approximating to the Real Translation Quality for Neural Machine Translation via Causal Motivated Methods,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162183220&doi=10.1145%2f3583684&partnerID=40&md5=ff17a7bc827f41806fd9081f05c070c1,"It is hard to evaluate translations objectively and accurately, which limits the applications of machine translation. In this article, we assume that the above phenomenon is caused by noise interference during translation evaluation, and we handle the problem through a perspective of causal inference. We assume that the observable translation score is affected by the unobservable true translation quality and some noise simultaneously. If there is a variable that is related to the noise and independent to the true translation quality, the related noise can be eliminated by removing the effect of that variable from the observed score. Based on the above causality hypothesis, this article studies the length bias problem of beam search for neural machine translation (NMT) and the input related noise problem of translation quality estimation (QE). For the NMT length bias problem, we conduct the experiments on four typical NMT tasks (Uyghur-Chinese, Chinese-English, English-German, and English-French) with different scales of datasets. Comparing with previous approaches, the proposed causal motivated method is model-agnostic and does not require supervised training. For QE tasks, we conduct the experiments on the WMT'20 submissions. Experimental results show that the denoised QE results gain better Pearson's correlation scores with human assessed scores compared to the original submissions. Further analyses on the NMT and QE tasks also demonstrate the rationality of the empirical assumptions made on our methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",causal inference; half-sibling regression; Neural machine translation; quality estimation,Computational linguistics; Correlation methods; Neural machine translation; Noise pollution; Beam search; Bias problems; Causal inferences; Half-sibling regression; Machine translations; Noise interference; Noise problems; Quality estimation; Translation quality; Unobservable; Computer aided language translation
A Framework for Online Hate Speech Detection on Code-mixed Hindi-English Text and Hindi Text in Devanagari,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162092055&doi=10.1145%2f3568673&partnerID=40&md5=d8a449357016ad28de4f71b93fce34a7,"Social Media has been growing and has provided the world with a platform to opine, debate, display, and discuss like never before. It has a major influence in research areas that analyze human behavior and social groups, and the phenomenon of social interactions is even being used in areas such as Internet of Things. This constant stream of data connecting individuals and organizations across the globe has had a tremendous impact on the functioning of society and even has the power to sway elections. Despite having numerous benefits, social media has certain issues such as the prevalence of fake news, which has also led to the rise of the hate speech phenomenon. Due to lax security throughout these social media platforms, these issues continue to exist without any repercussions. This leads to cyberbullying, defamation, and presents grave security concerns. Even though some work has been done independently on native scripts, hate speech detection, and code-mixed data, there exists a lack of academic work and research in the area of detecting hate speech in transliterated code-mixed data and in-text containing native language scripts. Research in this field is inhibited greatly due to the multiple variations in grammar and spelling and in general a lack of availability of annotated datasets, especially when it comes to native languages. This article comes up with a method to automate hate speech detection in code-mixed and native language text. The article presents an architecture containing a Tabnet classifier-based model trained on features extracted using MuRIL from transliterated code-mixed textual data. The article also shows that the same model works well on features extracted from text in Devanagari despite being trained on transliterated data.  © 2023 Association for Computing Machinery.",code-mixing; Cyber security; cyber security systems; hate speech detection; Hindi text classification; natural language processing; TabNet,Behavioral research; Classification (of information); Codes (symbols); Cybersecurity; Data mining; Fake detection; Natural language processing systems; Online systems; Speech recognition; Text processing; Code-mixing; Cybe security system; Cyber security; Hate speech detection; Hindi text classification; Language processing; Natural language processing; Natural languages; Speech detection; Tabnet; Text classification; Social networking (online)
Mutual Supervised Fusion & Transfer Learning with Interpretable Linguistic Meaning for Social Data Analytics,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162120141&doi=10.1145%2f3568675&partnerID=40&md5=111c40ed1d21bc6d8a1dc6aa43bd1e8a,"Social data analytics is often taken as the most commonly used method for community discovery, product recommendations, knowledge graph, and so on. In this study, social data are firstly represented in different feature spaces by using various feature extraction algorithms. Then we build a transfer learning model to leverage knowledge from multiple feature spaces. During modeling, since the assumption that the training and the testing data have the same distribution is always true, we give a theorem and its proof which asserts the necessary and sufficient condition for achieving a minimum testing error. We also theoretically demonstrate that maximizing the classification error consistency across different feature spaces can improve the classification performance. Additionally, the cluster assumption derived from semi-supervised learning is introduced to enhance knowledge transfer. Finally, a Tagaki-Sugeno-Kang (TSK) fuzzy system-based learning algorithm is proposed, which can generate interpretable fuzzy rules. Experimental results not only demonstrate the promising social data classification performance of our proposed approach but also show its interpretability which is missing in many other models.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesTSK fuzzy system; classification error consensus; cluster assumption; mutual supervised fusion; social data analytics,Classification (of information); Data mining; Errors; Fuzzy inference; Fuzzy systems; Knowledge management; Learning algorithms; Learning systems; Additional key word and phrasestsk fuzzy system; Classification error consensus; Classification errors; Cluster assumptions; Data analytics; Feature space; Key words; Mutual supervised fusion; Social data analytic; Social datum; Data Analytics
Hybrid Deep Learning Model for Sarcasm Detection in Indian Indigenous Language Using Word-Emoji Embeddings,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161972132&doi=10.1145%2f3519299&partnerID=40&md5=61c551c3d79c74c7c335847a4751f4df,"Automated sarcasm detection is deemed as a complex natural language processing task and extending it to a morphologically-rich and free-order dominant indigenous Indian language Hindi is another challenge in itself. The scarcity of resources and tools such as annotated corpora, lexicons, dependency parser, Part-of-Speech tagger, and benchmark datasets engorge the linguistic challenges of sarcasm detection in low-resource languages like Hindi. Furthermore, as context incongruity is imperative to detect sarcasm, various linguistic, aural and visual cues can be used to predict target utterance as sarcastic. While pre-Trained word embeddings capture the meanings, semantic relationships and different types of contexts in the form of word representations, emojis can also render useful contextual information, analogous to human facial expressions, for gauging sarcasm. Thus, the goal of this research is to demonstrate the use of a hybrid deep learning model trained using two embeddings, namely word and emoji embeddings to detect sarcasm. The model is validated on a Hindi tweets dataset, Sarc-H, manually annotated with sarcastic and non-sarcastic labels. The preliminary results clearly depict the importance of using emojis for sarcasm detection, with our model attaining an accuracy of 97.35% with an F-score of 0.9708. The research validates that automated feature engineering facilitates efficient and repeatable predictive model for detecting sarcasm in indigenous, low-resource languages.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSarcasm; deep learning; embeddings; emojis; indigenous,Computational linguistics; Deep learning; Embeddings; Natural language processing systems; Speech recognition; Additional key word and phrasessarcasm; Deep learning; Embeddings; Emojis; Indigenous; Key words; Language processing; Learning models; Low resource languages; Natural languages; Semantics
General and Domain-adaptive Chinese Spelling Check with Error-consistent Pretraining,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162094521&doi=10.1145%2f3564271&partnerID=40&md5=46854c4943ee47254258e427ecacb870,"The lack of label data is one of the significant bottlenecks for Chinese Spelling Check. Existing researches use the automatic generation method by exploiting unlabeled data to expand the supervised corpus. However, there is a big gap between the real input scenario and automatically generated corpus. Thus, we develop a competitive general speller ECSpell, which adopts the Error-consistent masking strategy to create data for pretraining. This error-consistency masking strategy is used to specify the error types of automatically generated sentences consistent with the real scene. The experimental result indicates that our model outperforms previous state-of-the-art models on the general benchmark.Moreover, spellers often work within a particular domain in real life. Due to many uncommon domain terms, experiments on our built domain-specific datasets show that general models perform terribly. Inspired by the common practice of input methods, we propose to add an alterable user dictionary to handle the zero-shot domain-adaption problem. Specifically, we attach a User Dictionary guided inference module (UD) to a general token classification-based speller. Our experiments demonstrate that ECSpellUD, namely, ECSpell combined with UD, surpasses all the other baselines broadly, even approaching the performance on the general benchmark.1  © 2023 Association for Computing Machinery.",Chinese spelling check; domain-adaptive; user dictionary,Benchmarking; Zero-shot learning; Automatic Generation; Automatically generated; Chinese spelling check; Domain-adaptive; Generation method; Pre-training; Research use; Spelling checks; Unlabeled data; User dictionary; Errors
Reusable Component Retrieval: A Semantic Search Approach for Low-Resource Languages,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162161045&doi=10.1145%2f3564604&partnerID=40&md5=1779c3149b5004bd6a7b849e14b2ec64,"A common practice among programmers is to reuse existing code, accomplished by performing natural language queries through search engines. The main aim of code retrieval is to search for the most relevant snippet from a corpus of code snippets. However, code retrieval frameworks for low-resource languages are insufficient. Retrieving the most relevant code snippet efficiently can be accomplished only by eliminating the semantic gap between the code snippets residing in the repository and the user's query (natural language description). The primary objective of the research is to contribute to this field by providing a code search framework that can be extended for low-resource languages. The secondary objective is to provide a code retrieval mechanism that is semantically relevant to the user query and provide programmers with the ability to locate source code that they want to use when developing new applications. The proposed approach is implemented using a web platform to search for source code. As code retrieval is a sophisticated task, the proposed approach incorporates a semantic search mechanism. This research uses a semantic model for code retrieval, which generates meanings or synonyms of words. The proposed model integrates ontologies and Natural Language Processing. System performance measures and classification accuracy are computed using precision, recall, and F1-score. We also compare the proposed approach with state-of-the-art baseline models. The retrieved results are ranked, showing that our approach significantly outperforms robust code matching. Our evaluation shows that semantic matching leads to improved source code retrieval. This study marks a substantial advancement in integrating programming expertise with code retrieval techniques. Moreover, our system lets users know when and how it is used for successful semantic searching.  © 2023 Association for Computing Machinery.",information retrieval; Ontologies; source code retrieval; source code search; web semantics,Codes (symbols); Computer programming languages; Information retrieval; Natural language processing systems; Ontology; Semantic Web; Semantics; Code retrievals; Low resource languages; Natural languages; Ontology's; Semantic search; Source code retrieval; Source code searches; Source codes; User query; Web semantics; Search engines
Stock Price Trends Prediction Based on the Classical Models with Key Information Fusion of Ontologies,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162163344&doi=10.1145%2f3592599&partnerID=40&md5=091b65decc4c5cde550a4c3c63ba59bc,"An ontology of the financial field can support effective association and integration of financial knowledge. Based on behavioral finance, social media is increasingly applied as one of the data sources for information fusion in stock forecasting to approximate the patterns of market changes. By predicting Tesla (TSLA) stock price trends, this study finds that satisfactory forecasting results can be achieved using classical models and incorporating key information features from the technical indicator ontology class and the investor behavior ontology class, even in the face of the impact of the COVID-19 epidemic. In the post-epidemic period, the back propagation neural network (BPNN) model is used to predict the price trend of TSLA for the next five trading days with an accuracy of up to 91.34%, an F1 score of 0.91, and a return of up to 268.42% obtained from simulated trading. This study extends the research on stock forecasting using fused information in the ontology of the financial field, providing a new basis for general investors in the selection of fusion information and the application of trading strategies and providing effective support for organizations to make intelligent financial decisions under uncertainty.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",feature fusion; Financial ontology; sentiment analysis; social media; trends prediction,Backpropagation; Commerce; Electronic trading; Financial markets; Forecasting; Information fusion; Investments; Neural networks; Ontology; Social networking (online); Classical modeling; Features fusions; Financial ontology; Ontology's; Price trends; Sentiment analysis; Social media; Stock forecasting; Stock price; Trend prediction; Sentiment analysis
Context-Aware Emotion Detection from Low-resource Urdu Language Using Deep Neural Network,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162123422&doi=10.1145%2f3528576&partnerID=40&md5=4cbc2d231e9c9637d7ccada37d22fe3f,"Emotion detection (ED) plays a vital role in determining individual interest in any field. Humans use gestures, facial expressions, and voice pitch and choose words to describe their emotions. Significant work has been done to detect emotions from the textual data in English, French, Chinese, and other high-resource languages. However, emotion classification has not been well studied in low-resource languages (i.e., Urdu) due to the lack of labeled corpora. This article presents a publicly available Urdu Nastalique Emotions Dataset (UNED) of sentences and paragraphs annotated with different emotions and proposes a deep learning (DL)-based technique for classifying emotions in the UNED corpus. Our annotated UNED corpus has six emotions for both paragraphs and sentences. We perform extensive experimentation to evaluate the quality of the corpus and further classify it using machine learning and DL approaches. Experimental results show that the developed DL-based model performs better than generic machine learning approaches with an F1 score of 85% on the UNED sentence-based corpus and 50% on the UNED paragraph-based corpus.  © 2023 Association for Computing Machinery.",annotated UNED corpus; Emotion detection (ED); Urdu Nastalique Emotions Dataset (UNED),Classification (of information); Learning systems; Annotated urdu nastalique emotion dataset corpus; Context-Aware; Emotion detection; Facial Expressions; Human use; Individual interests; Textual data; Urdu nastalique emotion dataset; Voice pitch; Deep neural networks
Editorial for the Special Issue on Computational Linguistics Processing in Low-Resource Indigenous Languages,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163215061&doi=10.1145%2f3591208&partnerID=40&md5=1ef4fee00f59786cfa5936c6a66c3adf,[No abstract available],,
Social Relationship Analysis Using State-of-the-art Embeddings,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162115365&doi=10.1145%2f3539608&partnerID=40&md5=37e47cbc6f7d177d644a0104c5262ec6,"Detection of human relationships from their interactions on social media is a challenging problem with a wide range of applications in different areas, like targeted marketing, cyber-crime, fraud, defense, planning, and human resource, to name a few. All previous work in this area has only dealt with the most basic types of relationships. The proposed approach goes beyond the previous work to efficiently handle the hierarchy of social relationships. This article introduces a novel technique named Quantifiable Social Relationship (QSR) analysis for quantifying social relationships to analyze relationships between agents from their textual conversations. QSR uses cross-disciplinary techniques from computational linguistics and cognitive psychology to identify relationships. QSR utilizes sentiment and behavioral styles displayed in the conversations for mapping them onto level II relationship categories. Then, for identifying the level III relationship categories, QSR uses level II relationships, sentiments, interactions, and word embeddings as key features. QSR employs natural language processing techniques for feature engineering and state-of-the-art embeddings generated by word2vec, global vectors (glove), and bidirectional encoder representations from transformers (bert). QSR combines the intrinsic conversational features with word embeddings for classifying relationships. QSR achieves an accuracy of up to 89% for classifying relationship subtypes. The evaluation shows that QSR can accurately identify the hierarchical relationships between agents by extracting intrinsic and extrinsic features from textual conversations between agents.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Agents interaction model; behavioral model; hierarchical relationship analysis; machine learning; quantifiable relationships; social relationship,Crime; Cybersecurity; Embeddings; Natural language processing systems; Social aspects; Agent interaction; Agent interaction model; Behavioral model; Embeddings; Hierarchical relationship analyse; Interaction modeling; Machine-learning; Quantifiable relationship; Relationship analysis; Social relationships; Machine learning
Better Localness for Non-Autoregressive Transformer,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162175390&doi=10.1145%2f3587266&partnerID=40&md5=d91c29a7e12da02c06eb69bc9640cdb3,"The Non-Autoregressive Transformer, due to its low inference latency, has attracted much attention from researchers. Although, the performance of the non-autoregressive transformer has been significantly improved in recent years, there is still a gap between the non-autoregressive transformer and the autoregressive transformer. Considering the success of localness on the autoregressive transformer, in this work, we consider incorporating localness into the non-autoregressive transformer. Specifically, we design a dynamic mask matrix according to the query tokens, key tokens, and relative distance, and unify the localness module for self-attention and cross-attention module. We conduct experiments on several benchmark tasks, and the results show that our model can significantly improve the performance of the non-autoregressive transformer.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention module; localness; Non-autoregressive; translation,Database systems; Attention module; Auto-regressive; Dynamic masks; Localness; matrix; Non-autoregressive; Performance; Relative distances; Translation; Benchmarking
QEST: Quantized and Efficient Scene Text Detector Using Deep Learning,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162068174&doi=10.1145%2f3526217&partnerID=40&md5=13ac7e6e1309f1352882ffb4ad4a46f4,"Scene text detection is complicated and one of the most challenging tasks due to different environmental restrictions, such as illuminations, lighting conditions, tiny and curved texts, and many more. Most of the works on scene text detection have overlooked the primary goal of increasing model accuracy and efficiency, resulting in heavy-weight models that require more processing resources. A novel lightweight model has been developed in this article to improve the accuracy and efficiency of scene text detection. The proposed model relies on ResNet50 and MobileNetV2 as backbones with quantization used to make the resulting model lightweight. During quantization, the precision has been changed from float32 to float16 and int8 for making the model lightweight. In terms of inference time and Floating-Point Operations Per Second, the proposed method outperforms the state-of-The-Art techniques by around 30-100 times. Here, well-known datasets, i.e., ICDAR2015 and ICDAR2019, have been utilized for training and testing to validate the performance of the proposed model. Finally, the findings and discussion indicate that the proposed model is more efficient than the existing schemes.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesDeep neural network; edge computing; floating point operations per second; inference time; model quantization; resource constrained,Deep learning; Digital arithmetic; Efficiency; Well testing; Additional key word and phrasesdeep neural network; Edge computing; Floating point operations per seconds; Inference time; Key words; Modeling quantizations; Neural-networks; Resource constrained; Scene Text; Text detection; Edge computing
"Social Network Analysis: A Survey on Measure, Structure, Language Information Analysis, Privacy, and Applications",2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163196164&doi=10.1145%2f3539732&partnerID=40&md5=0ef51741fbdd17f805ee4a8d3c590692,"The rapid growth in popularity of online social networks provides new opportunities in computer science, sociology, math, information studies, biology, business, and more. Social network analysis (SNA) is a paramount technique supporting understanding social relationships and networks. Accordingly, certain studies and reviews have been presented focusing on information dissemination, influence analysis, link prediction, and more. However, the ultimate aim is for social network background knowledge and analysis to solve real-world social network problems. SNA still has several research challenges in this context, including users' privacy in online social networks. Inspired by these facts, we have presented a survey on social network analysis techniques, visualization, structure, privacy, and applications. This detailed study has started with the basics of network representation, structure, and measures. Our primary focus is on SNA applications with state-of-The-Art techniques. We further provide a comparative analysis of recent developments on SNA problems in the sequel. The privacy preservation with SNA is also surveyed. In the end, research challenges and future directions are discussed to suggest to researchers a starting point for their research.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesInformation diffusion; community detection; influence maximization; link prediction; social network analysis,Information dissemination; Additional key word and phrasesinformation diffusion; Community detection; Influence maximizations; Information applications; Information privacy; Key words; Language informations; Link prediction; Research challenges; Social Network Analysis; Social networking (online)
Arabic Span Extraction-based Reading Comprehension Benchmark (ASER) and Neural Baseline Models,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162163449&doi=10.1145%2f3579047&partnerID=40&md5=7cd5195ce0489456426a125dcb84b486,"Machine reading comprehension (MRC) requires machines to read and answer questions about a given text. This can be achieved through either predicting answers or extracting them. Extracting answers from text involves predicting the first and last index of the answer span within the paragraph. Training machines to answer questions requires datasets that are created for such a purpose. The lack of availability of benchmarking datasets for the Arabic language has hindered research into machine reading comprehension from Arabic text. The aim of this article is to propose an Arabic Span-Extraction-based Reading Comprehension Benchmark (ASER) and complement it with neural baseline models for performance evaluations. Detailed steps are depicted for building and evaluating ASER, which is an Arabic dataset created manually for the task of machine reading comprehension. It contains 10,000 records from different domains and is divided into training and testing sets. The results of ASER evaluation led to the conclusion that it is a challenging benchmark since the answers have varying lengths and human performance resulted in an exact match of 42%. On the other hand, two main baseline models were the focus of ASER experimentation: the sequence-to-sequence (Seq2Seq) model with different neural networks and the bidirectional attention flow (BIDAF) model. These experiments were implemented using different embeddings, and the results showed an exact match with lower values than human performance.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Arabic benchmark; Arabic machine reading comprehension; Arabic Span Extraction based Reading Comprehension Benchmark (ASER); bidirectional attention flow models; Neural reading comprehension; sequence to sequence models; Text extraction,Benchmarking; Arabic benchmark; Arabic machine reading comprehension; Arabic span extraction based reading comprehension benchmark (arabic span extraction-based reading comprehension benchmark); Bidirectional attention flow model; Flow modelling; Neural reading comprehension; Reading comprehension; Sequence models; Sequence to sequence model; Text extraction; Extraction
Joint Intent Detection Model for Task-oriented Human-Computer Dialogue System using Asynchronous Training,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162162238&doi=10.1145%2f3558096&partnerID=40&md5=08571df80f88c58e081279b361edadd8,"How to accurately understand low-resource languages is the core of the task-oriented human-computer dialogue system. Language understanding consists of two sub-Tasks, i.e., intent detection and slot filling. Intent detection still faces challenges due to semantic ambiguity and implicit intentions with users' input. Moreover, separately modeling intent detection and slot filling significantly decrease the correctness and relevance between questions and answers. To address these issues, we propose a joint intent detection method using asynchronous training strategy. The proposed method firstly encodes local text information extracted by CNN and relationship information among words emphasized by attention structure. Later, a joint intent detection model with asynchronous training strategy is proposed by either fusing hidden states of intent detection and slot filling layers, or adopting the key information to fine-Tune the whole network, greatly increasing the relevance of intent detection and slot filling subtasks. The accuracy achieved by the proposed method tested on an open-source airline travel dataset and a self-collected electricity service dataset, i.e., ATIS and ECSF, are 97.49% and 89.68%, respectively, which proves the effectiveness of joint learning and asynchronous training.  © 2023 Association for Computing Machinery.",asynchronous training; Intent detection; joint modeling; task-oriented human-computer dialogue system,Semantics; Speech processing; Asynchronoi training; Detection models; Dialogue systems; Human-computer dialogues; Intent detection; Joint models; Subtask; Task-oriented; Task-oriented human-computer dialog system; Training strategy; Filling
User-based Hierarchical Network of Sina Weibo Emotion Analysis,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162134029&doi=10.1145%2f3579048&partnerID=40&md5=2d0233f11a4d3d6b82c19497b6a291a3,"Emotion analysis on Sina Weibo has a great impetus for government agencies to survey public opinion and enterprises to track market demand. Most of the existing emotion analysis work on Sina Weibo focuses on mining the information contained in a single Weibo, ignoring the problem of inaccurate information extraction caused by the lack of contextual information in Weibo texts. Inspired by humans judging user emotional states from Weibo texts, this article creates a Weibo text five-category emotion classification dataset based on active users and proposes a user-based hierarchical network for Weibo emotion analysis. First, use the multi-head attention mechanism and convolutional neural network set in the information extraction module to analyze a single Weibo text to fully extract the emotional information contained in the text; at the same time, through the moving window set in the relevant information capture module, obtain other Weibo texts posted by the same user within a period, and capture the effective correlation information between Weibo texts; then, the dual text representation obtained above is concatenated, and through the information interaction layer, the relevant information is retrieved again, and the text representation is updated; finally, the classifier output the five-category emotion labels corresponding to each Weibo text. We demonstrate the model's effectiveness through experiments and analysis in the results.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSocial media; datasets; emotion analysis; neural networks,Convolutional neural networks; Information retrieval; Multilayer neural networks; Social aspects; Social networking (online); Text processing; Additional key word and phrasessocial medium; Dataset; Emotion analysis; Government agencies; Hierarchical network; Key words; Neural-networks; Public enterprise; Sina-weibo; Text representation; Classification (of information)
Scene Graph Semantic Inference for Image and Text Matching,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162093499&doi=10.1145%2f3563390&partnerID=40&md5=0ce170a6d8aa9ef9f86c367f47393986,"With the rapid development of information technology, image and text data have increased dramatically. Image and text matching techniques enable computers to understand information from both visual and text modalities and match them based on semantic content. Existing methods focus on visual and textual object co-occurrence statistics and learning coarse-level associations. However, the lack of intramodal semantic inference leads to the failure of fine-level association between modalities. Scene graphs can capture the interactions between visual and textual objects and model intramodal semantic associations, which are crucial for the understanding of scenes contained in images and text. In this article, we propose a novel scene graph semantic inference network (SGSIN) for image and text matching that effectively learns fine-level semantic information in vision and text to facilitate bridging cross-modal discrepancies. Specifically, we design two matching modules and construct scene graphs within each matching module for aggregating neighborhood information to refine the semantic representation of each object and achieve fine-level alignment of visual and textual modalities. We perform extended experiments in Flickr30K and MSCOCO and achieve state-of-the-art results, which validate the advantages of our proposed approach.  © 2023 Association for Computing Machinery.",Image and text matching; scene graph; semantic inference,Semantics; Graph semantics; Image and text matching; Image data; Matching techniques; Matchings; Scene-graphs; Semantic inference; Technology data; Text data; Text-matching; Association reactions
An Intelligent Approach Based on Cleaning up of Inutile Contents for Extremism Detection and Classification in Social Networks,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162088682&doi=10.1145%2f3575802&partnerID=40&md5=51b6a5611359dc2c8706b94a4999114b,"Extremism is a growing threat worldwide that presents a significant danger to public safety and national security. Social networks provide extremists with spaces to spread their ideas through commentaries or tweets, often in Asian English. In this paper, we propose an intelligent approach that cleans the text's content, analyzes its sentiment, and extracts its features after converting it to digital data for machine learning treatments. We apply 16 intelligent machine learning classifiers for extremism detection and classification. The proposed artificial intelligence methods for Asian English language data are used to extract the essential features from the text. Our evaluation of the proposed model with an extremism dataset proves its effectiveness compared to the standard classification models based on various performance metrics. The proposed model achieves 93,6% accuracy for extremism detection and 97,0% for extremism classification.  © 2023 Association for Computing Machinery.",extremism; Machine learning; Natural Language Processing; sentiment analysis; social networks,Classification (of information); Machine learning; National security; Social networking (online); Content analysis; Extremism; Language processing; Machine-learning; Natural language processing; Natural languages; Public safety; Sentiment analysis; Social network; Text content; Sentiment analysis
A Confusion Method for the Protection of User Topic Privacy in Chinese Keyword-based Book Retrieval,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158006726&doi=10.1145%2f3571731&partnerID=40&md5=78c5073f2a3366f2e0e4687ac74a0e06,"In this article, aiming at a Chinese keyword-based book search service, from a technological perspective, we propose to modify a user query sequence carefully to confuse the user query topics and thus protect the user topic privacy on the untrusted server, without compromising the accuracy of each book search service. First, we propose a client-based framework for the privacy protection of book search, and then a privacy model to formulate the constraints in terms of accuracy, efficiency, and security, which the cover queries generated based on a user query sequence should meet. Second, we present a modification algorithm for a user query sequence, based on some heuristic strategies, which can quickly generate a cover query sequence meeting the privacy model by replacing, deleting, and adding keywords for each user query. Finally, both theoretical analysis and experimental evaluation demonstrate the effectiveness of the proposed approach, i.e., which can improve the security of users' topic privacy on the untrusted server without compromising the efficiency, accuracy, and usability of an existing Chinese keyword book search service, so it has a positive impact for the construction of a privacy-preserving text retrieval platform under an untrusted network environment.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",book search; Chinese; privacy model; privacy protection; topic privacy,Privacy-preserving techniques; Book search; Chinese; Chinese keywords; Keyword-based; Privacy models; Privacy protection; Query sequence; Search services; Topic privacy; User query; Efficiency
Supervised Machine Learning Method for Ontology-based Financial Decisions in the Stock Market,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162127497&doi=10.1145%2f3554733&partnerID=40&md5=a778cf295077174b57f6c04e476040a9,"For changing semantics, ontological and information presentation, as well as computational linguistics for Asian social networks, are one of the most essential platforms for offering enhanced and real-time data mapping, as well as huge data access across diverse big data sources on the web architecture, information extraction mining, statistical modeling and data modeling, database control, and so on. The concept of opinion or sentiment analysis is often used to predict or classify the textual data, sentiment, affect, subjectivity, and other emotional states in online text. Recognizing the message's positive and negative thoughts or opinions by examining the author's goals will aid in a better understanding of the text's content in terms of the stock market. An intelligent ontology and knowledge Asian social network solution can improve the effectiveness of a company's decision making support procedures by deriving important information about users from a wide variety of web sources. However, ontology is concerned primarily with problem-solving knowledge discovery. The utilization of Internet-based modernizations welcomed a significant effect on the Indian stock exchange. News related to the stock market in the most recent decade plays a vital role for the brokers or users. This article focuses on predicting stock market news sentiments based on their polarity and textual information using the concept of ontological knowledge-based Convolution Neural Network (CNN) as a machine learning approach. Optimal features are essential for the sentiment classification model to predict the stock's textual reviews' exact sentiment. Therefore, the swarm-based Artificial Bee Colony (ABC) algorithm is utilized with the Lexicon feature extraction approach using a novel fitness function. The main motivation for combining ABC and CNN is to accelerate model training, which is why the suggested approach is effective in predicting emotions from stock news.  © 2023 Association for Computing Machinery.",Artificial Bee Colony Algorithm (ABC); Convolution Neural Networks (CNN); Lexicon feature extraction; opinion mining; sentiment analysis; Stock market,Character recognition; Classification (of information); Commerce; Convolution; Data mining; Decision making; Electronic trading; Feature extraction; Financial markets; Forecasting; Knowledge based systems; Learning systems; Ontology; Optimization; Semantics; Social networking (online); Supervised learning; Artificial bee colony algorithm; Artificial bees; Bee colony algorithms; Convolution neural network; Features extraction; Lexicon feature extraction; Opinion mining; Sentiment analysis; Supervised machine learning; Sentiment analysis
OdeBERT: One-stage Deep-supervised Early-exiting BERT for Fast Inference in User Intent Classification,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162138772&doi=10.1145%2f3587464&partnerID=40&md5=91a769c86211b1b31ccd47cf1cb31dcf,"User intent classification is a vital task for analyzing users' essential requirements from the users' input query in information retrieval systems, question answering systems, and dialogue systems. Pre-Trained language model Bidirectional Encoder Representation from Transformers (BERT) has been widely applied to the user intent classification task. However, BERT is compute intensive and time-consuming during inference and usually causes latency in real-Time applications. To improve the inference efficiency of BERT for the user intent classification task, this article proposes a new network named one-stage deep-supervised early-exiting BERT as one-stage deep-supervised early-exiting BERT (OdeBERT). In addition, a deep supervision strategy is developed to incorporate the network with internal classifiers by one-stage joint training to improve the learning process of classifiers by extracting discriminative category features. Experiments are conducted on publicly available datasets, including ECDT, SNIPS, and FDQuestion. The results show that the OdeBERT can speed up original BERT 12 times faster at most with the same performance, outperforming state-of-The-Art baseline methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",BERT; deep supervision; inference; OdeBERT; user intent classification,Information retrieval systems; Learning systems; Search engines; Speech processing; Bidirectional encoder representation from transformer; Classification tasks; Deep supervision; Fast inference; Inference; Information-retrieval systems; One-stage deep-supervised early-exiting BERT; Question answering systems; User input; User intent classification; Classification (of information)
Editorial: Ontology-based Knowledge Presentation and Computational Linguistics for Semantic Big Social Data Analytics in Asian Social Networks,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162181474&doi=10.1145%2f3594719&partnerID=40&md5=d4cfc689af9f40dd960260f2ff644c19,"Data-driven ontology-based knowledge (OK) presentation and computational linguistics for evolving semantic Asian social networks (ASNs) can make one of the most important platforms that provide robust and real-time data mapping in massive access across the heterogeneous big data sources in the web that is named OK-ASN. It benefits from computational intelligence, web-of-things (WoT) architecture, semantic features, statistical learning and pattern recognition, database management, computer vision, cyber-security, and language processing. OK-ASN is a critical strategy for WoT big data mining and enterprises from social media to medical and industrial sectors.  © 2023 Copyright held by the owner/author(s).",Asian social networks; computational linguistics; knowledge graph; ontology; Social data,Big data; Computational linguistics; Data Analytics; Data mining; Knowledge graph; Pattern recognition; Semantic Web; Semantics; Asian social network; Data analytics; Data driven; Data mappings; Knowledge graphs; Knowledge presentation; Ontology's; Ontology-based; Real-time data; Social datum; Ontology
WH2D2N2: Distributed AI-enabled OK-ASN Service for Web of Things,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162138524&doi=10.1145%2f3564242&partnerID=40&md5=294df1f7282671a3cf3492cc17bcf244,"Model data-driven ontology and knowledge presentation for evolving semantic Asian social networks (OK-ASN) is a critical strategy for web of things (WoT) services. Meanwhile, Deep Neural Network (DNN)-based OK-ASN service in WoT is growing rapidly. However, most DNN-based services cannot utilize the potential of WoT fully, as heterogeneity exists in WoT. Therefore, this article proposes a novel framework called Web-based Heterogeneous Hierarchical Distributed Deep Neural Network (WH2D2N2) to deploy the DNNs for OK-ASN services on WoT, overcoming the heterogeneity. The architecture of the system and the designed Edge-Cloud-Joint execute scheme utilize heterogeneous devices to make DNN inference ubiquitous and output two types of results to meet various requirements. To bring robustness to OK-ASN services, a global scheduling is designed to arrange the workflow dynamically. The results of our experiments prove the efficiency of the execute scheme and the global scheduling in the system.  © 2023 Association for Computing Machinery.",Distributed inference; heterogeneity; ubiquitousness; web ecology,Deep neural networks; Semantics; Web services; Data driven; Distributed AI; Distributed inference; Global scheduling; Heterogeneity; Knowledge presentation; Modeling data; Ontology's; Ubiquitousness; Web ecology; Internet of things
TAM GAN: Tamil Text to Naturalistic Image Synthesis Using Conventional Deep Adversarial Networks,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162101271&doi=10.1145%2f3584019&partnerID=40&md5=0840de0e1ccf0df6598b2aad57ec4427,"Text-to-image synthesis has advanced recently as a prospective area for improvement in computer vision applications. The image synthesis model follows significant neural network architectures such as Generative Adversarial Networks (GANs). The flourishing text-to-image generation approaches can nominally reflect the meaning of the text in generated images. Still, they need the prospect of providing the necessary details and eloquent object features. Intelligent systems are trained in text-to-image synthesis applications for various languages. However, their contribution to regional languages is yet to be explored. Autoencoders prompt the synthesis of images, but they result in blurriness, which results in clear output and essential features of the picture. Based on textual descriptions, The GAN model is capable of producing realistic images of a high quality that can be used in various applications, like fashion design, photo editing, computer-aided design, and educational platforms. The proposed method uses two-stage processing to create a language model using a BERT model called TAM-BERT and an existing MuRIL BERT, followed by image synthesis using a GAN. The work was conducted using the Oxford-102 dataset, and the model's efficiency was evaluated using the F1-Score measure.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",BERT; Computer vision; feature matching; Generative Adversarial Network (GAN); L1Norm; language model; latent vectors; MuRIL BERT,Computational linguistics; Computer aided design; Computer vision; Image enhancement; Intelligent systems; Network architecture; Adversarial networks; BERT; Features matching; Generative adversarial network; Images synthesis; L1 norm; Language model; Latent vectors; MuRIL BERT; Prospectives; Generative adversarial networks
Filtering and Extended Vocabulary based Translation for Low-resource Language Pair of Sanskrit-Hindi,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160818971&doi=10.1145%2f3580495&partnerID=40&md5=2bdb588a1cd8efeff6e0b85d78231c4b,"Neural Machine Translation (NMT) is widely employed for language translation tasks because it performs better than the conventional statistical and phrase-based approaches. However, NMT techniques involve challenges, such as requiring a large and clean corpus of parallel data and the inability to deal with rare words. They need to be faster for real-time applications. More work needs to be done using NMT to address the challenges in translating Sanskrit, one of the oldest and rich languages known to the world, with its morphological richness and limited multilingual parallel corpus. There is usually no similar data between a language pair; hence, no application exists so far that can translate Sanskrit to/from other languages. This study presents an in-depth analysis to address these challenges with the help of a low-resource Sanskrit-Hindi language pair. We employ a novel training corpus filtering with extended vocabulary in a zero-shot transformer architecture. The structure of the Sanskrit language is thoroughly investigated to justify the use of each step. Furthermore, the proposed method is analyzed based on variations in sentence length and also applied to a high-resource language pair in order to demonstrate its efficacy. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",extended vocabulary; filtering; Hindi; low resource; neural machine translation; Sanskrit; transformers; Zero-shot,Computational linguistics; Computer aided language translation; Zero-shot learning; Extended vocabulary; Hindi; Language pairs; Language translation; Low resource; Low resource languages; Phrase-based approach; Sanskrit; Transformer; Zero-shot; Neural machine translation
Urdu Short Paraphrase Detection at Sentence Level,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161477984&doi=10.1145%2f3586009&partnerID=40&md5=3a5bccb394a7954ab2aca41364cacfe7,"Paraphrase detection systems uncover the relationship between two text fragments and classify them as paraphrased when they convey the same idea; otherwise non-paraphrased. Previously, the researchers have mainly focused on developing resources for the English language for paraphrase detection. There have been very few efforts for paraphrase detection in South Asian languages. However, no research has been conducted on sentence-level paraphrase detection in Urdu, a low-resourced language. It is mainly due to the unavailability of the corpora that focus on the sentence level. The available related studies on the Urdu language only focus on text reuse detection tasks at the passage and document levels. Therefore, this study aims to develop a large-scale manually annotated benchmark Urdu paraphrase detection corpus at the sentence level, based on real cases from journalism. The proposed Urdu Sentential Paraphrases (USP) corpus contains 4,900 sentences (2,941 paraphrased and 1,959 non-paraphrased), manually collected from the Urdu newspapers. Moreover, several techniques were proposed, developed, and compared as a secondary contribution, including Word Embedding (WE), Sentence Transformers (ST), and feature-fusion techniques. N-gram is treated as the baseline technique for our research. The experimental results indicate that our proposed feature-fusion technique is the most suitable for the Urdu paraphrase detection task. Furthermore, the performance increases when features of the proposed (ST) and baseline (N-gram) are combined for the classification task. In addition, The proposed techniques have also been applied to the UPPC corpus to check their performance at the document level. The best result we obtained using the feature fusion technique (F1 = 0.855). Our corpus is available and free to download for research purposes.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Urdu corpus generation; Urdu language; Urdu paraphrase detection; Urdu paraphrases,Detection tasks; Features fusions; Fusion techniques; N-grams; Performance; Sentence level; Urdu corpus generation; Urdu language; Urdu paraphrase; Urdu paraphrase detection; Computational linguistics
Learning Category Distribution for Text Classification,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161585734&doi=10.1145%2f3585279&partnerID=40&md5=0bba4ae4fe0eff9378f9fd8c1ee48b7d,"Label smoothing has a wide range of applications in the machine learning field. Nonetheless, label smoothing only softens the targets by adding a uniform distribution into a one-hot vector, which cannot truthfully reflect the underlying relations among categories. However, learning category relations is of vital importance in many fields such as emotion taxonomy and open set recognition. In this work, we propose a method to obtain the label distribution for each category (category distribution) to reveal category relations. Furthermore, based on the learned category distribution, we calculate new soft targets to improve the performance of model classification. Compared with existing methods, our algorithm can improve neural network models without any side information or additional neural network module by considering category relations. Extensive experiments have been conducted on four original datasets and 10 constructed noisy datasets with three basic neural network models to validate our algorithm. The results demonstrate the effectiveness of our algorithm on the classification task. In addition, three experiments (arrangement, clustering, and similarity) are also conducted to validate the intrinsic quality of the learned category distribution. The results indicate that the learned category distribution can well express underlying relations among categories.  © 2023 Copyright held by the owner/author(s).",Category distribution; neural networks; text classification,Classification (of information); Emotion Recognition; Learning systems; Category distribution; Label distribution; Learning fields; Machine-learning; Neural network model; Neural-networks; Performance; Soft targets; Text classification; Uniform distribution; Neural network models
Introduction to the Special Issue of Recent Advances in Computational Linguistics for Asian Languages,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160424993&doi=10.1145%2f3588316&partnerID=40&md5=68e00189164e00393fa6b6dcf68d7a6f,[No abstract available],,
Research and Implementation of Automatic Indexing Method of PDF for Digital Publishing,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160314281&doi=10.1145%2f3501400&partnerID=40&md5=1c2d39aa4a4dd6ef165215f21cff0bc9,"With the rapid development of mobile Internet technology and artificial intelligence technology, the digital publishing industry is in urgent need of using intelligent technology to change the current way of content production and service. Most of the e-book resources owned by publishing enterprises are in PDF format, which is not suitable for reading on mobile devices, and it is not convenient to directly extract key information and construct knowledge graph. With this in mind, this article designs a PDF automatic indexing scheme that can identify all the element information in PDF and output structured data automatically and then extract all the key information in it to generate a keyword library with tag weights. The scheme mainly involves two key technical points: parsing PDF based on text features and grammar rules and extracting keywords based on tag weights. The former visualizes the text block in PDF into a rectangular area, divides the elements by clustering algorithm, and, finally, outputs structured data containing all the information. The latter combines the tags and their weights in the structured data and extracts the keywords in it by the inter-word relation algorithm. The structured data and keywords database produced by this scheme can be used to produce intelligent e-book and build knowledge graph, thus helping publishing enterprises to transform from a content service provider to an intelligent knowledge service provider. This transformation can deeply excavate the core value of the content held by the publishing industry and promote the digitization and intelligentization process of the whole industry.  © 2023 Association for Computing Machinery.",automatic indexing; keywords extraction; Natural language processing; tag weight; text aggregation,Automatic indexing; Clustering algorithms; Data mining; Graphic methods; Knowledge graph; Content services; Digital publishing; Keywords extraction; Language processing; Natural language processing; Natural languages; Publishing industry; Structured data; Tag weight; Text aggregation; Natural language processing systems
Survey of Authorship Identification Tasks on Arabic Texts,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161312192&doi=10.1145%2f3564156&partnerID=40&md5=ebcf02142383b5ea741c0fbe139af6f8,"Authorship identification is the process of extracting and analysing the writing styles of authors to identify the authorship. From the writing style, the author and his/her different characteristics can be recognised, which is very useful in digital forensics and cyber investigations. In the literature, authorship identification tasks were addressed on both long and short documents and performed on different languages, such as English, Arabic, Chinese, and Greek. This survey has reviewed the authorship identification tasks for the Arabic language to contribute to this area of research by exploring Arabic language performance and challenges. A total of 27 prominent Arabic studies of each authorship identification domain were reviewed considering the used data, selected features, utilised methods, and results. After a review of the various studies, it was concluded that the results of authorship identification tasks vary based on mostly the selected features and used dataset. Furthermore, the effective features differ from one dataset to another based on the various types of the Arabic language. However, all authorship identification tasks involving the Arabic language face considerable challenges with data pre-processing due to the challenging Arabic concatenative morphology.  © 2023 Association for Computing Machinery.",Arabic texts; authorship attribution; Authorship identification; authorship verification; stylometry,Data handling; Arabic languages; Arabic texts; Authorship attribution; Authorship identification; Authorship verification; Data preprocessing; Performance; Stylometry; Writing style; Digital forensics
Hybrid Pipeline for Building Arabic Tunisian Dialect-standard Arabic Neural Machine Translation Model from Scratch,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160327728&doi=10.1145%2f3568674&partnerID=40&md5=d4c5ab66a7f6c5694ea2a15570d3cf57,"Deep Learning is one of the most promising technologies compared to other methods in the context of machine translation. It has been proven to achieve impressive results on large amounts of parallel data for well-endowed languages. Nevertheless, for low-resource languages such as the Arabic Dialects, Deep Learning models failed due to the lack of available parallel corpora. In this article, we present a method to create a parallel corpus to build an effective NMT model able to translate into MSA, Tunisian Dialect texts present in social networks. For this, we propose a set of data augmentation methods aiming to increase the size of the state-of-the-art parallel corpus. By evaluating the impact of this step, we noticed that it has effectively boosted both the size and the quality of the corpus. Then, using the resulted corpus, we compare the effectiveness of CNN, RNN and transformers models to translate Tunisian Dialect into MSA. Experiments show that a better translation is achieved by the transformer model with a BLEU score of 60 vs., respectively, 33.36 and 53.98 with RNN and CNN models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Arabic Tunisian Dialect; data augmentation; Modern Standard Arabic; Neural Machine Translation,Computational linguistics; Computer aided language translation; Deep learning; Learning systems; Arabic tunisian dialect; CNN models; Data augmentation; Machine translation models; Machine translations; Modern standard arabic; Modern standards; Parallel corpora; Standard arabics; Transformer modeling; Neural machine translation
Domain-Invariant Feature Progressive Distillation with Adversarial Adaptive Augmentation for Low-Resource Cross-Domain NER,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160395316&doi=10.1145%2f3570502&partnerID=40&md5=e76d8dafc217b75140238036ca4e491d,"Considering the expensive annotation in Named Entity Recognition (NER), Cross-domain NER enables NER in low-resource target domains with few or without labeled data, by transferring the knowledge of high-resource domains. However, the discrepancy between different domains causes the domain shift problem and hampers the performance of cross-domain NER in low-resource scenarios. In this article, we first propose an adversarial adaptive augmentation, where we integrate the adversarial strategy into a multi-task learner to augment and qualify domain adaptive data. We extract domain-invariant features of the adaptive data to bridge the cross-domain gap and alleviate the label-sparsity problem simultaneously. Therefore, another important component in this article is the progressive domain-invariant feature distillation framework. A multi-grained MMD (Maximum Mean Discrepancy) approach in the framework to extract the multi-level domain invariant features and enable knowledge transfer across domains through the adversarial adaptive data. Advanced Knowledge Distillation (KD) schema processes progressively domain adaptation through the powerful pre-trained language models and multi-level domain invariant features. Extensive comparative experiments over four English and two Chinese benchmarks show the importance of adversarial augmentation and effective adaptation from high-resource domains to low-resource target domains. Comparison with two vanilla and four latest baselines indicates the state-of-the-art performance and superiority confronted with both zero-resource and minimal-resource scenarios. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNER; adversarial augmentation; cross-domain; domain adaptation; knowledge distillation; low-resource,Data mining; High level languages; Knowledge management; Additional key word and phrasesner; Adversarial augmentation; Cross-domain; Domain adaptation; Invariant features; Key words; Knowledge distillation; Low-resource; Named entity recognition; Target domain; Distillation
State of the Art of Automation in Sign Language: A Systematic Review,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161334566&doi=10.1145%2f3564769&partnerID=40&md5=ef99e5b20cc90b9c1c09dbe7fd8f3d43,"Sign language is the fundamental communication language of deaf people. Efforts to develop sign language generation systems can make the life of these people smooth and effortless. Despite the importance of sign language generation systems, there is a paucity of a systematic literature review. This is the foremost recognizable scholastic literature review of sign language generation systems. It presents a scholastic database of the literature between 1998 and 2020 and suggests classification criteria to systematize research studies. Four hundred fourteen research studies were recognized and reviewed for their direct pertinence to sign language generation systems. One hundred sixty-two research studies were subsequently chosen, examined, and classified. Each of the 162 chosen research papers was categorized based on 30 sign languages and was further comparatively analyzed based on seven comparison parameters (input form, translation technologies, application domain, use of parsers/grammars, manual/non-manual features, accuracy, and output form). It is evident from our research findings that the majority of research on sign language generation was carried out using data-driven approaches in the absence of proper grammar rules and generated only manual signs. This research study may provide researchers a roadmap toward future research directions and facilitate the compilation of information in the field of sign language generation.  © 2023 Association for Computing Machinery.",HamNoSys; Interlingua; Machine translation; SiGML; virtual avatar,Computational linguistics; Translation (languages); Generation systems; Hamnosys; Interlingua; Language generation; Machine translations; Research studies; SiGML; Sign language; State of the art; Virtual avatar; Classification (of information)
A Weak-Region Enhanced Bayesian Classification for Spam Content-Based Filtering,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146153801&doi=10.1145%2f3510420&partnerID=40&md5=2e092e90d1a18f9778f6daa51a383fa8,"This article proposes an improved Bayesian scheme by focusing on the region in which Bayesian may fail to correctly identify labels and improve classification performance by handling those errors. Bayesian method, as a probabilistic classifier, uses Bayes' theorem to calculate the probability of an instance belonging to a class, where the class label with a maximum probability is assigned to the instance. In a spam detection problem, it can be considered that the prediction of the Bayesian classifier is weak when the probability obtained for classes spam and non-spam are close to each other. Therefore, we define a threshold to determine weak prediction against strong prediction. A hybrid strategy using a two-layer Bayesian approach is presented: basic Bayesian (BBayes) and corrected weak region Bayesian (CWRBayes), which are concerned with strong and weak predictions, respectively. Both techniques, BBayes and CWRBayes, have the same classification mechanism, but they use different feature selection mechanisms. The proposed methods are implemented and evaluated over two datasets of spam e-mails, and the results show that the proposed method has better performance than the baseline of the naïve Bayesian and some other Bayesian variants.  © 2023 Association for Computing Machinery.",Bayesian; feature selection; spam detection; text classification,Bayesian networks; Classification (of information); Feature Selection; Text processing; Baye's theorem; Bayesian; Bayesian classification; Bayesian methods; Classification performance; Content based filtering; Features selection; Probabilistic classifiers; Spam detection; Text classification; Forecasting
KenSwQuAD - A Question Answering Dataset for Swahili Low-resource Language,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161609702&doi=10.1145%2f3578553&partnerID=40&md5=3b91ad99ad5a846eb5c428eb665daa3b,"The need for question-answering (QA) datasets in low-resource languages is the motivation of this research, leading to the development of the Kencorpus Swahili Question Answering Dataset (KenSwQuAD). This dataset is annotated from raw story texts of Swahili, a low-resource language that is predominantly spoken in eastern Africa and in other parts of the world. Question-answering datasets are important for machine comprehension of natural language for tasks such as internet search and dialog systems. Machine learning systems need training data such as the gold-standard question-answering set developed in this research. The research engaged annotators to formulate QA pairs from Swahili texts collected by the Kencorpus project, a Kenyan languages corpus. The project annotated 1,445 texts from the total 2,585 texts with at least 5 QA pairs each, resulting in a final dataset of 7,526 QA pairs. A quality assurance set of 12.5% of the annotated texts confirmed that the QA pairs were all correctly annotated. A proof of concept on applying the set to the QA task confirmed that the dataset can be usable for such tasks. KenSwQuAD has also contributed to resourcing of the Swahili language.  © 2023 Copyright held by the owner/author(s).",low-resource languages; question answer; Swahili,Learning systems; Natural language processing systems; Search engines; Dialogue systems; Eastern Africa; Internet search systems; Low resource languages; Machine learning systems; Natural languages; Question answer; Question Answering; Swahilus; Training data; Quality assurance
Research on Chinese Audio and Text Alignment Algorithm Based on AIC-FCM and Doc2Vec,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160327828&doi=10.1145%2f3532852&partnerID=40&md5=35041e6436c9c4e5bb88f4e7dfef4d25,"""Audiobook""is a multimedia-based reading technology that has emerged in recent years. Realizing the alignment of e-book text and book audio is the most important part of its processing. This article describes an audio and text alignment algorithm using deep learning and neural network technology to improve the efficiency and quality of audiobook production. The algorithm first uses dual-threshold endpoint detection technology to segment long audio into short audio with sentence dimensions and recognizes it as short text. The threshold is calculated by AIC-FCM optimized based on simulated annealing genetic algorithm. Then the algorithm uses Doc2vec optimized by the threshold prediction method based on the average length of the short text to calculate the text similarity. Finally, proofread and output the text sequence and audio segment aligned in the time dimension to meet the needs of audiobook production. Experiments show that compared to traditional audio and text alignment algorithms, the proposed algorithm is closer to the ideal segmentation result in long audio segmentation, and the alignment effect is basically the same as Doc2vec and the time complexity is reduced by about 35%.  © 2023 Association for Computing Machinery.",akaike information criterion; Audio and text alignment; Doc2vec; dual threshold endpoint detection; fuzzy C-means clustering algorithm,Character recognition; Clustering algorithms; Deep learning; Fuzzy clustering; Genetic algorithms; Akaike's information criterions; Alignment algorithms; Audio alignments; C mean clustering algorithms; Doc2vec; Dual threshold endpoint detection; End point detection; Fuzzy C-mean clustering algorithm; Fuzzy C-Means clustering; Text alignments; Simulated annealing
Malayalam Natural Language Processing: Challenges in Building a Phrase-Based Statistical Machine Translation System,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152967207&doi=10.1145%2f3579163&partnerID=40&md5=5b304229ae8d5960e9a33dc42bc4bdd2,"Statistical Machine Translation (SMT) is a preferred Machine Translation approach to convert the text in a specific language into another by automatically learning translations using a parallel corpus. SMT has been successful in producing quality translations in many foreign languages, but there are only a few works attempted in South Indian languages. The article discusses on experiments conducted with SMT for Malayalam language and analyzes how the methods defined for SMT in foreign languages affect a Dravidian language, Malayalam. The baseline SMT model does not work for Malayalam due to its unique characteristics like agglutinative nature and morphological richness. Hence, the challenge is to identify where precisely the SMT model has to be modified such that it adapts the challenges of the language peculiarity into the baseline model and give better translations for English to Malayalam translation. The alignments between English and Malayalam sentence pairs, subjected to the training process in SMT, plays a crucial role in producing quality output translation. Therefore, this work focuses on improving the translation model of SMT by refining the alignments between English-Malayalam sentence pairs. The phrase alignment algorithms align the verb and noun phrases in the sentence pairs and develop a new set of alignments for the English-Malayalam sentence pairs. These alignment sets refine the alignments formed from Giza++ produced as a result of EM training algorithm. The improved Phrase-Based SMT model trained using these refined alignments resulted in better translation quality, as indicated by the AER and BLUE scores.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",alignments; Dravidian language; Machine Translation; Malayalam; Natural Language Processing; Statistical Machine Translation,Computational linguistics; Computer aided language translation; Natural language processing systems; Speech transmission; Dravidian language; Foreign language; Language processing; Machine translation models; Machine translations; Malayalams; Natural language processing; Natural languages; Phrase-based statistical machine translation; Statistical machine translation; Machine translation
Building Dialogue Understanding Models for Low-resource Language Indonesian from Scratch,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161484858&doi=10.1145%2f3575803&partnerID=40&md5=5571f3603585bcbfe6098d390eaf5adf,"Using off-the-shelf resources from resource-rich languages to transfer knowledge to low-resource languages has received a lot of attention. The requirements of enabling the model to achieve the reliable performance, including the scale of required annotated data and the effective framework, are not well guided. To address the first question, we empirically investigate the cost-effectiveness of several methods for training intent classification and slot-filling models from scratch in Indonesia (ID) using English data. Confronting the second challenge, we propose a Bi-Confidence-Frequency Cross-Lingual transfer framework (BiCF), which consists of ""BiCF Mixing"", ""Latent Space Refinement""and ""Joint Decoder"", respectively, to overcome the lack of low-resource language dialogue data. BiCF Mixing based on the word-level alignment strategy generates code-mixed data by utilizing the importance-frequency and translating-confidence. Moreover, Latent Space Refinement trains a new dialogue understanding model using code-mixed data and word embedding models. Joint Decoder based on Bidirectional LSTM (BiLSTM) and Conditional Random Field (CRF) is used to obtain experimental results of intent classification and slot-filling. We also release a large-scale fine-labeled Indonesia dialogue dataset (ID-WOZ1) and ID-BERT for experiments. BiCF achieves 93.56% and 85.17% (F1 score) on intent classification and slot filling, respectively. Extensive experiments demonstrate that our framework performs reliably and cost-efficiently on different scales of manually annotated Indonesian data.  © 2023 Association for Computing Machinery.",Dialogue datasets; indonesian; intent classification; slot-filling,Cost effectiveness; Decoding; Filling; Large dataset; Mixing; Random processes; Dialog dataset; Filling model; Indonesia; Indonesian; Intent classification; Low resource languages; Mixed data; Reliable performance; Resource-Rich; Slot-filling; Classification (of information)
Context-aware Urdu Information Retrieval System,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160359438&doi=10.1145%2f3502854&partnerID=40&md5=1cbd24e8a79baa12a6a64b327f3a8ed9,"World Wide Web (WWW) is playing a vital role for sharing dynamic knowledge in every field of life. The information on web comprises a huge amount of data in different forms such as structured, semi structured, or few is totally in unstructured format. Due to huge size of information, searching from larger textual data about the specific topic or getting precise information is a challenging task. All this leads to the problem of word sense ambiguity (WSA). Urdu language-based information retrieval system using different techniques related to Web Semantic Search Engine architecture is proposed to efficiently retrieve the relevant information and solve the problem of WSA. The proposed system has average precision ratio 96% as compared to average precision ratio of 74% and 75% average precision Google for single word query. For the long text queries, our system outperforms the existing famous search engines with 92% accuracy such as Bing and Google having 16.50% and 16% accuracy, respectively. Similarly, the proposed system for single word query, the recall ratio is 32.25% as compared to 25% and 25% of Bing and Google. The results of recall ratio for long text query are improved as well, showing 6.38% as compared to 6.20% and 4.8% of Bing and Google, respectively. The results showed that the proposed system gives better and efficient results as compared to the existing systems for Urdu language.  © 2023 Association for Computing Machinery.",context-based; corpus; information retrieval; keywords; ontology; quad extraction; searching and indexing; semantic web; triplets; Uniform Resource Identifier; Urdu language; Web Semantic Search Engine; WSA,Information retrieval; Search engines; Context-based; Corpus; Keyword; Ontology's; Quad extraction; Searching and indexing; Semantic search engines; Semantic-Web; Sense ambiguities; Triplet; Uniform resource identifiers; Urdu language; Web semantic search engine; Web semantics; Word sense; Word sense ambiguity; Semantic Web
A Comprehensive Roadmap on Bangla Text-based Sentiment Analysis,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161520390&doi=10.1145%2f3572783&partnerID=40&md5=238629e4d8d96f8344f14077126a571b,"The effortless expansion of Internet access has eventually transformed the dissemination behavior toward E-Mode. Thus, the usage of online or, more specifically, ""Digital""texts has expanded abruptly. ""Bangla,""the seventh most spoken language globally, has no different nature. Communication in the Bangla language has also been exposed on the Internet, which describes the feelings of individuals in any specific context. These enormously generated data from diverse sources have drawn the interest of the researchers working in the Natural Language Processing domain. Despite its relatively complicated structure, a lesser amount of annotated data, as well as a limited number of frameworks and approaches, exist. This lacking of resources has kept several stones unturned in this diverse, emotion-rich, and widely spoken language. To bridge the lacking and absence of resources, this article aims to provide a generalized deduced working procedure in this domain. To do so, the existing research work in the domain of sentiment analysis using Bangla text has been collected, evaluated, and summarized. Also, in this article, the techniques used in pre-processing, feature extraction, and eventually used algorithms have been identified and discussed. Considering these facts, this research work sketches a tentative blueprint of sentiment analysis using Bangla text. Additionally, this article discusses existing regional language corpora such as Tamil, Urdu, and Hindi, as well as English and methodologies used to extract emotional essence from Bangla language comparing other languages. That will assist in determining the probable chosen path of exploring Bangla in a deeper aspect. Moreover, this work has deduced and presented a generalized framework that will direct aspiring researchers to decide the pathway of choosing data vis-à-vis methodologies based on their interests.  © 2023 Association for Computing Machinery.",Emotion Recognition; low-resource language; Natural Language Processing; Sentiment analysis; sentiment analysis in Bangla; text classification,Classification (of information); Emotion recognition; Language processing; Low resource languages; Natural language processing; Natural languages; Sentiment analyse in bangla; Sentiment analysis; Spoken languages; Text classification; Emotion Recognition
Impact of Tokenization on Language Models: An Analysis for Turkish,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161021988&doi=10.1145%2f3578707&partnerID=40&md5=5714031224065af048e3c71a6a3cfc6b,"Tokenization is an important text preprocessing step to prepare input tokens for deep language models. WordPiece and BPE are de facto methods employed by important models, such as BERT and GPT. However, the impact of tokenization can be different for morphologically rich languages, such as Turkic languages, in which many words can be generated by adding prefixes and suffixes. We compare five tokenizers at different granularity levels, that is, their outputs vary from the smallest pieces of characters to the surface form of words, including a Morphological-level tokenizer. We train these tokenizers and pretrain medium-sized language models using the RoBERTa pretraining procedure on the Turkish split of the OSCAR corpus. We then fine-tune our models on six downstream tasks. Our experiments, supported by statistical tests, reveal that the morphological-level tokenizer delivers a challenging performance with de facto tokenizers. Furthermore, we find that increasing the vocabulary size improves the performance of Morphological- and Word-level tokenizers more than that of de facto tokenizers. The ratio of the number of vocabulary parameters to the total number of model parameters can be empirically chosen as 20% for de facto tokenizers and 40% for other tokenizers to obtain a reasonable trade-off between model size and performance.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Language model; morphological analysis; tokenization; vocabulary size,Economic and social effects; Different granularities; Language model; Morphological analysis; Performance; Pre-processing step; Text preprocessing; Tokenization; Tokenizer; Turkishs; Vocabulary size; Computational linguistics
Bidirectional Sentence Ordering with Interactive Decoding,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152904243&doi=10.1145%2f3561510&partnerID=40&md5=d208c8fbcf632b7c951563490443dd8e,"Sentence ordering aims at restoring orders of shuffled sentences in a paragraph. Previous methods usually predict orders in a single direction, i.e., from head to tail. However, unidirectional prediction inevitably causes error accumulation, which restricts performance. In this article, we propose a bidirectional ordering method, which predicts orders in both head-to-tail and tail-to-head directions at the same time. In our bidirectional ordering method, two directions can interact with each other and help alleviate the error accumulation problem of ordering. Experiments demonstrate that our method can effectively improve performance of previous models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSentence ordering; bidirectional; interactive,Additional key word and phrasessentence ordering; Bidirectional; Error accumulation; Improve performance; Interactive; Key words; Performance; Predict orders; Sentence ordering; Two directions
Open-Domain Response Generation in Low-Resource Settings using Self-Supervised Pre-Training of Warm-Started Transformers,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156139452&doi=10.1145%2f3579164&partnerID=40&md5=1f8a1692e0906719a19cbadf34c9963c,"Learning response generation models constitute the main component of building open-domain dialogue systems. However, training open-domain response generation models requires large amounts of labeled data and pre-trained language generation models that are often nonexistent for low-resource languages. In this article, we propose a framework for training open-domain response generation models in low-resource settings. We consider Dialectal Arabic (DA) as a working example. The framework starts by warm-starting a transformer-based encoder-decoder with pre-trained language model parameters. Next, the resultant encoder-decoder model is adapted to DA by employing self-supervised pre-training on large-scale unlabeled data in the desired dialect. Finally, the model is fine-tuned on a very small labeled dataset for open-domain response generation. The results show significant performance improvements on three spoken Arabic dialects after adopting the framework's three stages, highlighted by higher BLEU and lower Perplexity scores compared with multiple baseline models. Specifically, our models are capable of generating fluent responses in multiple dialects with an average human-evaluated fluency score above 4. Our data is made publicly available.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Arabic dialect; language models; Response generation,Computational linguistics; Signal encoding; Speech processing; Arabic dialects; Dialectal arabics; Dialogue systems; Encoder-decoder; Labeled data; Language model; Large amounts; Low-resource settings; Pre-training; Response generation; Decoding
Hyper Parameter Optimization of CRNN for Printed Devanagari Script Recognition using Taguchi's Method,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161587664&doi=10.1145%2f3578549&partnerID=40&md5=bc8aa02950f1cc933a9bc9406cb05e6c,"The Devanagari script is one of the most widely used scripts worldwide. The existing deep learning-based optical character recognition system for printed Devanagari scripts using Convolutional Neural Network - Recurrent Neural Network, or CRNN is not robust enough to recognize any randomly printed Devanagari scanned document. At present, the hyper-parameters of the CRNN system are selected randomly either with the trial-and-error or grid search methods. Moreover, there is no optimized way to choose the hyper-parameters of the CRNN, which improves the recognition accuracy for Devanagari documents. Furthermore, the lack of standard Devanagari script datasets has hampered the development of word recognizers. In this paper, the hyper-parameter of the CRNN system is optimized using Taguchi's method of optimization. The performance of the hyper-parameters optimized CRNN system is compared with the current state-of-the-art text recognition CRNN network. The results reveal that the CRNN optimized with Taguchi's method performs better than the CRNN-based systems.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CRNN; Devanagari script; hyper-parameters; optimization; Taguchi method,Convolutional neural networks; Optical character recognition; Recurrent neural networks; Convolutional neural network; CRNN; Devanagari script; Grid-search method; Hyper-parameter; Hyper-parameter optimizations; Optical character recognition system; Optimisations; Taguchi's methods; Trial and error; Taguchi methods
A Multi-View Learning Approach for Detecting Personality Disorders Among Arab Social Media Users,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161522800&doi=10.1145%2f3572906&partnerID=40&md5=725d5de226ee166b1549a801463b934f,"Multi-view fusion approaches have gained increasing interest in the past few years by researchers. This interest comes due to the many perspectives that datasets can be looked at and evaluated. One of the most urging areas that require constant leveraging with latest technologies and multi-perspective judgments is the area of psychology. In this article, a novel multi-view fusion model using deep learning algorithms is presented to detect popular types of personality disorders among Arab users of the Twitter platform in an expert-driven fashion, based on the descriptions of the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition. To the best of our knowledge, the work presented is the first of its kind with no publicly available datasets that report statements around personality disorders in the Arabic language, and thus we created AraPerson, a dataset that consists of 8,000 textual tweets coupled with 8,000 images that prescribe mental statuses for a total of 150 users collected with regular expressions generated under the supervision of domain experts. The dataset was fed into a baseline multi-view model by combining a CNN model with a Bi-LSTM model to detect two types of popular personality disorders by analyzing textual and visual posts on 150 user profiles. The experiments were followed with fusing the DenseNet model with the Bi-LSTM model, experimenting with the effect of using concatenation, addition, and multiplication methods for vectors' combination. The work presented in this article is unprecedented, specifically in a controversial area such as personality disorders detection among Arab communities. The best reported accuracy is 87%, which is quite promising, as the two types of personality disorders investigated are highly overlapping.  © 2023 Association for Computing Machinery.",Arabic language; image processing; multi-view learning; natural language processing; Personality disorders,Learning algorithms; Long short-term memory; Natural language processing systems; Social networking (online); User profile; Arabic languages; Images processing; Language processing; Learning approach; Multi-view learning; Multi-views; Natural language processing; Natural languages; Personality disorder; Social media; Image processing
HateCircle and Unsupervised Hate Speech Detection Incorporating Emotion and Contextual Semantics,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161597049&doi=10.1145%2f3576913&partnerID=40&md5=12a77df6b26dcec44f4a16b4591fb975,"The explosive growth of social media has fueled an extensive increase in online freedom of speech. The worldwide platform of human voice creates possibilities to assail other users without facing any consequences, and flout social etiquettes, resulting in an inevitable increase of hate speech. Nowadays, English hate speech detection is a popular research area, but the prevalence of implicit hate content in regional languages desire effective language-independent models. The proposed research is the first unsupervised Hindi and Bengali hate content detection framework consisting of three significant concepts: HateCircle, hate tweet classification, and code-switch data preparation algorithms. The novel HateCircle method is proposed to detect hate orientation for each term by co-occurrence patterns of words, contextual semantics, and emotion analysis. The efficient multiclass hate tweet classification algorithm is proposed with parts of speech tagging, Euclidean distance, and the Geometric median methods. The detection of hate content is more efficient in the native script compared to the Roman script, so the transliteration algorithm is also proposed for code-switch data preparation. The experimentation evaluates the combination of various lexicons with our enriched hate lexicon that achieves a maximum of 0.74 F1-score for the Hindi and 0.88 F1-score for the Bengali datasets. The novel HateCircle and hate tweet detection framework evaluates with our proposed parts of speech tagging and Geometric median detection methods. Results reveal that HateCircle and hate tweet detection framework also achieves a maximum of 0.73 accuracy for the Hindi and 0.78 accuracy for the Bengali dataset. The experiment results signify that contextual semantic hate speech detection research with a language-independency feature offsets the growth of implicit abusive text in social media.  © 2023 Association for Computing Machinery.",code-switch script; contextual semantics; emotion analysis; hate speech detection; Indian languages; Low-resource languages; parts-of-speech tagging; social media,Classification (of information); Computational linguistics; Emotion Recognition; Semantics; Social networking (online); Code-switch script; Contextual semantics; Emotion analysis; Hate speech detection; Indian languages; Low resource languages; Part of speech tagging; Parts-of-speech tagging; Social media; Speech detection; Speech recognition
Attention Mechanism Architecture for Arabic Sentiment Analysis,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161490842&doi=10.1145%2f3578265&partnerID=40&md5=10183f8d9f7c881c7a5cf5209f9fad02,"This article tackles the problem of sentiment analysis in the Arabic language where a new deep learning model has been put forward. The proposed model uses a hybrid bidirectional gated recurrent unit (BiGRU) and bidirectional long short-term memory (BiLSTM) additive-attention model where the Bidirectional GRU/LSTM reads the individual sentence input from left to right and vice versa, enabling the capture of the contextual information. However, the model is trained on two types of embeddings: FastText and local learnable embeddings. The BiLSTM and BiGRU architectures are put into competition to identify the best hyperparameter set for the model. The developed model has been tested on three large-scale commonly employed Arabic sentiment dataset: large-scale Arabic Book Reviews Dataset (ABRD), Hotel Arabic-Reviews Dataset (HARD), and Books Reviews in the Arabic Dataset (BRAD). The testing results demonstrate that our model outperforms both the baseline models and the state-of-the-art models reported in the original references of these datasets, achieving accuracy scores of 98.6%, 96.19%, 95.65% for LARB, HARD, and BRAD, respectively. Furthermore, to demonstrate the generalization capabilities of our model, the performances of the model have been evaluated on three other natural language processing tasks: news categorization, offensive speech detection, and Russian sentiment analysis. The results demonstrated the developed model is language- and task-independent, which offers new perspectives for the application of the developed models in several other natural language processing challenges.  © 2023 Copyright held by the owner/author(s).",Arabic sentiment analysis; attention mechanism; language understanding; pretrained word embeddings,Deep learning; Embeddings; Large dataset; Reviews; Arabic sentiment analyse; Attention mechanisms; Book reviews; Developed model; Embeddings; Language understanding; Large-scales; Natural languages; Pretrained word embedding; Sentiment analysis; Sentiment analysis
"Approaches, Methods, and Resources for Assessing the Readability of Arabic Texts",2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161581132&doi=10.1145%2f3571510&partnerID=40&md5=1442315165c385efe51844f16c5a7115,"Text readability assessment is a well-known problem that has acquired even more importance in today's information-rich world. In this article, we survey various approaches to measuring and assessing the readability of texts. Our specific goal is to provide a perspective on the state-of-the-art in readability assessment research for Arabic, which differs significantly from other languages on which readability studies have tended to focus. We provide background on readability assessment research and tools for English, for which readability studies are the most advanced. We then survey approaches adopted for Arabic, both classical formula-based approaches and studies that combine Machine Learning (ML) with Natural Language Processing (NLP) techniques. The works we cover target text corpora for different audiences: school-age first language readers (L1), foreign language learners (L2), and adult readers in non-academic contexts. Therefore, we explore differences between reading in L1 and L2 and consider how they play out specifically in Arabic after describing language characteristics that may impact readability. Finally, we highlight challenges for Arabic readability research and propose multiple future directions to improve readability assessment and related applications that would benefit from more attention.  © 2023 Association for Computing Machinery.",Arabic; computational linguistics; features; Machine Learning (ML); Readability; readability formulas; reading difficulty; text complexity,Computational linguistics; Learning algorithms; Natural language processing systems; Arabic; Arabic texts; Feature; Machine learning; Machine-learning; Readability; Readability formula; Reading difficulty; State of the art; Text complexity; Machine learning
Automated Generation of Human-readable Natural Arabic Text from RDF Data,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161493427&doi=10.1145%2f3582262&partnerID=40&md5=029b37a852da8a0911d828a4f7ee6f6c,"With the advances in Natural Language Processing (NLP), the industry has been moving towards human-directed artificial intelligence (AI) solutions. Recently, chatbots and automated news generation have captured a lot of attention. The goal is to automatically generate readable text from tabular data or web data commonly represented in Resource Description Framework (RDF) format. The problem can then be formulated as Data-to-text (D2T) generation from structured non-linguistic data into human-readable natural language. Despite the significant work done for the English language, no efforts are being directed towards low-resource languages like the Arabic language. This work promotes the development of the first RDF data-to-text (D2T) generation system for the Arabic language while trying to address the low-resource limitation. We develop several models for the Arabic D2T task using transfer learning from large language models (LLM) such as AraBERT, AraGPT2, and mT5. These models include a baseline Bi-LSTM Sequence-to-Sequence (Seq2Seq) model, as well as encoder-decoder transformers like BERT2BERT, BERT2GPT, and T5. We then provide a detailed comparative study highlighting the strengths and limitations of these methods setting the stage for further advancement in the field. We also introduce a new Arabic dataset (AraWebNLG) that can be used for new model development in the field. To ensure a comprehensive evaluation, general-purpose automated metrics (BLEU and Perplexity scores) are used as well as task-specific human evaluation metrics related to the accuracy of the content selection and fluency of the generated text. The results highlight the importance of pre-training on a large corpus of Arabic data and show that transfer learning from AraBERT gives the best performance. Text-to-text pre-training using mT5 achieves second best performance results even with multilingual weights.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data-to-text; datasets; language models; Low-resource languages; neural networks; RDF,Automation; Computational linguistics; Learning systems; Long short-term memory; Natural language processing systems; Resource Description Framework (RDF); Arabic languages; Data-to-text; Dataset; Human-readable; Language model; Low resource languages; Natural languages; Neural-networks; Resources description frameworks; Transfer learning; Petroleum reservoir evaluation
Composing Word Embeddings for Compound Words Using Linguistic Knowledge,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152968645&doi=10.1145%2f3561299&partnerID=40&md5=7561ee8a7225b67b7fe93eda450a42a3,"In recent years, the use of distributed representations has been a fundamental technology for natural language processing. However, Japanese has multiple compound words, and often we must compare the meanings of a word and a compound word. Moreover, word boundaries in Japanese are unspecific because Japanese does not have delimiters between words, e.g., (grape picking) is one word according to one dictionary, whereas and are different words according to another dictionary. This study describes an attempt to compose word embeddings of a compound word from its constituent words in Japanese. We used ""short unit""and ""long unit,""both of which are the units of terms in UniDic - a Japanese dictionary compiled by the National Institute for Japanese Language and Linguistics - for constituent and compound words, respectively. Furthermore, we composed a word embedding of a compound word from the word embeddings of two constituent words using a neural network. The training data for the word embedding of compound words was created using a corpus generated by concatenating the corpora divided by constituent and compound words. We propose using linguistic knowledge for compositing word embedding to demonstrate how it improves the composition performance. We compared cosine similarity between composed and correct word embeddings of compound words to assess models with and without linguistic knowledge. Furthermore, we evaluated our methods by the ranking of synonyms using a thesaurus. We compared several frameworks and algorithms that use three types of linguistic knowledge - semantic patterns, parts of speech patterns, and compositionality score - and then investigated which linguistic knowledge improves the composition performance. The experiments demonstrated that the multitask models with the classification task of the parts of speech patterns and the estimation task of compositionality scores achieved high performances.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesWord embedding; compound word; constituent word; Japanese; linguistic knowledge; multitask learning; parts of speech,Classification (of information); Natural language processing systems; Semantics; Additional key word and phrasesword embedding; Compound words; Constituent word; Embeddings; Japanese; Key words; Linguistic knowledge; Multitask learning; Part Of Speech; Performance; Embeddings
Fusion Pre-trained Emoji Feature Enhancement for Sentiment Analysis,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161486650&doi=10.1145%2f3578582&partnerID=40&md5=630fd4d3502d032d6a16da4bd702d5db,"Emoji are often used in social media to enrich users' emotions, and they play an important role in the task of social media sentiment analysis. In practice, researchers are more likely to consider emoji as special symbols and treat them separately from the text. Some existing methods use emoji as a dictionary for matching or converting emoji into text. However, these methods disregard the relationship between emoji and context, blue and they do not reflect the emotions that users are expected to express. It is challenging to incorporate the original emotions of emoji in social media sentiment analysis. In this article, we propose the EPE model: Emoji Pre-trained feature Enhanced sentiment analysis. Specifically, we collected 8 million tweets and selected 5 million tweets with pre-trained emoji with context using the BERT model. We labeled 20,000 tweets as a three-category dataset and used Bi-LSTM with an attention layer to extract text features. Emoji were retained as key emotion information and combined with text features in the final layer as a connected vector for final prediction. Experimental results with our dataset showed that the proposed EPE model achieved better performance than other baseline models.  © 2023 Association for Computing Machinery.",emoji; feature combination; fusion pre-trained; Sentiment analysis; social media,Long short-term memory; Social networking (online); Emoji; Feature combination; Feature enhancement; Fusion pre-trained; Sentiment analysis; Social media; Special symbols; Special Treat; Text feature; User emotions; Sentiment analysis
Channel Attention TextCNN with Feature Word Extraction for Chinese Sentiment Analysis,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161544217&doi=10.1145%2f3571716&partnerID=40&md5=0d776ad51064467751fc87e24239b99a,"Chinese short text sentiment analysis can help understand society's views on various hot topics. Many existing sentiment analysis methods are based on sentiment dictionaries. Still, sentiment dictionaries are easily affected by subjective factors. They require a lot of time to build as well as maintenance to prevent obsolescence. For the aim of extracting rich information within texts more effectively, we propose a Channel Attention TextCNN with Feature Word Extraction model (CAT-FWE). The feature word extraction module helps us choose words that affect the sentiment of reviews. Then, these words are integrated with multi-level semantic information to enhance the information of sentences. In addition, the channel attention textCNN module that is a promotion of traditional TextCNN tends to pay more attention to those meaningful features. It eliminates the impacts of features that do not make any sense effectively. We apply our CAT-FWE model to both fine-grained classification and binary classification tasks for Chinese short texts. Experiment results show that it can improve the performance of emotion recognition.  © 2023 Association for Computing Machinery.",attention; feature word extraction; sentiment analysis; Social platforms,Emotion Recognition; Obsolescence; Semantics; Analysis method; Attention; Chinese short-text; Extraction modeling; Feature word extraction; Feature words; Hot topics; Sentiment analysis; Sentiment dictionaries; Social platform; Extraction
Improving the Robustness of Loanword Identification in Social Media Texts,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161543773&doi=10.1145%2f3572773&partnerID=40&md5=b49ca05e54eb87ee122792ede049c3e5,"As a potential bilingual resource, loanwords play a very important role in many natural language processing tasks. If loanwords in a low-resource language can be identified effectively, the generated donor-receipt word pairs will benefit many cross-lingual natural language processing tasks. However, most studies on loanword identification mainly focus on formal texts such as news and government documents. Loanword identification in social media texts is still an under-studied field. Since it faces many challenges and can be widely used in several downstream tasks, more efforts should be put on loanword identification in social media texts. In this study, we present a multi-task learning architecture with deep bi-directional recurrent neural networks for loanword identification in social media texts, where different task supervision can happen at different layers. The multi-task neural network architecture learns higher-order feature representations from word and character sequences along with basic spell error checking, part-of-speech tagging, and named entity recognition information. Experimental results on Uyghur loanword identification in social media texts in five donor languages (Chinese, Arabic, Russian, Turkish, and Farsi) show that our method achieves the best performance compared with several strong baseline systems. We also combine the loanword detection results into the training data of neural machine translation for low-resource language pairs. Experiments show that models trained on the extended datasets achieve significant improvements compared with the baseline models in all language pairs.  © 2023 Association for Computing Machinery.",Loanword identification; low-resource natural language processing; multi-task learning; social media texts,Computational linguistics; Learning algorithms; Learning systems; Multilayer neural networks; Natural language processing systems; Network architecture; Social networking (online); Speech recognition; Bilingual resources; Language pairs; Language processing; Loanword identification; Low resource languages; Low-resource natural language processing; Multitask learning; Natural languages; Social media; Social medium text; Recurrent neural networks
Dynamic Convolution-based Encoder-Decoder Framework for Image Captioning in Hindi,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161494424&doi=10.1145%2f3573891&partnerID=40&md5=ec6423634413ffc821d37eeef7e64ece,"In sequence-to-sequence modeling tasks, such as image captioning, machine translation, and visual question answering, encoder-decoder architectures are state of the art. An encoder, convolutional neural network (CNN) encodes input images into fixed dimensional vector representation in the image captioning task, whereas a decoder, a recurrent neural network, performs language modeling and generates the target descriptions. Recent CNNs use the same operation over every pixel; however, all the image pixels are not equally important. To address this, the proposed method uses a dynamic convolution-based encoder for image encoding or feature extraction, Long-Short-Term-Memory as a decoder for language modeling, and X-Linear attention to make the system robust. Encoders, attentions, and decoders are important aspects of the image captioning task; therefore, we experiment with various encoders, decoders, and attention mechanisms. Most of the works for image captioning have been carried out for the English language in the existing literature. We propose a novel approach for caption generation from images in Hindi. Hindi, widely spoken in South Asia and India, is the fourth most-spoken language globally; it is India's official language. The proposed method utilizes dynamic convolution operation on the encoder side to obtain a better image encoding quality. The Hindi image captioning dataset is manually created by translating the popular MSCOCO dataset from English to Hindi. In terms of BLEU scores, the performance of the proposed method is compared with other baselines, and the results obtained show that the proposed method outperforms different baselines. Manual human assessment in terms of adequacy and fluency of the captions generated further determines the efficacy of the proposed method in generating good-quality captions.  © 2023 Association for Computing Machinery.",attention; deep learning; dynamic convolution; Hindi,Computational linguistics; Decoding; Encoding (symbols); Modeling languages; Neural machine translation; Pixels; Recurrent neural networks; Signal encoding; Attention; Deep learning; Dynamic convolution; Encoder-decoder; Hindi; Image captioning; Image encoding; Language model; Modeling task; Sequence models; Convolution
Multi-task Label-wise Transformer for Chinese Named Entity Recognition,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161540649&doi=10.1145%2f3576025&partnerID=40&md5=5f135d4f096fd4652ec083dd9831057d,"Benefiting from the improvement of positional encoding and the introduction of lexical knowledge, Transformer has achieved superior performance than the prevailing BiLSTM-based models in named entity recognition (NER) task. However, existing Transformer-based models for Chinese NER pay less attention to the information captured by the bottom layers of Transformer and the significance of representation subspace where each head of Transformer is projected. In this article, we propose Multi-Task Label-Wise Transformer (MTLWT). From a global perspective, we assign entity boundary prediction (EBP) and entity type prediction (ETP) tasks to the first two layers. In this way, we stimulate lower layers to participate more in constructing character representation. Besides, in each multi-head self-attention (MHSA) layer, we provide a specific focus for each individual head, making the head project into a significant subspace. Experiments on four datasets from different domains show that our proposed model achieves comparable performance with other state-of-the-art models. In particular, MTLWT outperforms the other frameworks without external knowledge on all the datasets.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Chinese named entity recognition; multi-task learning,Bottom layers; Chinese named entity recognition; Encodings; Global perspective; Knowledge transformers; Lexical knowledge; Multi tasks; Multitask learning; Named entity recognition; Performance; Learning systems
Neural Variational Gaussian Mixture Topic Model,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161501200&doi=10.1145%2f3578583&partnerID=40&md5=91c204276f1a389c33a30b71362d0ebc,"Neural variational inference-based topic modeling has gained great success in mining abstract topics from documents. However, these topic models usually mainly focus on optimizing the topic proportions for documents, while the quality and the internal construction of topics are usually neglected. Specifically, these models lack the guarantee that semantically related words are supposed to be assigned to the same topic and are difficult to ensure the interpretability of topics. Moreover, many topical words recur frequently in the top words of different topics, which makes the learned topics semantically redundant and similar, and of little significance for further study. To solve the above problems, we propose a novel neural topic model called Neural Variational Gaussian Mixture Topic Model (NVGMTM). We use Gaussian distribution to depict the semantic relevance between words in the topics. Each topic in NVGMTM is considered as a multivariate Gaussian distribution over words in the word-embedding space. Thus, semantically related words share similar probabilities in each topic, which makes the topics more coherent and interpretable. Experimental results on two public corpora show the proposed model outperforms the state-of-the-art baselines.  © 2023 Association for Computing Machinery.",Neural variational gaussian mixture topic model; topic discrimination; topic quality,Semantics; Gaussian-mixtures; Interpretability; Multivariate Gaussian Distributions; Neural variational gaussian mixture topic model; Semantic relevance; Semantically-related words; Topic discrimination; Topic Modeling; Topic quality; Variational inference; Gaussian distribution
Bravely Say I Don't Know: Relational Question-Schema Graph for Text-to-SQL Answerability Classification,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161569803&doi=10.1145%2f3579030&partnerID=40&md5=9dfea4e05d9c381ce0769a0cc3689f0a,"Recently, the Text-to-SQL task has received much attention. Many sophisticated neural models have been invented that achieve significant results. Most current work assumes that all the inputs are legal and the model should generate an SQL query for any input. However, in the real scenario, users are allowed to enter the arbitrary text that may not be answered by an SQL query. In this article, we focus on the issue-answerability classification for the Text-to-SQL system, which aims to distinguish the answerability of the question according to the given database schema. Existing methods concatenate the question and the database schema into a sentence, then fine-tune the pre-trained language model on the answerability classification task. In this way, the database schema is regarded as sequence text that may ignore the intrinsic structure relationship of the schema data, and the attention that represents the correlation between the question token and the database schema items is not well designed. To this end, we propose a relational Question-Schema graph framework that can effectively model the attention and relation between question and schema. In addition, a conditional layer normalization mechanism is employed to modulate the pre-trained language model to generate better question representation. Experiments demonstrate that the proposed framework outperforms all existing models by large margins, achieving new state of the art on the benchmark TRIAGESQL. Specifically, the model attains 88.41%, 78.24%, and 75.98% in Precision, Recall, and F1, respectively. Additionally, it outperforms the baseline by approximately 4.05% in Precision, 6.96% in Recall, and 6.01% in F1.  © 2023 Association for Computing Machinery.",answerability classification; relational graph; Text-to-SQL,Computational linguistics; Database systems; Query processing; Text processing; 'current; Answerability classification; Classification tasks; Database schemas; Language model; Neural modelling; Relational graph; Schema graph; SQL query; Text-to-SQL; Classification (of information)
Tamil Offensive Language Detection: Supervised versus Unsupervised Learning Approaches,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161559459&doi=10.1145%2f3575860&partnerID=40&md5=efc1bf1b414ce8581ff5e72763d245e0,"Studies on natural language processing are mainly conducted in English, with very few exploring languages that are under-resourced, including the Dravidian languages. We present a novel work in detecting offensive language using a corpus collected from YouTube containing comments in Tamil. The study specifically aims to compare two machine learning approaches - namely, supervised and unsupervised - to detect offensive patterns in textual communications. In the first setup, offensive language detection models were developed using traditional machine learning algorithms such as Random Forest, Logistic Regression, Support Vector Machine, and AdaBoost, and assessed based on human labeling. Conversely, we used K-means (K = 2) to cluster the unlabeled data before training the same set of machine learning algorithms to detect offensive communications. Performance scores indicate unsupervised clustering to be more effective than human labeling with ensemble classifiers achieving an impressive accuracy of 99.70% and 99.87% respectively for balanced and imbalanced datasets, hence showing that the unsupervised approach can be used effectively to detect offensive language in low-resourced languages.  © 2023 Association for Computing Machinery.",Dravidian; machine learning; Offensive language; social media; Tamil,Classification (of information); Forestry; K-means clustering; Learning systems; Logistic regression; Natural language processing systems; Random forests; Social networking (online); Support vector machines; Dravidian; Human labelling; Language detection; Learning approach; Machine learning algorithms; Machine-learning; Natural languages; Offensive languages; Social media; Tamil; Adaptive boosting
Short Text Classification of Chinese with Label Information Assisting,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149393321&doi=10.1145%2f3582301&partnerID=40&md5=a5a7b71bf03322312ffc80988e207414,"As a common language form in oral communication, short text is hard to be used in the applications such as intent understanding, text classification and so on due to its limited content and information, as well as irregular expression and missing components. To increase the availability of short texts in real applications, we propose a Label Information Assisting-based Model (LIAM) for Chinese short text classification. In the model, we jointly use sentence-level features and word-level features to reduce text information loss. And the sentence-level features are fused with relevant label information by the Label Information Extending and Fusion (LIEF) module while the word-level features are also enhanced with assistance of relevant label information. By utilizing the text-related information from labels as extended information, the model enriches and enhances the features of short text, benefiting classification. To verify the correctness and effectiveness of the proposed method, we conduct extensive experiments on four Chinese datasets and six sub-datasets with different models. The experimental results show that LIAM presented can effectively enrich information for text and much improve the performance of short text classification. It performs much better than other methods do. What is more, the less the training set, the greater the advantages of the model.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",information mining; label information embedding; Short text classification,Data mining; Text processing; Common languages; Information embedding; Information mining; Label information; Label information embedding; Oral communication; Sentence level; Short text classifications; Short texts; Word level; Classification (of information)
Detection and Cross-domain Evaluation of Cyberbullying in Facebook Activity Contents for Turkish,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160200464&doi=10.1145%2f3580393&partnerID=40&md5=c76a521005513bf2f192d3fa843551c1,"Cyberbullying refers to bullying and harassment of defenseless or vulnerable people such as children, teenagers, and women through any means of communication (e.g., e-mail, text messages, wall posts, tweets) over any online medium (e.g., social media, blogs, online games, virtual reality environments). The effect of cyberbullying may be severe and irreversible and it has become one of the major problems of cyber-societies in today's electronic world. Prevention of cyberbullying activities as well as the development of timely response mechanisms require automated and accurate detection of cyberbullying acts. This study focuses on the problem of cyberbullying detection over Facebook activity content written in Turkish. Through extensive experiments with the various machine and deep learning algorithms, the best estimator for the task is chosen and then employed for both cross-domain evaluation and profiling of cyber-aggressive users. The results obtained with fivefold cross-validation are evaluated with an average-macro F1 score. These results show that BERT is the best estimator with an average macro F1 of 0.928, and employing it on various datasets collected from different OSN domains produces highly satisfying results. This article also reports detailed profiling of cyber-aggressive users by providing even more information than what is visible to the naked eye.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cyber-aggression; cyberbullying; Facebook; machine learning; online social networks,Computer crime; Deep learning; E-learning; Learning algorithms; Learning systems; User profile; Virtual reality; Cross-domain evaluations; Cybe-aggression; Cyber bullying; Facebook; Machine-learning; On-line games; Online medium; Social media; Turkishs; Virtual-reality environment; Social networking (online)
Emotion Detection in Code-Mixed Roman Urdu-English Text,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152633888&doi=10.1145%2f3552515&partnerID=40&md5=83dab49f8e348d67705f6b5fbda445b1,"Emotion detection is a widely studied topic in natural language processing due to its significance in a number of application areas. A plethora of studies have been conducted on emotion detection in European as well as Asian languages. However, a large majority of these studies have been conducted in monolingual settings, whereas little attention has been paid to emotion detection in code-mixed text. Specifically, merely one study has been conducted on emotion detection in Roman Urdu (RU) and English (EN) code-mixed text despite the fact that such text is widely used in social media platforms. A careful examination of the existing study has revealed several issues which justify that this area requires attention of researchers. For instance, more than 37% of the messages in the contemporary corpus are monolingual sentences representing that a purely code-mixed emotion analysis corpus is non-existent. To that end, this study has scrapped 400,000 sentences from three social media platforms to identify 20,000 RU-EN code-mixed sentences. Subsequently, an iterative approach is employed to develop emotion detection guidelines. These guidelines have been used to develop a large RU-EN emotion detection (RU-EN-Emotion) corpus in which 20,000 sentences are annotated as Neutral or Emotion-sentence. The sentences having emotions are further annotated with the respective emotions. Subsequently, 102 experiments are performed to evaluate the effectiveness of six classical machine learning techniques and six deep learning techniques. The results show, (a) CNN is the most effective technique when used with GloVe embeddings, and (b) our developed RU-EN-Emotion corpus is more useful than the contemporary corpus, as it employs a two-level classification approach.  © 2023 Association for Computing Machinery.",code-mixed text; corpus generation; emotion detection; Roman Urdu text processing; South Asian languages; Urdu language processing,Codes (symbols); Deep learning; Iterative methods; Learning algorithms; Learning systems; Linguistics; Natural language processing systems; Social networking (online); Code-mixed text; Corpus generation; Emotion detection; Language processing; Natural languages; Roman urdu text processing; Social media platforms; South Asian languages; Text-processing; Urdu language processing; Text processing
Document-Level Relation Extraction with Path Reasoning,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161582396&doi=10.1145%2f3572898&partnerID=40&md5=51abaa041a86ac3b0de3820930245f0b,"Document-level relation extraction (DocRE) aims to extract relations among entities across multiple sentences within a document by using reasoning skills (i.e., pattern recognition, logical reasoning, coreference reasoning, etc.) related to the reasoning paths between two entities. However, most of the advanced DocRE models only attend to the feature representations of two entities to determine their relation, and do not consider one complete reasoning path from one entity to another entity, which may hinder the accuracy of relation extraction. To address this issue, this article proposes a novel method to capture this reasoning path from one entity to another entity, thereby better simulating reasoning skills to classify relation between two entities. Furthermore, we introduce an additional attention layer to summarize multiple reasoning paths for further enhancing the performance of the DocRE model. Experimental results on a large-scale document-level dataset show that the proposed approach achieved a significant performance improvement on a strong heterogeneous graph-based baseline.  © 2023 Association for Computing Machinery.",Document-level relation extraction; graph neural network; path reasoning,Extraction; Graph neural networks; Graph theory; Knowledge graph; Large dataset; Pattern recognition; Coreference; Document-level relation extraction; Extraction modeling; Feature representation; Graph neural networks; Logical reasoning; Novel methods; Path reasoning; Performance; Relation extraction; Graphic methods
Learning Reliable Neural Networks with Distributed Architecture Representations,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161603892&doi=10.1145%2f3578709&partnerID=40&md5=c07b5dda4ccd32282bc41139c8daa8f3,"Neural architecture search (NAS) has shown the strong performance of learning neural models automatically in recent years. But most NAS systems are unreliable due to the architecture gap brought by discrete representations of atomic architectures. In this article, we improve the performance and robustness of NAS via narrowing the gap between architecture representations. More specifically, we apply a general contraction mapping to model neural networks with distributed representations (Neural Architecture Search with Distributed Architecture Representations (ArchDAR)). Moreover, for a better search result, we present a joint learning approach to integrating distributed representations with advanced architecture search methods. We implement our ArchDAR in a differentiable architecture search model and test learned architectures on the language modeling task. On the Penn Treebank data, it outperforms a strong baseline significantly by 1.8 perplexity scores. Also, the search process with distributed representations is more stable, which yields a faster structural convergence when it works with the differentiable architecture search model.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",language modeling; natural language processing; Neural architecture search; neural networks,Architecture; Computational linguistics; Learning algorithms; Learning systems; Modeling languages; Network architecture; Distributed architecture; Distributed representation; Language model; Language processing; Natural language processing; Natural languages; Neural architecture search; Neural architectures; Neural-networks; Performance; Natural language processing systems
Abstractive Summarization of Text Document in Malayalam Language: Enhancing Attention Model Using POS Tagging Feature,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152954112&doi=10.1145%2f3561819&partnerID=40&md5=ee9a5ff69757547ef9b7e8abab2c3c2a,"Over the past few years, researchers are showing huge interest in sentiment analysis and summarization of documents. The primary reason being that huge volumes of information are available in textual format, and this data has proven helpful for real-world applications and challenges. The sentiment analysis of a document will help the user comprehend the content's emotional intent. Abstractive summarization algorithms generate a condensed version of the text, which can then be used to determine the emotion represented in the text using sentiment analysis. Recent research in abstractive summarization concentrates on neural network-based models, rather than conjunctions-based approaches, which might improve the overall efficiency. Neural network models like attention mechanism are tried out to handle complex works with promising results. The proposed work aims to present a novel framework that incorporates the part of speech tagging feature to the word embedding layer, which is then used as the input to the attention mechanism. With POS feature being part of the input layer, this framework is capable of dealing with words containing contextual and morphological information. The relevance of POS tagging here is due to its strong reliance on the language's syntactic, contextual, and morphological information. The three main elements in the work are pre-processing, POS tagging feature in the embedding phase, and the incorporation of it into the attention mechanism. The word embedding provides the semantic concept about the word, while the POS tags give an idea about how significant the words are in the context of the content, which corresponds to the syntactic information. The proposed work was carried out in Malayalam, one of the prominent Indian languages. A widely used and accepted dataset from the English language was translated to Malayalam for conducting the experiments. The proposed framework gives a ROUGE score of 28, which outperformed the baseline models.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesAbstractive summarization; attention mechanism; encoder decoder; Malayalam language; POS tagging,Computational linguistics; Embeddings; Semantics; Signal encoding; Syntactics; Additional key word and phrasesabstractive summarization; Attention mechanisms; Contextual information; Embeddings; Encoder-decoder; Key words; Malayalam language; Malayalams; POS tagging; Sentiment analysis; Sentiment analysis
A Comparative Study of Speaker Role Identification in Air Traffic Communication Using Deep Learning Approaches,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160936469&doi=10.1145%2f3572792&partnerID=40&md5=dba831fc414e8adc4cbe86a001d44cc3,"Automatic spoken instruction understanding (SIU) of the controller-pilot conversations in the air traffic control (ATC) requires not only recognizing the words and semantics of the speech but also determining the role of the speaker. However, few of the published works on the automatic understanding systems in air traffic communication focus on speaker role identification (SRI). In this article, we formulate the SRI task of controller-pilot communication as a binary classification problem. Furthermore, the text-based, speech-based, and speech-and-text-based multi-modal methods are proposed to achieve a comprehensive comparison of the SRI task. To ablate the impacts of the comparative approaches, various advanced neural network architectures are applied to optimize the implementation of text-based and speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to achieve the SRI task by considering both the speech and textual modality features. To aggregate modality features, the modal fusion module is proposed to fuse and squeeze acoustic and textual representations by modal attention mechanism and self-attention pooling layer, respectively. Finally, the comparative approaches are validated on the ATCSpeech corpus collected from a real-world ATC environment. The experimental results demonstrate that all the comparative approaches worked for the SRI task, and the proposed MMSRINet shows competitive performance and robustness compared with the other methods on both seen and unseen data, achieving 98.56% and 98.08% accuracy, respectively.  © 2023 Association for Computing Machinery.",air traffic control; multi-modal learning; Speaker role identification; speech classification; spoken instruction understanding; text classification,Air navigation; Air traffic control; Classification (of information); Control towers; Deep learning; Network architecture; Neural networks; Semantics; Speech communication; Speech recognition; Air traffics; Comparative approach; Multi-modal; Multi-modal learning; Role identification; Speaker role identification; Speech classification; Spoken instruction understanding; Text classification; Traffic communication; Controllers
Modelling of Speech Parameters of Punjabi by Pre-trained Deep Neural Network Using Stacked Denoising Autoencoders,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160292608&doi=10.1145%2f3568308&partnerID=40&md5=daf1010618beeb9b37d1d5b5994da598,"Statistical parametric speech synthesis techniques such as deep neural network (DNN) and hidden Markov model (HMM) have grown in popularity since last decade over the concatenative speech synthesis approaches by modelling excitation and spectral parameters of speech to synthesize the waveforms from the written text. Due to inappropriate acoustic modelling, speech synthesized using HMM-based speech synthesis sounds muffled. DNN tried to improve the acoustic model by replacing decision trees in HMM with powerful regression model. Further, the performance of a deep neural network is greatly enhanced using pre-learning either restricted Boltzmann machines (RBM) or autoencoders. RBMs are capable to map multi-modal property of speech but result in spectral distortion of synthesized speech waveforms as non-consideration of reconstruction error. This article proposed the model of deep neural network, which is pre-trained using stacked denoising autoencoders to map speech parameters of the Punjabi language. Denoising autoencoders work by adding noise in the training data and then reconstructing the original measurements to reduce the reconstruction error. The synthesized voice using the proposed model showed the VARN of 0.82, F0 RMSE (Hz) 9.03, and V/UV error rate of 4.04% have been observed.  © 2023 Association for Computing Machinery.",Pre-training deep neural network; reconstruction error; restricted Boltzmann machines; stacked denoising auto-encoders,Decision trees; Errors; Hidden Markov models; Learning systems; Regression analysis; Speech synthesis; Acoustics model; Auto encoders; De-noising; Hidden-Markov models; Pre-training; Pre-training deep neural network; Reconstruction error; Restricted boltzmann machine; Stacked denoising auto-encoder; Synthesised; Deep neural networks
Target-Oriented Knowledge Distillation with Language-Family-Based Grouping for Multilingual NMT,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152623737&doi=10.1145%2f3546067&partnerID=40&md5=f9f864f5fc7b9898275feeb8c289988f,"Multilingual NMT has developed rapidly, but still has performance degradation caused by language diversity and model capacity constraints. To achieve the competitive accuracy of multilingual translation despite such limitations, knowledge distillation, which improves the student network by matching the teacher network's output, has been applied and shown enhancement by focusing on the important parts of the teacher distribution. However, existing knowledge distillation methods for multilingual NMT rarely consider the knowledge, which has an important function as the student model's target, in the process. In this article, we propose two distillation strategies that effectively use the knowledge to improve the accuracy of multilingual NMT. First, we introduce a language-family-based approach, guiding to select appropriate knowledge for each language pair. By distilling the knowledge of multilingual teachers that each processes a group of languages classified by language families, the multilingual model overcomes accuracy degradation caused by linguistic diversity. Second, we propose target-oriented knowledge distillation, which intensively focuses on the ground-Truth target of knowledge with a penalty strategy. Our method provides a sensible distillation by penalizing samples without actual targets, while additionally targeting the ground-Truth targets. Experiments using TED Talk datasets demonstrate the effectiveness of our method with BLEU scores increment. Discussions of distilled knowledge and further observations of the methods also validate our results.  © 2023 Association for Computing Machinery.",knowledge distillation; language family; Multilingual neural machine translation; neural machine translation,Computational linguistics; Computer aided language translation; Neural machine translation; Capacity constraints; Ground truth; Knowledge distillation; Language family; Multilingual neural machine translation; Multilingual translations; Performance degradation; Student network; Target oriented; Teachers'; Distillation
Improved Heuristic Data Management and Protection Algorithm for Digital China Cultural Datasets,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151825962&doi=10.1145%2f3394114&partnerID=40&md5=a1c03e491d97aa97077977982fdb8dcb,"In the present scenario sustainable management and protection of digital cultural datasets are considered as a significant area of research. In the recent past, the protection and management of cultural data are facing several new challenges and opportunities. Though several researchers explored their work on managing and protecting cultural data, efficiently and reliability of the present data management algorithm seems to be more complicated due to its incompetence in managing data in an optimized manner. This work presents an improved heuristic big data management algorithm for cultural datasets which is considered as a new discipline of digital cultural heritage specially established for strengthening strategic and interdisciplinary research. The scientific operation and management mechanism of digital protection of cultural heritage is experimentally validated and results show promising outcomes.  © 2023 Association for Computing Machinery.",China cultural datasets; digital data; heuristic data management algorithm; interdisciplinary,China cultural dataset; Digital cultural heritages; Digital datas; Heuristic data management algorithm; Interdisciplinary; Operation and management; Operation mechanism; Protection algorithms; Protection and management; Sustainable management; Information management
Transfer Learning for Low-Resource Multilingual Relation Classification,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152619927&doi=10.1145%2f3554734&partnerID=40&md5=0499e9b448d9422143b399057802c488,"Relation classification (sometimes called relation extraction) requires trustworthy datasets for fine-Tuning large language models, as well as for evaluation. Data collection is challenging for Indian languages, because they are syntactically and morphologically diverse, as well as different from resource-rich languages like English. Despite recent interest in deep generative models for Indian languages, relation classification is still not well served by public datasets. In response, we present IndoRE, a dataset with 21K entity-And relation-Tagged gold sentences in three Indian languages (Bengali, Hindi, and Telugu), plus English. We start with a multilingual BERT (mBERT)-based system that captures entity span positions and type information, and provides competitive performance on monolingual relation classification. Using this baseline system, we explore transfer mechanisms between languages and the scope to reduce expensive data annotation while achieving reasonable relation extraction performance. Specifically, we(a)study the accuracy-efficiency trade-off between expensive, manually labeled gold instances vs. automatically translated and aligned silver instances to train a relation extractor,(b)device a simple mechanism for budgeted gold data annotation by intelligently converting distant-supervised silver training instances to gold training instances with human annotators using active learning, and finally(c)propose an ensemble model to provide a performance boost over that achieved via limited gold training instances. We release the dataset for future research.1  © 2023 Association for Computing Machinery.",Relation extraction,Budget control; Classification (of information); Economic and social effects; Extraction; Gold; Large dataset; Learning systems; Petroleum reservoir evaluation; Transfer learning; Data annotation; Data collection; Fine tuning; Indian languages; Language model; Performance; Relation classifications; Relation extraction; Resource-Rich; Transfer learning; Silver
Deep Neural Network with Embedding Fusion for Chinese Named Entity Recognition,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160325702&doi=10.1145%2f3570328&partnerID=40&md5=03a6bbe7be6cf4532625295677e65a29,"Chinese Named Entity Recognition (NER) is an essential task in natural language processing, and its performance directly impacts the downstream tasks. The main challenges in Chinese NER are the high dependence of named entities on context and the lack of word boundary information. Therefore, how to integrate relevant knowledge into the corresponding entity has become the primary task for Chinese NER. Both the lattice LSTM model and the WC-LSTM model did not make excellent use of contextual information. Additionally, the lattice LSTM model had a complex structure and did not exploit the word information well. To address the preceding problems, we propose a Chinese NER method based on the deep neural network with multiple ways of embedding fusion. First, we use a convolutional neural network to combine the contextual information of the input sequence and apply a self-attention mechanism to integrate lexicon knowledge, compensating for the lack of word boundaries. The word feature, context feature, bigram feature, and bigram context feature are obtained for each character. Second, four different features are used to fuse information at the embedding layer. As a result, four different word embeddings are obtained through cascading. Last, the fused feature information is input to the encoding and decoding layer. Experiments on three datasets show that our model can effectively improve the performance of Chinese NER.  © 2023 Association for Computing Machinery.",Chinese Named Entity Recognition; deep neural network; natural language processing; self-attention mechanism,Deep neural networks; Long short-term memory; Natural language processing systems; Attention mechanisms; Chinese named entity recognition; Context features; Contextual information; Embeddings; Language processing; Natural language processing; Natural languages; Performance; Self-attention mechanism; Embeddings
Identification and Extraction of Features from Malayalam Poems for Analyzing Syllable Duration Patterns,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152889567&doi=10.1145%2f3561298&partnerID=40&md5=e17a6992aed06480956951b0eea95cbb,"Text-to-speech (TTS) synthesis is an active area of research to generate synthetic speech from the underlying text. Compared to English and many European languages, TTS is yet to mature in Malayalam, the principal language of the South Indian state of Kerala. A syllable has to be uttered with proper durational and prosodic characteristics to emulate natural speech. When it comes to poems in Malayalam, many of them have an inherent rhythm attached to them. In Malayalam, this property is characterized by the Vruta [28] in which the poem is written. Vruta decides the meter of narration of the poem. Therefore, it is only consequential that Vruta can give away vital cues about the durational and prosodic characteristics of the poem verses recited. This study intends to identify the features that determine the durational characteristics of a poem written in a particular Vruta and develop an algorithm to extract those features required to build a dataset to model the duration of syllable utterances for tuneful TTS in Malayalam. Poems written in three Vrutas, namely Kakali, Manjari, and Keka, are considered in this study. Nineteen extractible features from the orthographic representation of a poem are identified for this purpose. A standard dataset is built using these extracted features. Later, support vector machine and feed forward neural network based estimators are proposed to model the duration of Malayalam poem syllables for tuneful speech synthesis. The hyperparameters are optimized using the GridsearchCV algorithm from the Scikit-learn machine learning library [15].  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesFeature extraction; duration analysis; duration of Malayalam poems; text-to-speech synthesis in Malayalam,Feedforward neural networks; Speech synthesis; Support vector machines; Additional key word and phrasesfeature extraction; Duration analyse; Duration of malayalam poem; Duration patterns; Key words; Malayalams; Prosodics; Text to speech; Text-to-speech synthesis in malayalam; Extraction
An Effective Learning Evaluation Method Based on Text Data with Real-time Attribution - A Case Study for Mathematical Class with Students of Junior Middle School in China,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136281435&doi=10.1145%2f3474367&partnerID=40&md5=7ae6ce8eaed54288d1ea58c672cf4034,"In today's intelligent age, the vigorous development of education-based information analysis technology has had a profound impact on the education and teaching process. The use of computational linguistics technology to extract teaching data for learning evaluation is an important hot domain in this research field. Therefore, the study of student learning assessment methods based on text data has become a key issue. The text data extracted from the education process has attributes related to time and operational attributes, which are important indicators to measure the effect of student learning effect. However, these attributes are not focused by the traditional educational effect evaluation method, which make the learning effect of students difficult to measure comprehensively and effectively. In response to this problem, this article first uses perception technology to extract learning text data based on time and operational attributes. Secondly, according to the real-time attributes of text data, such as time and operation attributes, a learning evaluation method based on real-time text data is proposed. Finally, this article compares the traditional evaluation method with the proposed method. The results show that using real-time attribute text data is more effective in students' learning measure.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesText data; educational technology; K-12; learning effectiveness; real time,Data mining; Learning systems; Additional key word and phrasestext data; Education process; Evaluation methods; K-12; Key words; Learning effectiveness; Learning evaluations; Real- time; Student learning; Text data; Students
An Intelligent Telugu Handwritten Character Recognition Using Multi-Objective Mayfly Optimization with Deep Learning-Based DenseNet Model,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160359525&doi=10.1145%2f3520439&partnerID=40&md5=05b3004ab6efec702aab4cfebd7b35f2,"The handwritten character recognition process has gained significant attention among research communities due to its application in assistive technologies for visually impaired people, human-robot interaction, automated registry for business documents, and so on. Handwritten character recognition of the Telugu language is difficult owing to the absence of a massive dataset and a trained convolutional neural network (CNN). This article introduces an intelligent Telugu character recognition process using a multi-objective mayfly optimization with deep learning (MOMFO-DL) model. The proposed MOMFO-DL technique involves the DenseNet-169 model as a feature extractor to generate a useful set of feature vectors. A functional link neural network (FLNN) is used as a classification model to recognize and classify the printer characters. The design of the MOMFO technique for the parameter optimization of the DenseNet model and FLNN model shows the novelty of the work. The use of MOMFO technique helps to optimally tune the parameters in such a way that the overall performance can be improved. The extensive experimental analysis takes place on benchmark datasets and the outcomes are examined with respect to different measures. The experimental results pointed out the supremacy of the MOMFO technique over the recent state-of-the-art methods.  © 2023 Association for Computing Machinery.",convolutional neural networks; deep learning; DenseNet; handwritten character recognition; machine learning; mayfly optimization algorithm; parameter tuning; Telugu language,Convolution; Convolutional neural networks; Deep neural networks; Human robot interaction; Learning systems; Multiobjective optimization; Convolutional neural network; Deep learning; Densenet; Handwritten character recognition; Machine-learning; Mayfly optimization algorithm; Multi objective; Optimization algorithms; Parameters tuning; Telugu language; Character recognition
Toward Explainable Dialogue System Using Two-stage Response Generation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152785058&doi=10.1145%2f3551869&partnerID=40&md5=10936a977aa465c1c67b8bb8f4f71cf9,"In recent years, neural networks have achieved impressive performance on dialogue response generation. However, most of these models still suffer from some shortcomings, such as yielding uninformative responses and lacking explainable ability. This article proposes a Two-stage Dialogue Response Generation model (TSRG), which specifies a method to generate diverse and informative responses based on an interpretable procedure between stages. TSRG involves a two-stage framework that generates a candidate response first and then instantiates it as the final response. The positional information and a resident token are injected into the candidate response to stabilize the multi-stage framework, alleviating the shortcomings in the multi-stage framework. Additionally, TSRG allows adjusting and interpreting the interaction pattern between the two generation stages, making the generation response somewhat explainable and controllable. We evaluate the proposed model on three dialogue datasets that contain millions of single-turn message-response pairs between web users. The results show that, compared with the previous multi-stage dialogue generation models, TSRG can produce more diverse and informative responses and maintain fluency and relevance.  © 2023 Association for Computing Machinery.",Asian language; explainable text generation; Natural language processing; neural response generation,Natural language processing systems; Asian languages; Explainable text generation; Language processing; Multi-stages; Natural language processing; Natural languages; Neural response; Neural response generation; Response generation; Text generations; Speech processing
Analyzing Variations of Everyday Japanese Conversations Based on Semantic Labels of Functional Expressions,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150860728&doi=10.1145%2f3552310&partnerID=40&md5=fbb0e716c29f51b45bde82ef03fb371d,"To achieve effective dialogue processing, the kinds of daily conversations people have must be clarified. Unfortunately, the characteristics of everyday conversations remain insufficiently investigated. In recent years, the Corpus of Everyday Japanese Conversation (CEJC) was developed, which is a large-scale corpus constructed by recording everyday Japanese conversations. In this article, we investigate the linguistic variations of everyday conversations in a multitude of situations using CEJC. We conducted factor analysis of it using the semantic categories of functional expressions that represent such subjective information as modality, thoughts, and communicative intention in addition to various tenses and facts. Our analysis identified seven factors that characterize everyday conversations and suggests that they are expressed by a combination of a dialogue's purpose (e.g., ""Explanation""and ""Suggestion"") and its manners (e.g., ""Politeness""and ""Involvement""). We also analyzed the BTSJ-Japanese natural conversation corpus with transcripts and recordings and the Nagoya University conversational corpus and confirmed the generalizability of these factors.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEveryday conversation; dialogue processing; factor analysis; semantic labels of functional expression,Multivariant analysis; Semantics; Additional key word and phraseseveryday conversation; Conducted factors; Dialogue processing; Factors analysis; Functional expression; Key words; Large-scales; Semantic category; Semantic label of functional expression; Semantic labels; Factor analysis
Morphologically Motivated Input Variations and Data Augmentation in Turkish-English Neural Machine Translation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160434783&doi=10.1145%2f3571073&partnerID=40&md5=72fe287c4b23884e743354e498f7e10c,"Success of neural networks in natural language processing has paved the way for neural machine translation (NMT), which rapidly became the mainstream approach in machine translation. Significant improvement in translation performance has been achieved with breakthroughs such as encoder-decoder networks, attention mechanism, and Transformer architecture. However, the necessity of large amounts of parallel data for training an NMT system and rare words in translation corpora are issues yet to be overcome. In this article, we approach NMT of the low-resource Turkish-English language pair. We employ state-of-the-art NMT architectures and data augmentation methods that exploit monolingual corpora. We point out the importance of input representation for the morphologically rich Turkish language and make a comprehensive analysis of linguistically and non-linguistically motivated input segmentation approaches. We prove the effectiveness of morphologically motivated input segmentation for the Turkish language. Moreover, we show the superiority of the Transformer architecture over attentional encoder-decoder models for the Turkish-English language pair. Among the employed data augmentation approaches, we observe back-translation to be the most effective and confirm the benefit of increasing the amount of parallel data on translation quality. This research demonstrates a comprehensive analysis on NMT architectures with different hyperparameters, data augmentation methods, and input representation techniques, and proposes ways of tackling the low-resource setting of Turkish-English NMT. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesNeural machine translation; attention; data augmentation; encoder-decoder; low-resource; morphology; Transformer; word segmentation,Computational linguistics; Computer aided language translation; Decoding; Natural language processing systems; Network architecture; Neural machine translation; Signal encoding; Additional key word and phrasesneural machine translation; Attention; Data augmentation; Encoder-decoder; Key words; Low-resource; Machine translations; Transformer; Turkishs; Word segmentation; Morphology
Multilingual News Feed Analysis using Intelligent Linguistic Particle Filtering Techniques,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160327925&doi=10.1145%2f3569899&partnerID=40&md5=2e218ffc0d5e6b5886335792ed389173,"Analyzing real-time news feeds and their impacts in the real world is a complex task in the social networking arena. Particularly, countries with a multilingual environment have various patterns and perceptions of news reports considering the diversity of the people. Multilingual and multimodal news analysis is an emerging trend for evaluating news source neutralities. Therefore, in this work, four new deep news particle filtering techniques were developed, including generic news analysis, sequential importance re-sampling (SIR)-based news particle filtering analysis, reinforcement learning (RL)-based multimodal news analysis, and deep Convolution neural network (DCNN)-based multi-news filtering approach, for news classification. Results indicate that these techniques, which primarily employ particle filtering with multilevel sampling strategies, produce 15% to 20% better performance than conventional news analysis techniques.  © 2023 Association for Computing Machinery.",artificial intelligence; Machine learning and deep learning; news feeds; particle filtering; sampling,Classification (of information); Importance sampling; Modal analysis; Reinforcement learning; Complex task; Filtering technique; Machine learning and deep learning; Machine-learning; Multi-modal; News feeds; Particle Filtering; Real- time; Real-world; Social-networking; Deep learning
Surface Realization Architecture for Low-resourced African Languages,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160280747&doi=10.1145%2f3567594&partnerID=40&md5=056a2b4835015a22e5cad4f491905782,"There has been growing interest in building surface realization systems to support the automatic generation of text in African languages. Such tools focus on converting abstract representations of meaning to a text. Since African languages are low-resourced, economical use of resources and general maintainability are key considerations. However, there is no existing surface realizer architecture that possesses most of the maintainability characteristics (e.g., modularity, reusability, and analyzability) that will lead to maintainable software that can be used for the languages. Moreover, there is no consensus surface realization architecture created for other languages that can be adapted for the languages in question. In this work, we solve this by creating a novel surface realizer architecture suitable for low-resourced African languages that abides by the features of maintainable software. Its design comes after a granular analysis, classification, and comparison of the architectures used by 77 existing NLG systems. We compare our architecture to existing architectures and show that it supports the most features of a maintainable software product.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",low-resourced languages; Natural language generation; software architecture; surface realisation,Abstracting; Computer software reusability; Maintainability; Natural language processing systems; Software architecture; Abstract representation; African languages; Automatic Generation; Building surface; Granular analysis; In-buildings; Low-resourced language; Natural language generation; Novel surfaces; Surface realization; Reusability
Topic-Based Unsupervised and Supervised Dictionary Induction,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153001344&doi=10.1145%2f3564698&partnerID=40&md5=126b261162ae29863749c0a7738bf0b3,"Word translation is a natural language processing task that provides translation between the words of a source and a target language. As a task, it reduces to the induction of a bilingual dictionary, which is typically performed by aligning word embeddings of the source language to word embeddings of the target language. To date, all the existing approaches have focused on performing a single, global alignment in word embedding space. However, semantic differences between the various languages, in addition to differences in the content of the corpora used for training the word embeddings, can hinder the effectiveness of such a global alignment. For this reason, in this article we propose conducting the alignment between the source and target embedding spaces by multiple mappings at topic level. The experimental results show that our approach has been able to achieve an average accuracy improvement of +3.30 percentage points over a state-of-the-art approach in unsupervised dictionary induction from languages as diverse as German, French, Italian, Spanish, Finnish, Turkish, and Chinese to English, and +3.95 points average improvement in supervised dictionary induction.  © 2023 Association for Computing Machinery.",dictionary induction; topic modeling; topic-based dictionary induction; word embedding alignment; Word translation,Natural language processing systems; Semantics; Translation (languages); Dictionary induction; Embeddings; Global alignment; Language processing; Natural languages; Target language; Topic Modeling; Topic-based dictionary induction; Word embedding alignment; Word translation; Embeddings
Developing a Large Benchmark Corpus for Urdu Semantic Word Similarity,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160300139&doi=10.1145%2f3566124&partnerID=40&md5=aaefe18527569741a4c0d53d3df52a42,"The semantic word similarity task aims to quantify the degree of similarity between a pair of words. In literature, efforts have been made to create standard evaluation resources to develop, evaluate, and compare various methods for semantic word similarity. The majority of these efforts focused on English and some other languages. However, the problem of semantic word similarity has not been thoroughly explored for South Asian languages, particularly Urdu. To fill this gap, this study presents a large benchmark corpus of 518 word pairs for the Urdu semantic word similarity task, which were manually annotated by 12 annotators. To demonstrate how our proposed corpus can be used for the development and evaluation of Urdu semantic word similarity systems, we applied two state-of-the-art methods: (1) a word embedding-based method and (2) a Sentence Transformer-based method. As another major contribution, we proposed a feature fusion method based on Sentence Transformers and word embedding methods. The best results were obtained using our proposed feature fusion method (the combination of best features of both methods) with a Pearson correlation score of 0.67. To foster research in Urdu (an under-resourced language), our proposed corpus will be free and publicly available for research purposes.  © 2023 Association for Computing Machinery.",corpus; cross-lingual sentence transformer; English; Semantic word similarity; Urdu; word embedding,Correlation methods; Embeddings; An under-resourced language; Corpus; Cross-lingual; Cross-lingual sentence transformer; Embeddings; English; Semantic word similarity; Under-resourced languages; Word embedding; Word similarity; Semantics
Khmer Sentiment Lexicon Based on PU Learning and Label Propagation Algorithm,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160363035&doi=10.1145%2f3564697&partnerID=40&md5=8c05e545f916a9426635802a3626123a,"The sentiment lexicon is an important tool for natural language processing tasks. In addition to being able to determine the sentiment polarity of words or phrases, it can assist attribute-level, sentence-level, and text-level sentiment analysis tasks. In light of the fact that tagging data and corpora for the Khmer language are scarce, where most resources related to sentiment lexicons are for English, this paper proposes a method for constructing a sentiment lexicon for Khmer based on Positive-Unlabeled learning (PU Learning) and the label propagation algorithm. Sentiment words are first extracted from a corpus using the Spy technique of PU learning method. The main idea is to purify the set of N-class examples, train the MLP classifier, and continuously delete spy words and increase the number of P-class words in the iterative process. Following this, the sentiment polarity of the candidate words is determined. By considering the problem of determining the sentiment polarity of the candidate words as one of calculating its probability distribution, a small number of labeled sentiment words and candidate words are used to construct a graph model. The contextual information of the candidate words is used to construct a simple supplementary graph model of the set of sentiment words through word co-occurrence and triangulation, where this enhances the correlation between data items. The sentiment polarity of the candidate words is then determined through the label propagation algorithm. The results of experiments show that the proposed method can be used to construct a Khmer sentiment lexicon with a small number of labeled data and a small corpus without requiring excessive manual labeling.  © 2023 Association for Computing Machinery.",graph model; Khmer sentiment lexicon; label propagation algorithm; PU learning; Spy technique,Backpropagation; Graph theory; Iterative methods; Learning systems; Probability distributions; Graph model; Khmer sentiment lexicon; Label propagation; Label propagation algorithm; Lexicon-based; Natural languages; Propagation algorithm; PU learning; Sentiment lexicons; Spy technique; Sentiment analysis
Building a Closed-Domain Question Answering System for a Low-Resource Language,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160291454&doi=10.1145%2f3566123&partnerID=40&md5=bfe76d7881a856073bc2b5695749887a,"In recent years, the Question Answering System (QAS) has been widely used to develop many systems, such as conversation systems, chatbots, and intelligent search. Depending on the amount of information or knowledge that the system processes, the system can be applied in answering the questions in an open domain or closed domain. There are many approaches to solving the QA problem, but the neural network models have yielded impressive and promising results, especially the Machine Reading Comprehension approach. In this article, we build a closed-domain QAS for a low-resource language, Vietnamese - specifically, ""The Postgraduate Admission of Ho Chi Minh City University of Food Industry, Vietnam.""In addition, we have created two datasets to serve our QAS: vi-SQuAD v1.1, which is automatically translated and edited from SQuAD (Stanford University Question Answering Dataset), and HUFI-PostGrad, which is manually collected. We use two main models for the system, including the Intent Classification model and the Machine Reading Comprehension model. Experimental results initially show that our QAS gives encouraging results.  © 2023 Association for Computing Machinery.",BERT; intent classification; machine reading comprehension; Question answering; SQuAD dataset,Classification (of information); Natural language processing systems; Search engines; BERT; Intent classification; Low resource languages; Machine reading comprehension; Question Answering; Question answering systems; Reading comprehension; Stanford University; Stanford university question answering dataset dataset; Neural network models
BSML: Bidirectional Sampling Aggregation-based Metric Learning for Low-resource Uyghur Few-shot Speaker Verification,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160320011&doi=10.1145%2f3564782&partnerID=40&md5=b130ba1395827d9afffb01493e277c16,"In recent years, text-independent speaker verification has remained a hot research topic, especially for the limited enrollment and/or test data. At the same time, due to the lack of sufficient training data, the study of low-resource few-shot speaker verification makes the models prone to overfitting and low accuracy of recognition. Therefore, a bidirectional sampling aggregation-based meta-metric learning method is proposed to solve the low-accuracy problem of speaker recognition in a low-resource environment with limited data, termed bidirectional sampling multi-scale Fisher feature fusion (BSML). First, the BSML method was used for effective feature enhancement in the feature extraction stage; second, a large number of similar and disjoint tasks were used to train the models to learn how to compare sample similarity; finally, new tasks were used to identify unknown samples by calculating the similarity of the samples. Extensive experiments are conducted on a short-duration text-independent speaker verification dataset generated from the THUYG-20 low-resource Uyghur with limited data, which comprised speech samples of diverse lengths. The experimental result has shown that the metric learning approach is effective in avoiding model overfitting and improving model generalization, with significant results in the identification of short-duration speaker verification in low-resource Uyghur with few-shot. It also demonstrates that BSML outperforms the state-of-the-art deep-embedding speaker recognition architectures and recent metric learning approach by at least 18%-67% in the few-shot test set. The ablation experiments further illustrate that our proposed approaches can achieve substantial improvement over prior methods and achieves better performance and generalization ability.  © 2023 Association for Computing Machinery.",bidirectional sampling; few-shot; limited data; low-resource language; metric learning; speaker verification; Uyghur,Deep learning; Learning systems; Bidirectional sampling; Few-shot; Limited data; Low resource languages; Metric learning; Overfitting; Sampling aggregations; Speaker verification; Text-independent speaker verification; Uyghur; Speech recognition
On the Effectiveness of Images in Multi-modal Text Classification: An Annotation Study,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160279983&doi=10.1145%2f3565572&partnerID=40&md5=2f9187e093e0f2869274899f9a37e980,"Combining different input modalities beyond text is a key challenge for natural language processing. Previous work has been inconclusive as to the true utility of images as a supplementary information source for text classification tasks, motivating this large-scale human study of labelling performance given text-only, images-only, or both text and images. To this end, we create a new dataset accompanied with a novel annotation method - Japanese Entity Labeling with Dynamic Annotation - to deepen our understanding of the effectiveness of images for multi-modal text classification. By performing careful comparative analysis of human performance and the performance of state-of-the-art multi-modal text classification models, we gain valuable insights into differences between human and model performance, and the conditions under which images are beneficial for text classification.  © 2023 Association for Computing Machinery.",Datasets; multi-modality; natural language processing; neural networks; text classification,Image annotation; Natural language processing systems; Text processing; Dataset; Labelings; Language processing; Multi-modal; Multi-modality; Natural language processing; Natural languages; Neural-networks; Performance; Text classification; Classification (of information)
SPK-CG: Siamese Network based Posterior Knowledge Selection Model for Knowledge Driven Conversation Generation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160355083&doi=10.1145%2f3569579&partnerID=40&md5=9d9fb458b31c107633da04bd4ef04b47,"Building a human-computer conversational system that can communicate with humans is a research hotspot in the field of artificial intelligence. Traditional dialogue systems tend to produce irrelevant and non-information responses, which reduce people's interest in engaging in a conversation. This often leads to boring conversations. To alleviate this problem, many researchers use external knowledge to assist conversation generation. The accuracy of knowledge selection is the prerequisite to ensure the quality of knowledge conversation. This approach has worked positively to a certain extent, but generally only searches knowledge information based on entity words themselves, without considering the specific conversation context. Therefore, if irrelevant knowledge is retrieved, the quality of conversation generation will be reduced. Motivated by this, we propose a novel neural knowledge-based conversation generation model, named Siamese Network based Posterior Knowledge Selection Model for Knowledge Driven Conversation Generation (SPK-CG). We have designed a novel knowledge selection mechanism to obtain knowledge information that is highly relevant to the context of the conversation. Specifically, the posterior knowledge distribution is used as a soft label to make the prior distribution consistent with the posterior distribution in the training process. At the same time, in order to narrow the gap between prior and posterior distributions and improve the accuracy of knowledge selection, we leverage siamese network and design multi-granularity matching module for knowledge selection. Compared with previous knowledge-based models, our method can select more appropriate knowledge and use the selected knowledge to generate responses that are more relevant to the conversation context. Extensive automatic and human evaluations demonstrate that our model has advantages over previous baselines.  © 2023 Association for Computing Machinery.",conversation generation; Conversation system; knowledge selection; Siamese network,Knowledge based systems; Speech processing; Conversation generation; Conversation systems; Conversational systems; Knowledge information; Knowledge selection; Network-based; Posterior distributions; Prior distribution; Selection model; Siamese network; Knowledge management
Integrating Heterogeneous Ontologies in Asian Languages Through Compact Genetic Algorithm with Annealing Re-sample Inheritance Mechanism,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160306936&doi=10.1145%2f3519298&partnerID=40&md5=2f1a460a0477851d1e427561b287afbc,"An ontology is a state-of-the-art knowledge modeling technique in the natural language domain, which has been widely used to overcome the linguistic barriers in Asian and European countries' intelligent applications. However, due to the different knowledge backgrounds of ontology developers, the entities in the ontologies could be defined in different ways, which hamper the communications among the intelligent applications built on them. How to find the semantic relationships among the entities that are lexicalized in different languages is called the Cross-lingual Ontology Matching problem (COM), which is a challenge problem in the ontology matching domain. To face this challenge, being inspired by the success of the Genetic Algorithm (GA) in the ontology matching domain, this work proposes a Compact GA with Annealing Re-sample Inheritance mechanism (CGA-ARI) to efficiently address the COM problem. In particular, a Cross-lingual Similarity Metric (CSM) is presented to distinguish two cross-lingual entities, a discrete optimal model is built to define the COM problem, and the compact encoding mechanism and the Annealing Re-sample Inheritance mechanism (ARI) are introduced to improve CGA's searching performance. The experiment uses Multifarm track to test CGA-ARI's performance, which includes 45 ontology pairs in different languages. The experimental results show that CGA-ARI is able to significantly improve the performance of GA and CGA and determine better alignments than state-of-the-art ontology matching systems.  © 2023 Association for Computing Machinery.",Annealing Re-sample Inheritance Mechanism; compact genetic algorithm; Cross-lingual ontology alignment,Annealing; Genetic algorithms; Modeling languages; Semantics; Annealing re-sample inheritance mechanism; Compact genetic algorithm; Cross-lingual; Cross-lingual ontology alignment; Inheritance mechanisms; Matching problems; Ontology alignment; Ontology matching; Ontology's; State of the art; Ontology
Unsupervised Parallel Sentences of Machine Translation for Asian Language Pairs,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160209686&doi=10.1145%2f3486677&partnerID=40&md5=09c608536382cc853c2c9d7cf1f92956,"Parallel sentence pairs play a very important role in many natural language processing tasks, especially cross-lingual tasks such as machine translation. So far, many Asian language pairs lack bilingual parallel sentences. As collecting bilingual parallel data is very time-consuming and difficult, it is very important for many low-resource Asian language pairs. While existing methods have shown encouraging results, they rely on bilingual data seriously or have some drawbacks in an unsupervised situation. To address these issues, we propose a new unsupervised similarity calculation and dynamic selection metric to obtain parallel sentence pairs in an unsupervised situation. First, our method maps bilingual word embedding by postdoc adversarial training, which rotates the source space to match the target without parallel data. Then, we introduce a new cross-domain similarity adaption to obtain parallel sentence pairs. Experimental results on real-world datasets show that our model can obtain better accuracy and recall on mining parallel sentence pairs. We also show that the extracted bilingual sentence corpora can significantly improve the performance of neural machine translation.  © 2023 Association for Computing Machinery.",adversarial training; machine translation; Parallel data; unsupervised method,Computer aided language translation; Natural language processing systems; Neural machine translation; Adversarial training; Asian languages; Bilinguals; Cross-lingual; Language pairs; Language processing; Machine translations; Natural languages; Parallel data; Unsupervised method; Computational linguistics
Punctuation Prediction in Bangla Text,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160400516&doi=10.1145%2f3575804&partnerID=40&md5=359fa4927d77d129bea402a2c7baed59,"Punctuation prediction is critical as it can enhance the readability of machine-transcribed speeches or texts significantly by adding appropriate punctuation. Furthermore, systems like Automatic Speech Recognizer (ASR) produce texts that are unpunctuated, making the readability difficult for humans and also hampers the performance of various natural language processing (NLP) tasks. Such NLP related tasks have been investigated thoroughly for English; however, very limited work is done for punctuation prediction in the Bangla language. In this study, we train a bidirectional recurrent neural network (BRNN) along with Attention model with a plausibly large Bangla dataset. Afterwards, we apply extensive postprocessing techniques for predicting punctuation more accurately with the employed model. Initially, we perform experimentation with a relatively imbalanced dataset, and our model shows promising results F1=56.9 for Period) in punctuation prediction. Later, we also investigate the model's performance using a balanced Bangla dataset to achieve higher performance scores (F1=62.2 for Question). Thus, the goal of this study is to propose an efficient approach that can predict punctuation in Bangla texts effectively. Our study also includes investigation on how our postprocessing techniques affect the prediction performance. Being an early attempt for the punctuation prediction in Bangla text, our work is expected to significantly contribute in the NLP field for the Bangla language, and will pave the way for future work with the Bangla language in this direction. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesNeural networks; BRNN; natural language processing; punctuation prediction,Large dataset; Natural language processing systems; Recurrent neural networks; Speech recognition; Additional key word and phrasesneural network; Automatic speech recognizers; Bidirectional recurrent neural networks; Key words; Language processing; Natural language processing; Natural languages; Performance; Post-processing techniques; Punctuation prediction; Forecasting
A Decision Model for Ranking Asian Higher Education Institutes Using an NLP-Based Text Analysis Approach,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156188686&doi=10.1145%2f3534562&partnerID=40&md5=bb4f847a5a53ec868893075f62ab36cd,"Identification of the best institute for higher education has become one of the most challenging issues in the present education system. It has become more complicated as more institutes exist with extraordinary infrastructural facilities. Therefore, a decision model is required to identify the best institute for higher education based on multiple criteria. This article proposes a Natural Language Processing (NLP) -based decision model for the identification of the best higher education institute using MCDM methods. The existing decision models for the selection of the best higher education institutions consider a limited number of criteria for decision-making. In this proposed model, 17 criteria and 15 institute datasets have been identified for the development of the decision model through extensive research and experts opinion. The NLP-based text analysis approach is applied to extract the relevant information and convert it to a suitable format. As the relative importance of the criteria plays a crucial role in decision-making, CRITIC and Rank centroid methods are applied for the calculation of relative weights of criteria. TOPSIS method is used to generate the ranking grades of alternatives for each criterion. An objective function is defined to calculate the evaluation scores and select the best institute for higher education. It has been observed that the ranks obtained from the developed model match pretty well with the ranks obtained from other MCDM methods and the experts.  © 2023 Association for Computing Machinery.",Asian institutes; CRITIC; decision model; Natural Language Processing; text analysis; TOPSIS,Natural language processing systems; Analysis approach; Asian institute; CRITIC; Decision modeling; High educations; Language processing; Natural language processing; Natural languages; Text analysis; TOPSIS; Decision making
A Span-based Target-aware Relation Model for Frame-semantic Parsing,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160352082&doi=10.1145%2f3569581&partnerID=40&md5=ca05fcb3d7c3c5b7f329636dcef55e18,"Frame-semantic Parsing (FSP) is a challenging and critical task in Natural Language Processing (NLP). Most of the existing studies decompose the FSP task into frame identification (FI) and frame semantic role labeling (FSRL) subtasks, and adopt a pipeline model architecture that clearly causes error propagation problem. However, recent jointly learning models aim to address the above problem and generally treat FSP as a span-level structured prediction task, which, unfortunately, leads to cascading error propagation problem between roles and less-efficient solutions due to huge search space of roles. To address these problems, we reformulate the FSRL task into a target-aware relation classification task and propose a novel and lightweight jointly learning framework that simultaneously processes three subtasks of FSP, including frame identification, argument identification, and role classification. The novel task formulation and jointly learning with interaction mechanisms among subtasks can help improve the overall system performance and reduce the search space and time complexity, compared with existing methods. Extensive experimental results demonstrate that our proposed model significantly outperforms 10 state-of-the-art models in terms of F1 score across two benchmark datasets.  © 2023 Association for Computing Machinery.",frame identification; frame-semantic parsing; FrameNet; relation model,Learning algorithms; Learning systems; Natural language processing systems; Error propagation; Frame identification; Frame semantics; Frame-semantic parsing; FrameNet; Relation models; Search spaces; Semantic parsing; Semantic role labeling; Subtask; Semantics
Effective College English Teaching Based on Teacher-student Interactive Model,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160336564&doi=10.1145%2f3486676&partnerID=40&md5=6a00ce36d08f4ff02c9db0375bb390e5,"English has become an utterly crucial device to take part in global verbal exchange and competition. It is essential to enhance English teaching's flexibility to meet the desires to improve the market economy. Therefore, powerful coaching strategies and language identification are considered challenging factors in existing methods. The proposed model includes hypothesized relationships among college students' conception of learning English, their perceptions of the study room environment, and their approaches to learning. They are examined using the Pre-trained Teacher-Student Fixed Interactive Model (PTSFIM). This model proposes a new way to develop the teaching process providing the baseline of record excellence towards a strategic performance control framework for an institute. The traditional strategies emphasize the benefits of the interactive approach and accentuate their effectiveness through Structural Multivariate Equation (SME) analysis in enhancing students' innovative thinking, research, and reasoning abilities. The reciprocal instructional analysis optimizes students' models to memorize for a longer duration. The evaluation of the study's outcomes suggests that interactive learning can assist college students that predict different results in participating inside the speech system and gain the best knowledge. The simulation analysis is performed based on accuracy, performance, and efficiency proves the reliability of the proposed framework.  © 2023 Association for Computing Machinery.",English; language; learning; student; teaching,Economics; Learning systems; Reliability analysis; College english teachings; College students; English; English teaching; Interactive models; Language; Learning; Market economies; Strategy identification; Teachers'; Students
Chinese Grammatical Error Correction Using Pre-trained Models and Pseudo Data,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160358334&doi=10.1145%2f3570209&partnerID=40&md5=8254edb0286ec8660125a9d3e3ab117a,"In recent studies, pre-trained models and pseudo data have been key factors in improving the performance of the English grammatical error correction (GEC) task. However, few studies have examined the role of pre-trained models and pseudo data in the Chinese GEC task. Therefore, we develop Chinese GEC models based on three pre-trained models: Chinese BERT, Chinese T5, and Chinese BART, and then incorporate these models with pseudo data to determine the best configuration for the Chinese GEC task. On the natural language processing and Chinese computing (NLPCC) 2018 GEC shared task test set, all our single models outperform the ensemble models developed by the top team of the shared task. Chinese BART achieves an F score of 37.15, which is a state-of-the-art result. We then combine our Chinese GEC models with three kinds of pseudo data: Lang-8 (MaskGEC), Wiki (MaskGEC), and Wiki (Backtranslation). We find that most models can benefit from pseudo data, and BART+Lang-8 (MaskGEC) is the ideal setting in terms of accuracy and training efficiency. The experimental results demonstrate the effectiveness of the pre-trained models and pseudo data on the Chinese GEC task and provide an easily reproducible and adaptable baseline for future works. Finally, we annotate the error types of the development data; the results show that word-level errors dominate all error types, and word selection errors must be addressed even when using pre-trained models and pseudo data. Our codes are available at https://github.com/wang136906578/BERT-encoder-ChineseGEC.  © 2023 Association for Computing Machinery.",Chinese grammatical error correction; NLP education application; pre-trained model; pseudo data,Error correction; Chinese grammatical error correction; Error correction models; Error types; Errors correction; Grammatical errors; Key factors; NLP education application; Performance; Pre-trained model; Pseudo data; Natural language processing systems
Deep Learning in Computational Linguistics for Chinese Language Translation,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160327612&doi=10.1145%2f3519386&partnerID=40&md5=9be2ca746adbe43940df8d85f91e968a,"Applying artificial intelligence to Chinese language translation in computational linguistics is of practical significance for economic boosts and cultural exchanges. In the present work, the bi-directional long short-term memory (BiLSTM) network is employed to extract Chinese text features regarding the overlapping semantic roles in Chinese language translation and hard-to-converge training of high-dimensional text word vectors in text classification during translation. In addition, AlexNet is optimized to extract the local features of the text and meanwhile update and learn network parameters in the deep network. Then, the attention mechanism is introduced to build a forecasting algorithm of Chinese language translation based on BiLSTM and improved AlexNet. Last, the forecasting algorithm is simulated to validate its performance. Some state-of-the-art algorithms are selected for a comparative experiment, including long short-term memory, regions with convolutional neural network features, AlexNet, and support vector machine. Results demonstrate that the forecasting algorithm proposed here can achieve a feature identification accuracy of 90.55%, at least an improvement of 4.24% over other algorithms. In addition, it provides an area under the curve of above 90%, a training duration of about 54.21 seconds, and a test duration of about 19.07 seconds. Regarding the performance of Chinese language translation, the algorithm proposed here provides a bilingual evaluation understudy (BLEU) value of 28.21 on the training set, with a performance gain ratio reaching 111.55%; on the test set, its BLEU reaches 40.45, with a performance gain ratio of 129.80%. Hence, this forecasting algorithm is notably superior to other algorithms, which can enhance the machine translation performance. Through experiments, the Chinese language translation algorithm constructed here improves translation performance while ensuring a high correct identification rate, providing experimental references for the later intelligent development of Chinese language translation in computational linguistics.  © 2023 Association for Computing Machinery.",bilingual evaluation understudy (BLEU); BiLSTM; Chinese language translation; computational linguistics; deep learning,Brain; Classification (of information); Computational linguistics; Convolutional neural networks; Forecasting; Neural machine translation; Semantics; Support vector machines; Text processing; Bi-directional; Bi-directional long short-term memory; Bilingual evaluation understudy; Bilinguals; Chinese language; Chinese language translation; Deep learning; Forecasting algorithm; Language translation; Performance; Long short-term memory
Lyrics Analysis of the Arab Singer Abdel ElHalim Hafez,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153486077&doi=10.1145%2f3544100&partnerID=40&md5=0db805a8ac7afc9414b421f6726e204b,"In this work we analyze the lyrics of one of the most famous and influential Arab artists in the twentieth century, namely, (Abdel ElHalim Hafez). Lyrics analysis provides a deep insight into the artist's career evolution and his interactions with the surrounding environment including the social, political, and economic conditions. In order to perform such analysis we had to collect and compile the lyrics of Abdel ElHalim accompanied with the necessary metadata into an organized and structured form. The data are preprocessed by removing stop words and doing some normalization operations over the songs' prose. We did not perform any lemmatization or stemming as the original form of the tokens convey much more information than the source words. We performed a lexical analysis in order to study both the lexical density and diversity over the course of Abdel ElHalim's career life. We have as well studied the most significant words, idioms, and terms played in the songs using tools such as word clouds and more quantitative measures such as term frequency-inverse document frequency. We have divided the career life of Abdel ElHalim into sub-decades of length 5 years and all analyses are done both in a yearly fashion and more coarsely over such sub-decades. We have found a strong correlation between our statistical analysis and the socio-political status in Egypt and the Arab world during that time. This is especially relevant knowing that Abdel ElHalim is very much truly considered the son of the generation of the 1952 revolution in Egypt. The significance of Abdel ElHalim and his lyrics stem essentially from being contemporaneous to radical changes in Egypt across all sectors including political (support of liberation movements across the world, and the conflict with Israel), and socio-economic (especially changing the social class structure in Egypt). We also investigated the potential effectiveness of PoS (Part of Speech) tagging in genre analysis and classification.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesLyrics analysis; lexical analysis; PoS tagging; TFIDF,Syntactics; Text processing; Additional key word and phraseslyric analyse; Key words; Lexical analysis; Part of speech tagging; Parts-of-speech tagging; Political conditions; Social conditions; Surrounding environment; TFIDF; Twentieth century; Computational linguistics
Meta-ED: Cross-lingual Event Detection Using Meta-learning for Indian Languages,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152909898&doi=10.1145%2f3555340&partnerID=40&md5=c704c1f37dc81d2b0da85d25b4ff7674,"Lack of annotated data is a major concern in Event Detection (ED) tasks for low-resource languages. Cross-lingual ED seeks to address this issue by transferring information across various languages to improve overall performance. In this article, we propose a method for cross-lingual ED with a few training instances. We present a model agnostic meta-learning approach for few-shot cross-lingual ED that is able to find good parameter initialization and enables fast adaptation to new low-resource languages. We evaluate our model on four Indian languages. The results show that our approach significantly outperforms the base model.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesEvent Detection; cross-lingual; few-shot; Indian languages; low-resource languages; meta-learning; multilingual pretrained model,Additional key word and phrasesevent detection; Cross-lingual; Detection tasks; Events detection; Few-shot; Indian languages; Key words; Low resource languages; Metalearning; Multilingual pretrained model; Learning systems
A Novel Assistive Glove to Convert Arabic Sign Language into Speech,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152940744&doi=10.1145%2f3545113&partnerID=40&md5=8c71ef2fc3769f100ec65c13747ec3ca,"People with speech disorders often communicate through special gestures and sign language gestures. However, other people around them might not understand the meaning of those gestures. The research described in this article is aimed at providing an assistive device to help those people communicate with others by translating their gestures into a spoken voice that others can understand. The proposed device includes an electronic glove that is worn on the hand. It employs an MPU6050 accelerometer/gyro with 6 degrees of freedom to continuously monitor hand orientation and movement, plus a potentiometer for each finger, to monitor changes in finger posture. The signals from the MPU6050 and the potentiometers are routed to an Arduino board, where they are processed to determine the meaning of each gesture, which is then voiced using the audio streams stored in an SD memory card. The audio output drives a speaker, allowing the listener to understand the meaning of each gesture. We built a database with the help of 10 deaf people who cannot speak. We asked them to wear the glove while performing a set of 40 Arabic sign language words and recorded the resulting data stream from the glove. That data was then used to train seven different learning algorithms. The results showed that the Decision Tree learning algorithm achieved the highest accuracy of 98%. A usability study was then conducted to determine the usefulness of the assistive device in real-Time.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesAssistive technology; Arabic sign language; assistive glove; deaf,Decision trees; Digital storage; Learning algorithms; Potentiometers (electric measuring instruments); 6 degree of freedom; Additional key word and phrasesassistive technology; Arabic sign language; Assistive; Assistive devices; Assistive glove; Deaf; Key words; Sign language; Speech disorders; Degrees of freedom (mechanics)
A Study on the Performance of Recurrent Neural Network based Models in Maithili Part of Speech Tagging,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152934671&doi=10.1145%2f3540260&partnerID=40&md5=2be9c59bde6d15cb09815a8f0d669552,"This article presents our effort in developing a Maithili Part of Speech (POS) tagger. Substantial effort has been devoted to developing POS taggers in several Indian languages, including Hindi, Bengali, Tamil, Telugu, Kannada, Punjabi, and Marathi; but Maithili did not achieve much attention from the research community. Maithili is one of the official languages of India, with around 50 million native speakers. So, we worked on developing a POS tagger in Maithili. For the development, we use a manually annotated in-house Maithili corpus containing 56,126 tokens. The tagset contains 27 tags. We train a conditional random fields (CRF) classifier to prepare a baseline system that achieves an accuracy of 82.67%. Then, we employ several recurrent neural networks (RNN)-based models, including Long-short Term Memory (LSTM), Gated Recurrent Unit (GRU), LSTM with a CRF layer (LSTM-CRF), and GRU with a CRF layer (GRU-CRF) and perform a comparative study. We also study the effect of both word embedding and character embedding in the task. The highest accuracy of the system is 91.53%.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesPart of speech tagging; Maithili language; neural model for NLP; Recurrent neural network,Computational linguistics; Long short-term memory; Multilayer neural networks; Natural language processing systems; Random processes; Syntactics; Additional key word and phrasespart of speech tagging; Key words; Maithilus language; Network-based modeling; Neural model for NLP; Neural modelling; Part-of-speech tagger; Part-of-speech tags; Random fields; Speech tagging; Embeddings
GA-SCS: Graph-Augmented Source Code Summarization,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152924343&doi=10.1145%2f3554820&partnerID=40&md5=3a85283890a1348552257c94b1bc7bf2,"Automatic source code summarization system aims to generate a valuable natural language description for a program, which can facilitate software development and maintenance, code categorization, and retrieval. However, previous sequence-based research did not consider the long-distance dependence and highly structured characteristics of source code simultaneously. In this article, we present a Transformer-based Graph-Augmented Source Code Summarization (GA-SCS), which can effectively incorporate inherent structural and textual features of source code to generate an effective code description. Specifically, we develop a graph-based structure feature extraction scheme leveraging abstract syntax tree and graph attention networks to mine global syntactic information. And then, to take full advantage of the lexical and syntactic information of code snippets, we extend the original attention to a syntax-informed self-Attention mechanism in our encoder. In the training process, we also adopt a reinforcement learning strategy to enhance the readability and informativity of generated code summaries. We utilize the Java dataset and Python dataset to evaluate the performance of different models. Experimental results demonstrate that our GA-SCS model outperforms all competitive methods on BLEU, METEOR, ROUGE, and human evaluations.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesProgram comprehension; natural language processing; reinforcement learning; self-Attention; source code summarization,Abstracting; Codes (symbols); Graphic methods; Learning algorithms; Learning systems; Natural language processing systems; Python; Software design; Syntactics; Trees (mathematics); Additional key word and phrasesprogram comprehension; Key words; Language processing; Natural language processing; Natural languages; Reinforcement learnings; Self-attention; Source code summarization; Source codes; Syntactic information; Reinforcement learning
Development of an Efficient Method to Detect Mixed Social Media Data with Tamil-English Code Using Machine Learning Techniques,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152965947&doi=10.1145%2f3563775&partnerID=40&md5=0da4314d85ffbf5d8f5ebe274ca858f1,"On social networking sites, online hate speech has become more prevalent due to the quick expansion of mobile computing and Web technology. Previous research has found that being exposed to Internet hate speech has substantial offline implications for historically disadvantaged communities. Therefore, there is a lot of interest in research on automated hate-based comment and post detection. Hate speech can have an influence on any population group, but some are more vulnerable than others. From this background, detecting and reporting such hate related comments and posts can help to avoid the harmful effects of hate speech. There are some studies available on this context and it was found that machine learning algorithms are more efficient in detecting abusive texts in social media. In this research, we applied selected seven machine learning algorithms such as Support Vector Machine (SVM), Naïve Bayes (NB), Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), Gradient Boost (GB) and K Nearest Neighbor (KNN) to detect hate speech and compare the performances of those algorithms to develop an ensemble model. Researchers collected and combined Tamil - English code-mixed hate speech tweets dataset which was created in HASOC. This dataset's tweets are divided into two groups: not offensive and offensive. This dataset includes 35,442 tweets. In this research, NB has obtained highest F1 scores in detecting offensive and not offensive tweets with highest weighted average. But SVM has obtained highest accuracy in detecting Tamil - English hate speech texts with 80% in 10-fold cross-validation. Based on the stand-alone performances, researchers developed two ensemble classifiers including max-voting and averaging ensemble. Averaging ensemble classification obtained 90.67% in accuracy. The research study's findings are significant because these results can be applied as a model for Tamil - English code-mixed hate speech to evaluate future research works using various algorithms for identifying hate contents more accurately and professionally.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesTamil; code mixed; English; hate speech; machine learning and ensemble classification,Learning algorithms; Learning systems; Logistic regression; Nearest neighbor search; Social networking (online); Social sciences computing; Speech recognition; Support vector machines; Additional key word and phrasestamil; Code mixed; English; Ensemble classification; Hate speech; Key words; Machine learning algorithms; Machine learning classification; Naive bayes; Support vectors machine; Decision trees
Impact of Similarity Measures in Graph-based Automatic Text Summarization of Konkani Texts,2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152928912&doi=10.1145%2f3554943&partnerID=40&md5=d75f26f5362cff7ccfd12c05f66cb7f9,"Automatic text summarization is a popular area in Natural Language Processing and Machine Learning. In this work, we adopt a graph-based text summarization approach, using PageRank algorithm, for automatically summarizing Konkani text documents. Konkani is an Indo-Aryan language spoken primarily in the state of Goa, which is on the west coast of India. It is a low-resource language with limited language processing tools. Such tools are readily available in other popular languages of choice for automatic text summarization, like English. The Konkani language dataset used for this purpose is based on Konkani folktales. We examine the impact of various language-independent and language-dependent similarity measures on the construction of the graph. The language-dependent similarity measures use pre-Trained fastText word embeddings. A fully connected undirected graph is constructed for each document with the sentences represented as the graph's vertices. The vertices are connected to each other based on how strongly they are related to one another. Thereafter, PageRank algorithm is used for ranking the scores of the vertices. The top-ranking sentences are used to generate the summary. ROUGE toolkit was used for evaluating the quality of these system-generated summaries, and the performance was evaluated against human generated ""gold-standard""abstracts and also compared with baselines and benchmark systems. The experimental results show that language-independent similarity measures performed well compared to language-dependent similarity measures despite not using language-specific tools, such as stop-words list, stemming, and word embeddings.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesTextRank; automatic text summarization; fastText; graph-based method; Konkani; low-resource language; PageRank; similarity measures,Benchmarking; Embeddings; Graphic methods; Natural language processing systems; Text processing; Additional key word and phrasestextrank; Automatic text summarization; Fasttext; Graph-based; Graph-based methods; Key words; Konkani; Low resource languages; Page ranks; Similarity measure; Undirected graphs
"An Arabic Manuscript Regions Detection, Recognition and Its Applications for OCRing",2023,ACM Transactions on Asian and Low-Resource Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148996815&doi=10.1145%2f3532609&partnerID=40&md5=e4b8d6a95f6bba74fe63bd17004588e1,"The problem of Region of Interest (RoI) in document layout analysis and document recognition has recently become an essential topic in OCRing systems. Arabic manuscript layout analysis and OCRing recognition using language detection, document category, and RoI with Keras and TensorFlow are terms of the state-of-The-Art that should be investigated. This article investigates the problem of Arabic manuscript recognition problems with respect to in OCRing-based recognition. A new framework architecture, which integrates Fast Gradient Sign Method (FGSM) using Keras and TensorFlow with adversarial image generation during training procedure is proposed. Also, the article tries to improve the OCRing accuracy of the image enhancement, alignment, layout analysis, and recognition using deep learning in multilingual system. RoIs detections will be performed using a custom trained deep learning model using bounding box regression with Keras and TensorFlow. This topic investigates an extension of Page Segmentation Method (PSM) to enhance OCRing parameter modes and enhances Arabic OCRing system accuracy from reinforcement strategy. Therefore, the article achieves a significant improvement of OCRing results due to the three parameters: language identification, document category, and RoI types (Table, Title, Paragraph, figure, and list). This model is based on ""region proposal algorithm""as a basis of CNN object detectors to find the number of the RoIs. Therefore, the proposed framework performs three distinctive tasks: (1) CNN architecture for adversarial training, (2) an implementation of the FGSM with Keras and TensorFlow, and (3) an adversarial training script implementation with the CNN and the FGSM method. The experiments on Arabic manuscript dataset including Arabic text, English/Arabic documents, and Latin digits' datasets, demonstrate the accuracy of the proposed method. Moreover, the proposed framework performs well and succeeded in defending against adversarial attacks or adversarial images. The experimental results on our collected dataset illustrate the novelty of our proposed framework over the other existing PSM methods to be extended and updated to improve the quality of the OCRing system. The results show that the influence of PSM after expanding using the RoI types, language ID, and document/manuscript category can improve the OCRing accuracy. Also, the experimental results show significant performance by the new framework model with accuracy reached to 99% compared to relative methods. © 2023 Association for Computing Machinery.",Arabic OCR; Convolution Neural Networks (CNN); Deep Learning; Layout analysis; RoIs (Bounded Box),Deep learning; Image segmentation; Learning systems; Network architecture; Object detection; Arabic OCR; Bounded box; Convolution neural network; Deep learning; Layout analysis; Page segmentation; Region-of-interest; Regions of interest; Rois (bounded box); Image enhancement
