Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
"RAIDShield: Characterizing, monitoring, and proactively protecting against disk failures",2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954317814&doi=10.1145%2f2820615&partnerID=40&md5=0e1518b84e92bc1a04a7339bd16a33fa,"Modern storage systems orchestrate a group of disks to achieve their performance and reliability goals. Even though such systems are designed to withstand the failure of individual disks, failure of multiple disks poses a unique set of challenges. We empirically investigate disk failure data from a large number of production systems, specifically focusing on the impact of disk failures on RAID storage systems. Our data covers about one million SATA disks from six disk models for periods up to 5 years. We show how observed disk failures weaken the protection provided by RAID. The count of reallocated sectors correlates strongly with impending failures. With these findings we designed RAIDSHIELD, which consists of two components. First, we have built and evaluated an active defense mechanism that monitors the health of each disk and replaces those that are predicted to fail imminently. This proactive protection has been incorporated into our product and is observed to eliminate 88% of triple disk errors, which are 80% of all RAID failures. Second, we have designed and simulated a method of using the joint failure probability to quantify and predict how likely a RAID group is to face multiple simultaneous disk failures, which can identify disks that collectively represent a risk of failure even when no individual disk is flagged in isolation. We find in simulation that RAID-level analysis can effectively identify most vulnerable RAID-6 systems, improving the coverage to 98% of triple errors. We conclude with discussions of operational considerations in deploying RAIDSHIELD more broadly and new directions in the analysis of disk errors. One interesting approach is to combine multiple metrics, allowing the values of different indicators to be used for predictions. Using newer field data that reports an additional metric, medium errors, we find that the relative efficacy of reallocated sectors and medium errors varies across disk models, offering an additional way to predict failures. Copyright © 2015 ACM.",Disk errors; Magnetic disks; RAID; Reliability,Digital storage; Reliability; Active defense mechanisms; Joint failure probabilities; Magnetic disk; Performance and reliabilities; Pro-active protection; Production system; RAID; Storage systems; Errors
BetrFS: Write-optimization in a kernel file system,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954309911&doi=10.1145%2f2798729&partnerID=40&md5=2d6c162fdcba4273f4e26091ae0b2286,"The Bε-tree File System, or BetrFS (pronounced ""better eff ess""), is the first in-kernel file system to use a write-optimized data structure (WODS). WODS are promising building blocks for storage systems because they support both microwrites and large scans efficiently. Previous WODS-based file systems have shown promise but have been hampered in several ways, which BetrFS mitigates or eliminates altogether. For example, previous WODS-based file systems were implemented in user space using FUSE, which superimposes many reads on a write-intensive workload, reducing the effectiveness of the WODS. This article also contributes several techniques for exploiting write-optimization within existing kernel infrastructure. BetrFS dramatically improves performance of certain types of large scans, such as recursive directory traversals, as well as performance of arbitrary microdata operations, such as file creates, metadata updates, and small writes to files. BetrFS can make small, random updates within a large file 2 orders of magnitude faster than other local file systems. BetrFS is an ongoing prototype effort and requires additional data-structure tuning to match current general-purpose file systems on some operations, including deletes, directory renames, and large sequential writes. Nonetheless, many applications realize significant performance improvements on BetrFS. For instance, an in-place rsync of the Linux kernel source sees roughly 1.6-22× speedup over commodity file systems. Copyirght © 2015 ACM.",B<sup>ε</sup>-trees; File system; Write optimization,Computer operating systems; Data structures; Digital storage; Forestry; Linux; Trees (mathematics); Additional datum; Building blockes; File systems; Local file systems; Orders of magnitude; Storage systems; Structure tuning; Write-intensive workloads; File organization
Skylight - A window on shingled disk operation,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946575168&doi=10.1145%2f2821511&partnerID=40&md5=3b52219773e326359fc3b2ffd8b68a51,"We introduce Skylight, a novel methodology that combines software and hardware techniques to reverse engineer key properties of drive-managed Shingled Magnetic Recording (SMR) drives. The software part of Skylight measures the latency of controlled I/O operations to infer important properties of drive-managed SMR, including type, structure, and size of the persistent cache; type of cleaning algorithm; type of block mapping; and size of bands. The hardware part of Skylight tracks drive head movements during these tests, using a high-speed camera through an observation window drilled through the cover of the drive. These observations not only confirm inferences from measurements, but resolve ambiguities that arise from the use of latency measurements alone. We show the generality and efficacy of our techniques by running them on top of three emulated and two real SMR drives, discovering valuable performance-relevant details of the behavior of the real SMR drives. © 2015 ACM.",Algorithms; Design; Experimentation; Measurement; Performance; Reliability,Algorithms; Design; Hardware; High speed cameras; Measurement; Reliability; Experimentation; Latency measurements; Novel methodology; Observation window; Performance; Shingled magnetic recording (SMR); Software and hardwares; Software parts; Drives
Introduction to the special issue on USENIX FAST 2015,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946548688&doi=10.1145%2f2825000&partnerID=40&md5=98ee9afde83f14c50a04c14600c5a09b,[No abstract available],,
Shuffle index: Efficient and private access to outsourced data,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946561014&doi=10.1145%2f2747878&partnerID=40&md5=e8bf7fee43f6b5618623fe09ca395d73,"Data outsourcing and cloud computing have been emerging at an ever-growing rate as successful approaches for allowing users and companies to rely on external services for storing and managing data. As data and access to them are not under the control of the data owner, there is a clear need to provide proper confidentiality protection. Such requirements concern the confidentiality not only of the stored data (content) but also of the specific accesses (or patterns of them) that users make on such data. In this article, we address these issues and propose an approach for guaranteeing content, access, and pattern confidentiality in a data outsourcing scenario. The proposed solution is based on the definition of a shuffle index structure, which adapts traditional B+-trees and, by applying a combination of techniques (covers, caches, and shuffling), ensures confidentiality of the data and of queries over them, protecting each single access as well as sequences thereof. The proposed solution also supports update operations over the data, while making reads and writes not recognizable as such by the server. We show that the shuffle index exhibits a limited performance cost, thus resulting effectively usable in practice. © 2015 ACM.",Design; Management; Security,Design; Management; Outsourcing; B+-trees; Data outsourcing; Index structure; Outsourced datum; Performance costs; Security; Trees (mathematics)
Accelerating file system metadata access with byte-addressable nonvolatile memory,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938323978&doi=10.1145%2f2766453&partnerID=40&md5=f291b43776a98b5e998991da01af3d18,"File system performance is dominated by small and frequent metadata access. Metadata is stored as blocks on the hard disk drive. Partial metadata update results in whole-block read or write, which significantly amplifies disk I/O. Furthermore, a huge performance gap between the CPU and disk aggravates this problem. In this article, a file system metadata accelerator (referred to as FSMAC) is proposed to optimize metadata access by efficiently exploiting the persistency and byte-addressability of Nonvolatile Memory (NVM). The FSMAC decouples data and metadata access path, putting data on disk and metadata in byte-addressable NVM at runtime. Thus, data is accessed in a block from I/O the bus and metadata is accessed in a byteaddressable manner from the memory bus. Metadata access is significantly accelerated and metadata I/O is eliminated because metadata in NVM is no longer flushed back to the disk periodically. A lightweight consistency mechanism combining fine-grained versioning and transaction is introduced in the FSMAC. The FSMAC is implemented on a real NVDIMM platform and intensively evaluated under different workloads. Evaluation results show that the FSMAC accelerates the file system up to 49.2 times for synchronized I/O and 7.22 times for asynchronized I/O. Moreover, it can achieve significant performance speedup in network storage and database environment, especially for metadata-intensive or write-dominated workloads. © 2015 ACM.",File system; Metadata acceleration; Nonvolatile memory,File organization; Nonvolatile storage; Data and metadata; Evaluation results; File systems; Fine grained; Hard Disk Drive; In-network storages; Non-volatile memory; Performance gaps; Metadata
Frog: A framework for context-based file systems,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938314140&doi=10.1145%2f2720022&partnerID=40&md5=efdb8545bbf60afc952aea9a3896c491,"This article presents a framework, Frog, for Context-Based File Systems (CBFSs) that aim at simplifying the development of context-based file systems and applications. Unlike existing informed-based context-aware systems, Frog is a unifying informed-based framework that abstracts context-specific solutions as views, allowing applications to make view selections according to application behaviors. The framework can not only eliminate overheads induced by traditional context analysis, but also simplify the interactions between the context-based file systems and applications. Rather than propagating data through solution-specific interfaces, views in Frog can be selected by inserting their names in file path strings. With Frog in place, programmers can migrate an application from one solution to another by switching among views rather than changing programming interfaces. Since the data consistency issues are automatically enforced by the framework, file-system developers can focus their attention on context-specific solutions. We implement two prototypes to demonstrate the strengths and overheads of our design. Inspired by an observation that there aremore than 50% of small files (<4KB) in a file system, we create a Bi-context Archiving Virtual File System (BAVFS) that utilizes conservative and aggressive prefetching for the contexts of random and sequential reads. To improve the performance of random read-and-write operations, the Bi-context Hybrid Virtual File System (BHVFS) combines the update-in-place and update-out-of-place solutions for read-intensive and write-intensive contexts. Our experimental results show that the benefits of Frog-based CBFSs outweigh the overheads introduced by integrating multiple context-specific solutions. © 2015 ACM.",Context aware; File systems; Multiview,Data storage equipment; Application behaviors; Context-Aware; Context-aware systems; File systems; Multi-views; Multiple contexts; Programming interface; Specific interface; File organization
"On the trade-offs among performance, energy, and endurance in a versatile hybrid drive",2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938369092&doi=10.1145%2f2700312&partnerID=40&md5=5f398f88ac6f3f7490423c8b1c8f63b2,"There are trade-offs among performance, energy, and device endurance for storage systems. Designs optimized for one dimension or workload often suffer in another. Therefore, it is important to study the trade-offs to enable adaptation to workloads and dimensions. As Flash SSD has emerged, hybrid drives have been studied more closely. However, hybrids are mainly designed for high throughput, efficient energy consumption, or improving endurance-leaving quantitative study on the trade-offs unexplored. Past endurance studies also lack a concrete model to help study the trade-offs. Last, previous designs are often based on inflexible policies that cannot adapt easily to changing conditions. We designed and developed GreenDM, a versatile hybrid drive that combines Flash-based SSDs with traditional HDDs. The SSD can be used as cache or as primary storage for hot data. We present our endurance model together with GreenDM to study these trade-offs. GreenDM presents a block interface and requires no modifications to existing software. GreenDM offers tunable parameters to enable the system to adapt to many workloads. We have designed, developed, and carefully evaluated GreenDM with a variety of workloads using commodity SSD and HDD drives. We demonstrate the importance of versatility to enable adaptation to various workloads and dimensions. © 2015 ACM.",Hybrid drive; Solid-state drive; Trade-offs; Versatility,Commerce; Energy utilization; Flash-based SSDs; Hard disk storage; Block interfaces; Hybrid drive; Primary storages; Quantitative study; Solid state drives; Trade off; Tunable parameter; Versatility; Economic and social effects
Immortal graph: A system for storage and analysis of temporal graphs,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938300445&doi=10.1145%2f2700302&partnerID=40&md5=2f05d48fd7aef14e0f3106d859ce30eb,"Temporal graphs that capture graph changes over time are attracting increasing interest from research communities, for functions such as understanding temporal characteristics of social interactions on a timeevolving social graph. ImmortalGraph is a storage and execution engine designed and optimized specifically for temporal graphs. Locality is at the center of ImmortalGraph's design: temporal graphs are carefully laid out in both persistent storage andmemory, taking into account data locality in both time and graph-structure dimensions. ImmortalGraph introduces the notion of locality-aware batch scheduling in computation, so that common ""bulk"" operations on temporal graphs are scheduled to maximize the benefit of in-memory data locality. The design of ImmortalGraph explores an interesting interplay among locality, parallelism, and incremental computation in supporting common mining tasks on temporal graphs. The result is a highperformance temporal-graph system that is up to 5 times more efficient than existing database solutions for graph queries. The locality optimizations in ImmortalGraph offer up to an order of magnitude speedup for temporal iterative graph mining compared to a straightforward application of existing graph engines on a series of snapshots. © 2015 ACM.",Concurrent computing; Graph algorithms; Temporal graph,Biographies; Digital storage; Engines; Graph algorithms; Graphic methods; Iterative methods; Query languages; Concurrent computing; Incremental computation; Locality optimization; Research communities; Social interactions; Structure dimensions; Temporal characteristics; Temporal graphs; Graph structures
SmartCon: Smart context switching for fast storage devices,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926369856&doi=10.1145%2f2631922&partnerID=40&md5=ec1837829745ac501ddf023869a07cc8,"Handling of storage IO in modern operating systems assumes that such devices are slow and CPU cycles are valuable. Consequently, to effectively exploit the underlying hardware resources, for example, CPU cycles, storage bandwidth and the like, whenever an IO request is issued to such device, the requesting thread is switched out in favor of another thread that may be ready to execute. Recent advances in nonvolatile storage technologies and multicore CPUs make both of these assumptions increasingly questionable, and an unconditional context switch is no longer desirable. In this article, we propose a novel mechanism called SmartCon, which intelligently decides whether to service a given IO request in interrupt-driven manner or busy-wait-based manner based on not only the device characteristics but also dynamic parameters such as IO latency, CPU utilization, and IO size. We develop an analytic performance model to project the performance of SmartCon for forthcoming devices. We implement SmartCon mechanism on Linux 2.6 and perform detailed evaluation using three different IO devices: Ramdisk, low-end SSD, and high-end SSD. We find that SmartCon yields up to a 39% performance gain over the mainstream block device approach for Ramdisk, and up to a 45% gain for PCIe-based SSD and SATA-based SSDs. We examine the detailed behavior of TLB, L1, L2 cache and show that SmartCon achieves significant improvement in all cache misbehaviors. © 2015 ACM.",Context switch; I/O subsystem; Nonvolatile memory; Solid state disk,Buffer storage; Computer operating systems; Program processors; Virtual storage; Analytic performance modeling; Context switch; Device characteristics; Dynamic parameters; Hardware resources; Non-volatile memory; Solid state disks; Storage technology; Nonvolatile storage
"Rebuttal to ""Beyond MTTDL: A closed-form RAID-6 reliability equation""",2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926365938&doi=10.1145%2f2700311&partnerID=40&md5=645a7512d20e1a2ae2cd46ed43d63278,"A recent article on the reliability of RAID-6 storage systems overlooks certain relevant prior work published in the past 20 years and concludes that the widely used mean time to data loss (MTTDL) metric does not provide accurate results. In this note, we refute this position by invoking uncited relevant prior work and demonstrating that the MTTDL remains a useful metric. © 2015 ACM.",MTTDL; RAID; Reliability analysis; Stochastic modeling; Unrecoverable or latent sector errors,Digital storage; Reliability; Stochastic models; Stochastic systems; Closed form; Data loss; MTTDL; RAID; Reliability equations; Storage systems; Reliability analysis
An energy-efficient and reliable storage mechanism for data-intensive academic archive systems,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926343888&doi=10.1145%2f2720021&partnerID=40&md5=f53d8225f6d5ab62827465510aaca09f,"Previous studies proposed energy-efficient solutions, such as multispeed disks and disk spin-down methods, to conserve power in their respective storage systems. However, in most cases, the authors did not analyze the reliability of their solutions. According to research conducted by Google and the IDEMA standard, frequently setting the disk status to standby mode will increase the disk's Annual Failure Rate and reduce its lifespan. To resolve the issue, we propose an evaluation function called E3SaRC (Economic Evaluation of Energy Saving with Reliability Constraint), which considers the cost of hardware failure when applying energy-saving schemes. We also present an adaptive write cache mechanism called CacheRAID. The mechanism tries to mitigate the random access problems that implicitly exist in RAID techniques and thereby reduce the energy consumption of RAID disks. CacheRAID also addresses the issue of system reliability by applying a control mechanism to the spin-down algorithm. Our experimental results show that the CacheRAID storage system can reduce the power consumption of the conventional software RAID 5 system by 65% to 80%. Moreover, according to the E3SaRC measurement, the overall saved cost of CacheRAID is the largest among the systems that we compared. © 2015 ACM.",Disk spin-down; Energy-aware storage system; Power consumption; RAID; Reliability,Digital storage; Electric power utilization; Energy conservation; Energy utilization; Reliability; Disk spin downs; Economic evaluations; Energy saving schemes; Evaluation function; RAID; Reliability constraints; Storage systems; System reliability; Energy efficiency
Data sorting in flash memory,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926293649&doi=10.1145%2f2665067&partnerID=40&md5=f529625768ca4bf29dc089f49aa71f88,"Because flash memory now provides an economical solution for various portable devices and embedded systems, an NAND flash-based storage system has replaced the hard disk drive in many applications. Recently, the implementation of database systems using an NAND flash-based storage system has become an important research topic. In particular, the external sorting is an important operation in database systems. With the very distinctive characteristics of flash memory, the typical external sorting system that adopts a clustered sorting process can result in performance degradation and reduce the reliability of flash memory. In this article, we will propose an unclustered sorting method that considers the unique characteristics of flash memory, and we then propose a decision rule to exploit the advantages of both clustered and unclustered sorting. The decision rule can separate records according to their record length, sort them appropriately by the clustered and unclustered sorting, and merge the sorted results. The experimental results show that the proposed method can improve performance in an NAND flash-based storage system (i.e., solid-state drive). © 2015 ACM.",Flash memory; Memory structures; Sorting/searching; Storage management,Database systems; Digital storage; Embedded systems; Hard disk storage; Monolithic microwave integrated circuits; Sorting; Storage management; Hard Disk Drive; Improve performance; Memory structure; Performance degradation; Portable device; Solid state drives; Sorting process; Storage systems; Flash memory
Visualizing block IO workloads,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926286955&doi=10.1145%2f2651422&partnerID=40&md5=5488da6f5e00d8113c402a61586b9ec6,"Massive block IO systems are the workhorses powering many of today's largest applications. Databases, health care systems, and virtual machine images are examples for block storage applications. The massive scale of these workloads, and the complexity of the underlying storage systems, makes it difficult to pinpoint problems when they occur. This work attempts to shed light on workload patterns through visualization, aiding our intuition. We describe our experience in the last 3 years of analyzing and visualizing customer traces from XIV, an IBM enterprise block storage system. We also present results from applying the same visualization technology to Linux filesystems. We show how visualization aids our understanding of workloads and how it assists in resolving customer performance problems. © 2015 ACM.",Visualization,Computer operating systems; Flow visualization; Block storage systems; Health-care system; Performance problems; Storage systems; Visualization technologies; Workload patterns; Visualization
Design tradeoffs of SSDs: From energy consumption's perspective,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926351444&doi=10.1145%2f2644818&partnerID=40&md5=c06cdf542f23e4ea4d2646b180cb6a12,"In this work, we studied the energy consumption characteristics of various SSD design parameters. We developed an accurate energy consumption model for SSDs that computes aggregate, as well as component-specific, energy consumption of SSDs in sub-msec time scale. In our study, we used five different FTLs (page mapping, DFTL, block mapping, and two different hybrid mappings) and four different channel configurations (two, four, eight, and 16 channels) under seven different workloads (from large-scale enterprise systems to small-scale desktop applications) in a combinatorial manner. For each combination of the aforementioned parameters, we examined the energy consumption for individual hardware components of an SSD (microcontroller, DRAM, NAND flash, and host interface). The following are some of our findings. First, DFTL is the most energy-efficient address-mapping scheme among the five FTLs we tested due to its good write amplification and small DRAM footprint. Second, a significant fraction of energy is being consumed by idle flash chips waiting for the completion of NAND operations in the other channels. FTL should be designed to fully exploit the internal parallelism so that energy consumption by idle chips is minimized. Third, as a means to increase the internal parallelism, increasing way parallelism (the number of flash chips in a channel) is more effective than increasing channel parallelism in terms of peak energy consumption, performance, and hardware complexity. Fourth, in designing high-performance and energy-efficient SSDs, channel switching delay, way switching delay, and page write latency need to be incorporated in an integrated manner to determine the optimal configuration of internal parallelism. © 2015 ACM.",Energy consumption; FTL; NAND flash; Parallelism; Simulator; SSD,Energy utilization; Flash-based SSDs; Hardware; Integrated circuit design; Mapping; Simulators; Channel configuration; Channel switching delays; Energy consumption model; FTL; NAND Flash; Parallelism; SSD; Write amplifications; Energy efficiency
Low-complexity implementation of RAID based on Reed-Solomon codes,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925364138&doi=10.1145%2f2700308&partnerID=40&md5=897d1936bb93503ea6f42e9ed1b60066,Fast algorithms are proposed for encoding and reconstructing data in RAID based on Reed-Solomon codes. The proposed approach is based on the cyclotomic fast Fourier transform algorithm and enables one to significantly reduce the number of expensive Galois field multiplications required. The complexity of the obtained algorithms is much lower than those for existing MDS array codes. Software implementation of the proposed algorithms is discussed. The performance results show that the new algorithms provide substantially better performance compared with the standard algorithm. © 2015 ACM 1553-3077/2015/02-ART1 $15.00.,Algorithms; Design; Performance; Reliability,Codes (symbols); Design; Fast Fourier transforms; Reed-Solomon codes; Reliability; Array codes; Better performance; Cyclotomic fast Fourier transforms; Fast algorithms; Galois fields; Performance; Software implementation; Standard algorithms; Algorithms
Z-map: A zone-based flash translation layer with workload classification for solid-state drive,2015,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925351504&doi=10.1145%2f2629663&partnerID=40&md5=25fcc062349a78aa8e26146304b83ecc,"Existing space management and address mapping schemes for flash-based Solid-State-Drive (SSD) operate either at page or block granularity, with inevitable limitations in terms of memory requirement, performance, garbage collection, and scalability. To overcome these limitations, we proposed a novel space management and address mapping scheme for flash referred to as Z-MAP, which manages flash space at granularity of Zone. Each Zone consists of multiple numbers of flash blocks. Leveraging workload classification, Z-MAP explores Page-mapping Zone (Page Zone) to store random data and handle a large number of partial updates, and Block-mapping Zone (Block Zone) to store sequential data and lower the overall mapping table. Zones are dynamically allocated and a mapping scheme for a Zone is determined only when it is allocated. Z-MAP uses a small part of Flash memory or phase change memory as a streaming Buffer Zone to log data sequentially and migrate data into Page Zone or Block Zone based on workload classification. A two-level address mapping is designed to reduce the overall mapping table and address translation latency. Z-MAP classifies data before it is permanently stored into Flash memory so that different workloads can be isolated and garbage collection overhead can be minimized. Z-MAP has been extensively evaluated by trace-driven simulation and a prototype implementation on OpenSSD. Our benchmark results conclusively demonstrate that Z-MAP can achieve up to 76% performance improvement, 81% mapping table reduction, and 88% garbage collection overhead reduction compared to existing Flash Translation Layer (FTL) schemes. © 2015 ACM 1553-3077/2015/02-ART4 $15.00.",Design; Management; Measurement; Performance,Benchmarking; Design; Digital storage; Flash-based SSDs; Management; Mapping; Measurements; Monolithic microwave integrated circuits; Phase change memory; Refuse collection; Address translation; Flash translation layer; Memory requirements; Overhead reductions; Performance; Prototype implementations; Solid state drives (SSD); Trace driven simulation; Flash memory
HEAPO: Heap-based persistent object store,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920138549&doi=10.1145%2f2629619&partnerID=40&md5=66647c9296d817863c2ad6673a71262f,"In this work, we developed a Heap-Based Persistent Object Store (HEAPO) to manage persistent objects in byte-addressable Nonvolatile RAM (NVRAM). HEAPO defines its own persistent heap layout, the persistent object format, name space organization, object sharing and protection mechanism, and undo-only log-based crash recovery, all of which are effectively tailored for NVRAM. We put our effort into developing a lightweight and flexible layer to exploit the DRAM-like access latency of NVRAM. To address this objective, we developed (i) a native management layer for NVRAM to eliminate redundancy between in-core and on-disk copies of the metadata, (ii) an expandable object format, (iii) a burst trie-based global name space with local name space caching, (iv) static address binding, and (v) minimal logging for undo-only crash recovery. We implemented HEAPO at commodity OS (Linux 2.6.32) and measured the performance. By eliminating metadata redundancy, HEAPO improved the speed of creating, attaching, and expanding an object by 1.3×, 4.5×, and 3.8×, respectively, compared to memory-mapped file-based persistent object store. Burst trie-based name space organization of HEAPO yielded 7.6× better lookup performance compared to hashed B-tree-based name space of EXT4. We modified memcachedb to use HEAPO in maintaining its search structure. For hash table update, HEAPO-based memcachedb yielded 3.4× performance improvement against original memcachedb implementation which uses mmap() over ramdisk approach to maintain the key-value store in memory. © 2014 ACM 1553-3077/2014/12-ART3 $15.00.",Nonvolatile memory; Persistent heap; Persistent objects,Computer operating systems; Metadata; Redundancy; Lookup performance; Memory-Mapped file; Non-volatile memory; Non-volatile rams; Persistent heap; Persistent objects; Protection mechanisms; Space organizations; Dynamic random access storage
Efficient hybrid inline and out-of-line deduplication for Backup storage,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920112528&doi=10.1145%2f2641572&partnerID=40&md5=eacc4abb838107f4b5d777fad4069b78,"Backup storage systems often remove redundancy across backups via inline deduplication, which works by referring duplicate chunks of the latest backup to those of existing backups. However, inline deduplication degrades restore performance of the latest backup due to fragmentation, and complicates deletion of expired backups due to the sharing of data chunks. While out-of-line deduplication addresses the problems by forward-pointing existing duplicate chunks to those of the latest backup, it introduces additional I/Os of writing and removing duplicate chunks. We design and implement RevDedup, an efficient hybrid inline and out-of-line deduplication system for backup storage. It applies coarse-grained inline deduplication to remove duplicates of the latest backup, and then fine-grained out-of-line reverse deduplication to remove duplicates from older backups. Our reverse deduplication design limits the I/O overhead and prepares for efficient deletion of expired backups. Through extensive testbed experiments using synthetic and real-world datasets, we show that RevDedup can bring high performance to the backup, restore, and deletion operations, while maintaining high storage efficiency comparable to conventional inline deduplication. © 2014 ACM 1553-3077/2014/12-ART2 $15.00.",Backup storage; Deduplication,Data Sharing; Restoration; Backup storages; Coarse-grained; De duplications; Design and implements; Design limits; Fine grained; Real-world datasets; Storage efficiency; Digital storage
Introduction to the special issue on USENIX FAST 2014,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908868684&doi=10.1145%2f2670792&partnerID=40&md5=a2ebe5a86f13a269091198950e77b289,[No abstract available],,
STAIR codes: A general family of erasure codes for tolerating device and sector failures,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908891663&doi=10.1145%2f2658991&partnerID=40&md5=2a5603679b6b5677f7c32fbc871548a2,"Practical storage systems often adopt erasure codes to tolerate device failures and sector failures, both of which are prevalent in the field. However, traditional erasure codes employ device-level redundancy to protect against sector failures, and hence incur significant space overhead. Recent sector-disk (SD) codes are available only for limited configurations. By making a relaxed but practical assumption, we construct a general family of erasure codes called STAIR codes, which efficiently and provably tolerate both device and sector failures without any restriction on the size of a storage array and the numbers of tolerable device failures and sector failures. We propose the upstairs encoding and downstairs encoding methods, which provide complementary performance advantages for different configurations. We conduct extensive experiments on STAIR codes in terms of space saving, encoding/decoding speed, and update cost. We demonstrate that STAIR codes not only improve space efficiency over traditional erasure codes, but also provide better computational efficiency than SD codes based on our special code construction. Finally, we present analytical models that characterize the reliability of STAIR codes, and show that the support of a wider range of configurations by STAIR codes is critical for tolerating sector failure bursts discovered in the field. © 2014 ACM.",Device failures; Erasure codes; Reliability analysis; Sector failures,Computational efficiency; Efficiency; Encoding (symbols); Forward error correction; Reliability analysis; Signal encoding; Code construction; Device failures; Encoding methods; Encoding/decoding; Erasure codes; Space efficiencies; Space overhead; Storage systems; Stairs
Evaluating phase change memory for enterprise storage systems: A study of caching and tiering approaches,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908892185&doi=10.1145%2f2668128&partnerID=40&md5=f77d2b03ab8b2e1d87a6ce0569455314,"Storage systems based on Phase Change Memory (PCM) devices are beginning to generate considerable attention in both industry and academic communities. But whether the technology in its current state will be a commercially and technically viable alternative to entrenched technologies such as flash-based SSDs remains undecided. To address this, it is important to consider PCM SSD devices not just from a device standpoint, but also from a holistic perspective. This article presents the results of our performance study of a recent all-PCM SSD prototype. The average latency for a 4KiB random read is 6.7μs, which is about 16x faster than a comparable eMLC flash SSD. The distribution of I/O response times is also much narrower than flash SSD for both reads and writes. Based on the performance measurements and real-world workload traces, we explore two typical storage use cases: tiering and caching. We report that the IOPS/$ of a tiered storage system can be improved by 12-66% and the aggregate elapsed time of a server-side caching solution can be improved by up to 35% by adding PCM. Our results show that (even at current price points) PCM storage devices show promising performance as a new component in enterprise storage systems. © 2014 ACM.",Enterprise workloads; Flash memory; Phase change memory,Flash memory; Flash-based SSDs; Virtual storage; Academic community; Entrenched technologies; Holistic perspectives; New components; Performance measurements; Performance study; Phase change memory (pcm); Storage systems; Phase change memory
Checking the integrity of transactional mechanisms,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908888874&doi=10.1145%2f2675113&partnerID=40&md5=3168657b5700fd2f5ec361d0834f874d,"Data corruption is the most common consequence of file-system bugs. When such corruption occurs, offline check and recovery tools must be used, but they are error prone and cause significant downtime. Previously we showed that a runtime checker for the Ext3 file system can verify that metadata updates are consistent, helping detect corruption in metadata blocks at transaction commit time. However, corruption can still occur when a bug in the file system's transactional mechanism loses, misdirects, or corrupts writes. We show that a runtime checker must enforce the atomicity and durability properties of the file system on every write, in addition to checking transactions at commit time, to provide the strong guarantee that every block write will maintain file system consistency. We identify the invariants that need to be enforced on journaling and shadow paging file systems to preserve the integrity of committed transactions. We also describe the key properties that make it feasible to check these invariants for a file system. Based on this characterization, we have implemented runtime checkers for Ext3 and Btrfs. Our evaluation shows that both checkers detect data corruption effectively, and they can be used during normal operation with low overhead. © 2014 ACM.",Atomicity; Btrfs; Durability; Ext3; File system checker; Metadata consistency; Runtime verification,Crime; Durability; File organization; Metadata; Atomicity; Btrfs; Ext3; File system checkers; Run-time verification; Program debugging
Agility and performance in elastic distributed storage,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908866297&doi=10.1145%2f2668129&partnerID=40&md5=1081e39152cc799858929ffdd2a6ecff,"Elastic storage systems can be expanded or contracted to meet current demand, allowing servers to be turned off or used for other tasks. However, the usefulness of an elastic distributed storage system is limited by its agility: how quickly it can increase or decrease its number of servers. Due to the large amount of data they must migrate during elastic resizing, state of the art designs usually have to make painful trade-offs among performance, elasticity, and agility. This article describes the state of the art in elastic storage and a new system, called SpringFS, that can quickly change its number of active servers, while retaining elasticity and performance goals. SpringFS uses a novel technique, termed bounded write offloading, that restricts the set of servers where writes to overloaded servers are redirected. This technique, combined with the read offloading and passive migration policies used in SpringFS, minimizes the work needed before deactivation or activation of servers. Analysis of real-world traces from Hadoop deployments at Facebook and various Cloudera customers and experiments with the SpringFS prototype confirm SpringFS's agility, show that it reduces the amount of data migrated for elastic resizing by up to two orders of magnitude, and show that it cuts the percentage of active servers required by 67-82%, outdoing state-of-the-art designs by 6-120%. 2014 Copyright held by the Owner/Author.",Agility; Cloud storage; Distributed file systems; Elastic storage; Power; Write offloading,Economic and social effects; Elasticity; File organization; Multiprocessing systems; Agility; Cloud storages; Distributed file systems; Elastic storage; Power; Write offloading; Digital storage
Random slicing: Efficient and scalable data placement for large-scale storage systems,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906494503&doi=10.1145%2f2632230&partnerID=40&md5=2d0bd2c7a83b33be3f3215df70b24413,"The ever-growing amount of data requires highly scalable storage solutions. The most flexible approach is to use storage pools that can be expanded and scaled down by adding or removing storage devices. To make this approach usable, it is necessary to provide a solution to locate data items in such a dynamic environment. This article presents and evaluates the Random Slicing strategy, which incorporates lessons learned from table-based, rule-based, and pseudo-randomized hashing strategies and is able to provide a simple and efficient strategy that scales up to handle exascale data. Random Slicing keeps a small table with information about previous storage system insert and remove operations, drastically reducing the required amount of randomness while delivering a perfect load distribution. © 2014 ACM.",PRNG; Randomized data distribution; Scalability; Storage management,Scalability; Storage management; Virtual storage; Data distribution; Dynamic environments; Efficient strategy; Large-scale storage systems; Load distributions; PRNG; Scalable storage; Storage systems; Information management
A lightweight data location service for nondeterministic exascale storage systems,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906515102&doi=10.1145%2f2629451&partnerID=40&md5=0109dd885a0164e515627a860991739b,"In this article, we present LWDLS, a lightweight data location service designed for Exascale storage systems (storage systems with order of 1018 bytes) and geo-distributed storage systems (large storage systems with physically distributed locations). LWDLS provides a search-based data location solution, and enables free data placement, movement, and replication. In LWDLS, probe and prune protocols are introduced that reduce topology mismatch, and a heuristic flooding search algorithm (HFS) is presented that achieves higher search efficiency than pure flooding search while having comparable search speed and coverage to the pure flooding search. LWDLS is lightweight and scalable in terms of incorporating low overhead, high search efficiency, no global state, and avoiding periodic messages. LWDLS is fully distributed and can be used in nondeterministic storage systems and in deterministic storage systems to deal with cases where search is needed. Extensive simulations modeling large-scale High Performance Computing (HPC) storage environments provide representative performance outcomes. Performance is evaluated by metrics including search scope, search efficiency, and average neighbor distance. Results show that LWDLS is able to locate data efficiently with low cost of state maintenance in arbitrary network environments. Through these simulations, we demonstrate the effectiveness of protocols and search algorithm of LWDLS. © 2014 ACM.",Efficient search; Exascale; Nondeterministic; Scalability; Storage,Efficiency; Energy storage; Floods; Learning algorithms; Location; Multiprocessing systems; Scalability; Arbitrary networks; Distributed storage system; Efficient search; Exascale; Extensive simulations; High performance computing (HPC); Nondeterministic; Performance outcome; Digital storage
Design and prototype of a solid-state cache,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906499794&doi=10.1145%2f2629491&partnerID=40&md5=613ee2730858badbbf47feff996d2325,"The availability of high-speed solid-state storage has introduced a new tier into the storage hierarchy. Lowlatency and high-IOPS solid-state drives (SSDs) cache data in front of high-capacity disks. However, most existing SSDs are designed to be a drop-in disk replacement, and hence are mismatched for use as a cache. This article describes FlashTier, a system architecture built upon a solid-state cache (SSC), which is a flash device with an interface designed for caching. Management software at the operating system block layer directs caching. The FlashTier design addresses three limitations of using traditional SSDs for caching. First, FlashTier provides a unified logical address space to reduce the cost of cache block management within both the OS and the SSD. Second, FlashTier provides a new SSC block interface to enable a warm cache with consistent data after a crash. Finally, FlashTier leverages cache behavior to silently evict data blocks during garbage collection to improve performance of the SSC. We first implement an SSC simulator and a cache manager in Linux to perform an in-depth evaluation and analysis of FlashTier's design techniques. Next, we develop a prototype of SSC on the OpenSSD Jasmine hardware platform to investigate the benefits and practicality of FlashTier design. Our prototyping experiences provide insights applicable to managing modern flash hardware, implementing other SSD prototypes and new OS storage stack interface extensions. Overall, we find that FlashTier improves cache performance by up to 168% over consumer-grade SSDs and up to 52% over high-end SSDs. It also improves flash lifetime for write-intensive workloads by up to 60% compared to SSD caches with a traditional flash interface. © 2014 ACM.",Consistency; Device interface; Durability; Prototype; Solid-state cache,Availability; Computer operating systems; Design; Digital storage; Durability; Hardware; Consistency; Device interfaces; Evaluation and analysis; Prototype; Solid-state cache; Solid-state storage; System architectures; Write-intensive workloads; Flash-based SSDs
Caching strategies for high-performance storage media,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906502738&doi=10.1145%2f2633691&partnerID=40&md5=85a27a98ba6c05ade49961c8114bcd57,"Due to the large access latency of hard disks during data retrieval in computer systems, buffer caching mechanisms have been studied extensively in database and operating systems. By storing requested data into the buffer cache, subsequent requests can be directly serviced without accessing slow disk storage. Meanwhile, high-speed storage media like PCM (phase-change memory) have emerged recently, and one may wonder if the traditional buffer cache will be still effective for these high-speed storage media. This article answers the question by showing that the buffer cache is still effective in such environments due to the software overhead and the bimodal data access characteristics. Based on this observation, we present a new buffer cache management scheme appropriately designed for the system where the speed gap between cache and storage is narrow. To this end, we analyze the condition that caching will be effective and find the characteristics of access patterns that can be exploited in managing buffer cache for high performance storage like PCM. © 2014 ACM.",Buffer cache; Caching; Hard disk; Phase-change memory,Cache memory; Hard disk storage; Phase change memory; Storage management; Access latency; Access patterns; Buffer cache managements; Buffer caches; Caching; Caching mechanism; Caching strategy; Data retrieval; Digital storage
Read-performance optimization for deduplication-based storage systems in the cloud,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897568312&doi=10.1145%2f2512348&partnerID=40&md5=b472ab32ae7939fdf72735cbebfc7391,"Data deduplication has been demonstrated to be an effective technique in reducing the total data transferred over the network and the storage space in cloud backup, archiving, and primary storage systems, such as VM (virtual machine) platforms. However, the performance of restore operations from a deduplicated backup can be significantly lower than that without deduplication. The main reason lies in the fact that a file or block is split into multiple small data chunks that are often located in different disks after deduplication, which can cause a subsequent read operation to invoke many disk IOs involving multiple disks and thus degrade the read performance significantly. While this problem has been by and large ignored in the literature thus far, we argue that the time is ripe for us to pay significant attention to it in light of the emerging cloud storage applications and the increasing popularity of the VM platform in the cloud. This is because, in a cloud storage or VM environment, a simple read request on the client side may translate into a restore operation if the data to be read or a VM suspended by the user was previously deduplicated when written to the cloud or the VM storage server, a likely scenario considering the network bandwidth and storage capacity concerns in such an environment. To address this problem, in this article, we propose SAR, an SSD (solid-state drive)-Assisted Read scheme, that effectively exploits the high random-read performance properties of SSDs and the unique data-sharing characteristic of deduplication-based storage systems by storing in SSDs the unique data chunks with high reference count, small size, and nonsequential characteristics. In this way, many read requests to HDDs are replaced by read requests to SSDs, thus significantly improving the read performance of the deduplication-based storage systems in the cloud. The extensive trace-driven and VM restore evaluations on the prototype implementation of SAR show that SAR outperforms the traditional deduplication-based and flash-based cache schemes significantly, in terms of the average response times. © 2014 ACM.",Data deduplication; Read performance; Solid-state drive; Storage systems; Virtual machine,Computer simulation; Restoration; Space platforms; Data de duplications; Read performance; Solid-state drives; Storage systems; Virtual machines; Digital storage
Towards high-performance SAN with fast storage devices,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897482604&doi=10.1145%2f2577385&partnerID=40&md5=440a2c8a190594eab3db1d4e74404e66,"Storage area network (SAN) is one of the most popular solutions for constructing server environments these days. In these kinds of server environments, HDD-based storage usually becomes the bottleneck of the overall system, but it is not enough to merely replace the devices with faster ones in order to exploit their high performance. In other words, proper optimizations are needed to fully utilize their performance gains. In this work, we first adopted a DRAM-based SSD as a fast backend-storage in the existing SAN environment, and found significant performance degradation compared to its own capabilities, especially in the case of small-sized random I/O pattern, even though a high-speed network was used. We have proposed three optimizations to solve this problem: (1) removing software overhead in the SAN I/O path; (2) increasing parallelism in the procedures for handling I/O requests; and (3) adopting the temporal merge mechanism to reduce network overheads. We have implemented them as a prototype and found that our approaches make substantial performance improvements by up to 39% and 280% in terms of both the latency and bandwidth, respectively. © 2014 ACM.",Fast storage; InfiniBand; RDMA; SAN; Storage stack optimization,Dynamic random access storage; Optimization; Virtual storage; Infiniband; Network overhead; Performance degradation; Performance Gain; RDMA; SAN; SAN environment; Storage area networks; Hard disk storage
Analytic models of SSD write performance,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897499397&doi=10.1145%2f2577384&partnerID=40&md5=2c63b37dc360df3db7874f1ceff47577,"Solid-state drives (SSDs) update data by writing a new copy, rather than overwriting old data, causing prior copies of the same data to be invalidated. These writes are performed in units of pages, while space is reclaimed in units of multipage erase blocks, necessitating copying of any remaining valid pages in the block before reclamation. The efficiency of this cleaning process greatly affects performance under random workloads; in particular, in SSDs, the write bottleneck is typically internal media throughput, and write amplification due to additional internal copying directly reduces application throughput. We present the first nearly-exact closed-form solution for write amplification under greedy cleaning for uniformly-distributed random traffic, validate its accuracy via simulation, and show that its inaccuracies are negligible for reasonable block sizes and over provisioning ratios. In addition, we also present the first models which predict performance degradation for both LRW (least-recently-written) cleaning and greedy cleaning under simple non-uniform traffic conditions; simulation results show the first model to be exact and the second to be accurate within 2%. We extend the LRW model to arbitrary combinations of random traffic and demonstrate its use in predicting cleaning performance for real-world workloads. Using these analytic models, we examine the strategy of separating ""hot"" and ""cold"" data, showing that for our traffic model, such separation eliminates any loss in performance due to nonuniform traffic. We then show how a system which segregates hot and cold data into different block pools may shift free space between these pools in order to achieve improved performance, and how numeric methods may be used with ourmodel to find the optimum operating point, which approaches a write amplification of 1.0 for increasingly skewed traffic. We examine online methods for achieving this optimal operating point and show a control strategy based on our model which achieves high performance for a number of real-world block traces. © 2014 ACM.",Flash memory; Solid-state drives; Solid-state storage systems; Write amplification,Amplification; Analytical models; Cleaning; Flash memory; Lakes; Closed form solutions; Control strategies; Nonuniform traffic; Optimal operating point; Performance degradation; Solid-state drives; Solid-state storage; Write amplifications; Digital storage
Beyond MTTDL: A closed-form RAID 6 reliability equation,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897547383&doi=10.1145%2f2577386&partnerID=40&md5=c410e4b4980791014d49e9c2db205a2d,"We introduce a new closed-form equation for estimating the number of data-loss events for a redundant array of inexpensive disks in a RAID-6 configuration. The equation expresses operational failures, their restorations, latent (sector) defects, and disk media scrubbing by time-based distributions that can represent non-homogeneous Poisson processes. It uses two-parameter Weibull distributions that allows the distributions to take on many different shapes, modeling increasing, decreasing, or constant occurrence rates. This article focuses on the statistical basis of the equation. It also presents time-based distributions of the four processes based on an extensive analysis of field data collected over several years from 10,000s of commercially available systems with 100,000s of disk drives. Our results for RAID-6 groups of size 16 indicate that the closed-form expression yields much more accurate results compared to the MTTDL reliability equation and matching computationally- intensive Monte Carlo simulations. © 2014 ACM.",RAID reliability; Storage systems,Digital storage; Monte Carlo methods; Reliability; Closed form; Closed-form equations; Closed-form expression; Different shapes; Non-homogeneous Poisson process; Operational failures; Reliability equations; Storage systems; Weibull distribution
Sector-disk (SD) erasure codes for mixed failure modes in RAID systems,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893212576&doi=10.1145%2f2560013&partnerID=40&md5=506d7f244031aa8afb8bcbde635e8d03,"Traditionally, when storage systems employ erasure codes, they are designed to tolerate the failures of entire disks. However, the most common types of failures are latent sector failures, which only affect individual disk sectors, and block failures which arise through wear on SSD's. This article introduces SD codes, which are designed to tolerate combinations of disk and sector failures. As such, they consume far less storage resources than traditional erasure codes.We specify the codes with enough detail for the storage practitioner to employ them, discuss their practical properties, and detail an open-source implementation. © 2014 ACM.",Erasure codes; Fault tolerance; RAID; Sector failures; Storage systems,Data storage equipment; Fault tolerance; Hardware; Erasure codes; Open source implementation; RAID; RAID systems; Storage resources; Storage systems; Forward error correction
A unified buffer cache architecture that subsumes journaling functionality via nonvolatile memory,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893224439&doi=10.1145%2f2560010&partnerID=40&md5=06f79d25d2933055e4ea718efe006c07,"Journaling techniques are widely used in modern file systems as they provide high reliability and fast recovery from system failures. However, it reduces the performance benefit of buffer caching as journaling accounts for a bulk of the storage writes in real system environments. To relieve this problem, we present a novel buffer cache architecture that subsumes the functionality of caching and journaling by making use of nonvolatile memory such as PCM or STT-MRAM. Specifically, our buffer cache supports what we call the in-place commit scheme. This scheme avoids logging, but still provides the same journaling effect by simply altering the state of the cached block to frozen. As a frozen block still provides the functionality of a cache block, we show that in-place commit does not degrade cache performance. We implement our scheme on Linux 2.6.38 and measure the throughput and execution time of the scheme with various file I/O benchmarks. The results show that our scheme improves the throughput and execution time by 89% and 34% on average, respectively, compared to the existing Linux buffer cache with ext4 without any loss of reliability. © 2014 ACM.",Buffer cache; File system; Journaling; Nonvolatile memory; Reliability,Computer operating systems; File organization; MRAM devices; Reliability; Systems engineering; Buffer caches; Cache performance; File systems; High reliability; Journaling; Non-volatile memory; Performance benefits; System failures; Cache memory
A study of Linux file system evolution,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893329712&doi=10.1145%2f2560012&partnerID=40&md5=494734e567a884cf87a6e340613345a4,"We conduct a comprehensive study of file-system code evolution. By analyzing eight years of Linux filesystem changes across 5079 patches, we derive numerous new (and sometimes surprising) insights into the file-system development process; our results should be useful for both the development of file systems themselves as well as the improvement of bug-finding tools. © 2014 ACM.",Bug; Failure; File systems; Patch; Performance; Reliability,Computer operating systems; Computer system recovery; Reliability; Bug; Bug-finding tool; Development process; File systems; Filesystem; Linux file system; Patch; Performance; File organization
Ffsck: The fast file-system checker,2014,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893308643&doi=10.1145%2f2560011&partnerID=40&md5=bb24cf2f43918290aa71bee422619984,"Failures, errors, and bugs can corrupt file systems and cause data loss, despite the presence of journals and similar preventive techniques. While consistency checkers such as fsck can detect corruption and repair a damaged image, they are generally created as an afterthought, to be run only at rare intervals. Thus, checkers operate slowly, causing significant downtime for large scale storage systems. We address this dilemma by treating the checker as a key component of the overall file system, rather than a peripheral add-on. To this end, we present a modified ext3 file system, rext3, to directly support the fast file-system checker, ffsck. Rext3 colocates and self-identifies its metadata blocks, removing the need for costly seeks and tree traversals during checking. These modifications allow ffsck to scan and repair the file system at rates approaching the full sequential bandwidth of the underlying device. In addition, we demonstrate that rext3 generally performs competitively with ext3 and exceeds it in handling random reads and large writes. Finally, we apply our principles to FreeBSD's FFS file system and its checker, doing so in a lightweight fashion that preserves the file-system layout while still providing some of the performance gains from ffsck. © 2014 ACM.",File-system checking; File-system consistency,File organization; Repair; Data loss; File systems; Filesystem; FreeBSD; Large-scale storage systems; Performance Gain; Tree traversal; Program debugging
Design and evaluation of a new approach to raid-0 scaling,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890326235&doi=10.1145%2f2491054&partnerID=40&md5=98e25e6996efaac56efb2a6a80c23cfa,"Scaling up a RAID-0 volume with added disks can increase its storage capacity and I/O bandwidth simultaneously. For preserving a round-robin data distribution, existing scaling approaches require all the data to be migrated. Such large data migration results in a long redistribution time as well as a negative impact on application performance. In this article, we present a new approach to RAID-0 scaling called FastScale. First, FastScale minimizes data migration, while maintaining a uniform data distribution. It moves only enough data blocks from old disks to fill an appropriate fraction of new disks. Second, FastScale optimizes data migration with access aggregation and lazy checkpoint. Access aggregation enables data migration to have a larger throughput due to a decrement of disk seeks. Lazy checkpoint minimizes the number of metadata writes without compromising data consistency. Using several real system disk traces, we evaluate the performance of FastScale through comparison with SLAS, one of the most efficient existing scaling approaches. The experiments show that FastScale can reduce redistribution time by up to 86.06% with smaller application I/O latencies. The experiments also illustrate that the performance of RAID-0 scaled using FastScale is almost identical to, or even better than, that of the round-robin RAID-0. ©c 2013 ACM 1553-3077/2013/11-ART11 $15.00.",Access aggregation; Data migration; Lazy checkpoint; RAID scaling,Data storage equipment; Hardware; Application performance; Data consistency; Data distribution; Data migration; Design and evaluations; Lazy checkpoint; RAID scaling; Storage capacity; Experiments
Hybrid associative flash translation layer for the performance optimization of chip-level parallel flash memory,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890404158&doi=10.1145%2f2535931&partnerID=40&md5=c63c2d969e67a3c1c438e71d4b254f4d,"Flash memory is used widely in the data storage market, particularly low-price MultiLevel Cell (MLC) flash memory, which has been adopted by large-scale storage systems despite its low performance. To overcome the poor performance of MLC flash memory, a system architecture has been designed to optimize chip-level parallelism. This design increases the size of the page unit and the block unit, thereby simultaneously executing operations on multiple chips. Unfortunately, its Flash Translation Layer (FTL) generates many unused sectors in each page, which leads to unnecessary write operations. Furthermore, it reuses an earlier log block scheme, although it generates many erase operations because of its low space utilization. To solve these problems, we propose a hybrid associative FTL (Hybrid-FTL) to enhance the performance of the chiplevel parallel flash memory system. Hybrid-FTL reduces the number of write operations by utilizing all of the unused sectors. Furthermore, it reduces the overall number of erase operations by classifying data as hot, cold, or fragment data. Hybrid-FTL requires less mapping information in the DRAM and in the flash memory compared with previous FTL algorithms. © 2013 ACM 1553-3077/2013/11-ART13 $15.00.",Endurance; Flash memory; Flash translation layer; Solid state drive; Write amplification,Digital storage; Durability; Dynamic random access storage; Optimization; Flash translation layer; Large-scale storage systems; Multilevel cell flashes; Parallel flash memory; Performance optimizations; Solid state drives; System architectures; Write amplifications; Flash memory
Evaluation of a hybrid approach for efficient provenance storage,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890320238&doi=10.1145%2f2501986&partnerID=40&md5=705b66dc9ab010978466e01e21411540,"Provenance is the metadata that describes the history of objects. Provenance provides new functionality in a variety of areas, including experimental documentation, debugging, search, and security. As a result, a number of groups have built systems to capture provenance. Most of these systems focus on provenance collection, a few systems focus on building applications that use the provenance, but all of these systems ignore an important aspect: efficient long-term storage of provenance. In this article, we first analyze the provenance collected from multiple workloads and characterize the properties of provenance with respect to long-term storage. We then propose a hybrid scheme that takes advantage of the graph structure of provenance data and the inherent duplication in provenance data. Our evaluation indicates that our hybrid scheme, a combination of Web graph compression (adapted for provenance) and dictionary encoding, provides the best trade-off in terms of compression ratio, compression time, and query performance when compared to other compression schemes. © 2013 ACM 1553-3077/2013/11-ART14 $15.00.",Dictionary encoding; Provenance graphs; Storage; Web compression,Encoding (symbols); Energy storage; Building applications; Compression scheme; Graph structures; Hybrid approach; Hybrid scheme; Long-term storage; Provenance graphs; Query performance; Digital storage
Datacenter scale evaluation of the impact of temperature on hard disk drive failures,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885666859&doi=10.1145%2f2491472.2491475&partnerID=40&md5=e9b1dd2e3d6753635c04faa0166d2980,"With the advent of cloud computing and online services, large enterprises rely heavily on their datacenters to serve end users. A large datacenter facility incurs increased maintenance costs in addition to service unavailability when there are increased failures. Among different server components, hard disk drives are known to contribute significantly to server failures; however, there is very little understanding of the major determinants of disk failures in datacenters. In this work, we focus on the interrelationship between temperature, workload, and hard disk drive failures in a large scale datacenter. We present a dense storage case study from a population housing thousands of servers and tens of thousands of disk drives, hosting a large-scale online service at Microsoft. We specifically establish correlation between temperatures and failures observed at different location granularities: (a) inside drive locations in a server chassis, (b) across server locations in a rack, and (c) across multiple racks in a datacenter. We show that temperature exhibits a stronger correlation to failures than the correlation of disk utilization with drive failures. We establish that variations in temperature are not significant in datacenters and have little impact on failures. We also explore workload impacts on temperature and disk failures and show that the impact of workload is not significant. We then experimentally evaluate knobs that control disk drive temperature, including workload and chassis design knobs. We corroborate our findings from the real data study and show that workload knobs show minimal impact on temperature. Chassis knobs like disk placement and fan speeds have a larger impact on temperature. Finally, we also show the proposed cost benefit of temperature optimizations that increase hard disk drive reliability. © 2013 ACM.",Datacenter; Hard disk drives; Temperature impact,Chassis; Knobs; Windows operating system; Datacenter; Disk utilization; Hard Disk Drive; Impact of temperatures; Large enterprise; Server components; Temperature impact; Temperature optimization; Hard disk storage
Generalized optimal response time retrieval of replicated data from storage arrays,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885647362&doi=10.1145%2f2491472.2491474&partnerID=40&md5=b2281891084567dbf235737768d09d0b,"Declustering techniques reduce query response times through parallel I/O by distributing data among parallel disks. Recently, replication-based approaches were proposed to further reduce the response time. Efficient retrieval of replicated data from multiple disks is a challenging problem. Existing retrieval techniques are designed for storage arrays with identical disks, having no initial load or network delay. In this article, we consider the generalized retrieval problem of replicated data where the disks in the system might be heterogeneous, the disks may have initial load, and the storage arrays might be located on different sites. We first formulate the generalized retrieval problem using a Linear Programming (LP) model and solve it with mixed integer programming techniques. Next, the generalized retrieval problem is formulated as a more efficient maximum flow problem. We prove that the retrieval schedule returned by the maximum flow technique yields the optimal response time and this result matches the LP solution. We also propose a low-complexity online algorithm for the generalized retrieval problem by not guaranteeing the optimality of the result. Performance of proposed and state of the art retrieval strategies are investigated using various replication schemes, query types, query loads, disk specifications, network delays, and initial loads. © 2013 ACM.",Declustering; Generalized retrieval; Linear programming; Maximum flow; Replication; Storage arrays,Complex networks; Digital storage; Linear programming; Declustering; Generalized retrieval; Maximum flows; Replication; Storage arrays; Search engines
DepSky: Dependable and secure storage in a cloud-of-clouds,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886483953&doi=10.1145%2f2535929&partnerID=40&md5=1f859911b123016d06a0e2143aa51bf0,"The increasing popularity of cloud storage services has lead companies that handle critical data to think about using these services for their storage needs. Medical record databases, large biomedical datasets, historical information about power systems and financial data are some examples of critical data that could be moved to the cloud. However, the reliability and security of data stored in the cloud still remain major concerns. In this work we present DepSky, a system that improves the availability, integrity, and confidentiality of information stored in the cloud through the encryption, encoding, and replication of the data on diverse clouds that form a cloud-of-clouds. We deployed our system using four commercial clouds and used PlanetLab to run clients accessing the service from different countries. We observed that our protocols improved the perceived availability, and in most cases, the access latency, when compared with cloud providers individually. Moreover, the monetary costs of using DepSky in this scenario is at most twice the cost of using a single cloud, which is optimal and seems to be a reasonable cost, given the benefits. © 2013 ACM 1553-3077/2013/11-ART12 $15.00.",Byzantine quorum systems; Cloud computing; Cloud storage,Cloud computing; Costs; Security of data; Byzantine quorum systems; Cloud providers; Cloud storage services; Cloud storages; Financial data; Historical information; Medical record; Monetary costs; Digital storage
Exploiting redundancies and deferred writes to conserve energy in erasure-coded storage clusters,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885588453&doi=10.1145%2f2491472.2491473&partnerID=40&md5=6d58971065fc57fdc843664538c38d11,"We present a power-efficient scheme for erasure-coded storage clusters-ECS2-which aims to offer high energy efficiency with marginal reliability degradation. ECS2 utilizes data redundancies and deferred writes to conserve energy. In ECS2 parity blocks are buffered exclusively in active data nodes whereas parity nodes are placed into low-power mode. (k+r, k) RS-coded ECS2 can achieve (r + 1)/2-fault tolerance for k active data nodes and r-fault tolerance for all k+r nodes. ECS2 employs the following three optimizing approaches to improve the energy efficiency of storage clusters. (1) An adaptive threshold policy takes system configurations and I/O workloads into account to maximize standby time periods, (2) a selective activation policy minimizes the number of power-transitions in storage nodes; and (3) a region-based buffer policy speeds up the synchronization process by migrating parity blocks in a batch method. After implementing an ECS2- based prototype in a Linux cluster, we evaluated its energy efficiency and performance using four different types of I/O workloads. The experimental results indicate that compared to energy-oblivious erasure-coded storage, ECS2 can save the energy used by storage clusters up to 29.8% and 28.0% in read-intensive and write-dominated workloads when k = 6 and r = 3, respectively. The results also show that ECS2 accomplishes high power efficiency in both normal and failed cases without noticeably affecting the I/O performance of storage clusters. © 2013 ACM.",Clustered storage system; Erasure codes; Power efficiency; Selective activation policy,Computer operating systems; Energy efficiency; Fault tolerance; Clustered storage; Efficiency and performance; Erasure codes; High power efficiencies; Power efficiency; Reliability degradation; Selective activation; Synchronization process; Digital storage
BTRFS: The linux B-tree filesystem,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883561832&doi=10.1145%2f2501620.2501623&partnerID=40&md5=d4cf554cbd48a6910a3ab4e0c8d71756,"BTRFS is a Linux filesystem that has been adopted as the default filesystem in some popular versions of Linux. It is based on copy-on-write, allowing for efficient snapshots and clones. It uses B-trees as its main on-disk data structure. The design goal is to work well for many use cases and workloads. To this end, much effort has been directed to maintaining even performance as the filesystem ages, rather than trying to support a particular narrow benchmark use-case. Linux filesystems are installed on smartphones as well as enterprise servers. This entails challenges on many different fronts. Scalability. The filesystem must scale in many dimensions: disk space, memory, and CPUs. Data integrity. Losing data is not an option, and much effort is expended to safeguard the content. This includes checksums, metadata duplication, and RAID support built into the filesystem. Disk diversity. The system should work well with SSDs and hard disks. It is also expected to be able to use an array of different sized disks, which poses challenges to the RAID and striping mechanisms. This article describes the core ideas, data structures, and algorithms of this filesystem. It sheds light on the challenges posed by defragmentation in the presence of snapshots, and the tradeoffs required to maintain even performance in the face of a wide spectrum of workloads. © 2013 ACM 1553-3077/2013/08-ART7 15.00.",B-trees; Concurrency; Copy-on-write; Filesystem; RAID; Shadowing; Snapshots,Benchmarking; Data structures; Hard disk storage; Program processors; B trees; Concurrency; Copy-on-write; Filesystem; RAID; Shadowing; Snapshots; Computer operating systems
SCMFS: A file system for storage class memory and its extensions,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883574737&doi=10.1145%2f2501620.2501621&partnerID=40&md5=e984630c6ed3a17853728e4c15e3070b,"Modern computer systems have been built around the assumption that persistent storage is accessed via a slow, block-based interface. However, emerging nonvolatile memory technologies (sometimes referred to as storage class memory (SCM)), are poised to revolutionize storage systems. The SCM devices can be attached directly to the memory bus and offer fast, fine-grained access to persistent storage. In this article, we propose a new file system SCMFS, which is specially designed for Storage Class Memory. SCMFS is implemented on the virtual address space and utilizes the existing memory management module of the operating system to help mange the file system space. As a result, we largely simplified the file system operations of SCMFS, which allowed us a better exploration of performance gain from SCM. We have implemented a prototype in Linux and evaluated its performance through multiple benchmarks. The experimental results show that SCMFS outperforms other memory resident file systems, tmpfs, ramfs and ext2 on ramdisk, and achieves about 70% of memory bandwidth for file read/write operations. © 2013 ACM 1553-3077/2013/08-ART7 15.00.",File systems; Performance measurement; Storage class memory; Storage management,Benchmarking; Computer operating systems; File organization; Storage management; Emerging Non-volatile memory technology; File systems; Modern computer systems; Performance measurements; Persistent storage; Read/write operations; Storage-class memory; Virtual address space; Nonvolatile storage
Dynamic synchronous/asynchronous replication,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883588931&doi=10.1145%2f2508011&partnerID=40&md5=407df00fa2661a34b7ba0b126026df16,"Online, remote, data replication is critical for today's enterprise IT organization. Availability of data is key to the success of the organization. A few hours of downtime can cost from thousands to millions of dollars With increasing frequency, companies are instituting disaster recovery plans to ensure appropriate data availability in the event of a catastrophic failure or disaster that destroys a site (e.g. flood, fire, or earthquake). Synchronous and asynchronous replication technologies have been available for a long period of time. Synchronous replication has the advantage of no data loss, but due to latency, synchronous replication is limited by distance and bandwidth. Asynchronous replication on the other hand has no distance limitation, but leads to some data loss which is proportional to the data lag. We present a novel method, implemented within EMC Recover-Point, which allows the system to dynamically move between these replication options without any disruption to the I/O path. As latency grows, the system will move from synchronous replication to semi-synchronous replication and then to snapshot shipping. It returns to synchronous replication as more bandwidth is available and latency allows. © 2013 ACM 1553-3077/2013/08-ART7 15.00.",Remote replication,Bandwidth; Disasters; Asynchronous replication; Catastrophic failures; Data availability; Data replication; Disaster recovery plan; Dynamic synchronous; Remote replication; Synchronous replication; Industry
A prefetching scheme exploiting both data layout and access history on disk,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883564491&doi=10.1145%2f2508010&partnerID=40&md5=431504522e6b5576e036740c6cfa373e,"Prefetching is an important technique for improving effective hard disk performance. A prefetcher seeks to accurately predict which data will be requested and load it ahead of the arrival of the corresponding requests. Current disk prefetch policies in major operating systems track access patterns at the level of file abstraction. While this is useful for exploiting application-level access patterns, for two reasons filelevel prefetching cannot realize the full performance improvements achievable by prefetching. First, certain prefetch opportunities can only be detected by knowing the data layout on disk, such as the contiguous layout of file metadata or data from multiple files. Second, nonsequential access of disk data (requiring disk head movement) is much slower than sequential access, and the performance penalty for mis-prefetching a randomly located block, relative to that of a sequential block, is correspondingly greater. To overcome the inherent limitations of prefetching at logical file level, we propose to perform prefetching directly at the level of disk layout, and in a portable way. Our technique, called DiskSeen, is intended to be supplementary to, and to work synergistically with, any present file-level prefetch policies. DiskSeen tracks the locations and access times of disk blocks and, based on analysis of their temporal and spatial relationships, seeks to improve the sequentiality of disk accesses and overall prefetching performance. It also implements a mechanism to minimize mis-prefetching, on a per-application basis, to mitigate the corresponding performance penalty. Our implementation of the DiskSeen scheme in the Linux 2.6 kernel shows that it can significantly improve the effectiveness of prefetching, reducing execution times by 20%-60% for microbenchmarks and real applications such as grep, CVS, and TPC-H. Even for workloads specifically designed to expose its weaknesses, DiskSeen incurs only minor performance loss. © 2013 ACM 1553-3077/2013/08-ART7 15.00.",Buffer cache; Hard disk; Prefetching; Spatial locality,Data storage equipment; Hard disk storage; Hardware; Buffer caches; Inherent limitations; Performance penalties; Pre-fetching scheme; Prefetching; Prefetching performance; Spatial locality; Temporal and spatial; Computer operating systems
Improving bandwidth efficiency for consistent multistream storage,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875722545&doi=10.1145%2f2435204.2435206&partnerID=40&md5=6b02a157e9ff1be65eb50347fbabd5b2,"Synchronous small writes play a critical role in system availability because they safely log recent state modifications for fast recovery from crashes. Demanding systems typically dedicate separate devices to logging for adequate performance during normal operation and redundancy during state reconstruction. However, storage stacks enforce page-sized granularity in data transfers from memory to disk. Thus, they consume excessive storage bandwidth to handle small writes, which hurts performance. The problem becomes worse, as filesystems often handle multiple concurrent streams, which effectively generate random I/O traffic. In a journaled filesystem, we introduce wasteless journaling as a mount mode that coalesces synchronous concurrent small writes of data into full page-sized journal blocks. Additionally, we propose selective journaling to automatically activate wasteless journaling on data writes with size below a fixed threshold. We implemented a functional prototype of our design over a widely-used filesystem. Our modes are compared against existing methods using microbenchmarks and application-level workloads on stand-alone servers and a mul-titier networked system. We examine synchronous and asynchronous writes. Coalescing small data updates to the journal sequentially preserves filesystem consistency while it reduces consumed bandwidth up to several factors, decreases recovery time up to 22%, and lowers write latency up to orders of magnitude. © 2013 ACM.",Concurrency; Journaling; Logging; Small writes,Bandwidth; Data transfer; Flocculation; Logging (forestry); Bandwidth efficiency; Concurrency; Functional Prototypes; Journaling; Orders of magnitude; Small writes; State reconstruction; System availability; Digital storage
Ursa: Scalable load and power management in cloud storage systems,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875713901&doi=10.1145%2f2435204.2435205&partnerID=40&md5=b7abbead3b24aa48742e5deed861d297,"Enterprise and cloud data centers are comprised of tens of thousands of servers providing petabytes of storage to a large number of users and applications. At such a scale, these storage systems face two key challenges: (1) hot-spots due to the dynamic popularity of stored objects; and (2) high operational costs due to power and cooling. Existing storage solutions, however, are unsuitable to address these challenges because of the large number of servers and data objects. This article describes the design, implementation, and evaluation of Ursa, a system that scales to a large number of storage nodes and objects, and aims to minimize latency and bandwidth costs during system reconfiguration. Toward this goal, Ursa formulates an optimization problem that selects a subset of objects from hot-spot servers and performs topology-aware migration to minimize reconfiguration costs. As exact optimization is computationally expensive, we devise scalable approximation techniques for node selection and efficient divide-and-conquer computation. We also show that the same dynamic reconfiguration techniques can be leveraged to reduce power costs by dynamically migrating data off under-utilized nodes, and powering up servers neighboring existing hot-spots to reduce reconfiguration costs. Our evaluation shows that Ursa achieves cost-effective load management, is time-responsive in computing placement decisions (e.g., about two minutes for 10K nodes and 10M objects), and provides power savings of 15%-37%. © 2013 ACM.",Linear programming; Load management; Optimization; Power management; Storage,Cooling systems; Costs; Dynamic models; Electric load management; Energy management; Energy storage; Linear programming; Optimization; Approximation techniques; Cloud storage systems; Dynamic reconfiguration techniques; Load and power managements; Optimization problems; Power managements; Reconfiguration costs; System reconfiguration; Digital storage
Pyramid codes: Flexible schemes to trade space for access efficiency in reliable data storage systems,2013,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875724776&doi=10.1145%2f2435204.2435207&partnerID=40&md5=15de0275fe9c8dd957158fecc2f675e1,"We design flexible schemes to explore the tradeoffs between storage space and access efficiency in reliable data storage systems. Aiming at this goal, two new classes of erasure-resilient codes are introduced - Basic Pyramid Codes (BPC) and Generalized Pyramid Codes (GPC). Both schemes require slightly more storage space than conventional schemes, but significantly improve the critical performance of read during failures and unavailability. As a by-product, we establish a necessary matching condition to characterize the limit of failure recovery, that is, unless the matching condition is satisfied, a failure case is impossible to recover. In addition, we define a maximally recoverable (MR) property. For all ERC schemes holding the MR property, the matching condition becomes sufficient, that is, all failure cases satisfying the matching condition are indeed recoverable. We show that GPC is the first class of non-MDS schemes holding the MR property. © 2013 ACM.",Erasure codes; Fault tolerance; Reconstruction; Storage,Commerce; Energy storage; Fault tolerance; Image reconstruction; Access efficiencies; Conventional schemes; Data storage systems; Erasure codes; Failure recovery; Matching condition; Storage spaces; Trade space; Data storage equipment
Editorial note,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871252194&doi=10.1145%2f2385603.2385604&partnerID=40&md5=9c3aac801d9494f612b9998550f44738,[No abstract available],,
Revisiting storage for smartphones,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871233143&doi=10.1145%2f2385603.2385607&partnerID=40&md5=284e1d78933070b876f474c35127d013,"Conventional wisdom holds that storage is not a big contributor to application performance on mobile devices. Flash storage (the type most commonly used today) draws little power, and its performance is thought to exceed that of the network subsystem. In this article, we present evidence that storage performance does indeed affect the performance of several common applications such as Web browsing, maps, application install, email, and Facebook. For several Android smartphones, we find that just by varying the underlying flash storage, performance over WiFi can typically vary between 100% and 300% across applications; in one extreme scenario, the variation jumped to over 2000%. With a faster network (set up over USB), the performance variation rose even further. We identify the reasons for the strong correlation between storage and application performance to be a combination of poor flash device performance, random I/O from application databases, and heavy-handed use of synchronous writes. Based on our findings, we implement and evaluate a set of pilot solutions to address the storage performance deficiencies in smartphones. © 2012 ACM.",Android; Mobile; Mobile storage; Smartphones; Storage systems,Mobile devices; Robots; Android; Application performance; Facebook; Flash devices; Flash storage; Mobile; Mobile storage; Performance variations; Storage performance; Storage systems; Strong correlation; Smartphones
Recon: Verifying file system consistency at runtime,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871184004&doi=10.1145%2f2385603.2385608&partnerID=40&md5=dd10d96ab183603d022ed6835f17cae6,"File system bugs that corrupt metadata on disk are insidious. Existing reliability methods, such as checksums, redundancy, or transactional updates, merely ensure that the corruption is reliably preserved. Typical workarounds, based on using backups or repairing the file system, are painfully slow. Worse, the recovery may result in further corruption. We present Recon, a system that protects file system metadata from buggy file system operations. Our approach leverages file systems that provide crash consistency using transactional updates.We define declarative statements called consistency invariants for a file system. These invariants must be satisfied by each transaction being committed to disk to preserve file system integrity. Recon checks these invariants at commit, thereby minimizing the damage caused by buggy file systems. The major challenges to this approach are specifying invariants and interpreting file system behavior correctly without relying on the file system code. Recon provides a framework for file-system specific metadata interpretation and invariant checking. We show the feasibility of interpreting metadata and writing consistency invariants for the Linux ext3 file system using this framework. Recon can detect random as well as targeted file-system corruption at runtime as effectively as the offline e2fsck file-system checker, with low overhead. © 2012 ACM.",File system checker; Metadata consistency; Runtime verification,Computer operating systems; Crime; Redundancy; File systems; Invariant checking; Low overhead; Offline; Reliability methods; Run-time verification; Runtimes; Metadata
WAN-optimized replication of backup datasets using stream-informed delta compression,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871214454&doi=10.1145%2f2385603.2385606&partnerID=40&md5=a4c783576e9c5b7fe6561f22453f8c71,"Replicating data off site is critical for disaster recovery reasons, but the current approach of transferring tapes is cumbersome and error prone. Replicating across a wide area network (WAN) is a promising alternative, but fast network connections are expensive or impractical in many remote locations, so improved compression is needed to makeWAN replication truly practical.We present a new technique for replicating backup datasets across a WAN that not only eliminates duplicate regions of files (deduplication) but also compresses similar regions of files with delta compression, which is available as a feature of EMC Data Domain systems. Our main contribution is an architecture that adds stream-informed delta compression to already existing deduplication systems and eliminates the need for new, persistent indexes. Unlike techniques based on knowing a file's version or that use a memory cache, our approach achieves delta compression across all data replicated to a server at any time in the past. From a detailed analysis of datasets and statistics from hundreds of customers using our product, we achieve an additional 2X compression from delta compression beyond deduplication and local compression, which enables customers to replicate data that would otherwise fail to complete within their backup window. © 2012 ACM.",Backup storage; Deduplication; Delta compression; Network replication,Digital storage; Distributed computer systems; Wide area networks; Data domains; Data sets; Deduplication; Delta compression; Disaster recovery; Error prones; Local compression; Network connection; Remote location; Data compression
Introduction to the special issue USENIX FAST 2012,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871222677&doi=10.1145%2f2385603.2385605&partnerID=40&md5=be8bd6b20a9e5bcc88a087653ac2811d,[No abstract available],,
File system virtual appliances: Portable file system implementations,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870219664&doi=10.1145%2f2339118.2339120&partnerID=40&md5=35c92f7d7ada0d29e858790245579922,"File system virtual appliances (FSVAs) address the portability headaches that plague file system (FS) developers. By packaging their FS implementation in a virtual machine (VM), separate from the VM that runs user applications, they can avoid the need to port the file system to each operating system (OS) and OS version. A small FS-agnostic proxy, maintained by the core OS developers, connects the FSVA to whatever OS the user chooses. This article describes an FSVA design that maintains FS semantics for unmodified FS implementations and provides desired OS and virtualization features, such as a unified buffer cache and VM migration. Evaluation of prototype FSVA implementations in Linux and NetBSD, using Xen as the virtual machine manager (VMM), demonstrates that the FSVA architecture is efficient, FS-agnostic, and able to insulate file system implementations from OS differences that would otherwise require explicit porting. © 2012 ACM.",File systems; Operating systems; Virtual machines,Computer operating systems; Managers; Semantics; Buffer caches; File systems; Virtual appliance; Virtual machines; Virtual-machine managers; Virtualizations; Computer simulation
Generalized X-code: An efficient RAID-6 code for arbitrary size of disk array,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870208213&doi=10.1145%2f2339118.2339121&partnerID=40&md5=aad18986fa8b0d2d173cb042a4c3ab43,"Many RAID-6 codes have been proposed in the literature, but each has its limitations. Horizontal code has the ability to adapt to the arbitrary size of a disk array but its high computational complexity is a major shortcoming. In contrast, the computational complexity of vertical code (e.g. X-code) often achieves the theoretical optimality, but vertical code is limited to using a prime number as the size of the disk array In this article, we propose a novel efficient RAID-6 code for arbitrary size of disk array: generalized X-code. We move the redundant elements along their calculation diagonals in X-code onto two specific disks and change two data elements into redundant elements in order to realize our new code. The generalized X-code achieves optimal encoding and updating complexity and low decoding complexity; in addition, it has the ability to adapt to arbitrary size of disk array. Furthermore, we also provide a method for generalizing horizontal code to achieve optimal encoding and updating complexity while keeping the code's original ability to adapt to arbitrary size of disk array. © 2012 ACM.",Computational complexity; Generalized X-code; Number of disks; Size of disk array; Storage,Computational complexity; Digital storage; Encoding (symbols); Energy storage; Optimization; Data elements; Disk array; Generalized X-code; Low decoding complexity; Number of disks; Optimal encoding; Optimality; Prime number; Optimal systems
Efficient cooperative backup with decentralized trust management,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870179309&doi=10.1145%2f2339118.2339119&partnerID=40&md5=817d965af98629f4d8924b86ca0b5069,"Existing backup systems are unsatisfactory: commercial backup services are reliable but expensive while peer-to-peer systems are cheap but offer limited assurance of data reliability. This article introduces Friendstore, a system that provides inexpensive and reliable backup by giving users the choice to store backup data only on nodes they trust (typically those owned by friends and colleagues). Because it is built on trusted nodes, Friendstore is not burdened by the complexity required to cope with potentially malicious participants. Friendstore only needs to detect and repair accidental data loss and to ensure balanced storage exchange. The disadvantage of using only trusted nodes is that Friendstore cannot achieve perfect storage utilization. Friendstore is designed for a heterogeneous environment where nodes have very different access link speeds and available disk spaces. To ensure long-term data reliability, a node with limited upload bandwidth refrains from storing more data than its calculated maintainable capacity. A high bandwidth node might be limited by its available disk space. We introduce a simple coding scheme, called XOR(1,2), which doubles a node's ability to store backup information in the same amount of disk space at the cost of doubling the amount of data transferred during restore. Analysis and simulations using long-term node activity traces show that a node can reliably back up tens of gigabytes of data even with low upload bandwidth. © 2012 ACM.",Cooperative backup; Erasure code; Social network,Bandwidth; Access links; Analysis and simulation; Back up; Back-up systems; Backup service; Coding scheme; Cooperative backup; Data loss; Data reliability; Disk space; Erasure codes; Heterogeneous environments; High bandwidth; Malicious participant; Peer-to-Peer system; Social Networks; Storage utilization; Trust management; Digital storage
Transparent online storage compression at the block-level,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863607148&doi=10.1145%2f2180905.2180906&partnerID=40&md5=92ddf57cd260fe7fcc5b69792d7edad3,"In this work, we examine how transparent block-level compression in the I/O path can improve both the space efficiency and performance of online storage. We present ZBD, a block-layer driver that transparently compresses and decompresses data as they flow between the file-system and storage devices. Our system provides support for variable-size blocks, metadata caching, and persistence, as well as block allocation and cleanup. ZBD targets maintaining high performance, by mitigating compression and decompression overheads that can have a significant impact on performance by leveraging modern multicore CPUs through explicit work scheduling. We present two case-studies for compression. First, we examine how our approach can be used to increase the capacity of SSD-based caches, thus increasing their cost-effectiveness. Then, we examine how ZBD can improve the efficiency of online disk-based storage systems. We evaluate our approach in the Linux kernel on a commodity server with multicore CPUs, using Post-Mark, SPECsfs 2008, TPC-C, and TPC-H. Preliminary results show that transparent online block-level compression is a viable option for improving effective storage capacity, it can improve I/O performance up to 80% by reducing I/O traffic and seek distance, and has a negative impact on performance, up to 34%, only when single-thread I/O latency is critical. In particular, for SSD-based caching, our results indicate that, in line with current technology trends, compressed caching trades off CPU utilization for performance and enhances SSD efficiency as a storage cache up to 99%. © 2012 ACM.",Block-level compression; SSD-based I/O cache,Efficiency; Metadata; Program processors; Virtual storage; Block allocation; Case-studies; CPU utilization; Current technology; Disk-based; I/O latency; I/O performance; In-line; Linux kernel; Multi core; Significant impacts; Space efficiencies; SSD-based I/O cache; Storage caches; Storage capacity; Storage systems; Work scheduling; Online systems
Analysis of workload behavior in scientific and historical long-term data repositories,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863626117&doi=10.1145%2f2180905.2180907&partnerID=40&md5=20c2e789a484adcef52b373ee1e30727,"The scope of archival systems is expanding beyond cheap tertiary storage: scientific and medical data is increasingly digital, and the public has a growing desire to digitally record their personal histories. Driven by the increase in cost efficiency of hard drives, and the rise of the Internet, content archives have become a means of providing the public with fast, cheap access to long-term data. Unfortunately, designers of purposebuilt archival systems are either forced to rely on workload behavior obtained from a narrow, anachronistic view of archives as simply cheap tertiary storage, or extrapolate from marginally related enterprise workload data and traditional library access patterns. To close this knowledge gap and provide relevant input for the design of effective long-term data storage systems, we studied the workload behavior of several systems within this expanded archival storage space. Our study examined several scientific and historical archives, covering a mixture of purposes, media types, and access models-that is, public versus private. Our findings show that, for more traditional private scientific archival storage, files have become larger, but update rates have remained largely unchanged. However, in the public content archives we observed, we saw behavior that diverges from the traditional ""write-once, read-maybe"" behavior of tertiary storage. Our study shows that the majority of such data is modified-sometimes unnecessarily-relatively frequently, and that indexing services such as Google and internal data management processes may routinely access large portions of an archive, accounting for most of the accesses. Based on these observations, we identify areas for improving the efficiency and performance of archival storage systems. © 2012 ACM.",Archival storage; Tertiary storage; Trace analysis,Data storage equipment; Information management; Trace analysis; Access patterns; Archival storage systems; Archival systems; Cost efficiency; Data repositories; Data storage systems; Hard drives; Historical archive; Indexing Service; Knowledge gaps; Management process; Media types; Medical data; Storage spaces; Write once; Behavioral research
MFTL: A design and implementation for MLC flash memory storage systems,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863618078&doi=10.1145%2f2180905.2180908&partnerID=40&md5=d21a8f435578cd6db6ff0f0e7e8ed911,"NAND flash memory has gained its popularity in a variety of applications as a storage medium due to its low power consumption, nonvolatility, high performance, physical stability, and portability. In particular, Multi-Level Cell (MLC) flash memory, which provides a lower cost and higher density solution, has occupied the largest part of NAND flash-memory market share. However, MLC flash memory also introduces new challenges: (1) Pages in a block must be written sequentially. (2) Information to indicate a page being obsoleted cannot be recorded in its spare area due to the limitation on the number of partial programming. Since most of applications access NAND flash memory under FAT file system, this article designs an MLC Flash Translation Layer (MFTL) for flash-memory storage systems which takes constraints of MLC flash memory and access behaviors of FAT file system into consideration. A series of trace-driven simulations was conducted to evaluate the performance of the proposed scheme. Although MFTL is designed for MLC flash memory and FAT file system, it is applicable to SLC flash memory and other file systems as well. Our experiment results show that the proposed MFTL could achieve a good performance for various access patterns even on SLC flashmemory. © 2012 ACM.",FAT file system; Multi-level cell flash memory; Storage management,Competition; Monolithic microwave integrated circuits; NAND circuits; Access patterns; File systems; Flash memory storage systems; Flash translation layer; Low-power consumption; Market share; Multi-level cell flash memory; Multilevel cell; NAND flash memory; Nonvolatility; Physical stability; Storage management; Storage medium; Storage systems; Trace driven simulation; Flash memory
A caching-oriented management design for the performance enhancement of solid-state drives,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863250844&doi=10.1145%2f2093139.2093142&partnerID=40&md5=bece7457879effbbae41370da8cf32df,"While solid-state drives are excellent alternatives to hard disks in mobile devices, a number of performance and reliability issues need to be addressed. In this work, we design an efficient flash management scheme for the performance improvement of low-cost MLC flash memory devices. Specifically, we design an efficient flash management scheme for multi-chipped flash memory devices with cache support, and develop a twolevel address translation mechanism with an adaptive caching policy. We evaluated the approach on real workloads. The results demonstrate that it can improve the performance of multi-chipped solid-state drives through logical-to-physical mappings and concurrent accesses to flash chips. © 2012 ACM.",Cache; Flash memory; Performance; Solid-state disk,Drives; Flash memory; Hard disk storage; Mobile devices; Adaptive caching; Address translation; Cache; Cache support; Concurrent access; Flash chips; Management scheme; Performance; Performance enhancements; Performance improvements; Solid-state disk; Solid-state drives; Design
An adaptive write buffer management scheme for flash-based ssds,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863393336&doi=10.1145%2f2093139.2093140&partnerID=40&md5=e29b0db6aca991b7fbdb85627ad42a23,"Solid State Drives (SSD's) have shown promise to be a candidate to replace traditional hard disk drives. The benefits of SSD's over HDD's include better durability, higher performance, and lower power consumption, but due to certain physical characteristics of NAND flash, which comprise SSD's, there are some challenging areas of improvement and further research. We focus on the layout and management of the small amount of RAM that serves as a cache between the SSD and the system that uses it. Of the techniques that have previously been proposed to manage this cache, we identify several sources of inefficient cache space management due to the way pages are clustered in blocks and the limited replacement policy.We find that in many traces hot pages reside in otherwise cold blocks, and that the spatial locality ofmost clusters can be fully exploited in a limited time period, so we develop a hybrid page/block architecture along with an advanced replacement policy, called BPAC, or Block-Page Adaptive Cache, to exploit both temporal and spatial locality. Our technique involves adaptively partitioning the SSD on-disk cache to separately hold pages with high temporal locality in a page list and clusters of pages with low temporal but high spatial locality in a block list. In addition, we have developed a novel mechanism for flash-based SSD's to characterize the spatial locality of the disk I/O workload and an approach to dynamically identify the set of low spatial locality clusters. We run trace-driven simulations to verify our design and find that it outperforms other popular flash-aware cache schemes under different workloads. For instance, compared to a popular flash aware cache algorithm BPLRU, BPAC reduces the number of cache evictions by up to 79.6% and 34% on average. © 2012 ACM.",Flash-aware cache; NAND flash memory; SSD; Write buffer,Data storage equipment; Hardware; Buffer management; Cache algorithms; Cache scheme; Disk I/O; Flash-aware cache; Hard Disk Drive; Lower-power consumption; NAND Flash; NAND flash memory; Physical characteristics; Replacement policy; Solid state drives; Space management; Spatial locality; SSD; Temporal locality; Time-periods; Trace driven simulation; Write buffer; Hard disk storage
HPDA: A hybrid parity-based disk array for enhanced performance and reliability,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863298286&doi=10.1145%2f2093139.2093143&partnerID=40&md5=3e5063392f92a4869e35064180b463f1,"Flash-based Solid State Drive (SSD) has been productively shipped and deployed in large scale storage systems. However, a single flash-based SSD cannot satisfy the capacity, performance and reliability requirements of the modern storage systems that support increasingly demanding data-intensive computing applications. Applying RAID schemes to SSDs to meet these requirements, while a logical and viable solution, faces many challenges. In this article, we propose a Hybrid Parity-based Disk Array architecture (short for HPDA), which combines a group of SSDs and two hard disk drives (HDDs) to improve the performance and reliability of SSD-based storage systems. In HPDA, the SSDs (data disks) and part of one HDD (parity disk) compose a RAID4 disk array. Meanwhile, a second HDD and the free space of the parity disk are mirrored to form a RAID1-style write buffer that temporarily absorbs the small write requests and acts as a surrogate set during recovery when a disk fails. The write data is reclaimed to the data disks during the lightly loaded or idle periods of the system. Reliability analysis shows that the reliability of HPDA, in terms of MTTDL (Mean Time To Data Loss), is better than that of either pure HDD-based or SSD-based disk array. Our prototype implementation of HPDA and the performance evaluations show that HPDA significantly outperforms either HDD-based or SSD-based disk array. © 2012 ACM.",Performance; RAID; Reliability; SSD; Storage systems,Reliability; Reliability analysis; Data loss; Data-intensive computing; Disk array; Enhanced performance; Free spaces; Hard disk drives; Performance; Performance evaluation; Prototype implementations; RAID; Solid state drives; SSD; Storage systems; Viable solutions; Hard disk storage
Efficient software implementations of large finite fields GF (2 n) for secure storage applications,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863248422&doi=10.1145%2f2093139.2093141&partnerID=40&md5=b3ce7de54f51b56e262b62379d4c92ff,"Finite fields are widely used in constructing error-correcting codes and cryptographic algorithms. In practice, error-correcting codes use small finite fields to achieve high-throughput encoding and decoding. Conversely, cryptographic systems employ considerably larger finite fields to achieve high levels of security. We focus on developing efficient software implementations of arithmetic operations in reasonably large finite fields as needed by secure storage applications. In this article, we study several arithmetic operation implementations for finite fields ranging from GF(2 32) to GF(2 128). We implement multiplication and division in these finite fields by making use of precomputed tables in smaller fields, and several techniques of extending smaller field arithmetic into larger field operations. We show that by exploiting known techniques, as well as new optimizations, we are able to efficiently support operations over finite fields of interest. We perform a detailed evaluation of several techniques, and show that we achieve very practical performance for both multiplication and division. Finally, we show how these techniques find applications in the implementation of HAIL, a highly available distributed cloud storage layer. Using the newly implemented arithmetic operations in GF(2 64), HAIL improves its performance by a factor of two, while simultaneously providing a higher level of security. © 2012 ACM.",Cloud storage systems; Cryptographic algorithms; Finite field arithmetic,Algorithms; Cryptography; Information theory; Precipitation (meteorology); Arithmetic operations; Cryptographic algorithms; Cryptographic systems; Encoding and decoding; Error correcting code; Field arithmetic; Finite field arithmetic; Finite fields; High-throughput; Larger fields; Secure storage; Software implementation; Storage layers; Storage systems; Support operations; Finite element method
A study of practical deduplication,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857732276&doi=10.1145%2f2078861.2078864&partnerID=40&md5=9fb088c525a628543d2820d6e961afc9,"We collected file system content data from 857 desktop computers at Microsoft over a span of 4 weeks. We analyzed the data to determine the relative efficacy of data deduplication, particularly considering whole-file versus block-level elimination of redundancy. We found that whole-file deduplication achieves about three quarters of the space savings of the most aggressive block-level deduplication for storage of live file systems, and 87% of the savings for backup images.We also studied file fragmentation, finding that it is not prevalent, and updated prior file system metadata studies, finding that the distribution of file sizes continues to skew toward very large unstructured files. © 2012 ACM.",Data; Deduplication; Filesystem; Study; Windows,Metadata; Windows; Content data; Data; Deduplication; File sizes; File systems; Filesystem; MicroSoft; Space savings; Study; Personal computers
Emulating goliath storage systems with David,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857741164&doi=10.1145%2f2078861.2078862&partnerID=40&md5=aac1afe685c48805a61e67870b1e7f2a,"Benchmarking file and storage systems on large file-system images is important, but difficult and often infeasible. Typically, running benchmarks on such large disk setups is a frequent source of frustration for file-system evaluators; the scale alone acts as a strong deterrent against using larger, albeit realistic, benchmarks. To address this problem, we develop David: a system that makes it practical to run large benchmarks using modest amount of storage or memory capacities readily available on most computers. David creates a ""compressed"" version of the original file-system image by omitting all file data and laying out metadata more efficiently; an online storage model determines the runtime of the benchmark workload on the original uncompressed image. David works under any file system, as demonstrated in this article with ext3 and btrfs. We find that David reduces storage requirements by orders of magnitude; David is able to emulate a 1-TB target workload using only an 80 GB available disk, while still modeling the actual runtime accurately. David can also emulate newer or faster devices, for example, we show how David can effectively emulate a multidisk RAID using a limited amount of memory. © 2012 ACM.",Emulation; File systems; Scalability; Storage systems,Data storage equipment; Hardware; Scalability; Emulation; File systems; Large disks; Memory capacity; Orders of magnitude; Runtimes; Storage model; Storage requirements; Storage systems; Target workloads; Metadata
Making the common case the only case with anticipatory memory allocation,2012,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863282318&doi=10.1145%2f2078861.2078863&partnerID=40&md5=a51242175cad8ae7abe36c8b3bf17cae,"We present anticipatory memory allocation (AMA), a new method to build kernel code that is robust to memory-allocation failures. AMA avoids the usual difficulties in handling allocation failures through a novel combination of static and dynamic techniques. Specifically, a developer, with assistance from AMA static analysis tools, determines how much memory a particular call into a kernel subsystem will need, and then preallocates said amount immediately upon entry to the kernel; subsequent allocation requests are serviced from the preallocated pool and thus guaranteed never to fail.We describe the static and runtime components of AMA, and then present a thorough evaluation of Linux ext2-mfr, a case study in which we transform the Linux ext2 file system into a memory-failure robust version of itself. Experiments reveal that ext2-mfr avoids memory-allocation failures successfully while incurring little space or time overhead. © 2012 ACM.",Fault recovery; File systems; Preallocation; Static analysis,Computer operating systems; Fault recovery; File systems; Preallocation; Runtimes; Static and dynamic; Static analysis
Guest editorial,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455130028&doi=10.1145%2f2027066.2027067&partnerID=40&md5=478b8f16de132830cf6a08ea3a0cb83f,[No abstract available],,
Reducing repair traffic in P2P backup systems: Exact regenerating codes on hierarchical codes,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455132318&doi=10.1145%2f2027066.2027070&partnerID=40&md5=0b09ec8f08d482a897b7b4a75392b79d,"Peer to peer backup systems store data on ""unreliable"" peers that can leave the system at any moment. In this case, the only way to assure durability of the data is to add redundancy using either replication or erasure codes. Erasure codes are able to provide the same reliability as replication requiring much less storage space. Erasure coding breaks the data into blocks that are encoded and then stored on different nodes. However, when storage nodes permanently abandon the system, new redundant blocks must be created, which is referred to as repair. For ""classical"" erasure codes, generating a new block requires the transmission of k blocks over the network, resulting in a high repair traffic. Recently, two new classes of erasure codes, Regenerating Codes and Hierarchical Codes, have been proposed that significantly reduce the repair traffic. Regenerating Codes reduce the amount of data uploaded by each peer involved in the repair, while Hierarchical Codes reduce the number of nodes participating in the repair. In this article we propose to combine these two codes to devise a new class of erasure codes called ER-Hierarchical Codes that combine the advantages of both. © 2011 ACM.",Durability; Hierarchical codes; Maintenance; P2P backup systems; Regenerating codes; Reliability; Repair degree; Storage,Durability; Peer to peer networks; Repair; Back-up systems; Erasure codes; Erasure coding; Hierarchical codes; Peer to peer; Regenerating codes; Storage nodes; Storage spaces; Hierarchical systems
A hybrid approach to failed disk recovery using RAID-6 codes: Algorithms and performance evaluation,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455168608&doi=10.1145%2f2027066.2027071&partnerID=40&md5=dd6736f428f14a7c19cba545b7d96f12,"The current parallel storage systems use thousands of inexpensive disks to meet the storage requirement of applications. Data redundancy and/or coding are used to enhance data availability, for instance, Rowdiagonal parity (RDP) and EVENODD codes, which are widely used in RAID-6 storage systems, provide data availability with up to two disk failures. To reduce the probability of data unavailability, whenever a single disk fails, disk recovery will be carried out. We find that the conventional recovery schemes of RDP and EVENODD codes for a single failed disk only use one parity disk. However, there are two parity disks in the system, and both can be used for single disk failure recovery. In this article, we propose a hybrid recovery approach that uses both parities for single disk failure recovery, and we design efficient recovery schemes for RDP code (RDOR-RDP) and EVENODD code (RDOR-EVENODD). Our recovery scheme has the following attractive properties: (1) ""read optimality"" in the sense that our scheme issues the smallest number of disk reads to recover a single failed disk and it reduces approximately 1/4 of disk reads compared with conventional schemes; (2) ""load balancing property"" in that all surviving disks will be subjected to the same (or almost the same) amount of additional workload in rebuilding the failed disk. We carry out performance evaluation to quantify the merits of RDOR-RDP and RDOR-EVENODD on some widely used disks with DiskSim. The offline experimental results show that RDOR-RDP and RDOREVENODD outperform the conventional recovery schemes of RDP and EVENODD codes in terms of total recovery time and recovery workload on individual surviving disk. However, the improvements are less than the theoretical value (approximately 25%), as RDOR-RDP and RDOR-EVENODD change the disk access pattern from purely sequential to a more random one compared with their conventional schemes. © 2011 ACM.",Disk failure; EVENODD code; RAID recovery; RDP code; Recovery algorithm,Algorithms; Disk failure; EVENODD code; RAID recovery; RDP code; Recovery algorithm; Recovery
You choose: Choosing your storage device as a performance interface to consolidated I/O service,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455168612&doi=10.1145%2f2027066.2027069&partnerID=40&md5=78d123b20ccb41ac085c93c1067ae305,"Currently the QoS requirements for storage systems are usually presented in the form of service-level agreement (SLA) to bound I/O measures such as latency and throughput of I/O requests. However, SLA is not an effective performance interface for users to specify their required I/O service quality for two major reasons. First, for users it is difficult to determine appropriate latency and throughput bounds to ensure their required application performance without resource over-provisioning. Second, for storage system administrators it is a challenge to estimate a user's real resource demand because the specified SLA measures are not consistently correlated with the user's resource demand. This makes resource provisioning and scheduling less informative and can greatly reduce system efficiency. We propose the concept of reference storage system (RSS), which can be a storage system chosen by users and whose performance can be measured offline and mimicked online, as a performance interface between applications and storage servers. By designating an RSS to represent I/O performance requirement, a user can expect the performance received from a shared storage server servicing his I/O workload is not worse than the performance received from the RSS servicing the same workload. The storage system is responsible for implementing the RSS interface. The key enabling techniques are a machine learning model that derives request-specific performance requirements and an RSS-centric scheduling that efficiently allocates resource among requests from different users. The proposed scheme, named as YouChoose, supports the user-chosen performance interface through efficiently implementing and migrating virtual storage devices in a host storage system. Our evaluation based on trace-driven simulations shows that YouChoose can precisely implement the RSS performance interface, achieve a strong performance assurance and isolation, and improve the efficiency of a consolidated storage system consisting of different types of storage devices. © 2011 ACM.",I/O QoS; Machine learning; Model; Performance interface; Quality of service,Learning systems; Quality of service; Scheduling; Application performance; Effective performance; Enabling techniques; I/O performance; I/O workload; Machine-learning; Offline; Performance interface; Performance requirements; QoS requirements; Resource demands; Resource provisioning; Service level agreements; Service Quality; Shared storage; Storage servers; Storage systems; System efficiency; Trace driven simulation; Virtual storage
Understanding and improving computational science storage access through continuous characterization,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455127318&doi=10.1145%2f2027066.2027068&partnerID=40&md5=015c4c84be72fcfc204988e721b1237f,"Computational science applications are driving a demand for increasingly powerful storage systems. While many techniques are available for capturing the I/O behavior of individual application trial runs and specific components of the storage system, continuous characterization of a production system remains a daunting challenge for systems with hundreds of thousands of compute cores and multiple petabytes of storage. As a result, these storage systems are often designed without a clear understanding of the diverse computational science workloads they will support. In this study, we outline a methodology for scalable, continuous, systemwide I/O characterization that combines storage device instrumentation, static file system analysis, and a new mechanism for capturing detailed application-level behavior. This methodology allows us to identify both system-wide trends and application-specific I/O strategies. We demonstrate the effectiveness of our methodology by performing a multilevel, two-month study of Intrepid, a 557-teraflop IBM Blue Gene/P system. During that time, we captured application-level I/O characterizations from 6,481 unique jobs spanning 38 science and engineering projects. We used the results of our study to tune example applications, highlight trends that impact the design of future storage systems, and identify opportunities for improvement in I/O characterization methodology. © 2011 ACM.",I/O characterization; Parallel file systems,File organization; Supercomputers; Systems analysis; Virtual storage; Blue Gene; Computational science; File systems; New mechanisms; Parallel file system; Petabytes; Production system; Science and engineering; Specific component; Storage systems; Characterization
PRE-BUD: Prefetching for energy-efficient parallel i/o systems with buffer disks,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960085459&doi=10.1145%2f1970343.1970346&partnerID=40&md5=22acf02a251d19aed317f254e6d58db9,"A critical problem with parallel I/O systems is the fact that disks consume a significant amount of energy. To design economically attractive and environmentally friendly parallel I/O systems, we propose an energy-aware prefetching strategy (PRE-BUD) for parallel I/O systems with disk buffers. We introduce a new architecture that provides significant energy savings for parallel I/O systems using buffer disks while maintaining high performance. There are two buffer disk configurations: (1) adding an extra buffer disk to accommodate prefetched data, and (2) utilizing an existing disk as the buffer disk. PRE-BUD is not only able to reduce the number of power-state transitions, but also to increase the length and number of standby periods. As such, PRE-BUD conserves energy by keeping data disks in the standby state for increased periods of time. Compared with the first prefetching configuration, the second configuration lowers the capacity of the parallel disk system. However, the second configuration is more cost-effective and energy-efficient than the first one. Finally, we quantitatively compare PRE-BUD with both disk configurations against three existing strategies. Empirical results show that PRE-BUD is able to reduce energy dissipation in parallel disk systems by up to 50 percent when compared against a non-energy aware approach. Similarly, our strategy is capable of conserving up to 30 percent energy when compared to the dynamic power management technique. © 2011 ACM.",Buffer disks; Energy conservation; Prefetching,Energy dissipation; Energy management; Buffer disks; Critical problems; Dynamic power management; Empirical results; Energy aware; Energy efficient; Environmentally-friendly; Non-energy; Parallel disk systems; Parallel I/O systems; Prefetching; Energy efficiency
Fast file existence checking in archiving systems,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960081629&doi=10.1145%2f1970343.1970345&partnerID=40&md5=f8d2d55dac86bff6297274f0184b2f44,"This article presents a new FastHash-based File Existence Checking (FHFEC) method for archiving systems. During the archiving process, there are many submissions which are actually unchanged files that do not need to be re-archived. In this system, instead of comparing the entire files, only digests of the files are compared. Strong cryptographic hash functions with a low probability of collision can be used as digests. We propose a fast algorithm to check if a certain hash, that is, a corresponding file, is already stored in the system. The algorithm is based on dividing the whole domain of hashes into equally sized regions, and on the existence of a pointer array, which has exactly one pointer for each region. Each pointer points to the location of the first stored hash from the corresponding region and has a null value if no hash from that region exists. The entire structure can be stored in random access memory or, alternatively, on a dedicated hard disk. A statistical performance analysis has been performed that shows that in certain cases FHFEC performs nearly optimally. Extensive simulations have confirmed these analytical results. The performance of FHFEC has been compared to the performance of a binary search (BIS) and B+tree, which are commonly used in file systems and databases for table indices. The results show that FHFEC significantly outperforms both of them. © 2011 ACM.",Archiving; File systems management; Files backup/recovery; Files sorting/searching; Hash-table; Performance evaluation,Hash functions; Archiving; File systems management; Files backup/recovery; Files sorting/searching; Hash-table; Performance evaluation; Random access storage
A driver-layer caching policy for removable storage devices,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960101365&doi=10.1145%2f1970343.1970344&partnerID=40&md5=47ba6ec7ce10788afd28230184743993,"The growing popularity of flash memory is expected to draw attention to the limitations of file-system performance over flash memory. This work was motivated by the modular designs of operating system components such as bus and device drivers. A filter-driver-layered caching design is proposed to resolve the performance gap among file systems and to improve their performance with the considerations of flash memory characteristics. An efficient hybrid tree structure is presented to organize and manipulate the intervals of cached writes. Algorithms are proposed in the merging, padding, and removing of the data of writes. The effectiveness of the proposed approach is demonstrated with some analysis study of FATformatted and NTFS-formatted USB flash disks. The proposed cohesive caching policy was implemented as a filter driver in Windows XP/Vista for performance evaluation. In the experiments, a ten-fold or larger performance improvement was usually achieved when the cache size was only 64KB. Other substantial improvements were also observed in the experiments. For example, the proposed design enabled FATformatted and NTFS-formatted flash-memory devices to copy Linux image files 93% and 14% faster than conventional flash drives, respectively. © 2011 ACM.",Caching; Driver; File system; Flash memory; Merging; Padding; Removable storage; USB; Windows,Computer operating systems; Design; Experiments; Merging; Trees (mathematics); Virtual storage; Caching; Driver; File system; Padding; Removable storage; USB; Flash memory
Online availability upgrades for parity-based RAIDs through supplementary parity augmentations,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960056592&doi=10.1145%2f1970338.1970341&partnerID=40&md5=9f924e82f3417d9296b12c06cdf2d406,"In this article, we propose a simple but powerful online availability upgrade mechanism, Supplementary Parity Augmentations(SPA), to address the availability issue in parity-based RAID systems. The basic idea of SPA is to store and update the supplementary parity units on one or a few newly augmented spare disks for online RAID systems in the operational mode, thus achieving the goals of improving the reconstruction performance while tolerating multiple disk failures and latent sector errors simultaneously. By applying the exclusive OR operations appropriately among supplementary parity, full parity, and data units, SPA can reconstruct the data on the failed disks with a fraction of the original overhead that is proportional to the supplementary parity coverage, thus significantly reducing the overhead of data regeneration and decreasing recovery time in parity-based RAID systems. Our extensive trace-driven simulation study shows that SPA can significantly improve the reconstruction performance of the RAID5 and RAID5+0 systems, at an acceptable performance overhead imposed in the operational mode. Moreover, our reliability analytical modeling and sequential Monte-Carlo simulation demonstrate that SPA is consistently more than double the MTTDL of the RAID5 system and improves the reliability of the RAID5+0 system noticeably. © 2011 ACM.",,Fault tolerant computer systems; Nonvolatile storage; Analytical modeling; Data units; Exclusive OR operation; Monte Carlo Simulation; Multiple disk failures; Operational modes; RAID systems; Recovery time; Trace driven simulation; Upgrade mechanism; Online systems
A hybrid flash translation layer with adaptive merge for SSDs,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960039724&doi=10.1145%2f1970338.1970339&partnerID=40&md5=2d0a114cc49035f00ff88c49c4c97edf,"The Flash Translation Layer (FTL) in Solid-State Disks (SSDs) maps logical addresses to physical addresses for disk drive virtualization. In order to reduce garbage collection overhead, we propose full associative striped block-level mapping. In addition, an adaptive merge is proposed to avoid excessive data block reconstructions during garbage collection. With these mechanisms, the write latency is improved up to 78% in comparison with the previous multichannel hybrid FTLs in a sample PC trace. The performance improvements stem from 52% reduced garbage collection. © 2011 ACM.",,Waste disposal; Data blocks; Disk drive; Flash translation layer; Garbage collection; Multi-channel; Performance improvements; Physical address; Virtualizations; Refuse collection
Minimum density RAID-6 Codes,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960074613&doi=10.1145%2f1970338.1970340&partnerID=40&md5=91f90286b15541dfe7ca980853a5d69f,"RAID-6 codes protect disk array storage systems from two-disk failures. This article presents a complete treatment of a class of RAID-6 codes, called minimum density RAID-6 codes, that have an optimal blend of performance properties. There are two families of minimal density RAID-6 codes: Blaum-Roth codes and Liberation codes, and a separate special-purpose code called the Liber8tion code. The first of these have been known since the late 1990's, while the latter two are new constructions. In this article, we motivate, demonstrate, and evaluate the minimum density codes, comparing them to EVENODD and RDP codes, which represent the state-of-the-art in RAID-6. Following that, we prove that the codes indeed fit the RAID- 6 methodology, and cite their implementation in an open-source library. © 2011 ACM.",,Disk array; Minimum density; Open-source libraries; Performance properties; Special-purpose codes
Request bridging and interleaving: Improving the performance of small synchronous updates under seek-optimizing disk subsystems,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960784269&doi=10.1145%2f1970348.1970349&partnerID=40&md5=934410fbf2abaf2cda4ad765b8f588d4,"Write-through caching in modern disk drives enables the protection of data in the event of power failures as well as from certain disk errors when the write-back cache does not. Host system can achieve these benefits at the price of significant performance degradation, especially for small disk writes. We present new blocklevel techniques to address the performance problem of write-through caching disks. Our techniques are strongly motivated by some interesting results when the disk-level caching is turned off. By extending the conventional request merging, request bridging increases the request size and amortizes the inherent delays in the disk drive across more bytes of data. Like sector interleaving, request interleaving rearranges requests to prevent the disk head from missing the target sector position in close proximity, and thus reduces disk latency. We have evaluated our block-level approach using a variety of I/O workloads and shown that it increases disk I/O throughput by up to about 50%. For some real-world workloads, the disk performance is comparable or even superior to that of using the write-back disk cache. In practice, our simple yet effective solutions achieve better tradeoffs between data reliability and disk performance when applied to writethrough caching disks. © 2011 ACM.",Cross-layer optimizations; Disk drive; Disk scheduling algorithm; Request merging; Trace-driven analysis,Digital storage; Cross layer optimization; Disk drive; Disk performance; Disk scheduling algorithms; Effective solution; Performance degradation; Performance problems; Trace-driven analysis; Merging
Disk scrubbing versus intradisk redundancy for RAID storage systems,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960821102&doi=10.1145%2f1970348.1970350&partnerID=40&md5=a6521bf11dfe4250ef09c249d3909384,"Two schemes proposed to cope with unrecoverable or latent media errors and enhance the reliability of RAID systems are examined. The first scheme is the established, widely used, disk scrubbing scheme, which operates by periodically accessing disk drives to detect media-related unrecoverable errors. These errors are subsequently corrected by rebuilding the sectors affected. The second scheme is the recently proposed intradisk redundancy scheme, which uses a further level of redundancy inside each disk, in addition to the RAID redundancy across multiple disks. A new model is developed to evaluate the extent to which disk scrubbing reduces the unrecoverable sector errors. The probability of encountering unrecoverable sector errors is derived analytically under very general conditions regarding the characteristics of the read/write process of uniformly distributed random workloads and for a broad spectrum of disk scrubbing schemes, which includes the deterministic and random scrubbing schemes. We show that the deterministic scrubbing scheme is the most efficient one. We also derive closed-form expressions for the percentage of unrecoverable sector errors that the scrubbing scheme detects and corrects, the throughput performance, and theminimum scrubbing period achievable under operation with random, uniformly distributed I/O requests. Our results demonstrate that the reliability improvement due to disk scrubbing depends on the scrubbing frequency and the load of the system, and, for heavy-write workloads, may not reach the reliability level achieved by a simple interleaved parity-check (IPC)-based intradisk redundancy scheme, which is insensitive to the load. In fact, for small unrecoverable sector error probabilities, the IPC-based intradisk redundancy scheme achieves essentially the same reliability as that of a system operating without unrecoverable sector errors. For heavy loads, the reliability achieved by the scrubbing scheme can be orders of magnitude less than that of the intradisk redundancy scheme. Finally, the I/O and throughput performances are evaluated by means of analysis and event-driven simulation. © 2011 ACM.",MTTDL; RAID; Reliability analysis; Stochastic modeling; Unrecoverable or latent sector errors,Random errors; Reliability analysis; Stochastic models; Stochastic systems; Closed-form expression; Error probabilities; Event-driven simulations; MTTDL; Orders of magnitude; RAID; Reliability improvement; Throughput performance; Redundancy
PRESIDIO: A framework for efficient archival data storage,2011,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960818185&doi=10.1145%2f1970348.1970351&partnerID=40&md5=030c4da7cc81bd59c8b31adea94c6695,"The ever-increasing volume of archival data that needs to be reliably retained for long periods of time and the decreasing costs of disk storage, memory, and processing have motivated the design of low-cost, highefficiency disk-based storage systems. However, managed disk storage is still expensive. To further lower the cost, redundancy can be eliminated with the use of interfile and intrafile data compression. However, it is not clear what the optimal strategy for compressing data is, given the diverse collections of data. To create a scalable archival storage system that efficiently stores diverse data, we present PRESIDIO, a framework that selects from different space-reduction efficent storage methods (ESMs) to detect similarity and reduce or eliminate redundancy when storing objects. In addition, the framework uses a virtualized content addressable store (VCAS) that hides from the user the complexity of knowing which space-efficient techniques are used, including chunk-based deduplication or delta compression. Storing and retrieving objects are polymorphic operations independent of their content-based address. A new technique, harmonic super-fingerprinting, is also used for obtaining successively more accurate (but also more costly) measures of similarity to identify the existing objects in a very large data set that are most similar to an incoming new object. The PRESIDIO design, when reported earlier, had comprehensively introduced for the first time the notion of deduplication, which is now being offered as a service in storage systems by major vendors. As an aid to the design of such systems, we evaluate and present various parameters that affect the efficiency of a storage system using empirical data. © 2011 ACM.",Archival storage systems; CAS; Content-addressable storage; Data compression; Progressive compression,Associative storage; Calcium; Data compression; Redundancy; Storage as a service (STaaS); Archival data storages; Archival storage systems; Delta compression; Disk based storage; Optimal strategies; Progressive compression; Space reductions; Very large datum; Digital storage
Qufiles: The right file at the right time,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957564959&doi=10.1145%2f1837915.1837920&partnerID=40&md5=b8819593b956961774e11e9ab472b26b,"A quFile is a unifying abstraction that simplifies data management by encapsulating different physical representations of the same logical data. Similar to a quBit (quantum bit), the particular representation of the logical data displayed by a quFile is not determined until the moment it is needed. The representation returned by a quFile is specified by a data-specific policy that can take context into account such as the application requesting the data, the device on which data is accessed, screen size, and battery status. We demonstrate the generality of the quFile abstraction by using it to implement six case studies: resource management, copy-on-write versioning, data redaction, resource-aware directories, application-aware adaptation, and platform-specific encoding. Most quFile policies were expressed using less than one hundred lines of code. Our experimental results show that, with caching and other performance optimizations, quFiles add less than 1% overhead to application-level file system. © 2010 ACM.",Context-aware file systems; Copy-on-write versions; Data management; Distributed storage,Abstracting; Security of data; Application-aware adaptation; Battery status; Copy-on-write versions; Data management; Distributed storage; File systems; Lines of code; Logical data; Performance optimizations; Quantum bits; Resource aware; Resource management; Screen sizes; Versioning; Information management
DFS: A file system for virtualized flash storage,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957559094&doi=10.1145%2f1837915.1837922&partnerID=40&md5=3cbf9f7a19f480751b352e05715d9418,"We present the design, implementation, and evaluation of Direct File System (DFS) for virtualized flash storage. Instead of using traditional layers of abstraction, our layers of abstraction are designed for directly accessing flash memory devices. DFS has two main novel features. First, it lays out its files directly in a very large virtual storage address space provided by FusionIO's virtual flash storage layer. Second, it leverages the virtual flash storage layer to perform block allocations and atomic updates. As a result, DFS performs better and is much simpler than a traditional Unix file system with similar functionalities. Our microbenchmark results show that DFS can deliver 94,000 I/O operations per second (IOPS) for direct reads and 71,000 IOPS for direct writes with the virtualized flash storage layer on FusionIO's ioDrive. For direct access performance, DFS is consistently better than ext3 on the same platform, sometimes by 20%. For buffered access performance, DFS is also consistently better than ext3, and sometimes by over 149%. Our application benchmarks show that DFS outperforms ext3 by 7% to 250% while requiring less CPU power. © 2010 ACM.",Filesystem; Flash memory,Abstracting; Benchmarking; Virtual storage; Address space; Block allocation; CPU power; Direct access; File systems; Filesystem; Flash memory devices; Flash storage; I/O operations; Micro-benchmark; Flash memory
I/O deduplication: Utilizing content similarity to improve I/O performance,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957564609&doi=10.1145%2f1837915.1837921&partnerID=40&md5=df1322d8741a2585f045f561012b28bd,"Duplication of data in storage systems is becoming increasingly common. We introduce I/O Deduplication,a storage optimization that utilizes content similarity for improving I/O performance by eliminating I/O operations and reducing the mechanical delays during I/O operations. I/O Deduplication consists of three main techniques: content-based caching, dynamic replica retrieval, and selective duplication. Each of these techniques is motivated by our observations with I/O workload traces obtained from actively-used production storage systems, all of which revealed surprisingly high levels of content similarity for both stored and accessed data. Evaluation of a prototype implementation using these workloads showed an overall improvement in disk I/O performance of 28 to 47% across these workloads. Further breakdown also showed that each of the three techniques contributed significantly to the overall performance improvement. © 2010 ACM.",Content-based caching; Content-based I/O scheduling; I/O deduplication; Storage systems,Software prototyping; Content similarity; Content-based; Deduplication; Disk I/O; Dynamic replica; I/O operations; I/O performance; I/O workload; Performance improvements; Prototype implementations; Storage optimization; Storage systems; Scheduling
Optimizing energy and performance for server-class file system workloads,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957607728&doi=10.1145%2f1837915.1837918&partnerID=40&md5=66344e6943e18e6441413834bbc4cdb9,"Recently, power has emerged as a critical factor in designing components of storage systems, especially for power-hungry data centers. While there is some research into power-aware storagestack components, there are no systematic studies evaluating each component's impact separately. Various factors like workloads, hardware configurations, and software configurations impact the performance and energy efficiency of the system. This article evaluates the file system's impact on energy consumption and performance. We studied several popular Linux file systems, with various mount and format options, using the FileBench workload generator to emulate four server workloads: Web, database, mail, and fileserver, on two different hardware configurations. The file system design, implementation, and available features have a significant effect on CPU/disk utilization, and hence on performance and power. We discovered that default file system options are often suboptimal, and even poor. In this article we show that a careful matching of expected workloads and hardware configuration to a single software configuration-the file system-can improve power-performance efficiency by a factor ranging from 1.05 to 9.4 times. © 2010 ACM.",Benchmarks; Energy efficiency; File systems; Storage systems,Computer hardware; Computer operating systems; Energy utilization; Benchmarks; Class file; Critical factors; Data centers; Designing components; Energy consumption; File systems; Hardware configurations; Linux file system; Optimizing energy; Power-aware; Power-performance efficiency; Software configuration; Storage systems; Systematic study; Energy efficiency
Membrane: Operating system support for restartable file systems,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957591911&doi=10.1145%2f1837915.1837919&partnerID=40&md5=0150f6bc8d1f6853a03330c57011794f,"We introduce Membrane, a set of changes to the operating system to support restartable file systems. Membrane allows an operating system to tolerate a broad class of file system failures, and does so while remaining transparent to running applications; upon failure, the file system restarts, its state is restored, and pending application requests are serviced as if no failure had occurred. Membrane provides transparent recovery through a lightweight logging and checkpoint infrastructure, and includes novel techniques to improve performance and correctness of its faultanticipation and recovery machinery. We tested Membrane with ext2, ext3, and VFAT. Through experimentation, we show that Membrane induces little performance overhead and can tolerate a wide range of file system crashes. More critically, Membrane does so with little or no change to existing file systems, thus improving robustness to crashes without mandating intrusive changes to existing file-system code. © 2010 ACM.",,Computer operating systems; Machinery; Systems engineering; File systems; Novel techniques; Operating system support; Operating systems; Running applications; System codes; Membranes
Understanding latent sector errors and how to protect against them,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957569492&doi=10.1145%2f1837915.1837917&partnerID=40&md5=6342a247f2971be08e1d16be6763d345,"Latent sector errors (LSEs) refer to the situation where particular sectors on a drive become inaccessible. LSEs are a critical factor in data reliability, since a single LSE can lead to data loss when encountered during RAID reconstruction after a disk failure or in systems without redundancy. LSEs happen at a significant rate in the field [Bairavasundaram et al. 2007], and are expected to grow more frequent with new drive technologies and increasing drive capacities. While two approaches, data scrubbing and intra-disk redundancy, have been proposed to reduce data loss due to LSEs, none of these approaches has been evaluated on real field data. This articlemakes two contributions.We provide an extended statistical analysis of latent sector errors in the field, specifically from the view point of how to protect against LSEs. In addition to providing interesting insights into LSEs, we hope the results (including parameters for models we fit to the data) will help researchers and practitioners without access to data in driving their simulations or analysis of LSEs. Our second contribution is an evaluation of five different scrubbing policies and five different intra-disk redundancy schemes and their potential in protecting against LSEs. Our study includes schemes and policies that have been suggested before, but have never been evaluated on field data, as well as new policies that we propose based on our analysis of LSEs in the field. © 2010 ACM.",Data loss; Failure data; Failure modeling; Field data; Latent sector errors; Parity; Redundancy; Scrubbing; Storage reliability,Computer simulation; Disks (structural components); Errors; Redundancy; Data loss; Failure data; Failure modeling; Field data; Latent sector errors; Parity; Scrubbing; Storage reliability; Quality assurance
Guest editorial: FAST'10,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957550207&doi=10.1145%2f1837915.1837916&partnerID=40&md5=1f49945a5f5d400db1f50318015bc650,[No abstract available],,
Differential RAID: Rethinking RAID for SSD reliability,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955197441&doi=10.1145%2f1807060.1807061&partnerID=40&md5=d74df814131b3ce915225da3494c8484,"SSDs exhibit very different failure characteristics compared to hard drives. In particular, the bit error rate (BER) of an SSD climbs as it receives more writes. As a result, RAID arrays composed from SSDs are subject to correlated failures. By balancing writes evenly across the array, RAID schemes can wear out devices at similar times. When a device in the array fails towards the end of its lifetime, the high BER of the remaining devices can result in data loss. We propose Diff-RAID, a parity-based redundancy solution that creates an age differential in an array of SSDs. Diff-RAID distributes parity blocks unevenly across the array, leveraging their higher update rate to age devices at different rates. To maintain this age differential when old devices are replaced by new ones, Diff-RAID reshuffles the parity distribution on each drive replacement. We evaluate Diff-RAID's reliability by using real BER data from 12 flash chips on a simulator and show that it is more reliable than RAID-5, in some cases by multiple orders of magnitude. We also evaluate Diff-RAID's performance using a software implementation on a 5-device array of 80 GB Intel X25-M SSDs and show that it offers a trade-off between throughput and reliability. © 2010 ACM.",Flash; RAID; SSD,Drives; Quality assurance; Software reliability; Data loss; Device arrays; Failure characteristics; Flash chips; Hard drives; Multiple orders; RAID arrays; Software implementation; Bit error rate
Extract and infer quickly: obtaining sector geometry of modern hard disk drives,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955205862&doi=10.1145%2f1807060.1807063&partnerID=40&md5=ce9891d18edd668c3c290b0cbe8be241,"The modern hard disk drive is a complex and complicated device. It consists of 2-4 heads, thousands of sectors per track, several hundred thousands of tracks, and tens of zones. The beginnings of adjacent tracks are placed with a certain angular offset. Sectors are placed on the tracks and accessed in some order. Angular offset and sector placement order vary widely subject to vendors and models. The success of an efficient file and storage subsystem design relies on the proper understanding of the underlying storage device characteristics. The characterization of hard disk drives has been a subject of intense research for more than a decade. The scale and complexity of state-of-the-art hard disk drive technology calls for a new way of extracting and analyzing the characteristics of the hard disk drive. In this work, we develop a novel disk characterization suite, DIG (Disk Geometry Analyzer), which allows us to rapidly extract and characterize the key performance metrics of the modern hard disk drive. Development of this tool is accompanied by thorough examination of four off-the-shelf hard disk drives. DIG consists of three key ingredients: O(1) a track boundary detection algorithm; O(log n) a zone boundary detection algorithm; and hybrid sampling based seek time profiling. We particularly focus on addressing the scalability aspect of disk characterization. With DIG, we are able to extract key metrics of hard disk drives, for example, track sizes, zone information, sector geometry and so on, within 3-20 minutes. DIG allows us to determine the sector layout mechanism of the underlying hard disk drive, for example, hybrid serpentine, cylinder serpentine, and surface serpentine, and to a build complete sector map from LBN to the three dimensional space of (Cylinder, Head, Sector). Examining the hard disk drives with DIG, we made a number of important observations. In modern hard disk drives, head switch overhead is far greater than track switch overhead. It seems that hard disk drive vendors put greater emphasis on reducing the number of head switches for data access. Most disk vendors use surface serpentine, cylinder serpentine, or hybrid serpentine schemes in laying sectors on the platters. The legacy seek time model, which takes the form of a + b √d leaves much to be desired for use in modern hard disk drives especially for short seeks (less than 5000 tracks). We compare the performance of the DIG against the existing state-of-the-art disk profiling algorithm. Compared to the existing state-of-the-art disk characterization algorithm, the DIG algorithm significantly decreases the time to extract comprehensive sector geometry information from 1920 minutes to 7 minutes and 1927 minutes to 180 minutes in best and worst case scenarios, respectively. © 2010 ACM.",Hard disk; Performance characterizaction; Sector geometery; Seek time; Track skew; Zone,Algorithms; Computational geometry; Cylinders (shapes); Disks (structural components); Drives; Serpentine; Signal detection; Silicate minerals; Adjacent track; Boundary detection algorithms; Data access; Disk geometries; Geometry information; Hard Disk Drive; Hard disks; Performance metrics; Sampling-based; Seek time; Storage devices; Storage subsystems; Three dimensional space; Track switches; Worst case scenario; Zone boundaries; Hard disk storage
A strategy to emulate NOR flash with NAND flash,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955189170&doi=10.1145%2f1807060.1807062&partnerID=40&md5=b15d5597006a424d1501a71ad8d66d07,"This work is motivated by a strong market demand for the replacement of NOR flash memory with NAND flash memory to cut down the cost of many embedded-system designs, such as mobile phones. Different from LRU-related caching or buffering studies, we are interested in prediction- based prefetching based on given execution traces of application executions. An implementation strategy is proposed for the storage of the prefetching information with limited SRAM and runtime overheads. An efficient prediction procedure is presented based on information extracted from application executions to reduce the performance gap between NAND flash memory and NOR flash memory in reads. With the behavior of a target application extracted from a set of collected traces, we show that data access to NOR flash memory can respond effectively over the proposed implementation. © 2010 ACM.",Data prefetching; Flash memory; NAND; NOR,NAND circuits; Static random access storage; Telecommunication equipment; Application execution; Data access; Data prefetching; Embedded system design; Execution trace; Implementation strategies; Market demand; NAND Flash; NAND flash memory; NOR flash; NOR flash memory; Prefetching; Runtime overheads; Target application; Flash memory
SOPA: Selecting the optimal caching policy adaptively,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955181479&doi=10.1145%2f1807060.1807064&partnerID=40&md5=2a916da215341618ddc60ae6bf861f09,"With the development of storage technology and applications, new caching policies are continuously being introduced. It becomes increasingly important for storage systems to be able to select the matched caching policy dynamically under varying workloads. This article proposes SOPA, a cache framework to adaptively select the matched policy and perform policy switches in storage systems. SOPA encapsulates the functions of a caching policy into a module, and enables online policy switching by policy reconstruction. SOPA then selects the policy matched with the workload dynamically by collecting and analyzing access traces. To reduce the decision-making cost, SOPA proposes an asynchronous decision making process. The simulation experiments show that no single caching policy performed well under all of the different workloads. With SOPA, a storage system could select the appropriate policy for different workloads. The real-system evaluation results show that SOPA reduced the average response time by up to 20.3% and 11.9% compared with LRU and ARC, respectively. © 2010 ACM.",Caching policies; Policy adaptation; Policy switch,Cost reduction; Switching functions; Caching policy; Decision making process; Policy adaptation; Response time; Simulation experiments; Storage systems; Storage technology; System evaluation; Decision making
Optimizing MEMS-based storage devices for mobile battery-powered systems,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950847409&doi=10.1145%2f1714454.1714455&partnerID=40&md5=0e2e34c601adbd33b5b888c7e6fce7ef,"An emerging storage technology, called MEMS-based storage, promises nonvolatile storage devices with ultrahigh density, high rigidity, a small form factor, and low cost. For these reasons, MEMS-based storage devices are suitable for battery-powered mobile systems such as PDAs. For deployment in such systems, MEMS-based storage devices must consume little energy. This work mainly targets reducing the energy consumption of this class of devices. We derive the operation modes of a MEMS-based storage device and systemically devise a policy in each mode for energy saving. Three types of policies are presented: power management, shutdown, and data-layout policy. Combined, these policies reduce the total energy consumed by a MEMS-based storage device. A MEMS-based storage device that enforces these policies comes close to Flash with respect to energy consumption and response time. However, enhancement on the device level is still needed; we present some suggestions to resolve this issue. © 2010 ACM.",Design space; Energy efficiency; Green storage; Mobile systems; Probe storage,Approximation theory; Energy efficiency; Energy policy; Probes; Battery-powered systems; Design space; Design spaces; Energy consumption; Energy saving; Low costs; Mobile systems; Nonvolatile storage device; Operation mode; Power managements; Probe storage; Response time; Small form factors; Storage devices; Storage technology; Total energy; Ultrahigh density; Nonvolatile storage
NCQ vs. I/O scheduler: Preventing unexpected misbehaviors,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950793978&doi=10.1145%2f1714454.1714456&partnerID=40&md5=e24aa3f70f22da8de01ea07489986dc1,"Native Command Queueing (NCQ) is an optimization technology to maximize throughput by reordering requests inside a disk drive. It has been so successful that NCQ has become the standard in SATA 2 protocol specification, and the great majority of disk vendors have adopted it for their recent disks. However, there is a possibility that the technology may lead to an information gap between the OS and a disk drive. A NCQ-enabled disk tries to optimize throughput without realizing the intention of an OS, whereas the OS does its best under the assumption that the disk will do as it is told without specific knowledge regarding the details of the disk mechanism. Let us call this expectation discord, which may cause serious problems such as request starvations or performance anomaly. In this article, we (1) confirm that expectation discord actually occurs in real systems; (2) propose software-level approaches to solve them; and (3) evaluate our mechanism. Experimental results show that our solution is simple, cheap (no special hardware required), portable, and effective. © 2010 ACM.",Hybrid scheduling; I/O prioritization; NCQ; SATA 2; Starvation detection,Scheduling; Disk drive; Hybrid scheduling; I/O scheduler; Information gap; Maximize throughput; Optimization technology; Performance anomaly; Prioritization; Protocol specifications; Real systems; Special hardware; Specific knowledge; Disks (structural components)
FRASH: Exploiting storage class memory in hybrid file system for hierarchical storage,2010,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950849330&doi=10.1145%2f1714454.1714457&partnerID=40&md5=7cc7b7c1d236ba25e1b68f066972efd9,"In this work, we develop a novel hybrid file system, FRASH, for storage-class memory and NAND Flash. Despite the promising physical characteristics of storage-class memory, its scale is an order of magnitude smaller than the current storage device scale. This fact makes it less than desirable for use as an independent storage device. We carefully analyze in-memory and on-disk file system objects in a log-structured file system, and exploit memory and storage aspects of the storage-class memory to overcome the drawbacks of the current log-structured file system. FRASH provides a hybrid view storage-class memory. It harbors an in-memory data structure as well as a on-disk structure. It provides nonvolatility to key data structures which have been maintained in-memory in a legacy log-structured file system. This approach greatly improves the mount latency and effectively resolves the robustness issue. By maintaining on-disk structure in storage-class memory, FRASH provides byte-addressability to the file system object and metadata for page, and subsequently greatly improves the I/O performance compared to the legacy log-structured approach. While storage-class memory offers byte granularity, it is still far slower than its DRAM counter part. We develop a copy-on-mount technique to overcome the access latency difference between main memory and storage-class memory. Our file system was able to reduce the mount time by 92% and file system I/O performance was increased by 16%. © 2010 ACM.",Flash storage; Log-structured file system,Data structures; Disks (structural components); Metadata; Mountings; Access latency; Disk files; Disk structures; File systems; Flash storage; Hierarchical storage; I/O performance; Main memory; NAND Flash; Nonvolatility; Order of magnitude; Physical characteristics; Robustness issues; Storage devices; Structured approach; Structured files; Flash memory
Preventing history forgery with secure provenance,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73849140516&doi=10.1145%2f1629080.1629082&partnerID=40&md5=60c3281fb090ba7d4d0f2a297985fff9,"As increasing amounts of valuable information are produced and persist digitally, the ability to determine the origin of data becomes important. In science, medicine, commerce, and government, data provenance tracking is essential for rights protection, regulatory compliance, management of intelligence and medical data, and authentication of information as it flows through workplace tasks. While significant research has been conducted in this area, the associated security and privacy issues have not been explored, leaving provenance information vulnerable to illicit alteration as it passes through untrusted environments. In this article, we show how to provide strong integrity and confidentiality assurances for data provenance information at the kernel, file system, or application layer. We describe Sprov, our provenance-aware system prototype that implements provenance tracking of data writes at the application layer, which makes Sprov extremely easy to deploy. We present empirical results that show that, for real-life workloads, the runtime overhead of Sprov for recording provenance with confidentiality and integrity guarantees ranges from 1% to 13%, when all file modifications are recorded, and from 12% to 16%, when all file read and modifications are tracked. © 2009 ACM.",Audit; Confidentiality; Integrity; Lineage; Provenance; Security,Network security; Regulatory compliance; Application layers; Data provenance; Empirical results; File systems; Medical data; Recording provenance; Rights protection; Runtime overheads; Security and privacy issues; System prototype; Quality assurance
Introduction to special issue FAST 2009,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73849149044&doi=10.1145%2f1629080.1629081&partnerID=40&md5=11eef825b8cfb8f34e3093b5f11f0c08,[No abstract available],,
CA-NFS: A congestion-aware network file system,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73849149035&doi=10.1145%2f1629080.1629085&partnerID=40&md5=a5c23eb3d296643fe802e944f36d53a5,"We develop a holistic framework for adaptively scheduling asynchronous requests in distributed file systems. The system is holistic in that it manages all resources, including network bandwidth, server I/O, server CPU, and client and server memory utilization. It accelerates, defers, or cancels asynchronous requests in order to improve application-perceived performance directly. We employ congestion pricing via online auctions to coordinate the use of system resources by the file system clients so that they can detect shortages and adapt their resource usage. We implement our modifications in the Congestion-Aware Network File System (CA-NFS), an extension to the ubiquitous network file system (NFS). Our experimental result shows that CA-NFS results in a 20% improvement in execution times when compared with NFS for a variety of workloads. © 2009 ACM.",Congestion; File systems; NFS; Performance; Scalability,Electronic commerce; Scalability; Congestion pricing; Congestion-aware; Distributed file systems; Execution time; File systems; Network bandwidth; Network file system; Online auctions; Performance scalability; Resource usage; Server memory; System resources; Ubiquitous networks; Traffic congestion
Generating realistic impressions for file-system benchmarking,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73849149036&doi=10.1145%2f1629080.1629086&partnerID=40&md5=c7bd5ebc32547b7b72ae71fc9489375f,"The performance of file systems and related software depends on characteristics of the underlying file-system image (i.e., file-system metadata and file contents). Unfortunately, rather than benchmarking with realistic file-system images, most system designers and evaluators rely on ad hoc assumptions and (often inaccurate) rules of thumb. Furthermore, the lack of standardization and reproducibility makes file-system benchmarking ineffective. To remedy these problems, we develop Impressions, a framework to generate statistically accurate file-system images with realistic metadata and content. Impressions is flexible, supporting user-specified constraints on various file-system parameters using a number of statistical techniques to generate consistent images. In this article, we present the design, implementation, and evaluation of Impressions and demonstrate its utility using desktop search as a case study. We believe Impressions will prove to be useful to system developers and users alike. © 2009 ACM.",File and storage system benchmarking,Metadata; Desktop search; File and storage system benchmarking; File contents; File systems; Reproducibilities; Statistical techniques; Storage systems; System designers; System developers; User-specified constraints; Benchmarking
Cumulus: Filesystem backup to the cloud,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73849139655&doi=10.1145%2f1629080.1629084&partnerID=40&md5=756e717a4a6d1ad83cbbdf579cf42f1d,"Cumulus is a system for efficiently implementing filesystem backups over the Internet, specifically designed under a thin cloud assumptionthat the remote datacenter storing the backups does not provide any special backup services, but only a least-common-denominator storage interface. Cumulus aggregates data from small files for storage and uses LFS-inspired segment cleaning to maintain storage efficiency. While Cumulus can use virtually any storage service, we show its efficiency is comparable to integrated approaches. © 2009 ACM.",Backup; Cloud storage,Backup; Backup service; Filesystem; Integrated approach; Small files; Storage efficiency; Storage services; Clouds
Causality-based versioning,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73849113741&doi=10.1145%2f1629080.1629083&partnerID=40&md5=bea8ba96e7e17349ec0303cd6ae2ef41,"Versioning file systems provide the ability to recover from a variety of failures, including file corruption, virus and worm infestations, and user mistakes. However, using versions to recover from data-corrupting events requires a human to determine precisely which files and versions to restore. We can create more meaningful versions and enhance the value of those versions by capturing the causal connections among files, facilitating selection and recovery of precisely the right versions after data corrupting events. We determine when to create new versions of files automatically using the causal relationships among files. The literature on versioning file systems usually examines two extremes of possible version-creation algorithms: open-to-close versioning and versioning on every write. We evaluate causal versions of these two algorithms and introduce two additional causality-based algorithms: Cycle-Avoidance and Graph-Finesse. We show that capturing and maintaining causal relationships imposes less than 7% overhead on a versioning system, providing benefit at low cost. We then show that Cycle-Avoidance provides more meaningful versions of files created during concurrent program execution, with overhead comparable to open/close versioning. Graph-Finesse provides even greater control, frequently at comparable overhead, but sometimes at unacceptable overhead. Versioning on every write is an interesting extreme case, but is far too costly to be useful in practice. © 2009 ACM.",Causality; Data provenance,Viruses; Causal relationships; Concurrent program execution; Data provenance; Extreme case; File corruption; File systems; Low costs; Versioning; Versioning systems; Concurrency control
Dynamic load balancing for I/O-intensive applications on clusters,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449176435&doi=10.1145%2f1629075.1629078&partnerID=40&md5=500a843209c630bdcac5067d24dd01fa,"Load balancing for clusters has been investigated extensively, mainly focusing on the effective usage of global CPU and memory resources. However, previous CPU- or memory-centric load balancing schemes suffer significant performance drop under I/O-intensive workloads due to the imbalance of I/O load. To solve this problem, we propose two simple yet effective I/O-aware load-balancing schemes for two types of clusters: (1) homogeneous clusters where nodes are identical and (2) heterogeneous clusters, which are comprised of a variety of nodes with different performance characteristics in computing power, memory capacity, and disk speed. In addition to assigning I/O-intensive sequential and parallel jobs to nodes with light I/O loads, the proposed schemes judiciously take into account both CPU and memory load sharing in the system. Therefore, our schemes are able to maintain high performance for a wide spectrum of workloads. We develop analytic models to study mean slowdowns, task arrival, and transfer processes in system levels. Using a set of real I/O-intensive parallel applications and synthetic parallel jobs with various I/O characteristics, we show that our proposed schemes consistently improve the performance over existing non-I/O-aware load-balancing schemes, including CPU- and Memory-aware schemes and a PBS-like batch scheduler for parallel and sequential jobs, for a diverse set of workload conditions. Importantly, this performance improvement becomes much more pronounced when the applications are I/O-intensive. For example, the proposed approaches deliver 23.6 - 88.0 % performance improvements for I/O-intensive applications such as LU decomposition, Sparse Cholesky, Titan, Parallel text searching, and Data Mining. When I/O load is low or well balanced, the proposed schemes are capable of maintaining the same level of performance as the existing non-I/O-aware schemes. © 2009 ACM.",Clusters; Heterogeneity; I/O-intensive applications; Load balancing; Storage systems,Analytic models; Batch schedulers; Cholesky; Computing power; Disk speed; Dynamic load balancing; Heterogeneous clusters; Homogeneous cluster; I/O-intensive applications; Load-Balancing; Load-balancing schemes; Lu decomposition; Memory capacity; Memory load; Memory resources; Parallel application; Parallel jobs; Parallel text; Performance characteristics; Performance improvements; Sequential jobs; Storage systems; System levels; Transfer process; Wide spectrum; Scheduling algorithms
"Higher reliability redundant disk arrays: Organization, operation, and coding",2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449170582&doi=10.1145%2f1629075.1629076&partnerID=40&md5=bc898c9bf4e73f347345b79bee25c8d8,"Parity is a popular form of data protection in redundant arrays of inexpensive/independent disks (RAID). RAID5 dedicates one out of N disks to parity to mask single disk failures, that is, the contents of a block on a failed disk can be reconstructed by exclusive-ORing the corresponding blocks on surviving disks. RAID5 can mask a single disk failure, and it is vulnerable to data loss if a second disk failure occurs. The RAID5 rebuild process systematically reconstructs the contents of a failed disk on a spare disk, returning the system to its original state, but the rebuild process may be unsuccessful due to unreadable sectors. This has led to two disk failure tolerant arrays (2DFTs), such as RAID6 based on Reed-Solomon (RS) codes. EVENODD, RDP (Row-Diagonal-Parity), the X-code, and RM2 (Row-Matrix) are 2DFTs with parity coding. RM2 incurs a higher level of redundancy than two disks, while the X-code is limited to a prime number of disks. RDP is optimal with respect to the number of XOR operations at the encoding, but not for short write operations. For small symbol sizes EVENODD and RDP have the same disk access pattern as RAID6, while RM2 and the X-code incur a high recovery cost with two failed disks. We describe variations to RAID5 and RAID6 organizations, including clustered RAID, different methods to update parities, rebuild processing, disk scrubbing to eliminate sector errors, and the intra-disk redundancy (IDR) method to deal with sector errors. We summarize the results of recent studies of failures in hard disk drives. We describe Markov chain reliability models to estimate RAID mean time to data loss (MTTDL) taking into account sector errors and the effect of disk scrubbing. Numerical results show that RAID5 plus IDR attains the same MTTDL level as RAID6, while incurring a lower performance penalty. We conclude with a survey of analytic and simulation studies of RAID performance and tools and benchmarks for RAID performance evaluation. © 2009 ACM.",Disk array; Disk failure studies; Performance evaluation; RAID; Reliability evaluation,Disks (machine components); Errors; Fault tolerant computer systems; Hard disk storage; Markov processes; Quality assurance; Redundancy; Simulators; Access patterns; Clustered RAID; Data loss; Data protection; Disk array; Disk failure; Disk failure tolerant; Disk scrubbing; Hard Disk Drive; Markov Chain; matrix; Numerical results; Performance evaluation; Performance penalties; Prime number; Recovery costs; Redundant arrays of inexpensive/independent disks; Reed-Solomon; Reliability Evaluation; Reliability model; Simulation studies; Small symbol size; Write operations; XOR operation; Disks (structural components)
Divide-and-conquer scheme for strictly optimal retrieval of range queries,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449196759&doi=10.1145%2f1629075.1629077&partnerID=40&md5=ca29ee50803050b735582a06d5aad3e8,"Declustering distributes data among parallel disks to reduce retrieval cost using I/O parallelism. Many schemes were proposed for single copy declustering of spatial data. Recently, declustering using replication gained a lot of interest and several schemes with different properties were proposed. It is computationally expensive to verify optimality of replication schemes designed for range queries and existing schemes verify optimality for up to 50 disks. In this article, we propose a novel method to find replicated declustering schemes that render all spatial range queries optimal. The proposed scheme uses threshold based declustering, divisibility of large queries for optimization and optimistic approach to compute maximum flow. The proposed scheme is generic and works for any number of dimensions. Experimental results show that using 3 copies there exist allocations that render all spatial range queries optimal for up to 750 disks in 2 dimensions and with the exception of several values for up to 100 disks in 3 dimensions. The proposed scheme improves search for strictly optimal replicated declustering schemes significantly and will be a valuable tool to answer open problems on replicated declustering. © 2009 ACM.",Declustering; Number theory; Parallel I/0; Replication; Spatial range query; Threshold,Cost reduction; Disks (machine components); Disks (structural components); Number theory; 3-dimension; Declustering; Declustering scheme; Divide and conquer; Maximum flows; Novel methods; Open problems; Optimality; Optimistic approach; Parallel disks; Range query; Spatial data; Spatial range queries; Optimization
A file assignment strategy independent of workload characteristic assumptions,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449150673&doi=10.1145%2f1629075.1629079&partnerID=40&md5=c013b873d47f3abc5ee1a640b50a4460,"The problem of statically assigning nonpartitioned files in a parallel I/O system has been extensively investigated. A basic workload characteristic assumption of most existing solutions to the problem is that there exists a strong inverse correlation between file access frequency and file size. In other words, the most popular files are typically small in size, while the large files are relatively unpopular. Recent studies on the characteristics of Web proxy traces suggested, however, the correlation, if any, is so weak that it can be ignored. Hence, the following two questions arise naturally. First, can existing algorithms still perform well when the workload assumption does not hold Second, if not, can one develop a new file assignment strategy that is immune to the workload assumption To answer these questions, we first evaluate the performance of three well-known file assignment algorithms with and without the workload assumption, respectively. Next, we develop a novel static nonpartitioned file assignment strategy for parallel I/O systems, called static round-robin (SOR), which is immune to the workload assumption. Comprehensive experimental results show that SOR consistently improves the performance in terms of mean response time over the existing schemes. © 2009 ACM.",File assignment; Load balancing; Parallel I/O; Workload characteristics; Zipfian distribution,Assignment algorithms; Assignment strategies; File access; File sizes; Inverse correlation; Load-Balancing; Mean response time; Parallel I/O; Parallel I/O systems; Round Robin; Web proxy; Workload characteristics; Internet service providers
Efficient management of idleness in storage systems,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67651024191&doi=10.1145%2f1534912.1534913&partnerID=40&md5=3e1d961fa9e8a8ff2ec03f14607610f8,"Various activities that intend to enhance performance, reliability, and availability of storage systems are scheduled with low priority and served during idle times. Under such conditions, idleness becomes a valuable resource that needs to be efficiently managed. A common approach in system design is to be nonwork conserving by idle waiting, that is, delay the scheduling of background jobs to avoid slowing down upcoming foreground tasks. In this article, we complement idle waiting with the estimation of background work to be served in every idle interval to effectively manage the trade-off between the performance of foreground and background tasks. As a result, the storage system is better utilized without compromising foreground performance. Our analysis shows that if idle times have low variability, then idle waiting is not necessary. Only if idle times are highly variable does idle waiting become necessary to minimize the impact of background activity on foreground performance. We further show that if there is burstiness in idle intervals, then it is possible to predict accurately the length of incoming idle intervals and use this information to serve more background jobs without affecting foreground performance.",Background jobs; Continuous data histogram; Foreground jobs; Idle periods; Idleness; Low priority work; Performance guarrante; Resource management; Storage systems,Natural resources management; Resource allocation; Background jobs; Continuous data histogram; Foreground jobs; Idle periods; Idleness; Low priority work; Performance guarrante; Resource management; Storage systems; Mobile telecommunication systems
Storing semi-structured data on disk drives,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67651061615&doi=10.1145%2f1534912.1534915&partnerID=40&md5=ac71d05bb6ce73892f4e8a31a2d5e650,"Applications that manage semi-structured data are becoming increasingly commonplace. Current approaches for storing semi-structured data use existing storage machinery; they either map the data to relational databases, or use a combination of flat files and indexes. While employing these existing storage mechanisms provides readily available solutions, there is a need to more closely examine their suitability to this class of data. Particularly, retrofitting existing solutions for semi-structured data can result in a mismatch between the tree structure of the data and the access characteristics of the underlying storage device (disk drive). This study explores various possibilities in the design space of native storage solutions for semi-structured data by exploring alternative approaches that match application data access characteristics to those of the underlying disk drive. For evaluating the effectiveness of the proposed native techniques in relation to the existing solution, we experiment with XML data using the XPathMark benchmark. Extensive evaluation reveals the strengths and weaknesses of the proposed native data layout techniques. While the existing solutions work really well for deep-focused queries into a semi-structured document (those that result in retrieving entire subtrees), the proposed native solutions substantially outperform for the non-deep-focused queries, which we demonstrate are at least as important as the deep-focused. We believe that native data layout techniques offer a unique direction for improving the performance of semi-structured data stores for a variety of important workloads. However, given that the proposed native techniques require circumventing current storage stack abstractions, further investigation is warranted before they can be applied to general-purpose storage systems.",Semi-structured data; Storage management; XML,Drives; Machinery; Markup languages; Retrofitting; XML; Alternative approach; Application data; Data layouts; Design spaces; Disk drive; Flat files; Native storage; Relational Database; Semi-structured data; Semi-structured documents; Storage devices; Storage management; Storage mechanism; Storage systems; Subtrees; Tree structures; XML data; XPathMark; Disks (structural components)
"POTSHARDSa secure, recoverable, long-term archival storage system",2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67651039764&doi=10.1145%2f1534912.1534914&partnerID=40&md5=805359fc0f2e3c0fe9cf0659c7fac1fd,"Users are storing ever-increasing amounts of information digitally, driven by many factors including government regulations and the public's desire to digitally record their personal histories. Unfortunately, many of the security mechanisms that modern systems rely upon, such as encryption, are poorly suited for storing data for indefinitely long periods of time; it is very difficult to manage keys and update cryptosystems to provide secrecy through encryption over periods of decades. Worse, an adversary who can compromise an archive need only wait for cryptanalysis techniques to catch up to the encryption algorithm used at the time of the compromise in order to obtain secure data. To address these concerns, we have developed POTSHARDS, an archival storage system that provides long-term security for data with very long lifetimes without using encryption. Secrecy is achieved by using unconditionally secure secret splitting and spreading the resulting shares across separately managed archives. Providing availability and data recovery in such a system can be difficult; thus, we use a new technique, approximate pointers, in conjunction with secure distributed RAID techniques to provide availability and reliability across independent archives. To validate our design, we developed a prototype POTSHARDS implementation. In addition to providing us with an experimental testbed, this prototype helped us to understand the design issues that must be addressed in order to maximize security.",Approximate pointers; Archival storage; Secret splitting,Laws and legislation; Network security; Approximate pointers; Archival storage; Archival storage systems; Cryptosystems; Data recovery; Design issues; Distributed RAID; Encryption algorithms; Experimental testbed; Government regulation; Long lifetime; Secret splitting; Secure data; Security mechanism; Cryptography
Umbrella file system: Storage management across heterogeneous devices,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849274265&doi=10.1145%2f1502777.1502780&partnerID=40&md5=86806346eb828a9148d6e2cb85a771c2,"With the advent of and recent developments in Flash storage, device characteristic diversity is becoming both more prevalent and more distinct. In this article, we describe the Umbrella File System (UmbrellaFS), a stackable file system designed to provide flexibility in matching diversity of file access characteristics to diversity of device characteristics through a user or system administrator specified policy. We present the design and results from a prototype implementation of UmbrellaFS on both Linux 2.4 and 2.6. The results show that UmbrellaFS has little overhead for most file system operations while providing an ability better to utilize the differences in Flash and traditional hard drives. With appropriate use of rules, we have shown improvements of up to 44% in certain situations. © 2009 ACM.",Device characteristics; Flash drives; Namespaces; Policy-driven storage,Computer operating systems; Software prototyping; Device characteristics; File access; File systems; Flash drives; Flash storage; Hard drives; Heterogeneous devices; Namespaces; Policy-driven storage; Prototype implementations; Stackable file system; Storage management; System administrators; Drives
P/PA-SPTF: Parallelism-aware request scheduling algorithms for MEMS-based storage devices,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849248962&doi=10.1145%2f1502777.1502778&partnerID=40&md5=84358dcfd11240dfac1a16d734164154,"MEMS-based storage is foreseen as a promising storage media that provides high-bandwidth, low-power consumption, high-density, and low cost. Due to these versatile features, MEMS storage is anticipated to be used for a wide range of applications from storage for small handheld devices to high capacity mass storage servers. However, MEMS storage has vastly different physical characteristics compared to a traditional disk. First, MEMS storage has thousands of heads that can be activated simultaneously. Second, the media of MEMS storage is a square structure which is different from the platter structure of disks. This article presents a new request scheduling algorithm for MEMS storage called P-SPTF that makes use of the aforementioned characteristics. P-SPTF considers the parallelism of MEMS storage as well as the seek time of requests on the two dimensional square structure. We then present another algorithm called PA-SPTF that considers the aging factor so that starvation resistance is improved. Simulation studies show that PA-SPTF improves the performance of MEMS storage by up to 39.2% in terms of the average response time and 62.4% in terms of starvation resistance compared to the widely acknowledged SPTF algorithm. We also show that there exists a spectrum of scheduling algorithms that subsumes both the P-SPTF and PA-SPTF algorithms. © 2009 ACM.",MEMS-based storage; Parallelism; Scheduling; Seek time; Starvation,Disks (structural components); MEMS; Microelectromechanical devices; Aging factors; Hand held device; High bandwidth; High capacity; High-density; Low costs; Low-power consumption; Mass storage; Parallelism; Physical characteristics; Response time; Seek time; Simulation studies; Starvation; Storage devices; Storage media; Scheduling algorithms
A new approach to secure logging,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849178422&doi=10.1145%2f1502777.1502779&partnerID=40&md5=a6633a5051782335bd048efe53025edf,"The need for secure logging is well-understood by the security professionals, including both researchers and practitioners. The ability to efficiently verify all (or some) log entries is important to any application employing secure logging techniques. In this article, we begin by examining the state of the art in secure logging and identify some problems inherent to systems based on trusted third-party servers. We then propose a different approach to secure logging based upon recently developed Forward-Secure Sequential Aggregate (FssAgg) authentication techniques. Our approach offers both space-efficiency and provable security. We illustrate two concrete schemes-one private-verifiable and one public-verifiable-that offer practical secure logging without any reliance on online trusted third parties or secure hardware. We also investigate the concept of immutability in the context of forward-secure sequential aggregate authentication to provide finer grained verification. Finally we evaluate proposed schemes and report on our experience with implementing them within a secure logging system. © 2009 ACM.",Forward secure sequential aggregate (FssAgg) authentication; Forward-secure stream integrity; MACs; Secure logging; Signatures; Truncation attack,Aggregates; Authentication; Harvesting; Logging (forestry); Forward secure sequential aggregate (FssAgg) authentication; Forward-secure stream integrity; MACs; Secure logging; Signatures; Truncation attack; Well logging
Kinesis: A new approach to replica placement in distributed storage systems,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60449086447&doi=10.1145%2f1480439.1480440&partnerID=40&md5=2d7cbd7b55ff58dd133165a25074d2aa,"Kinesis is a novel data placement model for distributed storage systems. It exemplifies three design principles: structure (division of servers into a few failure-isolated segments), freedom of choice (freedom to allocate the best servers to store and retrieve data based on current resource availability), and scattered distribution (independent, pseudo-random spread of replicas in the system). These design principles enable storage systems to achieve balanced utilization of storage and network resources in the presence of incremental system expansions, failures of single and shared components, and skewed distributions of data size and popularity. In turn, this ability leads to significantly reduced resource provisioning costs, good user-perceived response times, and fast, parallelized recovery from independent and correlated failures. This article validates Kinesis through theoretical analysis, simulations, and experiments on a prototype implementation. Evaluations driven by real-world traces show that Kinesis can significantly outperform the widely used Chain replica-placement strategy in terms of resource requirements, end-to-end delay, and failure recovery. © 2009 ACM.",Load balancing; Multiple-choice paradigm; Storage system,Software prototyping; Data placements; Data sizes; Design Principles; Distributed storage systems; End-to-end delays; Failure recoveries; Load balancing; Multiple-choice paradigm; Network resources; New approaches; On currents; Placement strategies; Prototype implementations; Pseudo randoms; Real-world; Replica placements; Resource availabilities; Resource provisioning; Resource requirements; Response time; Shared components; Skewed distributions; Storage system; System expansions; Electric network analysis
GRID codes: Strip-based erasure codes with high fault tolerance for storage systems,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60449118872&doi=10.1145%2f1480439.1480444&partnerID=40&md5=1743296bf671e6036bf1528fc392e701,"As storage systems grow in size and complexity, they are increasingly confronted with concurrent disk failures together with multiple unrecoverable sector errors. To ensure high data reliability and availability, erasure codes with high fault tolerance are required. In this article, we present a new family of erasure codes with high fault tolerance, named GRID codes. They are called such because they are a family of strip-based codes whose strips are arranged into multi-dimensional grids. In the construction of GRID codes, we first introduce a concept of matched codes and then discuss how to use matched codes to construct GRID codes. In addition, we propose an iterative reconstruction algorithm for GRID codes. We also discuss some important features of GRID codes. Finally, we compare GRID codes with several categories of existing codes. Our comparisons show that for large-scale storage systems, our GRID codes have attractive advantages over many existing erasure codes: (a) They are completely XOR-based and have very regular structures, ensuring easy implementation; (b) they can provide up to 15 and even higher fault tolerance; and (c) their storage efficiency can reach up to 80% and even higher. All the advantages make GRID codes more suitable for large-scale storage systems. © 2009 ACM.",Disk failure; Erasure code; Fault tolerance; Storage system; Unrecoverable sector error,Disks (structural components); Errors; Fault tolerance; Fault tolerant computer systems; Quality assurance; Data reliabilities; Dimensional grids; Disk failure; Erasure code; Iterative reconstruction algorithms; Regular structures; Storage efficiencies; Storage system; Unrecoverable sector error; Reliability
Rethinking FTP: Aggressive block reordering for large file transfers,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60449092042&doi=10.1145%2f1480439.1480442&partnerID=40&md5=59958ca0d1f342c4028e8c0fd2b4d9f6,"Whole-file transfer is a basic primitive for Internet content dissemination. Content servers are increasingly limited by disk arm movement, given the rapid growth in disk density, disk transfer rates, server network bandwidth, and content size. Individual file transfers are sequential, but the block access sequence on a content server is effectively random when many slow clients access large files concurrently. Although larger blocks can help improve disk throughput, buffering requirements increase linearly with block size. This article explores a novel block reordering technique that can reduce server disk traffic significantly when large content files are shared. The idea is to transfer blocks to each client in any order that is convenient for the server. The server sends blocks to each client opportunistically in order to maximize the advantage from the disk reads it issues to serve other clients accessing the same file. We first illustrate the motivation and potential impact of aggressive block reordering using simple analytical models. Then we describe a file transfer system using a simple block reordering algorithm, called Circus. Experimental results with the Circus prototype show that it can improve server throughput by a factor of two or more in workloads with strong file access locality. © 2009 ACM.",Disk access; File transfer protocols; Scheduling,Internet protocols; Scheduling; Servers; Analytical models; Arm movements; Block sizes; Buffering requirements; Content servers; Disk access; Disk traffics; Disk transfer rates; File access; File transfer protocols; File transfer systems; File transfers; Internet contents; Network bandwidths; Potential impacts; Rapid growths; Disks (structural components)
JFTL: A flash translation layer based on a journal remapping for flash memory,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60449113416&doi=10.1145%2f1480439.1480443&partnerID=40&md5=0d76a39eac937385109416164ad23a9d,"In flash memory-based storage, a Flash Translation Layer (FTL) manages the mapping between the logical addresses of a file system and the physical addresses of the flash memory. When a journaling file system is set up on the FTL, the consistency of the file system is guaranteed by duplications of the same file system changes in both the journal region of the file system and the home locations of the changes. However, these duplications inevitably degrade the performance of the file system. In this article we present an efficient FTL, called JFTL, based on a journal remapping technique. The FTL uses an address mapping method to write all the data to a new region in a process known as an out-of-place update. Because of this process, the existing data in flash memory is not overwritten by such an update. By using this characteristic of the FTL, the JFTL remaps addresses of the logged file system changes to addresses of the home locations of the changes, instead of writing the changes once more to flash memory. Thus, the JFTL efficiently eliminates redundant data in the flash memory as well as preserving the consistency of the journaling file system. Our experiments confirm that, when associated with a writeback or ordered mode of a conventional EXT3 file system, the JFTL enhances the performance of EXT3 by up to 20%. Furthermore, when the JFTL operates with a journaled mode of EXT3, there is almost a twofold performance gain in many cases. Moreover, the recovery performance of the JFTL is much better than that of the FTL. © 2009 ACM.",Flash memory; Flash translation layer; Garbage detection; Journal remapping; Journaling file system,Translation (languages); Flash translation layer; Garbage detection; Journal remapping; Journaling file system; Mapping methods; Performance gains; Physical address; Recovery performance; Redundant datum; Write backs; Flash memory
QoS for storage subsystems using IEEE-1394,2009,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60449095385&doi=10.1145%2f1480439.1480441&partnerID=40&md5=0d4c436b391eefe1ed8b0c898d5490a7,"IEEE-1394 is widely adopted in various commercial products for computing, communication, and entertainment. Although many services with Quality-of-Service (QoS) supports are now available in systems over IEEE-1394, little work is done for QoS-based resource allocation. In this article, we aim at the design of a bandwidth reservation mechanism and its policy for isochronous requests, such as those from cameras. We then address the QoS support issue for asynchronous requests, such as those from disks, and an analytic framework for probability-based QoS guarantees. This work is concluded by the proposing of a topology configuration algorithm for IEEE-1394 devices. The capability of the proposed methodology and the analytic framework are evaluated by a series of experiments over a Linux-based system prototype. © 2009 ACM.",I/O subsystem; IEEE-1394; Quality-of-service; Real time,Machine design; Planning; Resource allocation; Bandwidth reservations; Commercial products; I/O subsystem; IEEE-1394; Linux-based systems; QoS guarantees; Qos supports; QoS-based resource allocations; Real time; Storage subsystems; Topology configurations; Quality of service
Portably solving file races with hardness amplification,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-59449102169&doi=10.1145%2f1416944.1416948&partnerID=40&md5=5e5b4f2e0d98b6e0054a3173a389d30f,"The file-system API of contemporary systems makes programs vulnerable to TOCTTOU (timeof-check-to-time-of-use) race conditions. Existing solutions either help users to detect these problems (by pinpointing their locations in the code), or prevent the problem altogether (by modifying the kernel or its API). But the latter alternative is not prevalent, and the former is just the first step: Programmers must still address TOCTTOU flaws within the limits of the existing API with which several important tasks cannot be accomplished in a portable straightforward manner. Recently, Dean and Hu [2004] addressed this problem and suggested a probabilistic hardness amplification approach that alleviated the matter. Alas, shortly after, Borisov et al. [2005] responded with an attack termed filesystem maze that defeated the new approach. We begin by noting that mazes constitute a generic way to deterministically win many TOCTTOU races (gone are the days when the probability was small). In the face of this threat, we: (1) develop a new user-level defense that can withstand mazes; and (2) show that our method is undefeated even by much stronger hypothetical attacks that provide the adversary program with ideal conditions to win the race (enjoying complete and instantaneous knowledge about the defending programs actions and being able to perfectly synchronize accordingly). The fact that our approach is immune to these unrealistic attacks suggests it can be used as a simple and portable solution to a large class of TOCTTOU vulnerabilities, without requiring modifications to the underlying operating system. © 2008 ACM.",Race conditions; Time-of-check-to-time-of-use; TOCTTOU,Amplification; Hardness; Hazards and race conditions; File systems; Hardness amplifications; Large class; New approaches; Operating systems; Race conditions; Time-of-check-to-time-of-use; TOCTTOU; Application programming interfaces (API)
An analysis of data corruption in the storage stack,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-59449105324&doi=10.1145%2f1416944.1416947&partnerID=40&md5=c30c34a763f21284ce71da77926b4561,"An important threat to reliable storage of data is silent data corruption. In order to develop suitable protection mechanisms against data corruption, it is essential to understand its characteristics. In this article, we present the first large-scale study of data corruption. We analyze corruption instances recorded in production storage systems containing a total of 1.53 million disk drives, over a period of 41 months. We study three classes of corruption: checksum mismatches, identity discrepancies, and parity inconsistencies. We focus on checksum mismatches since they occur the most. We find more than 400,000 instances of checksum mismatches over the 41-month period. We find many interesting trends among these instances, including: (i) nearline disks (and their adapters) develop checksum mismatches an order of magnitude more often than enterprise-class disk drives, (ii) checksum mismatches within the same disk are not independent events and they show high spatial and temporal locality, and (iii) checksum mismatches across different disks in the same storage system are not independent. We use our observations to derive lessons for corruption-proof system design. © 2008 ACM.",Data corruption; Disk drive reliability,Disks (machine components); Drives; Analysis of datum; Checksum; Data corruption; Disk drive reliability; Order of magnitudes; Proof systems; Protection mechanisms; Spatial and temporal localities; Storage systems; Disks (structural components)
Are disks the dominant contributor for storage failures? A comprehensive study of storage subsystem failure characteristics,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-59449086113&doi=10.1145%2f1416944.1416946&partnerID=40&md5=9a9f3055d9117ce0525e5c4a5c7cb823,"Building reliable storage systems becomes increasingly challenging as the complexity of modern storage systems continues to grow. Understanding storage failure characteristics is crucially important for designing and building a reliable storage system. While several recent studies have been conducted on understanding storage failures, almost all of them focus on the failure characteristics of one componentdisksand do not study other storage component failures. This article analyzes the failure characteristics of storage subsystems. More specifically, we analyzed the storage logs collected from about 39,000 storage systems commercially deployed at various customer sites. The dataset covers a period of 44 months and includes about 1,800,000 disks hosted in about 155,000 storage-shelf enclosures. Our study reveals many interesting findings, providing useful guidelines for designing reliable storage systems. Some of our major findings include: (1) In addition to disk failures that contribute to 20 - 55% of storage subsystem failures, other components such as physical interconnects and protocol stacks also account for a significant percentage of storage subsystem failures. (2) Each individual storage subsystem failure type, and storage subsystem failure as a whole, exhibits strong self-correlations. In addition, these failures exhibit bursty patterns. (3) Storage subsystems configured with redundant interconnects experience 30 - 40% lower failure rates than those with a single interconnect. (4) Spanning disks of a RAID group across multiple shelves provides a more resilient solution for storage subsystems than within a single shelf. © 2008 ACM.",Disk failures; Failure characteristics; Storage subsystem; Storage system,Disks (machine components); Comprehensive studies; Data sets; Disk failures; Failure characteristics; Failure rates; Protocol stacks; Redundant interconnects; Storage components; Storage subsystem; Storage system; Disks (structural components)
Write off-loading: Practical power management for enterprise storage,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-59449092139&doi=10.1145%2f1416944.1416949&partnerID=40&md5=fdf472ddb79d2aebeacd76b3f69e69b9,"In enterprise data centers power usage is a problem impacting server density and the total cost of ownership. Storage uses a significant fraction of the power budget and there are no widely deployed power-saving solutions for enterprise storage systems. The traditional view is that enterprise workloads make spinning disks down ineffective because idle periods are too short. We analyzed block-level traces from 36 volumes in an enterprise data center for one week and concluded that significant idle periods exist, and that they can be further increased by modifying the read/write patterns using write off-loading. Write off-loading allows write requests on spun-down disks to be temporarily redirected to persistent storage elsewhere in the data center. The key challenge is doing this transparently and efficiently at the block level, without sacrificing consistency or failure resilience. We describe our write off-loading design and implementation that achieves these goals. We evaluate it by replaying portions of our traces on a rack-based testbed. Results show that just spinning disks down when idle saves 28 - 36% of energy, and write off-loading further increases the savings to 45 - 60%. © 2008 ACM.",Disk spin-down; DiskEnergy; Energy; Enterprise storage; Power; Write off-loading,Disks (machine components); Disks (structural components); Energy management; Satellite communication systems; Spin dynamics; Disk spin-down; DiskEnergy; Energy; Enterprise storage; Power; Write off-loading; Loading
Introduction to special issue of USENIX FAST 2008,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-59449106701&doi=10.1145%2f1416944.1416945&partnerID=40&md5=c4b900ecd22a530d3fc6bb6eeb3a52db,[No abstract available],,
A nine year study of file system and storage benchmarking,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849098947&doi=10.1145%2f1367829.1367831&partnerID=40&md5=8c061bd82f1e2608fa2dceed9d0808fc,"Benchmarking is critical when evaluating performance, but is especially difficult for file and storage systems. Complex interactions between I/O devices, caches, kernel daemons, and other OS components result in behavior that is rather difficult to analyze. Moreover, systems have different features and optimizations, so no single benchmark is always suitable. The large variety of workloads that these systems experience in the real world also adds to this difficulty. In this article we survey 415 file system and storage benchmarks from 106 recent papers. We found that most popular benchmarks are flawed and many research papers do not provide a clear indication of true performance. We provide guidelines that we hope will improve future performance evaluations. To show how some widely used benchmarks can conceal or overemphasize overheads, we conducted a set of experiments. As a specific example, slowing down read operations on ext2 by a factor of 32 resulted in only a 2 - 5% wall-clock slowdown in a popular compile benchmark. Finally, we discuss future work to improve file system and storage benchmarking. © 2008 ACM.",Benchmarks; File systems; Storage systems,Benchmarking; Computer networks; Multitasking; Osmium; Paper; Research; Set theory; Complex interactions; file systems; Future performance; Future work; I/O devices; Read operations; Real world; Research papers; Storage systems; Storage (materials)
A new approach to dynamic self-tuning of database buffers,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849092214&doi=10.1145%2f1353452.1353455&partnerID=40&md5=60058348dcf99238f820c62096e03c38,"Current businesses rely heavily on efficient access to their databases. Manual tuning of these database systems by performance experts is increasingly infeasible: For small companies, hiring an expert may be too expensive; for large enterprises, even an expert may not fully understand the interaction between a large system and its multiple changing workloads. This trend has led major vendors to offer tools that automatically and dynamically tune a database system. Many database tuning knobs concern the buffer pool for caching data and disk pages. Specifically, these knobs control the buffer allocation and thus the cache miss probability, which has direct impact on performance. Previous methods for automatic buffer tuning are based on simulation, black-box control, gradient descent, and empirical equations. This article presents a new approach, using calculations with an analytically-derived equation that relates miss probability to buffer allocation; this equation fits four buffer replacement policies, as well as twelve datasets from mainframes running commercial databases in large corporations. The equation identifies a buffer-size limit that is useful for buffer tuning and powering down idle buffers. It can also replace simulation in predicting I/O costs. Experiments with PostgreSQL illustrate how the equation can help optimize online buffer partitioning, ensure fairness in buffer reclamation, and dynamically retune the allocation when workloads change. It is also used, in conjunction with DB2's interface for retrieving miss data, for tuning DB2 buffer allocation to achieve targets for differentiated service. © 2008 ACM.",Autonomic computing; Buffer allocation; Miss probability,Computer networks; Dynamical systems; Gradient methods; Industry; Knobs; Multitasking; Numerical methods; Probability; Tuning; Black boxes; buffer allocation; Cache misses; Caching data; Data-sets; Differentiated service (DiffServ); Direct impact; Empirical equations; Gradient descent (GD); large systems; manual tuning; new approaches; PostgreSQL; Self tuning (ST); Small companies; Tuning knobs; Database systems
A new intra-disk redundancy scheme for high-reliability RAID storage systems in the presence of unrecoverable errors,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849104000&doi=10.1145%2f1353452.1353453&partnerID=40&md5=98657a1f01169e40c111c67e4ac886be,"Today's data storage systems are increasingly adopting low-cost disk drives that have higher capacity but lower reliability, leading to more frequent rebuilds and to a higher risk of unrecoverable media errors. We propose an efficient intradisk redundancy scheme to enhance the reliability of RAID systems. This scheme introduces an additional level of redundancy inside each disk, on top of the RAID redundancy across multiple disks. The RAID parity provides protection against disk failures, whereas the proposed scheme aims to protect against media-related unrecoverable errors. In particular, we consider an intradisk redundancy architecture that is based on an interleaved parity-check coding scheme, which incurs only negligible I/O performance degradation. A comparison between this coding scheme and schemes based on traditional Reed - Solomon codes and single-parity-check codes is conducted by analytical means. A new model is developed to capture the effect of correlated unrecoverable sector errors. The probability of an unrecoverable failure associated with these schemes is derived for the new correlated model, as well as for the simpler independent error model. We also derive closed-form expressions for the mean time to data loss of RAID-5 and RAID-6 systems in the presence of unrecoverable errors and disk failures. We then combine these results to characterize the reliability of RAID systems that incorporate the intradisk redundancy scheme. Our results show that in the practical case of correlated errors, the interleaved parity-check scheme provides the same reliability as the optimum, albeit more complex, Reed - Solomon coding scheme. Finally, the I/O and throughput performances are evaluated by means of analysis and event-driven simulation. © 2008 ACM.",File and I/O systems; RAID; Reliability analysis; Stochastic modeling,Codes (standards); Codes (symbols); Coding errors; Data storage equipment; Disks (structural components); Errors; Failure (mechanical); Input output programs; Mathematical models; Nonvolatile storage; Optical communication; Programming theory; Quality assurance; Redundancy; Reed-Solomon codes; Risk assessment; Storage (materials); Sulfate minerals; Throughput; Closed-form expressions; Coding schemes; Data losses; Data storage systems; disk drives; disk failures; error modeling; Event-driven simulations; High-reliability; Input output (I/O) performance; new model; RAID storage; Random array of independent drives disk (RAID) systems; redundancy scheme; Reed solomon (RS); Reed-Solomon coding scheme; Single-parity-check (SPC); Reliability
Intel® Turbo Memory: Nonvolatile disk caches in the storage hierarchy of mainstream computer systems,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849118782&doi=10.1145%2f1367829.1367830&partnerID=40&md5=3764c580cf98623eacb24ecac3238e03,"Hard-disk drives are a significant bottleneck to system performance and are also responsible for a significant fraction of total system power consumption. Intel Turbo Memory addresses these problems by adding a new layer to the storage hierarchy: a platform-based and nonvolatile, disk cache. In this article, we describe the hardware and software elements of the Intel Turbo Memory architecture. We show how it supports the new ReadyBoost and ReadyDrive features in Microsoft Vista and describe its key caching algorithms. We present performance, power savings, and wear-leveling results achieved by Intel Turbo Memory. © 2008 ACM.",Disk cache; NAND; Nonvolatile memory; Solid-state disk; Write-back,Computer networks; Computer systems; Computers; Data storage equipment; Disks (structural components); Energy conservation; Energy storage; Hard disk storage; Multitasking; Nonvolatile storage; Software architecture; Storage (materials); Caching algorithms; Disk caching; Hard disk drives (HDDs); Hardware and software; Mainstream (MS); memory addressing; Memory architecture(MA); Microsoft (CO); Non-volatile; Power consumption (CE); Power savings; storage hierarchy; system performances; Cache memory
Predictive data grouping: Defining the bounds of energy and latency reduction through predictive data grouping and replication,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849096822&doi=10.1145%2f1353452.1353454&partnerID=40&md5=6435ceeb6c66ffe6aed4bc8658fc93c5,"We demonstrate that predictive grouping is an effective mechanism for reducing disk arm movement, thereby simultaneously reducing energy consumption and data access latency. We further demonstrate that predictive grouping has untapped dramatic potential to further improve access performance and limit energy consumption. Data retrieval latencies are considered a major bottleneck, and with growing volumes of data and increased storage needs it is only growing in significance. Data storage infrastructure is therefore a growing consumer of energy at data-center scales, while the individual disk is already a significant concern for mobile computing (accounting for almost a third of a mobile system's energy demands). While improving responsiveness of storage subsystems and hence reducing latencies in data retrieval is often considered contradictory with efforts to reduce disk energy consumption, we demonstrate that predictive data grouping has the potential to simultaneously work towards both these goals. Predictive data grouping has advantages in its applicability compared to both prior approaches to reducing latencies and to reducing energy usage. For latencies, grouping can be performed opportunistically, thereby avoiding the serious performance penalties that can be incurred with prior applications of access prediction (such as predictive prefetching of data). For energy, we show how predictive grouping can even save energy use for an individual disk that is never idle. Predictive data grouping with effective replication results in a reduction of the overall mechanical movement required to retrieve data. We have built upon our detailed measurements of disk power consumption, and have estimated both the energy expended by a hard disk for its mechanical components, and that needed to move the disk arm. We have further compared, via simulation, three models of predictive grouping of on-disk data, including an optimal arrangement of data that is guaranteed to minimize disk arm movement. These experiments have allowed us to measure the limits of performance improvement achievable with optimal data grouping and replication strategies on a single device, and have further allowed us to demonstrate the potential of such schemes to reduce energy consumption of mechanical components by up to 70%. © 2008 ACM.",Data grouping; Latency; Layout optimization; Power; Power management; Replication,"Computer networks; Data storage equipment; Disks (structural components); Energy conservation; Energy management; Energy policy; Energy resources; Machine design; Mathematical models; Mechanical engineering; Multitasking; Storage (materials); Wireless telecommunication systems; (p ,p ,t) measurements; Arm movements; Data accessing; Data groupings; Data retrieval; Data storage infrastructure; Disk data; Energy consumption; energy demands; Hard disks; Individual (PSS 544-7); latency reduction; limit energy; Mechanical components; Mechanical movements; Mobile systems; Optimal data; performance improvements; Performance penalties; Power consumption (CE); pre-fetching; Reducing energy; Reducing energy consumption; Replication strategies; Save energy; Storage Subsystem (STO); Data reduction"
"B-trees, shadowing, and clones",2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40749137424&doi=10.1145%2f1326542.1326544&partnerID=40&md5=335866431b74e93a421cef5747376059,"B-trees are used by many file systems to represent files and directories. They provide guaranteed logarithmic time key-search, insert, and remove. File systems like WAFL and ZFS use shadowing, or copy-on-write, to implement snapshots, crash recovery, write-batching, and RAID. Serious difficulties arise when trying to use b-trees and shadowing in a single system. This article is about a set of b-tree algorithms that respects shadowing, achieves good concurrency, and implements cloning (writeable snapshots). Our cloning algorithm is efficient and allows the creation of a large number of clones. We believe that using our b-trees would allow shadowing file systems to better scale their on-disk data structures. © 2008 ACM.",B-trees; Concurrency; Copy-on-write; Shadowing; Snapshots,Algorithms; Concurrency control; Data structures; File editors; B-trees; Cloning algorithm; Shadowing; Writeable snapshots; Trees (mathematics)
Storage optimization for large-scale distributed stream-processing systems,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40749098631&doi=10.1145%2f1326542.1326547&partnerID=40&md5=da7c494145ffcc5aa8319d59196bb57c,"We consider storage in an extremely large-scale distributed computer system designed for stream processing applications. In such systems, both incoming data and intermediate results may need to be stored to enable analyses at unknown future times. The quantity of data of potential use would dominate even the largest storage system. Thus, a mechanism is needed to keep the data most likely to be used. One recently introduced approach is to employ retention value functions, which effectively assign each data object a value that changes over time in a prespecified way [Douglis et al.2004]. Storage space for data entering the system is reclaimed automatically by deleting data of the lowest current value. In such large systems, there will naturally be multiple file systems available, each with different properties. Choosing the right file system for a given incoming stream of data presents a challenge. In this article we provide a novel and effective scheme for optimizing the placement of data within a distributed storage subsystem employing retention value functions. The goal is to keep the data of highest overall value, while simultaneously balancing the read load to the file system. The key aspects of such a scheme are quite different from those that arise in traditional file assignment problems. We further motivate this optimization problem and describe a solution, comparing its performance to other reasonable schemes via simulation experiments. © 2008 ACM.",File assignment problem; Load balancing; Optimization; Storage management; Streaming systems; Theory,Distributed computer systems; Function evaluation; Problem solving; Resource allocation; File assignment problem; Retention value functions; Storage management; Streaming systems; Data storage equipment
Workload-based generation of administrator hints for optimizing database storage utilization,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40749127207&doi=10.1145%2f1326542.1326545&partnerID=40&md5=aea18ba431dc40dba1baf0361613cea8,"Database storage management at data centers is a manual, time-consuming, and error-prone task. Such management involves regular movement of database objects across storage nodes in an attempt to balance the I/O bandwidth utilization across disk drives. Achieving such balance is critical for avoiding I/O bottlenecks and thereby maximizing the utilization of the storage system. However, manual management of the aforesaid task, apart from increasing administrative costs, encumbers the greater risks of untimely and erroneous operations. We address the preceding concerns with STORM, an automated approach that combines low-overhead information gathering of database access and storage usage patterns with efficient analysis to generate accurate and timely hints for the administrator regarding data movement operations. STORM's primary objective is minimizing the volume of data movement required (to minimize potential down-time or reduction in performance) during the reconfiguration operation, with the secondary constraints of space and balanced I/O-bandwidth-utilization across the storage devices. We analyze and evaluate STORM theoretically, using a simulation framework, as well as experimentally. We show that the dynamic data layout reconfiguration problem is NP-hard and we present a heuristic that provides an approximate solution in O(Nlog(N/M) + (N/M)2) time, where M is the number of storage devices and N is the total number of database objects residing in the storage devices. A simulation study shows that the heuristic converges to an acceptable solution that is successful in balancing storage utilization with an accuracy that lies within 7% of the ideal solution. Finally, an experimental study demonstrates that the STORM approach can improve the overall performance of the TPC-C benchmark by as much as 22%, by reconfiguring an initial random, but evenly distributed, placement of database objects. © 2008 ACM.",Algorithms; Design; Performance,Bandwidth; Data storage equipment; Information retrieval; Problem solving; Data centers; Disk drives; Error-prone task; Database systems
Niobe: A practical replication protocol,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40749108998&doi=10.1145%2f1326542.1326543&partnerID=40&md5=20d4108cee6783f5e300d6350992cf4a,"The task of consistently and reliably replicating data is fundamental in distributed systems, and numerous existing protocols are able to achieve such replication efficiently. When called on to build a large-scale enterprise storage system with built-in replication, we were therefore surprised to discover that no existing protocols met our requirements. As a result, we designed and deployed a new replication protocol called Niobe. Niobe is in the primary-backup family of protocols, and shares many similarities with other protocols in this family. But we believe Niobe is significantly more practical for large-scale enterprise storage than previously published protocols. In particular, Niobe is simple, flexible, has rigorously proven yet simply stated consistency guarantees, and exhibits excellent performance. Niobe has been deployed as the backend for a commercial Internet service; its consistency properties have been proved formally from first principles, and further verified using the TLA + specification language. We describe the protocol itself, the system built to deploy it, and some of our experiences in doing so. © 2008 ACM.",Enterprise storage; Replication,Distributed computer systems; Internet; Network protocols; Specification languages; Built-in replication; Enterprise storage system; Niobe; Data processing
A utility-based unified disk scheduling framework for shared mixed-media services,2008,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40749140759&doi=10.1145%2f1326542.1326546&partnerID=40&md5=3134a9824a95ecaa43d4d12c93afab4c,"We present a new disk scheduling framework to address the needs of a shared multimedia service that provides differentiated multilevel quality-of-service for mixed-media workloads. In such a shared service, requests from different users have different associated performance objectives and utilities, in accordance with the negotiated service-level agreements (SLAs). Service providers typically provision resources only for average workload intensity, so it becomes important to handle workload surges in a way that maximizes the utility of the served requests. We capture the performance objectives and utilities associated with these multiclass diverse workloads in a unified framework and formulate the disk scheduling problem as a reward maximization problem. We map the reward maximization problem to a minimization problem on graphs and, by novel use of graph-theoretic techniques, design a scheduling algorithm that is computationally efficient and optimal in the class of seek-optimizing algorithms. Comprehensive experimental studies demonstrate that the proposed algorithm outperforms other disk schedulers under all loads, with the performance improvement approaching 100% under certain high load conditions. In contrast to existing schedulers, the proposed scheduler is extensible to new performance objectives (workload type) and utilities by simply altering the reward functions associated with the requests. © 2008 ACM.",Disk scheduling; GSP; Profit maximization; Shortest path,Distributed computer systems; Multimedia systems; Problem solving; Resource allocation; Disk scheduling; Profit maximization; Shortest path; Scheduling
Contributing storage using the transparent file system,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048929806&doi=10.1145%2f1288783.1288787&partnerID=40&md5=3f5ee4548330731cb9e3f2a516ffad63,"Contributory applications allow users to donate unused resources on their personal computers to a shared pool. Applications such as SETI@home, Folding@home, and Freenet are now in wide use and provide a variety of services, including data processing and content distribution. However, while several research projects have proposed contributory applications that support peer-to-peer storage systems, their adoption has been comparatively limited. We believe that a key barrier to the adoption of contributory storage systems is that contributing a large quantity of local storage interferes with the principal user of the machine. To overcome this barrier, we introduce the Transparent File System (TFS). TFS provides background tasks with large amounts of unreliable storage - -all of the currently available space - -without impacting the performance of ordinary file access operations. We show that TFS allows a peer-to-peer contributory storage system to provide 40% more storage at twice the performance when compared to a user-space storage mechanism. We analyze the impact of TFS on replication in peer-to-peer storage systems and show that TFS does not appreciably increase the resources needed for file replication. © 2007 ACM.",Aging; Contributory systems; Fragmentation; Peer-to-peer,Data processing; Distributed computer systems; Personal computers; Resource allocation; User interfaces; Contributory systems; Fragmentation; Peer-to-peer; Transparent File System (TFS); Data storage equipment
Strong accountability for network storage,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048935605&doi=10.1145%2f1288783.1288786&partnerID=40&md5=2b6cc01e48a2c2c302d162d19dc3659e,"This article presents the design, implementation, and evaluation of CATS, a network storage service with strong accountability properties. CATS offers a simple web services interface that allows clients to read and write opaque objects of variable size. This interface is similar to the one offered by existing commercial Internet storage services. CATS extends the functionality of commercial Internet storage services by offering support for strong accountability. A CATS server annotates read and write responses with evidence of correct execution, and offers audit and challenge interfaces that enable clients to verify that the server is faithful. A faulty server cannot conceal its misbehavior, and evidence of misbehavior is independently verifiable by any participant. CATS clients are also accountable for their actions on the service. A client cannot deny its actions, and the server can prove the impact of those actions on the state views it presented to other clients. Experiments with a CATS prototype evaluate the cost of accountability under a range of conditions and expose the primary factors influencing the level of assurance and the performance of a strongly accountable storage server. The results show that strong accountability is practical for network storage systems in settings with strong identity and modest degrees of write-sharing. We discuss how the accountability concepts and techniques used in CATS generalize to other classes of network services. © 2007 ACM.",Accountability; Accountable services; Accountable storage,Internet; Software prototyping; Systems analysis; Web services; Accountability; Accountable services; Accountable storage; Network storage; Data storage equipment
PARAID: A gear-shifting power-aware RAID,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048963241&doi=10.1145%2f1288783.1289721&partnerID=40&md5=08187d22caa5eb71835e1c3e38eb861d,"Reducing power consumption for server-class computers is important, since increased energy usage causes more heat dissipation, greater cooling requirements, reduced computational density, and higher operating costs. For a typical data center, storage accounts for 27% of energy consumption. Conventional server-class RAIDs cannot easily reduce power because loads are balanced to use all disks, even for light loads. We have built the power-aware RAID (PARAID), which reduces energy use of commodity server-class disks without specialized hardware. PARAID uses a skewed striping pattern to adapt to the system load by varying the number of powered disks. By spinning disks down during light loads, PARAID can reduce power consumption, while still meeting performance demands, by matching the number of powered disks to the system load. Reliability is achieved by limiting disk power cycles and using different RAID encoding schemes. Based on our five-disk prototype, PARAID uses up to 34% less power than conventional RAIDs while achieving similar performance and reliability. © 2007 ACM.",Energy efficiency; Power savings; RAID,Computational methods; Cooling; Electric power utilization; Heat losses; Software prototyping; Computational density; Encoding schemes; Gear shifting power aware RAID; Powered disks; Servers
A five-year study of file-system metadata,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048932345&doi=10.1145%2f1288783.1288788&partnerID=40&md5=267013c4cf82c75286546d5a1ff8e3c3,"For five years, we collected annual snapshots of file-system metadata from over 60,000 Windows PC file systems in a large corporation. In this article, we use these snapshots to study temporal changes in file size, file age, file-type frequency, directory size, namespace structure, file-system population, storage capacity and consumption, and degree of file modification. We present a generative model that explains the namespace structure and the distribution of directory sizes. We find significant temporal trends relating to the popularity of certain file types, the origin of file content, the way the namespace is used, and the degree of variation among file systems, as well as more pedestrian changes in size and capacities. We give examples of consequent lessons for designers of file systems and related software. © 2007 ACM.",File systems; Generative model; Longitudinal study,Computer operating systems; File organization; Mathematical models; Systems analysis; Directory size; File systems; Generative models; Longitudinal study; Metadata
"Understanding disk failure rates: What does an MTTF of 1,000,000 hours mean to you",2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36049041275&doi=10.1145%2f1288783.1288785&partnerID=40&md5=600f64dd4d19e510d213103093bb817c,"Component failure in large-scale IT installations is becoming an ever-larger problem as the number of components in a single cluster approaches a million. This article is an extension of our previous study on disk failures [Schroeder and Gibson 2007] and presents and analyzes field-gathered disk replacement data from a number of large production systems, including high-performance computing sites and internet services sites. More than 110,000 disks are covered by this data, some for an entire lifetime of five years. The data includes drives with SCSI and FC, as well as SATA interfaces. The mean time-to-failure (MTTF) of those drives, as specified in their datasheets, ranges from 1,000,000 to 1,500,000 hours, suggesting a nominal annual failure rate of at most 0.88%. We find that in the field, annual disk replacement rates typically exceed 1%, with 2-4% common and up to 13% observed on some systems. This suggests that field replacement is a fairly different process than one might predict based on datasheet MTTF. We also find evidence, based on records of disk replacements in the field, that failure rate is not constant with age, and that rather than a significant infant mortality effect, we see a significant early onset of wear-out degradation. In other words, the replacement rates in our data grew constantly with age, an effect often assumed not to set in until after a nominal lifetime of 5 years. Interestingly, we observe little difference in replacement rates between SCSI, FC, and SATA drives, potentially an indication that disk-independent factors such as operating conditions affect replacement rates more than component-specific ones. On the other hand, we see only one instance of a customer rejecting an entire population of disks as a bad batch, in this case because of media error rates, and this instance involved SATA disks. Time between replacement, a proxy for time between failure, is not well modeled by an exponential distribution and exhibits significant levels of correlation, including autocorrelation and long-range dependence. © 2007 ACM.",Annual failure rates; Annual replacement rates; Datasheet MTTF; Failure correlation; Hard drive failure; Hard drive replacements; Infant mortality; MTTF; Storage reliability; Time between failure; Wear-out,Cluster analysis; Information technology; Internet; Magnetic disk storage; Problem solving; Websites; Annual failure rates; Annual replacement rates; Datasheet MTTF; Failure correlation; Hard drive replacements; Infant mortality; Storage reliability; Time between failure; Wear-out; Computer system recovery
Introduction to special issue USENIX FAST 2007,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048943921&doi=10.1145%2f1288783.1288784&partnerID=40&md5=3552a940003136bafb6810b8a6929d11,[No abstract available],,
Optimal multistream sequential prefetching in a shared cache,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36049042568&doi=10.1145%2f1288783.1288789&partnerID=40&md5=0afc6df8381cc3e002b3162efae98f07,"Prefetching is a widely used technique in modern data storage systems. We study the most widely used class of prefetching algorithms known as sequential prefetching. There are two problems that plague the state-of-the-art sequential prefetching algorithms: (i) cache pollution, which occurs when prefetched data replaces more useful prefetched or demand-paged data, and (ii) prefetch wastage, which happens when prefetched data is evicted from the cache before it can be used. A sequential prefetching algorithm can have a fixed or adaptive degree of prefetch and can be either synchronous (when it can prefetch only on a miss) or asynchronous (when it can also prefetch on a hit). To capture these distinctions we define four classes of prefetching algorithms: fixed synchronous (FS), fixed asynchronous (FA), adaptive synchronous (AS), and adaptive asynchronous (AsynchA). We find that the relatively unexplored class of AsynchA algorithms is in fact the most promising for sequential prefetching. We provide a first formal analysis of the criteria necessary for optimal throughput when using an AsynchA algorithm in a cache shared by multiple steady sequential streams. We then provide a simple implementation called AMP (adaptive multistream prefetching) which adapts accordingly, leading to near-optimal performance for any kind of sequential workload and cache size. Our experimental setup consisted of an IBM xSeries 345 dual processor server running Linux using five SCSI disks. We observe that AMP convincingly outperforms all the contending members of the FA, FS, and AS classes for any number of streams and over all cache sizes. As anecdotal evidence, in an experiment with 100 concurrent sequential streams and varying cache sizes, AMP surpasses the FA, FS, and AS algorithms by 29 - 172%, 12 - 24%, and 21 - 210%, respectively, while outperforming OBL by a factor of 8. Even for complex workloads like SPC1-Read, AMP is consistently the best-performing algorithm. For the SPC2 video-on-demand workload, AMP can sustain at least 25% more streams than the next best algorithm. Furthermore, for a workload consisting of short sequences, where optimality is more elusive, AMP is able to outperform all the other contenders in overall performance. Finally, we implemented AMP in the state-of-the-art enterprise storage system, the IBM system storage DS8000 series. We demonstrated that AMP dramatically improves performance for common sequential and batch processing workloads and delivers up to a twofold increase in the sequential read capacity. © 2007 ACM.",Adaptive prefetching; Asynchronous prefetching; Cache pollution; Degree of prefetch; Fixed prefetching; Multistream read; Optimal prefetching; Prefetch wastage; Prestaging; Sequential prefetching; Synchronous prefetching; Trigger distance,Algorithms; Data acquisition; Data storage equipment; Problem solving; Video on demand; Adaptive prefetching; Asynchronous prefetching; Cache pollution; Degree of prefetch; Fixed prefetching; Multistream read; Optimal prefetching; Prefetch wastage; Prestaging; Sequential prefetching; Buffer storage
A buffer cache management scheme exploiting both temporal and spatial localities,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250379203&doi=10.1145%2f1242520.1242522&partnerID=40&md5=97b5ac88ed7dd71d721a6b2931520bc1,"On-disk sequentiality of requested blocks, or their spatial locality, is critical to real disk performance where the throughput of access to sequentially-placed disk blocks can be an order of magnitude higher than that of access to randomly-placed blocks. Unfortunately, spatial locality of cached blocks is largely ignored, and only temporal locality is considered in current system buffer cache managements. Thus, disk performance for workloads without dominant sequential accesses can be seriously degraded. To address this problem, we propose a scheme called DULO (DUal LOcality) which exploits both temporal and spatial localities in the buffer cache management. Leveraging the filtering effect of the buffer cache, DULO can influence the I/O request stream by making the requests passed to the disk more sequential, thus significantly increasing the effectiveness of I/O scheduling and prefetching for disk performance improvements. We have implemented a prototype of DULO in Linux 2.6.11. The implementation shows that DULO can significantly increases disk I/O throughput for real-world applications such as a Web server, TPC benchmark, file system benchmark, and scientific programs. It reduces their execution times by as much as 53%. © 2007 ACM.",Caching; File systems; Hard disk; Spatial locality; Temporal locality,Computer operating systems; File organization; Hard disk storage; Information management; Scheduling; Servers; Cached blocks; File systems; Spatial locality; Temporal locality; Cache memory
Extending ACID semantics to the file system,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250377546&doi=10.1145%2f1242520.1242521&partnerID=40&md5=f89b0a29d0302873b3cb897e075fba3b,"An organization's data is often its most valuable asset, but today's file systems provide few facilities to ensure its safety. Databases, on the other hand, have long provided transactions. Transactions are useful because they provide atomicity, consistency, isolation, and durability (ACID). Many applications could make use of these semantics, but databases have a wide variety of nonstandard interfaces. For example, applications like mail servers currently perform elaborate error handling to ensure atomicity and consistency, because it is easier than using a DBMS. A transaction-oriented programming model eliminates complex error-handling code because failed operations can simply be aborted without side effects. We have designed a file system that exports ACID transactions to user-level applications, while preserving the ubiquitous and convenient POSIX interface. In our prototype ACID file system, called Amino, updated applications can protect arbitrary sequences of system calls within a transaction. Unmodified applications operate without any changes, but each system call is transaction protected. We also built a recoverable memory library with support for nested transactions to allow applications to keep their in-memory data structures consistent with the file system. Our performance evaluation shows that ACID semantics can be added to applications with acceptable overheads. When Amino adds atomicity, consistency, and isolation functionality to an application, it performs close to Ext3. Amino achieves durability up to 46% faster than Ext3, thanks to improved locality. © 2007 ACM.",Databases; File system transactions; File systems; Ptrace monitors; Recoverable memory,Computer system recovery; Data structures; Data transfer; Database systems; Object oriented programming; Security of data; Storage allocation (computer); Arbitrary sequences; File system transactions; Ptrace monitors; Recoverable memory; Semantics
Building MEMS-based storage systems for streaming media,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250354468&doi=10.1145%2f1242520.1242523&partnerID=40&md5=bdf3cefac1952f507e19a657b6263e92,"The performance of streaming media servers has been limited by the dual requirements of high disk throughput (to service more clients simultaneously) and low memory use (to decrease system cost). To achieve high disk throughput, disk drives must be accessed with large IOs to amortize disk access overhead. Large IOs imply an increased requirement of expensive DRAM, and, consequently, greater overall system cost. MEMS-based storage, an emerging storage technology, is predicted to offer a price-performance point between those of DRAM and disk drives. In this study, we propose storage architectures that use the relatively inexpensive MEMS-based storage devices as an intermediate layer (between DRAM and disk drives) for temporarily staging large disk IOs at a significantly lower cost. We present data layout mechanisms and synchronized IO scheduling algorithms for the real-time storage and retrieval of streaming data within such an augmented storage system. Analytical evaluation suggests that MEMS-augmented storage hierarchies can reduce the cost and improve the throughput of streaming servers significantly. © 2007 ACM.",I/O scheduling; MEMS-based storage; Multidisk storage; Storage architecture; Streaming media,Data structures; Dynamic random access storage; MEMS; Scheduling; Servers; Storage allocation (computer); Disk drives; Multidisk storage; Storage architectures; Streaming media; Hard disk storage
Zoned-RAID,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047218801&doi=10.1145%2f1227835.1227836&partnerID=40&md5=b4f0c540bd35a5c1060323573c9222eb,"The RAID (Redundant Array of Inexpensive Disks) system has been widely used in practical storage applications for better performance, cost effectiveness, and reliability. This study proposes a novel variant of RAID named Zoned-RAID (Z-RAID). Z-RAID improves the performance of traditional RAID by utilizing the zoning property of modern disks which provides multiple zones with different data transfer rates within a disk. Z-RAID levels 1, 5, and 6 are introduced to enhance the effective data transfer rate of RAID levels 1, 5, and 6, respectively, by constraining the placement of data blocks in multizone disks. We apply the Z-RAID to a practical and popular application, streaming media server, that requires a high-data transfer rate as well as a high reliability. The analytical and experimental results demonstrate the superiority of Z-RAID to conventional RAID. Z-RAID provides a higher effective data transfer rate in normal mode with no disadvantage. In the presence of a disk failure, Z-RAID still performs as well as RAID. © 2007 ACM.",Disk array; Multizone disks; RAID,Cost effectiveness; Data acquisition; Data reduction; Data transfer; Reliability; Servers; Data blocks; Multizone disks; Redundant Array of Inexpensive Disks (RAID); Streaming media servers; Data storage equipment
Dynamic data reallocation in disk arrays,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047224812&doi=10.1145%2f1227835.1227837&partnerID=40&md5=32132d807f685670495d837bbe00746b,"We present an application which optimizes the configuration of data in a disk array. The application relies on very little input data which is readily available from many storage devices. We validate the application in a controlled experiment and in a production environment. In both cases, the reconfiguration process led to a 20 - 30% improvement in response time. © 2007 ACM.",Conservative models; Disk utilization,Data storage equipment; Mathematical models; Process planning; Production control; Conservative models; Data reallocation; Disk arrays; Disk utilization; Information management
SLAS: An efficient approach to scaling round-robin striped volumes,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047211123&doi=10.1145%2f1227835.1227838&partnerID=40&md5=d9a36a8436294fb221b928c0574419a8,"Round-robin striping, due to its uniform distribution and low-complexity computation, is widely used by applications which demand high bandwidth and massive storage. Because many systems are nonstoppable when their storage capacity and I/O bandwidth need increasing, an efficient and online mechanism to add more disks to striped volumes is very important. In this article, it is presented and proved that during data redistribution caused by scaling a round-robin striped volume, there is always a reordering window where data consistency can be maintained while changing the order of data movements. Furthermore, by exploiting the reordering window characteristic, SLAS is proposed to scale round-robin striped volumes, which reduces the cost of data redistribution effectively. First, SLAS applies a new mapping management solution based on a sliding window to support data redistribution without loss of scalability; second, it uses lazy updates of mapping metadata to decrease the number of metadata writes required by data redistribution; third, it changes the order of data chunk movements to aggregate reads/writes of data chunks. Our results from detailed simulations using real-system workloads show that, compared with the traditional approach, SLAS can reduce redistribution duration by up to 40.79% with similar maximum response time of foreground I/Os. Finally, our discussion indicates that the SLAS approach works for both disk addition and disk removal to/from striped volumes. © 2007 ACM.",I/O aggregation; Lazy updates; Online scaling; Reordering window; Sliding window; Striped volume,Bandwidth; Computer simulation; Data acquisition; Data storage equipment; Metadata; Scalability; I/O aggregation; Lazy updates; Online scaling; Reordering windows; Sliding windows; Striped volume; Computational complexity
PARAID: A Gear-Shifting Power-Aware RAID,2007,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981265731&doi=10.1145%2f1289720.1289721&partnerID=40&md5=961d7055d7b4a1da4986eeb3bfd69b4a,"Reducing power consumption for server-class computers is important, since increased energy usage causes more heat dissipation, greater cooling requirements, reduced computational density, and higher operating costs. For a typical data center, storage accounts for 27% of energy consumption. Conventional server-class RAIDs cannot easily reduce power because loads are balanced to use all disks, even for light loads. We have built the power-aware RAID (PARAID), which reduces energy use of commodity serverclass disks without specialized hardware. PARAID uses a skewed striping pattern to adapt to the system load by varying the number of powered disks. By spinning disks down during light loads, PARAID can reduce power consumption, while still meeting performance demands, by matching the number of powered disks to the system load. Reliability is achieved by limiting disk powercycles and using different RAID encoding schemes. Based on our five-disk prototype, PARAID uses up to 34% less power than conventional RAIDs while achieving similar performance and reliability. © 2007, ACM. All rights reserved.",Design; energy efficiency; Experimentation; Measurement; Performance; Power savings; RAID,
The design of efficient initialization and crash recovery for log-based file systems over flash memory,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846707008&doi=10.1145%2f1210596.1210600&partnerID=40&md5=5e98e8a8c11da75239fb5fd188ae6dd7,"While flash memory has been widely adopted for storage systems for various embedded systems, issues of performance and reliability have started receiving growing attention in recent years. How to provide efficient roll back and quick mounting for flash-memory file systems has become an important research topic in recent years, in addition to the work on effective garbage collection and superb runtime performance. Such an observation motivates our work on the investigation of efficient initialization and crash recovery of flash-memory file systems based on log structures. A methodology is proposed for the acceleration of mounting and crash recovery for log-based file systems. A system prototype based on a well-known flash-memory file system, YAFFS, was implemented with performance evaluation. Experimental results show that the proposed methodology can reduce mounting time significantly, regardless of whether the file system is properly unmounted. © 2006 ACM.",Crash recovery; Efficient initialization; Embedded systems; File systems; Flash memory; Storage systems,Computational methods; Data acquisition; Embedded systems; File organization; Logic design; File systems; Storage systems; Flash memory
An approach to virtual allocation in storage systems,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846701271&doi=10.1145%2f1210596.1210597&partnerID=40&md5=9ac8e1f7efdee9599d10f37d45e468d2,"This article presents virtual allocation, a scheme for flexible storage allocation. Virtual allocation separates storage allocation from the file system. It employs an allocate-on-write strategy which lets applications fit into the actual usage of storage space, without regard to the configured file system size. This improves flexibility by allowing storage space to be shared across different file systems. This article presents the design of virtual allocation and its evaluation through benchmarks. To illustrate our approach, we implemented a prototype system on PCs running Linux. We present the results from the prototype implementation and its evaluation. © 2006 ACM.",File systems; Storage allocation; Storage management; Storage systems,Benchmarking; Computer operating systems; File organization; Logic design; Personal computers; Computer operating system - Linux; File systems; Virtual allocation; Data storage equipment
Modeling and improving security of a local disk system for write-intensive workloads,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846667508&doi=10.1145%2f1210596.1210598&partnerID=40&md5=8742a865ff372165afcbf454fdf30dcb,"Since security is of critical importance for modern storage systems, it is imperative to protect stored data from being tampered with or disclosed. Although an increasing number of secure storage systems have been developed, there is no way to dynamically choose security services to meet disk requests' flexible security requirements. Furthermore, existing security techniques for disk systems are not suitable to guarantee desired response times of disk requests. We remedy this situation by proposing an adaptive strategy (referred to as AWARDS) that can judiciously select the most appropriate security service for each write request, while endeavoring to guarantee the desired response times of all disk requests. To prove the efficiency of the proposed approach, we build an analytical model to measure the probability that a disk request is completed before its desired response time. The model also can be used to derive the expected value of disk requests' security levels. Empirical results based on synthetic workloads as well as real I/O-intensive applications show that AWARDS significantly improves overall performance over an existing scheme by up to 358.9% (with an average of 213.4%). © 2006 ACM.",Data-intensive applications; Desired response time; Local disk; Quality of security; Security level,Data reduction; Data storage equipment; Mathematical models; Response time (computer systems); Security systems; Data-intensive applications; Desired response time; Local disk; Quality of security; Security level; Hard disk storage
Efficient indexing data structures for flash-based sensor devices,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846686729&doi=10.1145%2f1210596.1210601&partnerID=40&md5=8f4a49b5e05a5168a8752e4237961e9a,"Flash memory is the most prevalent storage medium found on modern wireless sensor devices (WSDs). In this article we present two external memory index structures for the efficient retrieval of records stored on the local flash memory of a WSD. Our index structures, MicroHash and MicroGF (micro grid files), exploit the asymmetric read/write and wear characteristics of flash memory in order to offer high-performance indexing and searching capabilities in the presence of a low-energy budget, which is typical for the devices under discussion. Both structures organize data and index pages on the flash media using a sorted by timestamp file organization. A key idea behind these index structures is that expensive random access deletions are completely eliminated. MicroHash enables equality searches by value in constant time and equality searches by timestamp in logarithmic time at a small cost of storing index pages on the flash media. Similarly, MicroGF enables spatial equality and proximity searches in constant time. We have implemented these index structures in nesC, the programming language of the TinyOS operating system. Our trace-driven experimentation with several real datasets reveals that our index structures offer excellent search performance at a small cost of constructing and maintaining the index. © 2006 ACM.",Access methods; Flash memory; Wireless sensor networks,Computer operating systems; Computer programming languages; Content based retrieval; Data structures; File organization; Flash memory; Online searching; Computer operating system - TinyOS; Computer programming language - nesC; Wireless sensor devices (WSD); Wireless sensor networks; Indexing (of information)
Improving duplicate elimination in storage systems,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846663173&doi=10.1145%2f1210596.1210599&partnerID=40&md5=a4768175add5a040c74f4449367fda85,"Minimizing the amount of data that must be stored and managed is a key goal for any storage architecture that purports to be scalable. One way to achieve this goal is to avoid maintaining duplicate copies of the same data. Eliminating redundant data at the source by not writing data which has already been stored not only reduces storage overheads, but can also improve bandwidth utilization. For these reasons, in the face of today's exponentially growing data volumes, redundant data elimination techniques have assumed critical significance in the design of modern storage systems.Intelligent object partitioning techniques identify data that is new when objects are updated, and transfer only these chunks to a storage server. In this article, we propose a new object partitioning technique, called fingerdiff, that improves upon existing schemes in several important respects. Most notably, fingerdiff dynamically chooses a partitioning strategy for a data object based on its similarities with previously stored objects in order to improve storage and bandwidth utilization. We present a detailed evaluation of fingerdiff, and other existing object partitioning schemes, using a set of real-world workloads. We show that for these workloads, the duplicate elimination strategies employed by fingerdiff improve storage utilization on average by 25%, and bandwidth utilization on average by 40% over comparable techniques. © 2006 ACM.",Content-based addressing; Duplicate elimination; Rabin's fingerprints; Storage management,Bandwidth; Computer architecture; Data flow analysis; Data reduction; Object recognition; Servers; Bandwidth utilization; Content based addressing; Intelligent object partitioning techniques; Storage architecture; Data storage equipment
Constructing collaborative desktop storage caches for large scientific datasets,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750485344&doi=10.1145%2f1168910.1168911&partnerID=40&md5=98794b8f95d5edee4cc12c193fa50196,"High-end computing is suffering a data deluge from experiments, simulations, and apparatus that creates overwhelming application dataset sizes. This has led to the proliferation of high-end mass storage systems, storage area clusters, and data centers. These storage facilities offer a large range of choices in terms of capacity and access rate, as well as strong data availability and consistency support. However, for most end-users, the ""last mile"" in their analysis pipeline often requires data processing and visualization at local computers, typically local desktop workstations. End-user workstations - despite having more processing power than ever before - are ill-equipped to cope with such data demands due to insufficient secondary storage space and I/O rates. Meanwhile, a large portion of desktop storage is unused. We propose the FreeLoader framework, which aggregates unused desktop storage space and I/O bandwidth into a shared cache/scratch space, for hosting large, immutable datasets and exploiting data access locality. This article presents the FreeLoader architecture, component design, and performance results based on our proof-of-concept prototype. Its architecture comprises contributing benefactor nodes, steered by a management layer, providing services such as data integrity, high performance, load balancing, and impact control. Our experiments show that FreeLoader is an appealing low-cost solution to storing massive datasets by delivering higher data access rates than traditional storage facilities, namely, local or remote shared file systems, storage systems, and Internet data repositories. In particular, we present novel data striping techniques that allow FreeLoader to efficiently aggregate a workstation's network communication bandwidth and local I/O bandwidth. In addition, the performance impact on the native workload of donor machines is small and can be effectively controlled. Further, we show that security features such as data encryptions and integrity checks can be easily added as filters for interested clients. Finally, we demonstrate how legacy applications can use the FreeLoader API to store and retrieve datasets. © 2006 Association for Computing Machinery.",Distributed storage; Parallel I/O; Scientific data management; Server-less storage system; Storage cache; Storage networking; Storage resoucce management; Storage scavenging; Striped storage,Bandwidth; Computer simulation; Computer supported cooperative work; Computer workstations; Data processing; Data storage equipment; Personal computers; Distributed storage; Parallel I/O; Scientific data management; Server less storage systems; Storage cache; Storage networking; Storage resource management; Storage scavenging; Striped storage; Database systems
Intelligent storage: Cross-layer optimization for soft real-time workload,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750436568&doi=10.1145%2f1168910.1168912&partnerID=40&md5=e73e80e54b0031647560babf3f672ea5,"In this work, we develop an intelligent storage system framework for soft real-time applications. Modern software systems consist of a collection of layers and information exchange across the layers is performed via well-defined interfaces. Due to the strictness and inflexibility of interface definition, it is not possible to pass the information specific to one layer to other layers. In practice, the exploitation of this information across the layers can greatly enhance the performance, reliability, and manageability of the system. We address the limitation of legacy interface definition via enabling intelligence in the storage system. The objective is to enable the lower-layer entity, for example, a physical or block device, to conjecture the semantic and contextual information of that application behavior which cannot be passed via the legacy interface. Based upon the knowledge obtained by the intelligence module, the system can perform a number of actions to improve the performance, reliability, security, and manageability of the system. Our intelligence storage system focuses on optimizing the I/O subsystem performance for a soft real-time application. Our intelligence framework consists of three components: the workload monitor, workload analyzer, and system optimizer. The workload monitor maintains a window of recent I/O requests and extracts feature vectors in regular intervals. The workload analyzer is trained to determine the class of the incoming workload by using the feature vector. The system optimizer performs various actions to tune the storage system for a given workload. We use confidence rate boosting to train the workload analyzer. This sophisticated learner achieves a higher than 97% accuracy of workload class prediction. We develop a prototype intelligence storage system on the legacy operating system platform. The system optimizer performs; (1) dynamic adjustment of the file-system-level read-ahead size; (2) dynamic adjustment of I/O request size; and (3) filtering of I/O requests. We examine the effect of this autonomic optimization via experimentation. We find that the storage level pro-active optimization greatly enhances the efficiency of the underlying storage system. The sophisticated intelligence module developed in this work does not restrict its usage for performance optimization. It can be effectively used as classification engine for generic autonomic computing environment, i.e. management, diagnosis, security and etc. © 2006 ACM.",Autonomic computing; Boosting; Cross layer optimization; File system; Intelligence; Machine learning; Multimedia; Storage,Electronic document exchange; Information management; Interfaces (computer); Learning systems; Multimedia systems; Optimization; Real time systems; Security of data; Autonomic computing; Boosting; Cross layer optimization; File systems; Data storage equipment
The Conquest file system: Better performance through a disk/persistent-RAM hybrid design,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750471167&doi=10.1145%2f1168910.1168914&partnerID=40&md5=3b57d15bff75fb7f15e68ae1101a0d2a,"Modem file systems assume the use of disk, a system-wide performance bottleneck for over a decade. Current disk caching and RAM file systems either impose high overhead to access memory content or fail to provide mechanisms to achieve data persistence across reboots. The Conquest file system is based on the observation that memory is becoming inexpensive, which enables all file system services to be delivered from memory, except for providing large storage capacity. Unlike caching, Conquest uses memory with battery backup as persistent storage, and provides specialized and separate data paths to memory and disk. Therefore, the memory data path contains no disk-related complexity. The disk data path consists of optimizations only for the specialized disk usage pattern. Compared to a memory-based file system, Conquest incurs little performance overhead. Compared to several disk-based file systems, Conquest achieves 1.3x to 19x faster memory performance, and 1.4× to 2.0× faster performance when exercising both memory and disk. Conquest realizes most of the benefits of persistent RAM at a fraction of the cost of a RAM-only solution. It also demonstrates that disk-related optimizations impose high overheads for accessing memory content in a memory-rich environment. © 2006 ACM.",File systems; Performance measurement; Persistent RAM; Storage management,Buffer storage; Computational complexity; Computer aided design; Data storage equipment; Optimization; Random access storage; File systems; Performance measurement; Persistent RAM; Storage management; Database systems
Storage performance virtualization via throughput and latency control,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750456172&doi=10.1145%2f1168910.1168913&partnerID=40&md5=1193da7a814c85c78b8cdc41dec0096f,"I/O consolidation is a growing trend in production environments due to increasing complexity in tuning and managing storage systems. A consequence of this trend is the need to serve multiple users and/or workloads simultaneously. It is imperative to ensure that these users are insulated from each other by virtualization in order to meet any service-level objective (SLO). Previous proposals for performance virtualization suffer from one or more of the following drawbacks: (1) They rely on a fairly detailed performance model of the underlying storage system; (2) couple rate and latency allocation in a single scheduler, making them less flexible; or (3) may not always exploit the full bandwidth offered by the storage system. This article presents a two-level scheduling framework that can be built on top of an existing storage utility. This framework uses a low-level feedback-driven request scheduler, called AVATAR, that is intended to meet the latency bounds determined by the SLO. The load imposed on AVATAR is regulated by a high-level rate controller, called SARC, to insulate the users from each other. In addition, SARC is work-conserving and tries to fairly distribute any spare bandwidth in the storage system to the different users. This framework naturally decouples rate and latency allocation. Using extensive I/O traces and a detailed storage simulator, we demonstrate that this two-level framework can simultaneously meet the latency and throughput requirements imposed by an SLO, without requiring extensive knowledge of the underlying storage system. © 2006 ACM.",Fairness; I/O scheduling; Performance isolation; Quality of service; Storage systems; Virtualization,Bandwidth; Computational complexity; Computer simulation; Mathematical models; Quality of service; Scheduling; User interfaces; Fairness; I/O scheduling; Performance isolation; Virtualization; Data storage equipment
Multicollective I/O: A technique for exploiting inter-file access patterns,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750476657&doi=10.1145%2f1168910.1168915&partnerID=40&md5=ad894c7c83d37d4439e64f03788a6af4,"The increasing gap between processor cycle times and access times to storage devices makes it necessary to use powerful optimizations. This is especially true for applications in the parallel computing domain that frequently perform large amounts of file I/O. Collective I/O strategy that coordinates the processes to perform I/O on each other's behalf has demonstrated a significant performance improvement. This article proposes a new concept called Multicollective I/O (MCIO) that expands the collective I/O to allow data from multiple files to be requested in a single I/O request, in contrast to allowing only multiple segments for a single file to be specified together. MCIO considers multiple arrays simultaneously by having a more global view of the overall I/O behavior exhibited by parallel applications. This article shows that determining the optimal MCIO access pattern is an NP-complete problem, and proposes two different heuristics for the access pattern detection problem, also called the assignment problem. Both heuristics have been implemented within a runtime library, and tested using a large-scale scientific application. Our results show that MCIO outperforms collective I/O by as much as 87%. Our runtime library-based implementation can be used by application users as well as by optimizing compilers. Based on our results, we recommend that future library designers for I/O-intensive applications include MCIO in their suite of optimizations. © 2006 ACM.",Collective I/O; File accesses; Parallel I/O; Runtime library; Software optimizations,Computational methods; Digital libraries; Heuristic methods; Input output programs; Optimization; Problem solving; Collective I/O; File accesses; Parallel I/O; Runtime library; Software optimizations; Data storage equipment
Using MEMS-based storage in computer systems - MEMS storage architectures,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745268866&doi=10.1145%2f1138041.1138042&partnerID=40&md5=79eb7c99086a77a9937233ce560f2d5a,"As an emerging nonvolatile secondary storage technology, MEMS-based storage exhibits several desirable properties including high performance, high storage volumic density, low power consumption, low entry cost, and small form factor. However, MEMS-based storage provides a limited amount of storage per device and is likely to be more expensive than magnetic disk. Systems designers will therefore need to make trade-offs to achieve well-balanced designs. We present an architecture in which MEMS devices are organized into MEMS storage enclosures with online spares. Such enclosures are proven to be highly reliable storage building bricks with no maintenance during their economic lifetimes. We also demonstrate the effectiveness of using MEMS as another layer in the storage hierarchy, bridging the cost and performance gap between MEMS storage and disk. We show that using MEMS as a disk cache can significantly improve system performance and cost-performance ratio. © 2006 ACM.",Cost-performance; Economic lifetime; Hybrid storage devices; Maintenance strategies; MEMS-based storage; Storage enclosures,Acoustic bulk wave devices; Computer architecture; Costs; Data storage equipment; Energy conservation; Magnetic storage; Microelectromechanical devices; Cost-performance; Economic lifetime; Hybrid storage devices; Maintenance strategies; MEMS-based storage; Storage enclosures; Computer systems
Versatility and unix semantics in namespace unification,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745273588&doi=10.1145%2f1138041.1138045&partnerID=40&md5=03357886f0fb0f6a86b80b68b19de0ae,"Administrators often prefer to keep related sets of files in different locations or media, as it is easier to maintain them separately. Users, however, prefer to see all files in one location for convenience. One solution that accommodates both needs is virtual namespace unification - providing a merged view of several directories without physically merging them. For example, namespace unification can merge the contents of several CD-ROM images without unpacking them, merge binary directories from different packages, merge views from several file servers, and more. Namespace unification can also enable snapshotting by marking some data sources read-only and then utilizing copy-on-write for the read-only sources. For example, an OS image may be contained on a read-only CD-ROM image - and the user's configuration, data, and programs could be stored in a separate read-write directory. With copy-on-write unification, the user need not be concerned about the two disparate file systems. It is difficult to maintain Unix semantics while offering a versatile namespace unification system. Past efforts to provide such unification often compromised on the set of features provided or Unix compatibility - resulting in an incomplete solution that users could not use. We designed and implemented a versatile namespace unification system called Unionfs. Unionfs maintains Unix semantics while offering advanced namespace unification features: dynamic insertion and removal of namespaces at any point in the merged view, mixing read-only and read-write components, efficient in-kernel duplicate elimination, NFS interoperability, and more. Since releasing our Linux implementation, it has been used by thousands of users and over a dozen Linux distributions, which helped us discover and solve many practical problems. © 2006 ACM.",Directory merging; Namespace management; Snapshotting; Stackable file systems; Unification,Computer operating systems; Data storage equipment; Image analysis; Information technology; Problem solving; ROM; Set theory; Directory merging; Namespace management; Snapshotting; Stackable file systems; Unification; Semantics
Efficient identification of hot data for flash memory storage systems,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745243691&doi=10.1145%2f1138041.1138043&partnerID=40&md5=d84b91c959669a0962e8cba65ad88e23,"Hot data identification for flash memory storage systems not only imposes great impacts on flash memory garbage collection but also strongly affects the performance of flash memory access and its lifetime (due to wear-levelling). This research proposes a highly efficient method for on-line hot data identification with limited space requirements. Different from past work, multiple independent hash functions are adopted to reduce the chance of false identification of hot data and to provide predictable and excellent performance for hot data identification. This research not only offers an efficient implementation for the proposed framework, but also presents an analytic study on the chance of false hot data identification. A series of experiments was conducted to verify the performance of the proposed method, and very encouraging results are presented. © 2006 ACM.",Flash memory; Garbage collection; Storage system; Workload locality,Computer peripheral equipment; Computer science; Data reduction; Data storage equipment; Identification (control systems); Information technology; Garbage collection; Limited space requirements; Storage systems; Workload locality; Flash memory
Thermal issues in disk drive design: Challenges and possible solutions,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745240052&doi=10.1145%2f1138041.1138044&partnerID=40&md5=930a8d0b922e2e674dcb56f1affbd116,"The importance of pushing the performance envelope of disk drives continues to grow in the enterprise storage market. One of the most fundamental factors impacting disk drive design is heat dissipation, since it directly affects drive reliability. Until now, drive manufacturers have continued to meet the 40% annual growth target of the internal data-rates (IDR) by increasing RPMs and shrinking platter sizes, both of which have counteracting effects on the heat dissipation within a drive. In this article, we shall show that we are getting to a point where it is going to be very difficult to stay on this roadmap. We first present detailed models that capture the close relationships between capacity, performance, and thermal characteristics over time. Using these models, we quantify the drop-off in IDR growth rates over the next decade if we are to adhere to the thermal design envelope. We motivate the need for continued improvements in IDR by showing that the response times of real workloads can be improved by 30-60% with a 10K increase in the RPM for disks used in their respective storage systems. We then present two dynamic thermal management (DTM) techniques that can be used to buy back some of this IDR loss. The first DTM technique exploits the thermal slack between what the drive was intended to support and the currently lower operating temperature to ramp up the RPM. The second DTM technique assumes that the drive is only designed for average case operation and dynamically throttles its activities to remain within the thermal envelope. © 2006 ACM.",Disk drive; Technology scaling; Thermal management,Heat losses; Mathematical models; Product design; Thermal effects; Thermoanalysis; Disk drive; Dynamic thermal management (DTM); Internal data-rates (IDR); Technology scaling; Thermal management; Data storage equipment
Spin MOSFETs as a basis for spintronics,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747222409&doi=10.1145%2f1149976.1149980&partnerID=40&md5=d17dd8f8eb6eaac4e362d15ce5e43e51,"This article reviews a recently proposed new class of spin transistors referred to as spin metal-oxide-semiconductor field-effect transistors (spin MOSFETs), and their integrated circuit applications. The fundamental device structures, operating principle, and theoretically predicted device performance are presented. Spin MOSFETs potentially exhibit significant magnetotransport effects, such as large magneto-current, and also satisfy important requirements for integrated circuit applications such as high transconductance, low power-delay product, and low off-current. Since spin MOSFETs can perform signal processing and logic operations and can store digital data using both charge transport and spin degrees of freedom, they are expected to be building blocks for memory cells and logic gates in spin-electronic integrated circuits. Novel spin-electronic integrated circuit architectures for nonvolatile memory and reconfigurable logic employing spin MOSFETs are also presented. © 2006 ACM.",MOSFETs; Spin MOSFETs; Spin transistors; Spintronics,Charge transfer; Electric currents; Integrated circuits; Logic gates; Nonvolatile storage; Transconductance; MOSFETs; Power-delay product; Spin MOSFETs; Spin transistors; Spintronics; MOSFET devices
Using MEMS-based storage in computer systems - Device modeling and management,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747152740&doi=10.1145%2f1149976.1149978&partnerID=40&md5=7bbe860175811656269f89d04e20e8b6,"MEMS-based storage is an emerging nonvolatile secondary storage technology. It promises high performance, high storage density, and low power consumption. With fundamentally different architectural designs from magnetic disk, MEMS-based storage exhibits unique two-dimensional positioning behaviors and efficient power state transitions. We model these low-level, device-specific properties of MEMS-based storage and present request scheduling algorithms and power management strategies that exploit the full potential of these devices. Our simulations show that MEMS-specific device management policies can significantly improve system performance and reduce power consumption. © 2006 ACM.",MEMS-based storage; Power management; Request scheduling,Algorithms; Computer architecture; Computer simulation; Magnetic disk storage; Mathematical models; Microelectromechanical devices; MEMS-based storage; Power management; Power state transitions; Request scheduling; Computer systems
On incremental file system development,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747188874&doi=10.1145%2f1149976.1149979&partnerID=40&md5=d8a9848c4f5f04da3bc7a8df886cf9ca,"Developing file systems from scratch is difficult and error prone. Using layered, or stackable, file systems is a powerful technique to incrementally extend the functionality of existing file systems on commodity OSes at runtime. In this article, we analyze the evolution of layering from historical models to what is found in four different present day commodity OSes: Solaris, FreeBSD, Linux, and Microsoft Windows. We classify layered file systems into five types based on their functionality and identify the requirements that each class imposes on the OS. We then present five major design issues that we encountered during our experience of developing over twenty layered file systems on four OSes. We discuss how we have addressed each of these issues on current OSes, and present insights into useful OS and VFS features that would provide future developers more versatile solutions for incremental file system development. © 2006 ACM.",Extensibility; I/O manager; IRP; Layered file systems; Stackable file systems; VFS; Vnode,Computer operating systems; Mathematical models; Software engineering; Systems analysis; Layered file systems; Software developers; Stackable file systems; Database systems
Authentication and integrity in outsourced databases,2006,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747176236&doi=10.1145%2f1149976.1149977&partnerID=40&md5=80227404772ddc693424e4be417f70a8,"In the Outsourced Database (ODB) model, entities outsource their data management needs to a third-party service provider. Such a service provider offers mechanisms for its clients to create, store, update, and access (query) their databases. This work provides mechanisms to ensure data integrity and authenticity for outsourced databases. Specifically, this article provides mechanisms that assure the querier that the query results have not been tampered with and are authentic (with respect to the actual data owner). It investigates both the security and efficiency aspects of the problem and constructs several secure and practical schemes that facilitate the integrity and authenticity of query replies while incurring low computational and communication costs. © 2006 ACM.",Authentication; Data authenticity; Data integrity; Integrity; Outsourced databases; Signature aggregation; Storage,Computation theory; Copyrights; Data structures; Information services; Outsourcing; Query languages; Authentication; Data authenticity; Data integrity; Outsourced databases; Signature aggregation; Database systems
Efficient Management for Large-Scale Flash-Memory Storage Systems with Resource Conservation,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001155071&doi=10.1145%2f1111609.1111610&partnerID=40&md5=28e0fc2492ebb28074a636f5ad617ae6,"Many existing approaches on flash-memory management are based on RAM-resident tables in which one single granularity size is used for both address translation and space management. As high-capacity flash memory is becoming more affordable than ever, the dilemma of how to manage the RAM space or how to improve the access performance is emerging for many vendors. In this article, we propose a tree-based management scheme which adopts multiple granularities in flash-memory management. Our objective is to not only reduce the run-time RAM footprint but also manage the write workload, due to housekeeping. The proposed method was evaluated under realistic workloads, where significant advantages over existing approaches were observed, in terms of the RAM space, access performance, and flash-memory lifetime. © 2005, ACM. All rights reserved.",Algorithm; consumer electronics; Design; embedded systems; Flash memory; memory management; Performance; portable devices; storage systems,
Scalable and Fault-Tolerant Support for Variable Bit-Rate Data in the Exedra Streaming Server,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36148976993&doi=10.1145%2f1111609.1111611&partnerID=40&md5=a5185bcbc52d8b1a4ea69cc4cacc9bcf,"We describe the design and implementation of the Exedra continuous media server, and experimentally evaluate alternative resource management policies using a prototype system that we built. Exedra has been designed to provide scalable and efficient support for variable bit-rate media streams whose compression efficiency leads to reduced storage space and bandwidth requirements in comparison to constant bit-rate streams of equivalent quality.We examine alternative disk striping policies, and quantify the benefits of innovative techniques for storage space allocation, buffer management, and resource reservation, which we developed to achieve both predictability and high-performance in handling disk and network data transfers of variable size. Additionally, we investigate the differences between diverse data replication schemes over disk arrays, and compare methods for disk access time reservation that enable tolerance of disk failures at minimal cost. Overall, we demonstrate the feasibility of building network media servers that exploit the latest advances in media compression technology towards reducing the cost of wide-scale streaming services for stored data. © 2005, ACM. All rights reserved.",Content distribution; Design; Experimentation; Measurement; multimedia compression; Performance; Reliability,
Triage: Performance Differentiation for Storage Systems Using Adaptive Control,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014276863&doi=10.1145%2f1111609.1111612&partnerID=40&md5=1fe34f4eec5b4a62651f57ee9e775fd4,"Ensuring performance isolation and differentiation among workloads that share a storage infrastructure is a basic requirement in consolidated data centers. Existing management tools rely on resource provisioning to meet performance goals; they require detailed knowledge of the system characteristics and the workloads. Provisioning is inherently slow to react to system and workload dynamics and, in the general case, it is not practical to provision for the worst case. We propose a software-only solution that ensures predictable performance for storage access. It is applicable to a wide range of storage systems and makes no assumptions about workload characteristics. We use an online feedback loop with an adaptive controller that throttles storage access requests to ensure that the available system throughput is shared among workloads according to their performance goals and their relative importance. The controller considers the system as a “black box” and adapts automatically to system and workload changes. The controller is distributed to ensure high availability under overload conditions, and it can be used for both block and file access protocols. The evaluation of Triage, our experimental prototype, demonstrates workload isolation and differentiation in an overloaded cluster file-system where workloads and system components are changing. © 2005, ACM. All rights reserved.",clustered file systems; control theory; Management; Performance; performance goals; performance management; policy-based management; QoS; Storage resource management,
Efficient Disk Replacement and Data Migration Algorithms for Large Disk Subsystems,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997752640&doi=10.1145%2f1084779.1084781&partnerID=40&md5=d8d86b82d68e0273291289d3ff422b45,"Random data placement, which is efficient and scalable for large-scale storage systems, has recently emerged as an alternative to traditional data striping. In this report, we study the disk replacement problem (DRP) to find a sequence of disk additions and removals for a storage system, while migrating the data and respecting the following constraints: (1) the data is initially balanced across the existing distributed disk configuration, (2) the data must again be balanced across the new configuration, and (3) the data migration cost must be minimized. In practice, migrating data from old disks to new devices is complicated by the fact that the total number of disks connected to the storage system is often limited by a fixed number of available slots and not all the old and new disks can be connected at the same time. This article presents solutions for both cases where the number of disk slots is either unconstrained or constrained. © 2005, ACM. All rights reserved.",Algorithms; data migration; Disk replacement; Management; randomized striping; storage resource management,
Performance Directed Energy Management for Main Memory and Disks,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013771039&doi=10.1145%2f1084779.1084782&partnerID=40&md5=689de0c71d9d7f4ab6d0610a5053316c,"Much research has been conducted on energy management for memory and disks. Most studies use control algorithms that dynamically transition devices to low power modes after they are idle for a certain threshold period of time. The control algorithms used in the past have two major limitations. First, they require painstaking, application-dependent manual tuning of their thresholds to achieve energy savings without significantly degrading performance. Second, they do not provide performance guarantees. This article addresses these two limitations for both memory and disks, making memory/disk energy-saving schemes practical enough to use in real systems. Specifically, we make four main contributions. (1)We propose a technique that provides a performance guarantee for control algorithms. We show that our method works well for all tested cases, even with previously proposed algorithms that are not performance-aware. (2) We propose a new control algorithm, Performance-Directed Dynamic (PD), that dynamically adjusts its thresholds periodically, based on available slack and recent workload characteristics. For memory, PD consumes the least energy when compared to previous hand-tuned algorithms combined with a performance guarantee. However, for disks, PD is too complex and its self-tuning is unable to beat previous hand-tuned algorithms. (3) To improve on PD, we propose a simpler, optimization-based, threshold-free control algorithm, Performance-Directed Static (PS). PS periodically assigns a static configuration by solving an optimization problem that incorporates information about the available slack and recent traffic variability to different chips/disks. We find that PS is the best or close to the best across all performance-guaranteed disk algorithms, including hand-tuned versions. (4) We also explore a hybrid scheme that combines PS and PD algorithms to further improve energy savings. © 2005, ACM. All rights reserved.",adaptation algorithms; Algorithms; control algorithms; Disk energy management; lowpower design; memory energy management; multiple-power mode device; performance guarantee,
Multiresolution Storage and Search in Sensor Networks,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024293703&doi=10.1145%2f1084779.1084780&partnerID=40&md5=f117c507b8177841d647dad3c371705e,"Wireless sensor networks enable dense sensing of the environment, offering unprecedented opportunities for observing the physical world. This article addresses two key challenges in wireless sensor networks: in-network storage and distributed search. The need for these techniques arises from the inability to provide persistent, centralized storage and querying in many sensor networks. Centralized storage requires multihop transmission of sensor data to Internet gateways which can quickly drain battery-operated nodes. Constructing a storage and search system that satisfies the requirements of data-rich scientific applications is a daunting task for many reasons: (a) the data requirements may be large compared to available storage and communication capacity of resource-constrained nodes, (b) user requirements are diverse and range from identification and collection of interesting event signatures to obtaining a deeper understanding of long-term trends and anomalies in the sensor events, and (c) many applications are in new domains where a priori information may not be available to reduce these requirements. This article describes a lossy, gracefully degrading storage model. We believe that such a model is necessary and sufficient for many scientific applications since it supports both progressive data collection for interesting events as well as long-term in-network storage for in-network querying and processing. Our system demonstrates the use of in-network wavelet-based summarization and progressive aging of summaries in support of long-term querying in storage and communicationconstrained networks. We evaluate the performance of our linux implementation and show that it achieves: (a) low communication overhead for multiresolution summarization, (b) highly efficient drill-down search over such summaries, and (c) efficient use of network storage capacity through load-balancing and progressive aging of summaries. © 2005, ACM. All rights reserved.",Algorithms; data aging; data storage; Design; drill-down query; multiresolution storage; Performance; wavelet processing; Wireless sensor networks,
Bridging the Digital Divide: Storage Media + Postal Network = Generic High-Bandwidth Communication,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949024394&doi=10.1145%2f1063786.1063791&partnerID=40&md5=d66691f910b6464c32a461e7e65c76f6,"Making high-bandwidth Internet access pervasively available to a large worldwide audience is a difficult challenge, especially in many developing regions. As we wait for the uncertain takeoff of technologies that promise to improve the situation, we propose to explore an approach that is potentially more easily realizable: the use of digital storage media transported by the postal system as a general digital communication mechanism. We shall call such a system a Postmanet. Compared to more conventional wide-area connectivity options, the Postmanet has several important advantages, including wide global reach, great bandwidth potential, low cost, and ease of incremental adoption. While the idea of sending digital content via the postal system is not a new one, none of the existing attempts have turned the postal system into a generic and transparent communication channel that not only can cater to a wide array of applications, but also effectively manage the many idiosyncrasies associated with using the postal system. In the proposed Postmanet, we see two recurring themes at many different levels of the system. One is the simultaneous exploitation of the Internet and the postal system so we can combine their latency and bandwidth advantages. The other is the exploitation of the abundant capacity and bandwidth of the Postmanet to improve its latency, cost, and reliability. © 2005, ACM. All rights reserved.",Design; Distributed systems; Experimentation; Measurement; peer-to-peer systems; Performance; postal system; storage systems; the digital divide,
Ext3cow: A Time-Shifting File System for Regulatory Compliance,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33244463191&doi=10.1145%2f1063786.1063789&partnerID=40&md5=f8bc27caad53f053830194b551d87257,"The ext3cow file system, built on the popular ext3 file system, provides an open-source file versioning and snapshot platform for compliance with the versioning and audtitability requirements of recent electronic record retention legislation. Ext3cow provides a time-shifting interface that permits a real-time and continuous view of data in the past. Time-shifting does not pollute the file system namespace nor require snapshots to be mounted as a separate file system. Further, ext3cow is implemented entirely in the file system space and, therefore, does not modify kernel interfaces or change the operation of other file systems. Ext3cow takes advantage of the fine-grained control of on-disk and in-memory data available only to a file system, resulting in minimal degradation of performance and functionality. Experimental results confirm this hypothesis; ext3cow performs comparably to ext3 on many benchmarks and on trace-driven experiments. © 2005, ACM. All rights reserved.",copy-on-write; Design; Measurement; Performance; Reliability; Versioning file systems,
Mining Block Correlations to ImproveStorage Performance,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013608562&doi=10.1145%2f1063786.1063790&partnerID=40&md5=92915eaa3757c851753a4899dfe73b8d,"Block correlations are common semantic patterns in storage systems. They can be exploited for improving the effectiveness of storage caching, prefetching, data layout, and disk scheduling. Unfortunately, information about block correlations is unavailable at the storage system level. Previous approaches for discovering file correlations in file systems do not scale well enough for discovering block correlations in storage systems. In this article, we propose two algorithms, C-Miner and C-Miner*, that use a data mining technique called frequent sequence mining to discover block correlations in storage systems. Both algorithms run reasonably fast with feasible space requirement, indicating that they are practical for dynamically inferring correlations in a storage system. C-Miner is a direct application of a frequent-sequence mining algorithm with a few modifications; compared with C-Miner, C-Miner* is redesigned for mining block correlations by making concessions for the specific problem of long sequences in storage system traces. Therefore, C-Miner* can discover 7-109% more correlation rules within 2-15 times shorter time than C-Miner. Moreover, we have also evaluated the benefits of block correlation-directed prefetching and data layout through experiments. Our results using real system workloads show that correlation-directed prefetching and data layout can reduce average I/O response time by 12-30% compared to the base case, and 7-25% compared to the commonly used sequential prefetching scheme for most workloads. © 2005, ACM. All rights reserved.",Algorithms; block correlations; file system management; Management; mining methods and algorithms; Performance; Storage management,
Improving Storage System Availability with D-GRAID,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901017200&doi=10.1145%2f1063786.1063787&partnerID=40&md5=4eb09a2f28bf120cc6633879cae84a88,"We present the design, implementation, and evaluation of D-GRAID, a gracefully degrading and quickly recovering RAID storage array. D-GRAID ensures that most files within the file system remain available even when an unexpectedly high number of faults occur. D-GRAID achieves high availability through aggressive replication of semantically critical data, and fault-isolated placement of logically related data. D-GRAID also recovers from failures quickly, restoring only live file system data to a hot spare. Both graceful degradation and live-block recovery are implemented in a prototype SCSI-based storage system underneath unmodified file systems, demonstrating that powerful “file-system like” functionality can be implemented within a “semantically smart” disk system behind a narrow block-based interface. © 2005, ACM. All rights reserved.",Algorithms; Block-based storage; Design; Disk array; fault isolation; file systems; RAID; Reliability; smart disks,
Network File Storage With Graceful Performance Degradation,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977961519&doi=10.1145%2f1063786.1063788&partnerID=40&md5=4388746a42ca1f72b480ef4129efff90,"A file storage scheme is proposed for networks containing heterogeneous clients. In the scheme, the performance measured by file-retrieval delays degrades gracefully under increasingly serious faulty circumstances. The scheme combines coding with storage for better performance. The problem is NP-hard for general networks; and this article focuses on tree networks with asymmetric edges between adjacent nodes. A polynomial-time memory-allocation algorithm is presented, which determines how much data to store on each node, with the objective of minimizing the total amount of data stored in the network. Then a polynomial-time data-interleaving algorithm is used to determine which data to store on each node for satisfying the quality-of-service requirements in the scheme. By combining the memory-allocation algorithm with the data-interleaving algorithm, an optimal solution to realize the file storage scheme in tree networks is established. © 2005, ACM. All rights reserved.",Algorithms; Domination; fault tolerance; file assignment; interleaving; memory allocation; Performance; Reliability; Theory,
"DISP: Practical, Efficient, Secure and Fault-Tolerant Distributed Data Storage",2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449094561&doi=10.1145%2f1044956.1044960&partnerID=40&md5=6dbf937e758729411042004b64d47ed7,"DISP is a practical client-server protocol for the distributed storage of immutable data objects. Unlike most other contemporary protocols, DISP permits applications to make explicit tradeoffs between total storage space, computational overhead, and guarantees of availability, integrity, and privacy on a per-object basis. Applications specify the degree of redundancy with which each item is encoded, what level of integrity checks are computed and stored with each item, and whether items are stored in an encrypted format. At one extreme, clients willing to pay the overhead are guaranteed privacy, integrity, and availability of data stored in the system as long as fewer than half the servers are Byzantine. At the other extreme, objects that do not require privacy or integrity in the face of Byzantine servers can be stored with very low computational and storage overhead. DISP is efficient in terms of message count, message size, and storage requirements: even in the worst case, the read and write protocols require a number of messages that are linear with respect to the number of servers. In terms of message size, DISP requires transferring only marginally more than L bytes to correctly read an object of size L, even in the face of Byzantine server failures. In this article we provide a description of DISP and an analysis of its fault-tolerant properties. We also analyze the complexity of the protocol and discuss several potential applications. We conclude with a description of our prototype implementation and measurements of its performance on commodity hardware. © 2005, ACM. All rights reserved.",Design; Distributed data storage; Reliability,
Consistent and Automatic Replica Regeneration,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009706278&doi=10.1145%2f1044956.1044958&partnerID=40&md5=2b77de24625462bae85c508e4c900ad6,"Reducing management costs and improving the availability of large-scale distributed systems require automatic replica regeneration, that is, creating new replicas in response to replica failures. A major challenge to regeneration is maintaining consistency when the replica group changes. Doing so is particularly difficult across the wide area where failure detection is complicated by network congestion and node overload. In this context, this article presents Om, the first read/write peer-to-peer, wide-area storage system that achieves high availability and manageability through online automatic regeneration while still preserving consistency guarantees. We achieve these properties through the following techniques. First, by utilizing the limited view divergence property in today's Internet and by adopting the witness model, Om is able to regenerate from any single replica, rather than requiring a majority quorum, at the cost of a small (10-6in our experiments) probability of violating consistency during each regeneration. As a result, Om can deliver high availability with a small number of replicas, while traditional designs would significantly increase the number of replicas. Next, we distinguish failure-free reconfigurations from failure-induced ones, enabling common reconfigurations to proceed with a single round of communication. Finally, we use a lease graph among the replicas and a two-phase write protocol to optimize for reads, so that reads in Om can be processed by any single replica. Experiments on PlanetLab show that consistent regeneration in Om completes in approximately 20 seconds. © 2005, ACM. All rights reserved.",Algorithms; availability; consistency; Design; Experimentation; Peer-to-peer storage systems; regeneration; Reliability; replication,
Cheap Recovery: A Key to Self-Managing State,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008940268&doi=10.1145%2f1044956.1044959&partnerID=40&md5=ba94b6244381ff37ac13a72731d7cb1b,"Cluster hash tables (CHTs) are key components of many large-scale Internet services due to their highly-scalable performance and the prevalence of the type of data they store. Another advantage of CHTs is that they can be designed to be as self-managing as a cluster of stateless servers. One key to achieving this extreme manageability is reboot-based recovery that is predictably fast and has modest impact on system performance and availability. This “cheap” recovery mechanism simplifies management in two ways. First, it simplifies failure detection by lowering the cost of acting on false positives. This enables one to use statistical techniques to turn hard-to-catch failures, such as node degradation, into failure, followed by recovery. Second, cheap recovery simplifies capacity planning by recasting repartitioning as failure plus recovery to achieve zero-downtime incremental scaling. These low-cost recovery and scaling mechanisms make it possible for the system to be continuously self-adjusting, a key property of self-managing systems. © 2005, ACM. All rights reserved.",,
Tunable Randomization for Load Management in Shared-Disk Clusters,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845902112&doi=10.1145%2f1044956.1044962&partnerID=40&md5=40295ce78b88353cf81273326c7b76c6,"We develop and evaluate a system for load management in shared-disk file systems built on clusters of heterogeneous computers. It balances workload by moving file sets among cluster server nodes. It responds to changing server resources that arise from failure and recovery, and dynamically adding or removing servers. It also realizes performance consistency-nearly uniform performance across all servers. The system is adaptive and self-tuning. It operates without any a priori knowledge of workload properties, or the capabilities of the servers. Rather, it continuously tunes load placement using a technique called adaptive, nonuniform (ANU) randomization. ANU randomization realizes the scalability and metadata reduction benefits of hash-based, randomized placement techniques, while avoiding hashing's drawbacks: load skew, inability to cope with heterogeneity, and lack of tunability. ANU randomization outperforms virtual-processor approaches to load balancing, while reducing the amount of shared state among servers and the amount of load movement. © 2005, ACM. All rights reserved.",Algorithms; computer clusters; heterogeneity; Load management; Management; Measurement; Performance; shared-disk file systems,
Reliability and Security of RAID Storage Systems and D2D Archives Using SATA Disk Drives,2005,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016299607&doi=10.1145%2f1044956.1044961&partnerID=40&md5=db28ebbb0bd12b6a2885db335a5246bf,"Information storage reliability and security is addressed by using personal computer disk drives in enterprise-class nearline and archival storage systems. The low cost of these serial ATA (SATA) PC drives is a tradeoff against drive reliability design and demonstration test levels, which are higher in the more expensive SCSI and Fibre Channel drives. This article discusses the tradeoff between SATA which has the advantage that fewer higher capacity drives are needed for a given system storage capacity, which further reduces cost and allows higher drive failure rates, and the use of additional storage system redundancy and drive failure prediction to maintain system data integrity using less reliable drives. RAID stripe failure probability is calculated using typical ATA and SCSI drive failure rates, for single and double parity data reconstruction failure, and failure due to drive unrecoverable block errors. Reliability improvement from drive failure prediction is also calculated, and can be significant. Todayf SATA drive specifications for unrecoverable block errors appear to allow stripe reconstruction failure, and additional in-drive parity blocks are suggested as a solution. The possibility of using low cost disks data for backup and archiving is discussed, replacing higher cost magnetic tape. This requires significantly better RAID stripe failure probability, and suitable drive technology alternatives are discussed. The failure rate of nonoperating drives is estimated using failure analysis results from ≈4000 drives. Nonoperating RAID stripe failure rates are thereby estimated. User data security needs to be assured in addition to reliability, and to extend past the point where physical control of drives is lost, such as when drives are removed from systems for data vaulting, repair, sale, or discard. Today, over a third of resold drives contain unerased user data. Security is proposed via the existing SATA drive secure-erase command, or via the existing SATA drive password commands, or by data encryption. Finally, backup and archival disc storage is compared to magnetic tape, a technology with a proven reliability record over the full half-century of digital data storage. In contrast, tape archives are not vulnerable to tape transport failure modes. Only failure modes in the archived tapes and reels will make data unrecoverable. © 2005, ACM. All rights reserved.",archival storage; Disk drive; failure prediction; SATA; secure erase; SMART; storage resource management; storage systems architecture,
EMPRESS: Accelerating Scientific Discovery through Descriptive Metadata Management,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146426732&doi=10.1145%2f3523698&partnerID=40&md5=e72aa0101765bc040a1fd16e6313355b,"High-performance computing scientists are producing unprecedented volumes of data that take a long time to load for analysis. However, many analyses only require loading in the data containing particular features of interest and scientists have many approaches for identifying these features. Therefore, if scientists store information (descriptive metadata) about these identified features, then for subsequent analyses they can use this information to only read in the data containing these features. This can greatly reduce the amount of data that scientists have to read in, thereby accelerating analysis. Despite the potential benefits of descriptive metadata management, no prior work has created a descriptive metadata system that can help scientists working with a wide range of applications and analyses to restrict their reads to data containing features of interest. In this article, we present EMPRESS, the first such solution. EMPRESS offers all of the features needed to help accelerate discovery: It can accelerate analysis by up to 300 ×, supports a wide range of applications and analyses, is high-performing, is highly scalable, and requires minimal storage space. In addition, EMPRESS offers features required for a production-oriented system: scalable metadata consistency techniques, flexible system configurations, fault tolerance as a service, and portability.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",accelerating scientific discovery; ATDM; data tagging; Decaf; Descriptive metadata; EMPRESS; HDF5; high-level indexing,Digital storage; Information management; Metadata; Accelerating scientific discovery; ATDM; Data tagging; Decaf; Descriptive metadata; EMPRESS; HDF5; High-level indexing; Metadata management; Scientific discovery; Fault tolerance
ctFS: Replacing File Indexing with Hardware Memory Translation through Contiguous File Allocation for Persistent Memory,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146430596&doi=10.1145%2f3565026&partnerID=40&md5=a965542026f9522b79a7c92867849c12,"Persistent byte-addressable memory (PM) is poised to become prevalent in future computer systems. PMs are significantly faster than disk storage, and accesses to PMs are governed by the Memory Management Unit (MMU) just as accesses with volatile RAM. These unique characteristics shift the bottleneck from I/O to operations such as block address lookup - for example, in write workloads, up to 45% of the overhead in ext4-DAX is due to building and searching extent trees to translate file offsets to addresses on persistent memory.We propose a novel contiguous file system, ctFS, that eliminates most of the overhead associated with indexing structures such as extent trees in the file system. ctFS represents each file as a contiguous region of virtual memory, hence a lookup from the file offset to the address is simply an offset operation, which can be efficiently performed by the hardware MMU at a fraction of the cost of software-maintained indexes. Evaluating ctFS on real-world workloads such as LevelDB shows it outperforms ext4-DAX and SplitFS by 3.6× and 1.8×, respectively.  © 2022 Copyright held by the owner/author(s).",data center; file system; memory allocation; page table; Persistent memory,Indexing (of information); Physical addresses; Random access storage; Address lookup; Datacenter; File allocation; Filesystem; Indexing structures; Lookups; Page table; Persistent memory; Storage and access; Virtual memory; Virtual addresses
Improving the Endurance of Next Generation SSD's using WOM-v Codes,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146438995&doi=10.1145%2f3565027&partnerID=40&md5=8563f30f42ce8ef702fcbe7d884ce8b9,"High density Solid State Drives, such as QLC drives, offer increased storage capacity, but a magnitude lower Program and Erase (P/E) cycles, limiting their endurance and hence usability. We present the design and implementation of non-binary, Voltage-Based Write-Once-Memory (WOM-v) Codes to improve the lifetime of QLC drives. First, we develop a FEMU based simulator test-bed to evaluate the gains of WOM-v codes on real world workloads. Second, we propose and implement two optimizations, an efficient garbage collection mechanism and an encoding optimization to drastically improve WOM-v code endurance without compromising performance. Third, we propose analytical approaches to obtain estimates of the endurance gains under WOM-v codes. We analyze the Greedy garbage collection technique with uniform page access distribution and the Least Recently Written (LRW) garbage collection technique with skewed page access distribution in the context of WOM-v codes. We find that although both approaches overestimate the number of required erase operations, the model based on greedy garbage collection with uniform page access distribution provides tighter bounds. A careful evaluation, including microbenchmarks and trace-driven evaluation, demonstrates that WOM-v codes can reduce Erase cycles for QLC drives by 4.4×-11.1× for real world workloads with minimal performance overheads resulting in improved QLC SSD lifetime.  © 2022 Association for Computing Machinery.",endurance; QLC flash; SSD; Write Once Memory Code,Drives; Flash-based SSDs; Garbage collection; Memory codes; Optimisations; Performance; QLC flash; Real-world; SSD; Storage capacity; Write once; Write once memory code; Refuse collection
"Ares: Adaptive, Reconfigurable, Erasure coded, Atomic Storage",2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140910334&doi=10.1145%2f3510613&partnerID=40&md5=f66769a636c3c606f91d81fb608b87e3,"Emulating a shared atomic, read/write storage system is a fundamental problem in distributed computing. Replicating atomic objects among a set of data hosts was the norm for traditional implementations (e.g., [11]) in order to guarantee the availability and accessibility of the data despite host failures. As replication is highly storage demanding, recent approaches suggested the use of erasure-codes to offer the same fault-tolerance while optimizing storage usage at the hosts. Initial works focused on a fixed set of data hosts. To guarantee longevity and scalability, a storage service should be able to dynamically mask hosts failures by allowing new hosts to join, and failed host to be removed without service interruptions. This work presents the first erasure-code -based atomic algorithm, called Ares, which allows the set of hosts to be modified in the course of an execution. Ares is composed of three main components: (i) a reconfiguration protocol, (ii) a read/write protocol, and (iii) a set of data access primitives (DAPs). The design of Ares is modular and is such to accommodate the usage of various erasure-code parameters on a per-configuration basis. We provide bounds on the latency of read/write operations and analyze the storage and communication costs of the Ares algorithm.  © 2022 Association for Computing Machinery.",Atomicity; distributed storage; erasure-codes; fault-tolerance; reconfiguration,Atoms; Digital storage; Distributed computer systems; Forward error correction; Atomic objects; Atomicity; Distributed storage; Erasure codes; Fixed sets; New hosts; Reconfigurable; Reconfiguration; Storage services; Storage systems; Fault tolerance
A Disk Failure Prediction Method Based on Active Semi-supervised Learning,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146436656&doi=10.1145%2f3523699&partnerID=40&md5=7a0d379cba6da4a82e93f2227d3c2be1,"Disk failure has always been a major problem for data centers, leading to data loss. Current disk failure prediction approaches are mostly offline and assume that the disk labels required for training learning models are available and accurate. However, these offline methods are no longer suitable for disk failure prediction tasks in large-scale data centers. Behind this explosive amount of data, most methods do not consider whether it is not easy to get the label values during the training or the obtained label values are not completely accurate. These problems further restrict the development of supervised learning and offline modeling in disk failure prediction. In this article, Active Semi-supervised Learning Disk-failure Prediction (ASLDP), a novel disk failure prediction method is proposed, which uses active learning and semi-supervised learning. According to the characteristics of data in the disk lifecycle, ASLDP carries out active learning for those clear labeled samples, which selects valuable samples with the most significant probability uncertainty and eliminates redundancy. For those samples that are unclearly labeled or unlabeled, ASLDP uses semi-supervised learning for pre-labeled by calculating the conditional values of the samples and enhances the generalization ability by active learning. Compared with several state-of-the-art offline and online learning approaches, the results on four realistic datasets from Backblaze and Baidu demonstrate that ASLDP achieves stable failure detection rates of 80-85% with low false alarm rates. In addition, we use a dataset from Alibaba to evaluate the generality of ASLDP. Furthermore, ASLDP can overcome the problem of missing sample labels and data redundancy in large data centers, which are not considered and implemented in all offline learning methods for disk failure prediction to the best of our knowledge. Finally, ASLDP can predict the disk failure 4.9 days in advance with lower overhead and latency.  © 2022 Association for Computing Machinery.",active learning; Disk failure prediction; machine learning; semi-supervised learning,Forecasting; Learning algorithms; Learning systems; Life cycle; Redundancy; Active Learning; Datacenter; Disk failure; Disk failure prediction; Failure prediction method; Failures prediction; Learning models; Machine-learning; Off-line learning; Semi-supervised learning; Machine learning
"The what, The from, and The to: The Migration Games in Deduplicated Systems",2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146428421&doi=10.1145%2f3565025&partnerID=40&md5=802810ed3948386a0dfcb5bb860cd732,"Deduplication reduces the size of the data stored in large-scale storage systems by replacing duplicate data blocks with references to their unique copies. This creates dependencies between files that contain similar content and complicates the management of data in the system. In this article, we address the problem of data migration, in which files are remapped between different volumes as a result of system expansion or maintenance. The challenge of determining which files and blocks to migrate has been studied extensively for systems without deduplication. In the context of deduplicated storage, however, only simplified migration scenarios have been considered.In this article, we formulate the general migration problem for deduplicated systems as an optimization problem whose objective is to minimize the system's size while ensuring that the storage load is evenly distributed between the system's volumes and that the network traffic required for the migration does not exceed its allocation.We then present three algorithms for generating effective migration plans, each based on a different approach and representing a different trade-off between computation time and migration efficiency. Our greedy algorithm provides modest space savings but is appealing thanks to its exceptionally short runtime. Its results can be improved by using larger system representations. Our theoretically optimal algorithm formulates the migration problem as an integer linear programming (ILP) instance. Its migration plans consistently result in smaller and more balanced systems than those of the greedy approach, although its runtime is long and, as a result, the theoretical optimum is not always found. Our clustering algorithm enjoys the best of both worlds: its migration plans are comparable to those generated by the ILP-based algorithm, but its runtime is shorter, sometimes by an order of magnitude. It can be further accelerated at a modest cost in the quality of its results.  © 2022 Association for Computing Machinery.",capacity planning; data migration; Deduplication,Computational efficiency; Digital storage; Economic and social effects; Information management; Integer programming; Capacity planning; Data blocks; Data-migration; Deduplication; Integer Linear Programming; Large-scale storage systems; Optimization problems; Runtimes; System expansion; System maintenance; Clustering algorithms
Toward Fast and Scalable Random Walks over Disk-Resident Graphs via Efficient I/O Management,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146441458&doi=10.1145%2f3533579&partnerID=40&md5=e72f5dbeb32f74efd6132e8e9c0df8c1,"Traditional graph systems mainly use the iteration-based model, which iteratively loads graph blocks into memory for analysis so as to reduce random I/Os. However, this iteration-based model limits the efficiency and scalability of running random walk, which is a fundamental technique to analyze large graphs. In this article, we first propose a state-aware I/O model to improve the I/O efficiency of running random walk, then we develop a block-centric indexing and buffering scheme for managing walk data, and leverage an asynchronous walk updating strategy to improve random walk efficiency. We implement an I/O-efficient graph system, GraphWalker, which is efficient to handle very large disk-resident graphs and also scalable to run tens of billions of random walks with only a single commodity machine. Experiments show that GraphWalker can achieve more than an order of magnitude speedup when compared with DrunkardMob, which is tailored for random walks based on the classical graph system GraphChi, as well as two state-of-the-art single-machine graph systems, Graphene and GraFSoft. Furthermore, when compared with the most recent distributed system KnightKing, GraphWalker still achieves comparable performance with only a single machine, thereby making it a more cost-effective alternative.  © 2022 Association for Computing Machinery.",Graph processing system; graph storage; random walk,Cost effectiveness; Digital storage; Iterative methods; Random processes; Graph processing; Graph processing system; Graph storage; Large disks; Large graphs; Load graphs; Processing systems; Random Walk; Single- machines; Updating strategy; Efficiency
Introduction to the Special Section on USENIX FAST 2022,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146447563&doi=10.1145%2f3564770&partnerID=40&md5=ae32ff6ab8abaa59e688b8387cdb3aa2,[No abstract available],,
Tunable Encrypted Deduplication with Attack-resilient Key Management,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146418643&doi=10.1145%2f3510614&partnerID=40&md5=ecf391970f9b87d38078c5c8f8852f74,"Conventional encrypted deduplication approaches retain the deduplication capability on duplicate chunks after encryption by always deriving the key for encryption/decryption from the chunk content, but such a deterministic nature causes information leakage due to frequency analysis. We present TED, a tunable encrypted deduplication primitive that provides a tunable mechanism for balancing the tradeoff between storage efficiency and data confidentiality. The core idea of TED is that its key derivation is based on not only the chunk content but also the number of duplicate chunk copies, such that duplicate chunks are encrypted by distinct keys in a controlled manner. In particular, TED allows users to configure a storage blowup factor, under which the information leakage quantified by an information-theoretic measure is minimized for any input workload. In addition, we extend TED with a distributed key management architecture and propose two attack-resilient key generation schemes that trade between performance and fault tolerance. We implement an encrypted deduplication prototype TEDStore to realize TED in networked environments. Evaluation on real-world file system snapshots shows that TED effectively balances the tradeoff between storage efficiency and data confidentiality, with small performance overhead.  © 2022 Association for Computing Machinery.",cloud storage; Encrypted deduplication,Fault tolerance; Information theory; Storage efficiency; Cloud storages; Data confidentiality; Deduplication; Encrypted deduplication; Encryption/decryption; Information leakage; Key-management; Performance; Storage efficiency; Tunables; Cloud storage
DEFUSE: An Interface for Fast and Correct User Space File System Access,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140824223&doi=10.1145%2f3494556&partnerID=40&md5=e894bcad9fbad4a3c345fed4e3c5d0b6,"Traditionally, the only option for developers was to implement file systems (FSs) via drivers within the operating system kernel. However, there exists a growing number of file systems (FSs), notably distributed FSs for the cloud, whose interfaces are implemented solely in user space to (i) isolate FS logic, (ii) take advantage of user space libraries, and/or (iii) for rapid FS prototyping. Common interfaces for implementing FSs in user space exist, but they do not guarantee POSIX compliance in all cases, or suffer from considerable performance penalties due to high amounts of wait context switchs between kernel and user space processes.We propose DEFUSE: an interface for user space FSs that provides fast accesses while ensuring access correctness and requiring no modifications to applications. DEFUSE: achieves significant performance improvements over existing user space FS interfaces thanks to its novel design that drastically reduces the number of wait context switchs for FS accesses. Additionally, to ensure access correctness, DEFUSE: maintains POSIX compliance for FS accesses thanks to three novel concepts of bypassed file descriptor (FD) lookup, FD stashing, and user space paging. Our evaluation spanning a variety of workloads shows that by reducing the number of wait context switchs per workload from as many as 16,000 or 41,000 with filesystem in user space down to 9 on average, DEFUSE: increases performance 2× over existing interfaces for typical workloads and by as many as 10× in certain instances.  © 2022 Association for Computing Machinery.",FUSE; Linux kernel; user-space file systems,Computation theory; Finite difference method; Linux; Descriptors; Distributed file systems; Filesystem; Linux kernel; Operating system kernel; Performance; System access; System prototyping; User spaces; User-space file system; File organization
WebAssembly-based Delta Sync for Cloud Storage Services,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140873684&doi=10.1145%2f3502847&partnerID=40&md5=e44be342a09269ed21887e9ba66bacde,"Delta synchronization (sync) is crucial to the network-level efficiency of cloud storage services, especially when handling large files with small increments. Practical delta sync techniques are, however, only available for PC clients and mobile apps, but not web browsers-the most pervasive and OS-independent access method. To bridge this gap, prior work concentrates on either reversing the delta sync protocol or utilizing the native client, all striving around the tradeoffs among efficiency, applicability, and usability and thus forming an ""impossible triangle.""Recently, we note the advent of WebAssembly (WASM), a portable binary instruction format that is efficient in both encoding size and load time. In principle, the unique advantages of WASM canmake web-based applications enjoy near-native runtime speed without significant cloud-side or client-side changes. Thus, we implement a straightforward WASM-based delta sync solution, WASMrsync, finding its quasi-asynchronous working manner and conventional In-situ Separate Memory Allocation greatly increase sync time and memory usage. To address them, we strategically devise sync-async code decoupling and streaming compilation, together with Informed In-place File Construction. The resulting solution, WASMrsync+, achieves comparable sync time as the state-of-the-art (most efficient) solution with nearly only half of memory usage, letting the ""impossible triangle""reach a reconciliation.  © 2022 Association for Computing Machinery.",Cloud storage service; delta synchronization; web browser; WebAssembly,Cloud storage; Efficiency; Web services; Cloud storage services; Delta synchronization; Large files; Level efficiencies; Memory usage; Mobile app; Network level; Synchronization technique; Synchronization time; Webassembly; Web browsers
Donag: Generating Efficient Patches and Diffs for Compressed Archives,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141167288&doi=10.1145%2f3507919&partnerID=40&md5=f681fd3ab9a8143623076bbeefb95c44,"Differencing between compressed archives is a common task in file management and synchronization. Applications include source code distribution, application updates, and document synchronization. General purpose binary differencing tools can create and apply patches to compressed archives, but don't consider the internal structure of the compressed archive or the file lifecycle. Therefore, they miss opportunities to save space based on the archive's internal structure and metadata. To address the gap, we develop a content-aware, format independent theory for differencing on compressed archives and propose a canonical form and digest for compressed archives. Based on them, we present Donag, a content-aware differencing and patching algorithm that produces smaller patches than general purpose binary differencing tools on versioned archives by exploiting the compressed archives' internal structure. Donag uses the VCDiff and BSDiff engines internally. We compare Donag's patches to ones produced by bsdiff, xdelta3, and Delta++ on three classes of compressed archives: open-source code repositories, large and small applications, and office productivity documents (DOCX, XLSX, PPTX). Donag's patches are typically 10% to 89% smaller than those produced by bsdiff, xdelta3, and Delta++, with reasonable memory overhead and throughput on commodity hardware. In the worst case, Donag's patches are negligibly larger. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",canonical forms; compression; delta files; Differencing; ZIP archives,Codes (symbols); Life cycle; Open source software; Canonical form; Compression; Content-aware; Delta file; Differencing; Differencing tools; File management; File synchronization; Internal structure; ZIP archive; Open systems
From Hyper-dimensional Structures to Linear Structures: Maintaining Deduplicated Data's Locality,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140919717&doi=10.1145%2f3507921&partnerID=40&md5=26439d63f1206c00375facaae612866d,"Data deduplication is widely used to reduce the size of backup workloads, but it has the known disadvantage of causing poor data locality, also referred to as the fragmentation problem. This results from the gap between the hyper-dimensional structure of deduplicated data and the sequential nature of many storage devices, and this leads to poor restore and garbage collection (GC) performance. Current research has considered writing duplicates to maintain locality (e.g., rewriting) or caching data in memory or SSD, but fragmentation continues to lower restore and GC performance.Investigating the locality issue, we design a method to flatten the hyper-dimensional structured deduplicated data to a one-dimensional format, which is based on classification of each chunk's lifecycle, and this creates our proposed data layout. Furthermore, we present a novel management-friendly deduplication framework, called MFDedup, that applies our data layout and maintains locality as much as possible. Specifically, we use two key techniques in MFDedup: Neighbor-duplicate-focus indexing (NDF) and Across-version-aware Reorganization scheme (AVAR). NDF performs duplicate detection against a previous backup, then AVAR rearranges chunks with an offline and iterative algorithm into a compact, sequential layout, which nearly eliminates random I/O during file restores after deduplication.Evaluation results with five backup datasets demonstrate that, compared with state-of-the-art techniques, MFDedup achieves deduplication ratios that are 1.12× to 2.19× higher and restore throughputs that are 1.92× to 10.02× faster due to the improved data layout. While the rearranging stage introduces overheads, it is more than offset by a nearly-zero overhead GC process. Moreover, the NDF index only requires indices for two backup versions, while the traditional index grows with the number of versions retained.  © 2022 Copyright held by the owner/author(s).",Fragmentation; garbage collection; restore,Life cycle; Refuse collection; Restoration; Virtual storage; Collection performance; Data layouts; Data locality; Deduplication; Dimensional structures; Fragmentation; Garbage collection; Linear structures; Reorganisation; Restore; Iterative methods
Introduction to the Special Section on SOSP 2021,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141158742&doi=10.1145%2f3542850&partnerID=40&md5=e07001b1e4518a1f1233d48f184666c8,[No abstract available],,
Lightweight Robust Size Aware Cache Management,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141171906&doi=10.1145%2f3507920&partnerID=40&md5=2ab145f81d39f39152347861874d6817,"Modern key-value stores, object stores, Internet proxy caches, and Content Delivery Networks (CDN) often manage objects of diverse sizes, e.g., blobs, video files of different lengths, images with varying resolutions, and small documents. In such workloads, size-aware cache policies outperform size-oblivious algorithms. Unfortunately, existing size-aware algorithms tend to be overly complicated and computationally expensive.Our work follows a more approachable pattern; we extend the prevalent (size-oblivious) TinyLFU cache admission policy to handle variable-sized items. Implementing our approach inside two popular caching libraries only requires minor changes. We show that our algorithms yield competitive or better hit-ratios and byte hit-ratios compared to the state-of-the-art size-aware algorithms such as AdaptSize, LHD, LRB, and GDSF. Further, a runtime comparison indicates that our implementation is faster by up to 3× compared to the best alternative, i.e., it imposes a much lower CPU overhead. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CDN; size aware caching; Software cache management; storage,Cache management; Content delivery network; Hit ratio; Key-value stores; Object store; Oblivious algorithms; Size aware caching; Software cache management; Software caches; Video files; Proxy caches
Exploiting Nil-external Interfaces for Fast Replicated Storage,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140931384&doi=10.1145%2f3542821&partnerID=40&md5=b3c884a38891fbc8070e007a4b170a06,"Do some storage interfaces enable higher performance than others? Can one identify and exploit such interfaces to realize high performance in storage systems? This article answers these questions in the affirmative by identifying nil-externality, a property of storage interfaces. A nil-externalizing (nilext) interface may modify state within a storage system but does not externalize its effects or system state immediately to the outside world. As a result, a storage system can apply nilext operations lazily, improving performance.In this article, we take advantage of nilext interfaces to build high-performance replicated storage. We implement Skyros, a nilext-aware replication protocol that offers high performance by deferring ordering and executing operations until their effects are externalized. We show that exploiting nil-externality offers significant benefit: For many workloads, Skyros provides higher performance than standard consensus-based replication. For example, Skyros offers 3× lower latency while providing the same high throughput offered by throughput-optimized Paxos.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fault-tolerance; replication; storage,Interface states; High-throughput; Improving performance; Low latency; Performance; Property; Replicated storage; Replication; Replication protocol; Storage systems; System state; Fault tolerance
Kangaroo: Theory and Practice of Caching Billions of Tiny Objects on Flash,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140916052&doi=10.1145%2f3542928&partnerID=40&md5=4293ca8b2ea7d71e4796b64d5a0fe2a0,"Many social-media and IoT services have very large working sets consisting of billions of tiny (≈100 B) objects. Large, flash-based caches are important to serving these working sets at acceptable monetary cost. However, caching tiny objects on flash is challenging for two reasons: (i) SSDs can read/write data only in multi-KB ""pages""that are much larger than a single object, stressing the limited number of times flash can be written; and (ii) very few bits per cached object can be kept in DRAM without losing flash's cost advantage. Unfortunately, existing flash-cache designs fall short of addressing these challenges: write-optimized designs require too much DRAM, and DRAM-optimized designs require too many flash writes.We present Kangaroo, a new flash-cache design that optimizes both DRAM usage and flash writes to maximize cache performance while minimizing cost. Kangaroo combines a large, set-associative cache with a small, log-structured cache. The set-associative cache requires minimal DRAM, while the log-structured cache minimizes Kangaroo's flash writes. Experiments using traces from Meta and Twitter show that Kangaroo achieves DRAM usage close to the best prior DRAM-optimized design, flash writes close to the best prior write-optimized design, and miss ratios better than both. Kangaroo's design is Pareto-optimal across a range of allowed write rates, DRAM sizes, and flash sizes, reducing misses by 29% over the state of the art. These results are corroborated by analytical models presented herein and with a test deployment of Kangaroo in a production flash cache at Meta.  © 2022 Copyright held by the owner/author(s).",caching; Flash; tiny objects,Integrated circuit design; Social networking (online); Cache design; Caching; Flash; Log structured; Optimized designs; Set-associative caches; Social media; Theory and practice; Tiny object; Working set; Pareto principle
Building GC-free Key-value Store on HM-SMR Drives with ZoneFS,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140839787&doi=10.1145%2f3502846&partnerID=40&md5=bb6717d593866ac36ba78abd5770b070,"Host-managed shingled magnetic recording drives (HM-SMR) are advantageous in capacity to harness the explosive growth of data. For key-value (KV) stores based on log-structured merge trees (LSM-trees), the HM-SMR drive is an ideal solution owning to its capacity, predictable performance, and economical cost. However, building an LSM-tree-based KV store on HM-SMR drives presents severe challenges in maintaining the performance and space utilization efficiency due to the redundant cleaning processes for applications and storage devices (i.e., compaction and garbage collection). To eliminate the overhead of on-disk garbage collection (GC) and improve compaction efficiency, this article presents GearDB, a GC-free KV store tailored for HM-SMR drives. GearDB improves the write performance and space efficiency through three new techniques: a new on-disk data layout, compaction windows, and a novel gear compaction algorithm. We further augment the read performance of GearDB with a new SSTable layout and read ahead mechanism. We implement GearDB with LevelDB, and use zonefs to access a real HM-SMR drive. Our extensive experiments confirm that GearDB achieves both high performance and space efficiency, i.e., on average 1.7× and 1.5× better than LevelDB in random write and read, respectively, with up to 86.9% space efficiency.  © 2022 Association for Computing Machinery.",garbage collection; gear compaction; host-managed SMR; Key-value store; LSM-Tree,Refuse collection; Storage efficiency; Virtual storage; Explosive growth; Garbage collection; Gear compaction; Host-managed SMR; Key-value stores; Log structured merge trees; LSM-tree; Performance efficiency; Shingled magnetic recordings; Space efficiencies; Compaction
Automatic Stream Identification to Improve Flash Endurance in Data Centers,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130289728&doi=10.1145%2f3470007&partnerID=40&md5=42915ed9d2193b870f7306ab69a2626e,"The demand for high performance I/O in Storage-as-a-Service (SaaS) is increasing day by day. To address this demand, NAND Flash-based Solid-state Drives (SSDs) are commonly used in data centers as cache-or top-tiers in the storage rack ascribe to their superior performance compared to traditional hard disk drives (HDDs). Meanwhile, with the capital expenditure of SSDs declining and the storage capacity of SSDs increasing, all-flash data centers are evolving to serve cloud services better than SSD-HDD hybrid data centers. During this transition, the biggest challenge is how to reduce the Write Amplification Factor (WAF) as well as to improve the endurance of SSD since this device has a limited program/erase cycles. A specified case is that storing data with different lifetimes (i.e., I/O streams with similar temporal fetching patterns such as reaccess frequency) in one single SSD can cause high WAF, reduce the endurance, and downgrade the performance of SSDs. Motivated by this, multi-stream SSDs have been developed to enable data with a different lifetime to be stored in different SSD regions. The logic behind this is to reduce the internal movement of data-when garbage collection is triggered, there are high chances of having data blocks with either all the pages being invalid or valid. However, the limitation of this technology is that the system needs to manually assign the same streamID to data with a similar lifetime. Unfortunately, when data arrives, it is not known how important this data is and how long this data will stay unmodified. Moreover, according to our observation, with different definitions of a lifetime (i.e., different calculation formulas based on selected features previously exhibited by data, such as sequentiality, and frequency), streamID identification may have varying impacts on the final WAF of multi-stream SSDs. Thus, in this article, we first develop a portable and adaptable framework to study the impacts of different workload features and their combinations on write amplification. We then propose a feature-based stream identification approach, which automatically co-relates the measurable workload attributes (such as I/O size, I/O rate, and so on.) with high-level workload features (such as frequency, sequentiality, and so on.) and determines a right combination of workload features for assigning streamIDs. Finally, we develop an adaptable stream assignment technique to assign streamID for changing workloads dynamically. Our evaluation results show that our automation approach of stream detection and separation can effectively reduce the WAF by using appropriate features for stream assignment with minimal implementation overhead.  © 2022 Association for Computing Machinery.",coherency; I/O stream detection; I/O workload characterization; multi-streaming; NAND flash endurance; Solid state drives; write amplification factor,Flash-based SSDs; Hard disk storage; Memory architecture; Servers; Amplification factors; Coherency; I/O stream detection; I/O workload characterization; Multistreaming; NAND Flash; NAND flash endurance; Solid state drive; Stream detections; Workload characterization; Write amplification factor; Write amplifications; NAND circuits
A Study of Failure Recovery and Logging of High-Performance Parallel File Systems,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130253689&doi=10.1145%2f3483447&partnerID=40&md5=5c842ed7b2931e8f22d3f4f0380bf710,"Large-scale parallel file systems (PFSs) play an essential role in high-performance computing (HPC). However, despite their importance, their reliability is much less studied or understood compared with that of local storage systems or cloud storage systems. Recent failure incidents at real HPC centers have exposed the latent defects in PFS clusters as well as the urgent need for a systematic analysis.To address the challenge, we perform a study of the failure recovery and logging mechanisms of PFSs in this article. First, to trigger the failure recovery and logging operations of the target PFS, we introduce a black-box fault injection tool called PFault, which is transparent to PFSs and easy to deploy in practice. PFault emulates the failure state of individual storage nodes in the PFS based on a set of pre-defined fault models and enables examining the PFS behavior under fault systematically.Next, we apply PFault to study two widely used PFSs: Lustre and BeeGFS. Our analysis reveals the unique failure recovery and logging patterns of the target PFSs and identifies multiple cases where the PFSs are imperfect in terms of failure handling. For example, Lustre includes a recovery component called LFSCK to detect and fix PFS-level inconsistencies, but we find that LFSCK itself may hang or trigger kernel panics when scanning a corrupted Lustre. Even after the recovery attempt of LFSCK, the subsequent workloads applied to Lustre may still behave abnormally (e.g., hang or report I/O errors). Similar issues have also been observed in BeeGFS and its recovery component BeeGFS-FSCK. We analyze the root causes of the abnormal symptoms observed in depth, which has led to a new patch set to be merged into the coming Lustre release. In addition, we characterize the extensive logs generated in the experiments in detail and identify the unique patterns and limitations of PFSs in terms of failure logging. We hope this study and the resulting tool and dataset can facilitate follow-up research in the communities and help improve PFSs for reliable high-performance computing.  © 2022 Association for Computing Machinery.",failure handling; file system checkers; high performance computing; logging; Parallel file systems; reliability; storage systems,Digital storage; Failure (mechanical); Recovery; Failure handling; Failure recovery; File system checkers; High performance computing; Large-scales; Parallel file system; Performance; Performance computing; Storage Clouds; Storage systems; File organization
Introduction to the Special Section on USENIX ATC 2021,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130267862&doi=10.1145%2f3519550&partnerID=40&md5=cf642488b33ece20e6459a5d69a5a692,[No abstract available],,
"Characterization Summary of Performance, Reliability, and Threshold Voltage Distribution of 3D Charge-Trap NAND Flash Memory",2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130277033&doi=10.1145%2f3491230&partnerID=40&md5=39013a3c451858458823e526ed8f350e,"Solid-state drive (SSD) gradually dominates in the high-performance storage scenarios. Three-dimension (3D) NAND flash memory owning high-storage capacity is becoming a mainstream storage component of SSD. However, the interferences of the new 3D charge-trap (CT) NAND flash are getting unprecedentedly complicated, yielding to many problems regarding reliability and performance. Alleviating these problems needs to understand the characteristics of 3D CT NAND flash memory deeply. To facilitate such understanding, in this article, we delve into characterizing the performance, reliability, and threshold voltage (Vth) distribution of 3D CT NAND flash memory. We make a summary of these characteristics with multiple interferences and variations and give several new insights and a characterization methodology. Especially, we characterize the skewed (Vth) distribution, (Vth) shift laws, and the exclusive layer variation in 3D NAND flash memory. The characterization is the backbone of designing more reliable and efficient flash-based storage solutions.  © 2022 Association for Computing Machinery.",3D CT NAND flash; characterization; performance; reliability; threshold voltage,Charge trapping; Flash-based SSDs; Memory architecture; NAND circuits; Threshold voltage; 3d charge-trap NAND flash; Characterization; Charge trap; NAND Flash; NAND flash memory; Performance; Performance reliability; Storage capacity; Three dimensions; Threshold voltage distribution; Reliability
HintStor: A Framework to Study I/O Hints in Heterogeneous Storage,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130240831&doi=10.1145%2f3489143&partnerID=40&md5=72e7b4c3e16848d486f83d3fefbea678,"To bridge the giant semantic gap between applications and modern storage systems, passing a piece of tiny and useful information, called I/O access hints, from upper layers to the storage layer may greatly improve application performance and ease data management in storage systems. This is especially true for heterogeneous storage systems that consist of multiple types of storage devices. Since ingesting external access hints will likely involve laborious modifications of legacy I/O stacks, it is very hard to evaluate the effect and take advantages of access hints. In this article, we design a generic and flexible framework, called HintStor, to quickly play with a set of I/O access hints and evaluate their impacts on heterogeneous storage systems. HintStor provides a new application/user-level interface, a file system plugin, and performs data management with a generic block storage data manager. We demonstrate the flexibility of HintStor by evaluating four types of access hints: File system data classification, stream ID, cloud prefetch, and I/O task scheduling on a Linux platform. The results show that HintStor can execute and evaluate various I/O access hints under different scenarios with minor modifications to the kernel and applications.  © 2022 Association for Computing Machinery.",block storage; data management; heterogeneous storage systems; I/O access hints,Computer operating systems; Semantics; Storage management; Virtual storage; Application performance; Block storage; Filesystem; Generic frameworks; Heterogeneous storage system; I/O access hint; Semantic gap; Storage layers; Storage systems; Upper layer; Information management
RACE: One-sided RDMA-conscious Extendible Hashing,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130239015&doi=10.1145%2f3511895&partnerID=40&md5=0f2443a2ced595205070151cd482e06b,"Memory disaggregation is a promising technique in datacenters with the benefit of improving resource utilization, failure isolation, and elasticity. Hashing indexes have been widely used to provide fast lookup services in distributed memory systems. However, traditional hashing indexes become inefficient for disaggregated memory, since the computing power in the memory pool is too weak to execute complex index requests. To provide efficient indexing services in disaggregated memory scenarios, this article proposes RACE hashing, a one-sided RDMA-Conscious Extendible hashing index with lock-free remote concurrency control and efficient remote resizing. RACE hashing enables all index operations to be efficiently executed by using only one-sided RDMA verbs without involving any compute resource in the memory pool. To support remote concurrent access with high performance, RACE hashing leverages a lock-free remote concurrency control scheme to enable different clients to concurrently operate the same hashing index in the memory pool in a lock-free manner. To resize the hash table with low overheads, RACE hashing leverages an extendible remote resizing scheme to reduce extra RDMA accesses caused by extendible resizing and allow concurrent request execution during resizing. Extensive experimental results demonstrate that RACE hashing outperforms state-of-the-art distributed in-memory hashing indexes by 1.4-13.7× in YCSB hybrid workloads.  © 2022 Association for Computing Machinery.",Disaggregated data center; hashing index structure; remote direct memory access,Lakes; Memory architecture; Datacenter; Dis-aggregated memory; Disaggregated data center; Disaggregation; Hashing index structure; Index structure; Lock-free; Memory pool; Remote direct memory access; Resources utilizations; Concurrency control
IS-HBase: An In-Storage Computing Optimized HBase with I/O Offloading and Self-Adaptive Caching in Compute-Storage Disaggregated Infrastructure,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130259225&doi=10.1145%2f3488368&partnerID=40&md5=2d6c0f61efcc33ad1dbef73ab700d951,"Active storage devices and in-storage computing are proposed and developed in recent years to effectively reduce the amount of required data traffic and to improve the overall application performance. They are especially preferred in the compute-storage disaggregated infrastructure. In both techniques, a simple computing module is added to storage devices/servers such that some stored data can be processed in the storage devices/servers before being transmitted to application servers. This can reduce the required network bandwidth and offload certain computing requirements from application servers to storage devices/servers. However, several challenges exist when designing an in-storage computing-based architecture for applications. These include what computing functions need to be offloaded, how to design the protocol between in-storage modules and application servers, and how to deal with the caching issue in application servers.HBase is an important and widely used distributed Key-Value Store. It stores and indexes key-value pairs in large files in a storage system like HDFS. However, its performance especially read performance, is impacted by the heavy traffics between HBase RegionServers and storage servers in the compute-storage disaggregated infrastructure when the available network bandwidth is limited. We propose an In-Storage-based HBase architecture, called IS-HBase, to improve the overall performance and to address the aforementioned challenges. First, IS-HBase executes a data pre-processing module (In-Storage ScanNer, called ISSN) for some read queries and returns the requested key-value pairs to RegionServers instead of returning data blocks in HFile. IS-HBase carries out compactions in storage servers to reduce the large amount of data being transmitted through the network and thus the compaction execution time is effectively reduced. Second, a set of new protocols is proposed to address the communication and coordination between HBase RegionServers at computing nodes and ISSNs at storage nodes. Third, a new self-adaptive caching scheme is proposed to better serve the read queries with fewer I/O operations and less network traffic. According to our experiments, the IS-HBase can reduce up to 97% network traffic for read queries and the throughput (queries per second) is significantly less affected by the fluctuation of available network bandwidth. The execution time of compaction in IS-HBase is only about 6.31%-41.84% of the execution time of legacy HBase. In general, IS-HBase demonstrates the potential of adopting in-storage computing for other data-intensive distributed applications to significantly improve performance in compute-storage disaggregated infrastructure.  © 2022 Association for Computing Machinery.",caching; compute-storage disaggregated infrastructure; database; HBase; In-storage computing; key-value store; performance improvement,Compaction; Network architecture; Virtual storage; Application Servers; Caching; Compute-storage disaggregated infrastructure; Device servers; Hbase; In-storage computing; Key-value stores; Network bandwidth; Performance; Performance improvement; Bandwidth
Power-optimized Deployment of Key-value Stores Using Storage Class Memory,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130305179&doi=10.1145%2f3511905&partnerID=40&md5=4da1cd58c730d0f6ccc5a4087ac0089d,"High-performance flash-based key-value stores in data-centers utilize large amounts of DRAM to cache hot data. However, motivated by the high cost and power consumption of DRAM, server designs with lower DRAM-per-compute ratio are becoming popular. These low-cost servers enable scale-out services by reducing server workload densities. This results in improvements to overall service reliability, leading to a decrease in the total cost of ownership (TCO) for scalable workloads. Nevertheless, for key-value stores with large memory footprints, these reduced DRAM servers degrade performance due to an increase in both IO utilization and data access latency. In this scenario, a standard practice to improve performance for sharded databases is to reduce the number of shards per machine, which degrades the TCO benefits of reduced DRAM low-cost servers. In this work, we explore a practical solution to improve performance and reduce the costs and power consumption of key-value stores running on DRAM-constrained servers by using Storage Class Memories (SCM).SCMs in a DIMM form factor, although slower than DRAM, are sufficiently faster than flash when serving as a large extension to DRAM. With new technologies like Compute Express Link, we can expand the memory capacity of servers with high bandwidth and low latency connectivity with SCM. In this article, we use Intel Optane PMem 100 Series SCMs (DCPMM) in AppDirect mode to extend the available memory of our existing single-socket platform deployment of RocksDB (one of the largest key-value stores at Meta). We first designed a hybrid cache in RocksDB to harness both DRAM and SCM hierarchically. We then characterized the performance of the hybrid cache for three of the largest RocksDB use cases at Meta (ChatApp, BLOB Metadata, and Hive Cache). Our results demonstrate that we can achieve up to 80% improvement in throughput and 20% improvement in P95 latency over the existing small DRAM single-socket platform, while maintaining a 43-48% cost improvement over our large DRAM dual-socket platform. To the best of our knowledge, this is the first study of the DCPMM platform in a commercial data center.  © 2022 Copyright held by the owner/author(s).",data centers; Intel Optane Memory; Key value stores; optimization; persistent memory; RocksDB,Cache memory; Cost reduction; Data reduction; Electric power utilization; Flash memory; Datacenter; Intel optane memory; Key-value stores; Low cost servers; Optimisations; Performance; Persistent memory; Rocksdb; Storage-class memory; Total cost of ownership; Dynamic random access storage
"SmartFVM: A Fast, Flexible, and Scalable Hardware-based Virtualization for Commodity Storage Devices",2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130221559&doi=10.1145%2f3511213&partnerID=40&md5=ca267d349b37b7c924b4b57bdd104441,"A computational storage device incorporating a computation unit inside or near its storage unit is a highly promising technology to maximize a storage server's performance. However, to apply such computational storage devices and take their full potential in virtualized environments, server architects must resolve a fundamental challenge: Cost-effective virtualization. This critical challenge can be directly addressed by the following questions: (1) how to virtualize two different hardware units (i.e., computation and storage), and (2) how to integrate them to construct virtual computational storage devices, and (3) how to provide them to users. However, the existing methods for computational storage virtualization severely suffer from their low performance and high costs due to the lack of hardware-assisted virtualization support.In this work, we propose SmartFVM-Engine, an FPGA card designed to maximize the performance and cost-effectiveness of computational storage virtualization. SmartFVM-Engine introduces three key ideas to achieve the design goals. First, it achieves high virtualization performance by applying hardware-assisted virtualization to both computation and storage units. Second, it further improves the performance by applying hardware-assisted resource orchestration for the virtualized units. Third, it achieves high cost-effectiveness by dynamically constructing and scheduling virtual computational storage devices. To the best of our knowledge, this is the first work to implement a hardware-assisted virtualization mechanism for modern computational storage devices.  © 2022 Association for Computing Machinery.",Computational storage,Cost effectiveness; Engines; Virtual reality; Virtualization; Computational storage; Hardware-assisted; High costs; Performance; Scalable hardware; Server performance; Storage servers; Storage units; Storage virtualization; Virtualizations; Virtual storage
Optimizing Storage Performance with Calibrated Interrupts,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126396426&doi=10.1145%2f3505139&partnerID=40&md5=4fd5269c087e76ae25eafe75e8522878,"After request completion, an I/O device must decide whether to minimize latency by immediately firing an interrupt or to optimize for throughput by delaying the interrupt, anticipating that more requests will complete soon and help amortize the interrupt cost. Devices employ adaptive interrupt coalescing heuristics that try to balance between these opposing goals. Unfortunately, because devices lack the semantic information about which I/O requests are latency-sensitive, these heuristics can sometimes lead to disastrous results.Instead, we propose addressing the root cause of the heuristics problem by allowing software to explicitly specify to the device if submitted requests are latency-sensitive. The device then ""calibrates""its interrupts to completions of latency-sensitive requests. We focus on NVMe storage devices and show that it is natural to express these semantics in the kernel and the application and only requires a modest two-bit change to the device interface. Calibrated interrupts increase throughput by up to 35%, reduce CPU consumption by as much as 30%, and achieve up to 37% lower latency when interrupts are coalesced.  © 2022 Association for Computing Machinery.",calibrating interrupts; hardware interrupts; NVMe,Flocculation; Optimization; Virtual storage; Calibrating interrupt; Device interfaces; Hardware interrupt; Heuristic problems; Interrupt coalescing; Low latency; NVMe; Root cause; Semantics Information; Storage performance; Semantics
Exploration and Exploitation for Buffer-Controlled HDD-Writes for SSD-HDD Hybrid Storage Server,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126390733&doi=10.1145%2f3465410&partnerID=40&md5=eaedcd7b3892a34846bcd974754c3c9c,"Hybrid storage servers combining solid-state drives (SSDs) and hard-drive disks (HDDs) provide cost-effectiveness and μs-level responsiveness for applications. However, observations from cloud storage system Pangu manifest that HDDs are often underutilized while SSDs are overused, especially under intensive writes. It leads to fast wear-out and high tail latency to SSDs. On the other hand, our experimental study reveals that a series of sequential and continuous writes to HDDs exhibit a periodic, staircase-shaped pattern of write latency, i.e., low (e.g., 35 μs), middle (e.g., 55 μs), and high latency (e.g., 12 ms), resulting from buffered writes within HDD's controller. It inspires us to explore and exploit the potential μs-level IO delay of HDDs to absorb excessive SSD writes without performance degradation.We first build an HDD writing model for describing the staircase behavior and design a profiling process to initialize and dynamically recalibrate the model parameters. Then, we propose a Buffer-Controlled Write approach (BCW) to proactively control buffered writes so that low- and mid-latency periods are scheduled with application data and high-latency periods are filled with padded data. Leveraging BCW, we design a mixed IO scheduler (MIOS) to adaptively steer incoming data to SSDs and HDDs. A multi-HDD scheduling is further designed to minimize HDD-write latency. We perform extensive evaluations under production workloads and benchmarks. The results show that MIOS removes up to 93% amount of data written to SSDs, reduces average and 99th-percentile latencies of the hybrid server by 65% and 85%, respectively.  © 2022 Association for Computing Machinery.",Hybrid storage; IO scheduling; tail latency,Buffer storage; Cost effectiveness; Stairs; Buffered writes; Cloud storage systems; Exploration and exploitation; Hybrid storages; IO scheduler; IO scheduling; Performance degradation; Shaped pattern; Storage servers; Tail latency; Scheduling
The Concurrent Learned Indexes for Multicore Data Storage,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126391795&doi=10.1145%2f3478289&partnerID=40&md5=bb66e129d0d49e6a8d4486ba7d883ed4,"We present XIndex, which is a concurrent index library and designed for fast queries. It includes a concurrent ordered index (XIndex-R) and a concurrent hash index (XIndex-H). Similar to a recent proposal of the learned index, the indexes in XIndex use learned models to optimize index efficiency. Compared with the learned index, for the ordered index, XIndex-R is able to handle concurrent writes effectively and adapts its structure according to runtime workload characteristics. For the hash index, XIndex-H is able to avoid the resize operation blocking concurrent writes. Furthermore, the indexes in XIndex can index string keys much more efficiently than the learned index. We demonstrate the advantages of XIndex with YCSB, TPC-C (KV), which is a TPC-C-inspired benchmark for key-value stores, and micro-benchmarks. Compared with ordered indexes of Masstree and Wormhole, XIndex-R achieves up to 3.2× and 4.4× performance improvement on a 24-core machine. Compared with hash indexes of Intel TBB HashMap, XIndex-H achieves up to 3.1× speedup. The performance further improves by 91% after adding the optimizations on indexing string keys. The library is open-sourced.1  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",concurrent algorithms; indexing; Learned indexes,Indexing (of information); Blockings; Concurrent algorithms; Data storage; Fast query; Indexing; Learned index; Multi-cores; Performance; Runtimes; Workload characteristics; Digital storage
Pattern-Based Prefetching with Adaptive Cache Management Inside of Solid-State Drives,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126396226&doi=10.1145%2f3474393&partnerID=40&md5=a79507d61122a66fa129a733a4d62051,"This article proposes a pattern-based prefetching scheme with the support of adaptive cache management, at the flash translation layer of solid-state drives (SSDs). It works inside of SSDs and has features of OS dependence and uses transparency. Specifically, it first mines frequent block access patterns that reflect the correlation among the occurred I/O requests. Then, it compares the requests in the current time window with the identified patterns to direct prefetching data into the cache of SSDs. More importantly, to maximize the cache use efficiency, we build a mathematical model to adaptively determine the cache partition on the basis of I/O workload characteristics, for separately buffering the prefetched data and the written data. Experimental results show that our proposal can yield improvements on average read latency by 1.8%-36.5% without noticeably increasing the write latency, in contrast to conventional SSD-inside prefetching schemes.  © 2022 Association for Computing Machinery.",adaptive cache management; frequent access pattern; I/O time; prefetching; SSD cache; SSDs,Access patterns; Adaptive cache management; Cache management; Flash translation layer; Frequent access patterns; I/O time; Pre-fetching scheme; Prefetching; Solid-state drive; Solid-state drive cache; Flash-based SSDs
Reprogramming 3D TLC Flash Memory based Solid State Drives,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126394561&doi=10.1145%2f3487064&partnerID=40&md5=bbd417b3fde3c6a890d042c97289404b,"NAND flash memory-based SSDs have been widely adopted. The scaling of SSD has evolved from plannar (2D) to 3D stacking. For reliability and other reasons, the technology node in 3D NAND SSD is larger than in 2D, but data density can be increased via increasing bit-per-cell. In this work, we develop a novel reprogramming scheme for TLCs in 3D NAND SSD, such that a cell can be programmed and reprogrammed several times before it is erased. Such reprogramming can improve the endurance of a cell and the speed of programming, and increase the amount of bits written in a cell per program/erase cycle, i.e., effective capacity. Our work is the first to perform a real 3D NAND SSD test to validate the feasibility of the reprogram operation. From the collected data, we derive the restrictions of performing reprogramming due to reliability challenges. Furthermore, a reprogrammable SSD (ReSSD) is designed to structure reprogram operations. ReSSD is evaluated in a case study in RAID 5 system (RSS-RAID). Experimental results show that RSS-RAID can improve the endurance by 35.7%, boost write performance by 15.9%, and increase effective capacity by 7.71%, with negligible overhead compared with conventional 3D SSD-based RAID 5 system.  © 2022 Association for Computing Machinery.",3D TLC SSD; RAID 5; reliability; reprogramming,Cells; Cytology; Flash-based SSDs; Memory architecture; Reliability; Three dimensional integrated circuits; 2D-To-3D; 3D stacking; 3d TLC SSD; Effective capacity; NAND flash memory; RAID 5; Reprogrammable; Reprogramming; Scalings; Technology nodes; NAND circuits
Introduction to the Special Section on USENIX OSDI 2021,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126466133&doi=10.1145%2f3507950&partnerID=40&md5=97cfc50943d1619ef1b23f02a0aab4ba,[No abstract available],,
Survey of Distributed File System Design Choices,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126394693&doi=10.1145%2f3465405&partnerID=40&md5=a6d4e54a2abf3fd753599ac942e43238,"Decades of research on distributed file systems and storage systems exists. New researchers and engineers have a lot of literature to study, but only a comparatively small number of high-level design choices are available when creating a distributed file system. And within each aspect of the system, typically several common approaches are used. So, rather than surveying distributed file systems, this article presents a survey of important design decisions and, within those decisions, the most commonly used options. It also presents a qualitative exploration of their tradeoffs. We include several relatively recent designs and their variations that illustrate other tradeoff choices in the design space, despite being underexplored. In doing so, we provide a primer on distributed file systems, and we also show areas that are overexplored and underexplored, in the hopes of inspiring new research.  © 2022 Copyright held by the owner/author(s).",design options; Distributed file systems; distributed storage; survey; taxonomy,File organization; Design decisions; Design option; Design spaces; Distributed file systems; Distributed storage; File storage; High-level design; Qualitative exploration; Storage systems; Surveys
Nap: Persistent Memory Indexes for NUMA Architectures,2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126391800&doi=10.1145%2f3507922&partnerID=40&md5=dd2b1be4bdedf4514169b6b89b0f2546,"We present Nap, a black-box approach that converts concurrent persistent memory (PM) indexes into non-uniform memory access (NUMA)-aware counterparts. Based on the observation that real-world workloads always feature skewed access patterns, Nap introduces a NUMA-aware layer (NAL) on the top of existing concurrent PM indexes, and steers accesses to hot items to this layer. The NAL maintains (1) per-node partial views in PM for serving insert/update/delete operations with failure atomicity and (2) a global view in DRAM for serving lookup operations. The NAL eliminates remote PM accesses to hot items without inducing extra local PM accesses. Moreover, to handle dynamic workloads, Nap adopts a fast NAL switch mechanism. We convert five state-of-the-art PM indexes using Nap. Evaluation on a four-node machine with Optane DC Persistent Memory shows that Nap can improve the throughput by up to 2.3× and 1.56× under write-intensive and read-intensive workloads, respectively.  © 2022 Association for Computing Machinery.",indexes; non-uniform memory access; Persistent memory,Dynamic random access storage; Black box approach; Global view; Index; Memory access; Memory index; Nonuniform memory access; Partial views; Persistent memory; Real-world; Skewed access patterns; Memory architecture
"RAIL: Predictable, Low Tail Latency for NVMe Flash",2022,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108416026&doi=10.1145%2f3465406&partnerID=40&md5=bd97ab0190c1a6239d7516553f8b3749,"Flash-based storage is replacing disk for an increasing number of data center applications, providing orders of magnitude higher throughput and lower average latency. However, applications also require predictable storage latency. Existing Flash devices fail to provide low tail read latency in the presence of write operations. We propose two novel techniques to address SSD read tail latency, including Redundant Array of Independent LUNs (RAIL) which avoids serialization of reads behind user writes as well as latency-aware hot-cold separation (HC) which improves write throughput while maintaining low tail latency. RAIL leverages the internal parallelism of modern Flash devices and allocates data and parity pages to avoid reads getting stuck behind writes. We implement RAIL in the Linux Kernel as part of the LightNVM Flash translation layer and show that it can reduce read tail latency by 7× at the 99.99th percentile, while reducing relative bandwidth by only 33%.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",LightNVM; Linux; OpenChannel; SSD,Datacenter; Flash devices; High-low; High-throughput; Lightnvm; Lower average; Number of datum; Open channels; Orders of magnitude; SSD; Linux
Octopus+: An RDMA-Enabled Distributed Persistent Memory File System,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122568158&doi=10.1145%2f3448418&partnerID=40&md5=7205886bdde663a2fd0b401c630c0850,"Non-volatile memory and remote direct memory access (RDMA) provide extremely high performance in storage and network hardware. However, existing distributed file systems strictly isolate file system and network layers, and the heavy layered software designs leave high-speed hardware under-exploited. In this article, we propose an RDMA-enabled distributed persistent memory file system, Octopus+, to redesign file system internal mechanisms by closely coupling non-volatile memory and RDMA features. For data operations, Octopus+ directly accesses a shared persistent memory pool to reduce memory copying overhead, and actively fetches and pushes data all in clients to rebalance the load between the server and network. For metadata operations, Octopus+ introduces self-identified remote procedure calls for immediate notification between file systems and networking, and an efficient distributed transaction mechanism for consistency. Octopus+ is enabled with replication feature to provide better availability. Evaluations on Intel Optane DC Persistent Memory Modules show that Octopus+ achieves nearly the raw bandwidth for large I/Os and orders of magnitude better performance than existing distributed file systems.  © 2021 Association for Computing Machinery.",distributed system; persistent memory; remote direct memory access; Storage system,File organization; Network layers; Shellfish; Software design; Access features; Distributed file systems; Distributed systems; Filesystem; High Speed; Layered softwares; Performance; Persistent memory; Remote direct memory access; Storage systems; Molluscs
Multi-objective Optimization of Data Placement in a Storage-as-a-Service Federated Cloud,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122595893&doi=10.1145%2f3452741&partnerID=40&md5=13fc1b30ece7618cfd4b8b104cd17a0d,"Cloud federation enables service providers to collaborate to provide better services to customers. For cloud storage services, optimizing customer object placement for a member of a federation is a real challenge. Storage, migration, and latency costs need to be considered. These costs are contradictory in some cases. In this article, we modeled object placement as a multi-objective optimization problem. The proposed model takes into account parameters related to the local infrastructure, the federated environment, customer workloads, and their SLAs. For resolving this problem, we propose CDP-NSGAIIIR, a Constraint Data Placement matheuristic based on NSGAII with Injection and Repair functions. The injection function aims to enhance the solutions' quality. It consists to calculate some solutions using an exact method then inject them into the initial population of NSGAII. The repair function ensures that the solutions obey the problem constraints and so prevents from exploring large sets of unfeasible solutions. It reduces drastically the execution time of NSGAII. Experimental results show that the injection function improves the HV of NSGAII and the exact method by up to 94% and 60%, respectively, while the repair function reduces the execution time by an average of 68%.  © 2021 Association for Computing Machinery.",cloud; cloud federation; Data placement; NSGAII; optimization,Digital storage; Multiobjective optimization; Cloud federations; Data placement; Exact methods; Federated clouds; Multi-objectives optimization; NSGA-II; Object placement; Optimisations; Repair functions; Service provider; Sales
GoSeed: Optimal Seeding Plan for Deduplicated Storage,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122582518&partnerID=40&md5=8cb64bb8250c0a13fef6996ec6df7a4f,"Deduplication decreases the physical occupancy of files in a storage volume by removing duplicate copies of data chunks, but creates data-sharing dependencies that complicate standard storage management tasks. Specifically, data migration plans must consider the dependencies between files that are remapped to new volumes and files that are not. Thus far, only greedy approaches have been suggested for constructing such plans, and it is unclear how they compare to one another and how much they can be improved. We set to bridge this gap for seeding - migration in which the target volume is initially empty. We prove that even this basic instance of data migration is NP-hard in the presence of deduplication. We then present GoSeed, a formulation of seeding as an integer linear programming (ILP) problem, and three acceleration methods for applying it to real-sized storage volumes. Our experimental evaluation shows that, while the greedy approaches perform well on ""easy""problem instances, the cost of their solution can be significantly higher than that of GoSeed's solution on ""hard""instances, for which they are sometimes unable to find a solution at all.  © 2021 Association for Computing Machinery.",capacity planning; data migration; Deduplication,Digital storage; Information management; Integer programming; Petroleum reservoir evaluation; Capacity planning; Data chunks; Data Sharing; Data-migration; Deduplication; Greedy approaches; Management tasks; NP-hard; Storage volumes; Target volumes; Storage management
Reparo: A Fast RAID Recovery Scheme for Ultra-large SSDs,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122597968&doi=10.1145%2f3450977&partnerID=40&md5=88e6ea61b759a699d8c85d531a982d3b,"A recent ultra-large SSD (e.g., a 32-TB SSD) provides many benefits in building cost-efficient enterprise storage systems. Owing to its large capacity, however, when such SSDs fail in a RAID storage system, a long rebuild overhead is inevitable for RAID reconstruction that requires a huge amount of data copies among SSDs. Motivated by modern SSD failure characteristics, we propose a new recovery scheme, called reparo, for a RAID storage system with ultra-large SSDs. Unlike existing RAID recovery schemes, reparo repairs a failed SSD at the NAND die granularity without replacing it with a new SSD, thus avoiding most of the inter-SSD data copies during a RAID recovery step. When a NAND die of an SSD fails, reparo exploits a multi-core processor of the SSD controller in identifying failed LBAs from the failed NAND die and recovering data from the failed LBAs. Furthermore, reparo ensures no negative post-recovery impact on the performance and lifetime of the repaired SSD. Experimental results using 32-TB enterprise SSDs show that reparo can recover from a NAND die failure about 57 times faster than the existing rebuild method while little degradation on the SSD performance and lifetime is observed after recovery.  © 2021 Association for Computing Machinery.",Die failure; RAID; storage system; ultra-large SSD,Computer system recovery; Dies; Memory architecture; NAND circuits; Building costs; Cost-efficient; Die failure; In-buildings; Performance; RAID; RAID storage; Recovery scheme; Storage systems; Ultra-large SSD; Recovery
Introduction to the Special Section on USENIX OSDI 2020,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122561019&doi=10.1145%2f3479434&partnerID=40&md5=e8ae94a49a712b18382b15b8ec62fa67,[No abstract available],,
NVLSM: A Persistent Memory Key-Value Store Using Log-Structured Merge Tree with Accumulative Compaction,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122558381&doi=10.1145%2f3453300&partnerID=40&md5=97135eb57383d01bda71a775a4066839,"Computer systems utilizing byte-addressable Non-Volatile Memory (NVM) as memory/storage can provide low-latency data persistence. The widely used key-value stores using Log-Structured Merge Tree (LSM-Tree) are still beneficial for NVM systems in aspects of the space and write efficiency. However, the significant write amplification introduced by the leveled compaction of LSM-Tree degrades the write performance of the key-value store and shortens the lifetime of the NVM devices. The existing studies propose new compaction methods to reduce write amplification. Unfortunately, they result in a relatively large read amplification. In this article, we propose NVLSM, a key-value store for NVM systems using LSM-Tree with new accumulative compaction. By fully utilizing the byte-addressability of NVM, accumulative compaction uses pointers to accumulate data into multiple floors in a logically sorted run to reduce the number of compactions required. We have also proposed a cascading searching scheme for reads among the multiple floors to reduce read amplification. Therefore, NVLSM reduces write amplification with small increases in read amplification. We compare NVLSM with key-value stores using LSM-Tree with two other compaction methods: leveled compaction and fragmented compaction. Our evaluations show that NVLSM reduces write amplification by up to 67% compared with LSM-Tree using leveled compaction without significantly increasing the read amplification. In write-intensive workloads, NVLSM reduces the average latency by 15.73%-41.2% compared to other key-value stores.  © 2021 Association for Computing Machinery.",key-value store; log-structured merge tree; Non-volatile memory,Floors; Forestry; Nonvolatile storage; Compaction methods; Data persistence; Key-value stores; Log structured merge trees; Low latency; Memory keys; Memory storage; Persistent memory; Read amplifications; Write amplifications; Compaction
XStore: Fast RDMA-Based Ordered Key-Value Store Using Remote Learned Cache,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122591282&doi=10.1145%2f3468520&partnerID=40&md5=ff39bf59eb2b80635196446b2d544da9,"RDMA (Remote Direct Memory Access) has gained considerable interests in network-attached in-memory key-value stores. However, traversing the remote tree-based index in ordered key-value stores with RDMA becomes a critical obstacle, causing an order-of-magnitude slowdown and limited scalability due to multiple round trips. Using index cache with conventional wisdom - caching partial data and traversing them locally - usually leads to limited effect because of unavoidable capacity misses, massive random accesses, and costly cache invalidations. We argue that the machine learning (ML) model is a perfect cache structure for the tree-based index, termed learned cache. Based on it, we design and implement XStore, an RDMA-based ordered key-value store with a new hybrid architecture that retains a tree-based index at the server to perform dynamic workloads (e.g., inserts) and leverages a learned cache at the client to perform static workloads (e.g., gets and scans). The key idea is to decouple ML model retraining from index updating by maintaining a layer of indirection from logical to actual positions of key-value pairs. It allows a stale learned cache to continue predicting a correct position for a lookup key. XStore ensures correctness using a validation mechanism with a fallback path and further uses speculative execution to minimize the cost of cache misses. Evaluations with YCSB benchmarks and production workloads show that a single XStore server can achieve over 80 million read-only requests per second. This number outperforms state-of-the-art RDMA-based ordered key-value stores (namely, DrTM-Tree, Cell, and eRPC+Masstree) by up to 5.9× (from 3.7×). For workloads with inserts, XStore still provides up to 3.5× (from 2.7×) throughput speedup, achieving 53M reqs/s. The learned cache can also reduce client-side memory usage and further provides an efficient memory-performance tradeoff, e.g., saving 99% memory at the cost of 20% peak throughput.  © 2021 Association for Computing Machinery.",index caching; machine learning model; RDMA-based key-value store; tree-based index structure,Benchmarking; Cache memory; Machine learning; Memory architecture; In networks; Index caching; Index structure; Key-value stores; Machine learning models; Memory keys; Remote direct memory access; Remote direct memory access-based key-value store; Tree-based; Tree-based index structure; Throughput
A Large-scale Analysis of Hundreds of In-memory Key-value Cache Clusters at Twitter,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119098024&doi=10.1145%2f3468521&partnerID=40&md5=5ca04e5241458381a14dddec926a76d0,"Modern web services use in-memory caching extensively to increase throughput and reduce latency. There have been several workload analyses of production systems that have fueled research in improving the effectiveness of in-memory caching systems. However, the coverage is still sparse considering the wide spectrum of industrial cache use cases. In this work, we significantly further the understanding of real-world cache workloads by collecting production traces from 153 in-memory cache clusters at Twitter, sifting through over 80 TB of data, and sometimes interpreting the workloads in the context of the business logic behind them. We perform a comprehensive analysis to characterize cache workloads based on traffic pattern, time-to-live (TTL), popularity distribution, and size distribution. A fine-grained view of different workloads uncover the diversity of use cases: many are far more write-heavy or more skewed than previously shown and some display unique temporal patterns. We also observe that TTL is an important and sometimes defining parameter of cache working sets. Our simulations show that ideal replacement strategy in production caches can be surprising, for example, FIFO works the best for a large number of workloads.  © 2021 Association for Computing Machinery.",Cache; datasets; in-memory key-value cache; key-value store; Twitter; workload analysis,Cache memory; Web services; Cache; Cache cluster; Dataset; In-memory key-value cache; Key values; Key-value stores; Memory caching; Memory keys; Time-to-live; Workload analysis; Social networking (online)
Toward Virtual Machine Image Management for Persistent Memory,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122569456&doi=10.1145%2f3450976&partnerID=40&md5=f5b2740138331c77ddd2b4357bbe4657,"Persistent memory's (PM) byte-addressability and high capacity will also make it emerging for virtualized environment. Modern virtual machine monitors virtualize PM using either I/O virtualization or memory virtualization. However, I/O virtualization will sacrifice PM's byte-addressability, and memory virtualization does not get the chance of PM image management. In this article, we enhance QEMU's memory virtualization mechanism. The enhanced system can achieve both PM's byte-addressability inside virtual machines and PM image management outside the virtual machines. We also design pcow, a virtual machine image format for PM, which is compatible with our enhanced memory virtualization and supports storage virtualization features including thin-provisioning, base image, snapshot, and striping. Address translation is performed with the help of the Extended Page Table, thus much faster than image formats implemented in I/O virtualization. We also optimize pcow considering PM's characteristics. We perform exhaustive performance evaluations on an x86 server equipping with Intel's Optane DC persistent memory. The evaluation demonstrates that our scheme boosts the overall performance by up to 50× compared with qcow2, an image format implemented in I/O virtualization, and brings almost no performance overhead compared with the native memory virtualization. The striping feature can also scale-out the virtual PM's bandwidth performance.  © 2021 Association for Computing Machinery.",memory virtualization; Persistent memory,Image enhancement; Network security; Virtual addresses; Virtual machine; Virtual reality; High capacity; High-capacity; Image format; Image management; Memory virtualization; Performance; Persistent memory; Virtual machine image; Virtualizations; Virtualized environment; Virtualization
Introduction to the Special Issue on USENIX ATC 2020,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108084203&doi=10.1145%2f3457170&partnerID=40&md5=f975b7281708de9f4d0bdd752301ea67,[No abstract available],,
Can Applications Recover from fsync Failures?,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108073396&doi=10.1145%2f3450338&partnerID=40&md5=e4bd56ebe3f983d2569961480173e519,"We analyze how file systems and modern data-intensive applications react to fsync failures. First, we characterize how three Linux file systems (ext4, XFS, Btrfs) behave in the presence of failures. We find commonalities across file systems (pages are always marked clean, certain block writes always lead to unavailability) as well as differences (page content and failure reporting is varied). Next, we study how five widely used applications (PostgreSQL, LMDB, LevelDB, SQLite, Redis) handle fsync failures. Our findings show that although applications use many failure-handling strategies, none are sufficient: fsync failures can cause catastrophic outcomes such as data loss and corruption. Our findings have strong implications for the design of file systems and applications that intend to provide strong durability guarantees.  © 2021 held by the owner/author(s). Publication rights licensed to ACM.",Durability; file system; fsync; fsync failures; persistence,Computer operating systems; Data loss; Data-intensive application; Failure handling; File systems; Linux file system; PostgreSQL; File organization
Design of LSM-tree-based Key-value SSDs with Bounded Tails,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108121539&doi=10.1145%2f3452846&partnerID=40&md5=ac12d4473a9eb49f29c06e35df09035d,"Key-value store based on a log-structured merge-tree (LSM-tree) is preferable to hash-based key-value store, because an LSM-tree can support a wider variety of operations and show better performance, especially for writes. However, LSM-tree is difficult to implement in the resource constrained environment of a key-value SSD (KV-SSD), and, consequently, KV-SSDs typically use hash-based schemes. We present PinK, a design and implementation of an LSM-tree-based KV-SSD, which compared to a hash-based KV-SSD, reduces 99th percentile tail latency by 73%, improves average read latency by 42%, and shows 37% higher throughput. The key idea in improving the performance of an LSM-tree in a resource constrained environment is to avoid the use of Bloom filters and instead, use a small amount of DRAM to keep/pin the top levels of the LSM-tree. We also find that PinK is able to provide a flexible design space for a wide range of KV workloads by leveraging the read-write tradeoff in LSM-trees.  © 2021 Association for Computing Machinery.",key-value SSD; key-value store; Log-structured merge-tree; tail latency,Bloom filters; Design and implementations; Flexible designs; Key values; Key-value stores; Log structured merge trees; Read latencies; Tree-based; Data storage equipment
Twizzler: A Data-centric OS for Non-volatile Memory,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108080431&doi=10.1145%2f3454129&partnerID=40&md5=376f22a4371eaaf59b45ca6cbc71ae00,"Byte-addressable, non-volatile memory (NVM) presents an opportunity to rethink the entire system stack. We present Twizzler, an operating system redesign for this near-future. Twizzler removes the kernel from the I/O path, provides programs with memory-style access to persistent data using small (64 bit), object-relative cross-object pointers, and enables simple and efficient long-term sharing of data both between applications and between runs of an application. Twizzler provides a clean-slate programming model for persistent data, realizing the vision of Unix in a world of persistent RAM. We show that Twizzler is simpler, more extensible, and more secure than existing I/O models and implementations by building software for Twizzler and evaluating it on NVM DIMMs. Most persistent pointer operations in Twizzler impose less than 0.5 ns added latency. Twizzler operations are up to faster than Unix, and SQLite queries are up to faster than on PMDK. YCSB workloads ran 1.1-faster on Twizzler than on native and NVM-optimized SQLite backends.  © 2021 held by the owner/author(s). Publication rights licensed to ACM.",global address space; memory hierarchy; non-volatile memory; NVM; Persistent memory; PMEM; single-level store,Application programs; Nonvolatile storage; Random access storage; UNIX; Building softwares; Clean slates; Data centric; Entire system; Non-volatile memory; Programming models; System redesign; Data Sharing
Repair Pipelining for Erasure-coded Storage: Algorithms and Evaluation,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108111413&doi=10.1145%2f3436890&partnerID=40&md5=d146c80a599e7562c6aece1d30692e7f,"We propose repair pipelining, a technique that speeds up the repair performance in general erasure-coded storage. By carefully scheduling the repair of failed data in small-size units across storage nodes in a pipelined manner, repair pipelining reduces the single-block repair time to approximately the same as the normal read time for a single block in homogeneous environments. We further design different extensions of repair pipelining algorithms for heterogeneous environments and multi-block repair operations. We implement a repair pipelining prototype, called ECPipe, and integrate it as a middleware system into two versions of Hadoop Distributed File System (HDFS) (namely, HDFS-RAID and HDFS-3) as well as Quantcast File System. Experiments on a local testbed and Amazon EC2 show that repair pipelining significantly improves the performance of degraded reads and full-node recovery over existing repair techniques.  © 2021 Association for Computing Machinery.",distributed storage systems; Erasure coding,File organization; Middleware; Hadoop distributed file system (HDFS); Heterogeneous environments; Middleware system; Multi blocks; Node recovery; Repair operations; Repair techniques; Storage nodes; Digital storage
Penalty-and Locality-aware Memory Allocation in Redis Using Enhanced AET,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108122293&doi=10.1145%2f3447573&partnerID=40&md5=729c464918058c3beba96606595989de,"Due to large data volume and low latency requirements of modern web services, the use of an in-memory key-value (KV) cache often becomes an inevitable choice (e.g., Redis and Memcached). The in-memory cache holds hot data, reduces request latency, and alleviates the load on background databases. Inheriting from the traditional hardware cache design, many existing KV cache systems still use recency-based cache replacement algorithms, e.g., least recently used or its approximations. However, the diversity of miss penalty distinguishes a KV cache from a hardware cache. Inadequate consideration of penalty can substantially compromise space utilization and request service time. KV accesses also demonstrate locality, which needs to be coordinated with miss penalty to guide cache management. In this article, we first discuss how to enhance the existing cache model, the Average Eviction Time model, so that it can adapt to modeling a KV cache. After that, we apply the model to Redis and propose pRedis, Penalty-and Locality-aware Memory Allocation in Redis, which synthesizes data locality and miss penalty, in a quantitative manner, to guide memory allocation and replacement in Redis. At the same time, we also explore the diurnal behavior of a KV store and exploit long-term reuse. We replace the original passive eviction mechanism with an automatic dump/load mechanism, to smooth the transition between access peaks and valleys. Our evaluation shows that pRedis effectively reduces the average and tail access latency with minimal time and space overhead. For both real-world and synthetic workloads, our approach delivers an average of 14.0%g 1/452.3% latency reduction over a state-of-the-art penalty-aware cache management scheme, Hyperbolic Caching (HC), and shows more quantitative predictability of performance. Moreover, we can obtain even lower average latency (1.1%g 1/45.5%) when dynamically switching policies between pRedis and HC.  © 2021 Association for Computing Machinery.",cache modeling; memory allocation; Penalty-and locality-aware,Approximation algorithms; Data reduction; Memory architecture; Web services; Cache management; Cache management schemes; Cache replacement algorithm; Large data volumes; Latency reduction; Least recently used; Space utilization; Synthetic workloads; Cache memory
Performance Modeling and Practical Use Cases for Black-Box SSDs,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108067357&doi=10.1145%2f3440022&partnerID=40&md5=7fd1a4fdab946d823e9f85655b0c0b5c,"Modern servers are actively deploying Solid-State Drives (SSDs) thanks to their high throughput and low latency. However, current server architects cannot achieve the full performance potential of commodity SSDs, as SSDs are complex devices designed for specific goals (e.g., latency, throughput, endurance, cost) with their internal mechanisms undisclosed to users. In this article, we propose SSDcheck, a novel SSD performance model to extract various internal mechanisms and predict the latency of next access to commodity black-box SSDs. We identify key performance-critical features (e.g., garbage collection, write buffering) and find their parameters (i.e., size, threshold) from each SSD by using our novel diagnosis code snippets. Then, SSDcheck constructs a performance model for a target SSD and dynamically manages the model to predict the latency of the next access. In addition, SSDcheck extracts and provides other useful internal mechanisms (e.g., fetch unit in multi-queue SSDs, background tasks triggering idle-time interval) for the storage system to fully exploit SSDs. By using those useful features and the performance model, we propose multiple practical use cases. Our evaluations show that SSDcheck's performance model is highly accurate, and proposed use cases achieve significant performance improvement in various scenarios.  © 2021 Association for Computing Machinery.",black-box SSDs; Performance modeling; storage system,Background tasks; Complex devices; Critical features; Garbage collection; High throughput; Performance Model; Performance potentials; Solid state drives; Data storage equipment
Introduction to the special section on USENIX Fast 2020,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100382423&doi=10.1145%2f3442685&partnerID=40&md5=17da25a09d86885f4a2843cb667103d6,[No abstract available],,
Kreon: An efficient memory-mapped key-value store for flash storage,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100406957&doi=10.1145%2f3418414&partnerID=40&md5=768cce97cf478923100f6ff38066908a,"Persistent key-value stores have emerged as a main component in the data access path of modern data processing systems. However, they exhibit high CPU and I/O overhead. Nowadays, due to power limitations, it is important to reduce CPU overheads for data processing. In this article, we propose Kreon, a key-value store that targets servers with flash-based storage, where CPU overhead and I/O amplification are more significant bottlenecks compared to I/O randomness. We first observe that two significant sources of overhead in key-value stores are: (a) The use of compaction in Log-Structured Merge-Trees (LSM-Tree) that constantly perform merging and sorting of large data segments and (b) the use of an I/O cache to access devices, which incurs overhead even for data that reside in memory. To avoid these, Kreon performs data movement from level to level by using partial reorganization instead of full data reorganization via the use of a full index per-level. Kreon uses memory-mapped I/O via a custom kernel path to avoid a user-space cache. For a large dataset, Kreon reduces CPU cycles/op by up to 5.8×, reduces I/O amplification for inserts by up to 4.61×, and increases insert ops/s by up to 5.3×, compared to RocksDB. © 2021 Association for Computing Machinery. All rights reserved.",Copy-on-write; Key-value stores; LSM-tree; Memory-mapped I/O,Data handling; Flash memory; Forestry; Large dataset; Trees (mathematics); Access devices; Data movements; Data processing systems; Data reorganization; Flash storage; Key-value stores; Log structured merge trees; Power limitations; Cache memory
Copy-on-abundant-write for NiMble file system clones,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100415454&doi=10.1145%2f3423495&partnerID=40&md5=61043e18fa28c1a190c41e0e4fb99a20,"Making logical copies, or clones, of files and directories is critical to many real-world applications and workflows, including backups, virtual machines, and containers. An ideal clone implementation meets the following performance goals: (1) creating the clone has low latency; (2) reads are fast in all versions (i.e., spatial locality is always maintained, even after modifications); (3) writes are fast in all versions; (4) the overall system is space efficient. Implementing a clone operation that realizes all four properties, which we call a nimble clone, is a long-standing open problem. This article describes nimble clones in B-ϵ-tree File System (BetrFS), an open-source, full-path-indexed, and write-optimized file system. The key observation behind our work is that standard copy-on-write heuristics can be too coarse to be space efficient, or too fine-grained to preserve locality. On the other hand, a write-optimized key-value store, such as a Bε-tree or an log-structured merge-tree (LSM)-tree, can decouple the logical application of updates from the granularity at which data is physically copied. In our write-optimized clone implementation, data sharing among clones is only broken when a clone has changed enough to warrant making a copy, a policy we call copy-on-abundant-write. We demonstrate that the algorithmic work needed to batch and amortize the cost of BetrFS clone operations does not Erode the performance advantages of baseline BetrFS; BetrFS performance even improves in a few cases. BetrFS cloning is efficient; for example, when using the clone operation for container creation, BetrFS outperforms a simple recursive copy by up to two orders-of-magnitude and outperforms file systems that have specialized Linux Containers (LXC) backends by 3-4×. © 2021 Association for Computing Machinery. All rights reserved.",B<sup>ε</sup>-trees; Clone; File system; Write optimization,Clone cells; Computer operating systems; Containers; Data Sharing; File organization; Forestry; Open systems; Optimization; Trees (mathematics); Copy on write; File systems; Key-value stores; Log structured merge trees; Open sources; Orders of magnitude; Space efficient; Spatial locality; Cloning
Strong and efficient consistency with consistency-aware durability,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100439566&doi=10.1145%2f3423138&partnerID=40&md5=aeb37456eb2108fd94a7aa299c006302,"We introduce consistency-aware durability or Cad, a new approach to durability in distributed storage that enables strong consistency while delivering high performance. We demonstrate the efficacy of this approach by designing cross-client monotonic reads, a novel and strong consistency property that provides monotonic reads across failures and sessions in leader-based systems; such a property can be particularly beneficial in geo-distributed and edge-computing scenarios. We build Orca, a modified version of ZooKeeper that implements Cad and cross-client monotonic reads. We experimentally show that Orca provides strong consistency while closely matching the performance of weakly consistent ZooKeeper. Compared to strongly consistent ZooKeeper, Orca provides significantly higher throughput (1.8-3.3×) and notably reduces latency, sometimes by an order of magnitude in geo-distributed settings. We also implement Cad in Redis and show that the performance benefits are similar to that of Cad's implementation in ZooKeeper. © 2021 Association for Computing Machinery. All rights reserved.",Consistency models; Durability; Persistence; Replication,Durability; Distributed storage; New approaches; Performance benefits; Strong consistency; Computer aided design
Reliability of SSDs in enterprise storage systems: A large-scale field study,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100444635&doi=10.1145%2f3423088&partnerID=40&md5=8bdeff94ee5f373722c27e77ad5d6462,"This article presents the first large-scale field study of NAND-based SSDs in enterprise storage systems (in contrast to drives in distributed data center storage systems). The study is based on a very comprehensive set of field data, covering 1.6 million SSDs of a major storage vendor (NetApp). The drives comprise three different manufacturers, 18 different models, 12 different capacities, and all major flash technologies (SLC, cMLC, eMLC, 3D-TLC). The data allows us to study a large number of factors that were not studied in prior works, including the effect of firmware versions, the reliability of TLC NAND, and the correlations between drives within a RAID system. This article presents our analysis, along with a number of practical implications derived from it. © 2021 Association for Computing Machinery. All rights reserved.",Enterprise storage systems; Field study; Reliability; Solid-state drives; Statistical analysis,Firmware; NAND circuits; Distributed data; Field data; Field studies; Flash technology; Number of factors; RAID systems; Storage systems; Digital storage
NVMM-oriented hierarchical persistent client caching for lustre,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100390138&doi=10.1145%2f3404190&partnerID=40&md5=e7267c03c6e3fb5ea7d83fecc7ad31a5,"In high-performance computing (HPC), data and metadata are stored on special server nodes and client applications access the servers' data and metadata through a network, which induces network latencies and resource contention. These server nodes are typically equipped with (slow) magnetic disks, while the client nodes store temporary data on fast SSDs or even on non-volatile main memory (NVMM). Therefore, the full potential of parallel file systems can only be reached if fast client side storage devices are included into the overall storage architecture. In this article, we propose an NVMM-based hierarchical persistent client cache for the Lustre file system (NVMM-LPCC for short). NVMM-LPCC implements two caching modes: a read and write mode (RW-NVMM-LPCC for short) and a read only mode (RO-NVMM-LPCC for short). NVMM-LPCC integrates with the Lustre Hierarchical Storage Management (HSM) solution and the Lustre layout lock mechanism to provide consistent persistent caching services for I/O applications running on client nodes, meanwhile maintaining a global unified namespace of the entire Lustre file system. The evaluation results presented in this article show that NVMM-LPCC can increase the average read throughput by up to 35.80 times and the average write throughput by up to 9.83 times compared with the native Lustre system, while providing excellent scalability. © 2021 Association for Computing Machinery. All rights reserved.",Direct access; Hierarchical storage management; Lustre; Non-volatile memory; Persistent caching,Metadata; Storage as a service (STaaS); Virtual storage; Client applications; Hierarchical storage management; High performance computing (HPC); Lustre file systems; Non-volatile main memory; Parallel file system; Resource contention; Storage architectures; Storage management
SSD-based workload characteristics and their performance implications,2021,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100383545&doi=10.1145%2f3423137&partnerID=40&md5=a6fcb607755c8845dcb2298215a5c51f,"Storage systems are designed and optimized relying on wisdom derived from analysis studies of file-system and block-level workloads. However, while SSDs are becoming a dominant building block in many storage systems, their design continues to build on knowledge derived from analysis targeted at hard disk optimization. Though still valuable, it does not cover important aspects relevant for SSD performance. In a sense, we are “searching under the streetlight,” possibly missing important opportunities for optimizing storage system design. We present the first I/O workload analysis designed with SSDs in mind. We characterize traces from four repositories and examine their “temperature” ranges, sensitivity to page size, and “logical locality.” We then take the first step towards correlating these characteristics with three standard performance metrics: write amplification, read amplification, and flash read costs. Our results show that SSD-specific characteristics strongly affect performance, often in surprising ways. © 2021 Association for Computing Machinery. All rights reserved.",I/O workload analysis; Locality; SSD; Workload characterization,Data storage equipment; Building blockes; Disk optimization; Read amplifications; Standard performance; Storage systems; Workload analysis; Workload characteristics; Write amplifications; Hard disk storage
Hybrid Codes: Flexible Erasure Codes with Optimized Recovery Performance,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096980845&doi=10.1145%2f3407193&partnerID=40&md5=34a73cf147648bfd0b5c73869935a52d,"Erasure codes are being extensively deployed in practical storage systems to prevent data loss with low redundancy. However, these codes require excessive disk I/Os and network traffic for recovering unavailable data. Among all erasure codes, Minimum Storage Regenerating (MSR) codes can achieve optimal repair bandwidth under the minimum storage during recovery, but some open issues remain to be addressed before applying them in real systems. Facing with the huge burden during recovery, erasure-coded storage systems need to be developed with high repair efficiency. Aiming at this goal, a new class of coding scheme is introduced - Hybrid Regenerating Codes (Hybrid-RC). The codes utilize the superiority of MSR codes to compute a subset of data blocks while some other parity blocks are used for reliability maintenance. As a result, our design is near-optimal with respect to storage and network traffic and shows great improvements in recovery performance.  © 2020 ACM.",Erasure coding,Computer system recovery; Forward error correction; Recovery; Repair; Coding scheme; Erasure codes; Network traffic; Recovery performance; Regenerating codes; Reliability maintenance; Repair efficiencies; Storage systems; Digital storage
Inspection and Characterization of App File Usage in Mobile Devices,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096974850&doi=10.1145%2f3404119&partnerID=40&md5=eaccfd07c64d965931ba391685920170,"While the computing power of mobile devices has been quickly evolving in recent years, the growth of mobile storage capacity is, however, relatively slower. A common problem shared by budget-phone users is that they frequently run out of storage space. This article conducts a deep inspection of file usage of mobile applications and their potential implications on user experience. Our major findings are as follows: First, mobile applications could rapidly consume storage space by creating temporary cache files, but these cache files quickly become obsolete after being re-used for a short period of time. Second, file access patterns of large files, especially executable files, appear highly sparse and random, and therefore large portions of file space are never visited. Third, file prefetching brings an excessive amount of file data into page cache but only a few prefetched data are actually used. The unnecessary memory pressure causes premature memory reclamation and prolongs application launching time. Through the feasibility study of two preliminary optimizations, we demonstrated a high potential to eliminate unnecessary storage and memory space consumption with a minimal impact on user experience.  © 2020 ACM.",file usage; i/O system; Measurements; memory space; storage space,Budget control; Digital storage; Mobile computing; Computing power; Deep inspection; Executable files; Feasibility studies; File access patterns; File pre-fetching; Memory pressure; Mobile applications; User experience
TH-DPMS: Design and Implementation of an RDMA-enabled Distributed Persistent Memory Storage System,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096989724&doi=10.1145%2f3412852&partnerID=40&md5=293d0a09fbef6dc7162737eb80a101fe,"The rapidly increasing data in recent years requires the datacenter infrastructure to store and process data with extremely high throughput and low latency. Fortunately, persistent memory (PM) and RDMA technologies bring new opportunities towards this goal. Both of them are capable of delivering more than 10 GB/s of bandwidth and sub-microsecond latency. However, our past experiences and recent studies show that it is non-trivial to build an efficient and distributed storage system with such new hardware. In this article, we design and implement TH-DPMS (<underline>T</underline>sing<underline>H</underline>ua <underline>D</underline>istributed <underline>P</underline>ersistent <underline>M</underline>emory <underline>S</underline>ystem) based on persistent memory and RDMA, which unifies the memory, file system, and key-value interface in a single system. TH-DPMS is designed based on a unified distributed persistent memory abstract, pDSM. pDSM acts as a generic layer to connect the PMs of different storage nodes via high-speed RDMA network and organizes them into a global shared address space. It provides the fundamental functionalities, including global address management, space management, fault tolerance, and crash consistency guarantees. Applications are enabled to access pDSM with a group of flexible and easy-to-use APIs by using either raw read/write interfaces or the transactional ones with ACID guarantees. Based on pDSM, we implement a distributed file system and a key-value store named pDFS and pDKVS, respectively. Together, they uphold TH-DPMS with high-performance, low-latency, and fault-tolerant data storage. We evaluate TH-DPMS with both micro-benchmarks and real-world memory-intensive workloads. Experimental results show that TH-DPMS is capable of delivering an aggregated bandwidth of 120 GB/s with 6 nodes. When processing memory-intensive workloads such as YCSB and Graph500, TH-DPMS improves the performance by one order of magnitude compared to existing systems and keeps consistent high efficiency when the workload size grows to multiple terabytes.  © 2020 ACM.",distributed system; persistent memory; remote direct memory access; Storage system,Application programming interfaces (API); Bandwidth; Fault tolerance; File organization; Multiprocessing systems; Address management; Design and implementations; Design and implements; Distributed file systems; Distributed storage system; Persistent memory; Shared address spaces; Space management; Digital storage
"Cost-effective, Energy-efficient, and Scalable Storage Computing for Large-scale AI Applications",2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096952782&doi=10.1145%2f3415580&partnerID=40&md5=7f9b620f73f52d5156f3d7e3205c0e72,"The growing volume of data produced continuously in the Cloud and at the Edge poses significant challenges for large-scale AI applications to extract and learn useful information from the data in a timely and efficient way. The goal of this article is to explore the use of computational storage to address such challenges by distributed near-data processing. We describe Newport, a high-performance and energy-efficient computational storage developed for realizing the full potential of in-storage processing. To the best of our knowledge, Newport is the first commodity SSD that can be configured to run a server-like operating system, greatly minimizing the effort for creating and maintaining applications running inside the storage. We analyze the benefits of using Newport by running complex AI applications such as image similarity search and object tracking on a large visual dataset. The results demonstrate that data-intensive AI workloads can be efficiently parallelized and offloaded, even to a small set of Newport drives with significant performance gains and energy savings. In addition, we introduce a comprehensive taxonomy of existing computational storage solutions together with a realistic cost analysis for high-volume production, giving a good big picture of the economic feasibility of the computational storage technology.  © 2020 ACM.",Computational storage; in-storage processing; neural network; object tracking; similarity search; solid-state drive,Cost benefit analysis; Cost effectiveness; Data handling; Energy efficiency; Image analysis; Large dataset; Object tracking; Storage as a service (STaaS); Economic feasibilities; Energy efficient; High-volume production; Image similarity; Performance Gain; Scalable storage; Storage solutions; Storage technology; Digital storage
Streaming Data Reorganization at Scale with DeltaFS Indexed Massive Directories,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096967349&doi=10.1145%2f3415581&partnerID=40&md5=eb130f286119a81e946b98e39f7bd3d5,"Complex storage stacks providing data compression, indexing, and analytics help leverage the massive amounts of data generated today to derive insights. It is challenging to perform this computation, however, while fully utilizing the underlying storage media. This is because, while storage servers with large core counts are widely available, single-core performance and memory bandwidth per core grow slower than the core count per die. Computational storage offers a promising solution to this problem by utilizing dedicated compute resources along the storage processing path. We present DeltaFS Indexed Massive Directories (IMDs), a new approach to computational storage. DeltaFS IMDs harvest available (i.e., not dedicated) compute, memory, and network resources on the compute nodes of an application to perform computation on data. We demonstrate the efficiency of DeltaFS IMDs by using them to dynamically reorganize the output of a real-world simulation application across 131,072 CPU cores. DeltaFS IMDs speed up reads by 1,740× while only slightly slowing down the writing of data during simulation I/O for in situ data processing.  © 2020 ACM.",computational storage; In situ processing,Data handling; In situ processing; Compute resources; Core performance; Memory bandwidths; Network resource; Processing paths; Real-world simulation; Storage servers; Streaming data; Digital storage
Introduction to the Special Section on Computational Storage,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096971897&doi=10.1145%2f3425305&partnerID=40&md5=027e4b09c45cdd7ac51727ea005143fa,[No abstract available],,
Bridging Storage Semantics Using Data Labels and Asynchronous I/O,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144366667&doi=10.1145%2f3415579&partnerID=40&md5=4094a7970539f6518a25eb3d784da319,"In the era of data-intensive computing, large-scale applications, in both scientific and the BigData communities, demonstrate unique I/O requirements leading to a proliferation of different storage devices and software stacks, many of which have conflicting requirements. Further, new hardware technologies and system designs create a hierarchical composition that may be ideal for computational storage operations. In this article, we investigate how to support a wide variety of conflicting I/O workloads under a single storage system. We introduce the idea of a Label, a new data representation, and, we present LABIOS: a new, distributed, Label- based I/O system. LABIOS boosts I/O performance by up to 17× via asynchronous I/O, supports heterogeneous storage resources, offers storage elasticity, and promotes in situ analytics and software defined storage support via data provisioning. LABIOS demonstrates the effectiveness of storage bridging to support the convergence of HPC and BigData workloads on a single platform. © 2020 ACM.",datalabels; elastic storage; energy-aware I/O; exascale I/O; heterogeneous I/O; Label-based I/O; storage bridging; task-based I/O,Application programs; Power management; Semantics; Datalabel; Elastic storage; Energy aware; Energy-aware I/O; Exascale; Exascale I/O; Heterogeneous I/O; Label-based I/O; Storage bridging; Task-based; Task-based I/O; Virtual storage
Cosmos+ OpenSSD: Rapid Prototype for Flash Storage Systems,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092369696&doi=10.1145%2f3385073&partnerID=40&md5=1bd407469fc448709c1357cbd25ee400,"As semiconductor technology has advanced, many storage systems have begun to use non-volatile memories as storage media. The organization and architecture of storage controllers have become more complex to meet various design requirements in terms of performance, response time, quality of service (QoS), and so on. In addition, due to the evolution of memory technology and the emergence of new applications, storage controllers employ new firmware algorithms and hardware modules. When designing storage controllers, engineers often evaluate the performance impact of using new software and hardware components using software simulators. However, this technique often yields limited evaluation accuracy because of the difficulty of modeling complex operations of components and the interactions among them. In this article, we present a reconfigurable flash storage controller design that serves as a rapid prototype. This design can be synthesized into a field-programmable gate array device and used in a realistic performance evaluation environment. We show the usefulness of our design by demonstrating the performance impact of design parameters. © 2020 ACM.",Flash memory; flash translation layer (FTL); solid state drive (SSD); storage system,Controllers; Data storage equipment; Field programmable gate arrays (FPGA); Firmware; Logic Synthesis; Quality of service; Semiconductor device manufacture; Controller designs; Evaluation accuracy; Memory technology; Non-volatile memory; Performance impact; Semiconductor technology; Software and hardwares; Software simulator; Storage as a service (STaaS)
B3-Tree: Byte-Addressable Binary B-Tree for Persistent Memory,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092349191&doi=10.1145%2f3394025&partnerID=40&md5=bf899c82efd7e3144e3cfed0cd1bdf96,"In this work, we propose B3-tree, a hybrid index for persistent memory that leverages the byte-addressability of the in-memory index and the page locality of B-trees. As in the byte-addressable in-memory index, B3-tree is updated by 8-byte store instructions. Also, as in disk-based index, B3-tree is failure-atomic since it makes every 8-byte store instruction transform a consistent index into another consistent index without the help of expensive logging. Since expensive logging becomes unnecessary, the number of cacheline flush instructions required for B3-tree is significantly reduced. Our performance study shows that B3-tree outperforms other state-of-the-art persistent indexes in terms of insert and delete performance. While B3-tree shows slightly worse performance for point query performance, the range query performance of B3-tree is 2x faster than FAST and FAIR B-tree because the leaf page size of B3-tree can be set to 8x larger than that of FAST and FAIR B-tree without degrading insertion performance. We also show that read transactions can access B3-tree without acquiring a shared lock because B3-tree remains always consistent while a sequence of 8-byte write operations are making changes to it. As a result, B3-tree provides high concurrency level comparable to FAST and FAIR B-tree. © 2020 ACM.",data structure; Non-volatile memory; persistent indexing,Data storage equipment; High concurrencies; Memory index; Performance study; Persistent memory; Query performance; Range query; State of the art; Write operations; Binary trees
Spiffy: Enabling File-System Aware Storage Applications,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092424240&doi=10.1145%2f3386368&partnerID=40&md5=767506053efb1637a044ca1b6ad776e7,"Many file-system applications such as defragmentation tools, file-system checkers, or data recovery tools, operate at the storage layer. Today, developers of these file-system aware storage applications require detailed knowledge of the file-system format, which requires significant time to learn, often by trial and error, due to insufficient documentation or specification of the format. Furthermore, these applications perform ad-hoc processing of the file-system metadata, leading to bugs and vulnerabilities. We propose Spiffy, an annotation language for specifying the on-disk format of a file system. File-system developers annotate the data structures of a file system, and we use these annotations to generate a library that allows identifying, parsing, and traversing file-system metadata, providing support for both offline and online storage applications. This approach simplifies the development of storage applications that work across different file systems because it reduces the amount of file-system-specific code that needs to be written. We have written annotations for the Linux Ext4, Btrfs, and F2FS file systems, and developed several applications for these file systems, including a type-specific metadata corruptor, a file-system converter, an online storage layer cache that preferentially caches files for certain users, and a runtime file-system checker. Our experiments show that applications built with the Spiffy library for accessing file-system metadata can achieve good performance and are robust against file-system corruption errors. © 2020 ACM.",Annotation language; Btrfs; Ext4; F2FS; file-system traversal; generic file-system aware applications; metadata parsing and serialization; robustness,Computer operating systems; Digital storage; File organization; Metadata; Online systems; Storage allocation (computer); Annotation languages; Data recovery; De-fragmentation; File system checkers; File systems; Online storages; Storage layers; Trial and error; Program debugging
"Cache What You Need to Cache: Reducing Write Traffic in Cloud Cache via ""one-Time-Access-Exclusion"" Policy",2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092355818&doi=10.1145%2f3397766&partnerID=40&md5=978f90b2dca0b75d1a0d778967b6f5a2,"The SSD has been playing a significantly important role in caching systems due to its high performance-to-cost ratio. Since the cache space is typically much smaller than that of the backend storage by one order of magnitude or even more, write density (defined as writes per unit time and space) of the SSD cache is therefore much more intensive than that of HDD storage, which brings about tremendous challenges to the SSD's lifetime. Meanwhile, under social network workloads, quite a lot writes to the SSD cache are unnecessary. For example, our study on Tencent's photo caching shows that about 61% of total photos are accessed only once, whereas they are still swapped in and out of the cache. Therefore, if we can predict these kinds of photos proactively and prevent them from entering the cache, we can eliminate unnecessary SSD cache writes and improve cache space utilization. To cope with the challenge, we put forward a ""one-time-access criteria""that is applied to the cache space and further propose a ""one-time-access-exclusion""policy. Based on these two techniques, we design a prediction-based classifier to facilitate the policy. Unlike the state-of-the-art history-based predictions, our prediction is non-history oriented, which is challenging to achieve good prediction accuracy. To address this issue, we integrate a decision tree into the classifier, extract social-related information as classifying features, and apply cost-sensitive learning to improve classification precision. Due to these techniques, we attain a prediction accuracy greater than 80%. Experimental results show that the one-time-access-exclusion approach results in outstanding cache performance in most aspects. Take LRU, for instance: applying our approach improves the hit rate by 4.4%, decreases the cache writes by 56.8%, and cuts the average access latency by 5.5%. © 2020 ACM.",machine learning; photo caching; social network; SSD,Decision trees; Forecasting; Hard disk storage; Image enhancement; Cache performance; Caching system; Classification precision; Cost-sensitive learning; Prediction accuracy; Prediction-based; Space utilization; State of the art; Classification (of information)
Batch-file Operations to Optimize Massive Files Accessing,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092349828&doi=10.1145%2f3394286&partnerID=40&md5=d3fcc22105984fe3413dddc5061eeb5e,"Existing local file systems, designed to support a typical single-file access mode only, can lead to poor performance when accessing a batch of files, especially small files. This single-file mode essentially serializes accesses to batched files one by one, resulting in a large number of non-sequential, random, and often dependent I/Os between file data and metadata at the storage ends. Such access mode can further worsen the efficiency and performance of applications accessing massive files, such as data migration. We first experimentally analyze the root cause of such inefficiency in batch-file accesses. Then, we propose a novel batch-file access approach, referred to as BFO for its set of optimized Batch-File Operations, by developing novel BFOr and BFOw operations for fundamental read and write processes, respectively, using a two-phase access for metadata and data jointly. The BFO offers dedicated interfaces for batch-file accesses and additional processes integrated into existing file systems without modifying their structures and procedures. In addition, based on BFOr and BFOw, we also propose the novel batch-file migration BFOm to accelerate the data migration for massive small files. We implement a BFO prototype on ext4, one of the most popular file systems. Our evaluation results show that the batch-file read and write performances of BFO are consistently higher than those of the traditional approaches regardless of access patterns, data layouts, and storage media, under synthetic and real-world file sets. BFO improves the read performance by up to 22.4× and 1.8× with HDD and SSD, respectively, and it boosts the write performance by up to 111.4× and 2.9× with HDD and SSD, respectively. BFO also demonstrates consistent performance advantages for data migration in both local and remote situations. © 2020 ACM.",Batch-file operations; layout-aware scheduler; two-phase access,File organization; Metadata; Consistent performance; Data and metadata; Efficiency and performance; Evaluation results; Local file systems; Poor performance; Read performance; Traditional approaches; Hard disk storage
Introduction to the Special Section on SOSP 2019,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087083291&doi=10.1145%2f3395778&partnerID=40&md5=6cb3984d83c49ef44c8c2ba1e068440d,[No abstract available],,
ShieldNVM: An Efficient and Fast Recoverable System for Secure Non-Volatile Memory,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086799140&doi=10.1145%2f3381835&partnerID=40&md5=344a1807ce1867d49ab53a93b792aaa5,"Data encryption and authentication are essential for secure non-volatile memory (NVM). However, the introduced security metadata needs to be atomically written back to NVM along with data, so as to provide crash consistency, which unfortunately incurs high overhead. To support fine-grained data protection and fast recovery for a secure NVM system without compromising the performance, we propose ShieldNVM. It first proposes an epoch-based mechanism to aggressively cache the security metadata in the metadata cache while retaining the consistency of them in NVM. Deferred spreading is also introduced to reduce the calculating overhead for data authentication. Leveraging the ability of data hash message authentication codes, we can always recover the consistent but old security metadata to its newest version. By recording a limited number of dirty addresses of the security metadata, ShieldNVM achieves fast recovering the secure NVM system after crashes. Compared to Osiris, a state-of-the-art secure NVM, ShieldNVM reduces system runtime by 39.1% and hash message authentication code computation overhead by 80.5% on average over NVM workloads. When system crashes happen, ShieldNVM's recovery time is orders of magnitude faster than Osiris. In addition, ShieldNVM also recovers faster than AGIT, which is the Osiris-based state-of-the-art mechanism addressing the recovery time of the secure NVM system. Once the recovery process fails, instead of dropping all data due to malicious attacks, ShieldNVM is able to detect and locate the area of the tampered data with the help of the tracked addresses. © 2020 ACM.",crash consistency; memory security; Non-volatile memory,Authentication; Codes (symbols); Cost reduction; Metadata; Network security; Nonvolatile storage; Recovery; Computation overheads; Data authentication; Malicious attack; Message authentication codes; Non-volatile memory; Orders of magnitude; Recovery process; State of the art; Cryptography
The Case for Custom Storage Backends in Distributed Storage Systems,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086803345&doi=10.1145%2f3386362&partnerID=40&md5=40831afc5fc16cb625c8d10f5c1292f9,"For a decade, the Ceph distributed file system followed the conventional wisdom of building its storage backend on top of local file systems. This is a preferred choice for most distributed file systems today, because it allows them to benefit from the convenience and maturity of battle-tested code. Ceph's experience, however, shows that this comes at a high price. First, developing a zero-overhead transaction mechanism is challenging. Second, metadata performance at the local level can significantly affect performance at the distributed level. Third, supporting emerging storage hardware is painstakingly slow. Ceph addressed these issues with BlueStore, a new backend designed to run directly on raw storage devices. In only two years since its inception, BlueStore outperformed previous established backends and is adopted by 70% of users in production. By running in user space and fully controlling the I/O stack, it has enabled space-efficient metadata and data checksums, fast overwrites of erasure-coded data, inline compression, decreased performance variability, and avoided a series of performance pitfalls of local file systems. Finally, it makes the adoption of backward-incompatible storage hardware possible, an important trait in a changing storage landscape that is learning to embrace hardware diversity. © 2020 ACM.",Ceph; distributed file system; file system; object storage; storage backend,Metadata; Multiprocessing systems; Virtual storage; Distributed file systems; Distributed storage system; Hardware diversity; Local file systems; Performance variability; Space efficient; Storage hardware; Transaction mechanism; File organization
Practical Quick File Server Migration,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086800962&doi=10.1145%2f3377322&partnerID=40&md5=0e1cc6c9117ff10f63850527ab2f822d,"Regular file server upgrades are indispensable to improve performance, robustness, and power consumption. In upgrading file servers, it is crucial to quickly migrate file-sharing services between heterogeneous servers with little downtime while minimizing performance interference. We present a practical quick file server migration scheme based on the postcopy approach that defers file copy until after switching servers. This scheme can (1) reduce downtime with on-demand file migration, (2) avoid performance interference using background migration, and (3) support heterogeneous servers with stub-based file management. We discuss several practical issues, such as intermittent crawling and traversal strategy, and present the solutions in our scheme. We also address several protocol-specific issues to achieve a smooth migration. This scheme is good enough to be adopted in production systems, as it has been demonstrated for several years in real operational environments. The performance evaluation demonstrates that the downtime is less than 3 seconds, and the first file access after switching servers does not cause a timeout in the default timeout settings; it takes less than 10 seconds in most cases and up to 84.55 seconds even in a large directory tree with a depth of 16 and a width of 1,000. Although the total migration time is approximately 3 times longer than the traditional precopy approach that copies all files in advance, our scheme allows the clients to keep accessing files with acceptable overhead. We also show that appropriate selection of traversal strategy reduces tail latency by 88%, and the overhead after the migration is negligible. © 2020 ACM.",File server; migration; postcopy,Maintenance; File sharing service; Heterogeneous servers; Improve performance; Operational environments; Practical issues; Production system; Several protocols; Total migration time; Green computing
"On Fault Tolerance, Locality, and Optimality in Locally Repairable Codes",2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086803310&doi=10.1145%2f3381832&partnerID=40&md5=c5c9a9ab68fbb64b5a3377f00b961a74,"Erasure codes in large-scale storage systems allow recovery of data from a failed node. A recently developed class of codes, locally repairable codes (LRCs), offers tradeoffs between storage overhead and repair cost. LRCs facilitate efficient recovery scenarios by adding parity blocks to the system. However, these additional blocks may eventually increase the number of blocks that must be reconstructed. Existing LRCs differ in their use of the parity blocks, in their locality semantics, and in their parameter space. Thus, existing theoretical models cannot directly compare different LRCs to determine which code offers the best recovery performance, and at what cost. We perform the first systematic comparison of existing LRC approaches. We analyze Xorbas, Azure's LRCs, and Optimal-LRCs in light of two new metrics: average degraded read cost and normalized repair cost. We show the tradeoff between these costs and the code's fault tolerance, and that different approaches offer different choices in this tradeoff. Our experimental evaluation on a Ceph cluster further demonstrates the different effects of realistic system bottlenecks on the benefit from each LRC approach. Despite these differences, the normalized repair cost metric can reliably identify the LRC approach that would achieve the lowest repair cost in each setup. © 2020 ACM.",Erasure codes; local repair,Digital storage; Fault tolerance; Recovery; Semantics; Different effects; Experimental evaluation; Large-scale storage systems; Number of blocks; Parameter spaces; Realistic systems; Recovery performance; Storage overhead; Codes (symbols)
Finding Bugs in File Systems with an Extensible Fuzzing Framework,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086805065&doi=10.1145%2f3391202&partnerID=40&md5=57709727c11897db8a11094ebc0435e7,"File systems are too large to be bug free. Although handwritten test suites have been widely used to stress file systems, they can hardly keep up with the rapid increase in file system size and complexity, leading to new bugs being introduced. These bugs come in various flavors: buffer overflows to complicated semantic bugs. Although bug-specific checkers exist, they generally lack a way to explore file system states thoroughly. More importantly, no turnkey solution exists that unifies the checking effort of various aspects of a file system under one umbrella. In this article, to highlight the potential of applying fuzzing to find any type of file system bugs in a generic way, we propose Hydra, an extensible fuzzing framework. Hydra provides building blocks for file system fuzzing, including input mutators, feedback engines, test executors, and bug post-processors. As a result, developers only need to focus on building the core logic for finding bugs of their interests. We showcase the effectiveness of Hydra with four checkers that hunt crash inconsistency, POSIX violations, logic assertion failures, and memory errors. So far, Hydra has discovered 157 new bugs in Linux file systems, including three in verified file systems (FSCQ and Yxv6). © 2020 ACM.",bug finding; crash consistency; File systems; fuzzing,Computer circuits; Computer operating systems; File organization; Semantics; Buffer overflows; Bug-free; Building blockes; Core logic; File systems; Linux file system; Memory error; Post-processor; Program debugging
SlimCache: An Efficient Data Compression Scheme for Flash-based Key-value Caching,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086802072&doi=10.1145%2f3383124&partnerID=40&md5=1023a88c28da047192b8e8bda50a9a64,"Flash-based key-value caching is becoming popular in data centers for providing high-speed key-value services. These systems adopt slab-based space management on flash and provide a low-cost solution for key-value caching. However, optimizing cache efficiency for flash-based key-value cache systems is highly challenging, due to the huge number of key-value items and the unique technical constraints of flash devices. In this article, we present a dynamic on-line compression scheme, called SlimCache, to improve the cache hit ratio by virtually expanding the usable cache space through data compression. We have investigated the effect of compression granularity to achieve a balance between compression ratio and speed, and we leveraged the unique workload characteristics in key-value systems to efficiently identify and separate hot and cold data. To dynamically adapt to workload changes during runtime, we have designed an adaptive hot/cold area partitioning method based on a cost model. To avoid unnecessary compression, SlimCache also estimates data compressibility to determine whether the data are suitable for compression or not. We have implemented a prototype based on Twitter's Fatcache. Our experimental results show that SlimCache can accommodate more key-value items in flash by up to 223.4%, effectively increasing throughput and reducing average latency by up to 380.1% and 80.7%, respectively. © 2020 ACM.",data compression; Flash memory; key-value caching; SSD,Data storage equipment; Cache efficiency; Cache hit ratio; Compression scheme; Low-cost solution; Partitioning methods; Space management; Technical constraints; Workload characteristics; Data compression ratio
Information Leakage in Encrypted Deduplication via Frequency Analysis,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083758833&doi=10.1145%2f3365840&partnerID=40&md5=66142b0b17faa6511c36aa14f7f7b41f,"Encrypted deduplication combines encryption and deduplication to simultaneously achieve both data security and storage efficiency. State-of-the-art encrypted deduplication systems mainly build on deterministic encryption to preserve deduplication effectiveness. However, such deterministic encryption reveals the underlying frequency distribution of the original plaintext chunks. This allows an adversary to launch frequency analysis against the ciphertext chunks and infer the content of the original plaintext chunks. In this article, we study how frequency analysis affects information leakage in encrypted deduplication, from both attack and defense perspectives. Specifically, we target backup workloads and propose a new inference attack that exploits chunk locality to increase the coverage of inferred chunks. We further combine the new inference attack with the knowledge of chunk sizes and show its attack effectiveness against variable-size chunks. We conduct trace-driven evaluation on both real-world and synthetic datasets and show that our proposed attacks infer a significant fraction of plaintext chunks under backup workloads. To defend against frequency analysis, we present two defense approaches, namely MinHash encryption and scrambling. Our trace-driven evaluation shows that our combined MinHash encryption and scrambling scheme effectively mitigates the severity of the inference attacks, while maintaining high storage efficiency and incurring limited metadata access overhead. © 2020 ACM.",cloud storage; encrypted deduplication; Frequency analysis,Digital storage; Efficiency; Network security; Deterministic encryptions; Frequency Analysis; Frequency distributions; Inference attacks; Information leakage; State of the art; Storage efficiency; Synthetic datasets; Cryptography
The Composite-File File System,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083762487&doi=10.1145%2f3366684&partnerID=40&md5=e8c81dbbca8b05af428e6299703e301a,"The design and implementation of traditional file systems typically use the one-to-one mapping of logical files to their physical metadata representations. File system optimizations generally follow this rigid mapping and miss opportunities for an entire class of optimizations. We designed, implemented, and evaluated a composite-file file system, which allows many-to-one mappings of files to metadata. Through exploring different mapping strategies, our empirical evaluation shows up to a 27% performance improvement under web server and software development workloads, for both disks and SSDs. This result demonstrates that our approach of relaxing file-to-metadata mapping is promising. © 2020 ACM.",Design; file systems; metadata; performance,File organization; Metadata; Software design; Design and implementations; Empirical evaluations; File systems; Logical file; Mapping strategy; Metadata mappings; Metadata representation; One-to-one mappings; Mapping
The Reliability of Modern File Systems in the face of SSD Errors,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083759594&doi=10.1145%2f3375553&partnerID=40&md5=8741b6783b9b1b9feb8e8439dfc29460,"As solid state drives (SSDs) are increasingly replacing hard disk drives, the reliability of storage systems depends on the failure modes of SSDs and the ability of the file system layered on top to handle these failure modes. While the classical paper on IRON File Systems provides a thorough study of the failure policies of three file systems common at the time, we argue that 13 years later it is time to revisit file system reliability with SSDs and their reliability characteristics in mind, based on modern file systems that incorporate journaling, copy-on-write, and log-structured approaches and are optimized for flash. This article presents a detailed study, spanning ext4, Btrfs, and F2FS, and covering a number of different SSD error modes. We develop our own fault injection framework and explore over 1,000 error cases. Our results indicate that 16% of these cases result in a file system that cannot be mounted or even repaired by its system checker. We also identify the key file system metadata structures that can cause such failures, and, finally, we recommend some design guidelines for file systems that are deployed on top of SSDs. © 2020 ACM.",file systems; Recovery; reliability; solid state drives,Errors; File organization; Hard disk storage; Copy on write; Fault injection; File systems; Hard Disk Drive; Log structured; Reliability characteristics; Solid state drives; Storage systems; Reliability
Fast Erasure Coding for Data Storage,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083759875&doi=10.1145%2f3375554&partnerID=40&md5=185836bb200549ec4b3c0a5fa2e4fe32,"Various techniques have been proposed in the literature to improve erasure code computation efficiency, including optimizing bitmatrix design and computation schedule, common XOR (exclusive-OR) operation reduction, caching management techniques, and vectorization techniques. These techniques were largely proposed individually, and, in this work, we seek to use them jointly. To accomplish this task, these techniques need to be thoroughly evaluated individually and their relation better understood. Building on extensive testing, we develop methods to systematically optimize the computation chain together with the underlying bitmatrix. This led to a simple design approach of optimizing the bitmatrix by minimizing a weighted computation cost function, and also a straightforward coding procedure - follow a computation schedule produced from the optimized bitmatrix to apply XOR-level vectorization. This procedure provides better performances than most existing techniques (e.g., those used in ISA-L and Jerasure libraries), and sometimes can even compete against well-known but less general codes such as EVENODD, RDP, and STAR codes. One particularly important observation is that vectorizing the XOR operations is a better choice than directly vectorizing finite field operations, not only because of the flexibility in choosing finite field size and the better encoding throughput, but also its minimal migration efforts onto newer CPUs. © 2020 ACM.",Erasure code; performance,Cost functions; Digital storage; Program processors; Computation costs; Computation efficiency; Design approaches; Encoding throughput; Extensive testing; Finite field operations; Management techniques; Vectorization techniques; Codes (symbols)
PBS: An Efficient Erasure-Coded Block Storage System Based on Speculative PartialWrites,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083761024&doi=10.1145%2f3365839&partnerID=40&md5=d3b7d78a13ab1d14c057111c559ea489,"Block storage provides virtual disks that can be mounted by virtual machines (VMs). Although erasure coding (EC) has been widely used in many cloud storage systems for its high efficiency and durability, current EC schemes cannot provide high-performance block storage for the cloud. This is because they introduce significant overhead to small write operations (which perform partial write to an entire EC group), whereas cloud-oblivious applications running on VMs are often small-write-intensive. We identify the root cause for the poor performance of partial writes in state-of-the-art EC schemes: for each partial write, they have to perform a time-consuming write-after-read operation that reads the current value of the data and then computes and writes the parity delta, which will be used to ""patch"" the parity in journal replay. In this article, we present a speculative partial write scheme (called PARIX) that supports fast small writes in erasure-coded storage systems. We transform the original formula of parity calculation to use the data deltas (between the current/original data values), instead of the parity deltas, to calculate the parities in journal replay. For each partial write, this allows PARIX to speculatively log only the new value of the data without reading its original value. For a series of n partial writes to the same data, PARIX performs pure write (instead of write-after-read) for the last n-1 ones while only introducing a small penalty of an extra network round-trip time to the first one. Based on PARIX, we design and implement PARIX Block Storage (PBS), an efficient block storage system that provides high-performance virtual disk service for VMs running cloud-oblivious applications. PBS not only supports fast partial writes but also realizes efficient full writes, background journal replay, and fast failure recovery with strong consistency guarantees. Both microbenchmarks and trace-driven evaluation show that PBS provides efficient block storage and outperforms state-of-the-art EC-based systems by orders of magnitude. © 2020 ACM.",block storage system; cloud oblivious applications; Erasure coding; partial write,Data storage equipment; Block storage systems; Cloud storage systems; Design and implements; Fast failure recovery; Orders of magnitude; Poor performance; State of the art; Strong consistency; Digital storage
Introduction to the Special Section on USENIX ATC 2019,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083785256&doi=10.1145%2f3383194&partnerID=40&md5=797bc6165dadac6e12d138230108c357,[No abstract available],,
Everyone Loves File,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083761216&doi=10.1145%2f3377877&partnerID=40&md5=0df8996ca6c0c9dfa21995d9235b7557,"Oracle File Storage Service (FSS) is an elastic filesystem provided as a managed NFS service. A pipelined Paxos implementation underpins a scalable block store that provides linearizable multipage limited-size transactions. Above the block store, a scalable B-tree holds filesystem metadata and provides linearizable multikey limited-size transactions. Self-validating B-tree nodes and housekeeping operations performed as separate transactions allow each key in a B-tree transaction to require only one page in the underlying block transaction. The filesystem provides snapshots by using versioned key-value pairs. The system is programmed using a nonblocking lock-free programming style. Presentation servers maintain no persistent local state making them scalable and easy to failover. A non-scalable Paxos-replicated hash table holds configuration information required to bootstrap the system. An additional B-tree provides conversational multi-key minitransactions for control-plane information. The system throughput can be predicted by comparing an estimate of the network bandwidth needed for replication to the network bandwidth provided by the hardware. Latency on an unloaded system is about 4 times higher than a Linux NFS server backed by NVMe, reflecting the cost of replication. FSS has been in production since January 2018 and holds tens of thousands of customer file systems comprising many petabytes of data. © 2020 ACM.",B-tree-based filesystem; cloud filesystem; Distributed filesystem; Paxos; two-phase commit,Bandwidth; Computer operating systems; Digital storage; Control planes; File systems; Key-value pairs; Network bandwidth; Non-blocking; Programming styles; System throughput; Unloaded system; File organization
Countering fragmentation in an enterprise storage system,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078230650&doi=10.1145%2f3366173&partnerID=40&md5=cc8c3f6f5761ad4843eaabe60264da83,"As a file system ages, it can experience multiple forms of fragmentation. Fragmentation of the free space in the file system can lower write performance and subsequent read performance. Client operations as well as internal operations, such as deduplication, can fragment the layout of an individual file, which also impacts file read performance. File systems that allow sub-block granular addressing can gather intra-block fragmentation, which leads to wasted free space. Similarly, wasted space can also occur when a file system writes a collection of blocks out to object storage as a single large object, because the constituent blocks can become free at different times. The impact of fragmentation also depends on the underlying storage media. This article studies each form of fragmentation in the NetApp® WAFL® file system, and explains how the file system leverages a storage virtualization layer for defragmentation techniques that physically relocate blocks efficiently, including those in read-only snapshots. The article analyzes the effectiveness of these techniques at reducing fragmentation and improving overall performance across various storage media. © 2020 Association for Computing Machinery.",Deduplication; File system; File system performance; Fragmentation; Snapshot; Storage system,Virtual storage; De duplications; File systems; Fragmentation; Snapshot; Storage systems; File organization
Characterizing output bottlenecks of a production supercomputer: Analysis and implications,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078303666&doi=10.1145%2f3335205&partnerID=40&md5=6d73cceccafe42dac915fe9b940fdc02,"This article studies the I/O write behaviors of the Titan supercomputer and its Lustre parallel file stores under production load. The results can inform the design, deployment, and configuration of file systems along with the design of I/O software in the application, operating system, and adaptive I/O libraries. We propose a statistical benchmarking methodology to measure write performance across I/O configurations, hardware settings, and system conditions. Moreover, we introduce two relative measures to quantify the write-performance behaviors of hardware components under production load. In addition to designing experiments and benchmarking on Titan, we verify the experimental results on one real application and one real application I/O kernel, XGC and HACC IO, respectively. These two are representative and widely used to address the typical I/O behaviors of applications. In summary, we find that Titan's I/O system is variable across the machine at fine time scales. This variability has two major implications. First, stragglers lessen the benefit of coupled I/O parallelism (striping). Peak median output bandwidths are obtained with parallel writes to many independent files, with no striping or write sharing of files across clients (compute nodes). I/O parallelism is most effective when the application'or its I/O libraries'distributes the I/O load so that each target stores files for multiple clients and each client writes files on multiple targets in a balanced way with minimal contention. Second, our results suggest that the potential benefit of dynamic adaptation is limited. In particular, it is not fruitful to attempt to identify “good locations” in the machine or in the file system: component performance is driven by transient load conditions and past performance is not a useful predictor of future performance. For example, we do not observe diurnal load patterns that are predictable. © 2020 E. Schweizerbart'sche Verlagsbuchhandlung. All rights reserved.",,Application programs; File organization; Libraries; Supercomputers; Benchmarking methodology; Component performance; Dynamic adaptations; Future performance; Hardware components; Hardware settings; Potential benefits; Real applications; Benchmarking
Instalytics: Cluster filesystem Co-design for big-data analytics,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078291307&doi=10.1145%2f3369738&partnerID=40&md5=abbf2986ec4a21fa7bef309752e7307e,"We present the design, implementation, and evaluation of INSTalytics, a co-designed stack of a cluster file system and the compute layer, for efficient big-data analytics in large-scale data centers. INSTalytics amplifies the well-known benefits of data partitioning in analytics systems; instead of traditional partitioning on one dimension, INSTalytics enables data to be simultaneously partitioned on four different dimensions at the same storage cost, enabling a larger fraction of queries to benefit from partition filtering and joins without network shuffle. To achieve this, INSTalytics uses compute-awareness to customize the three-way replication that the cluster file system employs for availability. A new heterogeneous replication layout enables INSTalytics to preserve the same recovery cost and availability as traditional replication. INSTalytics also uses compute-awareness to expose a new sliced-read API that improves performance of joins by enabling multiple compute nodes to read slices of a data block efficiently via co-ordinated request scheduling and selective caching at the storage nodes. We have built a prototype implementation of INSTalytics in a production analytics stack, and we show that recovery performance and availability is similar to physical replication, while providing significant improvements in query performance, suggesting a new approach to designing cloud-scale big-data analytics systems. © 2020 Association for Computing Machinery.",Big data query processing; Data center storage; Storage replication,Advanced Analytics; Big data; Data Analytics; Digital storage; File organization; Search engines; Analytics systems; Cluster File Systems; Data centers; Data query; Prototype implementations; Query performance; Recovery performance; Request scheduling; Data handling
Introduction to the special issue on USENIX Fast 2019,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079384227&doi=10.1145%2f3372347&partnerID=40&md5=79752a22b4a4ace60effc0f859cba0de,[No abstract available],,
EIC Message,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079399537&doi=10.1145%2f3372345&partnerID=40&md5=028f4188783bf6c1fdc0ebd9caae6633,[No abstract available],,
Graphone: A data store for real-time analytics on evolving graphs,2020,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078319585&doi=10.1145%2f3364180&partnerID=40&md5=9ad94ed187dad3e4e3510d28a15cae97,"There is a growing need to perform a diverse set of real-time analytics (batch and stream analytics) on evolving graphs to deliver the values of big data to users. The key requirement from such applications is to have a data store to support their diverse data access efficiently, while concurrently ingesting fine-grained updates at a high velocity. Unfortunately, current graph systems, either graph databases or analytics engines, are not designed to achieve high performance for both operations; rather, they excel in one area that keeps a private data store in a specialized way to favor their operations only. To address this challenge, we have designed and developed GraphOne, a graph data store that abstracts the graph data store away from the specialized systems to solve the fundamental research problems associated with the data store design. It combines two complementary graph storage formats (edge list and adjacency list) and uses dual versioning to decouple graph computations from updates. Importantly, it presents a new data abstraction, GraphView, to enable data access at two different granularities of data ingestions (called data visibility) for concurrent execution of diverse classes of real-time graph analytics with only a small data duplication. Experimental results show that GraphOne is able to deliver 11.40× and 5.36× average speedup in ingestion rate against LLAMA and Stinger, the two state-of-the-art dynamic graph systems, respectively. Further, they achieve an average speedup of 8.75× and 4.14× against LLAMA and 12.80× and 3.18× against Stinger for BFS and PageRank analytics (batch version), respectively. GraphOne also gains over 2,000× speedup against Kickstarter, a state-of-the-art stream analytics engine in ingesting the streaming edges and performing streaming BFS when treating first half as a base snapshot and rest as streaming edge in a synthetic graph. GraphOne also achieves an ingestion rate of two to three orders of magnitude higher than graph databases. Finally, we demonstrate that it is possible to run concurrent stream analytics from the same data store. © 2020 Association for Computing Machinery.",Batch analytics; Graph data management; Phrases: Graph systems; Stream analytics; Unified graph data store,Abstracting; Digital storage; Engines; Graph Databases; Batch analytics; Concurrent execution; Different granularities; Fundamental research; Graph data; Real-time analytics; Stream analytics; Three orders of magnitude; Information management
Determining data distribution for large disk enclosures with 3-D data templates,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077780553&doi=10.1145%2f3342858&partnerID=40&md5=0c34fc252ec396c4b6a61eb9dee8a7b5,"Conventional RAID solutions with fixed layouts partition large disk enclosures so that each RAID group uses its own disks exclusively. This achieves good performance isolation across underlying disk groups, at the cost of disk under-utilization and slow RAID reconstruction from disk failures. We propose RAID+, a new RAID construction mechanism that spreads both normal I/O and reconstruction workloads to a larger disk pool in a balanced manner. Unlike systems conducting randomized placement, RAID+ employs deterministic addressing enabled by the mathematical properties of mutually orthogonal Latin squares, based on which it constructs 3-D data templates mapping a logical data volume to uniformly distributed disk blocks across all disks. While the total read/write volume remains unchanged, with or without disk failures, many more disk drives participate in data service and disk reconstruction. Our evaluation with a 60-drive disk enclosure using both synthetic and real-world workloads shows that RAID+ significantly speeds up data recovery while delivering better normal I/O performance and higher multi-tenant system throughput. © 2019 Association for Computing Machinery.",Data distribution; Data recovery; Large disk enclosures; Mutually orthogonal Latin squares; Performance consistency,Enclosures; Data distribution; Data recovery; Large disks; Mutually orthogonal Latin squares; Performance consistency; Digital storage
Sketching volume capacities in deduplicated storage,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077779030&doi=10.1145%2f3369737&partnerID=40&md5=0d8b234e1be2e2f3af86a9e52a7cf997,"The adoption of deduplication in storage systems has introduced significant new challenges for storage management. Specifically, the physical capacities associated with volumes are no longer readily available. In this work, we introduce a new approach to analyzing capacities in deduplicated storage environments. We provide sketch-based estimations of fundamental capacity measures required for managing a storage system: How much physical space would be reclaimed if a volume or group of volumes were to be removed from a system (the reclaimable capacity) and how much of the physical space should be attributed to each of the volumes in the system (the attributed capacity). Our methods also support capacity queries for volume groups across multiple storage systems, e.g., how much capacity would a volume group consume after being migrated to another storage system? We provide analytical accuracy guarantees for our estimations as well as empirical evaluations. Our technology is integrated into a prominent all-flash storage array and exhibits high performance even for very large systems. We also demonstrate how this method opens the door for performing placement decisions at the data-center level and obtaining insights on deduplication in the field. © 2019 Association for Computing Machinery.",Capacity management; Deduplication; Estimation,Digital storage; Estimation; Capacity management; Capacity measures; De duplications; Empirical evaluations; New approaches; Storage systems; Very large systems; Volume capacity; Storage management
LDJ: Version consistency is almost free on commercial storage devices,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077781294&doi=10.1145%2f3365918&partnerID=40&md5=a7bd7eba0bf96afe2b19aff2a0d7b710,"In this article, we propose a simple but practical and efficient optimization scheme for journaling in ext4, called lightweight data journaling (LDJ). By compressing journaled data prior to writing, LDJ can perform comparable to or even faster than the default ordered journaling (OJ) mode in ext4 on top of both HDDs and flash storage devices, while still guaranteeing the version consistency of the data journaling (DJ) mode. This surprising result can be explained with three main reasons. First, on modern storage devices, the sequential write pattern dominating in DJ mode is more and more high-performant than the random one in OJ mode. Second, the compression significantly reduces the amount of journal writes, which will in turn make the write completion faster and prolong the lifespan of storage devices. Third, the compression also enables the atomicity of each journal write without issuing an intervening FLUSH command between journal data blocks and commit block, thus halving the number of costly FLUSH calls in LDJ. We have prototyped our LDJ by slightly modifying the existing ext4 with jbd2 for journaling and also e2fsck for recovery; less than 300 lines of source code were changed. Also, we carried out a comprehensive evaluation using four standard benchmarks and three real applications. Our evaluation results clearly show that LDJ outperforms the OJ mode by up to 9.6× on the real applications. © 2019 Association for Computing Machinery.",Compression; Crash consistency; File system,Benchmarking; Compaction; Comprehensive evaluation; Crash consistency; Data blocks; Evaluation results; File systems; Optimization scheme; Real applications; Source codes; Virtual storage
An attention-augmented deep architecture for hard drive status monitoring in large-scale storage systems,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074870944&doi=10.1145%2f3340290&partnerID=40&md5=aa5b58aca1c2386f75e8e49d050df068,"Data centers equipped with large-scale storage systems are critical infrastructures in the era of big data. The enormous amount of hard drives in storage systems magnify the failure probability, which may cause tremendous loss for both data service users and providers. Despite a set of reactive fault-tolerant measures such as RAID, it is still a tough issue to enhance the reliability of large-scale storage systems. Proactive prediction is an effective method to avoid possible hard-drive failures in advance. A series of models based on the SMART statistics have been proposed to predict impending hard-drive failures. Nonetheless, there remain some serious yet unsolved challenges like the lack of explainability of prediction results. To address these issues, we carefully analyze a dataset collected from a real-world large-scale storage system and then design an attention-augmented deep architecture for hard-drive health status assessment and failure prediction. The deep architecture, composed of a feature integration layer, a temporal dependency extraction layer, an attention layer, and a classification layer, cannot only monitor the status of hard drives but also assist in failure cause diagnoses. The experiments based on real-world datasets show that the proposed deep architecture is able to assess the hard-drive status and predict the impending failures accurately. In addition, the experimental results demonstrate that the attention-augmented deep architecture can reveal the degradation progression of hard drives automatically and assist administrators in tracing the cause of hard drive failures. © 2019 Association for Computing Machinery.",Attention mechanism; Deep neural network; Hard drive failure; Recurrent neural network; SMART,Drives; Forecasting; Hard disk storage; Large dataset; Network architecture; Recurrent neural networks; Attention mechanisms; Failure prediction; Failure Probability; Feature integration; Hard drives; Large-scale storage systems; Real-world datasets; SMART; Deep neural networks
Enabling efficient updates in KV storage via hashing: Design and performance evaluation,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074818834&doi=10.1145%2f3340287&partnerID=40&md5=e681632413bb471f21ca41a62f73080f,"Persistent key-value (KV) stores mostly build on the Log-Structured Merge (LSM) tree for high write performance, yet the LSM-tree suffers from the inherently high I/O amplification. KV separation mitigates I/O amplification by storing only keys in the LSM-tree and values in separate storage. However, the current KV separation design remains inefficient under update-intensive workloads due to its high garbage collection (GC) overhead in value storage. We propose HashKV, which aims for high update performance atop KV separation under update-intensive workloads. HashKV uses hash-based data grouping, which deterministically maps values to storage space to make both updates and GC efficient. We further relax the restriction of such deterministic mappings via simple but useful design extensions. We extensively evaluate various design aspects of HashKV. We show that HashKV achieves 4.6× update throughput and 53.4% less write traffic compared to the current KV separation design. In addition, we demonstrate that we can integrate the design of HashKV with state-of-the-art KV stores and improve their respective performance. © 2019 Association for Computing Machinery.",Hashing; Key-value storage; LSM-tree,Forestry; Separation; Garbage collection; Hashing; Key-value storages; Log structured; LSM-tree; State of the art; Storage spaces; Update intensives; Digital storage
ZoneTier: A zone-based storage tiering and caching co-design to integrate SSDs with SMR drives,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074816079&doi=10.1145%2f3335548&partnerID=40&md5=832354304d2ec416faee68a68ae06e9c,"Integrating solid-state drives (SSDs) and host-aware shingled magnetic recording (HA-SMR) drives can potentially build a cost-effective high-performance storage system. However, existing SSD tiering and caching designs in such a hybrid system are not fully matched with the intrinsic properties of HA-SMR drives due to their lacking consideration of how to handle non-sequential writes (NSWs). We propose ZoneTier, a zonebased storage tiering and caching co-design, to effectively control all the NSWs by leveraging the host-aware property of HA-SMR drives. ZoneTier exploits real-time data layout of SMR zones to optimize zone placement, reshapes NSWs generated from zone demotions to SMR preferred sequential writes, and transforms the inevitable NSWs to cleaning-friendly write traffics for SMR zones. ZoneTier can be easily extended to match host-managed SMR drives using proactive cleaning policy. We implemented a prototype of ZoneTier with user space data management algorithms and real SSD and HA-SMR drives, which are manipulated by the functions provided by libzbc and libaio. Our experiments show that ZoneTier can reduce zone relocation overhead by 29.41% on average, shorten performance recovery time of HA-SMR drives from cleaning by up to 33.37%, and improve performance by up to 32.31% than existing hybrid storage designs. © 2019 Association for Computing Machinery.",Host-aware; Shingled magnetic recording (SMR); Tiering and caching co-design; ZoneTier,Cleaning; Cost effectiveness; Drives; HPSS; Hybrid systems; Information management; Magnetic recording; Magnetic storage; Co-designs; Host-aware; Improve performance; Intrinsic property; Performance recovery; Shingled magnetic recording (SMR); Shingled magnetic recordings; ZoneTier; Digital storage
Level hashing: A high-performance and flexible-resizing persistent hashing index structure,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074861304&doi=10.1145%2f3322096&partnerID=40&md5=fd805e56b0a5bed10b83254d01484616,"Non-volatile memory (NVM) technologies as persistent memory are promising candidates to complement or replace DRAM for building future memory systems, due to having the advantages of high density, low power, and non-volatility. In main memory systems, hashing index structures are fundamental building blocks to provide fast query responses. However, hashing index structures originally designed for dynamic random access memory (DRAM) become inefficient for persistent memory due to new challenges including hardware limitations of NVMand the requirement of data consistency. To address these challenges, this article proposes level hashing, a write-optimized and high-performance hashing index scheme with low-overhead consistency guarantee and cost-efficient resizing. Level hashing provides a sharing-based two-level hash table, which achieves constant-scale worst-case time complexity for search, insertion, deletion, and update operations, and rarely incurs extra NVMwrites. To guarantee the consistency with low overhead, level hashing leverages log-free consistency schemes for deletion, insertion, and resizing operations, and an opportunistic log-free scheme for update operation. To cost-efficiently resize this hash table, level hashing leverages an in-place resizing scheme that only needs to rehash 1/3 of buckets instead of the entire table to expand a hash table and rehash 2/3 of buckets to shrink a hash table, thus significantly improving the resizing performance and reducing the number of rehashed buckets. Extensive experimental results show that the level hashing speeds up insertions by 1.4×-3.0×, updates by 1.2×-2.1×, expanding by over 4.3×, and shrinking by over 1.4× while maintaining high search and deletion performance compared with start-of-the-art hashing schemes. © 2019 Association for Computing Machinery.",Crash consistency; Hashing index structure; Persistent memory; Write optimization,Data structures; Crash consistency; Data consistency; Dynamic random access memory; Fundamental building blocks; Index structure; Non-volatile memory technology; Persistent memory; Resizing operations; Dynamic random access storage
Mitigating synchronous I/O overhead in file systems on open-channel SSDs,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068462134&doi=10.1145%2f3319369&partnerID=40&md5=2538a7b860034358ccdb771296e3c5d3,"Synchronous I/O has long been a design challenge in file systems. Although open-channel solid state drives (SSDs) provide better performance and endurance to file systems, they still suffer from synchronous I/Os due to the amplified writes and worse hot/cold data grouping. The reason lies in the controversy design choices between flash write and read/erase operations. While fine-grained logging improves performance and endurance in writes, it hurts indexing and data grouping efficiency in read and erase operations. In this article, we propose a flash-friendly data layout by introducing a built-in persistent staging layer to provide balanced read, write, and garbage collection performance. Based on this, we design a new flash file system (FS) named StageFS, which decouples the content and structure updates. Content updates are logically logged to the staging layer in a persistence-efficient way, which achieves better write performance and lower write amplification. The updated contents are reorganized into the normal data area for structure updates, with improved hot/cold grouping and in a page-level indexing way, which is more friendly to read and garbage collection operations. Evaluation results show that, compared to recent flash-friendly file system (F2FS), StageFS effectively improves performance by up to 211.4% and achieves low garbage collection overhead for workloads with frequent synchronization. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",File system; Flash memory; Open-channel SSD; Synchronous I/O,Flash memory; Indexing (of information); Refuse collection; Content and structure; Evaluation results; File systems; FLASH file systems; Open channels; Solid state drives; Synchronous I/O; Write amplifications; File organization
Introduction to the special section on OSDI'18,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074854335&doi=10.1145%2f3322101&partnerID=40&md5=6a95ba575cb741f509ddb23bc89fa468,[No abstract available],,
SolarDB: Toward a shared-everything database on distributed log-structured storage,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074823571&doi=10.1145%2f3318158&partnerID=40&md5=2c918667317fc47df10f2420f206bd64,"Efficient transaction processing over large databases is a key requirement for many mission-critical applications. Although modern databases have achieved good performance through horizontal partitioning, their performance deteriorates when cross-partition distributed transactions have to be executed. This article presents SolarDB, a distributed relational database system that has been successfully tested at a large commercial bank. The key features of SolarDB include (1) a shared-everything architecture based on a twolayer log-structured merge-tree; (2) a new concurrency control algorithm that works with the log-structured storage, which ensures efficient and non-blocking transaction processing even when the storage layer is compacting data among nodes in the background; and (3) find-grained data access to effectively minimize and balance network communication within the cluster. According to our empirical evaluations on TPC-C, Smallbank, and a real-world workload, SolarDB outperforms the existing shared-nothing systems by up to 50x when there are close to or more than 5% distributed transactions. © 2019 Association for Computing Machinery.",Concurrency control; Log-structured storage; Shared-everything architecture,Cluster computing; Concurrency control; Digital storage; Network architecture; Relational database systems; Trees (mathematics); Distributed transaction; Log structured; Log structured merge trees; Mission critical applications; Network communications; Non-blocking transaction; Shared-everything; Transaction processing; Distributed database systems
Cores: Towards scan-optimized columnar storage for nested records,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068466715&doi=10.1145%2f3321704&partnerID=40&md5=9f3c93121a1673879691f13c5b956702,"The relatively high cost of record deserialization is increasingly becoming the bottleneck of column-based storage systems in tree-structured applications [58]. Due to record transformation in the storage layer, unnecessary processing costs derived from fields and rows irrelevant to queries may be very heavy in nested schemas, significantly wasting the computational resources in large-scale analytical workloads. This leads to the question of how to reduce both the deserialization and IO costs of queries with highly selective filters following arbitrary paths in a nested schema. We present CORES (Column-Oriented Regeneration Embedding Scheme) to push highly selective filters down into column-based storage engines, where each filter consists of several filtering conditions on a field. By applying highly selective filters in the storage layer, we demonstrate that both the deserialization and IO costs could be significantly reduced. We show how to introduce fine-grained composition on filtering results. We generalize this technique by two pair-wise operations, rollup and drilldown, such that a series of conjunctive filters can effectively deliver their payloads in nested schema. The proposed methods are implemented on an open-source platform. For practical purposes, we highlight how to build a column storage engine and how to drive a query efficiently based on a cost model. We apply this design to the nested relational model especially when hierarchical entities are frequently required by ad hoc queries. The experiments, including a real workload and the modified TPCH benchmark, demonstrate that CORES improves the performance by 0.7×–26.9× compared to state-of-the-art platforms in scan-intensive workloads. © 2019 Association for Computing Machinery.",Bitset composition; Columnar storage; Filtering-pushdown; Nested schema; Sequential scan; Skipping scheme,Benchmarking; Engines; Reforestation; Virtual storage; Computational resources; Highly selective filters; Nested schema; Open source platforms; Pushdown; Sequential scan; Skipping scheme; Tree-structured applications; Cost reduction
TXFS: Leveraging file-system crash consistency to provide acid transactions,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065711267&doi=10.1145%2f3318159&partnerID=40&md5=7a551e6ba476145842ea082d13d18f19,"We introduce TxFS, a transactional file system that builds upon a file system's atomic-update mechanism such as journaling. Though prior work has explored a number of transactional file systems, TxFS has a unique set of properties: a simple API, portability across different hardware, high performance, low complexity (by building on the file-system journal), and full ACID transactions. We port SQLite, OpenLDAP, and Git to use TxFS and experimentally show that TxFS provides strong crash consistency while providing equal or better performance. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ACID transactions; Crash consistency; File systems; Operating systems,Computer operating systems; Data storage equipment; ACID Transactions; Crash consistency; File systems; Update mechanisms; File organization
Performance and resource utilization of fuse user-space file systems,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065781619&doi=10.1145%2f3310148&partnerID=40&md5=a09b37306bbd20acf5acd722c346febd,"Traditionally, file systems were implemented as part of operating systems kernels, which provide a limited set of tools and facilities to a programmer. As the complexity of file systems grew, many new file systems began being developed in user space. Low performance is considered the main disadvantage of user-space file systems but the extent of this problem has never been explored systematically. As a result, the topic of user-space file systems remains rather controversial: while some consider user-space file systems a “toy” not to be used in production, others develop full-fledged production file systems in user space. In this article, we analyze the design and implementation of a well-known user-space file system framework, FUSE, for Linux. We characterize its performance and resource utilization for a wide range of workloads. We present FUSE performance and also resource utilization with various mount and configuration options, using 45 different workloads that were generated using Filebench on two different hardware configurations. We instrumented FUSE to extract useful statistics and traces, which helped us analyze its performance bottlenecks and present our analysis results. Our experiments indicate that depending on the workload and hardware used, performance degradation (throughput) caused by FUSE can be completely imperceptible or as high as −83%, even when optimized; and latencies of FUSE file system operations can be increased from none to 4× when compared to Ext4. On the resource utilization side, FUSE can increase relative CPU utilization by up to 31% and underutilize disk bandwidth by as much as −80% compared to Ext4, though for many data-intensive workloads the impact was statistically indistinguishable. Our conclusion is that user-space file systems can indeed be used in production (non-“toy”) settings, but their applicability depends on the expected workloads. © 2019 Copyright held by the owner/author(s).",Linux FUSE; User-space file systems,Linux; Configuration options; Data-intensive workloads; Design and implementations; File systems; Hardware configurations; Performance bottlenecks; Performance degradation; Resource utilizations; File organization
Introduction to the special section on the 2018 USENIX annual technical conference (ATC'18),2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065700059&doi=10.1145%2f3322100&partnerID=40&md5=025d68bd1d166a6bb3851ca0e537f696,[No abstract available],,
Registor: A platform for unstructured data processing inside SSD storage,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065627202&doi=10.1145%2f3310149&partnerID=40&md5=ac749ffa2dcbf599f2a4ded363b24b60,"This article presents REGISTOR, a platform for regular expression grabbing inside storage. The main idea of Registor is accelerating regular expression (regex) search inside storage where large data set is stored, eliminating the I/O bottleneck problem. A special hardware engine for regex search is designed and augmented inside a flash SSD that processes data on-the-fly during data transmission from NAND flash to host. To make the speed of regex search match the internal bus speed of a modern SSD, a deep pipeline structure is designed in Registor hardware consisting of a file semantics extractor, matching candidates finder, regex matching units (REMUs), and results organizer. Furthermore, each stage of the pipeline makes the use of maximal parallelism possible. To make Registor readily usable by high-level applications, we have developed a set of APIs and libraries in Linux allowing Registor to process files in the SSD by recombining separate data blocks into files efficiently. A working prototype of Registor has been built in our newly designed NVMe-SSD. Extensive experiments and analyses have been carried out to show that Registor achieves high throughput, reduces the I/O bandwidth requirement by up to 97%, and reduces CPU utilization by as much as 82% for regex search in large datasets. © 2019 Association for Computing Machinery.",Hardware accelerator; Near data processing; Processing in storage; Regular expressions; SSD storage,Computer operating systems; Data handling; Flash-based SSDs; Large dataset; Pattern matching; Pipelines; Semantics; Bandwidth requirement; Bottleneck problem; Hardware accelerators; High level applications; High throughput; Regular expressions; Special hardware; Unstructured data; Search engines
Introduction to the special issue on ACM international systems and storage conference (SYSTOR) 2018,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065624103&doi=10.1145%2f3313898&partnerID=40&md5=22eeb7c6aa4550406b31ebd32de00fa3,[No abstract available],,
LeRNA: Parallelizing dependent loops using speculation,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065616235&doi=10.1145%2f3310368&partnerID=40&md5=b785d96d98f7fea7f1d9e7b31c34237f,"We present Lerna, an end-to-end tool that automatically and transparently detects and extracts parallelism from data-dependent sequential loops. Lerna uses speculation combined with a set of techniques including code profiling, dependency analysis, instrumentation, and adaptive execution. Speculation is needed to avoid conservative actions and detect actual conflicts. Lerna targets applications that are hard-to-parallelize due to data dependency. Our experimental study involves the parallelization of 13 applications with data dependencies. Results on a 24-core machine show an average of 2.7x speedup for micro-benchmarks and 2.5x for the macro-benchmarks. © 2019 Association for Computing Machinery.",Code parallelization; LLVM; Transactions,Data dependencies; Data dependent; Dependency analysis; LLVM; Micro-benchmark; Parallelizations; Parallelizing; Transactions; Data storage equipment
Liquid cloud storage,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062368810&doi=10.1145%2f3281276&partnerID=40&md5=0e7a7f3b29e44d2706d563cb4ba5a4a0,"A liquid system provides durable object storage based on spreading redundantly generated data across a network of hundreds to thousands of potentially unreliable storage nodes. A liquid system uses a combination of a large code, lazy repair, and flow storage organization. We show that a liquid system can be operated to enable flexible and essentially optimal combinations of storage durability, storage overhead, repair bandwidth usage, and access performance. © 2019 Association for Computing Machinery.",Algorithm design and analysis; Data storage systems; Data warehouses; Distributed algorithms; Distributed information systems; Equipment failure; Error compensation; Error correction codes; Failure analysis; Fault tolerance; Information entropy; Information science; Information theory; Network coding; Redundancy; Reed-Solomon codes; Reliability; Reliability engineering; Reliability theory; Robustness; Signal-to-noise ratio; Throughput; Time-varying channels,Codes (symbols); Data warehouses; Distributed computer systems; Distributed database systems; Error compensation; Failure analysis; Fault tolerance; Information science; Information theory; Liquids; Network coding; Parallel algorithms; Redundancy; Reed-Solomon codes; Reliability; Reliability analysis; Reliability theory; Repair; Robustness (control systems); Signal to noise ratio; Throughput; Algorithm design and analysis; Data storage systems; Distributed information systems; Equipment failures; Error correction codes; Information entropy; Reliability engineering; Time varying channel; Digital storage
On the lifecycle of the file,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062363933&doi=10.1145%2f3295463&partnerID=40&md5=140aa1a1752438ac6d4b0b3c844e3e26,"Users and Operating Systems (OSs) have vastly different views of files. OSs use files to persist data and structured information. To accomplish this, OSs treat files as named collections of bytes managed in hierarchical file systems. Despite their critical role in computing, little attention is paid to the lifecycle of the file, the evolution of file contents, or the evolution of file metadata. In contrast, users have rich mental models of files: they group files into projects, send data repositories to others, work on documents over time, and stash them aside for future use. Current OSs and Revision Control Systems ignore such mental models, persisting a selective, manually designated history of revisions. Preserving the mental model allows applications to better match how users view their files, making file processing and archiving tools more effective. We propose two mechanisms that OSs can adopt to better preserve the mental model: File Lifecycle Events (FLEs) that record a file's progression and Complex File Events (CFEs) that combine them into meaningful patterns. We present the Complex File Events Engine (CoFEE), which uses file system monitoring and an extensible rulebase (Drools) to detect FLEs and convert them into complex ones. CFEs are persisted in NoSQL stores for later querying. © 2019 Copyright held by the owner/author(s).",Complex events; File lifecycle; File relationships; Files; Mental model; Rule-based systems,File organization; Hierarchical systems; Knowledge based systems; Life cycle; Complex events; Data repositories; File processing; File relationships; Files; Mental model; Revision control systems; Structured information; Cognitive systems
Leveraging glocality for fast failure recovery in distributed ram storage,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062333245&doi=10.1145%2f3289604&partnerID=40&md5=066b175112e96926be3188b5467bdcd8,"                             Distributed RAM storage aggregates the RAM of servers in data center networks (DCN) to provide extremely high I/O performance for large-scale cloud systems. For quick recovery of storage server failures, MemCube [53] exploits the proximity of the BCube network to limit the recovery traffic to the recovery servers' 1-hop neighborhood. However, the previous design is applicable only to the symmetric BCube(n, k) network with n                             k                             +                             1                              nodes and has suboptimal recovery performance due to congestion and contention. To address these problems, in this article, we propose CubeX, which (i) generalizes the “1-hop” principle of MemCube for arbitrary cube-based networks and (ii) improves the throughput and recovery performance of RAM-based key-value (KV) store via cross-layer optimizations. At the core of CubeX is to leverage the glocality (= globality + locality) of cube-based networks: It scatters backup data across a large number of disks globally distributed throughout the cube and restricts all recovery traffic within the small local range of each server node. Our evaluation shows that CubeX not only efficiently supports RAM-based KV store for cube-based networks but also significantly outperforms MemCube and RAMCloud in both throughput and recovery time.                          © 2019 Association for Computing Machinery.",Cube-based networks; Distributed RAM storage; Fast recovery; Globality; Locality,Geometry; Random access storage; Recovery; Cross layer optimization; Data center networks; Fast failure recovery; Fast recovery; Globality; Locality; Recovery performance; Storage servers; Computer system recovery
TDDFS: A tier-aware data deduplication-based file system,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062218696&doi=10.1145%2f3295461&partnerID=40&md5=d026b209681be82720418125c5f6239d,"With the rapid increase in the amount of data produced and the development of new types of storage devices, storage tiering continues to be a popular way to achieve a good tradeoff between performance and cost-effectiveness. In a basic two-tier storage system, a storage tier with higher performance and typically higher cost (the fast tier) is used to store frequently-accessed (active) data while a large amount of less-active data are stored in the lower-performance and low-cost tier (the slow tier). Data are migrated between these two tiers according to their activity. In this article, we propose a Tier-aware Data Deduplication-based File System, called TDDFS, which can operate efficiently on top of a two-tier storage environment. Specifically, to achieve better performance, nearly all file operations are performed in the fast tier. To achieve higher cost-effectiveness, files are migrated from the fast tier to the slow tier if they are no longer active, and this migration is done with data deduplication. The distinctiveness of our design is that it maintains the non-redundant (unique) chunks produced by data deduplication in both tiers if possible. When a file is reloaded (called a reloaded file) from the slow tier to the fast tier, if some data chunks of the file already exist in the fast tier, then the data migration of these chunks from the slow tier can be avoided. Our evaluation shows that TDDFS achieves close to the best overall performance among various file-tiering designs for two-tier storage systems. © 2019 Association for Computing Machinery.",Data deduplication; Data migration; File system; Tiered storage,Cost effectiveness; Virtual storage; Active data; Data de duplications; Data migration; File systems; Large amounts; Non-redundant; Storage systems; Tiered storage; File organization
An exploratory study on software-defined data center hard disk drives,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066068691&doi=10.1145%2f3319405&partnerID=40&md5=788703739111b661696ed770623ca864,"This article presents a design framework aiming to reduce mass data storage cost in data centers. Its underlying principle is simple: Assume one may noticeably reduce the HDD manufacturing cost by significantly (i.e., at least several orders of magnitude) relaxing raw HDD reliability, which ensures the eventual data storage integrity via low-cost system-level redundancy. This is called system-assisted HDD bit cost reduction. To better utilize both capacity and random IOPS of HDDs, it is desirable to mix data with complementary requirements on capacity and random IOPS in each HDD. Nevertheless, different capacity and random IOPS requirements may demand different raw HDD reliability vs. bit cost trade-offs and hence different forms of system-assisted bit cost reduction. This article presents a software-centric design framework to realize dataadaptive system-assisted bit cost reduction for data center HDDs. Implementation is solely handled by the filesystem and demands only minor change of the error correction coding (ECC) module inside HDDs. Hence, it is completely transparent to all the other components in the software stack (e.g., applications, OS kernel, and drivers) and keeps fundamental HDD design practice (e.g., firmware, media, head, and servo) intact. We carried out analysis and experiments to evaluate its implementation feasibility and effectiveness. We integrated the design techniques into ext4 to further quantitatively measure its impact on system speed performance. © 2019 Association for Computing Machinery.",error-tolerance; filesystem design; local erasure coding; Reliability,Application programs; Cost reduction; Economic and social effects; Error correction; File organization; Firmware; Hard disk storage; Manufacture; Redundancy; Reliability; Erasure coding; Error correction coding; Error tolerance; Exploratory studies; Filesystem; Manufacturing cost; Orders of magnitude; Underlying principles; Data reduction
Crashmonkey and Ace: Systematically testing file-system crash consistency,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065606889&doi=10.1145%2f3320275&partnerID=40&md5=fb5aa4b414f1dbe36f032e49ccbf52f1,"We present CrashMonkey and Ace, a set of tools to systematically find crash-consistency bugs in Linux file systems. CrashMonkey is a record-and-replay framework which tests a given workload on the target file system by simulating power-loss crashes while the workload is being executed, and checking if the file system recovers to a correct state after each crash. Ace automatically generates all the workloads to be run on the target file system. We build CrashMonkey and Ace based on a new approach to test file-system crash consistency: bounded black-box crash testing (B3). B3 tests the file system in a black-box manner using workloads of file-system operations. Since the space of possible workloads is infinite, B3 bounds this space based on parameters such as the number of file-system operations or which operations to include, and exhaustively generates workloads within this bounded space. B3 builds upon insights derived from our study of crash-consistency bugs reported in Linux file systems in the last 5 years. We observed that most reported bugs can be reproduced using small workloads of three or fewer file-system operations on a newly created file system, and that all reported bugs result from crashes after fsync()-related system calls. CrashMonkey and Ace are able to find 24 out of the 26 crash-consistency bugs reported in the last 5 years. Our tools also revealed 10 new crash-consistency bugs in widely used, mature Linux file systems, 7 of which existed in the kernel since 2014. Additionally, our tools found a crash-consistency bug in a verified file system, FSCQ. The new bugs result in severe consequences like broken rename atomicity, loss of persisted files and directories, and data loss. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bugs; Crash consistency; File systems; Testing,Black-box testing; Crashworthiness; File organization; Linux; Testing; Bounded spaces; Bugs; Crash consistency; File systems; Linux file system; New approaches; Record-and-replay; Related systems; Program debugging
CGraph: A distributed storage and processing system for concurrent iterative graph analysis jobs,2019,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065607612&doi=10.1145%2f3319406&partnerID=40&md5=6c620fa0045ce0812040c1cd7d5ec487,"Distributed graph processing platforms usually need to handle massive Concurrent iterative Graph Processing (CGP) jobs for different purposes. However, existing distributed systems face high ratio of data access cost to computation for the CGP jobs, which incurs low throughput. We observed that there are strong spatial and temporal correlations among the data accesses issued by different CGP jobs, because these concurrently running jobs usually need to repeatedly traverse the shared graph structure for the iterative processing of each vertex. Based on this observation, this article proposes a distributed storage and processing system CGraph for the CGP jobs to efficiently handle the underlying static/evolving graph for high throughput. It uses a data-centric load-trigger-pushing model, together with several optimizations, to enable the CGP jobs to efficiently share the graph structure data in the cache/memory and their accesses by fully exploiting such correlations, where the graph structure data is decoupled from the vertex state associated with each job. It can deliver much higher throughput for the CGP jobs by effectively reducing their average ratio of data access cost to computation. Experimental results show that CGraph improves the throughput of the CGP jobs by up to 3.47× in comparison with existing solutions on distributed platforms. © 2019 Association for Computing Machinery.",Data access correlations; Data access cost; Throughput,Digital storage; Graph theory; Graphic methods; Throughput; Data access; Distributed platforms; Distributed storage; Distributed systems; Graph processing; Iterative processing; Processing systems; Spatial and temporal correlation; Distributed computer systems
HIL: A framework for compositional FTL development and provably-correct crash recovery,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058276085&doi=10.1145%2f3281030&partnerID=40&md5=aea5e40793db357cda248716175f29ad,"We present a framework called Hierarchically Interacting Logs (HIL) for constructing Flash Translation Layers (FTLs). The main goal of the HIL framework is to heal the Achilles heel -the crash recovery-of FTLs (hence, its name). Nonetheless, the framework itself is general enough to encompass not only block-mapped and page-mapped FTLs but also many of their variants, including hybrid ones, because of its compositional nature. Crash recovery within the HIL framework proceeds in two phases: structural recovery and functional recovery. During the structural recovery, residual effects due to program operations ongoing at the time of the crash are eliminated in an atomic manner using shadow paging. During the functional recovery, operations that would have been performed if there had been no crash are replayed in a redo-only fashion. Both phases operate in an idempotent manner, preventing repeated crashes during recovery from causing any additional problems. We demonstrate the practicality of the proposed HIL framework by implementing a prototype and showing that its performance during normal execution and also during crash recovery is at least as good as those of state-of-the-art SSDs. © 2018 Association for Computing Machinery.",,Data storage equipment; Hardware; Achilles heel; Crash recoveries; Flash translation layer; Functional recovery; Program operation; Residual effects; Shadow paging; State of the art; Recovery
LibPM: Simplifying application usage of persistent memory,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058315493&doi=10.1145%2f3278141&partnerID=40&md5=e8d82eed504e753fb0902865bbd21068,"Persistent Memory devices present properties that are uniquely different from prior technologies for which applications have been built. Unfortunately, the conventional approach to building applications fail to either efficiently utilize these new devices or provide programmers a seamless development experience. We have built LibPM, a Persistent Memory Library that implements an easy-to-use container abstraction for consuming PM. LibPM's containers are data hosting units that can store arbitrarily complex data types while preserving their integrity and consistency. Consequently, LibPM's containers provide a generic interface to applications, allowing applications to store and manipulate arbitrarily structured data with strong durability and consistency properties, all without having to navigate all the myriad pitfalls of programming PM directly. By providing a simple and high-performing transactional update mechanism, LibPM allows applications to manipulate persistent data at the speed of memory. The container abstraction and automatic persistent data discovery mechanisms within LibPM also simplify porting legacy applications to PM. From a performance perspective, LibPM closely matches and often exceeds the performance of state-of-the-art application libraries for PM. For instance, LibPM 's performance is 195× better for write intensive workloads and 2.6× better for read intensive workloads when compared with the state-of-the-art Pmem.IO persistent memory library. © 2018 Association for Computing Machinery.",Application persistence; Next-generation applications; Persistent memory,Abstracting; Building applications; Consistency property; Conventional approach; Development experiences; Legacy applications; Next-Generation Applications; Persistent memory; Write-intensive workloads; Containers
Management of next-generation NAND flash to achieve enterprise-level endurance and latency targets,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058329166&doi=10.1145%2f3241060&partnerID=40&md5=dd5c36da800b6a9d6c974bf5b1e0dee2,"Despite its widespread use in consumer devices and enterprise storage systems, NAND flash faces a growing number of challenges. While technology advances have helped to increase the storage density and reduce costs, they have also led to reduced endurance and larger block variations, which cannot be compensated solely by stronger ECC or read-retry schemes but have to be addressed holistically. Our goal is to enable low-cost NAND flash in enterprise storage for cost efficiency. We present novel flash-management approaches that reduce write amplification, achieve better wear leveling, and enhance endurance without sacrificing performance. We introduce block calibration, a technique to determine optimal read-threshold voltage levels that minimize error rates, and novel garbage-collection as well as dataplacement schemes that alleviate the effects of block health variability and show how these techniques complement one another and thereby achieve enterprise storage requirements. By combining the proposed schemes, we improve endurance by up to 15× compared to the baseline endurance of NAND flash without using a stronger ECC scheme. The flash-management algorithms presented herein were designed and implemented in simulators, hardware test platforms, and eventually in the flash controllers of production enterprise all-flash arrays. Their effectiveness has been validated across thousands of customer deployments since 2015. 2018 Copyright is held by the owner/author(s). © 2018 Association for Computing Machinery.",Data placement; Endurance; Flash memory; Level shifting; NVM controller design; NVM-based storage; Wear leveling,Durability; Memory architecture; NAND circuits; Threshold voltage; Wear of materials; Controller designs; Data placement; Garbage collection; Level-shifting; Storage requirements; Technology advances; Wear leveling; Write amplifications; Flash memory
Towards robust file system checkers,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058320950&doi=10.1145%2f3281031&partnerID=40&md5=b3ca1703b4714162092eec3576babe46,"File systems may become corrupted for many reasons despite various protection techniques. Therefore, most file systems come with a checker to recover the file system to a consistent state. However, existing checkers are commonly assumed to be able to complete the repair without interruption, which may not be true in practice. In this work, we demonstrate via fault injection experiments that checkers of widely used file systems (EXT4, XFS, BtrFS, and F2FS) may leave the file system in an uncorrectable state if the repair procedure is interrupted unexpectedly. To address the problem, we first fix the ordering issue in the undo logging of e2fsck and then build a general logging library (i.e., rfsck-lib) for strengthening checkers. To demonstrate the practicality, we integrate rfsck-lib with existing checkers and create two new checkers: rfsck-ext, a robust checker for Ext-family file systems, and rfsck-xfs, a robust checker for XFS file systems, both of which require only tens of lines of modification to the original versions. Both rfsck-ext and rfsck-xfs are resilient to faults in our experiments. Also, both checkers incur reasonable performance overhead (i.e., up to 12%) compared to the original unreliable versions. Moreover, rfsck-ext outperforms the patched e2fsck by up to nine times while achieving the same level of robustness. © 2018 Association for Computing Machinery.",Data corruption; Fault tolerance; File-system faults,Fault tolerance; Repair; Consistent state; Data corruption; Fault injection; File system checkers; File systems; Protection techniques; File organization
Introduction to the special issue on SYSTOR 2017,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061198962&doi=10.1145%2f3287097&partnerID=40&md5=d1b2f6505d1f4c9bda304d8ae731ea2b,[No abstract available],,
FlashNet: Flash/network stack co-design,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058329051&doi=10.1145%2f3239562&partnerID=40&md5=5d8803973338e4ff4156d943a0d97e24,"During the past decade, network and storage devices have undergone rapid performance improvements, delivering ultra-lowlatency and several Gbps of bandwidth. Nevertheless, current network and storage stacks fail to deliver this hardware performance to the applications, often due to the loss of I/O efficiency from stalled CPU performance. While many efforts attempt to address this issue solely on either the network or the storage stack, achieving high-performance for networked-storage applications requires a holistic approach that considers both. In this article, we present FlashNet, a software I/O stack that unifies high-performance network properties with flash storage access andmanagement. FlashNet builds on RDMA principles and abstractions to provide a direct, asynchronous, end-to-end data path between a client and remote flash storage. The key insight behind FlashNet is to co-design the stack's components (an RDMA controller, a flash controller, and a file system) to enable cross-stack optimizations and maximize I/O efficiency. In micro-benchmarks, FlashNet improves 4kB network I/O operations per second (IOPS by 38.6% to 1.22M, decreases access latency by 43.5% to 50.4μs, and prolongs the flash lifetime by 1.6-5.9× for writes. We illustrate the capabilities of FlashNet by building a Key-Value store and porting a distributed data store that uses RDMA on it. The use of FlashNet's RDMA API improves the performance of KV store by 2× and requires minimum changes for the ported data store to access remote flash devices. 2018 Copyright is held by the owner/author(s). © 2018 Association for Computing Machinery.",Flash; Network storage; Operating systems; Performance; RDMA,Computer operating systems; Virtual storage; Distributed data stores; Flash; Hardware performance; High performance networks; Network storage; Performance; Performance improvements; RDMA; Efficiency
Performance characterization of NVMe-over-fabrics storage disaggregation,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058293717&doi=10.1145%2f3239563&partnerID=40&md5=a2f6fd4557f9dbcd65e56a28d82aefc8,"Storage disaggregation separates compute and storage to different nodes to allow for independent resource scaling and, thus, better hardware resource utilization. While disaggregation of hard-drives storage is a common practice, NVMe-SSD (i.e., PCIe-based SSD) disaggregation is considered more challenging. This is because SSDs are significantly faster than hard drives, so the latency overheads (due to both network and CPU processing) as well as the extra compute cycles needed for the offloading stack become much more pronounced. In this work, we characterize the overheads of NVMe-SSD disaggregation. We show that NVMe-over- Fabrics (NVMe-oF)-a recently released remote storage protocol specification-reduces the overheads of remote access to a bare minimum, thus greatly increasing the cost-efficiency of Flash disaggregation. Specifically, while recent work showed that SSD storage disaggregation via iSCSI degrades application-level throughput by 20%, we report on negligible performance degradation with NVMe-oF-both when using stress-tests as well as with a more-realistic KV-store workload. Copyright 2018 held by Owner/Author.",Network storage; NVMe-over-fabrics; Performance characterization,Data storage equipment; Hardware; Application level; Common practices; Cost efficiency; Hardware resource utilization; Network storage; Performance characterization; Performance degradation; Protocol specifications; Hard disk storage
Exploiting internal parallelism for address translation in solid-state drives,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061204944&doi=10.1145%2f3239564&partnerID=40&md5=c09ea6b626fd2e1a8ecc0011b545d307,"Solid-state Drives (SSDs) have changed the landscape of storage systems and present a promising storage solution for data-intensive applications due to their low latency, high bandwidth, and low power consumption compared to traditional hard disk drives. SSDs achieve these desirable characteristics using internal parallelism-parallel access to multiple internal flash memory chips-and a Flash Translation Layer (FTL) that determines where data are stored on those chips so that they do not wear out prematurely. However, current state-of-the-art cache-based FTLs like the Demand-based Flash Translation Layer (DFTL) donot allow IO schedulers to take full advantage of internal parallelism, because they impose a tight coupling between the logical-to-physical address translation and the data access. To address this limitation, we introduce a new FTL design called Parallel-DFTL that works with the DFTL to decouple address translation operations from data accesses. Parallel-DFTL separates address translation and data access operations into different queues, allowing the SSD to use concurrent flash accesses for both types of operations. We also present a Parallel-LRU cache replacement algorithm to improve the concurrency of address translation operations. To compare Parallel-DFTL against existing FTL approaches, we present a Parallel-DFTL performance model and compare its predictions against those for DFTL and an ideal page-mapping approach. We also implemented the Parallel-DFTL approach in an SSD simulator using real device parameters, and used trace-driven simulation to evaluate Parallel-DFTL's efficacy. Our evaluation results show that Parallel-DFTL improved the overall performance by up to 32% for the real IO workloads we tested, and by up to two orders of magnitude with synthetic test workloads. We also found that Parallel-DFTL is able to achieve reasonable performance with a very small cache size and that it provides the best benefit for those workloads with large request size or with high write ratio. © 2018 Association for Computing Machinery.",Address translation; DFTL; Flash translation layer; Parallelism; SSD,Drives; Hard disk storage; Physical addresses; Address translation; Cache replacement algorithm; Data-intensive application; DFTL; Flash translation layer; Low-power consumption; Parallelism; Trace driven simulation; Flash-based SSDs
DIDACache: An integration of device and application for flash-based key-value caching,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061212732&doi=10.1145%2f3203410&partnerID=40&md5=d8b9652704e013ba16f7b196c286f7e1,"Key-value caching is crucial to today's low-latency Internet services. Conventional key-value cache systems, such as Memcached, heavily rely on expensive DRAM memory. To lower Total Cost of Ownership, the industry recently is moving toward more cost-eficient flash-based solutions, such as Facebook's McDipper [14] and Twitter's Fatcache [56]. These cache systems typically take commercial SSDs and adopt a Memcached-like scheme to store and manage key-value cache data in flash. Such a practice, though simple, is ineficient due to the huge semantic gap between the key-value cache manager and the underlying flash devices. In this article, we advocate to reconsider the cache system design and directly open device-level details of the underlying flash storage for key-value caching. We propose an enhanced flash-aware key-value cache manager, which consists of a novel unified address mapping module, an integrated garbage collection policy, a dynamic over-provisioning space management, and a customized wear-leveling policy, to directly drive the flash management. A thin intermediate library layer provides a slab-based abstraction of low-level flash memory space and an API interface for directly and easily operating flash devices. A special flash memory SSD hardware that exposes flash physical details is adopted to store key-value items. This co-design approach bridges the semantic gap and well connects the two layers together, which allows us to leverage both the domain knowledge of key-value caches and the unique device properties. In this way, we can maximize the eficiency of key-value caching on flash devices while minimizing its weakness. We implemented a prototype, called DIDACache, based on the Open-Channel SSD platform. Our experiments on real hardware show that we can significantly increase the throughput by 35.5%, reduce the latency by 23.6%, and remove unnecessary erase operations by 28%. © 2018 Association for Computing Machinery.",Key-value caching; NAND flash memory; Open-channel SSD,Application programming interfaces (API); Bridges; Cache memory; Costs; Dynamic random access storage; Knowledge management; Managers; Semantics; Social networking (online); Co-design approach; Garbage collection; Internet services; Key values; NAND flash memory; Open channels; Over provisioning; Total cost of ownership; Flash-based SSDs
Write energy reduction for PCM via pumping efficiency improvement,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061206990&doi=10.1145%2f3200139&partnerID=40&md5=f4533f8aa2874e3ad76f387288f5979b,"The emerging Phase Change Memory (PCM) is considered to be a promising candidate to replace DRAM as the next generation main memory due to its higher scalability and lower leakage power. However, the high write power consumption has become a major challenge in adopting PCM as main memory. In addition to the fact that writing to PCM cells requires high write current and voltage, current loss in the charge pumps also contributes a large percentage of high power consumption. The pumping eficiency of a PCM chip is a concave function of the write current. Leveraging the characteristics of the concave function, the overall pumping eficiency can be improved if the write current is uniform. In this article, we propose a peak-to-average (PTA) write scheme, which smooths the write current fluctuation by regrouping write units. In particular, we calculate the current requirements for each write unit by their values when they are evicted from the last level cache (LLC). When the write units are waiting in the memory controller, we regroup the write units by LLC-assisted PTA to reach the current-uniform goal. Experimental results show that LLC-assisted PTA achieved 13.4% of overall energy saving compared to the baseline. © 2018 Association for Computing Machinery.",Charge pump; Phase change memory (PCM); Pumping eficiency; Write regrouping,Charge pump circuits; Dynamic random access storage; Electric power utilization; Energy conservation; Energy efficiency; Molecular physics; Charge pump; Eficiency; High power consumption; Lastlevel caches (LLC); Memory controller; Phase change memory (pcm); Pumping efficiency; Write regrouping; Phase change memory
Efficient directory mutations in a full-path-indexed file system,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061181366&doi=10.1145%2f3241061&partnerID=40&md5=e04c3ae4b06fc68c0440344a53929657,"                             Full-path indexing can improve I/O eficiency for workloads that operate on data organized using traditional, hierarchical directories, because data is placed on persistent storage in scan order. Prior results indicate, however, that renames in a local file system with full-path indexing are prohibitively expensive. This article shows how to use full-path indexing in a file system to realize fast directory scans, writes, and renames. The article introduces a range-rename mechanism for eficient key-space changes in a write-optimized dictionary. This mechanism is encapsulated in the key-value Application Programming Interface (API) and simplifies the overall file system design. We implemented this mechanism in B                             ϵ                             -trees File System (BetrFS), an in-kernel, local file system for Linux. This new version, BetrFS 0.4, performs recursive greps 1.5x faster and random writes 1.2x faster than BetrFS 0.3, but renames are competitive with indirection-based file systems for a range of sizes. BetrFS 0.4 outperforms BetrFS 0.3, as well as traditional file systems, such as ext4, Extents File System (XFS), and Z File System (ZFS), across a variety of workloads.                          © 2018 Association for Computing Machinery.",,Application programming interfaces (API); Computer operating systems; Digital storage; File organization; Indexing (of information); Eficiency; File systems; Key values; Local file systems; Path indexing; Persistent storage; Space change; Computer systems programming
Bringing order to chaos: Barrier-enabled I/O stack for flash storage,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061198235&doi=10.1145%2f3242091&partnerID=40&md5=5afdcfb1684e0a7c54024f327d5134e3,"This work is dedicated to eliminating the overhead required for guaranteeing the storage order in the modern IO stack. The existing block device adopts a prohibitively expensive approach in ensuring the storage order among write requests: interleaving the write requests with Transfer-and-Flush. For exploiting the cache barrier command for flash storage, we overhaul the IO scheduler, the dispatch module, and the filesystem so that these layers are orchestrated to preserve the ordering condition imposed by the application with which the associated data blocks are made durable. The key ingredients of Barrier-Enabled IO stack are Epoch-based IO scheduling, Order-Preserving Dispatch, and Dual-Mode Journaling. Barrier-enabled IO stack can control the storage order without Transfer-and-Flush overhead. We implement the barrier-enabled IO stack in server as well as in mobile platforms. SQLite performance increases by 270% and 75%, in server and in smartphone, respectively. In a server storage, BarrierFS brings as much as by 43× and by 73× performance gain in MySQL and SQLite, respectively, against EXT4 via relaxing the durability of a transaction. © 2018 Association for Computing Machinery.",Block device; Filesystem; Linux; Storage,Computer operating systems; Energy storage; File organization; Linux; Scheduling; Block devices; Filesystem; Flash storage; IO scheduler; Mobile platform; Order conditions; Order preserving; Performance Gain; Digital storage
Introduction to the special issue on USENIX FAST 2018,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061190841&doi=10.1145%2f3242152&partnerID=40&md5=5809a915f8400ad832baead137af343b,[No abstract available],,
ROS: A rack-based optical storage system with inline accessibility for long-term data preservation,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061179303&doi=10.1145%2f3231599&partnerID=40&md5=d3790a623e0cf32fdf129d71db2928d1,"The combination of the explosive growth in digital data and the demand to preserve much of these data in the long term has made it imperative to find a more cost-effective way than HDD arrays and a more easily accessible way than tape libraries to store massive amounts of data. While modern optical discs are capable of guaranteeing more than 50-year data preservation without media replacement, individual optical discs' lack of the performance and capacity relative to HDDs or tapes has significantly limited their use in datacenters. This article presents a Rack-scale Optical disc library System, or ROS in short, which provides a PB-level total capacity and inline accessibility on thousands of optical discs built within a 42U Rack. A rotatable roller and robotic arm separating and fetching discs are designed to improve disc placement density and simplify the mechanical structure. A hierarchical storage system based on SSDs, hard disks, and optical discs is proposed to effectively hide the delay of mechanical operation. However, an optical library file system (OLFS) based on FUSE is proposed to schedule mechanical operation and organize data on the tiered storage with a POSIX user interface to provide an illusion of inline data accessibility. We further optimize OLFS by reducing unnecessary user/kernel context switches inheriting from legacy FUSE framework. We evaluate ROS on a few key performance metrics, including operation delays of the mechanical structure and software overhead in a prototype PB-level ROS system. The results show that ROS stacked on Samba and FUSE as network-attached storage (NAS) mode almost saturates the throughput provided by underlying samba via 10GbE network for external users, as well as in this scenario provides about 53ms file write and 15ms read latency, exhibiting its inline accessibility. Besides, ROS is able to effectively hide and virtualize internal complex operational behaviors and be easily deployable in datacenters. © 2018 Association for Computing Machinery.",Archive storage; File system; Hierarchical storage; Optical disc; Storage management,Compact disks; Cost effectiveness; Digital libraries; Hard disk storage; Optical disk storage; Software prototyping; User interfaces; Archive storage; File systems; Hierarchical storage; Hierarchical storage system; Mechanical operations; Network attached storage; Optical discs; Optical storage systems; Storage management
Protocol-aware recovery for consensus-based distributed storage,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061192560&doi=10.1145%2f3241062&partnerID=40&md5=ccb06b3879a4effe3c4978800314369f,"We introduce protocol-aware recovery (Par), a new approach that exploits protocol-specific knowledge to correctly recover from storage faults in distributed systems. We demonstrate the eficacy of Par through the design and implementation of corruption-tolerant replication (Ctrl), a Par mechanism specific to replicated state machine (RSM) systems. We experimentally show that the Ctrl versions of two systems, LogCabin and ZooKeeper, safely recover from storage faults and provide high availability, while the unmodified versions can lose data or become unavailable. We also show that the Ctrl versions achieve this reliability with little performance overheads. © 2018 Association for Computing Machinery.",Consensus; Data corruption; Fault tolerance; Storage faults,Crime; Fault tolerance; Fault tolerant computer systems; Recovery; Consensus; Data corruption; Design and implementations; Distributed storage; Distributed systems; High availability; New approaches; Specific knowledge; Digital storage
M-CLOCK: Migration-optimized page replacement algorithm for hybrid memory architecture,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061175426&doi=10.1145%2f3216730&partnerID=40&md5=66951a426309ed423726f3d4fcbde305,"Phase Change Memory (PCM) has drawn great attention as a main memory due to its attractive characteristics such as non-volatility, byte-addressability, and in-place update. However, since the capacity of PCM is not fully mature yet, hybrid memory architecture that consists of DRAM and PCM has been suggested as a main memory. In addition, page replacement algorithm based on hybrid memory architecture is actively being studied, because existing page replacement algorithms cannot be used on hybrid memory architecture in that they do not consider the two weaknesses of PCM: high write latency and low endurance. In this article, to mitigate the above hardware limitations of PCM, we revisit the page cache layer for the hybrid memory architecture and propose a novel page replacement algorithm, called M-CLOCK, to improve the performance of hybrid memory architecture and the lifespan of PCM. In particular, M-CLOCK aims to reduce the number of PCM writes that negatively affect the performance of hybrid memory architecture. Experimental results clearly show that M-CLOCK outperforms the state-of-the-art page replacement algorithms in terms of the number of PCM writes and effective memory access time by up to 98% and 9.4 times, respectively. © 2018 Association for Computing Machinery.",Hybrid memory architecture; Page replacement algorithm; Phase change memory,Cache memory; Clocks; Dynamic random access storage; Phase change memory; In-place update; Life span; Main memory; Memory access time; Nonvolatility; Page replacement algorithms; Phase change memory (pcm); State of the art; Memory architecture
Fail-slow at scale: Evidence of hardware performance faults in large production systems,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061206313&doi=10.1145%2f3242086&partnerID=40&md5=5d059680c1a668c7c184c5e17520c06e,"Fail-slow hardware is an under-studied failure mode. We present a study of 114 reports of fail-slow hardware incidents, collected from large-scale cluster deployments in 14 institutions. We show that all hardware types such as disk, SSD, CPU, memory, and network components can exhibit performance faults. We made several important observations such as faults convert from one form to another, the cascading root causes and impacts can be long, and fail-slow faults can have varying symptoms. From this study, we make suggestions to vendors, operators, and systems designers. © 2018 ACM.",,Hardware performance; Large-scale clusters; Production system; Root cause; Data storage equipment
ClfB-tree: Cacheline friendly persistent B-tree for NVRAM,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042936056&doi=10.1145%2f3129263&partnerID=40&md5=417157fae16a43161bdd7b9f95f96961,"Emerging byte-addressable non-volatile memory (NVRAM) is expected to replace block device storages as an alternative low-latency persistent storage device. If NVRAM is used as a persistent storage device, a cache line instead of a disk page will be the unit of data transfer, consistency, and durability. In this work, we design and develop clfB-tree—a B-tree structure whose tree node fits in a single cache line. We employ existing write combining store buffer and restricted transactional memory to provide a failure-atomic cache line write operation. Using the failure-atomic cache line write operations, we atomically update a clfB-tree node via a single cache line flush instruction without major changes in hardware. However, there exist many processors that do not provide SW interface for transactional memory. For those processors, our proposed clfB-tree achieves atomicity and consistency via in-place update, which requires maximum four cache line flushes. We evaluate the performance of clfB-tree on an NVRAM emulation board with ARM Cortex A-9 processor and a workstation that has Intel Xeon E7-4809 v3 processor. Our experimental results show clfB-tree outperforms wB-tree and CDDS B-tree by a large margin in terms of both insertion and search performance. © 2018 ACM 1553-3077/2018/02-ART5 $15.00",Data structure; Non-volatile memory; Persistent indexing,Data storage equipment; Data structures; Data transfer; Digital storage; Nonvolatile storage; Storage allocation (computer); Trees (mathematics); Virtual storage; Block devices; In-place update; Large margins; Non-volatile memory; Persistent storage; Search performance; Transactional memory; Write operations; Cache memory
UnistorFS: A union storage file system design for resource sharing between memory and storage on persistent RAM-based systems,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042942606&doi=10.1145%2f3177918&partnerID=40&md5=7474bf28b2338d843bcf4575d611ca2e,"With the advanced technology in persistent random access memory (PRAM), PRAM such as three-dimensional XPoint memory and Phase Change Memory (PCM) is emerging as a promising candidate for the next-generation medium for both (main) memory and storage. Previous works mainly focus on how to overcome the possible endurance issues of PRAM while both main memory and storage own a partition on the same PRAM device. However, a holistic software-level system design should be proposed to fully exploit the benefit of PRAM. This article proposes a union storage file system (UnistorFS), which aims to jointly manage the PRAM resource for main memory and storage. The proposed UnistorFS realizes the concept of using the PRAM resource as memory and storage interchangeably to achieve resource sharing while main memory and storage coexist on the same PRAM device with no partition or logical boundary. This approach not only enables PRAM resource sharing but also eliminates unnecessary data movements between main memory and storage since they are already in the same address space and can be accessed directly. At the same time, the proposed UnistorFS ensures the persistence of file data and sanity of the file system after power recycling. A series of experiments was conducted on a modified Linux kernel. The results show that the proposed UnistorFS can eliminate unnecessary memory accesses and outperform other PRAM-based file systems for 0.2–8.7 times in terms of read/write performance. © 2018 ACM",Embedded system; FAT; File mapping; NVM; Page cache,Computer operating systems; Digital storage; Embedded systems; File organization; Oils and fats; Phase change memory; Systems analysis; Advanced technology; File mappings; Generation mediums; Page caches; Phase change memory (pcm); Random access memory; Read/write performance; Resource sharing; Random access storage
An analysis of flash page reuse with WOM codes,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042947620&doi=10.1145%2f3177886&partnerID=40&md5=cf5299fcd7a48ba2352a78f3240cff5f,"Flash memory is prevalent in modern servers and devices. Coupled with the scaling down of flash technology, the popularity of flash memory motivates the search for methods to increase flash reliability and lifetime. Erasures are the dominant cause of flash cell wear, but reducing them is challenging because flash is a write-once medium— memory cells must be erased prior to writing. An approach that has recently received considerable attention relies on write-once memory (WOM) codes, designed to accommodate additional writes on write-once media. However, the techniques proposed for reusing flash pages with WOM codes are limited in their scope. Many focus on the coding theory alone, whereas others suggest FTL designs that are application specific, or not applicable due to their complexity, overheads, or specific constraints of multilevel cell (MLC) flash. This work is the first that addresses all aspects of page reuse within an end-to-end analysis of a general-purpose FTL on MLC flash. We use a hardware evaluation setup to directly measure the short- and long-term effects of page reuse on SSD durability and energy consumption, and show that FTL design must explicitly take them into account. We then provide a detailed analytical model for deriving the optimal garbage collection policy for such FTL designs, and for predicting the benefit from reuse on realistic hardware and workload characteristics. © 2018 ACM 1553-3077/2018/02-ART10 $15.00",Flash translation layer; NAND flash; Offline analysis; SSD; WOM codes,Codes (symbols); Computer hardware; Computer programming; Energy utilization; Hardware; ROM; Application specific; Flash translation layer; Hardware evaluations; Multilevel cell flashes; NAND Flash; Off-line analysis; WOM codes; Workload characteristics; Flash memory
Introduction to the special Issue on NVM and storage,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042935067&doi=10.1145%2f3180480&partnerID=40&md5=974d2c6e0f7e5f9f0f9a1bed6529e510,[No abstract available],,
A novel ReRAM-based processing-in-memory architecture for graph traversal,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042921991&doi=10.1145%2f3177916&partnerID=40&md5=5c69dd54e56ac0c3993a5630b789209f,"Graph algorithms such as graph traversal have been gaining ever-increasing importance in the era of big data. However, graph processing on traditional architectures issues many random and irregular memory accesses, leading to a huge number of data movements and the consumption of very large amounts of energy. To minimize the waste of memory bandwidth, we investigate utilizing processing-in-memory (PIM), combined with non-volatile metal-oxide resistive random access memory (ReRAM), to improve both computation and I/O performance. We propose a new ReRAM-based processing-in-memory architecture called RPBFS, in which graph data can be persistently stored and processed in place. We study the problem of graph traversal, and we design an efficient graph traversal algorithm in RPBFS. Benefiting from low data movement overhead and high bank-level parallel computation, RPBFS shows a significant performance improvement compared with both the CPU-based and the GPU-based BFS implementations. On a suite of real-world graphs, our architecture yields a speedup in graph traversal performance of up to 33.8×, and achieves a reduction in energy over conventional systems of up to 142.8×. © 2018 ACM",Architecture; BFS; Processing-in-memory; ReRAM,Architecture; Big data; Metals; Nonvolatile storage; RRAM; Conventional systems; Memory bandwidths; Parallel Computation; Performance improvements; Processing in memory; Real-world graphs; Resistive Random Access Memory (ReRAM); Traditional architecture; Memory architecture
Persisting RB-tree into NVM in a consistency perspective,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042947968&doi=10.1145%2f3177915&partnerID=40&md5=d999ea0d28adc9091fd92e1bc7ae0cd0,"Byte-addressable non-volatile memory (NVM) is going to reshape conventional computer systems. With advantages of low latency, byte-addressability, and non-volatility, NVM can be directly put on the memory bus to replace DRAM. As a result, both system and application softwares have to be adjusted to perceive the fact that the persistent layer moves up to the memory. However, most of the current in-memory data structures will be problematic with consistency issues if not well tuned with NVM. This article places emphasis on an important in-memory structure that is widely used in computer systems, i.e., the Red/Black-tree (RB-tree). Since it has a long and complicated update process, the RB-tree is prone to inconsistency problems with NVM. This article presents an NVM-compatible consistent RB-tree with a new technique named cascade-versioning. The proposed RB-tree (i) is all-time consistent and scalable and (ii) needs no recovery procedure after system crashes. Experiment results show that the RB-tree for NVM not only achieves the aim of consistency with insignificant spatial overhead but also yields comparable performance to an ordinary volatile RB-tree. © 2018 ACM",Cascade versioning; Red-black tree,Data storage equipment; Digital storage; Conventional computers; Memory structure; Non-volatile memory; Nonvolatility; Recovery procedure; Red black tree; System crashes; Versioning; Dynamic random access storage
Editor-in chief letter,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042936755&doi=10.1145%2f3180478&partnerID=40&md5=01b57301493af1817ae2596bfc5ce125,[No abstract available],,
Workload characterization for enterprise disk drives,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064536867&doi=10.1145%2f3151847&partnerID=40&md5=e637b62ceb2e5c1bc76b10bd413e57c7,"The article presents an analysis of drive workloads from enterprise storage systems. The drive workloads are obtained from field return units from a cross-section of enterprise storage system vendors and thus provides a view of the workload characteristics over a wide spectrum of end-user applications. The workload parameters that have been characterized include transfer lengths, access patterns, throughput, and utilization. The study shows that reads are the dominant workload accounting for 80% of the accesses to the drive. Writes are dominated by short block random accesses while reads range from random to highly sequential. A trend analysis over the period 2010–2014 shows that the workload has remained fairly constant even as the capacities of the drives shipped has steadily increased. The study shows that the data stored on disk drives is relatively cold—on average less than 4% of the drive capacity is accessed in a given 2h interval. © 2018 ACM",Enterprise storage systems; Storage workload characterization,Drives; Access patterns; Drive capacity; End-user applications; Storage systems; Transfer lengths; Trend analysis; Workload characteristics; Workload characterization; Digital storage
"Characterizing 3D floating gate NAND flash: Observations, analyses, and implications",2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062242664&doi=10.1145%2f3162616&partnerID=40&md5=e7607c55e8af2fbc1cf38c1c05b4bd17,"As both NAND flash memory manufacturers and users are turning their attentions from planar architecture towards three-dimensional (3D) architecture, it becomes critical and urgent to understand the characteristics of 3D NAND flash memory. These characteristics, especially those different from planar NAND flash, can significantly affect design choices of flash management techniques. In this article, we present a characterization study on the state-of-the-art 3D floating gate (FG) NAND flash memory through comprehensive experiments on an FPGA-based 3D NAND flash evaluation platform. We make distinct observations on its performance and reliability, such as operation latencies and various error patterns, followed by careful analyses from physical and circuit-level perspectives. Although 3D FG NAND flash provides much higher storage densities than planar NAND flash, it faces new performance challenges of garbage collection overhead and program performance variations and more complicated reliability issues due to, e.g., distinct location dependence and value dependence of errors. We also summarize the differences between 3D FG NAND flash and planar NAND flash and discuss implications on the designs of NAND flash management techniques brought by the architecture innovation. We believe that our work will facilitate developing novel 3D FG NAND flash-oriented designs to achieve better performance and reliability. © 2018 ACM.",3D floating gate NAND flash; Error pattern; MLC,Errors; Flash memory; Industrial management; NAND circuits; Reliability; 3-d nand flash memory; Characterization studies; Error patterns; Floating gates; Higher storage density; Performance and reliabilities; Performance challenges; Three dimensional (3D) architectures; Memory architecture
OrcFS: Orchestrated file system for flash storage,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060291400&doi=10.1145%2f3162614&partnerID=40&md5=ad2556cb0c5e706049a35c7e3736b232,"In this work, we develop the Orchestrated File System (OrcFS) for Flash storage. OrcFS vertically integrates the log-structured file system and the Flash-based storage device to eliminate the redundancies across the layers. A few modern file systems adopt sophisticated append-only data structures in an effort to optimize the behavior of the file system with respect to the append-only nature of the Flash memory. While the benefit of adopting an append-only data structure seems fairly promising, it makes the stack of software layers full of unnecessary redundancies, leaving substantial room for improvement. The redundancies include (i) redundant levels of indirection (address translation), (ii) duplicate efforts to reclaim the invalid blocks (i.e., segment cleaning in the file system and garbage collection in the storage device), and (iii) excessive over-provisioning (i.e., separate over-provisioning areas in each layer). OrcFS eliminates these redundancies via distributing the address translation, segment cleaning (or garbage collection), bad block management, and wear-leveling across the layers. Existing solutions suffer from high segment cleaning overhead and cause significant write amplification due to mismatch between the file system block size and the Flash page size. To optimize the I/O stack while avoiding these problems, OrcFS adopts three key technical elements. First, OrcFS uses disaggregate mapping, whereby it partitions the Flash storage into two areas, managed by a file system and Flash storage, respectively, with different granularity. In OrcFS, the metadata area and data area are maintained by 4Kbyte page granularity and 256Mbyte superblock granularity. The superblock-based storage management aligns the file system section size, which is a unit of segment cleaning, with the superblock size of the underlying Flash storage. It can fully exploit the internal parallelism of the underlying Flash storage, exploiting the sequential workload characteristics of the log-structured file system. Second, OrcFS adopts quasi-preemptive segment cleaning to prohibit the foreground I/O operation from being interfered with by segment cleaning. The latency to reclaim the free space can be prohibitive in OrcFS due to its large file system section size, 256Mbyte. OrcFS effectively addresses this issue via adopting a polling-based segment cleaning scheme. Third, the OrcFS introduces block patching to avoid unnecessary write amplification in the partial page program. OrcFS is the enhancement of the F2FS file system. We develop a prototype OrcFS based on F2FS and server class SSD with modified firmware (Samsung 843TN). OrcFS reduces the device mapping table requirement to 1/465 and 1/4 compared with the page mapping and the smallest mapping scheme known to the public, respectively. Via eliminating the redundancy in the segment cleaning and garbage collection, the OrcFS reduces 1/3 of the write volume under heavy random write workload. OrcFS achieves 56% performance gain against EXT4 in varmail workload. © 2018 ACM.","Flash memories,; Garbage collection; Log-structured file system",Cleaning; Data structures; Distributed computer systems; Firmware; Mapping; Redundancy; Refuse collection; Storage management; Virtual storage; Address translation; Bad block managements; Different granularities; Garbage collection; Large-file system; Log structured file systems; Workload characteristics; Write amplifications; Flash memory
Empirical evaluation and enhancement of enterprise storage system request scheduling,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064551644&doi=10.1145%2f3193741&partnerID=40&md5=fa125780310ce579d83be97a557f5b2b,"Since little has been reported in the literature concerning enterprise storage system file-level request scheduling, we do not have enough knowledge about how various scheduling factors affect performance. Moreover, we are in lack of a good understanding on how to enhance request scheduling to adapt to the changing characteristics of workloads and hardware resources. To answer these questions, we first build a request scheduler prototype based on WAFL®, a mainstream file system running on numerous enterprise storage systems worldwide. Next, we use the prototype to quantitatively measure the impact of various scheduling configurations on performance on a NetApp®’s enterprise-class storage system. Several observations have been made. For example, we discover that in order to improve performance, the priority of write requests and non-preempted restarted requests should be boosted in some workloads. Inspired by these observations, we further propose two scheduling enhancement heuristics called SORD (size-oriented request dispatching) and QATS (queue-depth aware time slicing). Finally, we evaluate them by conducting a wide range of experiments using workloads generated by SPC-1 and SFS2014 on both HDD-based and all-flash platforms. Experimental results show that the combination of the two can noticeably reduce average request latency under some workloads. © 2018 ACM.",Enterprise storage system; File-level request scheduling; WAFL,Hard disk storage; Empirical evaluations; File systems; Hardware resources; Improve performance; Request scheduling; Storage systems; Time slicing; WAFL; Scheduling
Cluster and single-node analysis of long-term deduplication patterns,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064533027&doi=10.1145%2f3183890&partnerID=40&md5=d8bf89ec2fc2cc402452531006c459ec,"Deduplication has become essential in disk-based backup systems, but there have been few long-term studies of backup workloads. Most past studies either were of a small static snapshot or covered only a short period that was not representative of how a backup system evolves over time. For this article, we first collected 21 months of data from a shared user file system; 33 users and over 4,000 snapshots are covered. We then analyzed the dataset, examining a variety of essential characteristics across two dimensions: single-node deduplication and cluster deduplication. For single-node deduplication analysis, our primary focus was individual-user data. Despite apparently similar roles and behavior among all of our users, we found significant differences in their deduplication ratios. Moreover, the data that some users share with others had a much higher deduplication ratio than average. For cluster deduplication analysis, we implemented seven published data-routing algorithms and created a detailed comparison of their performance with respect to deduplication ratio, load distribution, and communication overhead. We found that per-file routing achieves a higher deduplication ratio than routing by super-chunk (multiple consecutive chunks), but it also leads to high data skew (imbalance of space usage across nodes). We also found that large chunking sizes are better for cluster deduplication, as they significantly reduce data-routing overhead, while their negative impact on deduplication ratios is small and acceptable. We draw interesting conclusions from both single-node and cluster deduplication analysis and make recommendations for future deduplication systems design. © 2018 ACM.",Data routing algorithms; Large data set; User study,Back-up systems; Communication overheads; De duplications; Essential characteristic; Large datasets; Load distributions; Long term study; User study; Data storage equipment
A low-cost disk solution enabling LSM-tree to achieve high performance for mixed read/write workloads,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064534400&doi=10.1145%2f3162615&partnerID=40&md5=865a478b0e6463d6f92a6c348cecd0fe,"LSM-tree has been widely used in data management production systems for write-intensive workloads. However, as read and write workloads co-exist under LSM-tree, data accesses can experience long latency and low throughput due to the interferences to buffer caching from the compaction, a major and frequent operation in LSM-tree. After a compaction, the existing data blocks are reorganized and written to other locations on disks. As a result, the related data blocks that have been loaded in the buffer cache are invalidated since their referencing addresses are changed, causing serious performance degradations. To re-enable high-speed buffer caching during intensive writes, we propose Log-Structured buffered-Merge tree (simplified as LSbM-tree) by adding a compaction buffer on disks to minimize the cache invalidations on buffer cache caused by compactions. The compaction buffer efficiently and adaptively maintains the frequently visited datasets. In LSbM, strong locality objects can be effectively kept in the buffer cache with minimum or no harmful invalidations. With the help of a small on-disk compaction buffer, LSbM achieves a high query performance by enabling effective buffer caching, while retaining all the merits of LSM-tree for write-intensive data processing and providing high bandwidth of disks for range queries. We have implemented LSbM based on LevelDB. We show that with a standard buffer cache and a hard disk, LSbM can achieve 2x performance improvement over LevelDB. We have also compared LSbM with other existing solutions to show its strong cache effectiveness. © 2018 ACM.",Buffer cache; Compaction; LSM-tree,Cache memory; Compaction; Costs; Data handling; Forestry; Information management; Buffer caches; Cache invalidation; High-speed buffers; LSM-tree; Performance degradation; Production system; Query performance; Write-intensive workloads; Trees (mathematics)
Challenges and solutions for tracing storage systems: A case study with spectrum scale,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057231831&doi=10.1145%2f3149376&partnerID=40&md5=932601122b63c36ab96ceead53406601,"IBM Spectrum Scale’s parallel file system General Parallel File System (GPFS) has a 20-year development history with over 100 contributing developers. Its ability to support strict POSIX semantics across more than 10K clients leads to a complex design with intricate interactions between the cluster nodes. Tracing has proven to be a vital tool to understand the behavior and the anomalies of such a complex software product. However, the necessary trace information is often buried in hundreds of gigabytes of by-product trace records. Further, the overhead of tracing can significantly impact running applications and file system performance, limiting the use of tracing in a production system. In this research article, we discuss the evolution of the mature and highly scalable GPFS tracing tool and present the exploratory study of GPFS’ new tracing interface, FlexTrace, which allows developers and users to accurately specify what to trace for the problem they are trying to solve. We evaluate our methodology and prototype, demonstrating that the proposed approach has negligible overhead, even under intensive I/O workloads and with low-latency storage devices. © 2018 ACM.",GPFS; Parallel file system; Performance; Trace analysis,Semantics; Trace analysis; Virtual storage; Development history; Exploratory studies; GPFS; Parallel file system; Performance; Production system; Running applications; Trace information; File organization
Fast miss ratio curve modeling for storage cache,2018,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051445421&doi=10.1145%2f3185751&partnerID=40&md5=36afabca8735d99f739cc756feba3586,"The reuse distance (least recently used (LRU) stack distance) is an essential metric for performance prediction and optimization of storage cache. Over the past four decades, there have been steady improvements in the algorithmic efciency of reuse distance measurement. This progress is accelerating in recent years, both in theory and practical implementation. In this article, we present a kinetic model of LRU cache memory, based on the average eviction time (AET) of the cached data. The AET model enables fast measurement and use of low-cost sampling. It can produce the miss ratio curve in linear time with extremely low space costs. On storage trace benchmarks, AET reduces the time and space costs compared to former techniques. Furthermore, AET is a composable model that can characterize shared cache behavior through sampling and modeling individual programs or traces. © 2018 ACM.",Cache system; Data locality; Miss ratio curve,Data storage equipment; Cache systems; Composable modeling; Data locality; Fast measurement; Kinetic modeling; Least recently used; Miss ratio curves; Performance prediction; Cache memory
Introduction to the Special Issue on Massive Storage Systems and Technology 2017,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040335914&doi=10.1145%2f3148596&partnerID=40&md5=126d6a8b19dd3c1c90b9991addd81d89,[No abstract available],,
Modeling drive-managed SMR performance,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040100454&doi=10.1145%2f3139242&partnerID=40&md5=9322bf95973f3ab4b49522c87a4d76dc,"Accurately modeling drive-managed Shingled Magnetic Recording (SMR) disks is a challenge, requiring an array of approaches including both existing disk modeling techniques as well as new techniques for inferring internal translation layer algorithms. In this work, we present the first predictive simulation model of a generally available drive-managed SMR disk. Despite the use of unknown proprietary algorithms in this device, our model that is derived from external measurements is able to predict mean latency within a few percent, and with an Root Mean Square (RMS) cumulative latency error of 25% or less for most workloads tested. These variations, although not small, are in most cases less than three times the drive-to-drive variation seen among seemingly identical drives. © 2017 ACM.",Modeling; Performance; Shingled magnetic recording (SMR),Data storage equipment; Hardware; Models; Disk model; Model drive; Performance; Predictive simulation modeling; Root Mean Square; Shingled magnetic recording (SMR); Translation layers; Magnetic recording
hfplayer: Scalable Replay for Intensive Block I/O Workloads,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040307458&doi=10.1145%2f3149392&partnerID=40&md5=0986698fcf20ec6baa969c3128ef1759,"We introduce new methods to replay intensive block I/O workloads more accurately. These methods can be used to reproduce realistic workloads for benchmarking, performance validation, and tuning of a high-performance block storage device/system. In this article, we study several sources in the stock operating system that introduce uncertainty in the workload replay. Based on the remedies of these findings, we design and develop a new replay tool called hfplayer that replays intensive block I/O workloads in a similar unscaled environment with more accuracy. To replay a given workload trace in a scaled environment with faster storage or host server, the dependency between I/O requests becomes crucial since the timing and ordering of I/O requests is expected to change according to these dependencies. Therefore, we propose a heuristic way of speculating I/O dependencies in a block I/O trace. Using the generated dependency graph, hfplayer tries to propagate I/O related performance gains appropriately along the I/O dependency chains and mimics the original application behavior when it executes in a scaled environment with slower or faster storage system and servers. We evaluate hfplayer with a wide range of workloads using several accuracy metrics and find that it produces better accuracy when compared to other replay approaches. © 2017 ACM.",I/O workload; Storage area network; Storage performance; Workload replay,Virtual storage; Application behaviors; Dependency chains; Dependency graphs; Performance Gain; Performance validation; Storage area networks; Storage performance; Workload replay; Benchmarking
GDS-LC: A latency-and cost-aware client caching scheme for cloud storage,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038396092&doi=10.1145%2f3149374&partnerID=40&md5=fd75223fde8f7236c1058eec794949ef,"Successfully integrating cloud storage as a primary storage layer in the I/O stack is highly challenging. This is essentially due to two inherent critical issues: the high and variant cloud I/O latency and the per-I/O pricing model of cloud storage. To minimize the associated latency and monetary cost with cloud I/Os, caching is a crucial technology, as it directly influences how frequently the client has to communicate with the cloud. Unfortunately, current cloud caching schemes are mostly designed to optimize miss reduction as the sole objective and only focus on improving system performance while ignoring the fact that various cache misses could have completely distinct effects in terms of latency and monetary cost. In this article, we present a cost-aware caching scheme, called GDS-LC, which is highly optimized for cloud storage caching. Different from traditional caching schemes that merely focus on improving cache hit ratios and the classic cost-aware schemes that can only achieve a single optimization target, GDS-LC offers a comprehensive cache design by considering not only the access locality but also the object size, associated latency, and price, aiming at enhancing the user experience with cloud storage from two aspects: access latency and monetary cost. To achieve this, GDS-LC virtually partitions the cache space into two regions: a high-priority latency-aware region and a low-priority price-aware region. Each region is managed by a costaware caching scheme, which is based on GreedyDual-Size (GDS) and designed for a cloud storage scenario by adopting clean-dirty differentiation and latency normalization. The GDS-LC framework is highly flexible, and we present a further enhanced algorithm, called GDS-LCF, by incorporating access frequency in caching decisions. We have built a prototype to emulate a typical cloud client cache and evaluate GDS-LC and GDSLCF with Amazon Simple Storage Services (S3) in three different scenarios: local cloud, Internet cloud, and heterogeneous cloud. Our experimental results show that our caching schemes can effectively achieve both optimization goals: low access latency and low monetary cost. It is our hope that this work can inspire the community to reconsider the cache design in the cloud environment, especially for the purpose of integrating cloud storage into the current storage stack as a primary layer. © 2017 ACM.",Caching algorithms; Cloud storage; Storage systems,Economics; Optimization; Caching algorithm; Cloud environments; Cloud storages; Crucial technology; Improving systems; Optimization goals; Simple storage services; Storage systems; Costs
Building efficient key-value stores via a lightweight compaction tree,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038370499&doi=10.1145%2f3139922&partnerID=40&md5=9ea6bfdfc092c1e57de0fefb21deb0a0,"Log-Structure Merge tree (LSM-tree) has been one of the mainstream indexes in key-value systems supporting a variety of write-intensive Internet applications in today's data centers. However, the performance of LSM-tree is seriously hampered by constantly occurring compaction procedures, which incur significant write amplification and degrade the write throughput. To alleviate the performance degradation caused by compactions, we introduce a lightweight compaction tree (LWC-tree), a variant of LSM-tree index optimized for minimizing the write amplification and maximizing the system throughput. The lightweight compaction drastically decreases write amplification by appending data in a table and only merging the metadata that have much smaller size. Using our proposed LWC-tree, we have implemented three key-value LWC-stores on different storage mediums including Shingled Magnetic Recording (SMR) drives, Solid State Drives (SSD), and conventional Hard Disk Drives (HDDs). The LWC-store is particularly optimized for SMR drives, as it eliminates the multiplicative I/O amplification from both LSM-trees and SMR drives. Due to the lightweight compaction procedure, LWC-store reduces the write amplification by a factor of up to 5× compared to the popular LevelDB key-value store. Moreover, the random write throughput of the LWC-tree on SMR drives is significantly improved by up to 467% even compared with LevelDB on conventional HDDs. Furthermore, LWC-tree has wide applicability and delivers impressive performance improvement in various conditions, including different storage mediums (i.e., SMR, HDD, SSD) and various value sizes and access patterns (i.e., uniform and Zipfian). © 2017 ACM.",Lightweight compaction; LSM-tree; SMR drives; Write amplification,Compaction; Digital storage; Drives; Forestry; Magnetic disk storage; Magnetic storage; Throughput; Trees (mathematics); Internet application; LSM-tree; Performance degradation; Shingled magnetic recording (SMR); Solid state drives (SSD); System throughput; Write amplifications; Write throughputs; Hard disk storage
Client-side journaling for durable shared storage,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038380857&doi=10.1145%2f3149372&partnerID=40&md5=55d34c9d12491320b75f1f410028156c,"Hardware consolidation in the datacenter often leads to scalability bottlenecks from heavy utilization of critical resources, such as the storage and network bandwidth. Client-side caching on durable media is already applied at block level to reduce the storage backend load but has received criticism for added overhead, restricted sharing, and possible data loss at client crash. We introduce a journal to the kernel-level client of an object-based distributed filesystem to improve durability at high I/O performance and reduced shared resource utilization. Storage virtualization at the file interface achieves clear consistency semantics across data and metadata, supports native file sharing among clients, and provides flexible configuration of durable data staging at the host. Over a prototype that we have implemented, we experimentally quantify the performance and efficiency of the proposed Arion system in comparison to a production system. We run microbenchmarks and application-level workloads over a local cluster and a public cloud. We demonstrate reduced latency by 60% and improved performance up to 150% at reduced server network and disk bandwidth by 41% and 77%, respectively. The performance improvement reaches 92% for 16 relational databases as clients and gets as high as 11.3x with two key-value stores as clients. © 2017 ACM.",Cloud storage; Crash consistency; Distributed filesystems; Failure recovery; Scalability,Bandwidth; File organization; Scalability; Semantics; Virtual storage; Client side caching; Cloud storages; Crash consistency; Distributed file-system; Distributed filesystems; Failure recovery; Relational Database; Storage virtualization; Digital storage
CosaFS: A cooperative shingle-aware file system,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035044220&doi=10.1145%2f3149482&partnerID=40&md5=4279bddf740903ddd46c69f703eec5a8,"In this article, we design and implement a cooperative shingle-aware file system, called CosaFS, on heterogeneous storage devices that mix solid-state drives (SSDs) and shingled magnetic recording (SMR) technology to improve the overall performance of storage systems. The basic idea of CosaFS is to classify objects as hot or cold objects based on a proposed Lookahead with Recency Weight scheme. If an object is identified as a hot (small) object, then it will be served by SSD. Otherwise, cold (large) objects are stored on SMR. For an SMR, large objects can be accessed in large sequential blocks, rendering the performance of their accesses comparable with that of accessing the same large sequential blocks as if they were stored on a hard drive. Small objects, such as inodes and directories, are stored on the SSD where ""seeks"" for such objects are nearly free. With thorough empirical studies, we demonstrate that CosaFS, as a cooperative shingle-aware file system, with metadata separation and cache-assistance, is a very effective way to handle the disk-based data demanded by the shingled writes and outperforms the device- and host-side shingle-aware file systems in terms of throughput, IOPS, and access latency as well.",File system; Garbage collection; Shingled magnetic recording; Shingled write disk; Solid-state drive,Digital storage; Magnetic recording; Magnetic storage; Virtual storage; File systems; Garbage collection; Shingled magnetic recordings; Shingled write disks; Solid state drives; File organization
GCMix: An efficient data protection scheme against the paired page interference,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038406326&doi=10.1145%2f3149373&partnerID=40&md5=6f476ff5d4026fb859f041189014a4b6,"In multi-level cell (MLC) NAND flash memory, two logical pages are overlapped on a single physical page. Even after a logical page is programmed, the data can be corrupted if the programming of the coexisting logical page is interrupted. This phenomenon is called paired page interference. This article proposes a novel software technique to deal with the paired page interference without any additional hardware or extra page write. The proposed technique utilizes valid pages in the victim block during garbage collection (GC) as the backup against the interference, and pairs them with incoming pages written by the host. This approach eliminates undesirable page copy to backup pages against the interference. However, such a strategy has an adverse effect on the hot/cold separation policy, which is essential to improve the efficiency of GC. To limit the downside, we devise a metric to estimate the benefit of GCMix on-the-fly so that GCMix can be adaptively utilized only when the benefit outweighs the overhead. Evaluations using synthetic and real workloads show GCMix can effectively deal with the paired page interference, reducing the write amplification factor by up to 17.5%compared to the traditional technique, while providing comparable I/O performance. © 2017 ACM.",Flash memory; Flash memory cells; Locality; Multi-level cells; Paired page interference,Data storage equipment; Hardware; Flash memory cell; Garbage collection; Locality; Multi level cell (MLC); Multilevel cell; Software techniques; Traditional techniques; Write amplifications; Flash memory
Experience from two years of visualizing flash with SSDPlayer,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038408598&doi=10.1145%2f3149356&partnerID=40&md5=650168647d10c824ec07f83791fbf6e8,"Data visualization is a thriving field of computer science, with widespread impact on diverse scientific disciplines, from medicine and meteorology to visual data mining. Advances in large-scale storage systems, as well as low-level storage technology, played a significant role in accelerating the applicability and adoption of modern visualization techniques. Ironically, ""the cobbler's children have no shoes"": Researchers who wish to analyze storage systems and devices are usually limited to a variety of static histograms and basic displays. The dynamic nature of data movement on flash has motivated the introduction of SSDPlayer, a graphical tool for visualizing the various processes that cause data movement on solid-state drives (SSDs). In 2015, we used the initial version of SSDPlayer to demonstrate how visualization can assist researchers and developers in their understanding of modern, complex flash-based systems. While we continued to use SSDPlayer for analysis purposes, we found it extremely useful for education and presentation purposes as well. In this article, we describe our experience from two years of using, sharing, and extending SSDPlayer and how similar techniques can further advance storage systems research and education. © 2017 ACM.",Analysis; Flash; Simulation; SSD; Storage system management; Visualization,Computer supported cooperative work; Data mining; Data visualization; Digital storage; Display devices; Flow visualization; Visualization; Analysis; Flash; Large-scale storage systems; Scientific discipline; Simulation; Storage systems; Visual data mining; Visualization technique; Flash-based SSDs
Optimal repair layering for erasure-coded data centers: From theory to practice,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035066198&doi=10.1145%2f3149349&partnerID=40&md5=792fcb6c6c8bd96be26636a9807baad5,"Repair performance in hierarchical data centers is often bottlenecked by cross-rack network transfer. Recent theoretical results show that the cross-rack repair traffic can be minimized through repair layering, whose idea is to partition a repair operation into inner-rack and cross-rack layers. However, how repair layering should be implemented and deployed in practice remains an open issue. In this article, we address this issue by proposing a practical repair layering framework called DoubleR.We design two families of practical double regenerating codes (DRC), which not only minimize the cross-rack repair traffic but also have several practical properties that improve state-of-the-art regenerating codes. We implement and deploy DoubleR atop the Hadoop Distributed File System (HDFS) and show that DoubleR maintains the theoretical guarantees of DRC and improves the repair performance of regenerating codes in both node recovery and degraded read operations.",Erasure coding,Codes (symbols); File organization; Erasure coding; Hadoop distributed file system (HDFS); Hierarchical data; Network transfers; Regenerating codes; Repair operations; State of the art; Theoretical guarantees; Repair
SUPA: A single unified read-write buffer and pattern-change-aware FTL for the high performance of multi-channel SSD,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034982460&doi=10.1145%2f3129901&partnerID=40&md5=117d3cae6ee21197a09020b5665355d3,"To design the write buffer and flash translation layer (FTL) for a solid-state drive (SSD), previous studies bhave tried to increase overall SSD performance by parallel I/O and garbage collection overhead reduction Recent works have proposed pattern-based managements, which uses the request size and read- or writeintensiveness to apply different policies to each type of data. In our observation, the locations of read and write requests are closely related, and the pattern of each type of data can be changed. In thiswork,we propose SUPA, a single unified read-write buffer and pattern-change-aware FTL on multi-channel SSD architecture. To increase both read and write hit ratios on the buffer based on locality, we use a single unified read-write buffer for both clean and dirty blocks. With proposed buffer, we can increase buffer hit ratio up to 8.0% and reduce 33.6% and 7.5% of read and write latencies, respectively. To handle pattern-changed blocks, we add a pattern handler between the buffer and the FTL, which monitors channel status and handles data by applying one of the two different policies according to the pattern changes. With pattern change handling process, we can reduce 1.0% and 15.4% of read and write latencies, respectively. In total, our evaluations show that SUPA can get up to 2.0 and 3.9 times less read and write latency, respectively, without loss of lifetime in comparison to previous works. © 2017 ACM.",Flash Translation Layer; Multi-channel SSD; NAND flash memory; Pattern handler; Pattern-based management; Read-write unified buffer,Digital storage; Flash memory; Memory architecture; Flash translation layer; Multi channel; NAND flash memory; Pattern handler; Read-write unified buffer; Flash-based SSDs
Tiny-tail flash: Near-perfect elimination of garbage collection tail latencies in NAND SSDs,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033241847&doi=10.1145%2f3121133&partnerID=40&md5=2c920b7fe2f81251a78965ce0c6367e6,"Flash storage has become the mainstream destination for storage users. However, SSDs do not always deliver the performance that users expect. The core culprit of flash performance instability is thewell-known garbage collection (GC) process, which causes long delays as the SSD cannot serve (blocks) incoming I/Os, which then induces the long tail latency problem. We present ttFlash as a solution to this problem. ttFlash is a ""tiny-tail"" flash drive (SSD) that eliminates GC-induced tail latencies by circumventing GC-blocked I/Os with four novel strategies: Plane-blocking GC, rotating GC, GC-tolerant read, and GC-tolerant flush. These four strategies leverage the timely combination of modern SSD internal technologies such as powerful controllers, parity-based redundancies, and capacitor-backed RAM. Our strategies are dependent on the use of intra-plane copyback operations. Through an extensive evaluation, we show that ttFlash comes significantly close to a ""no-GC"" scenario. Specifically, between the 99 and 99.99th percentiles, ttFlash is only 1.0 to 2.6× slower than the no-GC case, while a base approach suffers from 5-138× GC-induced slowdowns. © 2017 ACM.",flash-based SSD; ttFlash,NAND circuits; Random access storage; Refuse collection; Flash drives; Flash storage; flash-based SSD; Garbage collection; Latency problem; Long delays; Novel strategies; ttFlash; Flash-based SSDs
TinyLFU: A highly efficient cache admission policy,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038365619&doi=10.1145%2f3149371&partnerID=40&md5=19107364e214863bf7f9ac35bb66f7bf,"This article proposes to use a frequency-based cache admission policy in order to boost the effectiveness of caches subject to skewed access distributions. Given a newly accessed item and an eviction candidate from the cache, our scheme decides, based on the recent access history, whether it is worth admitting the new item into the cache at the expense of the eviction candidate. This concept is enabled through a novel approximate LFU structure called TinyLFU, which maintains an approximate representation of the access frequency of a large sample of recently accessed items. TinyLFU is very compact and lightweight as it builds upon Bloom filter theory. We study the properties of TinyLFU through simulations of both synthetic workloads and multiple real traces from several sources. These simulations demonstrate the performance boost obtained by enhancing various replacement policies with the TinyLFU admission policy. Also, a new combined replacement and eviction policy scheme nicknamed W-TinyLFU is presented. W-TinyLFU is demonstrated to obtain equal or better hit ratios than other state-of-the-art replacement policies on these traces. It is the only scheme to obtain such good results on all traces. © 2017 ACM.",Admission policy; Caching; Counting bloom filter; Eviction policy; Sketches,Data storage equipment; Hardware; Access frequency; Admission policies; Caching; Counting bloom filters; Replacement policy; Sketches; State of the art; Synthetic workloads; Data structures
Ouroboroswear leveling for nvram using hierarchical block migration,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034971016&doi=10.1145%2f3139530&partnerID=40&md5=d6133cb930632900e745f4e0108ed380,"Emerging nonvolatile RAM (NVRAM) technologies have a limit on the number of writes that can be made to any cell, similar to the erasure limits in NAND Flash. This motivates the need for wear leveling techniques to distribute the writes evenly among the cells. Unlike NAND Flash, cells in NVRAM can be rewritten without the need for erasing the entire containing block, avoiding the issues of space reclamation and garbage collection, motivating alternate approaches to the problem. In this article, we propose a hierarchical wear-leveling model called Ouroboros wear leveling. Ouroboros uses a two-level strategy whereby frequent low-cost intraregion wear leveling at small granularity is combined with interregion wear leveling at a larger time interval and granularity. Ouroboros is a hybrid migration scheme that exploits correct demand predictions in making better wear-leveling decisions while using randomization to avoid wear-leveling attacks by deterministic access patterns. We also propose a way to optimize wear-leveling parameter settings to meet a target smoothness level under limited time and space overhead constraints for different memory architectures and trace characteristics. Several experiments are performed on synthetically generated memory traces with special characteristics, two block-level storage traces, and two memory-line-level memory traces. The results show that Ouroboros wear leveling can distribute writes smoothly across the whole NVRAM with no more than 0.2% space overhead and 0.52% time overhead for a 512GB memory. © 2017 ACM.",Lifetime endurance; NVRAM; Wear leveling,NAND circuits; Random access storage; Alternate approaches; Deterministic access; Garbage collection; Non-volatile rams; NVRAM; Parameter setting; Two-level strategies; Wear leveling; Memory architecture
Systematic erasure codes with optimal repair bandwidth and storage,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033211796&doi=10.1145%2f3109479&partnerID=40&md5=c54f20adf10b2f0663627f970393dacd,"Erasure codes are widely used in distributed storage systems to prevent data loss. Traditional codes suffer from a typical repair-bandwidth problem in which the amount of data required to reconstruct the lost data, referred to as the repair bandwidth, is often far more than the theoretical minimum. While many novel codes have been proposed in recent years to reduce the repair bandwidth, these codes either require extra storage and computation overhead or are only applicable to some special cases. To address the weaknesses of the existing solutions to the repair-bandwidth problem, we propose Z Codes, a general family of codes capable of achieving the theoretical lower bound of repair bandwidth versus storage. To the best of our knowledge, the Z codes are the first general systematic erasure codes that jointly achieve optimal repair bandwidth and storage. Further, we generalize the Z codes to the GZ codes to gain the Maximum Distance Separable (MDS) property. Our evaluations of a real system indicate that Z/GZ and Reed-Solomon (RS) codes show approximately close encoding and repairing speeds, while GZ codes achieve over 37.5% response time reduction for repairing the same size of data, compared to the RS and Cauchy Reed-Solomon (CRS) codes. © 2017 ACM.",Distributed storage system; erasure codes; failure tolerance; repair bandwidth,Bandwidth; Digital storage; Forward error correction; Multiprocessing systems; Repair; Computation overheads; Distributed storage system; Erasure codes; Failure tolerance; Maximum distance; Real systems; Response time reductions; Theoretical minimum; Codes (symbols)
Hybris: Robust hybrid cloud storage,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033234950&doi=10.1145%2f3119896&partnerID=40&md5=ce3b16815216ebd30391f0b32c148b74,"Besides well-known benefits, commodity cloud storage also raises concerns that include security, reliability, and consistency. We present Hybris key-value store, the first robust hybrid cloud storage system, aiming at addressing these concerns leveraging both private and public cloud resources. Hybris robustly replicates metadata on trusted private premises (private cloud), separately fromdata, which are dispersed (using replication or erasure coding) across multiple untrusted public clouds. Hybris maintains metadata stored on private premises at the order of few dozens of bytes per key, avoiding the scalability bottleneck at the private cloud. In turn, the hybrid design allows Hybris to efficiently and robustly tolerate cloud outages but also potential malice in clouds without overhead. Namely, to tolerate up to f malicious clouds, in the common case of the Hybris variant with data replication, writes replicate data across f + 1 clouds, whereas reads involve a single cloud. In the worst case, only up to f additional clouds are used. This is considerably better than earlier multi-cloud storage systems that required costly 3f + 1 clouds to mask f potentially malicious clouds. Finally, Hybris leverages strong metadata consistency to guarantee to Hybris applications strong data consistency without any modifications to the eventually consistent public clouds. We implemented Hybris in Java and evaluated it using a series of micro and macro-benchmarks. Our results show that Hybris significantly outperforms comparable multi-cloud storage systems and approaches the performance of bare-bone commodity public cloud storage. © 2017 ACM.",Cloud storage; consistency; hybrid cloud; reliability,Metadata; Reliability; Cloud storages; Commodity clouds; consistency; Data consistency; Data replication; Hybrid clouds; Key-value stores; Micro and macro; Digital storage
vNFS: Maximizing NFS performance with compounds and vectorized I/O,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033215464&doi=10.1145%2f3116213&partnerID=40&md5=11bc94c1291c03ebf6cb88bd1ae2b717,"Modern systems use networks extensively, accessing both services and storage across local and remote networks. Latency is a key performance challenge, and packing multiple small operations into fewer large ones is an effective way to amortize that cost, especially after years of significant improvement in bandwidth but not latency. To this end, the NFSv4 protocol supports a compounding feature to combine multiple operations. Yet compounding has been underused since its conception because the synchronous POSIX file-system API issues only one (small) request at a time. We propose vNFS, an NFSv4.1-compliant client that exposes a vectorized high-level API and leverages NFS compound procedures to maximize performance. We designed and implemented vNFS as a user-space RPC library that supports an assortment of bulk operations on multiple files and directories. We found it easy to modify several UNIX utilities, an HTTP/2 server, and Filebench to use vNFS. We evaluated vNFS under a wide range of workloads and network latency conditions, showing that vNFS improves performance even for low-latency networks. On high-latency networks, vNFS can improve performance by as much as two orders of magnitude. © 2017 ACM.",Compound procedures; File-system API; NFS; POSIX; VNFS,File organization; Compound procedures; File systems; High-latency networks; Low-latency networks; Multiple operations; Performance challenges; POSIX; VNFS; Application programming interfaces (API)
Efficient free space reclamation in WAFL,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033238324&doi=10.1145%2f3125647&partnerID=40&md5=e3249418e38406cdf37f8a6d3603b989,"NetApp® WAFL® is a transactional file system that uses the copy-on-write mechanism to support fast write performance and efficient snapshot creation. However, copy-on-write increases the demand on the file system to find free blocks quickly, which makes rapid free space reclamation essential. Inability to find free blocks quickly may impede allocations for incoming writes. Eficiency is also important, because the task of reclaiming free space may consume CPU and other resources at the expense of client operations. In this article, we describe the evolution (over more than a decade) of the WAFL algorithms and data structures for reclaiming space with minimal impact to the overall performance of the storage appliance. © 2017 ACM.",Deduplication; File system; File system performance; Free space reclamation; Garbage collection; Snapshots; Storage system,Digital storage; File organization; De duplications; File systems; Free spaces; Garbage collection; Snapshots; Storage systems; Reclamation
Redundancy does not imply fault tolerance: Analysis of distributed storage reactions to file-system faults,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033234478&doi=10.1145%2f3125497&partnerID=40&md5=6505e3867c13901212e561998861d9b4,"We analyze how modern distributed storage systems behave in the presence of file-system faults such as data corruption and read and write errors. We characterize eight popular distributed storage systems and uncover numerous problems related to file-system fault tolerance.We find that modern distributed systems donot consistently use redundancy to recover from file-system faults: a single file-system fault can cause catastrophic outcomes such as data loss, corruption, and unavailability. We also find that the above outcomes arise due to fundamental problems in file-system fault handling that are common across many systems. Our results have implications for the design of next-generation fault-tolerant distributed and cloud storage systems. © 2017 ACM.",Data corruption; Fault tolerance; File-system faults,Crime; Digital storage; Fault tolerance; File organization; Multiprocessing systems; Redundancy; Cloud storage systems; Data corruption; Distributed storage; Distributed storage system; Distributed systems; Fault-tolerant; File systems; Single-file systems; Fault tolerant computer systems
Application crash consistency and performance with CCFS,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033217204&doi=10.1145%2f3119897&partnerID=40&md5=2b95213f8cda949f00f6521f94febb62,"Recent research has shown that applications often incorrectly implement crash consistency. We present the Crash-Consistent File System (ccfs), a file system that improves the correctness of application-level crash consistency protocols while maintaining high performance. A key idea in ccfs is the abstraction of a stream. Within a stream, updates are committed in program order, improving correctness; across streams, there are no ordering restrictions, enabling scheduling flexibility and high performance. We empirically demonstrate that applications running atop ccfs achieve high levels of crash consistency. Further, we show that ccfs performance under standard file-system benchmarks is excellent, in the worst case on par with the highest performing modes of Linux ext4, and in some cases notably better. Overall, we demonstrate that both application correctness and high performance can be realized in a modern file system.",Crash consistency; File systems; Performance; Reordering,Computer operating systems; File organization; Application level; Consistency protocol; Crash consistency; File systems; Performance; Recent researches; Reordering; Scheduling flexibility; Benchmarking
Efficient and available in-memory kv-store with hybrid erasure coding and replication,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029889561&doi=10.1145%2f3129900&partnerID=40&md5=110450b3786d38f08fc58ea4f15149a7,"In-memory key/value store (KV-store) is a key building block for many systems like databases and large websites. Two key requirements for such systems are efficiency and availability, which demand a KV-store to continuously handle millions of requests per second. A common approach to availability is using replication, such as primary-backup (PBR), which, however, requires M + 1 times memory to tolerate M failures. This renders scarce memory unable to handle useful user jobs. This article makes the first case of building highly available in-memory KV-store by integrating erasure coding to achieve memory efficiency, while not notably degrading performance. A main challenge is that an in-memory KV-store has much scattered metadata. A single KV put may cause excessive coding operations and parity updates due to excessive small updates to metadata. Our approach, namely Cocytus, addresses this challenge by using a hybrid scheme that leverages PBR for small-sized and scattered data (e.g., metadata and key), while only applying erasure coding to relatively large data (e.g., value). To mitigate well-known issues like lengthy recovery of erasure coding, Cocytus uses an online recovery scheme by leveraging the replicated metadata information to continuously serve KV requests. To further demonstrate the usefulness of Cocytus, we have built a transaction layer by using Cocytus as a fast and reliable storage layer to store database records and transaction logs. We have integrated the design of Cocytus to Memcached and extend it to support inmemory transactions. Evaluation using YCSB with different KV configurations shows that Cocytus incurs low overhead for latency and throughput, can tolerate node failures with fast online recovery, while saving 33% to 46% memory compared to PBR when tolerating two failures. A further evaluation using the SmallBank OLTP benchmark shows that in-memory transactions can run atop Cocytus with high throughput, low latency, and low abort rate and recover fast from consecutive failures.",Erasure coding; Kv-store; Primary-backup replication; Transactions,Digital storage; Efficiency; Metadata; Erasure coding; Kv-store; Memory efficiency; Memory transactions; Metadata information; On-line recoveries; Primary-backup; Transactions; Recovery
Pannier: Design and analysis of a container-based flash cache for compound objects,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029839591&doi=10.1145%2f3094785&partnerID=40&md5=3bde1adcc63b5f707e7a5da43055a833,"Classic caching algorithms leverage recency, access count, and/or other properties of cached blocks at perblock granularity. However, for media such as flash which have performance and wear penalties for small overwrites, implementing cache policies at a larger granularity is beneficial. Recent research has focused on buffering small blocks and writing in large granularities, sometimes called containers, but it has not explored the ramifications and best strategies for caching compound blocks consisting of logically distinct, but physically co-located, blocks. Containers may have highly diverse blocks, with mixtures of frequently accessed, infrequently accessed, and invalidated blocks. We propose and evaluate Pannier, a flash cache layer that provides high performance while extending flash lifespan. Pannier uses three main techniques: (1) leveraging block access counts to manage cache containers, (2) incorporating block liveness as a property to improve flash cache space efficiency, and (3) designing a multi-step feedback controller to ensure a flash cache reaches its desired lifespan while maintaining performance. Our evaluation shows that Pannier improves flash cache performance and extends lifespan beyond previous per-block and container-Aware caching policies. More fundamentally, our investigation highlights the importance of creating new policies for caching compound blocks in flash.",Access count; Block consolidation; Eviction algorithm; Flash cache; I/o throttling,Data storage equipment; Access count; Cache performance; Design and analysis; Eviction algorithms; Feedback controller; Flash cache; Recent researches; Space efficiencies; Containers
Introduction to the special issue on USENIX FAST 2017,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033223054&doi=10.1145%2f3131620&partnerID=40&md5=4c3122911a625d1b3056f8b34a198c29,[No abstract available],,
High-performance General Functional Regenerating codes with near-optimal repair bandwidth,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027036172&doi=10.1145%2f3051122&partnerID=40&md5=87ce8c5b57b2ef8da1c086733beb7d1d,"Erasure codes are widely used in modern distributed storage systems to prevent data loss and server failures. Regenerating codes are a class of erasure codes that trade storage efficiency and computation for repair bandwidth reduction. However, their nonunified coding parameters and huge computational overhead prohibit their applications. Hence, we first propose a family of General Functional Regenerating (GFR) codes with uncoded repair, balancing storage efficiency and repair bandwidth with general parameters. The GFR codes take advantage of a heuristic repair algorithm, which makes efforts to employ as little repair bandwidth as possible to repair a single failure. Second, we also present a scheduled shift multiplication (SSM) algorithm, which accelerates the matrix product over the Galois field by scheduling the order of coding operations, so encoding and repairing of GFR codes can be executed by fast bitwise shifting and exclusive-OR. Compared to the traditional table-lookup multiplication algorithm, our SSM algorithm gains 1.2 to 2 X speedup in our experimental evaluations, with little effect on the repair success rate. © 2017 ACM.",Erasure codes; Failure recovery; Failure tolerance; Performance and evaluation; Repair bandwidth,Bandwidth; Digital storage; Efficiency; Forward error correction; Multiprocessing systems; Repair; Table lookup; Computational overheads; Distributed storage system; Erasure codes; Experimental evaluation; Failure recovery; Failure tolerance; Multiplication algorithms; Performance and evaluation; Codes (symbols)
Lazy exact deduplication,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023183798&doi=10.1145%2f3078837&partnerID=40&md5=94e83a480bbedafa6ed784787d72f8e8,"Deduplication aims to reduce duplicate data in storage systems by removing redundant copies of data blocks, which are compared to one another using fingerprints. However, repeated on-disk fingerprint lookups lead to high disk traffic, which results in a bottleneck. In this article, we propose a ""lazy"" data deduplication method, which buffers incoming fingerprints that are used to perform on-disk lookups in batches, with the aim of improving subsequent prefetching. In deduplication in general, prefetching is used to improve the cache hit rate by exploiting locality within the incoming fingerprint stream. For lazy deduplication, we design a buffering strategy that preserves locality in order to facilitate prefetching. Furthermore, as the proportion of deduplication time spent on I/O decreases, the proportion spent on fingerprint calculation and chunking increases. Thus, we also utilize parallel approaches (utilizing multiple CPU cores and a graphics processing unit) to further improve the overall performance. Experimental results indicate that the lazy method improves fingerprint identification performance by over 50% compared with an ""eager"" method with the same data layout. The GPU improves the hash calculation by a factor of 4.6 and multithreaded chunking by a factor of 4.16. Deduplication performance can be improved by over 45% on SSD and 80% on HDD in the last round on the real datasets. © 2017 ACM.",Deduplication; Disk I/O; GPU; Lazy method,Computer graphics; Computer graphics equipment; Digital storage; Hard disk storage; Program processors; Buffering strategy; Cache hit rates; Data de duplications; De duplications; Disk I/O; Fingerprint identification; Lazy method; Storage systems; Graphics processing unit
Reducing write amplification of flash storage through cooperative data management with NVM,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027053666&doi=10.1145%2f3060146&partnerID=40&md5=956d0f07a0b486c76e54f3697425a296,"Write amplification is a critical factor that limits the stable performance of flash-based storage systems. To reduce write amplification, this article presents a new technique that cooperatively manages data in flash storage and nonvolatile memory (NVM). Our scheme basically considers NVM as the cache of flash storage, but allows the original data in flash storage to be invalidated if there is a cached copy in NVM, which can temporarily serve as the original data. This scheme eliminates the copy-out operation for a substantial number of cached data, thereby enhancing garbage collection efficiency. Simulated results show that the proposed scheme reduces the copy-out overhead of garbage collection by 51.4% and decreases the standard deviation of response time by 35.4% on average. Measurement results obtained by implementing the proposed scheme in BlueDBM,1 an open-source flash development platform developed by MIT, show that the proposed scheme reduces the execution time and increases IOPS by 2-21% and 3-18%, respectively, for the workloads that we considered. This article is an extended version of Lee et al. [2016], which was presented at the 32nd International Conference on Massive Data Storage Systems and Technology in 2016. © 2017 ACM.",Endurance; Flash memory; Nonvolatile memory; Write amplification,Data storage equipment; Digital storage; Durability; Flash memory; Nonvolatile storage; Refuse collection; Development platform; Extended versions; Garbage collection; Non-volatile memory; Simulated results; Stable performance; Standard deviation; Write amplifications; Information management
Understanding and alleviating the impact of the flash address translation on solid state devices,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019680225&doi=10.1145%2f3051123&partnerID=40&md5=cf149f181b33af7ad339f305af08c737,"Flash-based solid state devices (SSDs) have been widely employed in consumer and enterprise storage systems. However, the increasing SSD capacity imposes great pressure on performing efficient logical to physical address translation in a page-level flash translation layer (FTL). Existing schemes usually employ a built-inRAMto storemapping information, called mapping cache, to speed up the address translation. Since only a fraction of the mapping table can be cached due to limited cache space, a large number of extra flash accesses are required for cache management and garbage collection, degrading the performance and lifetime of an SSD. In this paper, we first apply analytical models to investigate the key factors that incur extra flash accesses during address translation. Then, we propose a novel page-level FTL with an efficient translation page-level caching mechanism, named TPFTL, to minimize the extra flash accesses. TPFTL employs a twolevel least recently used (LRU) list with space-efficient optimizations to organize cached mapping entries. Inspired by the models, we further design a workload-adaptive loading policy combined with an efficient replacement policy to increase the cache hit rate and reduce the writebacks of replaced dirty entries. Finally, we evaluate TPFTL using extensive trace-driven simulations. Our evaluation results show that compared to the state-of-the-art FTLs, TPFTL significantly reduces the extra operations caused by address translation, achieving reductions on system response time and write amplification by up to 27.1% and 32.2%, respectively. © 2017 ACM.",Address translation; FTL; NAND flash memory; SSD,Flash memory; Mapping; Physical addresses; Solid state devices; Address translation; Flash translation layer; Garbage collection; Least recently used; NAND flash memory; System response time; Trace driven simulation; Write amplifications; Flash-based SSDs
Introduction to the special issue on MSST 2016,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019719030&doi=10.1145%2f3078405&partnerID=40&md5=0e454269de839da7d3b702c7fde6a8a6,"The research track of the 32nd International Conference on Massive Storage Systems and Technology (MSST 2016) was held at the campus of Santa Clara University, California from May 5-6, 2016. The conference received 69 submissions from 270 authors. Of all submitted articles, the program committee rigorously selected 17 long and 4 short articles, 21 in total. MSST’16 covered a wide range of topics on storage systems, ranging from low-cost archival storage to cloud and mobile storage to high-performance flash disks and non-volatile memory. For this special issue of ACM Transactions on Storage (ToS), three articles were selected to demonstrate the diverse spirit of MSST from one side and highlight the trend of the growing research on applications of non-volatile memory from the other side. The authors of the selected articles extended their original articles into the following journal articles.",,Digital storage; Flash memory; Nonvolatile storage; Archival storages; Flash disks; Journal articles; Massive storage systems; Mobile storage; Non-volatile memory; Program committee; Storage systems; Data storage equipment
Understanding I/O performance behaviors of cloud storage from a client's perspective,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019734491&doi=10.1145%2f3078838&partnerID=40&md5=dc19492d7f02e2468a739ca3ebe6b773,"Cloud storage has gained increasing popularity in the past few years. In cloud storage, data is stored in the service provider's data centers, and users access data via the network. For such a new storage model, our prior wisdom about conventional storage may not remain valid nor applicable to the emerging cloud storage. In this article, we present a comprehensive study to gain insight into the unique characteristics of cloud storage and optimize user experiences with cloud storage from a client's perspective. Unlike prior measurement work that mostly aims to characterize cloud storage providers or specific client applications, we focus on analyzing the effects of various client-side factors on the user-experienced performance. Through extensive experiments and quantitative analysis, we have obtained several important findings. For example, we find that (1) a proper combination of parallelism and request size can achieve optimized bandwidths, (2) a client's capabilities and geographical location play an important role in determining the end-to-end user-perceivable performance, and (3) the interference among mixed cloud storage requests may cause performance degradation. Based on our findings, we showcase a sampling- and inference-based method to determine a proper combination for different optimization goals. We further present a set of case studies on client-side chunking and parallelization for typical cloud-based applications. Our studies show that specific attention should be paid to fully exploiting the capabilities of clients and the great potential of cloud storage services. © 2017 ACM.",Cloud storage; Measurement; Performance analysis; Performance optimization; Storage systems,Data storage equipment; Hardware; Measurements; Cloud storage services; Cloud storages; Cloud-based applications; Geographical locations; Performance analysis; Performance degradation; Performance optimizations; Storage systems; Digital storage
"ExaPlan: Efficient queueing-based data placement, provisioning, and load balancing for large tiered storage systems",2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019672801&doi=10.1145%2f3078839&partnerID=40&md5=87ddcd2ca332580c2bd93ea339039cf2,"Multi-tiered storage, where each tier consists of one type of storage device (e.g., SSD, HDD, or disk arrays), is a commonly used approach to achieve both high performance and cost efficiency in large-scale systems that need to store data with vastly different access characteristics. By aligning the access characteristics of the data, either fixed-sized extents or variable-sized files, to the characteristics of the storage devices, a higher performance can be achieved for any given cost. This article presents ExaPlan, a method to determine both the data-to-tier assignment and the number of devices in each tier that minimize the system's mean response time for a given budget and workload. In contrast to other methods that constrain or minimize the system load, ExaPlan directly minimizes the system's mean response time estimated by a queueing model. Minimizing the mean response time is typically intractable as the resulting optimization problem is both nonconvex and combinatorial in nature. ExaPlan circumvents this intractability by introducing a parameterized data placement approach that makes it a highly scalable method that can be easily applied to exascale systems. Through experiments that use parameters from real-world storage systems, such as CERN and LOFAR, it is demonstrated that ExaPlan provides solutions that yield lower mean response times than previous works. It supports standalone SSDs and HDDs as well as disk arrays as storage tiers, and although it uses a static workload representation, we provide empirical evidence that underlying dynamic workloads have invariant properties that can be deemed static for the purpose of provisioning a storage system. ExaPlan is also effective as a load-balancing tool used for placing data across devices within a tier, resulting in an up to 3.6-fold reduction of response time compared with a traditional load-balancing algorithm, such as the Longest Processing Time heuristic. © 2017 ACM.",Load balancing; Optimization; Queueing theory; Tiered storage; Workload modeling,Budget control; Hard disk storage; Large scale systems; Optimization; Queueing theory; Resource allocation; Response time (computer systems); Scalability; Virtual storage; Invariant properties; Load balancing algorithms; Longest processing time; Mean response time; Optimization problems; Tiered storage; Underlying dynamics; Work-load models; Digital storage
Optimizing file systems with fine-grained metadata journaling on byte-addressable NVM,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019687475&doi=10.1145%2f3060147&partnerID=40&md5=cd3f08a64d77a8f9118693512b2d166a,"Journaling file systems have been widely adopted to support applications that demand data consistency. However, we observed that the overhead of journaling can cause up to 48.2% performance drop under certain kinds of workloads. On the other hand, the emerging high-performance, byte-addressable Nonvolatile Memory (NVM) has the potential to minimize such overhead by being used as the journal device. The traditional journaling mechanism based on block devices is nevertheless unsuitable for NVM due to the write amplification of metadata journal we observed. In this article, we propose a fine-grained metadata journal mechanism to fully utilize the low-latency byte-addressable NVM so that the overhead of journaling can be significantly reduced. Based on the observation that conventional block-based metadata journal contains up to 90% clean metadata that is unnecessary to be journalled, we design a fine-grained journal format for byte-addressable NVM which contains only modified metadata. Moreover, we redesign the process of transaction committing, checkpointing, and recovery in journaling file systems utilizing the new journal format. Therefore, thanks to the reduced amount of ordered writes for journals, the overhead of journaling can be reduced without compromising the file system consistency. To evaluate our fine-grained metadata journaling mechanism, we have implemented a journaling file system prototype based on Ext4 and JBD2 in Linux. Experimental results show that our NVM-based fine-grained metadata journaling is up to 15.8× faster than the traditional approach under FileBench workloads. © 2017 ACM.",File system; Metadata journaling; Non-volatile memory,Computer operating systems; Cost reduction; Data storage equipment; Digital storage; File organization; Nonvolatile storage; Block devices; Check pointing; File system consistencies; File systems; Mechanism-based; Non-volatile memory; Traditional approaches; Write amplifications; Metadata
WiscKey: Separating keys from values in ssd-conscious storage,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017203512&doi=10.1145%2f3033273&partnerID=40&md5=079e81bca25a3071472d98ee061c1fe0,"We present WiscKey, a persistent LSM-Tree-based key-value store with a performance-oriented data layout that separates keys from values to minimize I/O amplification. The design of WiscKey is highly SSD optimized, leveraging both the sequential and random performance characteristics of the device.We demonstrate the advantages ofWiscKey with bothmicrobenchmarks and YCSB workloads. Microbenchmark results show that WiscKey is 2.5 × to 111× faster than LevelDB for loading a database (with significantly better tail latencies) and 1.6 × to 14 × faster for random lookups. WiscKey is faster than both LevelDB and RocksDB in all six YCSB workloads.",Flash-based SSDs; LevelDB; WiscKey,Flash-based SSDs; Trees (mathematics); Data layouts; Key-value stores; LevelDB; Micro-benchmark; Performance characteristics; Performance-oriented; Tree-based; WiscKey; Digital storage
"Writes wrought right, and other adventures in file system optimization",2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017175371&doi=10.1145%2f3032969&partnerID=40&md5=7c1f26f55e2d40a11b649a53f6f8f272,"File systems that employ write-optimized dictionaries (WODs) can perform random-writes, metadata updates, and recursive directory traversals orders ofmagnitude faster than conventional file systems. However, previous WOD-based file systems have not obtained all of these performance gains without sacrificing performance on other operations, such as file deletion, file or directory renaming, or sequential writes. Using three techniques, late-binding journaling, zoning, and range deletion, we show that there is no fundamental trade-off in write-optimization. These dramatic improvements can be retained while matching conventional file systems on all other operations. BetrFS 0.2 delivers order-of-magnitude better performance than conventional file systems on directory scans and small random writes and matches the performance of conventional file systems on rename, delete, and sequential I/O. For example, BetrFS 0.2 performs directory scans 2.2 faster, and small random writes over two orders of magnitude faster, than the fastest conventional file system. But unlike BetrFS 0.1, it renames and deletes files commensurate with conventional file systems and performs large sequential I/O at nearly disk bandwidth. The performance benefits of these techniques extend to applications as well. BetrFS 0.2 continues to outperform conventional file systems on many applications, such as as rsync, git-diff, and tar, but improves git-clone performance by 35% over BetrFS 0.1, yielding performance comparable to other file systems. © 2017 ACM 1553-3077/2017/03-ART3 15.00.",Bϵ-Trees; File system; Write optimization,Economic and social effects; Disk bandwidth; File systems; Late bindings; Orders of magnitude; Performance benefits; Performance Gain; Trade off; File organization
Introduction to the Special Issue on USENIX FAST 2016,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017099092&doi=10.1145%2f3039209&partnerID=40&md5=9fa1f2300afa422a1726f7133f9c90ae,[No abstract available],,
CDF-LDPC: A new error correction method for SSD to improve the read performance,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014218715&doi=10.1145%2f3017430&partnerID=40&md5=e4c706bcc95e8979d1c0433d81456ce9,"The raw error rate of a Solid-State drive (SSD) increases gradually with the increase of Program/Erase (P/E) cycles, retention time, and read cycles. Traditional approaches often use Error Correction Code (ECC) to ensure the reliability of SSDs. For error-free flash memory pages, time costs spent on ECC are redundant and make read performance suboptimal. This article presents a CRC-Detect-First LDPC (CDF-LDPC) algorithm to optimize the read performance of SSDs. The basic idea is to bypass Low-Density Parity-Check (LDPC) decoding of error-free flash memory pages, which can be found using a Cyclic Redundancy Check (CRC) code. Thus, error-free pages can be read directly without sacrificing the reliability of SSDs. Experiment results show that the read performance is improved more than 50% compared with traditional approaches. In particular, when idle time of benchmarks and SSD parallelism are exploited, CDF-LDPC can be performed more efficiently. In this case, the read performance of SSDs can be improved up to about 80% compared to that of the state-of-art. © 2017 ACM 1553-3077/2017/02-ART2 $15.00.",Error correction code; Error detection code; Low density parity check; Read performance; Solid-state drives,Codes (symbols); Data communication equipment; Error detection; Errors; Flash memory; Satellite communication systems; Error correction codes; Error Detection Code; Low density parity check; Read performance; Solid state drives; Error correction
Treating the storage stack like a network,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014212449&doi=10.1145%2f3032968&partnerID=40&md5=cf3fd4553ec821820ab26004e97b3c57,"In a data center, an IO from an application to distributed storage traverses not only the network but also several software stages with diverse functionality. This set of ordered stages is known as the storage or IO stack. Stages include caches, hypervisors, IO schedulers, file systems, and device drivers. Indeed, in a typical data center, the number of these stages is often larger than the number of network hops to the destination. Yet, while packet routing is fundamental to networks, no notion of IO routing exists on the storage stack. The path of an IO to an endpoint is predetermined and hard coded. This forces IO with different needs (e.g., requiring different caching or replica selection) to flow through a one-size-fits-all IO stack structure, resulting in an ossified IO stack. This article proposes sRoute, an architecture that provides a routing abstraction for the storage stack. sRoute comprises a centralized control plane and ""sSwitches"" on the data plane. The control plane sets the forwarding rules in each sSwitch to route IO requests at runtime based on application-specific policies. A key strength of our architecture is that it works with unmodified applications and Virtual Machines (VMs). This article shows significant benefits of customized IO routing to data center tenants: for example, a factor of 10 for tail IO latency, more than 60% better throughput for a customized replication protocol, a factor of 2 in throughput for customized caching, and enabling live performance debugging in a running system. © 2017 ACM 1553-3077/2017/02-ART2 $15.00.",Data centers; Routing; SDS; Software-defined storage; Storage; Storage stack,Application programs; Distributed computer systems; Energy storage; Network architecture; Program debugging; Application specific; Centralized control; Data centers; Distributed storage; Performance debugging; Replica selections; Replication protocol; Routing; Digital storage
Isotope: ACID transactions for block storage,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014139350&doi=10.1145%2f3032967&partnerID=40&md5=7d8a42b371e98f76326132e709f0f849,"Existing storage stacks are top heavy and expect little from block storage. As a result, new high-level storage abstractions-and new designs for existing abstractions-are difficult to realize, requiring developers to implement from scratch complex functionality such as failure atomicity and fine-grained concurrency control. In this article, we argue that pushing transactional isolation into the block store (in addition to atomicity and durability) is both viable and broadly useful, resulting in simpler high-level storage systems that provide strong semantics without sacrificing performance.We present Isotope, a new block store that supports ACID transactions over block reads and writes. Internally, Isotope uses a new multiversion concurrency control protocol that exploits fine-grained, subblock parallelism in workloads and offers both strict serializability and snapshot isolation guarantees. We implemented several high-level storage systems over Isotope, including two key-value stores that implement the LevelDB API over a hash table and B-tree, respectively, and a POSIX file system. We show that Isotope's block-level transactions enable systems that are simple (100s of lines of code), robust (i.e., providing ACID guarantees), and fast (e.g., 415MB/s for random file writes). We also show that these systems can be composed using Isotope, providing applications with transactions across different high-level constructs such as files, directories, and key-value pairs. © 2017 ACM 1553-3077/2017/02-ART2 $15.00.",Block storage; Isolation; Transaction,Abstracting; Isotopes; Semantics; ACID Transactions; Fine-grained concurrency; High-level storage; Isolation; Key-value stores; Multiversion concurrency control; Snapshot isolation; Transaction; Concurrency control
Customizable SLO and its near-precise enforcement for storage bandwidth,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014142006&doi=10.1145%2f2998454&partnerID=40&md5=2ccfb3f8b94171cd195e2f198e6a664a,"Cloud service is being adopted as a utility for large numbers of tenants by renting Virtual Machines (VMs). But for cloud storage, unpredictable IO characteristics make accurate Service-Level-Objective (SLO) enforcement challenging. As a result, it has been very difficult to support simple-to-use and technology-agnostic SLO specifying a particular value for a specific metric (e.g., storage bandwidth). This is because the quality of SLO enforcement depends on performance error and fluctuation that measure the precision of SLO enforcement. High precision of SLO enforcement is critical for user-oriented performance customization and user experiences. To address this challenge, this article presents V-Cup, a framework for VM-oriented customizable SLO and its near-precise enforcement. It consists of multiple auto-tuners, each of which exports an interface for a tenant to customize the desired storage bandwidth for a VM and enable the storage bandwidth of the VM to converge on the target value with a predictable precision. We design and implement V-Cup in the Xen hypervisor based on the fair sharing scheduler for VM-level resource management. Our V-Cup prototype evaluation shows that it achieves satisfying performance guarantees through near-precise SLO enforcement. © 2017 ACM 1553-3077/2017/02-ART2 $15.00.",Cloud storage; End-to-end control; Service-level objective; Storage management,Bandwidth; Cloud storages; Design and implements; End to end; Performance error; Performance guarantees; Resource management; Service level objective; User experience; Storage management
Exploiting I/O reordering and I/O interleaving to improve application launch performance,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014203310&doi=10.1145%2f3024094&partnerID=40&md5=ee5e9e3cf7de20e8eade0de4276dbdb0,"Application prefetchers improve application launch performance through either I/O reordering or I/O interleaving. However, there has been no proposal to combine the two techniques together,missing the opportunity for further optimization. We present a new application prefetching technique to take advantage of both the approaches. We evaluated our method with a set of applications to demonstrate that it reduces cold start application launch time by 50%, which is an improvement of 22% from the I/O reordering technique. © 2017 ACM 1553-3077/2017/02-ART2 $15.00.",Application launch performance; I/O interleaving; I/O reordering; Prefetching,Hardware; Cold start; New applications; Prefetching; Prefetching techniques; Data storage equipment
The design and implementation of a rekeying-aware encrypted deduplication storage system,2017,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014231613&doi=10.1145%2f3032966&partnerID=40&md5=3fb0c6bd15f8c9c965ec5e0228631b9f,"Rekeying refers to an operation of replacing an existing key with a new key for encryption. It renews security protection to protect against key compromise and enable dynamic access control in cryptographic storage. However, it is non-trivial to realize efficient rekeying in encrypted deduplication storage systems, which use deterministic content-derived encryption keys to allow deduplication on ciphertexts. We design and implement a rekeying-aware encrypted deduplication (REED) storage system. REED builds on a deterministic version of all-or-nothing transform, such that it enables secure and lightweight rekeying, while preserving the deduplication capability.We propose two REED encryption schemes that trade between performance and security and extend REED for dynamic access control.We implement a REED prototype with various performance optimization techniques and demonstrate how we can exploit similarity to mitigate key generation overhead. Our trace-driven testbed evaluation shows that our REED prototype maintains high performance and storage efficiency. © 2017 ACM 1553-3077/2017/02-ART2 $15.00.",Cloud storage; Deduplication; Encryption; Rekeying,Access control; Cloud storages; De duplications; Design and implementations; Design and implements; Dynamic access control; Performance optimizations; Re-keying; Security protection; Cryptography
Tools for predicting the reliability of large-scale storage systems,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050967770&doi=10.1145%2f2911987&partnerID=40&md5=7cfc55faa00d49a5468d0d45145b4f2a,"Data-intensive applications require extreme scaling of their underlying storage systems. Such scaling, together with the fact that storage systems must be implemented in actual data centers, increases the risk of data loss from failures of underlying components. Accurate engineering requires quantitatively predicting reliability, but this remains challenging due to the need to account for extreme scale, redundancy scheme type and strength, distribution architecture, and component dependencies. This article introduces CQSIM-R, a tool suite for predicting the reliability of large-scale storage system designs and deployments. CQSIM-R includes (a) direct calculations based on an only-drives-fail failure model and (b) an event-based simulator for detailed prediction that handles failures of and failure dependencies among arbitrary (drive or nondrive) components. These are based on a common combinatorial framework for modeling placement strategies. The article demonstrates CQSIM-R using models of common storage systems, including replicated and erasure coded designs. New results, such as the poor reliability scaling of spread-placed systems and a quantification of the impact of data center distribution and rack-awareness on reliability, demonstrate the usefulness and generality of the tools. Analysis and empirical studies show the tools' soundness, performance, and scalability. © 2016 Association for Computing Machinery. All rights reserved.",Large scale; Storage systems; Tools,Forecasting; Redundancy; Tools; Data-intensive application; Distribution architecture; Failure dependency; Large scale; Large-scale storage systems; Placement strategy; Storage systems; Underlying components; Digital storage
FlexDPDP: Flexlist-based optimized dynamic provable data possession,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983775262&doi=10.1145%2f2943783&partnerID=40&md5=cd8d450d2b7483c3c2ab49e2a515ccb7,"With increasing popularity of cloud storage, efficiently proving the integrity of data stored on an untrusted server has become significant. Authenticated skip lists and rank-based authenticated skip lists (RBASL) have been used to provide support for provable data update operations in cloud storage. However, in a dynamic file scenario, an RBASL based on block indices falls short when updates are not proportional to a fixed block size; such an update to the file, even if small, may result in O(n) updates on the data structure for a file with n blocks. To overcome this problem, we introduce FlexList, a flexible length-based authenticated skip list. FlexList translates variable-size updates to O(⌈u/B⌉) insertions, removals, or modifications, where u is the size of the update and B is the (average) block size. We further present various optimizations on the four types of skip lists (regular, authenticated, rank-based authenticated, and FlexList). We build such a structure in O(n) time and parallelize this operation for the first time. We compute one single proof to answer multiple (non)membership queries and obtain efficiency gains of 35%, 35%, and 40% in terms of proof time, energy, and size, respectively. We propose a method of handling multiple updates at once, achieving efficiency gains of up to 60% at the server side and 90% at the client side. We also deployed our implementation of FlexDPDP (dynamic provable data possession (DPDP) with FlexList instead of RBASL) on PlanetLab, demonstrating that FlexDPDP performs comparable to the most efficient static storage scheme (provable data possession (PDP)) while providing dynamic data support. © 2016 ACM.",Authenticated dictionary; Cloud storage; Data integrity; Provable data possession; Skip list,Query processing; Security of data; Cloud storages; Data integrity; Efficiency gain; Membership query; Provable data possessions; Skip listes; Static storage; Untrusted server; Digital storage
Write skew and Zipf distribution: Evidence and implications,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975510161&doi=10.1145%2f2908557&partnerID=40&md5=d809e70cc91c1e90047bdb6161b96de6,"Understanding workload characteristics is essential to storage systems design and performance optimization. With the emergence of flash memory as a new viable storage medium, the new design concern of flash endurance arises, necessitating a revisit of workload characteristics, in particular, of the write behavior. Inspired by Web caching studies where a Zipf-like access pattern is commonly found, we hypothesize that write count distribution at the block level may also follow Zipf's Law. To validate this hypothesis, we study 48 block I/O traces collected from a wide variety of real and benchmark applications. Through extensive analysis, we demonstrate that the Zipf-like pattern indeed widely exists in write traffic provided its disguises are removed by statistical processing. This finding implies that write skew in a large class of applications could be analytically expressed and, thus, facilitates design tradeoff explorations adaptive to workload characteristics. © 2016 ACM.",Flash memory; Workloads; Write traffic; Zipf's law,Benchmarking; Computational linguistics; Monolithic microwave integrated circuits; Access patterns; Benchmark applications; Performance optimizations; Statistical processing; Workload characteristics; Workloads; Zipf distribution; Zipf's law; Flash memory
MultiLanes: Providing virtualized storage for OS-level virtualization on manycores,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976383961&doi=10.1145%2f2801155&partnerID=40&md5=02f15c9e862b3f8b8dd580340749d589,"OS-level virtualization is often used for server consolidation in data centers because of its high efficiency. However, the sharing of storage stack services among the colocated containers incurs contention on shared kernel data structures and locks within I/O stack, leading to severe performance degradation on manycore platforms incorporating fast storage technologies (e.g., SSDs based on nonvolatile memories). This article presents MultiLanes, a virtualized storage system for OS-level virtualization on manycores. MultiLanes builds an isolated I/O stack on top of a virtualized storage device for each container to eliminate contention on kernel data structures and locks between them, thus scaling themtomanycores. Meanwhile, we propose a set of techniques to tune the overhead induced by storage-device virtualization to be negligible, and to scale the virtualized devices to manycores on the host, which itself scales poorly. To reduce the contention within each single container, we further propose SFS, which runs multiple file-system instances through the proposed virtualized storage devices, distributes all files under each directory among the underlying file-system instances, then stacks a unified namespace on top of them. The evaluation of our prototype system built for Linux container (LXC) on a 32-core machine with both a RAM disk and a modern flash-based SSD demonstrates that MultiLanes scales much better than Linux in micro- and macro-benchmarks, bringing significant performance improvements, and that MultiLanes with SFS can further reduce the contention within each single container. © 2016 ACM.",Fast storage; Manycores; OS-level virtualization; Performance isolation; Scalability,Benchmarking; Computer operating systems; Containers; Data structures; File organization; Flash-based SSDs; Linux; Locks (fasteners); Nonvolatile storage; Random access storage; Scalability; Virtual reality; Virtual storage; Manycores; Non-volatile memory; Performance degradation; Performance isolation; Prototype system; Server consolidation; Storage technology; Virtualizations; Digital storage
LDM: Log disk mirroring with improved performance and reliability for SSD-based disk arrays,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971468108&doi=10.1145%2f2892639&partnerID=40&md5=11b62b509a20a39b59222716c3f010a3,"With the explosive growth in data volume, the I/O bottleneck has become an increasingly daunting challenge for big data analytics. Economic forces, driven by the desire to introduce flash-based Solid-State Drives (SSDs) into the high-end storage market, have resulted in hybrid storage systems in the cloud. However, a single flash-based SSD cannot satisfy the performance, reliability, and capacity requirements of enterprise or HPC storage systems in the cloud.While an array of SSDs organized in a RAID structure, such as RAID5, provides the potential for high storage capacity and bandwidth, reliability and performance problems will likely result from the parity update operations. In this article, we propose a Log Disk Mirroring scheme (LDM) to improve the performance and reliability of SSD-based disk arrays. LDM is a hybrid disk array architecture that consists of several SSDs and two hard disk drives (HDDs). In an LDM array, the two HDDs are mirrored as a write buffer that temporally absorbs the small write requests. The small and random write data are written on the mirroring buffer by using the logging technique that sequentially appends new data. The small write data are merged and destaged to the SSD-based disk array during the system idle periods. Our prototype implementation of the LDM array and the performance evaluations show that the LDM array significantly outperforms the pure SSD-based disk arrays by a factor of 20.4 on average, and outperforms HPDA by a factor of 5.0 on average. The reliability analysis shows that the MTTDL of the LDM array is 2.7 times and 1.7 times better than that of pure SSD-based disk arrays and HPDA disk arrays. © 2016 ACM.",Disk buffer; Log technique; Performance evaluation; Reliability analysis; Ssd-based disk arrays,Big data; Digital storage; Hard disk storage; Reliability; Reliability analysis; Disk array; Disk buffer; Hybrid storage systems; Log technique; Performance and reliabilities; Performance evaluation; Performance problems; Prototype implementations; Flash-based SSDs
Efficient deduplication in a distributed primary storage infrastructure,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971406228&doi=10.1145%2f2876509&partnerID=40&md5=d7047009b19e637467d3ef724851f0b6,"A large amount of duplicate data typically exists across volumes of virtual machines in cloud computing infrastructures. Deduplication allows reclaiming these duplicates while improving the cost-effectiveness of large-scale multitenant infrastructures. However, traditional archival and backup deduplication systems impose prohibitive storage overhead for virtual machines hosting latency-sensitive applications. Primary deduplication systems reduce such penalty but rely on special cluster filesystems, centralized components, or restrictive workload assumptions. Also, some of these systems reduce storage overhead by confining deduplication to off-peak periods that may be scarce in a cloud environment. We present DEDIS, a dependable and fully decentralized system that performs cluster-wide off-line deduplication of virtual machines' primary volumes. DEDIS works on top of any unsophisticated storage backend, centralized or distributed, as long as it exports a basic shared block device interface. Also, DEDIS does not rely on data locality assumptions and incorporates novel optimizations for reducing deduplication overhead and increasing its reliability. The evaluation of an open-source prototype shows that minimal I/O overhead is achievable even when deduplication and intensive storage I/O are executed simultaneously. Also, our design scales out and allows collocating DEDIS components and virtualmachines in the same servers, thus, sparing the need of additional hardware. © 2016 ACM.",Deduplication; Distributed systems; Primary storage,Cloud computing; Cost effectiveness; Cost reduction; Centralized components; Cloud computing infrastructures; Cloud environments; De duplications; Decentralized system; Distributed systems; Primary storages; Sensitive application; Digital storage
Efficient Memory-Mapped I/O on Fast Storage Device,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971385355&doi=10.1145%2f2846100&partnerID=40&md5=6177a055073a5b9ef96db3647a5a358c,"In modern operating systems, memory-mapped I/O (mmio) is an important access method that maps a file or file-like resource to a region of memory. The mapping allows applications to access data from files through memory semantics (i.e., load/store) and it provides ease of programming. The number of applications that use mmio are increasing because memory semantics can provide better performance than file semantics (i.e., read/write). As more data are located in the main memory, the performance of applications can be enhanced owing to the effect of a large cache. When mmio is used, hot data tend to reside in the main memory and cold data are located in storage devices such as HDD and SSD; data placement in the memory hierarchy depends on the virtual memory subsystem of the operating system. Generally, the performance of storage devices has a direct impact on the performance of mmio. It is widely expected that better storage devices will lead to better performance. However, the expectation is limited when fast storage devices are used since the virtual memory subsystem does not reflect the performance feature of those devices. In this article, we examine the Linux virtualmemory subsystem and mmio path to determine the influence of fast storage on the existing Linux kernel. Throughout our investigation, we find that the overhead of the Linux virtual memory subsystem, negligible on the HDD, prevents applications from using the full performance of fast storage devices. To reduce the overheads and fully exploit the fast storage devices, we present several optimization techniques. We modify the Linux kernel to implement our optimization techniques and evaluate our prototyped system with low-latency storage devices. Experimental results show that our optimized mmio has up to 7x better performance than the original mmio.We also compare our system to a system that has enough memory to keep all data in the main memory. The system with insufficient memory and our mmio achieves 92% performance of the resource-rich system. This result implies that our virtual memory subsystem for mmap can effectively extend the main memory with fast storage devices. © 2016 ACM.",Data-intensive; Memory-mapped; Nonvolatile memory; Virtual memory system,Computer operating systems; Digital storage; Hard disk storage; Linux; Memory architecture; Semantics; Virtual storage; Access methods; Data intensive; Data placement; Memory hierarchy; Non-volatile memory; Optimization techniques; Virtual memory; Virtual memory systems; Cache memory
SWANS: An interdisk wear-leveling strategy for RAID-0 structured SSD arrays,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973299608&doi=10.1145%2f2756555&partnerID=40&md5=f310ff42a2b60ab500cfaa90e95524f3,"NAND flash memory-based solid state disks (SSDs) have been widely used in enterprise servers. However, flash memory has limited write endurance, as a block becomes unreliable after a finite number of program/erase cycles. Existing wear-leveling techniques are essentially intradisk data distribution schemes, as they can only even wear out across the flash medium within a single SSD. When multiple SSDs are organized in an array manner in server applications, an interdisk wear-leveling technique, which can ensure a uniform wear-out distribution across SSDs, is much needed. In this article, we propose a novel SSD-array level wear leveling strategy called SWANS (Smoothing Wear Across N SSDs) for an SSD array structured in a RAID-0 format, which is frequently used in server applications. SWANS dynamically monitors and balances write distributions across SSDs in an intelligent way. Further, to evaluate its effectiveness, we build an SSD array simulator on top of a validated single SSD simulator. Next, SWANS is implemented in its array controller. Comprehensive experiments with real-world traces show that SWANS decreases the standard deviation of writes across SSDs on average by 16.7x. The gap in the total bytes written between the most written SSD and the least written SSD in an 8-SSD array shrinks at least 1.3x. © 2016 ACM.",NAND flash memory; Solid state disk; SSD array; Wear-leveling,Flash memory; Hard disk storage; Memory architecture; Monolithic microwave integrated circuits; NAND circuits; Data distribution schemes; Enterprise servers; NAND flash memory; Server applications; Solid state disks; SSD array; Standard deviation; Wear leveling; Flash-based SSDs
Storage workload identification,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973375199&doi=10.1145%2f2818716&partnerID=40&md5=6840b6d5d9a97a5dabb2c9d3670b3486,"Storage workload identification is the task of characterizing a workload in a storage system (more specifically, network storage system-NAS or SAN) and matching it with the previously known workloads. We refer to storage workload identification as ""workload identification"" in the rest of this article. Workload identification is an important problem for cloud providers to solve because (1) providers can leverage this information to colocate similar workloads to make the system more predictable and (2) providers can identify workloads and subsequently give guidance to the subscribers as to associated best practices (with respect to configuration) for provisioning those workloads. Historically, people have identified workloads by looking at their read/write ratios, random/sequential ratios, block size, and interarrival frequency. Researchers are well aware that workload characteristics change over time and that one cannot just take a point in time view of a workload, as that will incorrectly characterize workload behavior. Increasingly, manual detection of workload signature is becoming harder because (1) it is difficult for a human to detect a pattern and (2) representing a workload signature by a tuple consisting of average values for each of the signature components leads to a large error. In this article, we present workload signature detection and a matching algorithm that is able to correctly identify workload signatures and match them with other similar workload signatures. We have tested our algorithm on nine different workloads generated using publicly available traces and on real customer workloads running in the field to show the robustness of our approach. © 2016 ACM.",Storage workload identification; Workload signature,Algorithms; Average values; Cloud providers; Matching algorithm; Network storage systems; Signature detection; Storage systems; Workload characteristics; Workload signature; Data storage equipment
Internal parallelism of flash memory-based solid-state drives,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973402251&doi=10.1145%2f2818376&partnerID=40&md5=1b2648a01a16b6649c784f1045e4338d,"A unique merit of a solid-state drive (SSD) is its internal parallelism. In this article, we present a set of comprehensive studies on understanding and exploiting internal parallelism of SSDs. Through extensive experiments and thorough analysis, we show that exploiting internal parallelism of SSDs can not only substantially improve input/output (I/O) performance but also may lead to some surprising side effects and dynamics. For example, we find that with parallel I/Os, SSD performance is no longer highly sensitive to access patterns (random or sequential), but rather to other factors, such as data access interferences and physical data layout. Many of our prior understandings about SSDs also need to be reconsidered. For example, we find that with parallel I/Os, write performance could outperform reads and is largely independent of access patterns, which is opposite to our long-existing common understanding about slow random writes on SSDs. We have also observed a strong interference between concurrent reads and writes as well as the impact of physical data layout to parallel I/O performance. Based on these findings, we present a set of case studies in database management systems, a typical data-intensive application. Our case studies show that exploiting internal parallelism is not only the key to enhancing application performance, and more importantly, it also fundamentally changes the equation for optimizing applications. This calls for a careful reconsideration of various aspects in application and system designs. Furthermore, we give a set of experimental studies on new-generation SSDs and the interaction between internal and external parallelism in an SSD-based Redundant Array of Independent Disks (RAID) storage. With these critical findings, we finally make a set of recommendations to system architects and application designers for effectively exploiting internal parallelism. © 2016 ACM.",Flash memory; Internal parallelism; Solid state drive; Storage systems,Digital storage; Flash memory; Information management; Monolithic microwave integrated circuits; Servers; Application performance; Data-intensive application; Internal parallelism; Redundant array of independent disks; Solid state drives; Solid state drives (SSD); Storage systems; Strong interference; Flash-based SSDs
A user-friendly log viewer for storage systems,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973352504&doi=10.1145%2f2846101&partnerID=40&md5=ab251eb93100e81577cbcf5333fe8c1c,"System log files contains messages emitted from several modules within a system and carries valuable information about the system state such as device status and error conditions and also about the various tasks within the system such as program names, execution path, including function names and parameters, and the task completion status. For customers with remote support, the system collects and transmits these logs to a central enterprise repository, where these are monitored for alerts, problem forecasting, and troubleshooting. Very large log files limit the interpretability for the support engineers. For an expert, a large volume of log messages may not pose any problem; however, an inexperienced person may get flummoxed due to the presence of a large number of log messages. Often it is desired to present the log messages in a comprehensive manner where a person can view the important messages first and then go into details if required. In this article, we present a user-friendly log viewer where we first hide the unimportant or inconsequential messages from the log file. A user can then click a particular hidden view and get the details of the hided messages. Messages with low utility are considered inconsequential as their removal does not impact the end user for the aforesaid purpose such as problem forecasting or troubleshooting. We relate the utility of a message to the probability of its appearance in the due context. We present machine-learning-based techniques that computes the usefulness of individual messages in a log file. We demonstrate identification and discarding of inconsequential messages to shrink the log size to acceptable limits. We have tested this over real-world logs and observed that eliminating such low value data can reduce the log files significantly (30% to 55%), with minimal error rates (7% to 20%). When limited user feedback is available, we show modifications to the technique to learn the user intent and accordingly further reduce the error. © 2016 ACM.",Filtering; Learning; Log reduction,Artificial intelligence; Digital storage; Filtration; Human computer interaction; Learning systems; Acceptable limit; Central enterprise; Error condition; Important messages; Interpretability; Learning; Log reductions; Storage systems; Errors
Does RAID improve lifetime of SSD Arrays?,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973320149&doi=10.1145%2f2764915&partnerID=40&md5=c4cb7cf2b4ed45df1148b6a5530c1b0a,"Parity protection at the system level is typically employed to compose reliable storage systems. However, careful consideration is required when SSD-based systems employ parity protection. First, additional writes are required for parity updates. Second, parity consumes space on the device, which results in write amplification from less efficient garbage collection at higher space utilization. This article analyzes the effectiveness of SSD-based RAID and discusses the potential benefits and drawbacks in terms of reliability. A Markov model is presented to estimate the lifetime of SSD-based RAID systems in different environments. In a small array, our results show that parity protection provides benefit only with considerably low space utilizations and low data access rates. However, in a large system, RAID improves data lifetime even when we take write amplification into account. © 2016 ACM.",Flash memory; Lifetime; MTTDL; RAID; SSD; Write amplification,Markov processes; Garbage collection; Lifetime; MTTDL; Parity protection; Potential benefits; RAID; Space utilization; Write amplifications; Flash memory
H-Scale: A fast approach to scale disk arrays via hybrid stripe deployment,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973360124&doi=10.1145%2f2822895&partnerID=40&md5=9508d26fa21199b7bea47559407da678,"To satisfy the explosive growth of data in large-scale data centers, where redundant arrays of independent disks (RAIDs), especially RAID-5, are widely deployed, effective storage scaling and disk expansion methods are desired. However, a way to reduce the data migration overhead and maintain the reliability of the original RAID are major concerns of storage scaling. To address these problems, we propose a new RAID scaling scheme, H-Scale, to achieve fast RAID scaling via hybrid stripe layouts. H-Scale takes advantage of the loose restriction of stripe structures to choose migrated data and to create hybrid stripe structures. The main advantages of our scheme include: (1) dramatically reducing the data migration overhead and thus speeding up the scaling process, (2) maintaining the original RAID's reliability, (3) balancing the workload among disks after scaling, and (4) providing a general scaling approach for different RAID levels. Our theoretical analysis show that H-Scale outperforms existing scaling solutions in terms of data migration, I/O overheads, and parity update operations. Evaluation results on a prototype implementation demonstrate that H-Scale speeds up the online scaling process by up to 60% under SPC traces, and similar improvements on scaling time and user response time are also achieved by evaluations using standard benchmarks. © 2016 ACM.",Hybrid stripe; RAID; RAID scaling,Cost reduction; Evaluation results; Expansion methods; Hybrid stripe; Prototype implementations; RAID; RAID scaling; Scaling solutions; User response time; Digital storage
Trueerase: Leveraging an auxiliary data path for per-file secure deletion,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971422840&doi=10.1145%2f2854882&partnerID=40&md5=27cd8221bd3c43f7642330149f1c99c5,"One important aspect of privacy is the ability to securely delete sensitive data from electronic storage in such a way that it cannot be recovered; we call this action secure deletion. Short of physically destroying the entire storage medium, existing software secure-deletion solutions tend to be piecemeal at best - They may only work for one type of storage or file system, may force the user to delete all files instead of selected ones, may require the added complexities of encryption and key storage, may require extensive changes and additions to the computer's operating system or storage firmware, and may not handle system crashes gracefully. We present TrueErase, a holistic secure-deletion framework for individual systems that contain sensitive data. Through design, implementation, verification, and evaluation on both a hard drive and NAND flash, TrueErase shows that it is possible to construct a per-file, secure-deletion framework that can accommodate different storage media and legacy file systems, require limited changes to legacy systems, and handle common crash scenarios. TrueErase can serve as a building block by cryptographic systems that securely delete information by erasing encryption keys. The overhead is dependent on spatial locality, number of sensitive files, and workload (computational- or I/O-bound). © 2016 ACM.",Assured deletion; File systems; NAND flash; Secure deletion; Security; Storage,Computer operating systems; Cryptography; Energy storage; File organization; Firmware; Legacy systems; Memory architecture; Assured deletion; File systems; NAND Flash; Secure deletion; Security; Digital storage
Exploiting sequential and temporal localities to improve performance of NAND flash-based SSDs,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973375693&doi=10.1145%2f2905054&partnerID=40&md5=7ce86bf1b757785c76c0af632223f5a0,"NAND flash-based Solid-State Drives (SSDs) are becoming a viable alternative as a secondary storage solution for many computing systems. Since the physical characteristics of NAND flash memory are different from conventional Hard-Disk Drives (HDDs), flash-based SSDs usually employ an intermediate software layer, called a Flash Translation Layer (FTL). The FTL runs several firmware algorithms for logical-to physical mapping, I/O interleaving, garbage collection, wear-leveling, and so on. These FTL algorithms not only have a great effect on storage performance and lifetime, but also determine hardware cost and data integrity. In general, a hybrid FTL scheme has been widely used in mobile devices because it exhibits high performance and high data integrity at a low hardware cost. Recently, a demand-based FTL based on pagelevel mapping has been rapidly adopted in high-performance SSDs. The demand-based FTL more effectively exploits the device-level parallelism than the hybrid FTL and requires a small amount of memory by keeping only popular mapping entriesin DRAM. Becauseofthis caching mechanism, however, the demand-based FTL is not robust enough for power failures and requires extra reads tofetch missing mapping entries from NAND flash. In this article, we propose a new flash translation layer called LAST++. The proposed LAST++ scheme is based on the hybrid FTL, thus it has the inherent benefits of the hybrid FTL, including low resource requirements, strong robustness for power failures, and high read performance. By effectively exploiting the locality of I/O references, LAST++ increases device-level parallelism and reduces garbage collection overheads. This leads to a great improvement of I/O performance and makes it possible to overcome the limitations of the hybrid FTL. Our experimental results show that LAST++ outperforms the demand-based FTL by 27% for writes and 7% for reads, on average, while offering higher robustness against sudden power failures. LAST++ also improves write performance by 39%, on average, over the existing hybrid FTL. © 2016 ACM.",Address mapping; Flash translation layer; Garbage collection,Dynamic random access storage; Firmware; Hard disk storage; Hardware; Mapping; Memory architecture; NAND circuits; Outages; Refuse collection; Address mappings; Flash translation layer; Garbage collection; Improve performance; Page-level mappings; Physical characteristics; Resource requirements; Storage performance; Flash-based SSDs
Efficient dynamic provable possession of remote data via update trees,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034272920&doi=10.1145%2f2747877&partnerID=40&md5=ad4685095c7ab5251d882c080e006ee3,"The emergence and wide availability of remote storage service providers prompted work in the security community that allows clients to verify integrity and availability of the data that they outsourced to a not fully trusted remote storage server at a relatively low cost. Most recent solutions to this problem allow clients to read and update (i.e., insert, modify, or delete) stored data blocks while trying to lower the overhead associated with verifying the integrity of the stored data. In this work, we develop a novel scheme, performance of which favorably compares with the existing solutions. Our solution additionally enjoys a number of new features, such as a natural support for operations on ranges of blocks, revision control, and support for multiple user access to shared content. The performance guarantees that we achieve stem from a novel data structure called a balanced update tree and removing the need for interaction during update operations in addition to communicating the updates themselves. © 2016 ACM.",Balanced tree; Integrity verification; Outsourced storage; Provable data possession,Digital storage; Forestry; Balanced trees; Integrity verifications; Outsourced storages; Performance guarantees; Provable data possessions; Remote storage service; Revision control; Security community; Trees (mathematics)
Classifying data to reduce long-term data movement in shingled write disks,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964797813&doi=10.1145%2f2851505&partnerID=40&md5=bc36c128131e3c2f9bfa774b232de4d2,"Shingled magnetic recording (SMR) is a means of increasing the density of hard drives that brings a new set of challenges. Due to the nature of SMR disks, updating in place is not an option. Holes left by invalidated data can only be filled if the entire band is reclaimed, and a poor band compaction algorithm could result in spending a lot of time moving blocks over the lifetime of the device. We propose using write frequency to separate blocks to reduce data movement and develop a band compaction algorithm that implements this heuristic. We demonstrate how our algorithm results in improved data management, resulting in an up to 45% reduction in required data movements when compared to naive approaches to band management. © 2016 ACM.",Data placement; Shingled magnetic recording drives; Shingled write disks; Storage,Classification (of information); Compaction; Digital storage; Energy storage; Information management; Magnetic recording; Magnetic storage; Compaction algorithms; Data movements; Data placement; Hard drives; Moving block; Shingled magnetic recording (SMR); Shingled magnetic recordings; Shingled write disks; Data reduction
Improving flash-based disk cache with lazy adaptive replacement,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964769117&doi=10.1145%2f2737832&partnerID=40&md5=6748dd433307fdaf1af612218ce9ee85,"For years, the increasing popularity of flash memory has been changing storage systems. Flash-based solidstate drives (SSDs) are widely used as a new cache tier on top of hard disk drives (HDDs) to speed up data-intensive applications. However, the endurance problem of flash memory remains a concern and is getting worse with the adoption of MLC and TLC flash. In this article, we propose a novel cache management algorithm for flash-based disk cache named Lazy Adaptive Replacement Cache (LARC). LARC adopts the idea of selective caching to filter out seldom accessed blocks and prevent them from entering cache. This avoids cache pollution and preserves popular blocks in cache for a longer period of time, leading to a higher hit rate. Meanwhile, by avoiding unnecessary cache replacements, LARC reduces the volume of data written to the SSD and yields an SSD-friendly access pattern. In this way, LARC improves the performance and endurance of the SSD at the same time. LARC is self- Tuning and incurs little overhead. It has been extensively evaluated by both trace-driven simulations and synthetic benchmarks on a prototype implementation. Our experiments show that LARC outperforms state-of- Art algorithms for different kinds of workloads and extends SSD lifetime by up to 15.7 times. © 2016 ACM 1553-3077/2016/02-ART8 $15.00.",Cache algorithm; Endurance; Flash memory; Solid-state drive,Cache memory; Digital storage; Durability; Flash memory; Hard disk storage; Monolithic microwave integrated circuits; Cache algorithms; Cache replacement; Data-intensive application; Hard disk drives; Prototype implementations; Solid state drives; Synthetic benchmark; Trace driven simulation; Flash-based SSDs
Can we group storage? Statistical techniques to identify predictive groupings in storage system accesses,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957094535&doi=10.1145%2f2738042&partnerID=40&md5=fe4ab1212f9320d3d64a0ab384884467,"Storing large amounts of data for different users has become the new normal in a modern distributed cloud storage environment. Storing data successfully requires a balance of availability, reliability, cost, and performance. Typically, systems design for this balance with minimal information about the data that will pass through them. We propose a series of methods to derive groupings from data that have predictive value, informing layout decisions for data on disk. Unlike previous grouping work, we focus on dynamically identifying groupings in data that can be gathered from active systems in real time with minimal impact using spatiotemporal locality. We outline several techniques we have developed and discuss how we select particular techniques for particular workloads and application domains. Our statistical and machine-learning-based grouping algorithms answer questions such as ""What can a grouping be based on?"" and ""Is a given grouping meaningful for a given application?"" We design our models to be flexible and require minimal domain information so that our results are as broadly applicable as possible. We intend for this work to provide a launchpad for future specialized system design using groupings in combination with caching policies and architectural distinctions such as tiered storage to create the next generation of scalable storage systems. © 2016 ACM.",Data layout; Predictive modeling; Storage optimization; Tiered storage,Artificial intelligence; Learning systems; Real time systems; Systems analysis; Data layouts; Domain informations; Large amounts of data; Predictive modeling; Scalable storage systems; Statistical techniques; Storage optimization; Tiered storage; Digital storage
"NANDFlashSim: High-fidelity, microarchitecture-aware NAND flash memory simulation",2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957095686&doi=10.1145%2f2700310&partnerID=40&md5=6f2c58ae2a0d7f22decb0d4793d7bf26,"As the popularity of NAND flash expands in arenas from embedded systems to high-performance computing, a high-fidelity understanding of its specific properties becomes increasingly important. Further, with the increasing trend toward multiple-die, multiple-plane architectures and high-speed interfaces, flash memory systems are expected to continue to scale and cheapen, resulting in their broader proliferation. However, when designing NAND-based devices, making decisions about the optimal system configuration is nontrivial, because flash is sensitive to a number of parameters and suffers from inherent latency variations, and no available tools suffice for studying these nuances. The parameters include the architectures, such as multidie and multiplane, diverse node technologies, bit densities, and cell reliabilities. Therefore, we introduce NANDFlashSim, a high-fidelity, latency-variation-aware, and highly configurable NAND-flash simulator, which implements a detailed timing model for 16 state-of-the-art NAND operations. Using NANDFlashSim, we notably discover the following. First, regardless of the operation, reads fail to leverage internal parallelism. Second, MLC provides lower I/O bus contention than SLC, but contention becomes a serious problem as the number of dies increases. Third, many-die architectures outperform many-plane architectures for disk-friendly workloads. Finally, employing a high-performance I/O bus or an increased page size does not enhance energy savings. Our simulator is available at http://nfs.camelab.org. © 2016 ACM.",Cycle-level simulation; NAND flash memory; Non-volatile memory; Performance evaluation; Solid state disk,Data storage equipment; Digital storage; Embedded systems; Energy conservation; Memory architecture; Monolithic microwave integrated circuits; NAND circuits; System buses; Cycle-level simulation; NAND flash memory; Non-volatile memory; Performance evaluation; Solid state disks; Flash memory
GCTrees: Garbage collecting snapshots,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964788357&doi=10.1145%2f2857056&partnerID=40&md5=596263d620a2b607b7bc473ea5fb09c8,"File-system snapshots have been a key component of enterprise storage management since their inception. Creating and managing them efficiently, while maintaining flexibility and low overhead, has been a constant struggle. Although the current state-of-the-art mechanism-hierarchical reference counting-performs reasonably well for traditional small-file workloads, these workloads are increasingly vanishing from the enterprise data center, replaced instead with virtual machine and database workloads. These workloads center around a few very large files, violating the assumptions that allow hierarchical reference counting to operate efficiently. To better cope with these workloads, we introduce Generational Chain Trees (GCTrees), a novel method of space management that uses concepts of block lineage across snapshots rather than explicit reference counting. As a proof of concept, we create a prototype file system-gcext4, a modified version of ext4 that uses GCTrees as a basis for snapshots and copy-on-write. In evaluating this prototype empirically, we find that although they have a somewhat higher overhead for traditional workloads, GCTrees have dramatically lower overhead than hierarchical reference counting for large-file workloads, improving by a factor of 34 or more in some cases. Furthermore, gcext4 performs comparably to ext4 across all workloads, showing that GCTrees impose minor cost for their benefits. © 2016 ACM.",Clones; Data management; File systems; Snapshots,Cloning; Digital storage; File organization; Storage management; Database workload; Enterprise storage management; File system snapshots; File systems; Proof of concept; Reference counting; Snapshots; Virtual machines; Information management
Introduction to the special issue on MSST 2015,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964403119&doi=10.1145%2f2853993&partnerID=40&md5=df551fd0d0e6d28367d71f30b2530041,[No abstract available],,
LoneStar RAID: Massive array of offline disks for archival systems,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954308590&doi=10.1145%2f2840810&partnerID=40&md5=6ed563e57daadbd428a40ddb63710b3c,"The need for huge storage archives rises with the ever growing creation of data. With today's big data and data analytics applications, some of these huge archives become active in the sense that all stored data can be accessed at any time. Running and evolving these archives is a constant tradeoff between performance, capacity, and price.We present the LoneStar RAID, a disk-based storage architecture, which focuses on high reliability, low energy consumption, and cheap reads. It is designed for MAID systems with up to hundreds of disk drives per server and is optimized for ""write once, read sometimes"" workloads. We use dedicated data and parity disks, and export the data disks as individually accessible buckets. By intertwining disk groups into a two-dimensional RAID and improving single-disk reliability with intradisk redundancy, the system achieves an elastic fault tolerance that can at least recover all 3-disk failures. Furthermore, we integrate a cache to offload parity updates and a journal to track the RAID's state. The LoneStar RAID scheme provides a mean time to data loss (MTTDL) that competes with today's erasure codes and is optimized to require only a minimal set of running disk drives.",archival; disk-based; MAID; reliability,Digital storage; Energy utilization; Fault tolerance; Redundancy; Reliability; archival; Archival systems; Disk based storage; Disk reliabilities; Disk-based; High reliability; Low energy consumption; MAID; Big data
Blurred persistence: Efficient transactions in persistent memory,2016,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954308883&doi=10.1145%2f2851504&partnerID=40&md5=fe4f34c573475940426de56d39d0664f,"Persistent memory provides data durability in main memory and enables memory-level storage systems. To ensure consistency of such storage systems, memory writes need to be transactional and are carefully moved across the boundary between the volatile CPU cache and the persistent main memory. Unfortunately, cache management in the CPU cache is hardware-controlled. Legacy transaction mechanisms, which are designed for disk-based storage systems, are inefficient in ordered data persistence of transactions in persistent memory. In this article, we propose the Blurred Persistence mechanism to reduce the transaction overhead of persistent memory by blurring the volatility-persistence boundary. Blurred Persistence consists of two techniques. First, Execution in Log executes a transaction in the log to eliminate duplicated data copies for execution. It allows persistence of the volatile uncommitted data, which are detectable with reorganized log structure. Second, Volatile Checkpoint with Bulk Persistence allows the committed data to aggressively stay volatile by leveraging the data durability in the log, as long as the commit order across threads is kept. By doing so, it reduces the frequency of forced persistence and improves cache efficiency. Evaluations show that our mechanism improves system performance by 56.3% to 143.7% for a variety of workloads.",,Digital storage; Durability; Legacy systems; Cache efficiency; Cache management; Disk based storage; Persistent main memory; Persistent memory; Storage systems; Transaction mechanism; Volatility persistence; Cache memory
A Memory-Disaggregated Radix Tree,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196753715&doi=10.1145%2f3664289&partnerID=40&md5=58e0d0df52161df1bf71e928025d9a53,"Disaggregated memory (DM) is an increasingly prevalent architecture with high resource utilization. It separates computing and memory resources into two pools and interconnects them with fast networks. Existing range indexes on DM are based on B+ trees, which suffer from large inherent read and write amplifications. The read and write amplifications rapidly saturate the network bandwidth, resulting in low request throughput and high access latency of B+ trees on DM.In this article, we propose that the radix tree is more suitable for DM than the B+ tree due to smaller read and write amplifications. However, constructing a radix tree on DM is challenging due to the costly lock-based concurrency control, the bounded memory-side IOPS, and the complicated computing-side cache validation. To address these challenges, we design SMART, the first radix tree for disaggregated memory with high performance. Specifically, we leverage (1) a hybrid concurrency control scheme including lock-free internal nodes and fine-grained lock-based leaf nodes to reduce lock overhead, (2) a computing-side read-delegation and write-combining technique to break through the IOPS upper bound by reducing redundant I/Os, and (3) a simple yet effective reverse check mechanism for computing-side cache validation. Experimental results show that SMART achieves 6.1× higher throughput under typical write-intensive workloads and 2.8× higher throughput under read-only workloads in YCSB benchmarks, compared with state-of-the-art B+ trees on DM.  © 2024 Copyright held by the owner/author(s).",Disaggregated memory; radix tree; remote direct memory access,Cache memory; Concurrency control; Memory architecture; B trees; Computing resource; Dis-aggregated memory; High-throughput; Memory resources; Radix tree; Read amplifications; Remote direct memory access; Resources utilizations; Write amplifications; Locks (fasteners)
An End-to-end High-performance Deduplication Scheme for Docker Registries and Docker Container Storage Systems,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196207961&doi=10.1145%2f3643819&partnerID=40&md5=14d9e162b36ed599f825a3e2b0717c87,"The wide adoption of Docker containers for supporting agile and elastic enterprise applications has led to a broad proliferation of container images. The associated storage performance and capacity requirements place a high pressure on the infrastructure of container registries that store and distribute images and container storage systems on the Docker client side that manage image layers and store ephemeral data generated at container runtime. The storage demand is worsened by the large amount of duplicate data in images. Moreover, container storage systems that use Copy-on-Write (CoW) file systems as storage drivers exacerbate the redundancy. Exploiting the high file redundancy in real-world images is a promising approach to drastically reduce the growing storage requirements of container registries and improve the space efficiency of container storage systems. However, existing deduplication techniques significantly degrade the performance of both registries and container storage systems because of data reconstruction overhead as well as the deduplication cost.We propose DupHunter, an end-to-end deduplication scheme that deduplicates layers for both Docker registries and container storage systems while maintaining a high image distribution speed and container I/O performance. DupHunter is divided into three tiers: registry tier, middle tier, and client tier. Specifically, we first build a high-performance deduplication engine at the registry tier that not only natively deduplicates layers for space savings but also reduces layer restore overhead. Then, we use deduplication offloading at the middle tier to eliminate the redundant files from the client tier and avoid bringing deduplication overhead to the clients. To further reduce the data duplicates caused by CoWs and improve the container I/O performance, we utilize a container-aware storage system at the client tier that reserves space for each container and arranges the placement of files and their modifications on the disk to preserve locality. Under real workloads, DupHunter reduces storage space by up to 6.9× and reduces the GET layer latency up to 2.8× compared to the state-of-the-art. Moreover, DupHunter can improve the container I/O performance by up to 93% for reads and 64% for writes. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deduplication; Docker registry; docker storage driver; linux file system,Digital storage; File organization; Image enhancement; Information management; Linux; Redundancy; Deduplication; Docker registry; Docker storage driver; End to end; Enterprise applications; Linux file system; Performance; Storage capacity; Storage performance; Storage systems; Containers
Introduction to the Special Section on USENIX OSDI 2023,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196361409&doi=10.1145%2f3654801&partnerID=40&md5=9668d3c79af338eb3f576a8b418d7a31,[No abstract available],,
eZNS: Elastic Zoned Namespace for Enhanced Performance Isolation and Device Utilization,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196503303&doi=10.1145%2f3653716&partnerID=40&md5=2bad9189e2ab11894c5c2a9cb7fb6e81,"Emerging Zoned Namespace (ZNS) SSDs, providing the coarse-grained zone abstraction, hold the potential to significantly enhance the cost efficiency of future storage infrastructure and mitigate performance unpredictability. However, existing ZNS SSDs have a static zoned interface, making them in-adaptable to workload runtime behavior, unscalable to underlying hardware capabilities, and interfering with co-located zones. Applications either under-provision the zone resources yielding unsatisfied throughput, create over-provisioned zones and incur costs, or experience unexpected I/O latencies.We propose eZNS, an elastic-ZNS interface that exposes an adaptive zone with predictable characteristics. eZNS comprises two major components: a zone arbiter that manages zone allocation and active resources on the control plane, and a hierarchical I/O scheduler with read congestion control and write admission control on the data plane. Together, eZNS enables the transparent use of a ZNS SSD and closes the gap between application requirements and zone interface properties. Our evaluations over RocksDB demonstrate that eZNS outperforms a static zoned interface by 17.7% and 80.3% in throughput and tail latency, respectively, at most.  © 2024 Copyright held by the owner/author(s).",Disaggregation; SSD; Storage; ZNS,Digital storage; Zinc sulfide; Coarse-grained; Cost-efficiency; Device utilization; Disaggregation; Namespaces; Performance; Runtime behaviors; SSD; Storage infrastructure; Zoned namespace; II-VI semiconductors
Fastmove: A Comprehensive Study of On-Chip DMA and its Demonstration for Accelerating Data Movement in NVM-based Storage Systems,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196144558&doi=10.1145%2f3656477&partnerID=40&md5=87a45ddb436caaa8f613887aab007b2e,"Data-intensive applications executing on NVM-based storage systems experience serious bottlenecks when moving data between DRAM and NVM. We advocate for the use of the long-existing but recently neglected on-chip DMA to expedite data movement with three contributions. First, we explore new latency-oriented optimization directions, driven by a comprehensive DMA study, to design a high-performance DMA module, which significantly lowers the I/O size threshold to observe benefits. Second, we propose a new data movement engine, Fastmove, that coordinates the use of the DMA along with the CPU with DDIO-aware strategies, judicious scheduling, and load splitting such that the DMA's limitations are compensated, and the overall gains are maximized. Finally, with a general kernel-based design, simple APIs, and DAX file system integration, Fastmove allows applications to transparently exploit the DMA and its new features without code change. We run three data-intensive applications MySQL, GraphWalker, and Filebench atop NOVA, ext4-DAX, and XFS-DAX, with standard benchmarks like TPC-C, and popular graph algorithms like PageRank. Across single- and multi-socket settings, compared to the conventional CPU-only NVM accesses, Fastmove introduces to TPC-C with MySQL 1.13-2.16× speedups of peak throughput, reduces the average latency by 17.7-60.8%, and saves 37.1-68.9% CPU usage spent in data movement. It also shortens the execution time of graph algorithms with GraphWalker by 39.7-53.4%, and introduces 1.01-1.48× throughput speedups for Filebench. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",direct cache access; direct memory access; Persistent memory,Benchmarking; Cache memory; Integrated circuit design; Cache access; Data movements; Data-intensive application; Direct cache access; Direct memory access; Graph algorithms; On chips; Optimisations; Persistent memory; Storage systems; Dynamic random access storage
LVMT: An Efficient Authenticated Storage for Blockchain,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196194934&doi=10.1145%2f3664818&partnerID=40&md5=06731095298f11efd6b6ad582e0c9dce,"Authenticated storage access is the performance bottleneck of a blockchain, because each access can be amplified to potentially O(log n) disk I/O operations in the standard Merkle Patricia Trie (MPT) storage structure. In this article, we propose a multi-Layer Versioned Multipoint Trie (LVMT), a novel high-performance blockchain storage with significantly reduced I/O amplifications. LVMT uses the authenticated multipoint evaluation tree vector commitment protocol to update commitment proofs in constant time. LVMT adopts a multi-layer design to support unlimited key-value pairs and stores version numbers instead of value hashes to avoid costly elliptic curve multiplication operations. In our experiment, LVMT outperforms the MPT in real Ethereum traces, delivering read and write operations 6× faster. It also boosts blockchain system execution throughput by up to 2.7×. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",authenticated storage; Blockchain; merkle patricia trie; smart contracts; vector commitment,Blockchain; Authenticated storage; Block-chain; Disk I/O; Merkle patricia trie; Multi-layers; Multi-points; Patricia trie; Performance bottlenecks; Storage structures; Vector commitment; Smart contract
Index Shipping for Efficient Replication in LSM Key-Value Stores with Hybrid KV Placement,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196498195&doi=10.1145%2f3658672&partnerID=40&md5=5c6a5713d21bb2b5fe74c95c1bc1abe5,"Key-value (KV) stores based on the LSM tree have become a foundational layer in the storage stack of datacenters and cloud services. Current approaches for achieving reliability and availability favor reducing network traffic and send to replicas only new KV pairs. As a result, they perform costly compactions to reorganize data in both the primary and backup nodes, which increases device I/O traffic and CPU overhead, and eventually hurts overall system performance. In this article, we describe Tebis, an efficient LSM-based KV store that reduces I/O amplification and CPU overhead for maintaining the replica index. We use a primary-backup replication scheme that performs compactions only on the primary nodes and sends pre-built indexes to backup nodes, avoiding all compactions in backup nodes. Our approach includes an efficient mechanism to deal with pointer translation across nodes in the pre-built region index. Our results show that Tebis reduces resource utilization on backup nodes compared to performing full compactions: throughput is increased by 1.06 to 2.90×, CPU efficiency is increased by 1.21 to 2.78×, and I/O amplification is reduced by 1.7 to 3.27×, whereas network traffic increases by up to 1.32 to 3.76x.  © 2024 Copyright held by the owner/author(s).",B+ tree; flash; Key value stores; LSM tree; RDMA,Digital storage; B trees; Backup nodes; Cloud services; Datacenter; Flash; Key values; Key-value stores; LSM tree; Network traffic; RDMA; Compaction
A Contract-aware and Cost-effective LSM Store for Cloud Storage with Low Latency Spikes,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193236338&doi=10.1145%2f3643851&partnerID=40&md5=bed30a1590d21bdac781150e2a5d3b24,"Cloud storage is gaining popularity because features such as pay-as-you-go significantly reduce storage costs. However, the community has not sufficiently explored its contract model and latency characteristics. As LSM-Tree-based key-value stores (LSM stores) become the building block for numerous cloud applications, how cloud storage would impact the performance of key-value accesses is vital. This study reveals the significant latency variances of Amazon Elastic Block Store (EBS) under various I/O pressures, which challenges LSM store read performance on cloud storage. To reduce the corresponding tail latency, we propose Calcspar, a contract-aware LSM store for cloud storage, which efficiently addresses the challenges by regulating the rate of I/O requests to cloud storage and absorbing surplus I/O requests with the data cache. We specifically developed a fluctuation-aware cache to lower the high latency brought on by workload fluctuations. Additionally, we build a congestion-aware IOPS allocator to reduce the impact of LSM store internal operations on read latency. We evaluated Calcspar on EBS with different real-world workloads and compared it to the cutting-edge LSM stores. The results show that Calcspar can significantly reduce tail latency while maintaining regular read and write performance, keeping the 99th percentile latency under 550μs and reducing average latency by 66%. In addition, Calcspar has lower write prices and average latency compared to Cloud NoSQL services offered by cloud vendors.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cloud block storage; log-structured merge tree; storage cost; tail latency,Cost effectiveness; Cloud block storage; Cloud storages; Cost effective; Elastic blocks; Key-value stores; Log structured merge trees; Performance; Storage costs; Tail latency; Tree-based; Cloud storage
Tarazu: An Adaptive End-to-end I/O Load-balancing Framework for Large-scale Parallel File Systems,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193252150&doi=10.1145%2f3641885&partnerID=40&md5=cda15d8c3a1ddce8f9af5c774f98454a,"The imbalanced I/O load on large parallel file systems affects the parallel I/O performance of high-performance computing (HPC) applications. One of the main reasons for I/O imbalances is the lack of a global view of system-wide resource consumption. While approaches to address the problem already exist, the diversity of HPC workloads combined with different file striping patterns prevents widespread adoption of these approaches. In addition, load-balancing techniques should be transparent to client applications. To address these issues, we propose Tarazu, an end-to-end control plane where clients transparently and adaptively write to a set of selected I/O servers to achieve balanced data placement. Our control plane leverages real-time load statistics for global data placement on distributed storage servers, while our design model employs trace-based optimization techniques to minimize latency for I/O load requests between clients and servers and to handle multiple striping patterns in files. We evaluate our proposed system on an experimental cluster for two common use cases: the synthetic I/O benchmark IOR and the scientific application I/O kernel HACC-I/O. We also use a discrete-time simulator with real HPC application traces from emerging workloads running on the Summit supercomputer to validate the effectiveness and scalability of Tarazu in large-scale storage environments. The results show improvements in load balancing and read performance of up to 33% and 43%, respectively, compared to the state-of-the-art.  © 2024 Copyright held by the owner/author(s).",lustre; Parallel file system; progressive file layout; time series modeling,Benchmarking; Digital storage; File organization; Control planes; End to end; High-performance computing applications; Large-scales; Load-Balancing; Luster; Parallel file system; Progressive file layout; Striping pattern; Times series models; Supercomputers
Introduction to the Special Section on USENIX ATC 2023,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193233740&doi=10.1145%2f3635156&partnerID=40&md5=c5b8354e26751dbcf871a1208838b9f1,[No abstract available],,
Bridging Software-Hardware for CXL Memory Disaggregation in Billion-Scale Nearest Neighbor Search,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193297960&doi=10.1145%2f3639471&partnerID=40&md5=17c8b00eedae579264db5d8d56d66204,"We propose CXL-ANNS, a software-hardware collaborative approach to enable scalable approximate nearest neighbor search (ANNS) services. To this end, we first disaggregate DRAM from the host via compute express link (CXL) and place all essential datasets into its memory pool. While this CXL memory pool allows ANNS to handle billion-point graphs without an accuracy loss, we observe that the search performance significantly degrades because of CXL's far-memory-like characteristics. To address this, CXL-ANNS considers the node-level relationship and caches the neighbors in local memory, which are expected to visit most frequently. For the uncached nodes, CXL-ANNS prefetches a set of nodes most likely to visit soon by understanding the graph traversing behaviors of ANNS. CXL-ANNS is also aware of the architectural structures of the CXL interconnect network and lets different hardware components collaborate with each other for the search. Furthermore, it relaxes the execution dependency of neighbor search tasks and allows ANNS to utilize all hardware in the CXL network in parallel.Our evaluation shows that CXL-ANNS exhibits 93.3% lower query latency than state-of-the-art ANNS platforms that we tested. CXL-ANNS also outperforms an oracle ANNS system that has unlimited local DRAM capacity by 68.0%, in terms of latency.  © 2024 Copyright held by the owner/author(s).",Approximate Nearest Neighbor Search (ANNS); Compute Express Link (CXL); software/hardware co-design,Cache memory; Dynamic random access storage; Software design; Approximate near neighbor search; Approximate Nearest Neighbor Search; Collaborative approach; Compute express link; Disaggregation; Memory pool; Near neighbor searches; Search services; Software/hardware; Software/hardware co designs; Nearest neighbor search
Exploiting Data-pattern-aware Vertical Partitioning to Achieve Fast and Low-cost Cloud Log Storage,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193289036&doi=10.1145%2f3643641&partnerID=40&md5=1802d0ff4378b5ba0cd7499d6f26b59d,"Cloud logs can be categorized into on-line, off-line, and near-line logs based on the access frequency. Among them, near-line logs are mainly used for debugging, which means they prefer a low query latency for better user experience. Besides, the storage system for near-line logs prefers a low overall cost including the storage cost to store compressed logs, and the computation cost to compress logs and execute queries. These requirements pose challenges to achieving fast and cheap cloud log storage.This article proposes LogGrep, the first log compression and query tool that exploits both static and runtime patterns to properly structurize and organize log data in fine-grained units. The key idea of LogGrep is ""vertical partitioning"": it stores each log entry into multiple partitions by first parsing logs into variable vectors according to static patterns and then extracting runtime pattern(s) automatically within each variable vector. Based on such runtime patterns, LogGrep further decomposes the variable vectors into fine-grained units called ""Capsules""and stamps each Capsule with a summary of its values. During the query process, LogGrep can avoid decompressing and scanning Capsules that cannot match the keywords, with the help of the extracted runtime patterns and the Capsule stamps. We further show that the interactive debugging can well utilize the advantages of the vertical-partitioning-based method and mitigate its weaknesses as well. To this end, LogGrep integrates incremental locating and partial reconstruction to mitigate the read amplification incurred by vertical-partitioning-based method.We evaluate LogGrep on 37 cloud logs from the production environment of Alibaba Cloud and the public datasets. The results show that LogGrep can reduce the query latency and the overall cost by an order of magnitude compared with state-of-the-art works. Such results have confirmed that it is worthwhile applying a more sophisticated vertical-partitioning-based method to accelerate queries on compressed cloud logs.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cloud log; data compression; full-text query; runtime pattern; static pattern,Cloud storage; Costs; Cloud log; Full-text query; Log storages; Overall costs; Query latency; Runtime pattern; Runtimes; Static patterns; Text query; Vertical partitioning; Data compression
Polling Sanitization to Balance I/O Latency and Data Security of High-density SSDs,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193256505&doi=10.1145%2f3639826&partnerID=40&md5=d01ecfe3e16e7914739cc74570ab6031,"Sanitization is an effective approach for ensuring data security through scrubbing invalid but sensitive data pages, with the cost of impacts on storage performance due to moving out valid pages from the sanitization-required wordline, which is a logical read/write unit and consists of multiple pages in high-density SSDs. To minimize the impacts on I/O latency and data security, this article proposes a polling-based scheduling approach for data sanitization in high-density SSDs. Our method polls a specific SSD channel for completing data sanitization at the block granularity, meanwhile other channels can still service I/O requests. Furthermore, our method assigns a low priority to the blocks that are more likely to have future adjacent page invalidations inside sanitization-required wordlines, while selecting the sanitization block, to minimize the negative impacts of moving valid pages. Through a series of emulation experiments on several disk traces of real-world applications, we show that our proposal can decrease the negative effects of data sanitization in terms of the risk-performance index, which is a united time metric of I/O responsiveness and the unsafe time interval, by 16.34%, on average, compared to related sanitization methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data security; High-density SSDs; I/O latency; polling; sanitization; scheduling,Data pages; Effective approaches; High-density SSD; I/O latency; Low priorities; Polling; Sanitization; Sensitive datas; Storage performance; Wordlines; Sensitive data
Perseid: A Secondary Indexing Mechanism for LSM-Based Storage Systems,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189935914&doi=10.1145%2f3633285&partnerID=40&md5=2fe6ae56c0a0d79d6bef6f4126eb2555,"LSM-based storage systems are widely used for superior write performance on block devices. However, they currently fail to efficiently support secondary indexing, since a secondary index query operation usually needs to retrieve multiple small values, which scatter in multiple LSM components. In this work, we revisit secondary indexing in LSM-based storage systems with byte-addressable persistent memory (PM). Existing PM-based indexes are not directly competent for efficient secondary indexing. We propose Perseid, an efficient PM-based secondary indexing mechanism for LSM-based storage systems, which takes into account both characteristics of PM and secondary indexing. Perseid consists of (1) a specifically designed secondary index structure that achieves high-performance insertion and query, (2) a lightweight hybrid PM-DRAM and hash-based validation approach to filter out obsolete values with subtle overhead, and (3) two adapted optimizations on primary table searching issued from secondary indexes to accelerate non-index-only queries. Our evaluation shows that Perseid outperforms existing PM-based indexes by 3-7× and achieves about two orders of magnitude performance of state-of-the-art LSM-based secondary indexing techniques even if on PM instead of disks.  © 2024 Copyright held by the owner/author(s).",LSM-tree; persistent memory; secondary indexes,Indexing (of information); Query processing; Block devices; Index structure; Indexing mechanisms; LSM-tree; Performance; Persistent memory; Query operations; Secondary index; Storage systems; Validation approach; Dynamic random access storage
gLSM: Using GPGPU to Accelerate Compactions in LSM-tree-based Key-value Stores,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185704589&doi=10.1145%2f3633782&partnerID=40&md5=498230455ae8e6655fad6385824e5275,"Log-structured-merge tree or LSM-tree is a technological underpinning in key-value (KV) stores to support a wide range of performance-critical applications. By conducting data re-organization in the background by virtue of compaction operations, the KV stores have the potential to swiftly service write requests with sequential batched disk writes and read requests for KV items constantly sorted by the compaction. Compaction demands high I/O bandwidth and CPU speed to facilitate quality service to user read/write requests. With the emergence of high-speed SSDs, CPUs are increasingly becoming a performance bottleneck. To mitigate the bottleneck limiting the KV-store's performance and that of the applications supported by the store, we propose a system - gLSM - to leverage GPGPU to remarkably accelerate the compaction operations. gLSM fully utilizes the parallelism and computational capability inside GPGPUs to improve the compaction performance. We design a driver framework to parallelize compaction operations handled between a pair of CPU and GPGPU. We employ data independence and GPGPU-orient radix-sorting algorithm to concurrently conduct compaction. A key-value separation method is devised to slash the transfer of data volume from CPU-side memory to the GPGPU counterpart. The results reveal that gLSM improves the throughput and compaction bandwidth by up to a factor of 2.9 and 26.0, respectively, compared with the four state-of-the-art KV stores. gLSM also reduces the write latency by 73.3%. gLSM exhibits a performance improvement by up to 45% compared against its variant where there are no KV separation and collaboration sort modules. © 2024 Copyright held by the owner/author(s)",compaction; CUDA; GPGPU; key-value store; LSM-tree,Bandwidth; Bandwidth compression; Data transfer; Program processors; Compaction operations; Critical applications; CUDA; GPGPU; Key values; Key-value stores; Log structured merge trees; LSM-tree; Performance; Tree-based; Compaction
A Scalable Wear Leveling Technique for Phase Change Memory,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185725644&doi=10.1145%2f3631146&partnerID=40&md5=2385b3c9da6c76d31035a180c8a4fad5,"Phase Change Memory (PCM), one of the recently proposed non-volatile memory technologies, has been suffering from low write endurance. For example, a single-layer PCM cell could only be written approximately 108×. This limits the lifetime of a PCM-based memory to a few days rather than years when memory-intensive applications are running. Wear leveling techniques have been proposed to improve the write endurance of a PCM. Among those techniques, the region-based start-gap (RBSG) scheme is widely cited as achieving the highest lifetime. Based on our experiments, RBSG can achieve 97% of the ideal lifetime, but only for relatively small memory sizes (e.g., 8-32GB). As the memory size goes up, RBSG becomes less effective and its expected percentage of the ideal lifetime reduces to less than 57% for a 2TB PCM. In this article, we propose a table-based wear leveling scheme called block grouping to enhance the write endurance of a PCM with a negligible overhead. Our research results show that with a proper configuration and adoption of partial writes (writing back only 64B subblocks instead of a whole row to the PCM arrays) and internal row shift (shifting the subblocks in a row periodically so no subblock in a row will be written repeatedly), the proposed block grouping scheme could achieve 95% of the ideal lifetime on average for the Rodinia, NPB, and SPEC benchmarks with less than 1.74% performance overhead and up to 0.18% hardware overhead. Moreover, our scheme is scalable and achieves the same percentage of ideal lifetime for PCM of size from 8GB to 2TB. We also show that the proposed scheme can better tolerate memory write attacks than WoLFRaM (Wear Leveling and Fault Tolerance for Resistive Memories) and RBSG for a PCM of size 32GB or higher. Finally, we integrate an error-correcting pointer technique into our proposed block grouping scheme to make the PCM fault tolerant against hard errors. © 2024 Copyright held by the owner/author(s)",PCM; scalability; wear leveling,Benchmarking; Fault tolerance; Wear of materials; Memory size; Non-volatile memory technology; Phase change memory cells; Phase-change memory; Region-based; Research results; Single layer; Sub-blocks; Wear-Leveling; Write endurances; Phase change memory
Block-level Image Service for the Cloud,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185719362&doi=10.1145%2f3620672&partnerID=40&md5=78f5ce40000df1d5ed6ddf284b647a6f,"Businesses increasingly need agile and elastic computing infrastructure to respond quickly to real-world situations. By offering efficient process-based virtualization and a layered image system, containers are designed to enable agile and elastic application deployment. However, creating or updating large container clusters is still slow due to the image downloading and unpacking process. In this article, we present DADI Image Service (DADI), a block-level image service for increased agility and elasticity in deploying applications. DADI replaces the waterfall model of starting containers (downloading image, unpacking image, starting container) with fine-grained on-demand transfer of remote images, realizing instant start of containers. To accelerate the cold start of containers, DADI designs a pull-based prefetching mechanism that allows a host to read necessary image data beforehand at the granularity of image layers. We design a peer-to-peer-based decentralized image sharing architecture to balance traffic among all the participating hosts and propose a pull-push collaborative prefetching mechanism to accelerate cold start. DADI efficiently supports various kinds of runtimes including cgroups, QEMU, and so on, further realizing “build once, run anywhere.” DADI has been deployed at scale in the production environment of Alibaba, serving one of the world's largest ecommerce platforms. Performance results show that DADI can cold start 10, 000 containers on 1, 000 hosts within 4 s. © 2024 Copyright held by the owner/author(s)",Block storage; image layers; image service; on-demand data transfer,Containers; Digital storage; Image processing; Block storage; Cold-start; Computing infrastructures; Demand data; Elastic computing; Image layers; Image service; On demands; On-demand data transfer; Prefetching; Data transfer
Explorations and Exploitation for Parity-based RAIDs with Ultra-fast SSDs,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185763771&doi=10.1145%2f3627992&partnerID=40&md5=3f0d10580c1074e79aa8e847886c3232,"Following a conventional design principle that pays more fast-CPU-cycles for fewer slow-I/Os, popular software storage architecture Linux Multiple-Disk (MD) for parity-based RAID (e.g., RAID5 and RAID6) assigns one or more centralized worker threads to efficiently process all user requests based on multi-stage asynchronous control and global data structures, successfully exploiting characteristics of slow devices, e.g., Hard Disk Drives (HDDs). However, we observe that, with high-performance NVMe-based Solid State Drives (SSDs), even the recently added multi-worker processing mode in MD achieves only limited performance gain because of the severe lock contentions under intensive write workloads. In this paper, we propose a novel stripe-threaded RAID architecture, StRAID, assigning a dedicated worker thread for each stripe-write (one-for-one model) to sufficiently exploit high parallelism inherent among RAID stripes, multi-core processors, and SSDs. For the notoriously performance-punishing partial-stripe writes that induce extra read and write I/Os, StRAID presents a two-stage stripe write mechanism and a two-dimensional multi-log SSD buffer. All writes first are opportunistically batched in memory, and then are written into the primary RAID for aggregated full-stripe writes or conditionally redirected to the buffer for partial-stripe writes. These buffered data are strategically reclaimed to the primary RAID. We evaluate a StRAID prototype with a variety of benchmarks and real-world traces. StRAID is demonstrated to outperform MD by up to 5.8 times in write throughput. © 2024 Copyright held by the owner/author(s)",multi-thread scheduling; RAID systems; solid-state drive,Computer architecture; Computer operating systems; Conventional design; CPU cycles; Design Principles; Exploration and exploitation; Multi-thread scheduling; Performance; RAID systems; Solid-state drive; Ultra-fast; Workers'; Hard disk storage
An LSM Tree Augmented with B+ Tree on Nonvolatile Memory,2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185722087&doi=10.1145%2f3633475&partnerID=40&md5=48e025813e9d90fd1ebb77164aa2e11b,"Modern log-structured merge (LSM) tree-based key-value stores are widely used to process update-heavy workloads effectively as the LSM tree sequentializes write requests to a storage device to maximize storage performance. However, this append-only approach leaves many outdated copies of frequently updated key-value pairs, which need to be routinely cleaned up through the operation called compaction. When the system load is modest, compaction happens in background. However, at a high system load, it can quickly become the major performance bottleneck. To address this compaction bottleneck and further improve the write throughput of LSM tree-based key-value stores, we propose LAB-DB, which augments the existing LSM tree with a pair of B+ trees on byte-addressable nonvolatile memory (NVM). The auxiliary B+ trees on NVM reduce both compaction frequency and compaction time, hence leading to lower compaction overhead for writes and fewer storage accesses for reads. According to our evaluation of LAB-DB on RocksDB with YCSB benchmarks, LAB-DB achieves 94% and 67% speedups on two write-intensive workloads (Workload A and F), and also a 43% geomean speedup on read-intensive YCSB Workload B, C, D, and E. This performance gain comes with a low cost of NVM whose size is just 0.6% of the entire dataset to demonstrate the scalability of LAB-DB with an ever increasing volume of future datasets. © 2024 Copyright held by the owner/author(s)",compaction; key-value store; LSM tree; Nonvolatile memory,Benchmarking; Laboratories; Virtual storage; B trees; Heavy workloads; Key-value pairs; Key-value stores; Log structured merge trees; Non-volatile memory; Nonvolatile memory; Storage performance; System loads; Tree-based; Compaction
"Exploiting Flat Namespace to Improve File System Metadata Performance on Ultra-Fast, Byte-Addressable NVMs",2024,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185710662&doi=10.1145%2f3620673&partnerID=40&md5=72fc278f628af8a5b6ae39959e20a805,"The conventional file system provides a hierarchical namespace by structuring it as a directory tree. Tree-based namespace structure leads to inefficient file path walk and expensive namespace tree traversal, under-utilizing ultra-low access latency and superior sequential performance provided by non-volatile memories (NVMs). This article proposes FlatFS+, an NVM file system that features a flat namespace architecture while providing a compatible hierarchical namespace view. FlatFS+ incorporates three novel techniques: the direct file path walk model, range-optimized Br tree, and compressed index key design with scan and write dual optimization, to fully exploit flat namespace to improve file system metadata performance on ultra-fast, byte-addressable NVMs. Evaluation results demonstrate that FlatFS+ achieves significant performance improvements for metadata-intensive benchmarks and real-world applications compared to other file systems. © 2024 Copyright held by the owner/author(s)",File system; metadata management; non-volatile memory,Benchmarking; File organization; Nonvolatile storage; Access latency; Directory trees; Filesystem; Metadata management; Namespaces; Non-volatile memory; Performance; Tree traversal; Tree-based; Ultra-fast; Metadata
Introduction to the Special Section on USENIX FAST 2023,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177826623&doi=10.1145%2f3612820&partnerID=40&md5=993de2c67c83349d2246b3815dfdeece,[No abstract available],,
Practical Design Considerations for Wide Locally Recoverable Codes (LRCs),2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177853890&doi=10.1145%2f3626198&partnerID=40&md5=f30a9f8dcd9c7bf4f20007bf080213ea,"Most of the data in large-scale storage clusters is erasure coded. At exascale, optimizing erasure codes for low storage overhead, efficient reconstruction, and easy deployment is of critical importance. Locally recoverable codes (LRCs) have deservedly gained central importance in this field, because they can balance many of these requirements. In our work, we study wide LRCs; LRCs with large number of blocks per stripe and low storage overhead. These codes are a natural next step for practitioners to unlock higher storage savings, but they come with their own challenges. Of particular interest is their reliability, since wider stripes are prone to more simultaneous failures.We conduct a practically minded analysis of several popular and novel LRCs. We find that wide LRC reliability is a subtle phenomenon that is sensitive to several design choices, some of which are overlooked by theoreticians, and others by practitioners. Based on these insights, we construct novel LRCs called Uniform Cauchy LRCs, which show excellent performance in simulations and a 33% improvement in reliability on unavailability events observed by a wide LRC deployed in a Google storage cluster. We also show that these codes are easy to deploy in a manner that improves their robustness to common maintenance events. Along the way, we also give a remarkably simple and novel construction of distance-optimal LRCs (other constructions are also known), which may be of interest to theory-minded readers.  © 2023 Copyright held by the owner/author(s).",distributed storage systems; erasure codes; Reliability,Codes (symbols); Digital storage; Forward error correction; Multiprocessing systems; Design considerations; Distributed storage system; Efficient reconstruction; Erasure codes; Exascale; Large-scales; Low-storage; Number of blocks; Performance; Storage overhead; Reliability
From Missteps to Milestones: A Journey to Practical Fail-Slow Detection,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177878279&doi=10.1145%2f3617690&partnerID=40&md5=cd70ad0aec55a4f7894ba80da35f34e4,"The newly emerging ""fail-slow""failures plague both software and hardware where the victim components are still functioning yet with degraded performance. To address this problem, this article presents Perseus, a practical fail-slow detection framework for storage devices. Perseus leverages a light regression-based model to quickly pinpoint and analyze fail-slow failures at the granularity of drives. Within a 10-month close monitoring on 248K drives, Perseus managed to find 304 fail-slow cases. Isolating them can reduce the (node-level) 99.99th tail latency by 48%. We assemble a large-scale fail-slow dataset (including 41K normal drives and 315 verified fail-slow drives) from our production traces, based on which we provide root cause analysis on fail-slow drives covering a variety of ill-implemented scheduling, hardware defects, and environmental factors. We have released the dataset to the public for fail-slow study.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",datasets; Fail-slow failures; machine learning; root cause reasoning,Large dataset; Virtual storage; Dataset; Degraded performance; Detection framework; Fail-slow failure; Large-scales; Machine-learning; Regression-based model; Root cause; Root cause reasoning; Software and hardwares; Machine learning
Empowering Storage Systems Research with NVMeVirt: A Comprehensive NVMe Device Emulator,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177814527&doi=10.1145%2f3625006&partnerID=40&md5=3e62f165e87b4149424ed236375d8723,"There have been drastic changes in the storage device landscape recently. At the center of the diverse storage landscape lies the NVMe interface, which allows high-performance and flexible communication models required by these next-generation device types. However, its hardware-oriented definition and specification are bottlenecking the development and evaluation cycle for new revolutionary storage devices. Furthermore, existing emulators lack the capability to support the advanced storage configurations that are currently in the spotlight.In this article, we present NVMeVirt, a novel approach to facilitate software-defined NVMe devices. A user can define any NVMe device type with custom features, and NVMeVirt allows it to bridge the gap between the host I/O stack and the virtual NVMe device in software. We demonstrate the advantages and features of NVMeVirt by realizing various storage types and configurations, such as conventional SSDs, low-latency high-bandwidth NVM SSDs, zoned namespace SSDs, and key-value SSDs with the support of PCI peer-to-peer DMA and NVMe-oF target offloading. We also make cases for storage research with NVMeVirt, such as studying the performance characteristics of database engines and extending the NVMe specification for the improved key-value SSD performance.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",emulator; key-value SSD; NVMe device; SSD; Virtual device; ZNS SSD,II-VI semiconductors; Virtual storage; Zinc sulfide; Emulator; High performance communication; Key values; Key-value SSD; NVMe device; SSD; Storage systems; Systems research; Virtual devices; ZNS SSD; Specifications
FASTSync: A FAST Delta Sync Scheme for Encrypted Cloud Storage in High-bandwidth Network Environments,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177822978&doi=10.1145%2f3607536&partnerID=40&md5=159e83f5cc2f3a79a97cccdbb1d06b1f,"More and more data are stored in cloud storage, which brings two major challenges. First, the modified files in the cloud should be quickly synchronized to ensure data consistency, e.g., delta synchronization (sync) achieves efficient cloud sync by synchronizing only the updated part of the file. Second, the huge data in the cloud needs to be deduplicated and encrypted, e.g., Message-Locked Encryption (MLE) implements data deduplication by encrypting the content among different users. However, when combined, a few updates in the content can cause large sync traffic amplification for both keys and ciphertext in the MLE-based cloud storage, significantly degrading the cloud sync efficiency. A feature-based encryption sync scheme, FeatureSync, is proposed to address the delta amplification problem. However, with further improvement of the network bandwidth, the performance of FeatureSync stagnates. In our preliminary experimental evaluations, we find that the bottleneck of the computational overhead in the high-bandwidth network environments is the main bottleneck in FeatureSync. In this article, we propose an enhanced feature-based encryption sync scheme FASTSync to optimize the performance of FeatureSync in high-bandwidth network environments. The performance evaluations on a lightweight prototype implementation of FASTSync show that FASTSync reduces the cloud sync time by 70.3% and the encryption time by 37.3%, on average, compared with FeatureSync.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ciphertext synchronization; Cloud storage; delta synchronization; high-bandwidth network; message-locked encryption,Bandwidth; Cloud storage; Security of data; Synchronization; Ciphertext synchronization; Ciphertexts; Cloud storages; Delta synchronization; Feature-based; High-bandwidth networks; Message-locked encryption; Network environments; Performance; Synchronization scheme; Cryptography
Owner-free Distributed Symmetric Searchable Encryption Supporting Conjunctive Queries,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177835745&doi=10.1145%2f3607255&partnerID=40&md5=bef250cddc2737c99a1911a15379d01b,"Symmetric Searchable Encryption (SSE), as an ideal primitive, can ensure data privacy while supporting retrieval over encrypted data. However, existing multi-user SSE schemes require the data owner to share the secret key with all query users or always be online to generate search tokens. While there are some solutions to this problem, they have at least one weakness, such as non-supporting conjunctive query, result decryption assistance of the data owner, and unauthorized access. To solve the above issues, we propose an Owner-free Distributed Symmetric searchable encryption supporting Conjunctive query (ODiSC). Specifically, we first evaluate the Learning-Parity-with-Noise weak Pseudorandom Function (LPN-wPRF) in dual-cloud architecture to generate search tokens with the data owner free from sharing key and being online. Then, we provide fine-grained conjunctive query in the distributed architecture using additive secret sharing and symmetric-key hidden vector encryption. Finally, formal security analysis and empirical performance evaluation demonstrate that ODiSC is adaptively simulation-secure and efficient.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",conjunctive query; dual-cloud architecture; multi-user; Symmetric searchable encryption,Architecture; Computer architecture; Cryptography; Data privacy; Cloud architectures; Conjunctive query; Dual-cloud architecture; Encrypted data; Encryption schemes; Multiusers; Searchable encryptions; Secret key; Symmetric searchable encryption; Symmetrics; Cloud computing architecture
Hybrid Block Storage for Efficient Cloud Volume Service,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177867687&doi=10.1145%2f3596446&partnerID=40&md5=af1e2ce7293a87476760ed7e82ac001f,"The migration of traditional desktop and server applications to the cloud brings challenge of high performance, high reliability, and low cost to the underlying cloud storage. To satisfy the requirement, this article proposes a hybrid cloud-scale block storage system called Ursa. Trace analysis shows that the I/O patterns served by block storage have only limited locality to exploit. Therefore, instead of using solid state drives (SSDs) as a cache layer, Ursa proposes hybrid storage structure that directly stores primary replicas on SSDs and replicates backup replicas on hard disk drives (HDDs). At the core of Ursa's hybrid storage design is an adaptive journal that can bridge the performance gap between primary SSDs and backup HDDs for random writes by transforming small backup writes into journal appends, which are then asynchronously replayed and merged to backup HDDs. To efficiently index the journal, we design a novel range-optimized merge-tree structure that combines a continuous range of keys into a single composite key {offset,length}. Ursa integrates the hybrid structure with designs for high reliability, scalability, and availability. Experiments show that Ursa in its hybrid mode achieves almost the same performance as in its SSD-only mode (storing all replicas on SSDs), and outperforms other block stores (Ceph and Sheepdog) even in their SSD-only mode while achieving much higher CPU efficiency (IOPS and throughput per core).  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",block storage; cloud volume service; SSD-HDD hybrid,Trees (mathematics); Block storage; Cloud volume service; Desktop applications; Hard Disk Drive; High reliability; High-low; Hybrid storages; Performance; Server applications; Solid state drive-hard disk drive hybrid; Hard disk storage
Understanding Persistent-memory-related Issues in the Linux Kernel,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177872098&doi=10.1145%2f3605946&partnerID=40&md5=28faa1cef17c06db05ccfff287f074a5,"Persistent memory (PM) technologies have inspired a wide range of PM-based system optimizations. However, building correct PM-based systems is difficult due to the unique characteristics of PM hardware. To better understand the challenges as well as the opportunities to address them, this article presents a comprehensive study of PM-related issues in the Linux kernel. By analyzing 1,553 PM-related kernel patches in depth and conducting experiments on reproducibility and tool extension, we derive multiple insights in terms of PM patch categories, PM bug patterns, consequences, fix strategies, triggering conditions, and remedy solutions. We hope our results could contribute to the development of robust PM-based storage systems.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bug detection; kernel patches; Persistent memory; reliability,Bug detection; Condition; Kernel patch; Linux kernel; Memory technology; Memory-based systems; Persistent memory; Reproducibilities; Storage systems; System optimizations; Linux
The Security War in File Systems: An Empirical Study from A Vulnerability-centric Perspective,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177849790&doi=10.1145%2f3606020&partnerID=40&md5=834f8ba168d93fb56b8c74e94b7c5722,"This article presents a systematic study on the security of modern file systems, following a vulnerability-centric perspective. Specifically, we collected 377 file system vulnerabilities committed to the CVE database in the past 20 years. We characterize them from four dimensions: why the vulnerabilities appear, how the vulnerabilities can be exploited, what consequences can arise, and how the vulnerabilities are fixed. This way, we build a deep understanding of the attack surfaces faced by file systems, the threats imposed by the attack surfaces, and the good and bad practices in mitigating the attacks in file systems. We envision that our study will bring insights towards the future development of file systems, the enhancement of file system security, and the relevant vulnerability-mitigating solutions.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",file system vulnerabilities; Storage systems,Bad practices; Empirical studies; File system securities; File system vulnerability; Filesystem; Four dimensions; Good practices; Storage systems; System vulnerability; Systematic study; File organization
A High-performance RDMA-oriented Learned Key-value Store for Disaggregated Memory Systems,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177855899&doi=10.1145%2f3620674&partnerID=40&md5=db66d121a3320acb574bdc2367890840,"Disaggregated memory systems separate monolithic servers into different components, including compute and memory nodes, to enjoy the benefits of high resource utilization, flexible hardware scalability, and efficient data sharing. By exploiting the high-performance RDMA (Remote Direct Memory Access), the compute nodes directly access the remote memory pool without involving remote CPUs. Hence, the ordered key-value (KV) stores (e.g., B-trees and learned indexes) keep all data sorted to provide range query services via the high-performance network. However, existing ordered KVs fail to work well on the disaggregated memory systems, due to either consuming multiple network roundtrips to search the remote data or heavily relying on the memory nodes equipped with insufficient computing resources to process data modifications. In this article, we propose a scalable RDMA-oriented KV store with learned indexes, called ROLEX, to coalesce the ordered KV store in the disaggregated systems for efficient data storage and retrieval. ROLEX leverages a retraining-decoupled learned index scheme to dissociate the model retraining from data modification operations via adding a bias and some data movement constraints to learned models. Based on the operation decoupling, data modifications are directly executed in compute nodes via one-sided RDMA verbs with high scalability. The model retraining is hence removed from the critical path of data modification and asynchronously executed in memory nodes by using dedicated computing resources. ROLEX efficiently alleviates the fragmentation and garbage collection issues, due to allocating and reclaiming space via fixed-size leaves that are accessed via the atomic-size leaf numbers. Our experimental results on YCSB and real-world workloads demonstrate that ROLEX achieves competitive performance on the static workloads, as well as significantly improving the performance on dynamic workloads by up to 2.2× over state-of-the-art schemes on the disaggregated memory systems. We have released the open-source codes for public use in GitHub.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Disaggregated memory system; key-value store; learned index,Digital storage; Open systems; Scalability; Search engines; Trees (mathematics); Computing resource; Data modification; Dis-aggregated memory; Disaggregated memory system; Key-value stores; Learned index; Memory nodes; Memory systems; Performance; Remote direct memory access; Program processors
CostCounter: A Better Method for Collision Mitigation in Cuckoo Hashing,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168767071&doi=10.1145%2f3596910&partnerID=40&md5=f8cd975bcfa67ad30526c87a442537ca,"Hardware is often required to support fast search and high-throughput applications. Consequently, the performance of search algorithms is limited by storage bandwidth. Hence, the search algorithm must be optimized accordingly. We propose a CostCounter (CC) algorithm based on cuckoo hashing and an Improved CostCounter (ICC) algorithm. A better path can be selected when collisions occur using a cost counter to record the kick-out situation. Our simulation results indicate that the CC and ICC algorithms can achieve more significant performance improvements than Random Walk (RW), Breadth First Search (BFS), and MinCounter (MC). With two buckets and two slots per bucket, under the 95% memory load rate of the maximum load rate, CC and ICC are optimized on read-write times over 20% and 80% compared to MC and BFS, respectively. Furthermore, the CC and ICC algorithms achieve a slight improvement in storage efficiency compared with MC. In addition, we implement RW, MC, and the proposed algorithms using fine-grained locking to support a high throughput rate. From the test on field programmable gate arrays, we verify the simulation results and our algorithms optimize the maximum throughput over 23% compared to RW and 9% compared to MC under 95% of the memory capacity. The test results indicate that our CC and ICC algorithms can achieve better performance in terms of hardware bandwidth and memory load efficiency without incurring a significant resource cost.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cuckoo Hashing; FPGA search and update; throughput optimization,Bandwidth; Learning algorithms; Optimization; Storage efficiency; Breadth-first-search; Cuckoo hashing; FPGA search and update; High-throughput; Load rate; Memory load; Performance; Random Walk; Search Algorithms; Throughput optimization; Field programmable gate arrays (FPGA)
KVRangeDB: Range Queries for a Hash-based Key-Value Device,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168769274&doi=10.1145%2f3582013&partnerID=40&md5=fcef18f343e60098a328997515c3596d,"Key-value (KV) software has proven useful to a wide variety of applications including analytics, time-series databases, and distributed file systems. To satisfy the requirements of diverse workloads, KV stores have been carefully tailored to best match the performance characteristics of underlying solid-state block devices. Emerging KV storage device is a promising technology for both simplifying the KV software stack and improving the performance of persistent storage-based applications. However, while providing fast, predictable put and get operations, existing KV storage devices do not natively support range queries that are critical to all three types of applications described above.In this article, we present KVRangeDB, a software layer that enables processing range queries for existing hash-based KV solid-state disks (KVSSDs). As an effort to adapt to the performance characteristics of emerging KVSSDs, KVRangeDB implements log-structured merge tree key index that reduces compaction I/O, merges keys when possible, and provides separate caches for indexes and values. We evaluated the KVRangeDB under a set of representative workloads, and compared its performance with two existing database solutions: a Rocksdb variant ported to work with the KVSSD, and Wisckey, a key-value database that is carefully tuned for conventional block devices. On filesystem aging workloads, KVRangeDB outperforms Wisckey by 23.7× in terms of throughput and reduce CPU usage and external write amplifications by 14.3× and 9.8×, respectively.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Key value stores; KVSSD; range queries,Application programs; Distributed database systems; Virtual storage; Block devices; Key values; Key-value storages; Key-value stores; KV solid-state disk; Performance; Performance characteristics; Range query; Solid state disks; Time Series Database; File organization
A Universal SMR-aware Cache Framework with Deep Optimization for DM-SMR and HM-SMR Disks,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168764345&doi=10.1145%2f3588442&partnerID=40&md5=71495cfab9feb62a11af7462517da9d7,"To satisfy the enormous storage capacities required for big data, data centers have been adopting high-density shingled magnetic recording (SMR) disks. However, the weak fine-grained random write performance of SMR disks caused by their inherent write amplification and unbalanced read-write performance poses a severe challenge. Many studies have proposed solid-state drive (SSD) cache systems to address this issue. However, existing cache algorithms, such as the least recently used (LRU) algorithm, which is used to optimize cache popularity, and the MOST algorithm, which is used to optimize the write amplification factor, cannot exploit the full performance of the proposed cache systems because of their inappropriate optimization objectives. This article proposes a new SMR-aware cache framework called SAC+ to improve SMR-based hybrid storage. SAC+ integrates the two dominant types of SMR drives - namely, drive-managed and host-managed SMR drives - and provides a universal framework implementation. In addition, SAC+ integrally combines the drive characteristics to optimize I/O performance. The results of evaluations conducted using real-world traces indicate that SAC+ reduces the I/O time by 36-93% compared with state-of-the-art algorithms. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cache system; hybrid storage; Shingled magnetic recording,Digital storage; Magnetic storage; Cache systems; Datacenter; Fine grained; Hybrid storages; Magnetic recording disks; Optimisations; Performance; Shingled magnetic recordings; Storage capacity; Write amplifications; Magnetic recording
Visibility Graph-based Cache Management for DRAM Buffer Inside Solid-state Drives,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168770661&doi=10.1145%2f3586576&partnerID=40&md5=4d05921fe5b166a1cd8f18c5695b7cc8,"Most solid-state drives (SSDs) adopt an on-board Dynamic Random Access Memory (DRAM) to buffer the write data, which can significantly reduce the amount of write operations committed to the flash array of SSD if data exhibits locality in write operations. This article focuses on efficiently managing the small amount of DRAM cache inside SSDs. The basic idea is to employ the visibility graph technique to unify both temporal and spatial locality of references of I/O accesses, for directing cache management in SSDs. Specifically, we propose to adaptively generate the visibility graph of cached data pages and then support batch adjustment of adjacent or nearby (hot) cached data pages by referring to the connection situations in the visibility graph. In addition, we propose to evict the buffered data pages in batches by also referring to the connection situations, to maximize the internal flushing parallelism of SSD devices without worsening I/O congestion. The trace-driven simulation experiments show that our proposal can yield improvements on cache hits by between 0.8% and 19.8%, and the overall I/O latency by 25.6% on average, compared to state-of-the-art cache management schemes inside SSDs. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",batch adjustment and eviction; Cache Management; Solid-state drives; Temporal and Spatial Locality; Visibility Graph,Flash-based SSDs; Graphic methods; Visibility; Batch adjustment and eviction; Cache management; Data pages; Dynamic random access memory; Solid-state drive; Spatial locality; Temporal and spatial; Temporal locality; Visibility graphs; Write operations; Dynamic random access storage
Localized Validation Accelerates Distributed Transactions on Disaggregated Persistent Memory,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168775180&doi=10.1145%2f3582012&partnerID=40&md5=499f9ed9344b5babd08699e93de2670f,"Persistent memory (PM) disaggregation significantly improves the resource utilization and failure isolation to build a scalable and cost-effective remote memory pool in modern data centers. However, due to offering limited computing power and overlooking the bandwidth and persistence properties of real PMs, existing distributed transaction schemes, which are designed for legacy DRAM-based monolithic servers, fail to efficiently work on the disaggregated PM. In this article, we propose FORD, a Fast One-sided RDMA-based Distributed transaction system for the new disaggregated PM architecture. FORD thoroughly leverages one-sided remote direct memory access to handle transactions for bypassing the remote CPU in the PM pool. To reduce the round trips, FORD batches the read and lock operations into one request to eliminate extra locking and validations for the read-write data. To accelerate the transaction commit, FORD updates all remote replicas in a single round trip with parallel undo logging and data visibility control. Moreover, considering the limited PM bandwidth, FORD enables the backup replicas to be read to alleviate the load on the primary replicas, thus improving the throughput. To efficiently guarantee the remote data persistency in the PM pool, FORD selectively flushes data to the backup replicas to mitigate the network overheads. Nevertheless, the original FORD wastes some validation round trips if the read-only data are not modified by other transactions. Hence, we further propose a localized validation scheme to transfer the validation operations for the read-only data from remote to local as much as possible to reduce the round trips. Experimental results demonstrate that FORD significantly improves the transaction throughput by up to 3× and decreases the latency by up to 87.4% compared with state-of-the-art systems.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Disaggregated data center; distributed transaction processing; one-sided remote direct memory access; persistent memory,Bandwidth; Computing power; Cost effectiveness; Dynamic random access storage; Lakes; Locks (fasteners); Datacenter; Disaggregated data center; Distributed transaction; Distributed transaction processing; Localised; Memory pool; One-sided remote direct memory access; Persistent memory; Remote direct memory access; Round trip; Memory architecture
Performance Bug Analysis and Detection for Distributed Storage and Computing Systems,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160212869&doi=10.1145%2f3580281&partnerID=40&md5=bade241748aba42b2f85c610e9789fb8,"This article systematically studies 99 distributed performance bugs from five widely deployed distributed storage and computing systems (Cassandra, HBase, HDFS, Hadoop MapReduce and ZooKeeper). We present the TaxPerf database, which collectively organizes the analysis results as over 400 classification labels and over 2,500 lines of bug re-description. TaxPerf is classified into six bug categories (and 18 bug subcategories) by their root causes; resource, blocking, synchronization, optimization, configuration, and logic. TaxPerf can be used as a benchmark for performance bug studies and debug tool designs. Although it is impractical to automatically detect all categories of performance bugs in TaxPerf, we find that an important category of blocking bugs can be effectively solved by analysis tools. We analyze the cascading nature of blocking bugs and design an automatic detection tool called PCatch, which (i) performs program analysis to identify code regions whose execution time can potentially increase dramatically with the workload size; (ii) adapts the traditional happens-before model to reason about software resource contention and performance dependency relationship; and (iii) uses dynamic tracking to identify whether the slowdown propagation is contained in one job. Evaluation shows that PCatch can accurately detect blocking bugs of representative distributed storage and computing systems by observing system executions under small-scale workloads.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",blocking bugs; Storage and computing systems performance,Benchmarking; Classification (of information); Distributed computer systems; Program debugging; Blocking bug; Blockings; Computing system; Distributed computing systems; Distributed performance; Distributed storage system; Performance bugs; Storage and computing system performance; Storage systems; Systems performance; Computation theory
The Design of Fast and Lightweight Resemblance Detection for Efficient Post-Deduplication Delta Compression,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168768161&doi=10.1145%2f3584663&partnerID=40&md5=7609393585c10aad39e37eb3533e7896,"Post-deduplication delta compression is a data reduction technique that calculates and stores the differences of very similar but non-duplicate chunks in storage systems, which is able to achieve a very high compression ratio. However, the low throughput of widely used resemblance detection approaches (e.g., N-Transform) usually becomes the bottleneck of delta compression systems due to introducing high computational overhead. Generally, this overhead mainly consists of two parts: calculating the rolling hash byte by byte across data chunks and applying multiple transforms on all of the calculated rolling hash values. In this article, we propose Odess, a fast and lightweight resemblance detection approach, that greatly reduces the computational overhead for resemblance detection while achieving high detection accuracy and a high compression ratio. Odess first utilizes a novel Subwindow-based Parallel Rolling (SWPR) hash method using Single Instruction Multiple Data [1] (SIMD) to accelerate calculation of rolling hashes (corresponding to the first part of the overhead). Odess then uses a novel Content-Defined Sampling method to generate a much smaller proxy hash set from the whole rolling hash set and quickly applies transforms on this small hash set for resemblance detection (corresponding to the second part of the overhead).Evaluation results show that during the stage of resemblance detection, the Odess approach is ∼ 31.4× and ∼ 7.9× faster than the state-of-the-art N-Transform and Finesse (a recent variant of N-Transform [39]), respectively. When considering an end-to-end data reduction storage system, the Odess-based system's throughput is about 3.20× and 1.41× higher than the N-Transform- and Finesse-based systems' throughput, respectively, while maintaining the high compression ratio of N-Transform and achieving ∼ 1.22× higher compression ratio over Finesse.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",content-defined sampling; parallel rolling hash; Post-deduplication delta compression; resemblance detection; SIMD,Digital storage; Content-defined sampling; Deduplication; Delta compression; High compression ratio; Multiple data; Parallel rolling hash; Post-deduplication delta compression; Resemblance detection; Single instruction multiple data [1]; Storage systems; Data reduction
Derrick: A Three-layer Balancer for Self-managed Continuous Scalability,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168766301&doi=10.1145%2f3594543&partnerID=40&md5=f53a93893affe0202c0e4916529e689c,"Data arrangement determines the capacity, resilience, and performance of a distributed storage system. A scalable self-managed system must place its data efficiently not only during stable operation but also after an expansion, planned downscaling, or device failures. In this article, we present Derrick, a data balancing algorithm addressing these needs, which has been developed for HYDRAstor, a highly scalable commercial storage system. Derrick makes its decisions quickly in case of failures but takes additional time to find a nearly optimal data arrangement and a plan for reaching it when the device population changes. Compared to balancing algorithms in two other state-of-the-art systems, Derrick provides better capacity utilization, reduced data movement, and improved performance. Moreover, it can be easily adapted to meet custom placement requirements. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",capacity utilization; Data balancing; distributed storage,Cranes; Multiprocessing systems; Population statistics; Balancing algorithms; Capacity utilization; Data balancing; Distributed storage; Distributed storage system; ITS data; Performance; Self-managed; Stable operation; Three-layer; Digital storage
Introduction to the Special Section on USENIX ATC 2022,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153886972&doi=10.1145%2f3582557&partnerID=40&md5=4380a5252aa8259bebb4f8907fdb7d99,[No abstract available],,
Realizing Strong Determinism Contract on Log-Structured Merge Key-Value Stores,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153875494&doi=10.1145%2f3582695&partnerID=40&md5=7d3129ee11569a9f3bcd4dfaa88e1c46,"We propose Vigil-KV, a hardware and software co-designed framework that eliminates long-tail latency almost perfectly by introducing strong latency determinism. To make Get latency deterministic, Vigil-KV first enables a predictable latency mode (PLM) interface on a real datacenter-scale NVMe SSD, having knowledge about the nature of the underlying flash technologies. Vigil-KV at the system-level then hides the non-deterministic time window (associated with SSD's internal tasks and/or write services) by internally scheduling the different device states of PLM across multiple physical functions. Vigil-KV further schedules compaction/flush operations and client requests being aware of PLM's restrictions thereby integrating strong latency determinism into LSM KVs. We implement Vigil-KV upon a 1.92TB NVMe SSD prototype and Linux 4.19.91, but other LSM KVs can adopt its concept. We evaluate diverse Facebook and Yahoo scenarios with Vigil-KV, and the results show that Vigil-KV can reducethe tail latency of a baseline KV system by 3.19× while reducing the average latency by 34%, on average. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deterministic latency; Key-value stores; software/hardware co-design,Hardware-software codesign; Datacenter; Deterministic latency; Deterministics; Flash technology; Hardware and software; Key-value stores; Log structured; Long tail; Software/hardware co designs; System levels; Computer operating systems
TriCache: A User-Transparent Block Cache Enabling High-Performance Out-of-Core Processing with In-Memory Programs,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153878496&doi=10.1145%2f3583139&partnerID=40&md5=5371173b80dbda3439abfb09ab474627,"Out-of-core systems rely on high-performance cache sub-systems to reduce the number of I/O operations. Although the page cache in modern operating systems enables transparent access to memory and storage devices, it suffers from efficiency and scalability issues on cache misses, forcing out-of-core systems to design and implement their own cache components, which is a non-trivial task.This study proposes TriCache, a cache mechanism that enables in-memory programs to efficiently process out-of-core datasets without requiring any code rewrite. It provides a virtual memory interface on top of the conventional block interface to simultaneously achieve user transparency and sufficient out-of-core performance. A multi-level block cache design is proposed to address the challenge of per-access address translations required by a memory interface. It can exploit spatial and temporal localities in memory or storage accesses to render storage-to-memory address translation and page-level concurrency control adequately efficient for the virtual memory interface.Our evaluation shows that in-memory systems operating on top of TriCache can outperform Linux OS page cache by more than one order of magnitude, and can deliver performance comparable to or even better than that of corresponding counterparts designed specifically for out-of-core scenarios. © 2023 Copyright held by the owner/author(s).",block cache; buffer management; Page cache; solid-state drives,Cache memory; Linux; Virtual addresses; Address translation; Block cache; Buffer management; Core systems; Memory interface; Out-of-core; Page cache; Performance; Solid-state drive; Virtual memory; Concurrency control
TPFS: A High-Performance Tiered File System for Persistent Memories and Disks,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153885707&doi=10.1145%2f3580280&partnerID=40&md5=b6674d780d9e75d1181670d04efbdc1e,"Emerging fast, byte-addressable persistent memory (PM) promises substantial storage performance gains compared with traditional disks. We present TPFS, a tiered file system that combines PM and slow disks to create a storage system with near-PM performance and large capacity. TPFS steers incoming file input/output (I/O) to PM, dynamic random access memory (DRAM), or disk depending on the synchronicity, write size, and read frequency. TPFS profiles the application's access stream online to predict the behavior of file access. In the background, TPFS estimates the ""temperature""of file data and migrates the write-cold and read-hot file data from PM to disks. To fully utilize disk bandwidth, TPFS coalesces data blocks into large, sequential writes. Experimental results show that with a small amount of PM and a large solid-state drive (SSD), TPFS achieves up to 7.3× and 7.9× throughput improvement compared with EXT4 and XFS running on an SSD alone, respectively. As the amount of PM grows, TPFS's performance improves until it matches the performance of a PM-only file system. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data migration; File system; persistent memory,File organization; Hard disk storage; Data-migration; Dynamic random access memory; Filesystem; Input-output; Memory performance; Performance; Performance Gain; Persistent memory; Storage performance; Storage systems; Dynamic random access storage
An In-depth Comparative Analysis of Cloud Block Storage Workloads: Findings and Implications,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153848183&doi=10.1145%2f3572779&partnerID=40&md5=6ed03dc3e3fc662a63e4e3d80700aaaf,"Cloud block storage systems support diverse types of applications in modern cloud services. Characterizing their input/output (I/O) activities is critical for guiding better system designs and optimizations. In this article, we present an in-depth comparative analysis of production cloud block storage workloads through the block-level I/O traces of billions of I/O requests collected from two production systems, Alibaba Cloud and Tencent Cloud Block Storage. We study their characteristics of load intensities, spatial patterns, and temporal patterns. We also compare the cloud block storage workloads with the notable public block-level I/O workloads from the enterprise data centers at Microsoft Research Cambridge, and we identify the commonalities and differences of the three sources of traces. To this end, we provide 6 findings through the high-level analysis and 16 findings through the detailed analysis on load intensity, spatial patterns, and temporal patterns. We discuss the implications of our findings on load balancing, cache efficiency, and storage cluster management in cloud block storage systems. © 2023 Association for Computing Machinery.",Cloud block storage; I/O characterization; trace analysis,Block storage systems; Cloud block storage; Comparative analyzes; Input-output; Input/output characterization; Load intensity; Output characterization; Spatial patterns; Spatial temporals; Temporal pattern; Cloud storage
CacheSack: Theory and Experience of Google's Admission Optimization for Datacenter Flash Caches,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153858796&doi=10.1145%2f3582014&partnerID=40&md5=8558f4a0618f8bac478872d7aa2bc849,"This article describes the algorithm, implementation, and deployment experience of CacheSack, the admission algorithm for Google datacenter flash caches. CacheSack minimizes the dominant costs of Google's datacenter flash caches: disk IO and flash footprint. CacheSack partitions cache traffic into disjoint categories, analyzes the observed cache benefit of each subset, and formulates a knapsack problem to assign the optimal admission policy to each subset. Prior to this work, Google datacenter flash cache admission policies were optimized manually, with most caches using the Lazy Adaptive Replacement Cache algorithm. Production experiments showed that CacheSack significantly outperforms the prior static admission policies for a 7.7% improvement of the total cost of ownership, as well as significant improvements in disk reads (9.5% reduction) and flash wearout (17.8% reduction). © 2023 Copyright held by the owner/author(s).",distributed storage systems; Flash caches,Combinatorial optimization; % reductions; Admission algorithms; Admission policies; Algorithm implementation; Datacenter; Distributed storage system; Flash cache; Google+; Knapsack problems; Optimisations; Multiprocessing systems
PSA-Cache: A Page-state-aware Cache Scheme for Boosting 3D NAND Flash Performance,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153702631&doi=10.1145%2f3574324&partnerID=40&md5=6c0b83e793d1e37cd910580806635ab7,"Garbage collection (GC) plays a pivotal role in the performance of 3D NAND flash memory, where Copyback has been widely used to accelerate valid page migration during GC. Unfortunately, copyback is constrained by the parity symmetry issue: data read from an odd/even page must be written to an odd/even page. After migrating two odd/even consecutive pages, a free page between the two migrated pages will be wasted. Such wasted pages noticeably lower free space on flash memory and cause extra GCs, thereby degrading solid-state-disk (SSD) performance. To address this problem, we propose a page-state-aware cache scheme called PSA-Cache, which prevents page waste to boost the performance of NAND Flash-based SSDs. To facilitate making write-back scheduling decisions, PSA-Cache regulates write-back priorities for cached pages according to the state of pages in victim blocks. With high write-back-priority pages written back to flash chips, PSA-Cache effectively fends off page waste by breaking odd/even consecutive pages in subsequent garbage collections. We quantitatively evaluate the performance of PSA-Cache in terms of the number of wasted pages, the number of GCs, and response time. We compare PSA-Cache with two state-of-the-art schemes, GCaR and TTflash, in addition to a baseline scheme LRU. The experimental results unveil that PSA-Cache outperforms the existing schemes. In particular, PSA-Cache curtails the number of wasted pages of GCaR and TTflash by 25.7% and 62.1%, respectively. PSA-Cache immensely cuts back the number of GC counts by up to 78.7% with an average of 49.6%. Furthermore, PSA-Cache slashes the average write response time by up to 85.4% with an average of 30.05%.  © 2023 Association for Computing Machinery.",3D NAND flash; cache management; copyback; parity symmetry,Flash-based SSDs; Memory architecture; Refuse collection; 3d NAND flash; Cache management; Cache scheme; Copyback; Garbage collection; NAND Flash; NAND flash memory; Parity symmetry; Performance; Write-back; NAND circuits
FlatLSM: Write-Optimized LSM-Tree for PM-Based KV Stores,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153791261&doi=10.1145%2f3579855&partnerID=40&md5=b9974a54da1757da013c89ab1bd0f8b6,"The Log-Structured Merge Tree (LSM-Tree) is widely used in key-value (KV) stores because of its excwrite performance. But LSM-Tree-based KV stores still have the overhead of write-ahead log and write stall caused by slow L0 flush and L0-L1 compaction. New byte-addressable, persistent memory (PM) devices bring an opportunity to improve the write performance of LSM-Tree. Previous studies on PM-based LSM-Tree have not fully exploited PM's ""dual role""of main memory and external storage. In this article, we analyze two strategies of memtables based on PM and the reasons write stall problems occur in the first place. Inspired by the analysis result, we propose FlatLSM, a specially designed flat LSM-Tree for non-volatile memory based KV stores. First, we propose PMTable with separated index and data. The PM Log utilizes the Buffer Log to store KVs of size less than 256B. Second, to solve the write stall problem, FlatLSM merges the volatile memtables and the persistent L0 into large PMTables, which can reduce the depth of LSM-Tree and concentrate I/O bandwidth on L0-L1 compaction. To mitigate write stall caused by flushing large PMTables to SSD, we propose a parallel flush/compaction algorithm based on KV separation. We implemented FlatLSM based on RocksDB and evaluated its performance on Intel's latest PM device, the Intel Optane DC PMM with the state-of-the-art PM-based LSM-Tree KV stores, FlatLSM improves the throughput 5.2× on random write workload and 2.55× on YCSB-A.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",key-value stores; LSM-Tree; Persistent memory,Digital storage; Compaction algorithms; Dual role; Key values; Key-value stores; Log structured merge trees; Main-memory; Non-volatile memory; Performance; Persistent memory; Tree-based; Compaction
Introduction to the Special Section on USENIX OSDI 2022,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153893771&doi=10.1145%2f3584363&partnerID=40&md5=e6d49aa7e7423e81f5fd82b0b58fb7c3,[No abstract available],,
Principled Schedulability Analysis for Distributed Storage Systems Using Thread Architecture Models,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153894857&doi=10.1145%2f3574323&partnerID=40&md5=d8b82d2871ee2b88c31ad383948e6c52,"In this article, we present an approach to systematically examine the schedulability of distributed storage systems, identify their scheduling problems, and enable effective scheduling in these systems. We use Thread Architecture Models (TAMs) to describe the behavior and interactions of different threads in a system, and show both how to construct TAMs for existing systems and utilize TAMs to identify critical scheduling problems. We specify three schedulability conditions that a schedulable TAM should satisfy: completeness, local enforceability, and independence; meeting these conditions enables a system to easily support different scheduling policies. We identify five common problems that prevent a system from satisfying the schedulability conditions, and show that these problems arise in existing systems such as HBase, Cassandra, MongoDB, and Riak, making it difficult or impossible to realize various scheduling disciplines. We demonstrate how to address these schedulability problems using both direct and indirect solutions, with different trade-offs. To show how to apply our approach to enable scheduling in realistic systems, we develop Tamed-HBase and Muzzled-HBase, sets of modifications to HBase that can realize the desired scheduling disciplines, including fairness and priority scheduling, even when presented with challenging workloads. © 2023 Association for Computing Machinery.",performance isolation; Request scheduling; thread architecture,Architecture; Multiprocessing systems; Architecture modeling; Distributed storage system; Existing systems; Performance; Performance isolation; Request scheduling; Schedulability; Schedulability conditions; Scheduling problem; Thread architecture; Economic and social effects
ZNSwap: un-Block your Swap,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153873996&doi=10.1145%2f3582434&partnerID=40&md5=b745001e6b47f6ab0ebd05924aba6ef1,"We introduce ZNSwap , a novel swap subsystem optimized for the recent Zoned Namespace (ZNS) SSDs. ZNSwap leverages ZNS's explicit control over data management on the drive and introduces a space-efficient host-side Garbage Collector (GC) for swap storage co-designed with the OS swap logic. ZNSwap enables cross-layer optimizations, such as direct access to the in-kernel swap usage statistics by the GC to enable fine-grain swap storage management, and correct accounting of the GC bandwidth usage in the OS resource isolation mechanisms to improve performance isolation in multi-tenant environments. We evaluate ZNSwap using standard Linux swap benchmarks and two production key-value stores. ZNSwap shows significant performance benefits over the Linux swap on traditional SSDs, such as stable throughput for different memory access patterns, and 10× lower 99th percentile latency and 5× higher throughput for memcached key-value store under realistic usage scenarios. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",SSD; Swap; ZNS,Digital storage; II-VI semiconductors; Information management; Storage management; Zinc sulfide; Cross layer optimization; Garbage collectors; Key-value stores; Namespaces; Space efficient; SSD; Swap; Swap storages; Usage statistics; Zoned namespace; Linux
Boosting Cache Performance by Access Time Measurements,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149440150&doi=10.1145%2f3572778&partnerID=40&md5=f119c42825d982f949ad1d81dbf19dbd,"Most modern systems utilize caches to reduce the average data access time and optimize their performance. Recently proposed policies implicitly assume uniform access times, but variable access times naturally appear in domains such as storage, web search, and DNS resolution.Our work measures the access times for various items and exploits variations in access times as an additional signal for caching algorithms. Using such a signal, we introduce adaptive access time-aware cache policies that consistently improve the average access time compared with the best alternative in diverse workloads. Our adaptive algorithm attains an average access time reduction of up to 46% in storage workloads, up to 16% in web searches, and 8.4% on average when considering all experiments in our study.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",access time dataset; access times aware caching; Cross domain caching; dynamic caching,Digital storage; Information retrieval; Websites; Access time; Access time aware caching; Access time dataset; Average data; Cache performance; Cross domain caching; Cross-domain; Dynamic caching; Web searches; Adaptive algorithms
Editor-in-Chief Message,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149457803&doi=10.1145%2f3574325&partnerID=40&md5=4d2bd8a42f37fd1eef199cb86a206239,[No abstract available],,
Extending and Programming the NVMe I/O Determinism Interface for Flash Arrays,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149441435&doi=10.1145%2f3568427&partnerID=40&md5=c1371a8412cecda7c96e3fb82bfb76bf,"Predictable latency on flash storage is a long-pursuit goal, yet unpredictability stays due to the unavoidable disturbance from many well-known SSD internal activities. To combat this issue, the recent NVMe IO Determinism (IOD) interface advocates host-level controls to SSD internal management tasks. Although promising, challenges remain on how to exploit it for truly predictable performance.We present IODA,1 an I/O deterministic flash array design built on top of small but powerful extensions to the IOD interface for easy deployment. IODA exploits data redundancy in the context of IOD for a strong latency predictability contract. In IODA, SSDs are expected to quickly fail an I/O on purpose to allow predictable I/Os through proactive data reconstruction. In the case of concurrent internal operations, IODA introduces busy remaining time exposure and predictable-latency-window formulation to guarantee predictable data reconstructions. Overall, IODA only adds five new fields to the NVMe interface and a small modification in the flash firmware while keeping most of the complexity in the host OS. Our evaluation shows that IODA improves the 95-99.99th latencies by up to 75×. IODA is also the nearest to the ideal, no disturbance case compared to seven state-of-the-art preemption, suspension, GC coordination, partitioning, tiny-tail flash controller, prediction, and proactive approaches. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",predictable latency; Software/hardware co-design; SSD,Flash-based SSDs; Hardware-software codesign; Data reconstruction; Deterministics; Flash arrays; Flash storage; Internal activity; Management tasks; Performance; Predictable latency; Software/hardware co designs; SSD; Firmware
InDe: An Inline Data Deduplication Approach via Adaptive Detection of Valid Container Utilization,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147320780&doi=10.1145%2f3568426&partnerID=40&md5=ef9d8e6793892d4972274850a602894a,"Inline deduplication removes redundant data in real-time as data is being sent to the storage system. However, it causes data fragmentation: logically consecutive chunks are physically scattered across various containers after data deduplication. Many rewrite algorithms aim to alleviate the performance degradation due to fragmentation by rewriting fragmented duplicate chunks as unique chunks into new containers. Unfortunately, these algorithms determine whether a chunk is fragmented based on a simple pre-set fixed value, ignoring the variance of data characteristics between data segments. Accordingly, when backups are restored, they often fail to select an appropriate set of old containers for rewrite, generating a substantial number of invalid chunks in retrieved containers. To address this issue, we propose an inline deduplication approach for storage systems, called InDe, which uses a greedy algorithm to detect valid container utilization and dynamically adjusts the number of old container references in each segment. InDe fully leverages the distribution of duplicated chunks to improve the restore performance while maintaining high backup performance. We define an effectiveness metric, valid container referenced counts (VCRC), to identify appropriate containers for the rewrite. We design a rewrite algorithm F-greedy that detects valid container utilization to rewrite low-VCRC containers. According to the VCRC distribution of containers, F-greedy dynamically adjusts the number of old container references to only share duplicate chunks with high-utilization containers for each segment, thereby improving the restore speed. To take full advantage of the above features, we further propose another rewrite algorithm called F-greedy+ based on adaptive interval detection of valid container utilization. F-greedy+ makes a more accurate estimation of the valid utilization of old containers by detecting trends of VCRC's change in two directions and selecting referenced containers in the global scope. We quantitatively evaluate InDe using three real-world backup workloads. The experimental results show that compared with two state-of-the-art algorithms (Capping and SMR), our scheme improves the restore speed by 1.3×-2.4× while achieving almost the same backup performance. © 2023 Association for Computing Machinery.",Data deduplication; restore performance; storage system,Digital storage; Restoration; Adaptive detection; Data de duplications; Data fragmentation; Deduplication; Performance; Performance degradation; Real- time; Redundant data; Restore performance; Storage systems; Containers
End-to-end I/O Monitoring on Leading Supercomputers,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149435496&doi=10.1145%2f3568425&partnerID=40&md5=9a90c8511178e20b3ead87fa0e3f3615,"This paper offers a solution to overcome the complexities of production system I/O performance monitoring. We present Beacon, an end-to-end I/O resource monitoring and diagnosis system for the 40960-node Sunway TaihuLight supercomputer, currently the fourth-ranked supercomputer in the world. Beacon simultaneously collects and correlates I/O tracing/profiling data from all the compute nodes, forwarding nodes, storage nodes, and metadata servers. With mechanisms such as aggressive online and offline trace compression and distributed caching/storage, it delivers scalable, low-overhead, and sustainable I/O diagnosis under production use. With Beacon's deployment on TaihuLight for more than three years, we demonstrate Beacon's effectiveness with real-world use cases for I/O performance issue identification and diagnosis. It has already successfully helped center administrators identify obscure design or configuration flaws, system anomaly occurrences, I/O performance interference, and resource under- or over-provisioning problems. Several of the exposed problems have already been fixed, with others being currently addressed. Encouraged by Beacon's success in I/O monitoring, we extend it to monitor interconnection networks, which is another contention point on supercomputers. In addition, we demonstrate Beacon's generality by extending it to other supercomputers. Both Beacon codes and part of collected monitoring data are released.1  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",anomaly detection; bottleneck optimization; I/O diagnosis; I/O monitoring,Anomaly detection; Digital storage; Interconnection networks (circuit switching); Anomaly detection; Bottleneck optimisation; Diagnose system; End to end; I/O diagnose; I/O monitoring; Monitoring and diagnosis; Performance-monitoring; Production system; Resource monitoring systems; Supercomputers
Improving Storage Systems Using Machine Learning,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149437928&doi=10.1145%2f3568429&partnerID=40&md5=a28bbf50e26be236407e25a3805b4c4d,"Operating systems include many heuristic algorithms designed to improve overall storage performance and throughput. Because such heuristics cannot work well for all conditions and workloads, system designers resorted to exposing numerous tunable parameters to users - thus burdening users with continually optimizing their own storage systems and applications. Storage systems are usually responsible for most latency in I/O-heavy applications, so even a small latency improvement can be significant. Machine learning (ML) techniques promise to learn patterns, generalize from them, and enable optimal solutions that adapt to changing workloads. We propose that ML solutions become a first-class component in OSs and replace manual heuristics to optimize storage systems dynamically. In this article, we describe our proposed ML architecture, called KML. We developed a prototype KML architecture and applied it to two case studies: optimizing readahead and NFS read-size values. Our experiments show that KML consumes less than 4 KB of dynamic kernel memory, has a CPU overhead smaller than 0.2%, and yet can learn patterns and improve I/O throughput by as much as 2.3× and 15× for two case studies - even for complex, never-seen-before, concurrently running mixed workloads on different storage devices.  © 2023 Association for Computing Machinery.",Machine Learning; Operating systems; storage performance optimization; storage systems,Heuristic algorithms; Memory architecture; Optimization; Virtual storage; Case-studies; Condition; Heuristics algorithm; Learn+; Machine-learning; Operating system; Performance optimizations; Storage performance; Storage performance optimization; Storage systems; Machine learning
Efficient Crash Consistency for NVMe over PCIe and RDMA,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149436433&doi=10.1145%2f3568428&partnerID=40&md5=7f569c8fec7e2c8bd012d2808fa06bea,"This article presents crash-consistent Non-Volatile Memory Express (ccNVMe), a novel extension of the NVMe that defines how host software communicates with the non-volatile memory (e.g., solid-state drive) across a PCI Express bus and RDMA-capable networks with both crash consistency and performance efficiency. Existing storage systems pay a huge tax on crash consistency, and thus cannot fully exploit the multi-queue parallelism and low latency of the NVMe and RDMA interfaces. ccNVMe alleviates this major bottleneck by coupling the crash consistency to the data dissemination. This new idea allows the storage system to achieve crash consistency by taking the free rides of the data dissemination mechanism of NVMe, using only two lightweight memory-mapped I/Os (MMIOs), unlike traditional systems that use complex update protocol and synchronized block I/Os. ccNVMe introduces a series of techniques including transaction-aware MMIO/doorbell and I/O command coalescing to reduce the PCIe traffic as well as to provide atomicity. We present how to build a high-performance and crash-consistent file system named MQFS atop ccNVMe. We experimentally show that MQFS increases the IOPS of RocksDB by 36% and 28% compared to a state-of-the-art file system and Ext4 without journaling, respectively. © 2023 Association for Computing Machinery.",crash consistency; file system; NVMe; SSD; Storage protocol,File organization; Crash consistency; Data dissemination; Filesystem; Non-volatile memory; NVMe; PCI Express bus; Performance efficiency; SSD; Storage protocol; Storage systems; Flocculation
Oasis: Controlling Data Migration in Expansion of Object-based Storage Systems,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149440401&doi=10.1145%2f3568424&partnerID=40&md5=7fb20f13c2b9912b83e09bfa94e09677,"Object-based storage systems have been widely used for various scenarios such as file storage, block storage, blob (e.g., large videos) storage, and so on, where the data is placed among a large number of object storage devices (OSDs). Data placement is critical for the scalability of decentralized object-based storage systems. The state-of-the-art CRUSH placement method is a decentralized algorithm that deterministically places object replicas onto storage devices without relying on a central directory. While enjoying the benefits of decentralization such as high scalability, robustness, and performance, CRUSH-based storage systems suffer from uncontrolled data migration when expanding the capacity of the storage clusters (i.e., adding new OSDs), which is determined by the nature of CRUSH and will cause significant performance degradation when the expansion is nontrivial.This article presents MapX, a novel extension to CRUSH that uses an extra time-dimension mapping (from object creation times to cluster expansion times) for controlling data migration after cluster expansions. Each expansion is viewed as a new layer of the CRUSH map represented by a virtual node beneath the CRUSH root. MapX controls the mapping from objects onto layers by manipulating the timestamps of the intermediate placement groups (PGs). MapX is applicable to a large variety of object-based storage scenarios where object timestamps can be maintained as higher-level metadata. We have applied MapX to the state-of-the-art Ceph-RBD (RADOS Block Device) to implement a migration-controllable, decentralized object-based block store (called Oasis). Oasis extends the RBD metadata structure to maintain and retrieve approximate object creation times (for migration control) at the granularity of expansion layers. Experimental results show that the MapX-based Oasis block store outperforms the CRUSH-based Ceph-RBD (which is busy in migrating objects after expansions) by 3.17× ∼4.31× in tail latency, and 76.3% (respectively, 83.8%) in IOPS for reads (respectively, writes).  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",block storage system; cluster expansion; CRUSH; data migration; data placement; Object storage,Metadata; Scalability; Virtual storage; Block storage systems; Cluster expansion; CRUSH; Data placement; Data-migration; Decentralised; Object storage devices; Object storages; Object-based storage systems; State of the art; Mapping
Reliability Evaluation of Erasure-coded Storage Systems with Latent Errors,2023,ACM Transactions on Storage,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149435176&doi=10.1145%2f3568313&partnerID=40&md5=d8ea3a271635c7989ad73614ef563417,"Large-scale storage systems employ erasure-coding redundancy schemes to protect against device failures. The adverse effect of latent sector errors on the Mean Time to Data Loss (MTTDL) and the Expected Annual Fraction of Data Loss (EAFDL) reliability metrics is evaluated. A theoretical model capturing the effect of latent errors and device failures is developed, and closed-form expressions for the metrics of interest are derived. The MTTDL and EAFDL of erasure-coded systems are obtained analytically for (i) the entire range of bit error rates; (ii) the symmetric, clustered, and declustered data placement schemes; and (iii) arbitrary device failure and rebuild time distributions under network rebuild bandwidth constraints. The range of error rates that deteriorate system reliability is derived analytically. For realistic values of sector error rates, the results obtained demonstrate that MTTDL degrades, whereas, for moderate erasure codes, EAFDL remains practically unaffected. It is demonstrated that, in the range of typical sector error rates and for very powerful erasure codes, EAFDL degrades as well. It is also shown that the declustered data placement scheme offers superior reliability.  © 2023 Association for Computing Machinery.",EAFDL; MDS codes; MTTDL; RAID; reliability analysis; stochastic modeling; Storage; unrecoverable or latent sector errors,Bit error rate; Codes (symbols); Digital storage; Forward error correction; Stochastic systems; Data loss; Device failures; Error rate; Expected annual fraction of data loss; MDS code; Mean time to data loss; RAID; Stochastic-modeling; Unrecoverable or latent sector error; Reliability analysis
