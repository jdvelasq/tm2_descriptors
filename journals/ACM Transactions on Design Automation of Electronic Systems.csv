Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Memory management scheme to improve utilization efficiency and provide fast contiguous allocation without a statically reserved area,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952050393&doi=10.1145%2f2770871&partnerID=40&md5=2b35fc1880736acf341bc167e94d83ea,"Fast allocation of large blocks of physically contiguous memory plays a crucial role to boost the performance of multimedia applications in modern memory-constrained portable devices, such as smartphones, tablets, etc. Existing systems have addressed this issue by provisioning a large statically reserved memory area (SRA) in which only dedicated applications can allocate pages. However, this in turn degrades the performance of applications that are prohibited to utilize the SRA due to the reduced available memory pool. To overcome this drawback while maintaining the benefits of the SRA, we propose a new memory management scheme that uses a special memory region, called page-cache-preferred area (PCPA), in concert with a quick memory reclaiming algorithm. The key of the proposed scheme is to enhance the memory utilization efficiency by enabling to allocate page-cached pages of all applications in the PCPA until predetermined applications require to allocate big chunks of contiguous memory. At this point, clean page-cached pages in the PCPA are rapidly evicted without write-back to a secondary storage. Compared to the SRA scheme, experimental results show that the average launch time of real-world applications and the execution time of I/O-intensive benchmarks are reduced by 9.2% and 24.7%, respectively. © 2015 ACM.",Memory fragmentation; Memory management; Page cache,Benchmarking; Efficiency; Fast allocations; Memory fragmentation; Memory management; Memory utilization; Multimedia applications; Page cache; Secondary storage; Utilization efficiency; Cache memory
Offline washing schemes for residue removal in digital microfluidic biochips,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952059962&doi=10.1145%2f2798726&partnerID=40&md5=7299f1c1403182168d105bab699d0fe4,"A digital microfluidic biochip (DMB) is often deployed for multiplexing several assays in space and in time. The residue left by one assay may contaminate the droplets used for subsequent assays. Biochemical assays involving cell culture and those based on particle microfluidics also require sweeping of residual media from an active droplet on-chip. Thus, fluidic operations such as washing or residue removal need to be performed routinely either to clean contamination from the droplet pathways or to rinse off certain droplets on the chip. In this work, several graph-based techniques are presented for offline washing of biochips that may have either a regular geometry (e.g., a 2D array of electrodes), or an irregular geometry (e.g., an application-specific layout). The schemes can be used for total washing, that is, for cleaning the entire biochip or for selective washing of sites or pathways located sparsely on the chip. The problem of reducing the path length and washing time of the droplets is investigated with or without capacity constraints. The proposed algorithms for offline washing make use of several techniques such as graph traversal, integer linear programming (ILP) modeling, and customized heuristics based on the nature of the geometric distribution of the contamination profile. The contaminated pathways are assumed to be Manhattan or curved, and hence the techniques are applicable to the conventional field-actuated DMBs as well as to the emerging classes of light-actuated and active-matrix DMBs. These techniques will be useful in enhancing the reliability of a wide class of emerging digital microfluidic healthcare devices. © 2015 ACM.",Biochips; Contamination; Digital microfluidics; Washing,Assays; Bioassay; Biochips; Cell culture; Contamination; Digital devices; Digital microfluidics; Drop formation; Drops; Geometry; Graphic methods; Integer programming; Microarrays; Microfluidics; Probability distributions; Application specific; Capacity constraints; Contamination profiles; Digital microfluidic biochips; Geometric distribution; Graph-based techniques; Integer linear programming models; Irregular geometries; Washing
Enhanced test compaction for multicycle broadside tests by using state complementation,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952061412&doi=10.1145%2f2778953&partnerID=40&md5=2432f0dbd4fd684f22e66d776c11e274,"Multicycle tests support test compaction by allowing each test to detect more target faults. The ability of multicycle broadside tests to provide test compaction depends on the ability of primary input sequences to take the circuit between pairs of states that are useful for detecting target faults. This ability can be enhanced by adding design-for-testability (DFT) logic that allows states to be complemented. This article describes a test compaction procedure that uses such DFT logic to form a compact multicycle broadside test set for transition faults where the tests use constant primary input vectors. The use of complemented states also allows the procedure to increase the transition fault coverage beyond the transition fault coverage of a broadside test set. The procedure has the option of increasing the switching activity of the tests gradually in order to explore the tradeoff between the number of tests, the fault coverage, and the switching activity. © 2015 ACM.",Broadside tests; Design-for-testability; Multicycle tests; Test compaction; Transition faults,Compaction; Dynamic random access storage; Fault detection; Broadside tests; Detecting target; Multi cycle tests; Primary input vectors; Switching activities; Test Compaction; Transition fault coverage; Transition faults; Design for testability
Layout decomposition with pairwise coloring and adaptive multi-start for triple patterning lithography,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952032967&doi=10.1145%2f2764904&partnerID=40&md5=f34576a11901cf7b8308d831cd659c68,"In this article we present a pairwise coloring (PWC) approach to tackle the layout decomposition problem for triple patterning lithography (TPL). The main idea is to reduce the problem to a set of bi-coloring problems. The overall solution is refined by applying a bi-coloring method for pairs of color sets per pass. One obvious advantage of this method is that the existing double patterning lithography (DPL) techniques can be reused effortlessly. Moreover, we observe that each pass can be fulfilled efficiently by integrating an SPQR-tree-graph-division-based bi-coloring method. In addition, to prevent the solution getting stuck in the local minima, an adaptive multi-start (AMS) approach is incorporated. Adaptive starting points are generated according to the vote of previous solutions. The experimental results show that our method is competitive with other works on both solution quality and runtime performance. © 2015 ACM.",Adaptive multi-start; Design for manufacturability; Layout decomposition; Pairwise coloring; Triple patterning lithography,Design for manufacturability; Lithography; Machine design; Trees (mathematics); Coloring problems; Double patterning; Layout decomposition; Local minimums; Multistart; Run-time performance; Solution quality; Triple patterning; Coloring
Clock period minimization with minimum leakage power,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951970853&doi=10.1145%2f2778954&partnerID=40&md5=0bc6de288aac5c50ee4ccebfcf7cd398,"In the design of nonzero clock skew circuits, an increase of the short-path delay may improve circuit speed or reduce leakage power. However, the impact of increasing the short-path delay on the trade-off between circuit speed and leakage power has not been well studied. An analysis of previous works shows that they can be classified into two independent groups. One group uses extra buffers to increase the short-path delay for achieving the lower bound of the clock period; however, this group has a large overhead of leakage power. The other group uses the combination of threshold voltage assignment and gate sizing (TVA/GS) to increase the short-path delay as possible for reducing leakage power; however, this group often does not work with the lower bound of the clock period. Accordingly, this article considers the simultaneous application of buffer insertion and TVA/GS during clock skew scheduling. Our objective is to minimize the leakage power for working with the lower bound of the clock period. To the best of our knowledge, our approach is the first leakage-power-aware clock skew scheduling that guarantees working with the lower bound of the clock period. Benchmark data consistently show that our approach achieves good results in terms of both the circuit speed and the leakage power. © 2015 ACM.",Clock period minimization; Clock skew scheduling; Hold constraint; Leakage power; Low power; Sequential timing optimization; Short-path delay,Delay circuits; Economic and social effects; Electric clocks; Gates (transistor); Integrated circuit manufacture; Power management; Scheduling; Threshold voltage; Clock period minimization; Clock skew scheduling; Hold constraint; Leakage power; Low Power; Short-path; Timing optimization; Clocks
Complementary synthesis for encoder with flow control mechanism,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951933786&doi=10.1145%2f2794079&partnerID=40&md5=6c0a5f901a1be870e22cfe451c445061,"Complementary synthesis automatically generates an encoder's decoder with the assumption that the encoder's all input variables can always be uniquely determined by its output symbol sequence. However, to prevent the faster encoder from overwhelming the slower decoder, many encoders employ flow control mechanism that fails this assumption. Such encoders, when their output symbol sequences are too fast to be processed by the decoders, will stop transmitting data symbols, but instead transmitting idle symbols that can only uniquely determine a subset of the encoder's input variables. And the decoder should recognize and discard these idle symbols. This mechanism fails the assumption of all complementary synthesis algorithms, because some input variables can't be uniquely determined by the idle symbol. A novel algorithm is proposed to handle such encoders. First, it identifies all input variables that can be uniquely determined, and takes them as flow control variables. Second, it infers a predicate over these flow control variables that enables all other input variables to be uniquely determined. Third, it characterizes the decoder's Boolean function with Craig interpolant. Experimental results on several complex encoders indicate that this algorithm can always correctly identify the flow control variables, infer the predicates and generate the decoder's Boolean functions. © 2015 ACM.",Complementary synthesis; Craig interpolant; Decoder; Encoder; Finite-state transition system; Satisfiability solving,Algorithms; Boolean functions; Decoding; Interpolation; Synthesis (chemical); Complementary synthesis; Decoder; Encoder; Finite-state transition systems; Interpolants; Satisfiability solving; Flow control
A new uncertainty budgeting-based method for robust analog/mixed-signal design,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951993843&doi=10.1145%2f2778959&partnerID=40&md5=e1ac617d4db4434874eab13f9a80af47,"This article proposes a novel methodology for robust analog/mixed-signal IC design by introducing a notion of budget of uncertainty. This method employs a new conic uncertainty model to capture process variability and describes variability-affected circuit design as a set-based robust optimization problem. For a prespecified yield requirement, the proposed method conducts uncertainty budgeting by associating performance yield with the size of uncertainty set for process variations. Hence the uncertainty budgeting problem can be further translated into a tractable robust optimization problem. Compared with the existing robust design flow based on ellipsoid model, this method is able to produce more reliable design solutions by allowing varying size of conic uncertainty set at different design points. In addition, the proposed method addresses the limitation that the size of the ellipsoid model is calculated solely relying on the distribution of process parameters, while neglecting the dependence of circuit performance upon these design parameters. The proposed robust design framework has been verified on various analog/mixed-signal circuits to demonstrate its efficiency against the ellipsoid model. Up to 24% reduction of design cost has been achieved by using the uncertainty budgeting-based method. © 2015 ACM.",Budget of uncertainty; Performance yield; Process variations; Robust design; Uncertainty set,Analog integrated circuits; Budget control; Integrated circuit manufacture; Optimization; Budget of uncertainty; Performance yield; Process Variation; Robust designs; Uncertainty set; Design
Fuzzroute: A thermally efficient congestion-free global routing method for three-dimensional integrated circuits,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952013904&doi=10.1145%2f2767127&partnerID=40&md5=33021ce487ad7857a74236f686b58c29,"The high density of interconnects, closer proximity of modules, and routing phase are pivotal during the layout of a performance-centric three-dimensional integrated circuit (3D IC). Heuristic-based approaches are typically used to handle such NP-complete problems of global routing in 3D ICs. To overcome the inherent limitations of deterministic approaches, a novel methodology for multi-objective global routing based on fuzzy logic has been proposed in this article. The guiding information generated after the placement phase is used during routing with the help of a fuzzy expert system to achieve thermally efficient and congestion-free routing. A complete global routing solution is designed based on the proposed algorithms and the results are compared with selected fully established global routers, namely Labyrinth, FastRoute3.0, NTHU-R, BoxRouter 2.0, FGR, NTHU-Route2.0, FastRoute4.0, NCTU-GR, MGR, and NCTU-GR2.0. Experiments are performed over ISPD 1998 and 2008 benchmarks. The proposed router, called FuzzRoute, achieves balanced superiority in terms of routability, runtime, and wirelength over others. The improvements on routing time for Labyrinth, BoxRouter 2.0, and FGR are 91.81%, 86.87%, and 32.16%, respectively, for ISPD 1998 benchmarks. It may be noted that, though FastRoute3.0 achieves fastest runtime, it fails to generate congestion-free solutions for all benchmarks, which is overcome by the proposed FuzzRoute of the current article. It also shows wirelength improvements of 17.35%, 2.88%, 2.44%, 2.83%, and 2.10%, respectively, over others for ISPD 1998 benchmarks. For ISPD 2008 benchmark circuits it also provides 2.5%, 2.6%, 1 %, 1.1%, and 0.3% lesser wirelength and averagely runs 1.68×, 6.42×, 2.21×, 0.76×, and 1.54× faster than NTHU-Route2.0, FastRoute4.0, NCTU-GR, MGR, and NCTU-GR2.0, respectively. © 2015 Copyright is held by the Owner/Author. Publication rights licensed to ACM.",Fuzzified global routing; Fuzzy expert system; Global routing; VLSI layout design,Computational complexity; Expert systems; Fuzzy logic; Integrated circuits; Routers; VLSI circuits; Benchmark circuit; Deterministic approach; Fuzzy expert systems; Global routing; Guiding information; Inherent limitations; Three dimensional integrated circuits (3-D IC); VLSI layout; Three dimensional integrated circuits
Locality-aware network utilization balancing in NoCs,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951944492&doi=10.1145%2f2743012&partnerID=40&md5=88a468d4f3bce2816361a0283e0e4a9f,"Hierarchical and multi-network networks-on-chip (NoCs) have been proposed in the literature to improve the energy- and performance-efficient scalability of the traditional flat-mesh NoC architecture. Theoretically, based on a small-world network-based analysis, traditional hierarchical NoCs are expected to provide good scalability. However, the traditional theoretical analysis (e.g. for small-worldness) does not take into account the congestion phenomenon experienced in such networks. Counterintuitively, as shown in this work, breaking the hierarchy in traditional hierarchical NoCs and utilizing the proposed locality-aware network utilization (NU) balancing technique performs better. This improvement in performance is observed through experimental analysis, which is contrasted with the theoretical analysis that does not account for congestion. In addition to the novelties for hierarchical networks, the application of the proposed locality-aware NU balancing scheme is extended to multi-network NoC topologies (with already separated networks). Results of the analysis show the superiority of applying the locality-aware NU balancing technique for a throughput and energy-efficient scaling of the multi-network NoC architectures, much like those of the hierarchical NoCs. For instance, for a NoC with 1024 nodes, the proposed NU balancing technique provides up to 95% higher throughput efficiency and consumes up to 29% less energy per flit compared to the best NoC topology without the NU balancing technique. The analysis also helps to render the choice of a NoC topology for traffic patterns varying in locality and nonlocality on exascale computing CMPs. © 2015 ACM.",Hierarchical networks; Network utilization balancing; Network-on-chip (NoC),Distributed computer systems; Energy efficiency; MESH networking; Microprocessor chips; Network architecture; Scalability; Small-world networks; Topology; VLSI circuits; Balancing techniques; Exascale computing; Experimental analysis; Hierarchical network; Net work utilization; Network-based analysis; Network-on-chip(NoC); Throughput efficiency; Network-on-chip
A finite-point method for efficient gate characterization under multiple input switching,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952012029&doi=10.1145%2f2778970&partnerID=40&md5=85a35700a89d61fe0771e1958b76f122,"Timing characterization of standard cells is one of the essential steps in VLSI design. The traditional static timing analysis (STA) tool assumes single input switching models for the characterization of multiple input gates. However, due to technology scaling, increasing operating frequency, and process variation, the probability of the occurrence of multiple input switching (MIS) is increasing. On the other hand, considering all possible MIS scenarios for the characterization of multiple input logic gates, is computationally intensive. To improve the efficiency, this work proposes a finite-point-based characterization methodology for multiple input gates with the effects of MIS. Furthermore, delay variation due to MIS is integrated into the STA flow through propagation of switching windows. The proposed modeling methodology is validated using benchmark circuits at the 45nm technology node for various operating conditions. Experimental results demonstrate significant reduction in computation cost and data volume with less than ∼10% error compared to that of traditional SPICE simulation. © 2015 ACM.",Characterization; Design flow; Finite point method; Flow; Logic; Multiple input switching; Performance verification; Standard cell characterization,Adders; Characterization; Computation theory; SPICE; Switching; Design flows; Finite point method; Flow; Logic; Multiple inputs; Performance verification; Standard cell; Switching circuits
DARP-MP: Dynamically adaptable resilient pipeline design in multicore processors,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949648547&doi=10.1145%2f2755558&partnerID=40&md5=1dcea77e3c117088d9bf98a20fa1fe6a,"In this article, we demonstrate that the sensitized path delays in various microprocessor pipe stages exhibit intriguing temporal and spatial variations during the execution of real-world applications. To effectively exploit these delay variations, we propose dynamically adaptable resilient pipeline (DARP)-a series of runtime techniques to boost power-performance efficiency and fault tolerance in a pipelined microprocessor. DARP employs early error prediction to avoid amajor portion of the timing errors.We combine DARP with the state-of-art topologically homogeneous and power-performance heterogeneous (THPH) architecture to build up a new frontier for the energy efficiency of multicore processors (DARP-MP). Using a rigorous circuitarchitectural infrastructure, we demonstrate that DARP substantially improves the multicore processor performance (9.4-20%) and energy efficiency (10-28.6%) compared to state-of-the-art techniques. The energyefficiency improvements of DARP-MP are 42% and 49.9% compared against the original THPH and another state-of-art multicore power management scheme, respectively. © 2015 ACM.",Dynamic adjustment; Microprocessor pipeline; Multicore processor; Sensitized delay variation,Energy efficiency; Fault tolerance; Integrated circuit design; Microprocessor chips; Pipelines; Time delay; Delay variation; Dynamic adjustment; Multi-core processor; Power management scheme; Power performance; Power-performance efficiency; State-of-the-art techniques; Temporal and spatial variation; Pipeline processing systems
Security-aware design methodology and optimization for automotive systems,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951999885&doi=10.1145%2f2803174&partnerID=40&md5=f849e94cdee1918162b16d793bd38a63,"In this article, we address both security and safety requirements and solve security-aware design problems for the controller area network (CAN) protocol and time division multiple access (TDMA)-based protocols. To provide insights and guidelines for other similar security problems with limited resources and strict timing constraints, we propose a general security-aware design methodology to address security with other design constraints in a holistic framework and optimize design objectives. The security-aware design methodology is further applied to solve a security-aware design problem for vehicle-to-vehicle (V2V) communications with dedicated short-range communication (DSRC) technology. Experimental results demonstrate the effectiveness of our approaches in system design without violating design constraints and indicate that it is necessary to consider security together with other metrics during design stages. © 2015 ACM.",Automotive systems; Controller area network; Cyber-physical systems; Dedicated short-range communication; Mapping; Methodology; Time division multiple access; Time-triggered ethernet,Control system synthesis; Controllers; Dedicated short range communications; Design; Embedded systems; Mapping; Network protocols; Process control; Systems analysis; Time division multiple access; Vehicle to vehicle communications; Automotive Systems; Controller area network; Cyber physical systems (CPSs); Methodology; Time triggered Ethernet; Network security
Exploiting instruction set encoding for aging-aware microprocessor design,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952025709&doi=10.1145%2f2783435&partnerID=40&md5=9c2b6778914cce6486d51c82292f0450,"Microprocessors fabricated at nanoscale nodes are exposed to accelerated transistor aging due to bias temperature instability and hot carrier injection. As a result, device delays increase over time, reducing the mean time to failure (MTTF) and hence lifetime of the processor. To address this challenge, many (micro)-architectural techniques target the execution stage of the instruction pipeline, as this one is typically most critical. However, also the decoding stages can become aging critical and limit the microprocessor lifetime, as we will show in this work. Therefore, we propose a novel aging-aware instruction set-encoding methodology (ArISE) that improves the instruction encoding iteratively using a heuristic algorithm. In addition, the switching activities of the affected memory elements are considered in order to co-optimize lifetime and energy efficiency. Our experimental results show that MTTF of the decoding stages can be improved by 2.3× with negligible implementation costs. © 2015 ACM.",Aging; BTI; Decoder; HCI; Instruction set encoding; Microprocessor; Opcode; Transistor aging,Aging of materials; Decoding; Electric currents; Encoding (symbols); Energy efficiency; Field effect transistors; Heuristic algorithms; Heuristic methods; Hot carriers; Human computer interaction; Iterative methods; Microprocessor chips; Bias temperature instability; BTI; Decoder; Hot carrier injection; Instruction set; Microprocessor designs; Opcode; Switching activities; Integrated circuit design
DFT assisted techniques for peak launch-to-capture power reduction during launch-on-shift at-speed testing,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951938511&doi=10.1145%2f2790297&partnerID=40&md5=16b997901cad5578b1eec3bc3e77122f,"Scan-based testing is crucial to ensuring correct functioning of chips. In this scheme, the scan and capture phases are interleaved. It is well known that for large designs, excessive switching activity during the launch-to-capture window leads to high voltage droop on the power grid, ultimately resulting in false delay failures during at-speed test. This article proposes a new design-for-testability (DFT) scheme for launch-onshift (LOS) testing, which ensures that the combinational logic remains undisturbed between the interleaved capture phases, providing computer-aided-design (CAD) tools with extra search space for minimizing launchto-capture switching activity through test pattern ordering (TPO). We further propose a new TPO algorithm that keeps track of the don't cares during the ordering process, so that the don't care filling step after the ordering process yields a better reduction in launch-to-capture switching activity compared to any other technique in the literature. The proposed DFT-assisted technique, when applied to circuits in ITC99 benchmark suite, produces an average reduction of 17.68% in peak launch-to-capture switching activity (CSA) compared to the best known lowpower TPO technique. Even for circuits whose test cubes are not rich in don't care bits, the proposed technique produces an average reduction of 15% in peak CSA, while for the circuits with test cubes rich in don't care bits (≥75%), the average reduction is 24%. The proposed technique also reduces the average power dissipation (considering both scan cells and combinational logic) during the scan phase by about 43.5% on an average, compared to the adjacent filling technique. © 2015 ACM.",C-element; Critical path matching; Digital systems testing; Peak launch-to-capture switching activity; Scan flip-flop; X-filling,Algorithms; Computer aided design; Filling; Flip flop circuits; Integrated circuit testing; Scanning; Switching; C-element; Critical Paths; Digital system; Scan flip-flops; Switching activities; X-filling; Design for testability
An MDE approach for rapid prototyping and implementation of Dynamic Reconfigurable Systems,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951961388&doi=10.1145%2f2800784&partnerID=40&md5=70c409d6573ede095701008bdade5358,"This article presents a co-design methodology based on RecoMARTE, an extension to the well-known UML MARTE profile, which is used for the specification and automatic generation of Dynamic and Partially Reconfigurable Systems-on-Chip (DRSoC). This endeavor is part of a larger framework in which Model-Driven Engineering (MDE) techniques are extensively used for modeling and via model transformations, generating executable models, which are exploited by implementation tools to create reconfigurable systems. More specifically, the methodological aspects presented in this article are concerned with expediting the conception and implementation of the hardware platform and the integration of correct by construction reconfiguration controller. This article builds upon previous research by integrating previously separated endeavors to obtain a complete PR system generation chain, which aims at shielding the designer of many of the burdensome technological and tool-specific requirements. The methodology permits for the verification of the platform description at different stages in the development process (i.e., HDL for simulation, static FPGA implementation, controller simulation and verification). Furthermore, automation capabilities embedded in the flow enable the generation of the platform description and the integration of the reconfiguration controller executive seamlessly. In order to demonstrate the benefits of the proposed approach, we present a case study in which we target the creation of an image-processing application to be deployed onto an FPGA board. We present the required modeling strategies and we discuss how the generation chains are integrated with the back-end Xilinx tools (the most mature version of PR technology) to produce the necessary executable artifacts: VHDL for the platform description and a C description of the reconfiguration controller to be executed by an embedded processor. © 2015 ACM.",CAD; Discrete controller synthesis; FPGA; IP reuse; IP-XACT; Model driven engineering; Partial reconfiguration; UML MARTE,Chains; Computer hardware description languages; Controllers; Embedded systems; Field programmable gate arrays (FPGA); Image processing; Microprocessor chips; Structural design; System-on-chip; Discrete controller synthesis; IP reuse; IP-XACT; Model-driven Engineering; Partial reconfiguration; Uml marte; Computer aided design
Performance-driven unit-capacitor placement of successive-approximation-register ADCs,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951919302&doi=10.1145%2f2770872&partnerID=40&md5=dc436df3b7bcd4f8676b6d22e0804412,"The performance of many switched-capacitor analog integrated circuits, such as analog-to-digital converters (ADCs) and sample and hold circuits, is directly related to their accurate capacitance ratios. In general, capacitor mismatch can result from two sources of errors: random mismatch and systematic mismatch. Paralleling unit capacitance (UC) with a common-centroid structure can alleviate the random mismatch errors. The complexity of generating an optimal solution to the UC placement problem is extremely high, let alone if both placement and routing problems are to be optimized simultaneously. This article evaluates the performance of the UC placement generated in an existing work and proposes an alternative UC placement to achieve optimal ratio mismatch M and better linearity performance of SAR ADC design. Results show that the proposed UC placement achieves a ratio mismatch of M = 0.695, the effective number of bits ENOB = 8.314 bits, and the integral nonlinearity INL = 0.816 LSB (least significant bits) for a 9-bit SAR ADC design. © 2015 ACM.",Analog-to-digital converter (ADC); Common-centroid; Routing-aware placement; Spatial correlation; Successive-approximation-register (SAR) ADC; Unit capacitor,Analog integrated circuits; Approximation theory; Capacitance; Capacitors; Circuit theory; Digital integrated circuits; Random errors; Systematic errors; Analog to digital converters; Common centroid; Routing-aware placement; Spatial correlations; Successive approximation register; Analog to digital conversion
Lowering minimum supply voltage for power-efficient cache design by exploiting data redundancy,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952011765&doi=10.1145%2f2795229&partnerID=40&md5=f9f80f80da115fa62e32de7bc1f1222e,"Dynamic voltage scaling, SRAM reliability, cache replacement policy, VCCMIN Voltage scaling is known to be an efficient way of saving power and energy within a system, and large caches such as LLCs are good candidates for voltage scaling considering their constantly increasing size. However, the VCCMIN problem, in which the lower bound of scalable voltage is limited by process variation, has made it difficult to exploit the benefits of voltage scaling. Lowering VCCMIN incurs multibit faults, which cannot be efficiently resolved by current technologies due to their high complexity and power consumption. We overcame the limitation by exploiting the data redundancy of memory hierarchy. For example, cache coherence states and several layers of cache organization naturally expose the existence of redundancy within cache blocks. If blocks have redundant copies, their VCCMIN can be lowered; although more faults can occur in the blocks, they can be efficiently detected by simple error detection codes and recovered by reloading the redundant copies. Our scheme requires only minor modifications to the existing cache design. We verified our proposal on a cycle accurate simulator with SPLASH-2 and PARSEC benchmark suites and found that the VCCMIN of a 2MB L2 cache can be further lowered by 0.1V in 32nm technology with negligible degradation in performance. As a result, we could achieve 15.6% of reduction in dynamic power and 15.4% of reduction in static power compared to the previous minimum power. © 2015 ACM.",,Benchmarking; Cache memory; Static random access storage; Voltage scaling; Benchmark suites; Cache organization; Cache replacement policy; Current technology; Cycle-accurate simulators; Error Detection Code; Process Variation; SRAM reliabilities; Redundancy
Adaptive burst-writes (ABW): Memory requests scheduling to reduce write-induced interference,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952019127&doi=10.1145%2f2753757&partnerID=40&md5=d7225f60032912df3bdd968ca25de5d4,"Main memory latencies have become a major performance bottleneck for chip-multiprocessors (CMPs). Since reads are on the critical path, existing memory controllers prioritize reads over writes. However, writes must be eventually processed when the write queue is full. These writes are serviced in a burst to reduce the bus turnaround delay and increase the row-buffer locality. Unfortunately, a large number of reads may suffer long queuing delay when the burst-writes are serviced. The long write latency of future nonvolatile memory will further exacerbate the long queuing delay of reads during burst-writes. In this article, we propose a run-time mechanism, Adaptive Burst-Writes (ABW), to reduce the queuing delay of reads. Based on the row-buffer hit rate of writes and the arrival rate of reads, we dynamically control the number of writes serviced in a burst to trade off the write service time and the queuing latency of reads. For prompt adjustment, our history-based mechanism further terminates the burst-writes earlier when the row-buffer hit rate of writes in the previous burst-writes is low. As a result, our policy improves system throughput by up to 28% (average 10%) and 43% (average 14%) in CMPs with DRAM-based and PCM-based main memory. © 2015 ACM.",Memory controller; Memory request scheduling; Memory subsystem; Writeback-aware management,Economic and social effects; Interference suppression; Microprocessor chips; Queueing networks; Queueing theory; Scheduling; Chip multi-processors (CMPs); Memory controller; Memory subsystems; Non-volatile memory; Performance bottlenecks; Request scheduling; System throughput; Write-back; Dynamic random access storage
Enhancing the reliability of MLC NAND flash memory systems by read channel optimization,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942896886&doi=10.1145%2f2699866&partnerID=40&md5=915ce88818f5dc8c2994f214c44c6a2c,"NAND flash memory is not only the ubiquitous storagemedium in consumer applications but has also started to appear in enterprise storage systems as well. MLC and TLC flash technology made it possible to store multiple bits in the same silicon area as SLC, thus reducing the cost per amount of data stored. However, at current sub-20nm technology nodes, MLC flash devices fail to provide the levels of raw reliability, mainly cycling endurance, that are required by typical enterprise applications. Advanced signal processing and coding schemes are needed to improve the flash bit error rate and thus elevate the device reliability to the desired level. In this article, we report on the use of adaptive voltage thresholds and cell-to-cell interference cancellation in the read operation of NAND flash devices. We discuss how the optimal read voltage thresholds can be determined and assess the benefit of cancelling cell-to-cell interference in terms of cycling endurance, data retention, and resilience to read disturb. © 2015 ACM.",Characterization; NAND flash; Signal processing,Bit error rate; Cells; Cytology; Memory architecture; NAND circuits; Reliability; Signal processing; Advanced signal processing; Cell-to-cell interferences; Consumer applications; Cycling endurance; Device reliability; Enterprise applications; NAND Flash; NAND flash memory; Flash memory
TCONMAP: Technology mapping for parameterised FPGA configurations,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942927121&doi=10.1145%2f2751558&partnerID=40&md5=0703e79e4dd1f00243b5e3e9e96a9bab,"Parameterised configurations are FPGA configuration bitstreams in which the bits are defined as functions of user-defined parameters. From a parameterised configuration, it is possible to quickly and efficiently derive specialised, regular configuration bitstreams by evaluating these functions. The specialised bitstreams have different properties and functionality depending on the chosen values of the parameters. Themost important application of parameterised configurations is the generation of specialised configuration bitstreams for Dynamic Circuit Specialisation, a technique for optimising circuits at runtime using partial reconfiguration of the FPGA. Generating and using parameterised configurations requires a new FPGA tool flow. In this article, we present a new technology mapping algorithm for parameterised designs, called TCONMAP, that can be used to produce parameterised configurations in which both the configuration of the logic blocks and routing is a function of the parameters. In our experiments, we demonstrate that in using TCONMAP, the depth and area of the mapped circuit is close to the minimal depth and area attainable. Both Dynamic Circuit Specialisation and fine-grained modular reconfiguration are extracted by TCONMAP from the HDL description of the design requiring only simple parameter annotations. © 2015 ACM.",Dynamic circuit specialisation; FPGA; Runtime reconfiguration; Technology mapping,Binary sequences; Conformal mapping; Field programmable gate arrays (FPGA); Parameterization; FPGA configuration; Partial reconfiguration; Regular configuration; Run time reconfiguration; Specialisation; Technology mapping; Technology mapping algorithms; User-defined parameters; Reconfigurable hardware
An improved methodology for resilient design implementation,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942891363&doi=10.1145%2f2749462&partnerID=40&md5=61108279748979bcf3f631ea86c5ce5c,"Resilient design techniques are used to (i) ensure correct operation under dynamic variations and to (ii) improve design performance (e.g., timing speculation). However, significant overheads (e.g., 16% and 14% energy penalties due to throughput degradation and additional circuits) are incurred by existing resilient design techniques. For instance, resilient designs require additional circuits to detect and correct timing errors. Further, when there is an error, the additional cycles needed to restore a previous correct state degrade throughput, which diminishes the performance benefit of using resilient designs. In this work, we describe an improved methodology for resilient design implementation to minimize the costs of resilience in terms of power, area, and throughput degradation. Our methodology uses two levers: selective-endpoint optimization (i.e., sensitivity-based margin insertion) and clock skew optimization. We integrate the two optimization techniques in an iterative optimization flow which comprehends toggle rate information and the trade-off between cost of resilience and margin on combinational paths. Since the error-detection network can result in up to 9% additional wirelength cost, we also propose a matching-based algorithm for construction of the error-detection network to minimize this resilience overhead. Further, our implementations comprehend the impacts of signoff corners (in particular, hold constraints, and use of typical vs. slow libraries) and process variation, which are typically omitted in previous studies of resilience trade-offs. Our proposed flow achieves energy reductions of up to 21% and 10% compared to a conventional (with only margin used to attain robustness) design and a brute-force implementation (i.e., a typical resilient design, where resilient endpoints are (greedily) instantiated at timing-critical endpoints), respectively. We show that these benefits increase in the context of an adaptive voltage scaling strategy. © 2015 ACM.",Design optimization; Energy reduction; Resilience,Economic and social effects; Errors; Iterative methods; Voltage scaling; Adaptive voltage scaling; Clock skew optimization; Design optimization; Energy reduction; Iterative Optimization; Matching-based algorithm; Optimization techniques; Resilience; Design
Implementing an application-specific instruction-set processor for system-level dynamic program analysis engines,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942880607&doi=10.1145%2f2746238&partnerID=40&md5=5b8cf71c622fc2bb50889f4193b59aa6,"In recent years, dynamic program analysis (DPA) has been widely used in various fields such as profiling, finding bugs, and security. However, existing solutions have their ownweaknesses. Software solutions provide flexibility in DPA but they suffer from tremendous performance overhead. In contrast, core-level hardware engines rely on specialized integrated logics and attain extremely fast computation, but they have a limited functional extensibility because the logics are tightly coupled with the host processor. To mend this, a prior system-level approach utilizes an existing channel to integrate their hardware without necessitating the host architecture modification and introduced great potential in performance. Nevertheless, the prior work does not address the detailed design and implementation of the engine, which is quite essential to leverage the deployment on real systems. To address this, in this article, we propose an implementation of programmable DPA hardware engine, called program analysis unit (PAU). PAU is an application-specific instruction-set processor (ASIP) whose instruction set is customized to reflect common features of various DPA methods. With the specialized architecture and programmability of software, our PAU aims at fast computation and sufficient flexibility. In our case studies on several DPA techniques, we show that our ASIP approach can be successfully applicable to complex DPA schemes while providing hardware-backed power in performance and software-based flexibility in analysis. Recent experiments on our FPGA prototype revealed that the performance of PAU is 4.7-13.6 times faster than pure software DPA, and the power/area consumption is also acceptably small compared to today's mobile processors. © 2015 ACM.",Application-specific instruction-set processor (ASIP); Dynamic information flow tracking (DIFT); Dynamic program analysis (DPA); System-level analysis hardware,Application programs; Computation theory; Computational efficiency; Engines; Memory architecture; Program processors; Application specific instruction set processor; Dynamic information flow tracking; Dynamic program analysis; Hardware engines; Mobile processors; Software solution; System-level analysis; System-level approach; Program debugging
Fast simulation of networks-on-chip with priority-preemptive arbitration,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942932254&doi=10.1145%2f2755559&partnerID=40&md5=7b750963f8d0105a85ef751fc8fec060,"An increasingly time-consuming part of the design flow of on-chip multiprocessors is the simulation of the interconnect architecture. The accurate simulation of state-of-the art network-on-chip interconnects can take hours, and this process is repeated for each design iteration because it provides valuable insights on communication latencies that can greatly affect the overall performance of the system. In this article, we identify a time-predictable network-on-chip architecture and show that its timing behaviour can be predicted using models which are far less complex than the architecture itself. We then explore such a feature to produce simplified and lightweight simulation models that can produce latency figures with more than 90% accuracy and simulate more than 1,000 times faster when compared to a cycle-accurate model of the same interconnect. © 2015 ACM.",Network-on-chip; Transaction-level simulation,Computer architecture; Integrated circuit design; Network architecture; Network-on-chip; Servers; Communication latency; Design iteration; Interconnect architectures; Network-on-chip architectures; Networks on chips; On-chip multiprocessor; Time-consuming parts; Transaction level; Integrated circuit interconnects
Accurate analysis and prediction of enterprise service-level performance,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942849751&doi=10.1145%2f2757279&partnerID=40&md5=ab2915c59aaf1438f014ff2760b8210b,"An enterprise service-level performance time series is a sequence of data points that quantify demand, throughput, average order-delivery time, quality of service, or end-to-end cost. Analytical and predictive models of such time series can be embedded into an enterprise information system (EIS) in order to provide meaningful insights into potential business problems and generate guidance for appropriate solutions. Timeseries analysis includes periodicity detection, decomposition, and correlation analysis. Time-series prediction can be modeled as a regression problem to forecast a sequence of future time-series datapoints based on the given time series. The state-of-the-art (baseline) methods employed in time-series prediction generally apply advanced machine-learning algorithms. In this article, we propose a new univariate method for dealing with midterm time-series prediction. The proposed method first analyzes the hierarchical periodic structure in one time series and decomposes it into trend, season, and noise components. By discarding the noise component, the proposed method only focuses on predicting repetitive season and smoothed trend components. As a result, this method significantly improves upon the performance of baseline methods in midterm timeseries prediction. Moreover, we propose a new multivariate method for dealing with short-term time-series prediction. The proposed method utilizes cross-correlation information derived from multiple time series. The amount of data taken from each time series for training the regression model is determined by results from hierarchical cross-correlation analysis. Such a data-filtering strategy leads to improved algorithm efficiency and prediction accuracy. By combining statistical methods with advanced machine-learning algorithms, we have achieved a significantly superior performance in both short-term and midterm time-series predictions compared to state-of-the-art (baseline) methods. © 2015 ACM.",Machine learning; Optimization; Prediction,Correlation methods; Forecasting; Learning algorithms; Learning systems; Machine learning; Optimization; Predictive analytics; Quality of service; Regression analysis; Time series; Algorithm efficiency; Correlation analysis; Cross-correlation analysis; Enterprise information system; Multiple time series; Multivariate methods; Periodicity detection; Time series prediction; Time series analysis
H-matrix-based finite-element-based thermal analysis for 3D ICs,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942906087&doi=10.1145%2f2714563&partnerID=40&md5=6f467d32ed0f6ad04f31b9cad7ee2e9b,"In this article, we propose an efficient finite-element-based (FE-based) method for both steady and transient thermal analyses of high-performance integrated circuits based on the hierarchical matrix (H-matrix) representation. H-matrix has been shown to provide a data-sparse way to approximate the matrices and their inverses with almost linear-space and time complexities. In this work, we apply the H-matrix concept for solving heating diffusion problems modeled by parabolic partial differential equations (PDEs) based on the finite element method. We show that the matrix from a FE-based steady and transient thermal analysis can be represented by H-matrix without any approximation, and its inverse and Cholesky factors can be evaluated by H-matrix with controlled accuracy. We then show and prove that the memory and time complexities of the solver are bounded by O{script}(k1 Nlog N) and O{script}(k21 N log2 N), respectively, where k1 is a small quantity determined by accuracy requirements and N is the number of unknowns in the system. The comparison with existing product-quality LU solvers, CSPARSE and UMFPACK, on a number of 3D IC thermal matrices, shows that the new method is much more memory efficient than these methods, which however prevents CPU time comparison with those methods on large examples. But the proposed method can solve all the given thermal circuits with decent scalabilities, which shows good agreement with the predicted theoretical results. © 2015 ACM.",Finite element method; H-matrix; Integrated circuits; Thermal analysis,Integrated circuits; Inverse problems; Matrix algebra; Thermoanalysis; Three dimensional integrated circuits; Timing circuits; Transient analysis; Diffusion problems; H matrix; Hierarchical matrices; High-performance integrated circuits; Memory efficient; Parabolic partial differential equations; Steady and transient; Thermal circuits; Finite element method
VSSD: Performance isolation in a solid-state drive,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942876973&doi=10.1145%2f2755560&partnerID=40&md5=0d53354f6014265c15c393d3b24c4396,"Performance isolation is critical in shared storage systems, a popular storage solution. In a shared storage system, interference between requests from different users can affect the accuracy of I/O cost accounting, resulting in poor performance isolation. Recently, NAND flash-memory-based solid-state drives (SSDs) have been increasingly used in shared storage systems. However, interference in SSD-based shared storage systems has not been addressed. In this article, two types of interference, namely, queuing delay (QD) interference and garbage collection (GC) interference, are identified in a shared SSD. Additionally, a framework called VSSD is proposed to address these types of interference. VSSD is composed of two components: the FACO credit-based I/O scheduler designed to address QD interference and the ViSA flash translation layer designed to address GC interference. The VSSD framework aims to be implemented in the firmware running on an SSD controller. With VSSD, interference in an SSD can be eliminated and performance isolation can be ensured. Both synthetic and application workloads are used to evaluate the effectiveness of the proposed VSSD framework. The performance results show the following. First, QD and GC interference exists and can result in poor performance isolation between users on SSD-based shared storage systems. Second, VSSD is effective in eliminating the interference and achieving performance isolation between users. Third, the overhead of VSSD is insignificant. © 2015 ACM.",I/O scheduler; NAND flash memory; Performance isolation; SSD,Cost accounting; Firmware; Memory architecture; NAND circuits; Scheduling; Flash translation layer; Garbage collection; NAND flash memory; Performance isolation; Poor performance; Shared storage; Solid state drives; Storage solutions; Flash-based SSDs
Architecting the last-level cache for GPUs using STT-RAM technology,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942935230&doi=10.1145%2f2764905&partnerID=40&md5=48066a869e2a2d403137bff903422a81,"Future GPUs should have larger L2 caches based on the current trends in VLSI technology and GPU architectures toward increase of processing core count. Larger L2 caches inevitably have proportionally larger power consumption. In this article, having investigated the behavior of GPGPU applications, we present an efficient L2 cache architecture for GPUs based on STT-RAM technology. Due to its high-density and low-power characteristics, STT-RAM technology can be utilized in GPUs where numerous cores leave a limited area for on-chip memory banks. They have, however, two important issues, high energy and latency of write operations, that have to be addressed. Low retention time STT-RAMs can reduce the energy and delay of write operations. Nevertheless, employing STT-RAMs with low retention time in GPUs requires a thorough study on the behavior of GPGPU applications. Based on this investigation, we have architectured a two-part STT-RAM-based L2 cache with low-retention (LR) and high-retention (HR) parts. The proposed two-part L2 cache exploits a dynamic threshold regulator (DTR) to efficiently regulate the write threshold for migration of the data blocks from HR to LR, based on the behavior of the applications. Also, a Data and Access type Aware Cache Search mechanism (DAACS) is hired for handling the search of the requested data blocks in two parts of the cache. The STT-RAM L2 cache architecture proposed in this article can improve IPC by up to 171% (20% on average), and reduce the average consumed power by 28.9% compared to a conventional L2 cache architecture with equal on-chip area. © 2015 ACM.",GPGPU application; GPU; Retention time; STT-RAM,Cache memory; Graphics processing unit; Memory architecture; Program processors; Dynamic threshold; Last-level caches; Low-power characteristics; Retention time; Search mechanism; Stt rams; VLSI technology; Write operations; Random access storage
"Use it or lose it: Proactive, deterministic longevity in future chip multiprocessors",2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942935746&doi=10.1145%2f2770873&partnerID=40&md5=d3874543436e45e6cee6fd7c1af9377d,"Moore's Law scaling continues to yield higher transistor density with each succeeding process generation, leading to today'smany-core chip multiprocessors (CMPs) with tens or even hundreds of interconnected cores or tiles. Unfortunately, deep submicron CMOS process technology is marred by increasing susceptibility to wear. Prolonged operational stress gives rise to accelerated wearout and failure due to several physical failure mechanisms, including hot-carrier injection (HCI) and negative-bias temperature instability (NBTI). Each failure mechanism correlates with different usage-based stresses, all of which can eventually generate permanent faults. While the wearout of an individual core in many-core CMPs may not necessarily be catastrophic, a single fault in the interprocessor network-on-chip (NoC) fabric could render the entire chip useless, as it could lead to protocol-level deadlocks, or even partition away vital components such as the memory controller or other critical I/O. In this article, we study HCI- and NBTI-induced wear due to actual stresses caused by real workloads, applied onto the interconnect microarchitecture and develop a critical path model for NBTI-induced wearout. A key finding of this modeling is that, counter to prevailing wisdom, wearout in the CMP's on-chip interconnect is correlated with lack of load observed in the NoC routers rather than high load. We then develop a novel wearout-decelerating scheme in which routers under low load have their wear-sensitive components exercised without significantly impacting cycle time, pipeline depth, area, or power consumption of the overall router. A novel deterministic approach is proposed for the generation of appropriate exercise-mode data, ensuring design parameter targets are met. We subsequently show that the proposed design yields an ∼2,300× decrease in the rate of wear. © 2015 ACM.",Hot-carrier injection (HCI); Lifetime; Negative-bias temperature instability (NBTI); Network-on-chip; Reliability; Wearout,Electric currents; Failure (mechanical); Field effect transistors; Hot carriers; Integrated circuit interconnects; Multiprocessing systems; Negative bias temperature instability; Negative temperature coefficient; Network-on-chip; Outages; Reliability; Routers; Servers; Thermodynamic stability; Wear of materials; Deterministic approach; Hot carrier injection; Lifetime; Micro architectures; Network-on-chip(NoC); On chip interconnect; Sensitive components; Wearout; Integrated circuit design
An application adaptation approach to mitigate the impact of dynamic thermal management on video encoding,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942912647&doi=10.1145%2f2753758&partnerID=40&md5=572c7854caec2465ee4346808464be5b,"Due to limitations of cooling methods such as using fan and heat sink, dynamic thermal management (DTM) is being widely adopted to manage the temperature of computing systems. However, application of DTM can reduce the system performance and thereby affect the quality of real-time applications. Real-time video encoding, which has high computational need and hard deadlines, is a commonly used application that can be severely affected by the usage of DTM. We study the effect of DTM on a widely used H.264 video encoder and formulate a multidimensional optimization problem to maximize video quality and minimize bit rate while ensuring that the video encoder can run in real time in spite of DTM effects. We model the effects of adapting encoding parameters on video quality, bit rate, and encoder speed. We propose a dynamic application adaptation method to efficiently solve the optimization problem by optimally adapting the encoding parameters in response to DTM effects. In addition, we show that the proposed dynamic application adaptation method would reduce the need for cooling methods such as forced convection cooling. We implement the proposed approach on an Intel R® CoreTM 2 Duo platform where dynamic voltage and frequency scaling (DVFS) is used for DTM. Our measurements with several videos reveal that when DTM is applied, the video quality is affected significantly. However, using the proposed adaptation algorithm, the encoder can run in real time, and the quality loss is minimized with only a marginal increase in the bit rate. © 2015 ACM.",Cooling; Dynamic thermal management; Dynamic voltage and frequency scaling; Performance metrics; Performance optimization; Real-time systems; Video coding,Cooling; Cooling systems; Dynamic frequency scaling; Encoding (symbols); Image coding; Interactive computer systems; Motion compensation; Optimization; Real time systems; Signal encoding; Video signal processing; Voltage scaling; Adaptation algorithms; Application adaptation; Dynamic thermal management; Dynamic voltage and frequency scaling; Multidimensional optimization; Optimization problems; Performance metrics; Performance optimizations; Temperature control
Component-based synthesis of embedded systems using satisfiability modulo theories,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942900516&doi=10.1145%2f2746235&partnerID=40&md5=0fe09f3ab5276c59b0e09885d99a027e,"Constraint programming solvers, such as Satisfiability Modulo Theory (SMT) solvers, are capable tools in finding preferable configurations for embedded systems from large design spaces. However, constructing SMT constraint programs is not trivial, in particular for complex systems that exhibit multiple viewpoints and models. In this article we propose CoDeL: a component-based description language that allows system designers to express components as reusable building blocks of the system with their parameterizable properties, models, and interconnectivity. Systems are synthesized by allocating, connecting, and parameterizing the components to satisfy the requirements of an application. We present an algorithm that transforms component-based design spaces, expressible in CoDeL, to an SMT program, which, solved by state-of-the-art SMT solvers, determines the satisfiability of the synthesis problem, and delivers a correct-by-construction system configuration. Evaluation results for use cases in the domain of scheduling and mapping of distributed real-time processes confirm, first, the performance gain of SMT compared to traditional design space exploration approaches, second, the usability gains by expressing design problems in CoDeL, and third, the capability of the CoDeL/SMT approach to support the design of embedded systems. © 2015 ACM.",Components; Design space exploration; Embedded systems; Modeling; Satisfiability modulo theory; Systems specification methodology,Computer programming; Computer software reusability; Constraint theory; Formal logic; Models; Real time systems; Component based design; Components; Constraint programming; Correct-by-construction; Description languages; Design space exploration; Satisfiability modulo Theories; Systems specification; Embedded systems
Single-event multiple-transient characterization and mitigation via alternative standard cell placement methods,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942887217&doi=10.1145%2f2740962&partnerID=40&md5=306e7fd688d164c3bbe5197c6f4a996c,"As fabrication technology scales towards smaller transistor sizes and lower critical charge, single-event radiation effects are more likely to cause errant behavior in multiple, physically adjacent devices in modern integrated circuits (ICs), and with higher operating frequencies, this risk increasingly impacts design logic over memory as well. In order to increase future system reliability, circuit designers need greater awareness of multiple-transient charge-sharing effects during the early stages of their design flow with standard cell placement and routing. To measure the propagation and observability of multiple transients from single radiation events, this work uses several intra-pipeline combinational logic circuits at the 32nm technology node, investigates several different standard cell placements of each design, and analyzes those placements with a novel, physically realistic transient injection and simulation method. It is shown that (1) this simulation methodology, informed by experimental data, provides an increased realism over other works in traditional fault injection fields, (2) different placements of the same circuit where standard cells are grouped by logical hierarchy can result in different reliability behavior and benefits especially useful within the area of approximate computing, and (3) improved reliability through charge-sharing transient mitigation can be gained with no area penalty and minimal speed and power penalties by adjusting the placement of standard cells. © 2015 ACM.",Charge sharing; Combinational logic; Electronic design automation (EDA); Layout; Logical reconvergence; Multiple transient faults; Physical design; Placement; Radiation-induced faults; Single-event transient; Soft errors; Standard cells,Cells; Computation theory; Computer aided design; Computer aided software engineering; Cytology; Integrated circuit design; Integrated circuits; Logic design; Radiation effects; Radiation hardening; Reliability; Charge sharing; Combinational logic; Layout; Physical design; Placement; Radiation-induced; Re convergences; Single event transients; Soft error; Standard cell; Transient faults; Computer circuits
Robust and low-power digitally programmable delay element designs employing neuron-MOS mechanism,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942945093&doi=10.1145%2f2740963&partnerID=40&md5=51d2f8e7742fa19b006f93efc5ef51ff,"The feasibility of designing digitally programmable delay elements (PDEs) employing neuron-MOS mechanism is investigated in this work. By coupling the capacitors on the gate of the MOS transistor, the current flowing through the transistor can be digitally tuned without additional static power consumption. Various switching delays are generated by a clock buffer stage in this manner. Two types of neuron-MOS-based PDEs are suggested in this article. One of them is realized by directly applying capacitor-coupling technology on the transistors of an inverter as a clock buffer. The delay programmability is realized by tuning the charging/discharging current through the neuron-MOS inverter digitally. Since no additional transistor is introduced into the charging/discharging path, the performance fluctuation due to process variations on MOS transistors is reduced. The temperature effect is also partially compensated by the proposed neuron-MOS implementation. Another type of PDE circuit is proposed by employing a reliable reference-current-generator, where the neuron-MOS transistor acts as a linearly tunable resistance. A stable reference current is generated and used for charging/discharging the inverter as a clock buffer. As a result, the switching delay of the inverter is linearly programmed by digital input patterns. In general, both types of suggested PDE circuits achieve improved or fair performances over the robustness, power consumption, and linearity. © 2015 ACM.",Insensitivity to variations and temperature; Low-power; Programmable delay element,Clocks; Electric power utilization; Field effect transistors; Charging/discharging; Low Power; Neuron MOS inverters; Neuron MOS transistors; Programmable delay; Reference current generators; Reference currents; Static power consumption; Neurons
Constructing large and fast on-chip cache for mobile processors with multilevel cell STT-MRAM technology,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942942777&doi=10.1145%2f2764903&partnerID=40&md5=fc59097a4def320f38b38a7d3e794b24,"Modern mobile processors integrating an increasing number of cores into one single chip demand largecapacity, on-chip, last-level caches (LLCs) in order to achieve scalable performance improvements. However, adopting traditional memory technologies such as SRAM and embedded DRAM (eDRAM) leakage and scalability problems. Spin-transfer torque magnetic RAM (STT-MRAM) is a novel nonvolatile memory technology that has emerged as a promising alternative for constructing on-chip caches in high-end mobile processors. STT-MRAM has many advantages, such as short read latency, zero leakage from the memory cell, and better scalability than eDRAM and SRAM. Multilevel cell (MLC) STT-MRAM further enlarges capacity and reduces per-bit cost by storing more bits in one cell. However, MLC STT-MRAM has long write latency which limits the effectiveness of MLC STT-MRAM based LLCs. In this article, we address this limitation with three novel designs: line pairing (LP), line swapping (LS), and dynamic LP/LS enabler (DLE). LP forms fast cache lines by reorganizing MLC soft bits which are faster to write. LS dynamically stores frequently-written data into these fast cache lines. We then propose a dynamic LP/LS enabler (DLE) to enable LP and LS only if they help to improve the overall cache performance. Our experimental results show that the proposed designs improve system performance by 9-15% and reduce energy consumption by 14-21% for various types of mobile processors. © 2015 ACM.",Magnetic random access memory; Multilevel cell; Spin-transfer torque,Dynamic random access storage; Energy utilization; Magnetic recording; Magnetic storage; MRAM devices; Scalability; Static random access storage; Magnetic random access memory; Multi level cell (MLC); Multilevel cell; Non-volatile memory technology; Reduce energy consumption; Scalability problems; Scalable performance; Spin transfer torque; Cache memory
Built-in self-test and test scheduling for interposer-based 2.5D IC,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942928347&doi=10.1145%2f2757278&partnerID=40&md5=770c8aa579b1356f9ba6e25e93fb177f,"Interposer-based 2.5D integrated circuits (ICs) are seen today as a precursor to 3D ICs based on throughsilicon vias (TSVs). All the dies and the interposer in a 2.5D IC must be adequately tested for product qualification. We present an efficient built-in self-test (BIST) architecture for targeting defects in dies and in the interposer interconnects. The proposed BIST architecture can also be used for fault diagnosis during interconnect testing. To reduce the overall test cost, we describe a test scheduling and optimization technique under power constraints. We present simulation results to validate the BIST architecture and demonstrate fault detection, synthesis results to evaluate the area overhead of the proposed BIST architecture, and test scheduling results to highlight the effectiveness of the optimization approach. © 2015 ACM.",2.5D IC; Built-in self-test; Silicon interposer; Test-path scheduling,Fault detection; Integrated circuit interconnects; Scheduling; Three dimensional integrated circuits; Timing circuits; Built-in self-test architectures; Integrated circuits (ICs); Interconnect testing; Optimization approach; Optimization techniques; Product qualification; Silicon interposers; Through silicon vias; Built-in self test
FOLD: Extreme static test compaction by folding of functional test sequences,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942873946&doi=10.1145%2f2764455&partnerID=40&md5=788e24f4c441e6ec43b37816f7a214d6,"This article introduces a new approach to extreme static test compaction for functional test sequences that modifies the sequence in order to enhance the ability to omit test vectors from it and thus compact it. In the new approach, modification of the sequence and omission of test vectors from it are tightly coupled by focusing both subprocedures on subsequences of limited lengths. In a new process that is referred to as folding, a subsequence is partitioned into two halves, and the goal of the modification is to ensure that the two halves are as similar as possible. With similar halves, the expectation is that it will be possible to omit test vectors from the subsequence. Experimental results demonstrate that the procedure produces extremely short functional test sequences for benchmark circuits. © 2015 ACM.",Functional test sequences; Static test compaction; Test generation,Automation; Computer applications; Benchmark circuit; Functional test sequences; New approaches; Static test compaction; Subprocedures; Test generations; Test vectors; Tightly-coupled; Compaction
"Introduction to the special issue on reliable, resilient, and robust design of circuits and systems",2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942899451&doi=10.1145%2f2796541&partnerID=40&md5=9838c9261811b273150d883a4c26333b,[No abstract available],,
In-scratchpad memory replication: Protecting scratchpad memories in multicore embedded systems against soft errors,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942883720&doi=10.1145%2f2770874&partnerID=40&md5=4d44121c12ec977ce154ed2f33a3aff4,"Scratchpad memories (SPMs) are widely employed inmulticore embedded processors. Reliability is one of the major constraints in the embedded processor design, which is threatened with the increasing susceptibility of memory cells to multiple-bit upsets (MBUs) due to continuous technology down-scaling. This article proposes a low-cost and efficient data replication mechanism, called In-Scratchpad Memory Replication (ISMR), to correct MBUs in SPMs of multicore embedded processors. The main feature of ISMR is a smart controller, called Replication Management Unit (RMU), which is responsible for dynamically analyzing the activity of the SPM blocks at runtime and efficiently replicating the vulnerable SPM blocks into currently inactive SPM blocks. RMU exploits a 2-bit tag for each SPM block, where the value of each tag is determined by RMU according to the SPM access pattern. Accordingly, the proposed mechanism guarantees the replication of all vulnerable SPM blocks to provide error correction without decreasing the SPM utilization. To detect errors in SPM blocks, ISMR uses a 2-bit interleaved-parity code. As compared with the previous E-RAID 1 mechanism, the simulation results illustrate that for an 8-core embedded processor, the ISMR mechanism experiences 81% less energy consumption overhead and 48% less performance overhead. © 2015 ACM.",Multicore embedded system; Scratchpad memory; Soft errors,Energy utilization; Error correction; Memory architecture; Radiation hardening; Data replication; Embedded processor design; Embedded processors; Management unit; Multi-core embedded systems; Multiple bit upset; Scratch pad memory; Soft error; Embedded systems
Impact of cell failure on reliable cross-point resistive memory design,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942929031&doi=10.1145%2f2753759&partnerID=40&md5=927059ffcd110d2f34423082ceaea71b,"Resistive random access memory (ReRAM) technology is an emerging candidate for next-generation nonvolatile memory (NVM) architecture due to its simple structure, low programming voltage, fast switching speed, high on/off ratio, excellent scalability, good endurance, and great compatibility with silicon CMOS technology. The most attractive of the characteristics of ReRAM is its cross-point structure, which features a 4F2 cell size. In a cross-point structure, the existence of sneak current and resulting voltage loss due to the wire's resistance might cause read and write failures if not designed properly. In addition, a robust ReRAM design needs to deal with both soft and hard errors. In this article, we summarize mechanisms of both soft and hard errors of ReRAM cells and propose a unified model to characterize different failure behaviors. We quantitatively analyze the impact of cell failure types on the reliability of the cross-point array. We also propose an error-resilient architecture, which avoids unnecessary writes in the hard error detection unit. Assuming constant soft error rate, our approach can extend the lifetime of ReRAM up to 75% over a design without hard error detection and up to 12% over the design with a ""write-verify"" detection mechanism. Our approach yields greater significant lifetime improvement when considering postcycling retention degradation. © 2015 ACM.",Cross-point structure; Endurance failure; Resistive memory; Soft error,Cells; Error detection; Integrated circuit design; Memory architecture; Radiation hardening; RRAM; Cross point; Detection mechanism; Lifetime improvement; Non-volatile memory; Resistive memory; Resistive Random Access Memory (ReRAM); Retention degradation; Soft error; Failure (mechanical)
Least upper delay bound for VBR flows in networks-on-chip with virtual channels,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934761719&doi=10.1145%2f2733374&partnerID=40&md5=bb3dd6c0b1f0f31d9117947dab536e29,"Real-time applications such as multimedia and gaming require stringent performance guarantees, usually enforced by a tight upper bound on the maximum end-to-end delay. For FIFO multiplexed on-chip packet switched networks we consider worst-case delay bounds for Variable Bit-Rate (VBR) flows with aggregate scheduling, which schedules multiple flows as an aggregate flow. VBR Flows are characterized by a maximum transfer size (L), peak rate (p), burstiness (σ), and average sustainable rate (ρ). Based on network calculus, we present and prove theorems to derive per-flow end-to-end Equivalent Service Curves (ESC), which are in turn used for computing Least Upper Delay Bounds (LUDBs) of individual flows. In a realistic case study we find that the end-to-end delay bound is up to 46.9% more accurate than the case without considering the traffic peak behavior. Likewise, results also show similar improvements for synthetic traffic patterns. The proposed methodology is implemented in C++ and has low run-time complexity, enabling quick evaluation for large and complex SoCs. © 2015 ACM.",FIFO multiplexing; Network calculus; Network-on-chip (NoC); Performance evaluation; Worst-case delay bound,Aggregates; Calculations; Complex networks; Packet switching; Aggregate scheduling; Delay bound; End-to-end delay bounds; Network calculus; Network-on-chip(NoC); Performance evaluation; Performance guarantees; Real-time application; Network-on-chip
System-level observation framework for non-intrusive runtime monitoring of embedded systems,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934760631&doi=10.1145%2f2717310&partnerID=40&md5=890373be9598bf689cb57899556c6ec2,"As the complexity of embedded systems rapidly increases, the use of traditional analysis and debug methods encounters significant challenges in monitoring, analyzing, and debugging the complex interactions of various software and hardware components. This situation is further exacerbated for in-situ debugging and verification in which traditional debug and trace interfaces that require physical access are unavailable, infeasible, or cost prohibitive. In this article, we present a system-level observation framework that provides minimally intrusive methods for dynamically monitoring and analyzing deeply integrated hardware and software components within embedded systems. The system-level observation framework monitors hardware and software events by inserting additional logic for detecting designer-specified events within hardware cores to observe complex interaction across hardware and software boundaries at runtime, and provides visibility for monitoring complex execution behavior of software applications without affecting the system execution. © 2015 ACM.",Algorithms; Design; Experimentation; Measurement; Performance; Reliability; Verification,Algorithms; Application programs; Computer debugging; Design; Embedded systems; Measurement; Monitoring; Reliability; Verification; Cost prohibitive; Experimentation; Hardware and software; Hardware and software components; Performance; Runtime Monitoring; Software and hardwares; Software applications; Program debugging
Aging-and variation-aware delay monitoring using representative critical path selection,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934767427&doi=10.1145%2f2746237&partnerID=40&md5=8581233a77ac347cef5c4b4d4a1aeb0b,"Process together with runtime variations in temperature and voltage, as well as transistor aging, degrade path delay and may eventually induce circuit failure due to timing variations. Therefore, in-field tracking of path delays is essential, and to respond to this need, several delay sensor designs have been proposed in the literature. However, due to the significant overhead of these sensors and the large number of critical paths in today's IC, it is infeasible to monitor the delay of every critical path in silicon. We present an aging-and variationaware representative path selection technique based on machine learning that allows to measure the delay of a small set of paths and infer the delay of a larger pool of paths that are likely to fail due to delay variations. Simulation results for benchmark circuits highlight the accuracy of the proposed approach for predicting critical-path delay based on the selected representative paths. © 2015 ACM.",Monitoring; Performance; Reliability; Transistor aging; Variations,Automation; Computer applications; Monitoring; Reliability; Benchmark circuit; Circuit failures; Critical path delays; Delay variation; Performance; Run-time variations; Timing variations; Variations; Delay circuits
Explaining software failures by cascade fault localization,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934763465&doi=10.1145%2f2738038&partnerID=40&md5=83b196fe0423fca4c837e9a9f1f783aa,"During software debugging, a significant amount of effort is required for programmers to identify the root cause of a manifested failure. In this article, we propose a cascade fault localization method to help speed up this labor-intensive process via a combination of weakest precondition computation and constraint solving. Our approach produces a cause tree, where each node is a potential cause of the failure and each edge represents a casual relationship between two causes. There are two main contributions of this article that differentiate our approach from existing methods. First, our method systematically computes all potential causes of a failure and augments each cause with a proper context for ease of comprehension by the user. Second, our method organizes the potential causes in a tree structure to enable on-the-fly pruning based on domain knowledge and feedback from the user. We have implemented our new method in a software tool called CaFL, which builds upon the LLVM compiler and KLEE symbolic virtual machine. We have conducted experiments on a large set of public benchmarks, including real applications from GNU Coreutils and Busybox. Our results show that in most cases the user has to examine only a small fraction of the execution trace before identifying the root cause of the failure. © 2015 ACM.",Constraint; Fault localization; Satisfiability modulo theory (SMT); Weakest precondition,Computer Programing; Computer Programs; Failure; Problem Solving; Benchmarking; Computation theory; Failure (mechanical); Forestry; Open source software; Trees (mathematics); Casual relationships; Constraint; Constraint Solving; Fault localization; Labor intensive process; Satisfiability modulo Theories; Software debugging; Weakest precondition; Program debugging
Scheduling globally asynchronous locally synchronous programs for guaranteed response times,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934765895&doi=10.1145%2f2740961&partnerID=40&md5=a17f21ac526d9256d18770f1699313fc,"Safety-critical software systems need to guarantee functional correctness and bounded response times to external input events. Programs designed using reactive programming languages, based on formal mathematical semantics, can be automatically verified for functional correctness guarantees. Real-time guarantees on the other hand are much harder to achieve. In this article we provide a static analysis framework for guaranteeing response times for reactive programs developed using the Globally Asynchronous Locally Synchronous (GALS) model of computation. The proposed approach is applicable to scheduling of GALS programs for different target architectures with single or multiple processors or cores. A Satisfiability Modulo Theory (SMT) formulation in the quantifier free linear real arithmetic (QF LRA) logic is used for scheduling. A novel technique to encode rendezvous used in synchronization of globally asynchronous processes in the presence of locally synchronous parallelism and arbitrary preemption into QF LRA logic is presented. Finally, our SMT formulation is shown to produce schedules in reasonable time. © 2015 ACM.",Response time analysis; Satisfiability modulo theories; SystemJ; Worstcase reaction time,Computer circuits; Formal logic; Functional programming; Safety engineering; Scheduling; Semantics; Functional correctness; Globally asynchronous locally synchronous; Mathematical semantics; Response-time analysis; Safety-critical software systems; Satisfiability modulo Theories; SystemJ; Worstcase; Static analysis
A methodology to recover RTL IP functionality for automatic generation of SW applications,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934758180&doi=10.1145%2f2720019&partnerID=40&md5=b7308aceec1e5247df7a3a806f3c5d73,"With the advent of heterogeneous multiprocessor system-on-chips (MPSoCs), hardware/software partitioning is again on the rise both in research and in product development. In this new scenario, implementing intellectual-property (IP) blocks as SW applications rather than dedicated HWis an increasing trend to fully exploit the computation power provided by theMPSoC CPUs. On the other hand, whole libraries of IP blocks are available as RTL descriptions, most of them without a corresponding high-level SW implementation. In this context, this article presents a methodology to automatically generate SW applications in C++, by starting from existing RTL IPs implemented in hardware description language (HDL). The methodology exploits an abstraction algorithm to eliminate implementation details typical of HW descriptions (such as cycle-accurate functionality and data types) to guarantee relevant performance of the generated code. The experimental results show that, in many cases, the C++ code automatically generated in a few seconds with the proposed methodology is as efficient as the corresponding code manually implemented from scratch. © 2015 ACM.",Embedded software generation; IP reuse; RTL IP,C++ (programming language); Codes (symbols); Computer software reusability; Program processors; System-on-chip; Abstraction algorithms; Automatic Generation; Automatically generated; Hardware/software partitioning; Heterogeneous multiprocessor systems; IP reuse; RTL IP; Software generation; Computer hardware description languages
Array interleaving-an energy-efficient data layout transformation,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934760254&doi=10.1145%2f2747875&partnerID=40&md5=ef22607a8545ff17d5548663e0cde47b,"Optimizations related to memory accesses and data storage make a significant difference to the performance and energy of a wide range of data-intensive applications. These techniques need to evolve with modern architectures supporting wide memory accesses. We investigate array interleaving, a data layout transformation technique that achieves energy efficiency by combining the storage of data elements from multiple arrays in contiguous locations, in an attempt to exploit spatial locality. The transformation reduces the number of memory accesses by loading the right set of data into vector registers, thereby minimizing redundant memory fetches. We perform a global analysis of array accesses, and account for possibly different array behavior in different loop nests that might ultimately lead to changes in data layout decisions for the same array across program regions. Our technique relies on detailed estimates of the savings due to interleaving, and also the cost of performing the actual data layout modifications. We also account for the vector register widths and the possibility of choosing the appropriate granularity for interleaving. Experiments on several benchmarks show a 6-34% reduction in memory energy due to the strategy. © 2015 ACM.",Data layout; Memory energy optimization; SIMD architecture,Digital storage; Energy efficiency; Memory architecture; Data layout transformations; Data layouts; Data-intensive application; Energy efficient; Memory energies; Modern architectures; SIMD architecture; Spatial locality; Metadata
Layout-aware mixture preparation of biochemical fluids on application-specific digital microfluidic biochips,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934760706&doi=10.1145%2f2714562&partnerID=40&md5=fbf8f4e9fcee15e9dcf86017cfa6b278,"The recent proliferation of digital microfluidic (DMF) biochips has enabled rapid on-chip implementation of many biochemical laboratory assays or protocols. Sample preprocessing, which includes dilution and mixing of reagents, plays an important role in the preparation of assays. The automation of sample preparation on a digital microfluidic platform often mandates the execution of a mixing algorithm, which determines a sequence of droplet mix-split steps (usually represented as a mixing graph). However, the overall cost and performance of on-chip mixture preparation not only depends on the mixing graph but also on the resource allocation and scheduling strategy, for instance, the placement of boundary reservoirs or dispensers, mixer modules, storage units, and physical design of droplet-routing pathways. In this article, we first present a new mixing algorithm based on a number-partitioning technique that determines a layout-aware mixing tree corresponding to a given target ratio of a number of fluids. The mixing graph produced by the proposed method can be implemented on a chip with a fewer number of crossovers among droplet-routing paths as well as with a reduced reservoir-to-mixer transportation distance. Second, we propose a routing-aware resource-allocation scheme that can be used to improve the performance of a given mixing algorithm on a chip layout. The design methodology is evaluated on various test cases to demonstrate its effectiveness in mixture preparation with the help of two representative mixing algorithms. Simulation results show that on average, the proposed scheme can reduce the number of crossovers among droplet-routing paths by 89.7% when used in conjunction with the new mixing algorithm, and by 75.4% when an earlier algorithm [Thies et al. 2008] is used. © 2015 ACM.",Biochips; Design automation; Digital microfluidics; Dilution and mixing; Sample preparation,Biochips; Computer aided design; Drop formation; Graph algorithms; Mixers (machinery); Mixing; Mixtures; Resource allocation; Trees (mathematics); Allocation and scheduling; Biochemical laboratories; Design automations; Digital microfluidic biochips; On-chip implementations; Resource allocation schemes; Sample preparation; Transportation distance; Digital microfluidics
Decoupling capacitance design strategies for power delivery networks with power gating,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934768427&doi=10.1145%2f2700825&partnerID=40&md5=4d984242f42b2bee90736272af6eb236,"Power gating is a widely used leakage power saving strategy in modern chip designs. However, power gating introduces unique power integrity issues and trade-offs between switching and rush current (wake-up) supply noises. At the same time, the amount of power saving intrinsically trades off with power integrity. In addition, these trade-offs significantly vary with supply voltage. In this article, we propose systemic decoupling capacitors (decaps) optimization strategies that optimally trade-off between power integrity and leakage saving. Specially, new global decap and reroutable decap design concepts are proposed to relax the tight interaction between power integrity and leakage saving of power gated PDNs with a single supply voltage level. Furthermore, we propose a flexible decap allocation technique to deal with the design tradeoffs under multiple supply voltage levels. The proposed strategies are implemented in an automatic design flow for choosing the optimal amount of local decaps, global decaps and reroutable decaps. The conducted experiments demonstrate that leakage saving can be increased significantly compared with the conventional PDN design approach with a single supply voltage level using the proposed techniques without jeopardizing power integrity. For PDN designs operating at two supply voltage levels, the optimal performance is achieved at each voltage level. © 2015 ACM.",On-chip decaps; Power delivery network; Power gating,Economic and social effects; Electric network analysis; Electric power transmission; Leakage currents; De-coupling capacitance; Decoupling capacitor; Multiple supply voltages; On chips; Optimal performance; Optimization strategy; Power delivery network; Power gatings; Capacitance
Lazy-RTGC: A real-time lazy garbage collection mechanism with jointly optimizing average and worst performance for NAND flash memory storage systems,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934755369&doi=10.1145%2f2746236&partnerID=40&md5=6a7b184b0cf2df30a77f44ea84a7717c,"Due to many attractive and unique properties, NAND flash memory has been widely adopted in mission-critical hard real-time systems and some soft real-time systems. However, the nondeterministic garbage collection operation in NAND flash memory makes it difficult to predict the system response time of each data request. This article presents Lazy-RTGC, a real-time lazy garbage collection mechanism for NAND flash memory storage systems. Lazy-RTGC adopts two design optimization techniques: on-demand page-level address mappings, and partial garbage collection. On-demand page-level address mappings can achieve high performance of address translation and can effectively manage the flash space with the minimum RAM cost. On the other hand, partial garbage collection can provide the guaranteed system response time. By adopting these techniques, Lazy-RTGC jointly optimizes both the average and the worst system response time, and provides a lower bound of reclaimed free space. Lazy-RTGC is implemented in FlashSim and compared with representative real-time NAND flash memory management schemes. Experimental results show that our technique can significantly improve both the average and worst system performance with very low extra flash-space requirements. © 2015 ACM.",Garbage collection; NAND flash memory; Real-time system; Storage systems,Flash memory; Interactive computer systems; Mapping; Memory architecture; NAND circuits; Random access storage; Refuse collection; Response time (computer systems); Address translation; Design optimization; Garbage collection; Hard real-time systems; NAND flash memory; Soft real-time systems; Storage systems; System response time; Real time systems
High-throughput logic timing simulation on GPGPUs,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934760432&doi=10.1145%2f2714564&partnerID=40&md5=49c043d99edca4a496b9b86ea40bb988,"Many EDA tasks such as test set characterization or the precise estimation of power consumption, power droop and temperature development, require a very large number of time-aware gate-level logic simulations. Until now, such characterizations have been feasible only for rather small designs or with reduced precision due to the high computational demands. The new simulation system presented here is able to accelerate such tasks by more than two orders of magnitude and provides for the first time fast and comprehensive timing simulations for industrialsized designs. Hazards, pulse-filtering, and pin-to-pin delay are supported for the first time in a GPGPU accelerated simulator, and the system can easily be extended to even more realistic delay models and further applications. A sophisticated mapping with efficient memory utilization and access patterns as well as minimal synchronizations and control flow divergence is able to use the full potential of GPGPU architectures. To provide such a mapping, we combine for the first time the versatility of event-based timing simulation and multidimensional parallelism used in GPU-based gate-level simulators. The result is a throughput-optimized timing simulation algorithm, which runs many simulation instances in parallel and at the same time fully exploits gate-parallelism within the circuit. © 2015 ACM.",Gate-level simulation; General purpose computing on graphics processing unit (GP-GPU); Hazards; Parallel CAD; Pin-to-pin delay; Pulse-filtering; Timing simulation,Computation theory; Computer aided design; Computer graphics; Computer graphics equipment; Graphics processing unit; Hazards; Mapping; Memory architecture; Program processors; Timing circuits; Gate level simulation; General-purpose computing; Pin-to-pin delay; Pulse filtering; Timing simulations; Computer circuits
Design of ultra-low power scalable-throughput many-core DSP applications,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934753883&doi=10.1145%2f2720018&partnerID=40&md5=4d3f144e882921b1980f454953d58ff0,"We propose a system-level solution in designing process variation aware (PVA) scalable-throughput manycore systems for energy constrained applications. In our proposed methodology, we leverage the benefits of voltage scaling for obtaining energy efficiency while compensating for the loss in throughput by exploiting parallelism present in various DSP designs. We demonstrate that such a hybrid method consumes 6.27%-28.15% less power as compared to simple dynamic voltage scaling over different workload environments. Design details of a prototype chip fabricated on 90nm technology node and its findings are presented. Chip consists of 8 homogeneous FIR cores, which are capable of running from near-threshold to nominal voltages. In our 20 chip population, we observe 7% variation in speed among the cores at nominal voltage (0.9V) and 26% at near threshold voltage (0.55V).We also observe 54% variation in power consumption of the cores. For any desired throughput, the optimum number of cores and their optimum operating voltage is chosen based on the speed and power characteristics of the cores present inside the chip. We will also present analysis on energy-efficiency of such systems based on changes in ambient temperature © 2015 ACM.",90nm; Active unit scaling; ASIC chip; DSP; DVFS; Energy-efficient; Implementation; Low-power; Many core; Multicore; Near threshold; Process variation; Temperature variation; Voltage scaling,Digital signal processing; Energy efficiency; Temperature; Threshold voltage; Voltage scaling; 90nm; Active unit scaling; ASIC chips; DVFS; Energy efficient; Implementation; Low Power; Many core; Multi core; Near thresholds; Process Variation; Temperature variation; Design
Adaptive generation of unique IDs for digital chips through analog excitation,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934753761&doi=10.1145%2f2732408&partnerID=40&md5=bb2ca6303019462bbf7a45cb7d4a1cd1,"Globalization of the integrated circuit design and manufacturing flow has successfully ameliorated design complexity and fabrication cost challenges, and helped deliver cost-effective products while meeting stringent time-to-market requirements. On the flip side, it has resulted in various forms of security vulnerabilities in the supply chain that involves designers, fabs, test facilities, and distributors until the end-product reaches customers. One of the biggest threats to semiconductor industry today is the entry of aged, reject, or cloned parts, that is, counterfeit chips, into the supply chain, leading to annual revenue losses in the order of billions of dollars. While traceability of chips between trusted parties can help monitor the supply chain at various points in the flow, existing solutions are in the form of integrating costly hardware units on chip, or utilizing easy-to-circumvent inspection-based detection techniques. In this article, we propose a technique for adaptive unique ID generation that leverages process variations, enabling chip traceability. The proposed method stimulates digital chips with an analog signal from the supply lines, which serve as primary inputs to each gate in the signal path. Using a sinusoidal signal that exercises the transistors as gain components, we create a chip-specific response that can be post-processed into a digital ID. The proposed technique enables quick and cost-effective authenticity validation that requires no on-chip hardware support. Our simulation and experimentation on actual chips show that the proposed technique is capable of generating unique IDs even in the presence of environmental noise. © 2015 ACM.",Counterfeiting; Hardware security; ID generation,Cloning; Cost effectiveness; Costs; Hardware; Hardware security; Semiconductor device manufacture; Supply chains; Counterfeiting; Detection techniques; Environmental noise; ID generation; Manufacturing flow; Security vulnerabilities; Semiconductor industry; Sinusoidal signals; Product design
Marching-based wear-leveling for PCM-based storage systems,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924045020&doi=10.1145%2f2699831&partnerID=40&md5=f033e306137384a937e8c0e1330ab855,"Improving the performance of storage systems without losing the reliability and sanity/integrity of file systems is a major issue in storage system designs. In contrast to existing storage architectures, we consider a PCM-based storage architecture to enhance the reliability of storage systems. In PCM-based storage systems, the major challenge falls on how to prevent the frequently updated (meta)data from wearing out their residing PCM cells without excessively searching and moving metadata around the PCM space and without extensively updating the index structures of file systems. In this work, we propose an adaptive wearleveling mechanism to prevent any PCM cell from being worn out prematurely by selecting appropriate data for swapping with constant search/sort cost. Meanwhile, the concept of indirect pointers is designed in the proposed mechanism to swap data without any modification to the file system's indexes. Experiments were conducted based on well-known benchmarks and realistic workloads to evaluate the effectiveness of the proposed design, for which the results are encouraging. © 2015 ACM.",Metadata; Phase-change memory; Storage system; Wear leveling,File organization; Metadata; Phase change memory; Wear of materials; File systems; Index structure; Storage architectures; Storage systems; Wear leveling; Search engines
A tool for analog/RF BIST evaluation using statistical models of circuit parameters,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924051143&doi=10.1145%2f2699837&partnerID=40&md5=a6b7608486c452a0720dd1975d3fc9a7,"Testing analog integrated circuits is expensive in terms of both test equipment and time. To reduce the cost, Design-For-Test techniques (DFT) such as Built-In Self-Test (BIST) have been developed. For a given Circuit Under Test (CUT), the choice of a suitable technique should be made at the design stage as a result of the analysis of test metrics such as test escapes and yield loss. However, it is very hard to carry out this estimation for analog/RF circuits by using fault simulation techniques. Instead, the estimation of parametric test metrics is made possible by Monte Carlo circuit-level simulations and the construction of statistical models. These models represent the output parameter space of the CUT in which the test metrics are defined. In addition, models of the input parameter space may be required to accelerate the simulations and obtain higher confidence in the DFT choices. In this work, we describe a methodological flow for the selection of most adequate statistical models and several techniques that can be used for obtaining these models. Some of these techniques have been integrated into a Computer-Aided Test (CAT) tool for the automation of the process of test metrics estimation. This estimation is illustrated for the case of a BIST solution for CMOS imager pixels that requires the use of advanced statistical modeling techniques. © 2015 ACM.",Analog/RF test; BIST evaluation; Computer-aided test; Copula theory; Statistical modeling; Test metrics estimation,Analog integrated circuits; Built-in self test; Design for testability; Electric network parameters; Equipment testing; Integrated circuit design; Monte Carlo methods; Statistics; Timing circuits; Analog/rf tests; Computer aided tests; Copula theory; Statistical modeling; Test metrics; Parameter estimation
Robust design space modeling,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924081921&doi=10.1145%2f2668118&partnerID=40&md5=489d93a6eba050c7a4de048d4b61d8fa,"Architectural design spaces of microprocessors are often exponentially large with respect to the pending processor parameters. To avoid simulating all configurations in the design space, machine learning and statistical techniques have been utilized to build regression models for characterizing the relationship between architectural configurations and responses (e.g., performance or power consumption). However, this article shows that the accuracy variability of many learning techniques over different design spaces and benchmarks can be significant enough to mislead the decision-making. This clearly indicates a high risk of applying techniques that work well on previous modeling tasks (each involving a design space, benchmark, and design objective) to a new task, due to which the powerful tools might be impractical. Inspired by ensemble learning in the machine learning domain, we propose a robust framework called ELSE to reduce the accuracy variability of design space modeling. Rather than employing a single learning technique as in previous investigations, ELSE employs distinct learning techniques to build multiple base regression models for each modeling task. This is not a trivial combination of different techniques (e.g., always trusting the regression model with the smallest error). Instead, ELSE carefully maintains the diversity of base regression models and constructs a metamodel from the base models that can provide accurate predictions even when the base models are far from accurate. Consequently, we are able to reduce the number of cases in which the final prediction errors are unacceptably large. Experimental results validate the robustness of ELSE: compared with the widely used artificial neural network over 52 distinct modeling tasks, ELSE reduces the accuracy variability by about 62%. Moreover, ELSE reduces the average prediction error by 27% and 85% for the investigated MIPS and POWER design spaces, respectively. © 2015 ACM.",Architectural simulation; Design space modeling; Ensemble learning,Decision making; Errors; Forecasting; Integrated circuit design; Machine learning; Regression analysis; Accurate prediction; Architectural simulation; Average prediction error; Design spaces; Ensemble learning; Final prediction errors; Learning techniques; Statistical techniques; Architectural design
Prolonging lifetime of PCM-based main memories through on-demand page pairing,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924048524&doi=10.1145%2f2699867&partnerID=40&md5=b6d8fd4cc79c5827ff05c6f2f0e37300,"With current memory scalability challenges, Phase-Change Memory (PCM) is viewed as an attractive replacement to DRAM. The preliminary concern for PCM applicability is its limited write endurance that results in fast wear-out of memory cells. Worse, process variation in the deep-nanometer regime increases the variation in cell lifetime, resulting in an early and sudden reduction in main memory capacity due to the wear-out of a few cells. Recent studies have proposed redirection or correction schemes to alleviate this problem, but all suffer poor throughput or latency. In this article, we show that one of the inefficiency sources in current schemes, even when wear-leveling algorithms are used, is the nonuniform write endurance limit incurred by process variation, that is, when some memory pages have reached their endurance limit, other pages may be far from their limit. In this line, we present a technique that aims to displace a faulty page to a healthy page. This technique, called On-Demand Page Paired PCM (OD3P, for short), when applied at page level, can improve PCM time-to-failure by 20% on average for different multithreaded and multiprogrammed workloads while also improving IPC by 14% on average compared to previous page-level techniques. The comparison between line-level OD3P and previous line-level techniques reveals about 2× improvement of lifetime and performance. Categories and Subject Descriptors: B.3.1 [Semiconductor Memories]: Phase Change Memory General Terms: Design, Performance. © 2015 ACM.",Multilevel cell; Page pairing; Phase-change memory,Cells; Cytology; Dynamic random access storage; Multiprogramming; Semiconductor storage; Wear of materials; Correction schemes; Multilevel cell; Nano-meter regimes; Page pairing; Phase change memory (pcm); Process Variation; Semiconductor memory; Wear-leveling algorithms; Phase change memory
Place: Electrostatics-based placement using fast fourier transform and nesterov's method,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924087321&doi=10.1145%2f2699873&partnerID=40&md5=5722a89a4ff8174115ce9a9c66024b4c,"We develop a flat, analytic, and nonlinear placement algorithm, ePlace, which is more effective, generalized, simpler, and faster than previous works. Based on the analogy between placement instance and electrostatic system, we develop a novel placement density function eDensity, which models every object as positive charge and the density cost as the potential energy of the electrostatic system. The electric potential and field distribution are coupled with density using a well-defined Poisson's equation, which is numerically solved by spectral methods based on fast Fourier transform (FFT). Instead of using the conjugate gradient (CG) nonlinear solver in previous placers, we propose to use Nesterov'smethod which achieves faster convergence. The efficiency bottleneck on line search is resolved by predicting the steplength using a closed-form equation of Lipschitz constant. The placement performance is validated through experiments on the ISPD 2005 and ISPD 2006 benchmark suites, where ePlace outperforms all state-of-the-art placers (Capo10.5, FastPlace3.0, RQL, MAPLE, ComPLx, BonnPlace, POLAR, APlace3, NTUPlace3, mPL6) with much shorter wirelength and shorter or comparable runtime. On average, of all the ISPD 2005 benchmarks, ePlace outperforms the leading placer BonnPlace with 2.83% shorter wirelength and runs 3.05× faster; and on average, of all the ISPD 2006 benchmarks, ePlace outperforms the leading placer MAPLE with 4.59% shorter wirelength and runs 2.84× faster. © 2015 ACM.",Analytic placement; Density function; Electrostatics; Fast fourier transform; Nesterov's method; Nonlinear optimization; Poisson's equation; Preconditioning; Spectral methods,Benchmarking; Dissociation; Electric potential; Electrostatics; Fast Fourier transforms; Nonlinear equations; Nonlinear programming; Placers; Poisson distribution; Potential energy; Probability density function; Spectroscopy; Analytic placement; Nesterov's methods; Non-linear optimization; Preconditioning; Spectral methods; Poisson equation
ASP-based encoding model of architecture synthesis for smart cameras in distributed networks,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924044756&doi=10.1145%2f2701419&partnerID=40&md5=94f7c59086a0ce1820548749dab91532,"A synthesis approach based on Answer Set Programming (ASP) for heterogeneous system-on-chips to be used in distributed camera networks is presented. In such networks, the tight resource limitations represent a major challenge for application development. Starting with a high-level description of applications, the physical constraints of the target devices, and the specification of network configuration, our goal is to produce optimal computing infrastructures made of a combination of hardware and software components for each node of the network. Optimization aims at maximizing speed while minimizing chip area and power consumption. Additionally, by performing the architecture synthesis simultaneously for all cameras in the network, we are able tominimize the overall utilization of communication resources and consequently reduce power consumption. Because of its reconfiguration capabilities, a Field Programmable Gate Array (FPGA) has been chosen as the target device, which enhances the exploration of several design alternatives. We present several realistic network scenarios to evaluate and validate the proposed synthesis approach. © 2015 ACM.",Answer set programming; Architecture synthesis; Distributed smart cameras; FPGA; Multiobjective optimization; Power minimization; Reconfigurable systems; System design; Technology mapping,Application programs; Cameras; Electric power utilization; Field programmable gate arrays (FPGA); Logic programming; Logic Synthesis; Multiobjective optimization; Network coding; Reconfigurable architectures; Reconfigurable hardware; Structural design; System-on-chip; Systems analysis; Answer set programming; Architecture synthesis; Distributed Smart Cameras; Power minimization; Reconfigurable systems; Technology mapping; Network architecture
Automated iterative pipelining for ASIC design,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924073967&doi=10.1145%2f2660768&partnerID=40&md5=3415c10466412f52de2ab8009ae37a15,"We describe an automated pipelining approach for optimally balanced pipeline implementation that achieves low area cost as well as meeting timing requirements. Most previous automatic pipelining methods have focused on Instruction Set Architecture (ISA)-based designs and themain goal of suchmethods generally has been maximizing performance as measured in terms of instructions per clock (IPC). By contrast, we focus on datapath-oriented designs (e.g., DSP filters for image or communication processing applications) in ASIC design flows. The goal of the proposed pipelining approach is to find the optimally pipelined design that not only meets the user-specified target clock frequency, but also seeks to minimize area cost of a given design. Unlike most previous approaches, the proposed methods incorporate the use of accurate area and timing information (iteratively achieved by synthesizing every interim pipelined design) to achieve higher accuracy during design exploration. When compared with exhaustive design exploration that considers all possible pipeline patterns, the two heuristic pipelining methods presented here involve only a small area penalty (typically under 5%) while offering dramatically reduced computational complexity. Experimental validation is performed with commercial ASIC design tools and described for applications including polynomial function evaluation, FIR filters, matrix multiplication, and discrete wavelet transform filter designs with a 90nm standard cell library. © 2015 ACM.",Asic designs; Design area optimization; Pipelined hardware architecture; Pipelining; Timing error resolution,Application specific integrated circuits; Clocks; Computer architecture; Discrete wavelet transforms; FIR filters; Heuristic methods; Iterative methods; Optimization; Pipe linings; Pipelines; Area optimization; ASIC design; Experimental validations; Instruction set architecture; Pipeline implementation; Pipelined hardware; Processing applications; Timing errors; Integrated circuit design
"Reconfigurable scan networks: Modeling, verification, and optimalpattern generation",2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924036627&doi=10.1145%2f2699863&partnerID=40&md5=564a6b17eaff9c04b13866bc2526ab1e,"Efficient access to on-chip instrumentation is a key requirement for post-silicon validation, test, debug, bringup, and diagnosis. Reconfigurable scan networks, as proposed by, for example, IEEE Std 1687-2014 and IEEE Std 1149.1-2013, emerge as an effective and affordable means to cope with the increasing complexity of on-chip infrastructure. Reconfigurable scan networks are often hierarchical and may have complex structural and functional dependencies. Common approaches for scan verification based on static structural analysis and functional simulation are not sufficient to ensure correct operation of these types of architectures. To access an instrument in a reconfigurable scan network, a scan-in bit sequence must be generated according to the current state and structure of the network. Due to sequential and combinational dependencies, the access pattern generation process (pattern retargeting) poses a complex decision and optimization problem. This article presents the first generalized formal model that considers structural and functional dependencies of reconfigurable scan networks and is directly applicable to 1687-2014-based and 1149.1-2013-based scan architectures. This model enables efficient formal verification of complex scan networks, as well as automatic generation of access patterns. The proposed pattern generation method supports concurrent access to multiple target scan registers (access merging) and generates short scan-in sequences. © 2015 ACM.",1149.1-2013; 1687-2014; Access pattern generation; Design for debug and diagnosis; DFT verification; Ijtag; Jtag; Pattern retargeting; Reconfigurable scan network,Design for testability; Formal verification; IEEE Standards; Network architecture; Reconfigurable architectures; 1149.1-2013; 1687-2014; Access patterns; Design for debug; Ijtag; Jtag; Pattern retargeting; Reconfigurable; Complex networks
Data-driven optimization of order admission policies in a digital print factory,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924064424&doi=10.1145%2f2699836&partnerID=40&md5=39f4c80f4631bf816a332016fbdd42a1,"On-demand digital print service is an example of a real-time embedded enterprise system. It offers mass customization and exemplifies personalized manufacturing services. Once a print order is submitted to the print factory by a client, the print service provider (PSP) needs to make a real-time decision on whether to accept or refuse this order. Based on the print factory's current capacity and the order's properties and requirements, an order is refused if its acceptance is not profitable for the PSP. The order is accepted with the most appropriate due date in order to maximize the profit that can result from this order. We have developed an automated learning-based order admission framework that can be embedded into an enterprise environment to provide real-time admission decisions for new orders. The framework consists of three classifiers: Support Vector Machine (SVM), Decision Tree (DT), and Bayesian Probabilistic Model (BPM). The classifiers are trained by history orders and used to predict completion status for new orders. A decision integration technique is implemented to combine the results of the classifiers and predict due dates. Experimental results derived using real factory data from a leading print service provider and Weka open-source software show that the order completion status prediction accuracy is significantly improved by the decision integration strategy. The proposed multiclassifier model also outperforms a standalone regression model. © 2015 ACM.",Machine learning; Optimization; Prediction,Forecasts; Models; Optimization; Printing; Decision trees; Forecasting; Learning systems; Open systems; Optimization; Profitability; Regression analysis; Support vector machines; Bayesian probabilistic models; Data-driven optimization; Enterprise environment; Integration strategy; Integration techniques; Manufacturing service; Multi-classifier models; Print-service providers; Open source software
A generalized definition of unnecessary test vectors in functional test sequences,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924065883&doi=10.1145%2f2699853&partnerID=40&md5=0a6ebe1e36b65db26b90cf40a6193e64,"A class of static test compaction procedures for functional test sequences is based on the omission of unnecessary test vectors. According to the definition used by these procedures, a test vector is unnecessary if all the target faults continue to be detected after it is omitted. This article introduces a more general definition of unnecessary test vectors that allows additional ones to be omitted. According to this definition, a test vector is unnecessary if every target fault can be detected by a sequence that is obtained after omitting the vector, and possibly other vectors. The article develops a procedure for omitting test vectors based on this definition and discusses its effects on the storage requirements and test application time. © 2015 ACM.",Functional test sequences; Sequential test generation; Static test compaction; Test vector omission,Compaction; Vectors; Functional test sequences; Sequential tests; Static test compaction; Storage requirements; Target faults; Test application time; Test vectors; Testing
Obstacle-avoiding algorithm in X-architecture based on discrete particle swarm optimization for VLSI design,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924069533&doi=10.1145%2f2699862&partnerID=40&md5=906e2e6e1409ee4b6efb4b8e84fa2e1b,"Obstacle-avoiding Steiner minimal tree (OASMT) construction has become a focus problem in the physical design of modern very large-scale integration (VLSI) chips. In this article, an effective algorithm is presented to construct an OASMT based on X-architecturex for a given set of pins and obstacles. First, a kind of special particle swarm optimization (PSO) algorithm is proposed that successfully combines the classic genetic algorithm (GA), and greatly improves its own search capability. Second, a pretreatment strategy is put forward to deal with obstacles and pins, which can provide a fast information inquiry for the whole algorithm by generating a precomputed lookup table. Third, we present an efficient adjustment method, which enables particles to avoid all the obstacles by introducing some corner points of obstacles. Finally, an excellent refinement method is discussed to further enhance the quality of the final routing tree, which can improve the quality of the solution by 7.93% on average. To our best knowledge, this is the first time to specially solve the single-layer obstacle-avoiding problem in X-architecture. Experimental results show that the proposed algorithm can further shorten wirelength in the presence of obstacles. And it achieves the best solution quality in a reasonable runtime among the existing algorithms. © 2015 ACM.",Adjustment; Obstacle avoiding; Particle swarm optimization (pso); Pretreatment strategy; Refinement; Steiner tree,Genetic algorithms; Integrated circuit design; Table lookup; VLSI circuits; Adjustment; Obstacle-avoiding; Pre-Treatment; Refinement; Steiner trees; Particle swarm optimization (PSO)
A fault-aware toolchain approach for FPGA fault tolerance,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924069933&doi=10.1145%2f2699838&partnerID=40&md5=58ed3172c07c162b53fae15fe8a5f612,"As the size and density of silicon chips continue to increase, maintaining acceptable manufacturing yields has become increasingly difficult. Recent works suggest that lithography techniques are reaching their limits with respect to enabling high yield fabrication of small-scale devices, thus there is an increasing need for techniques that can tolerate fabrication time defects. One candidate technology to help combat these defects is reconfigurable hardware. The flexible nature of reconfigurable devices, such as Field Programmable Gate Arrays (FPGAs), makes it possible for them to route around defective areas of a chip after the device has been packaged and deployed into the field. This work presents a technique that aims to increase the effective yield of FPGA manufacturing by reclaiming a portion of chips that would be ordinarily classified as unusable. In brief, we propose a modification to existing commercial toolchain flows to make them fault aware. A phase is added to identify faults within the chip. The locations of these faults are then used by the toolchain to avoid faults during the placement and routing phase. Specifically, we have applied our approach to the Xilinx commercial toolchain flow and evaluated its tolerance to both logic and routing resource faults. Our findings show that, at a cost of 5-10% in device frequency performance, the modified toolchain flow can tolerate up to 30% of logic resources being faulty and, depending on the nature of the target application, can tolerate 1-30% of the device's routing resources being faulty. These results provide strong evidence that commercial toolchains not designed for the purpose of tolerating faults can still be greatly leveraged in the presence of faults to place and route circuits in an efficient manner. © 2015 ACM.",Design automation; Fault tolerance; Reconfigurable hardware,Computer aided design; Computer hardware; Defects; Fault tolerance; Field programmable gate arrays (FPGA); Lithography; Manufacture; Design automations; Frequency performance; Manufacturing yield; Placement and routing; Reconfigurable devices; Routing resources; Small scale devices; Target application; Reconfigurable hardware
Conditional diagnosability of cayley graphs generated by transposition trees under the PMC model,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924085293&doi=10.1145%2f2699854&partnerID=40&md5=2fcb6cc534dff7558193383b1d28214b,"Processor fault diagnosis has played an essential role inmeasuring the reliability of a multiprocessor system. The diagnosability of many well-known multiprocessor systems has been widely investigated. Conditional diagnosability is a novel measure of diagnosability by adding a further condition that any fault set cannot contain all the neighbors of every node in the system. Several known structural properties of Cayley graphs are exhibited. Based on these properties, we investigate the conditional diagnosability of Cayley graphs generated by transposition trees under the PMC model and show that it is 4n- 11 for n ≥ 4 except for the n-dimensional star graph for which it has been shown to be 8n-21 for n ≥ 5 (refer to Chang andHsieh [2014]). © 2015 ACM.",Cayley graphs; Conditional diagnosability; Fault tolerance; Interconnection networks; Multiprocessor systems; PMC model,Defects; Diagnosis; Models; Networks; Fault tolerance; Forestry; Graph Databases; Graphic methods; Interconnection networks (circuit switching); Multiprocessing systems; Cayley graphs; Conditional diagnosability; Diagnosability; Fault set; Multi processor systems; PMC model; Star graphs; Trees (mathematics)
Applying pay-burst-only-once principle for periodic power management in hard real-time pipelined multiprocessor systems,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924072831&doi=10.1145%2f2699865&partnerID=40&md5=8a05a45ebc44fd27e6bd72564799c134,"Pipelined computing is a promising paradigm for embedded system design. Designing a power management policy to reduce the power consumption of a pipelined system with nondeterministic workload is, however, nontrivial. In this article, we study the problem of energy minimization for coarse-grained pipelined systems under hard real-time constraints and propose new approaches based on an inverse use of the pay-burst-onlyonce principle. We formulate the problem by means of the resource demands of individual pipeline stages and propose two new approaches, a quadratic programming-based approach and fast heuristic, to solve the problem. In the quadratic programming approach, the problem is transformed into a standard quadratic programming with box constraint and then solved by a standard quadratic programming solver. Observing the problem is NP-hard, the fast heuristic is designed to solve the problem more efficiently. Our approach is scalable with respect to the numbers of pipeline stages. Simulation results using real-life applications are presented to demonstrate the effectiveness of our methods. © 2015 ACM.",Energy; Pay-burst-only-once; Periodic power management; Real-time system; Scheduling,Embedded software; Interactive computer systems; Inverse problems; Pipelines; Power management; Quadratic programming; Scheduling; Energy; Energy minimization; Management policy; Multi processor systems; Pay-burst-only-once; Pipelined systems; Real-life applications; Resource demands; Real time systems
Yield Improvement for 3D wafer-to-wafer stacked ICs using wafer matching,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924063329&doi=10.1145%2f2699832&partnerID=40&md5=55f924a4ed043feb58c08b165afd26ee,"Three-Dimensional Stacked IC (3D-SIC) using Through-Silicion Vias (TSVs) is an emerging technology that provides heterogeneous integration, higher performance, and lower power consumption compared to traditional ICs. Stacking 3D-SICs usingWafer-to-Wafer (W2W) has several advantages such as high stacking throughput, high TSV density, and the ability to handle thin wafers and small dies. However, it suffers from low-compound yield as the stacking of good dies on bad dies and vice versa cannot be prevented. This article investigates wafer matching as a means for yield improvement. It first defines a complete wafer matching framework consisting of different scenarios, each a combination of a matching process (defines the order of wafer selection), a matching criterion (defines whether good or bad dies are matched), wafer rotation (defines either wafers are rotated or not), and a repository type. The repository type specifies whether either the repository is filled immediately after each wafer selection (i.e., running repository) or after all wafers are matched (i.e., static repository). A mapping of prior work on the framework shows that existing research has mainly explored scenarios based on static repositories. Therefore, the article analyzes scenarios based on running repositories. Simulation results show that scenarios based on running repositories improve the compound yield with up to 13.4% relative to random W2W stacking; the improvement strongly depends on the number of stacked dies, die yield, repository size, as well as on the used matching process. Moreover, the results reveal that scenarios based on running repositories outperform those of static repositories in terms of yield improvement at significant runtime reduction (three orders of magnitude) and lower memory complexity (from exponential to linear in terms of stack size). © 2015 ACM.",3D integration; Matching criterion; Wafer matching,Dies; Timing circuits; 3-D integration; Emerging technologies; Heterogeneous integration; Lower-power consumption; Matching criterion; Memory complexity; Three orders of magnitude; Wafer matching; Three dimensional integrated circuits
Reconfigurable binding against FPGA replay attacks,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924029560&doi=10.1145%2f2699833&partnerID=40&md5=cd26e3994182fd33d0542dab6daf77b2,"The FPGA replay attack, where an attacker downgrades an FPGA-based system to the previous version with known vulnerabilities, has become a serious security and privacy concern for FPGA design. Current FPGA intellectual property (IP) protection mechanisms target the protection of FPGA configuration bitstreams by watermarking or encryption or binding. However, these mechanisms fail to prevent replay attacks. In this article, based on a recently reported PUF-FSM binding method that protects the usage of configuration bitstreams, we propose to reconfigure both the physical unclonable functions (PUFs) and the locking scheme of the finite state machine (FSM) in order to defeat the replay attack. We analyze the proposed scheme and demonstrate how replay attack would fail in attacking systems protected by the reconfigurable binding method. We implement two ways to build reconfigurable PUFs and propose two practical methods to reconfigure the locking scheme. Experimental results show that the two reconfigurable PUFs can generate significantly distinct responses with average reconfigurability of more than 40%. The reconfigurable locking schemes only incur a timing overhead less than 1%. © 2015 ACM.",Binding; Field-programmable gate array (FPGA); Intellectual property (IP) protection; Physical unclonable functions (PUFs); Replay attacks,Binary sequences; Cryptography; Hardware security; Intellectual property; Locks (fasteners); Reconfigurable hardware; Binding; Binding methods; FPGA configuration; Intellectual property protection; Practical method; Reconfigurability; Replay attack; Security and privacy; Field programmable gate arrays (FPGA)
The Design and experiments of a SID-based power-aware simulator for embedded multicore systems,2015,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924019804&doi=10.1145%2f2699834&partnerID=40&md5=430ca1d7a1d8840b0883ff8d8f020e74,"Embedded multicore systems are playing increasingly important roles in the design of consumer electronics. The objective of such systems is to optimize both performance and power characteristics of mobile devices. However, currently there are no power metrics supporting popular application design platforms (such as SID) that application developers use to develop their applications. This hinders the ability of application developers to optimize power consumption. In this article we present the design and experiments of a SIDbased power-aware simulation framework for embedded multicore systems. The proposed power estimation flow includes two phases: IP-level power modeling and power-aware system simulation. The first phase employs PowerMixerIP to construct the power model for the processor IP and other major IPs, while the second phase involves a power abstract interpretation method for summarizing the simulation trace, then, with a CPE module, estimating the power consumption based on the summarized trace information and the input of IP power models. In addition, a Manager component is devised to map each digital signal processor (DSP) component to a host thread and maintain the access to shared resources. The aim is to maintain the simulation performance as the number of simulated DSP components increases. A power-profiling API is also supported that developers of embedded software can use to tune the granularity of power-profiling for a specific code section of the target application. We demonstrate via case studies and experiments how application developers can use our SID-based power simulator for optimizing the power consumption of their applications. We characterize the power consumption of DSP applications with the DSPstone benchmark and discuss how compiler optimization levels with SIMD intrinsics influence the performance and power consumption. A histogram application and an augmented-reality application based on human-face-based RMS(recognition, mining, and synthesis) application are deployed as running examples on multicore systems to demonstrate how our power simulator can be used by developers in the optimization process to illustrate different views of power dissipations of applications. © 2015 ACM.",Dsp; Embedded processor; Multicore simulation; Power modeling,Abstracting; Application programming interfaces (API); Application programs; Augmented reality; Digital signal processing; Digital signal processors; Electric power utilization; Embedded systems; Integrated circuit design; Power management; Simulators; Abstract interpretations; Augmented reality applications; Compiler optimizations; Digital signal processors (DSP); Embedded processors; Multicore simulations; Power model; Simulation performance; Benchmarking
Multiplierless design of folded DSP blocks,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914706555&doi=10.1145%2f2663343&partnerID=40&md5=9414e52807ef27ec14aee84dec76bdeb,"This article addresses the problem of minimizing the implementation cost of the time-multiplexed constant multiplication (TMCM) operation that realizes the multiplication of an input variable by a single constant selected from a set of multiple constants at a time. It presents an efficient algorithm, called ORPHEUS, that finds a multiplierless TMCM design by sharing logic operators, namely adders, subtractors, adders/subtractors, and multiplexors (MUXes). Moreover, this article introduces folded design architectures for the digital signal processing (DSP) blocks, such as finite impulse response (FIR) filters and linear DSP transforms, and describes how these folded DSP blocks can be efficiently realized using TMCM operations optimized by ORPHEUS. Experimental results indicate that ORPHEUS can find better solutions than existing TMCM algorithms, yielding TMCM designs requiring less area. They also show that the folded architectures lead to alternative designs with significantly less area, but incurring an increase in latency and energy consumption, compared to the parallel architecture. © 2014 ACM.",Algorithms; Area optimization; Design; Discrete cosine transforms; Finite impulse response filter; Folded architecture; Integer cosine transforms; Multiplierless design; Time-multiplexed constant multiplication,Adders; Algorithms; Cosine transforms; Design; Discrete cosine transforms; Energy utilization; FIR filters; Impulse response; Integer programming; Parallel architectures; Time division multiplexing; Area optimization; Constant multiplication; Folded architectures; Integer cosine transform; Multiplierless design; Digital signal processing
Compiler optimization for reducing leakage power in multithread BSP programs,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914695692&doi=10.1145%2f2668119&partnerID=40&md5=81cef0c89ccd65ed2cb511a246967fd5,"Multithread programming is widely adopted in novel embedded system applications due to its high performance and flexibility. This article addresses compiler optimization for reducing the power consumption of multithread programs. A traditional compiler employs energy management techniques that analyze component usage in control-flow graphs with a focus on single-thread programs. In this environment the leakage power can be controlled by inserting on and off instructions based on component usage information generated by flow equations. However, these methods cannot be directly extended to a multithread environment due to concurrent execution issues. This article presents a multithread power-gating framework composed of multithread power-gating analysis (MTPGA) and predicated power-gating (PPG) energy management mechanisms for reducing the leakage power when executing multithread programs on simultaneous multithreading (SMT) machines. Our multithread programming model is based on hierarchical bulk-synchronous parallel (BSP) models. Based on a multithread component analysis with dataflow equations, our MTPGA framework estimates the energy usage of multithread programs and inserts PPG operations as power controls for energy management. We performed experiments by incorporating our power optimization framework into SUIF compiler tools and by simulating the energy consumption with a post-estimated SMT simulator based on Wattch toolkits. The experimental results show that the total energy consumption of a system with PPG support and our power optimization method is reduced by an average of 10.09% for BSP programs relative to a system without a power-gating mechanism on leakage contribution set to 30%; and the total energy consumption is reduced by an average of 4.27% on leakage contribution set to 10%. The results demonstrate our mechanisms are effective in reducing the leakage energy of BSP multithread programs. © 2014 ACM.",Compilers for low power; Leakage power reduction; Multithreading; Power-gating mechanisms,Computer systems programming; Data flow analysis; Energy management; Energy utilization; Flow graphs; Leakage currents; Power control; Program compilers; Bulk synchronous parallel models; Compilers for low power; Embedded system applications; Leakage power reduction; Multi-threading; Power-gating mechanisms; Simultaneous multi-threading; Total energy consumption; Concurrency control
SmartCap: Using machine learning for power adaptation of smartphone's application processor,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914689692&doi=10.1145%2f2651402&partnerID=40&md5=e5a038d91270e2eda8833a449e5a46a2,"Power efficiency is increasingly critical to battery-powered smartphones. Given that the using experience is most valued by the user, we propose that the power optimization should directly respect the user experience. We conduct a statistical sample survey and study the correlation among the user experience, system runtime activities, and computational performance of an application processor. We find that there exists a minimal frequency requirement, called ""saturated frequency"". Above this frequency, the device consumes more power but provides little improvements in user experience. This studymotivates an intelligent self-adaptive scheme, SmartCap, that automatically identifies the most power-efficient state of the application processor. Compared to prior Linux power adaptation schemes, SmartCap can help save power from 11% to 84%, depending on applications, with little decline in user experience. © 2014 ACM.",Algorithms; Design; Low power; Machine learning; Performance; Smartphone; User experience,Algorithms; Computer operating systems; Design; Learning systems; Machine learning; Smartphones; Application processors; Computational performance; Low Power; Performance; Power adaptation; Power efficiency; Power Optimization; Statistical samples; User experience
A formal approach to incremental converter synthesis for system-on-chip design,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914696639&doi=10.1145%2f2663344&partnerID=40&md5=dfac1e0a4deaee82d31e30f93651e5c6,"A system-on-chip (SoC) contains numerous intellectual property blocks, or IPs. Protocol mismatches between IPs may affect the system-level functionality of the SoC. Mismatches are addressed by introducing converters to control inter-IP interactions. Current approaches towards converter generation find limited practical application as they use restrictive models, lack formal rigour, handle a small subset of commonly encountered mismatches, and/or are not scalable. We propose a formal technique for SoC design using incremental converter synthesis. The proposed formulation provides precise models for protocols and requirements, and provides a scalable algorithm that allows adding multiple components and requirements to an SoC incrementally. We prove that the technique is sound and complete. Experimental results obtained using real-life AMBA benchmarks show the scalability and wide range of mismatches handled by our approach. © 2014 ACM.",Algorithms; Correct-by-construction design; Design; Performance; Protocol conversion; System-on-a-chip; Two-player games; Verification,Algorithms; Application specific integrated circuits; Design; Game theory; Programmable logic controllers; System-on-chip; Verification; Correct-by-construction design; Performance; Protocol conversion; System on a chip; Two-player games; Integrated circuit design
Synthesizing optimal switching lattices,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914689712&doi=10.1145%2f2661632&partnerID=40&md5=2d101c34923252179461ccda3783174f,"The use of nanoscale technologies to create electronic devices has revived interest in the use of regular structures for defining complex logic functions. One such structure is the switching lattice, a two-dimensional lattice of four-terminal switches. We show how to directly construct switching lattices of polynomial size from arbitrary logic functions; we also show how to synthesize minimal-sized lattices by translating the problem to the satisfiability problem for a restricted class of quantified Boolean formulas. The synthesis method is an anytime algorithm that uses modern SAT solving technology and dichotomic search. It improves considerably on an earlier proposal for creating switching lattices for arbitrary logic functions. © 2014 ACM.",Logic synthesis; SAT; Switching lattice,Boolean algebra; Logic Synthesis; Nanotechnology; Switching; Any-time algorithms; Electronic device; Nanoscale technologies; Optimal switching; Quantified Boolean formulas; Regular structure; Satisfiability problems; Two-dimensional lattices; Computer circuits
Hybrid cache designs for reliable hybrid high and ultra-low voltage operation,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914680421&doi=10.1145%2f2658988&partnerID=40&md5=e4babb8eff0fc6ebeadc05b7e650191c,"Geometry scaling of semiconductor devices enables the design of ultra-low-cost (e.g., below 1 USD) battery-powered resource-constrained ubiquitous devices for environment, urban life, and body monitoring. These sensor-based devices require high performance to react in front of infrequent particular events as well as extreme energy efficiency in order to extend battery lifetime during most of the time when low performance is required. In addition, they require real-time guarantees. The most suitable technological solution for these devices consists of using hybrid processors able to operate at: (i) high voltage to provide high performance and (ii) near-/subthreshold voltage to provide ultra-low energy consumption. However, the most efficient SRAM memories for each voltage level differ and trading off different SRAM designs is mandatory. This is particularly true for cache memories, which occupy most of the processor's area. In this article, we propose new, simple, single-Vcc-domain hybrid L1 cache architectures suitable for reliable hybrid high and ultra-low voltage operation. In particular, the cache is designed by combining heterogeneous SRAM cell types: some of the cache ways are optimized to satisfy high-performance requirements during high voltage operation, whereas the rest of the ways provide ultra-low energy consumption and reliability during near-/subthreshold voltage operation. We analyze the performance, energy, and power impact of the proposed cache designs when using them to implement L1 caches in a processor. Experimental results show that our hybrid caches can efficiently and reliably operate across a wide range of voltages, consuming little energy at near-/subthreshold voltage as well as providing high performance at high voltage without decreasing reliability levels to provide strong performance guarantees, as required for our target market. © 2014 ACM.",Cache memories; Design; Embedded real time; Hybrid voltage operation; Low energy; Performance; Reliability,Buffer storage; Design; Electric batteries; Energy efficiency; Energy utilization; Logic design; Reliability; Semiconductor devices; Static random access storage; High-voltage operation; Low energy; Performance; Performance guarantees; Performance requirements; Real time; Technological solution; Voltage operations; Cache memory
Clock tree synthesis considering slew effect on supply voltage variation,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914703477&doi=10.1145%2f2651401&partnerID=40&md5=a8cf75f7d0ef013f0bd4bffbdf34ad45,"This work tackles a problem of clock power minimization within a skew constraint under supply voltage variation. This problem is defined in the ISPD 2010 benchmark. Unlike mesh and cross link that reduce clock skew uncertainty by multiple driving paths, our focus is on controlling skew uncertainty in the structure of the tree. We observe that slow slew amplifies supply voltage variation, which induces larger path delay variation and skew uncertainty. To obtain the optimality, we formulate a symmetric clock tree synthesis as a mathematical programming problem in which the slew effect is considered by an NLDM-like cell delay variation model. A symmetry-to-asymmetry tree transformation is proposed to further reduce wire loading. Experimental results show that the proposed four methods save up to 20% of clock tree capacitance loading. Beyond controlling slew to suppress supply-voltage-variation-induced skew, we also discuss the strategies of clock tree synthesis under variant variation scenarios and the limitations of the ISPD 2010 benchmark. 2014 Copyright is held by the Owner/Author. Publication rights licensed to ACM.",Clock tree optimization; Robust design; Slew; Voltage variation,Optimization; Power; Problem Solving; Synthesis; Trees; Capacitance; Clock distribution networks; Electric clocks; Forestry; Mathematical programming; Cell delay variation; Clock tree optimization; Clock tree synthesis; Mathematical programming problem; Robust designs; Slew; Supply voltage variation; Voltage variation; Trees (mathematics)
Designing hybrid DRAM/PCM main memory systems utilizing dual-phase compression,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914695491&doi=10.1145%2f2658989&partnerID=40&md5=03a683e85552d755977667593c155d1f,"The last few years have witnessed the emergence of a promising new memory technology, namely Phase-Change Memory (PCM). Due to its inherent ability to scale deeply into the nanoscale regime and its low power consumption, PCM is increasingly viewed as an attractive alternative for the memory subsystem of future microprocessor architectures. However, PCM is marred by a duo of potentially show-stopping deficiencies, that is, poor write performance (especially when compared to the prevalent and ubiquitous DRAM technology) and limited durability. These weaknesses have urged designers to develop various supporting architectural techniques to aid and complement the operation of the PCM while mitigating its innate flaws. One promising such solution is the deployment of hybridized memory architectures that fuse DRAM and PCM, in order to combine the best attributes of each technology. In this article, we introduce a novel Dual-Phase Compression (DPC) scheme and its architectural design aimed at DRAM/PCM hybrids, which caters to the limitations of PCM technology while optimizing memory performance. The DPC technique is specifically optimized for PCM-based environments and is transparent to the operation of the remaining components of the memory subsystem. Furthermore, the proposed architecture is imbued with a multifaceted wear-leveling technique to enhance the durability and prolong the lifetime of the PCM. Extensive simulations with traces from real applications running on a full-system simulator demonstrate 20.4% performance improvement and 46.9% energy reduction, on average, as compared to a baseline DRAM/PCM hybrid implementation. Additionally, the multifaceted wear-leveling technique is shown to significantly prolong the lifetime of the PCM. © 2014 ACM.",Algorithms; Data compression; Design; Durability; Low power; Performance; Phase-change memory,Algorithms; Data compression; Design; Durability; Dynamic random access storage; Integrated circuit design; Memory architecture; Nanotechnology; Wear of materials; Extensive simulations; Full system simulators; Hybrid implementation; Low Power; Microprocessor architectures; Performance; Phase change memory (pcm); Proposed architectures; Phase change memory
Editorial: ACM transactions on design automation of electronics systems and beyond,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914689253&doi=10.1145%2f2676865&partnerID=40&md5=87ae536a0d92c95297617216448bdc57,[No abstract available],,
Reducing contention in Shared Last-Level Cache for throughput processors,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914695498&doi=10.1145%2f2676550&partnerID=40&md5=fa36126deb30105eca86e2224bd9437a,"Deploying the Shared Last-Level Cache (SLLC) is an effective way to alleviate the memory bottleneck in modern throughput processors, such as GPGPUs. A commonly used scheduling policy of throughput processors is to render the maximum possible thread-level parallelism. However, this greedy policy usually causes serious cache contention on the SLLC and significantly degrades the system performance. It is therefore a critical performance factor that the thread scheduling of a throughput processor performs a careful trade-off between the thread-level parallelism and cache contention. This article characterizes and analyzes the performance impact of cache contention in the SLLC of throughput processors. Based on the analyses and findings of cache contention and its performance pitfalls, this article formally formulates the aggregate working-set-size-constrained thread scheduling problem that constrains the aggregate working-set size on concurrent threads. With a proof to be NP-hard, this article has integrated a series of algorithms to minimize the cache contention and enhance the overall system performance on GPGPUs. The simulation results on NVIDIA's Fermi architecture have shown that the proposed thread scheduling scheme achieves up to 61.6% execution time enhancement over a widely used thread clustering scheme. When compared to the state-of-the-art technique that exploits the data reuse of applications, the improvement on execution time can reach 47.4%. Notably, the execution time improvement of the proposed thread scheduling scheme is only 2.6% from an exhaustive searching scheme. 2014 Copyright is held by the Owner/Author. Publication rights licensed to ACM.",Cache contention; Shared last-level cache; Thread scheduling; Thread-level parallelism; Throughput processors,Aggregates; Economic and social effects; Program processors; Scheduling; Cache contention; Exhaustive searching; Last-level caches; Performance factors; Scheduling policies; State-of-the-art techniques; Thread level parallelism; Thread scheduling; Parallel processing systems
Scaling input stimulus generation through hybrid static and dynamic analysis of RTL,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914706493&doi=10.1145%2f2676549&partnerID=40&md5=8e7a0cdebc5279ccf8172ab922554c9b,"We enhance STAR, an automatic technique for functional input vector generation for design validation. STAR statically analyzes the source code of the Register-Transfer Level (RTL) design. The STAR approach is a hybrid between RTL symbolic execution and concrete simulation that offsets the disadvantages of both. The symbolic execution, which follows the concrete simulation path, extracts constraints for that path. The guard in the path constraints is then mutated and passed to an SMT solver. A satisfiable assignment generates a valid input vector. However, STAR suffers the problem of path explosion during symbolic execution. In this article, we present an explored symbolic state caching method to attack path explosion. Explored symbolic states are states starting from which all subpaths have been explored. Each explored symbolic state is stored in the form of bitmap encoding of branches to ease comparison. When the explored symbolic state is reached again in the following symbolic execution, all subpaths can be pruned. In addition, we use two types of optimizations: (a) dynamic UD chain slicing; and (b) local conflict resolution to improve the running efficiency of STAR. We demonstrate that the results of the enhanced STAR are promising in showing high coverage on benchmark RTL designs, and the runtime of the test generation process is reduced from several hours to less than 20 minutes. © 2014 ACM.",Coverage; Design verification; SMT; Static analysis; Symbolic execution,Model checking; Static analysis; Surface mount technology; Automatic technique; Conflict Resolution; Coverage; Design verification; Register transfer level; Running efficiency; Static and dynamic analysis; Symbolic execution; Stars
Design of hardened embedded systems on multi-FPGA platforms,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914674733&doi=10.1145%2f2676551&partnerID=40&md5=9d6468e3db4fb6c97ec7b026e6d822cd,"The aim of this article is the definition of a reliability-aware methodology for the design of embedded systems on multi-FPGA platforms. The designed system must be able to detect the occurrence of faults globally and autonomously, in order to recover or to mitigate their effects. Two categories of faults are identified, based on their impact on the device elements; (i) recoverable faults, transient problems that can be fixed without causing a lasting effect namely and (ii) nonrecoverable faults, those that cause a permanent problem,making the portion of the fabric unusable. While some aspects can be taken from previous solutions available in literature, several open issues exist. In fact, no complete design methodology handling all the peculiar issues of the considered scenario has been proposed yet, a gap we aim at filling with our work. The final system exposes reliability properties and increases its overall lifetime and availability. © 2014 ACM.",Design; Fault tolerance; Multi-FPGA; Partitioning; Reliability,Availability; Design; Fault tolerance; Field programmable gate arrays (FPGA); Integrated circuit design; Reliability; Design Methodology; Multi-FPGA; Partitioning; Reliability properties; Transient problems; Embedded systems
Gate-level information flow tracking for security lattices,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914674730&doi=10.1145%2f2676548&partnerID=40&md5=4fa82b041bd997b7b6f21240ae33272d,"High-assurance systems found in safety-critical infrastructures are facing steadily increasing cyber threats. These critical systems require rigorous guarantees in information flow security to prevent confidential information from leaking to an unclassified domain and the root of trust from being violated by an untrusted party. To enforce bit-tight information flow control, gate-level information flow tracking (GLIFT) has recently been proposed to precisely measure and manage all digital information flows in the underlying hardware, including implicit flows through hardware-specific timing channels. However, existing work in this realm either restricts to two-level security labels or essentially targets two-input primitive gates and several simple multilevel security lattices. This article provides a general way to expand the GLIFT method for multilevel security. Specifically, it formalizes tracking logic for an arbitrary Boolean gate under finite security lattices, presents a precise tracking logic generation method for eliminating false positives in GLIFT logic created in a constructive manner, and illustrates application scenarios of GLIFT for enforcing multilevel information flow security. Experimental results show various trade-offs in precision and performance of GLIFT logic created using different methods. It also reveals the area and performance overheads that should be expected when expanding GLIFT for multilevel security. © 2014 ACM.",Design; Formal method; Gate-level information flow tracking; Hardware security; High-assurance system; Multilevel security; Security; Security lattice; Verification,Design; Economic and social effects; Fault tolerant computer systems; Formal methods; Hardware security; Safety engineering; Security of data; Verification; Gate level information flow tracking; High assurance systems; Multi-level security; Security; Security lattice; Computer circuits
Dataflow graph partitioning for area-efficient high-level synthesis with systems perspective,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914674698&doi=10.1145%2f2660769&partnerID=40&md5=cb8610231079ea1999daa6ab67691916,"Area efficiency in datapath synthesis is a widely accepted goal of high-level synthesis. Applications represented by their dataflow graphs are synthesized using resource sharing principles to reduce the area. However, existing resource sharing algorithms focus on absolute area reduction and maximal resource sharing. This kind of a design approach leads to constraints on how often, in terms of number of clock cycles, a new set of input data can be fed to an application. It also leads to very large multiplexers in case of very big dataflow graphswith hundreds of nodes. An adaptive dataflow graph partitioning algorithm is proposed that partitions a graph taking into account a user-defined constraint on how often a new set of input data (generally referred to as data initiation interval) is available. At the same time, a resource sharing algorithm is applied to such partitions in order to reduce area. Multiple design points are generated for a given dataflow graph with different area and time measures to enable a designer to make decisions. We demonstrate our graph partitioning algorithm using synthetically generated large dataflow graphs and on some benchmark applications. 2014 Copyright is held by the Owner/Author. Publication rights licensed to ACM.",Automatic synthesis; Computer-aided design; Datapath synthesis; Design-space exploration; FPGA; High-level synthesis,Axial flow; Benchmarking; Computer aided design; Field programmable gate arrays (FPGA); Graph algorithms; Graph theory; High level synthesis; Input output programs; Automatic synthesis; Benchmark applications; Data paths; Design approaches; Design space exploration; Graph partitioning algorithms; Multiple design points; Number of clock cycles; Data flow analysis
"Efficient coverage-driven stimulus generation using simultaneous SAT solving, with application to SystemVerilog",2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914674693&doi=10.1145%2f2651400&partnerID=40&md5=3b2320582796c82a168da6f8f2199de3,"SystemVerilog provides powerful language constructs for verification, and one of them is the covergroup functional coverage model. This model is designed as a complement to assertion verification, that is, it has the advantage of defining cross-coverage over multiple coverage points. In this article, a coverage-driven verification (CDV) approach is formulated as a simultaneous Boolean satisfiability (SAT) problem that is based on covergroups. The coverage bins defined by the functional model are converted into Conjunction Normal Form (CNF) and then solved together by our proposed simultaneous SAT algorithm PLNSAT to generate stimuli for improving coverage. The basic PLNSAT algorithm is then extended in our second proposed algorithm GPLNSAT, which exploits additional information gleaned from the structure of SystemVerilog covergroups. Compared to generating stimuli separately, the simultaneous SAT approaches can share learned knowledge across each coverage target, thus reducing the overall solving time drastically. Experimental results on a UART circuit and the largest ITC benchmark circuits show that the proposed algorithms can achieve 10.8x speedup on average and outperform state-of-the-art techniques in most of the benchmarks. © 2014 ACM.",Algorithms; Covergroup; Formal methods; Simultaneous satisfiability; Verification,Algorithms; Formal logic; Formal methods; Verification; Assertion verification; Boolean satisfiability; Coverage-driven verifications; Covergroup; Functional coverage; Language constructs; Satisfiability; State-of-the-art techniques; Formal verification
An efficient hardware-based higher radix floating point MAC design,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914674750&doi=10.1145%2f2667224&partnerID=40&md5=9084318d64d15267cfed6e95b86156fa,"This article proposes an effective way of implementing a multiply accumulate circuit (MAC) for high-speed floating point arithmetic operations. The real-world applications related to digital signal processing and the like demand high-performance computation with greater accuracy. In general, digital signals are represented as a sequence of signed/unsigned fixed/floating point numbers. The final result of a MAC operation can be computed by feeding the mantissa of the previousMAC result as one of the partial products to a Wallace tree multiplier or Braun multiplier. Thus, the separate accumulation circuit can be avoided by keeping the circuit depth still within the bounds of the Wallace tree multiplier, namely O(log2 n), or Braun multiplier, namely O(n). In this article, three kinds of floating point MACs are proposed. The experimental results show 48.54% of improvement in worst path delay achieved by the proposed floating point MAC using a radix-2 Wallace structure compared with a conventional floating point MAC without a pipeline using a 45nm technology library. The same proposed design gives 39.92% of improvement in worst path delay without a pipeline using a radix-4 Braun structure as compared with a conventional design. In this article, a radix-32 Q32.32-format-based floating point MAC is proposed using a Wallace tree/Braun multiplier. Also this article discusses the msb prediction problem and its solution in floating point arithmetic that is not available in modern fused multiply-add designs. The performance results show comparisons between the proposed floating point MAC with various floating point MAC designs for radix-2,-4,-8, and -16. The proposed design has lesser depth than a conventional floating point MAC as well as a lower area requirement than other ways of floating point MAC implementation, both with/without a pipeline. © 2014 ACM.",Algorithms; Carry-lookahead adder; Carry-save adder; Design; Digital signal processing; Floating point arithmetic; Fused multiply add; High-performance circuits; Multiply-accumulate circuit; Performance; Wallace tree multiplier,Algorithms; Electric Circuits; Pipe Lines; Adders; Algorithms; Design; Digital signal processing; Forestry; Pipelines; Carry look-ahead adder; Carry save adder; Fused multiply-add; High-performance circuits; Multiply accumulate; Performance; Wallace tree; Digital arithmetic
Understanding SRAM stability via bifurcation analysis: Analytical models and scaling trends,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906724312&doi=10.1145%2f2647957&partnerID=40&md5=937a644ccd4cc80c8253a36056905dfb,"In the past decades, aggressive scaling of transistor feature size has been a primary force driving higher Static Random Access Memory (SRAM) integration density. Due to technology scaling, nanometer SRAM designs become increasingly vulnerable to stability challenges. The traditional way of analyzing stability is through the use of Static Noise Margins (SNMs). SNMs are not capable of capturing the key nonlinear dynamics associated with memory operations, leading to imprecise characterization of stability. This work rigorously develops dynamic stability concepts and, more importantly, captures them in physically based analytical models. By leveraging nonlinear stability theory, we develop analytical models that characterize the minimum required amplitude and duration of injected current noises that can flip the SRAM state. These models, which are parameterized in key design, technology, and operating condition parameters, provide important design insights and offer a basis for predicting scaling trends of SRAM dynamic stability. © 2014 ACM.",Analytical model; Critical current; Critical time; Dynamic noise margin; Static noise margin; Static random access memory; Voltage transfer curves,Analytical models; Bifurcation (mathematics); Critical currents; Logic design; Stability; Critical time; Dynamic noise margins; Static noise margin; Static random access memory; Voltage transfer curves; Static random access storage
High-level test synthesis: A survey from synthesis process flow perspective,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906767423&doi=10.1145%2f2627754&partnerID=40&md5=23ac60e84d51370c302e7b262b36166b,High-level test synthesis is a special class of high-level synthesis having testability as one of the important components. This article presents a detailed survey on recent developments in high-level test synthesis from a synthesis process flow perspective. It also presents a survey on controller synthesis techniques for testability. © 2014 ACM.,Allocation; Behavioral modification; Controller synthesis; High-level test synthesis; Partitioning; Scheduling,Controllers; Scheduling; Allocation; Behavioral modifications; Controller synthesis; Partitioning; Test synthesis; Surveys
A hybrid technique for discrete gate sizing based on lagrangian relaxation,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906766621&doi=10.1145%2f2647956&partnerID=40&md5=f3cb267b5f3cff9608a2de1f911b4ddd,"Discrete gate sizing has attracted a lot of attention recently as the EDA industry faces the challenge of optimizing large standard cell-based circuits. The discrete nature of the problem, along with complex timing models, stringent design constraints, and ever-increasing circuit sizes, make the problem very difficult to tackle. Lagrangian Relaxation (LR) is an effective technique to handle complex constrained optimization problems and therefore has been successfully applied to solve the gate sizing problem. This article proposes an improved Lagrangian relaxation formulation for discrete gate sizing that relaxes timing, maximum gate input slew, and maximum gate output capacitance constraints. Based on such formulation, we propose a hybrid technique composed of three steps. First, a topological greedy heuristic solves the LR formulation. Such a heuristic is applied assuming a slightly increased target clock period (backoff factor) to better explore the solution space. Second, a delay recovery heuristic reestablishes the original target clock with small power overhead. Third, a power recovery heuristic explores the remaining slacks to further reduce power. Experiments on the ISPD 2012 Contest benchmarks show that our hybrid technique provides less leakage power than the state-of-the-Art work for every circuit from the ISPD 2012 Contest infrastructure, achieving up to 24% less leakage. In addition, our technique achieves a much better compromise between leakage reduction and runtime, obtaining, on average, 9% less leakage power while running 8.8 times faster. © 2014 ACM.",Discrete gate sizing; Electronic design automation (EDA); Greedy heuristic; Hybrid technique; Lagrangian relaxation; Leakage power,Clocks; Constrained optimization; Lagrange multipliers; Timing circuits; Electronic design automation; Gate sizing; Greedy heuristics; Hybrid techniques; LaGrangian relaxation; Leakage power; Gates (transistor)
Power and area efficiency NoC router design for application-specific SoC by using buffer merging and resource sharing,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906724233&doi=10.1145%2f2633604&partnerID=40&md5=d835dbeb7754baab3f37fb351bde6c88,"Network-on-Chip (NoC) is an efficient on-chip communication architecture specifically for System-on-A-Chip (SoC) design. However, the input buffers of a NoC router often take a significant portion of the silicon area and power consumption. Besides, the performance of a NoC is also greatly affected by the buffer size. In this article, a static buffer merging and resource sharing method is proposed for the application-specific SoC minimizing the NoC buffer. When given an application-specific task graph and the dataflow distribution, the proposed method statically merges rarely used buffers and generates the suitable number of input buffers for each router at design timely. The merged buffer is shared by several input directions. The experimental result shows that the buffer can be utilized more effectively after the resource sharing. Based on the synthesized design with TSMC 90nm technology, the proposed method reduces an average of 42.23% area and 35.13% power while providing similar performance. © 2014 ACM.",Area efficiency; Buffer structure; Low power; Network-on-Chip,Application specific integrated circuits; Design; Low power electronics; Merging; Routers; Servers; VLSI circuits; 90nm technologies; Area efficiency; Buffer structures; Low Power; Network-on-chip(NoC); On chip communication; Resource sharing methods; System-on-a-chip designs; Network-on-chip
Wavesync: Low-latency source-synchronous bypass network-on-chip architecture,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906735247&doi=10.1145%2f2647950&partnerID=40&md5=723e2b859905af340abfccfd9a2e8379,"WaveSync is a network-on-chip architecture for a globally asynchronous locally-synchronous (GALS) design. The WaveSync design facilitates low-latency communication leveraging the source-synchronous clock sent along with the data to time components in the datapath of a downstream router, reducing the number of synchronizations needed. WaveSync accomplishes this by partitioning the router components at each node into different clock domains, each synchronized with one of the orthogonal incoming source-synchronous clocks in a GALS 2D mesh network. The data and clock subsequently propagate through each node/router synchronously until the destination is reached, regardless of the number of hops thismay take. As long as the data travels in the path of clock propagation and no congestion is encountered, it will be propagated without latching as if in a long combinatorial path, with both the clock and the data accruing delay at the same rate. The result is that the need for synchronization between the mesochronous nodes and/or the asynchronous control associated with the typical GALS network is completely eliminated. To further reduce the latency overhead of synchronization, for those occasions when synchronization is still required (when a flit takes a turn or arrives at the destination), we propose a novel less-than-one-cycle synchronizer. The proposed WaveSync network outperforms conventional GALS networks by 87-90% in average latency, synthesized using a 45nm CMOS library. © 2014 ACM.",Bypass routing; Globally asynchronous locally synchronous; Half-cycle synchronizer; Low-latency communication; Network-on-chip; Router design; Source synchronous; Synchronization,Computer architecture; Electric clocks; Network-on-chip; Synchronization; VLSI circuits; Bypass routing; Globally asynchronous locally synchronous; Half-cycle synchronizer; Low-latency communication; Router design; Source synchronous; Routers
Scalable power management using multilevel reinforcement learning for multiprocessors,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906724519&doi=10.1145%2f2629486&partnerID=40&md5=b7c693d02e8507b49e671da3ebd970eb,"Dynamic power management has become an imperative design factor to attain the energy efficiency in modern systems. Among various power management schemes, learning-based policies that are adaptive to different environments and applications have demonstrated superior performance to other approaches. However, they suffer the scalability problem for multiprocessors due to the increasing number of cores in a system. In this article, we propose a scalable and effective online policy called MultiLevel Reinforcement Learning (MLRL). By exploiting the hierarchical paradigm, the time complexity of MLRL is O(nlg n) for n cores and the convergence rate is greatly raised by compressing redundant searching space. Some advanced techniques, such as the function approximation and the action selection scheme, are included to enhance the generality and stability of the proposed policy. By simulating on the SPLASH-2 benchmarks, MLRL runs 53% faster and outperforms the state-of-the-Art work with 13.6% energy saving and 2.7% latency penalty on average. The generality and the scalability of MLRL are also validated through extensive simulations. © 2014 ACM.",Dynamic power management; Multiprocessors; Reinforcement learning,Energy efficiency; Multiprocessing systems; Reinforcement learning; Scalability; Convergence rates; Dynamic power management; Extensive simulations; Function approximation; Multiprocessors; Power management scheme; Redundant searching; Scalability problems; Power management
Multilevel simulation of nonfunctional properties by piecewise evaluation,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906776858&doi=10.1145%2f2647955&partnerID=40&md5=1cdd1f2c2a48ec853b1a230cac3279f3,"As the technology shrinks, nonfunctional properties (NFPs) such as reliability, vulnerability, power consumption, or heat dissipation become as important as system functionality. As NFPs often influence each other, depend on the application and workload of a system, and exhibit nonlinear behavior, NFP simulation over long periods of system operation is computationally expensive, if feasible at all. This article presents a piecewise evaluation method for efficient NFP simulation. Simulation time is divided into intervals called evaluation windows, within which the NFP models are partially linearized. High-speed functional system simulation is achieved by parallel execution of models at different levels of abstraction. A trade-off between simulation speed and accuracy is met by adjusting the size of the evaluation window. As an example, the piecewise evaluation technique is applied to analyze aging caused by two mechanisms, namely Negative Bias Temperature Instability (NBTI) and Hot Carrier Injection (HCI), in order to identify reliability hotspots. Experiments show that the proposed technique yields considerable simulation speedup at a marginal loss of accuracy. © 2014 ACM.",Aging analysis; Hot carrier injection (HCI); Multilevel simulation; Negative bias temperature instability (NBTI); Nonfunctional properties; Parallel simulation; Transaction-level modeling (TLM),Hot carriers; Integrated circuits; Negative temperature coefficient; Thermodynamic stability; Hot carrier injection; Multilevel simulation; Negative bias temperature instability; Non functional properties; Parallel simulations; Transaction level modeling; Computer simulation
Implementation and analysis of history-based output channel selection strategies for adaptive routers in mesh NoCs,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906734695&doi=10.1145%2f2647952&partnerID=40&md5=6c00c08c166a988c7dd9f61735ca9d4d,"The efficiency and effectiveness of an adaptive router in an NoC-based multicore system is evaluated by the performance it achieves under varying inter-core communication traffic. A well-designed selection strategy plays an important role in an adaptive router to act upon dynamic traffic variations. The effectiveness of a selection strategy depends on what metric is used to represent congestion, how precisely this metric captures the actual congestion, and how much cost is involved in capturing the congestion on a real-time scale. Congestion is formed over a period of time due to cumulative and chain reaction effects. We propose novel history-based selection strategies that could be used with any adaptive, deadlock-free,minimal routing in mesh NoCs. Buffer occupancy time and rate of flit flow across reachable ports of neighboring routers in the recent past are captured, propagated, and maintained in a cost-effective way to compute the selection metric. Experimental results on real and synthetic workloads show that our proposed selection strategies significantly outperform state-of-the-Art techniques. © 2014 ACM.",Adaptive routing; Congestion management; Load balancing; Selection strategy,Resource allocation; Routers; Adaptive routing; Buffer occupancy; Congestion management; Inter-core communications; Multi-core systems; Selection strategy; State-of-the-art techniques; Synthetic workloads; Network-on-chip
Quantifying notions of extensibility in flexray schedule synthesis,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906761538&doi=10.1145%2f2647954&partnerID=40&md5=c69cf0e8acf10b3e67c63d77b104fc76,"FlexRay has now become a well-established in-vehicle communication bus at most original equipment manufacturers (OEMs) such asBMW, Audi, and GM. Given the increasing cost of verification and the high degree of crosslinking between components in automotive architectures, an incremental design process is commonly followed. In order to incorporate FlexRay-based designs in such a process, the resulting schedules must be extensible, that is: (i) when messages are added in later iterations, they must preserve deadline guarantees of already scheduled messages, and (ii) they must accommodate as many new messages as possible without changes to existing schedules. Apart from extensible scheduling having not received much attention so far, traditional metrics used for quantifying them cannot be trivially adapted to FlexRay schedules. This is because they do not exploit specific properties of the FlexRay protocol. In this article we, for the first time, introduce new notions of extensibility for FlexRay that capture all the protocol-specific properties. In particular, we focus on the dynamic segment of FlexRay and we present a number of metrics to quantify extensible schedules. Based on the introducedmetrics, we propose strategies to synthesize extensible schedules and compare the results of different scheduling algorithms. We demonstrate the applicability of the results with industrial-size case studies and also show that the proposed metrics may also be visually represented, thereby allowing for easy interpretation. © 2014 ACM.",Automotive; Extensibility; FlexRay; Schedule synthesis,Scheduling; Automotive; Degree of cross-linking; Extensibility; Flexray; In-vehicle communication; Incremental design process; Original equipment manufacturers; Specific properties; Scheduling algorithms
Statistical peak temperature prediction and thermal yield improvement for 3D chip multiprocessors,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906737572&doi=10.1145%2f2633606&partnerID=40&md5=85d0861ef9b1a8b4dfff1a2976e05fac,"Thermal issues have become critical roadblocks for achieving highly reliable three-dimensional (3D) integrated circuits (ICs). The presence of process variations further exacerbates these problems. In this article, we propose techniques for the efficient evaluation and mitigation of the impact of leakage power variations on the temperature profile of 3D Chip Multiprocessors (CMPs). Experimental results demonstrate that, due to the impact of process variations, a 4-tier 3D implementation can be more than 40?C hotter and 23% leakier than its 2D counterpart. To determine the maximum temperature of each fabricated 3D IC, we propose an accurate learning-based model for peak temperature prediction. Based on the learning model, we then propose two post-fabrication techniques to increase the thermal yield of 3D CMPs: (1) tier restacking and (2) thermally-Aware die matching. Experimental results show that: (1) the proposed prediction model achieves more than 98% accuracy, and (2) the proposed thermally-Aware, post-fabrication optimization techniques significantly improve the thermal yield from only 51% to 99% for 3D CMPs. © 2014 ACM.",3D IC; Chip multiprocessor; Leakage power; Prediction; Process variation; Statistical model; Temperature; Thermal hotspot,Forecasting; Temperature; Three dimensional integrated circuits; Chip Multiprocessor; Hot spot; Leakage power; Process Variation; Statistical modeling; Microprocessor chips
Exact logic and fault simulation in presence of unknowns dominik erb,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902950650&doi=10.1145%2f2611760&partnerID=40&md5=a059393b779622f98f107c7b31240b5e,"Logic and fault simulation are essential techniques in electronic design automation. The accuracy of standard simulation algorithms is compromised by unknown or X-values. This results in a pessimistic overestimation of X-valued signals in the circuit and a pessimistic underestimation of fault coverage. This work proposes efficient algorithms for combinational and sequential logic as well as for stuck-at and transition-delay fault simulation that are free of any simulation pessimism in presence of unknowns. The SAT-based algorithms exactly classifiy all signal states. During fault simulation, each fault is accurately classified as either undetected, definitely detected, or possibly detected. The pessimism with respect to unknowns present in classic algorithms is thoroughly investigated in the experimental results on benchmark circuits. The applicability of the proposed algorithms is demonstrated on larger industrial circuits. The results show that, by accurate analysis, the number of detected faults can be significantly increased without increasing the test-set size.",Exact fault simulation; Exact logic simulation; SAT; Simulation pessimism; Unknown values,Automation; Computer applications; Fault simulation; Logic simulations; SAT; Simulation pessimism; Unknown values; Algorithms
Integrated resource allocation and binding in clock mesh synthesis,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902967493&doi=10.1145%2f2611762&partnerID=40&md5=ede43804c9b3546689bdb3a9f95b8066,"The clock distribution network in a synchronous digital circuit delivers a clock signal to every storage element, that is, clock sink in the circuit. However, since the continued technology scaling increases PVT (processvoltage-temperature) variation, the increase of clock-skew variation is highly likely to cause performance degradation or system failure at runtime. Recently, to mitigate the clock-skew variation, many researchers have taken a profound interest in the clock mesh network. However, though the structure of the clock mesh network is excellent in tolerating timing variations, it demands significantly high power consumption due to the use of excessive mesh wire and buffer resources. Thus, optimizing the resources required in the mesh clock synthesis while maintaining the variation tolerance is crucially important. The three major tasks that greatly affect the cost of the resulting clock mesh are: (1) mesh segment allocation, (2) mesh buffer allocation and sizing, and (3) clock sink binding to mesh segments. Previous clock mesh optimization approaches solve the three tasks sequentially, one by one at a time, to manage the runtime complexity of the tasks at the expense of losing the quality of results. However, since the three tasks are tightly interrelated, simultaneously optimizing all three tasks is essential, if the runtime is ever permitted, to synthesize an economical clock mesh network. In this work, we propose an approach that is able to tackle the problem in an integrated fashion by combining the three tasks into an iterative framework of incremental updates and solving them simultaneously to find a globally optimal allocation of mesh resources while taking into account the clock-skew tolerance constraints. The core parts of this work are a precise analysis on the relation among the resource optimization tasks and an establishment of a mechanism for effective and efficient integration of the tasks. In particular, to handle the runtime problem, we propose a set of speedup techniques, that is, modeling the RC circuit for eliminating redundant matrix multiplications, exploiting a sliding-window scheme, and quickly estimating the buffer sizing effect, which are fitted into our context of fast clock-skew estimation in mesh resource optimization as well as an invention of early decision policies. Through extensive experiments with benchmark circuits, it is shown that our proposed clock mesh synthesizer is able to reduce the worst-case clock skew, total mesh wirelength, total size of mesh driving buffers, and total clock mesh power consumption including short-circuit power by 25.0%, 13.2%, 10.9%, and 11.0% on average compared to that produced by the best-known clock mesh synthesis method (MeshWorks), respectively. Categories and Subject Descriptors: B.7.2 [Integrated Circuits]: Design Aids General Terms: Design, Algorithms, Performance, Reliability. © 2014 ACM 1084-4309/2014/06-ART30 $15.00.",Clock mesh; Resource allocation; Resource binding; Synthesis; Variation tolerance,Clock distribution networks; Complex networks; Electric clocks; Iterative methods; Synthesis (chemical); High power consumption; Integrated resources; MAtrix multiplication; Performance degradation; Resource binding; Resource optimization; Tolerance constraints; Variation tolerances; Resource allocation
MAESTRO-holistic actor-oriented modeling of nonfunctional properties and firmware behavior for mpsocs,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902991185&doi=10.1145%2f2594481&partnerID=40&md5=8f2e5fe8a1c868e5540caf3a2cf38846,"Modeling and evaluating nonfunctional properties such as performance, power, and reliability of embedded systems are tasks of utmost importance. In this article, we introduceMAESTRO, a methodology for themodeling and evaluation of nonfunctional properties and embedded firmware of MPSoC architecture components at the Electronic System Level (ESL). In contrast to existing design flows that provide predefined performance models, MAESTRO defines a flexible approach that allows to define virtual prototypes that can be easily customized and extended to evaluate multiple nonfunctional properties of interest at different levels of abstraction. In MAESTRO, a design is composed purely from actor-oriented models. This enables typical ESL features such as automatic design space exploration and synthesizability of HW and SW components, typically missing in very general design flows. Unique to MAESTRO is the separation and coordination of the interaction between application functionality, firmware, and performance models for the evaluation of nonfunctional properties, and their complex interactions within a single Model-of-Computation (MoC). The main advantages of MAESTRO are: (I) Extensible modeling of interdependent nonfunctional properties of heterogeneous MPSoC components; (II) high flexibility to investigate the appropriate trade-off between modeling effort and accuracy of nonfunctional property evaluators; (III) a holistic approach for modeling application functionality as well as firmware affecting the evaluation of nonfunctional properties. Regarding (II), we present a mobile baseband processor platform use-case, executing a GSM paging application. To demonstrate (I) and (III), we present the modeling of a complex ESL processor virtual prototype, running a soft real-time application and equipped with both a power and reliability manager. © 2014 ACM 1084-4309/2014/06-ART23 $15.00.",Firmware; Hardware/software interfaces; Interface definition languages; Nonfunctional properties; Simulation support systems,Coordination reactions; Design; Multiprocessing systems; Paging systems; System-on-chip; Automatic design space explorations; Electronic system level; Hardware/Software interfaces; Interface definition languages; Non functional properties; Reliability of embedded systems; Simulation support systems; Soft real-time applications; Firmware
Incremental analysis of power grids using backward random walks,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902959361&doi=10.1145%2f2611763&partnerID=40&md5=efe62c3feaf0866ff6d2f4df0d1a975d,"Power grid design and analysis is a critical part of modern VLSI chip design and demands tools for accurate modeling and efficient analysis. The process of power grid design is inherently iterative, during which numerous small changes are made to an initial design, either to enhance the design or to fix design constraint violations. Due to the large sizes of power grids inmodern chips, updating the solution for these perturbations can be a computationally intensive task. In this work, we first introduce an accurate modeling methodology for power grids that, contrary to conventional models, can result in asymmetrical equations. Next, we propose an efficient and accurate incremental solver that utilizes the backward random walks to identify the region of influence of the perturbation. The solution of the network is then updated for this significantly smaller region only. The proposed algorithm is capable of handling both symmetrical and asymmetrical power grid equations. Moreover, it can handle consecutive perturbations without any degradation in the quality of the solution. Experimental results show speedups of up to 13× for our incremental solver, as compared to a full resolve of the power grid. Categories and Subject Descriptors: B.8.2 [Performance and Reliability]: Performance Analysis and Design Aids General Terms: Reliability.",Architectures; Digital circuits; Incremental analysis; Low power; Power grid; Random walks; Reliability,Architecture; Design; Digital circuits; Electric power distribution; Iterative methods; Random processes; Reliability; Reliability analysis; Conventional models; Incremental analysis; Low Power; Performance analysis and design aids; Performance and reliabilities; Power grids; Random Walk; Region of influences; Integrated circuit interconnects
Garbage collection for multiversion index in flash-based embedded databases,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902960244&doi=10.1145%2f2611757&partnerID=40&md5=677d94723966a81ee969b98a10dfd4a7,"Recently, flash-based embedded databases have gained their momentum in various control and monitoring systems, such as cyber-physical systems (CPSes). To support the functionality to access the historical data, a multiversion index is adopted to simultaneously maintain multiple versions of data items, as well as their index information. However, maintaining a multiversion index on flash memory incurs considerable performance overheads on garbage collection, which is to reclaim the spaces occupied by the outdated/invalid data items and their index information on flash memory. In this work, we propose an efficient garbage collection strategy to solve the garbage collection issues of flash-based multiversion databases. In particular, a version-trackingmethod is proposed to accelerate the performance on the process on identifying/reclaiming the space of invalid data and their indexes, and a pre-summary method is also designed to solve the cascading update problem that is caused by the write-once nature of flashmemory and is worsened when more versions refer to the same data item. The capability of the proposed strategy is then verified by analytical and experimental studies. © 2014 ACM 1084-4309/2014/06-ART25 $15.00.",Cyber-physical systems (CPSes); Database systems; Flash memory; Garbage collection,Database systems; Flash memory; Refuse collection; Control and monitoring systems; Cyber physical systems (CPSs); Embedded database; Garbage collection; Historical data; Index information; Multi-version; Embedded systems
SPMCloud: Towards the single-chip embedded scratchpad memory-based storage cloud,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902983983&doi=10.1145%2f2611755&partnerID=40&md5=8aec69fed0ce8136c9ac3c07782b7246,"The era of cloud computing on-a-chip is enabled by the aggressive move towards many-core platforms and the rapid adoption of Network-on-Chips. As a result, there is a need for large-scale distributed onchip shared memories that are reliable, low power, and seamlessly manageable. In this work, we propose SPMCloud, a novel scratchpad-memory-based cloud-inspired volatile storage subsystem designed to meet the needs of future-generation many-core platforms. SPMCloud is composed of several concepts, including: (1) a highly scalable data-center-like memory subsystem that exploits two enterprise-network-inspired memory configurations, namely, embedded Network Attached Storage (eNAS) and embedded Storage Area Network (eSAN), and (2) on-demand allocation of reliablememory space through memory virtualization and the use of embedded RAIDs.Our experimental results onMediabench/CHStone benchmarks show that the SPMCloud's fully distributed reliable memory subsystems can achieve 48% energy savings and 70% latency reduction on average over state-of-the-art NoC memory reliability techniques. We then evaluate the scalability of the SPMCloud and compare it with traditional SPM allocation policies. The SPMCloud's dynamic allocator outperforms the best competition by an average 60% (eNAS) and 46% (eSAN) when the platform runs at 250 MHz and by an average 80% (eNAS) and 40% when running at 1 GHz. Moreover, the SPMCloud achieves an average 83% energy savings across all configurations (number of cores) with respect to the best competitors when running at 250 MHz and 1 GHz. We then studied the SPM hit ratio across the various allocation policies discussed in this article and showed that on average the SPMCloud's priority-driven dynamic allocation policy achieves 93.5% SPM hit ratio, 0.6% higher hit ratio than the closest allocation policy. We then showed that the eNAS and eSAN achieve an average of 67.9% and 29% reduction in execution time, respectively, over the best competitor. Similarly, the eNAS and eSAN achieve an average of 82.7% and 82.3% energy savings, respectively, over the best competitor. Furthermore, we evaluated the scalability of the SPMCloud and its performance/energy efficiency when providing support for some of the heavier ERAID levels, and showed that the eNAS/eSAN configurations with SECDED achieve an average of 51.5% and 34.9% reduction in execution time, respectively, over the best competitor with SECDED. Similarly, the eNAS/eSAN configurations with E-RAID Level 1+ SECDED achieve an average of 82.3% and 75.6% energy savings, respectively, over the best competitor. © 2014 ACM 1084-4309/2014/06-ART22 $15.00.",Distributedmemories; Manycore platforms; Network-on-chip; Reliability; Virtualization,Digital storage; Energy conservation; Memory architecture; Reliability; Routers; Scalability; Virtual reality; VLSI circuits; Allocation policies; Distributedmemories; Dynamic allocations; Many-core; Memory configuration; Memory virtualization; On-demand allocations; Virtualizations; Network-on-chip
Integrated coherence prediction: Towards efficient cache coherence on noc-based multicore architectures,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902974306&doi=10.1145%2f2611756&partnerID=40&md5=283d6a35c8514942338a857706f623ce,"Multicore architectures with Network-on-Chips (NoCs) have been widely recognized as the de facto design for the efficient utilization of the continuously increasing density of transistors on a chip. A key challenge in designing such an NoC-based multicore processor is maintaining cache coherence in an efficient manner. Directory-based protocols avoid the bandwidth overhead of snoop-based protocols, therefore scaling to a large number of cores. However, conventional directory structures add significant indirection delay to cache-tocache accesses in larger multicore processor. In this article we propose a novel hardware coherence technique, called integrated coherence prediction (ICP). This approach adopts a prediction technique for managing shared data to reduce or eliminate the cache-to-cache delay in coherence accesses. ICP has two unique features that differ from previous coherence prediction techniques. First, ICP introduces a new integrated prediction scheme that combines two kinds of predictors: owner predictor, which predicts the data writers and avoids the indirection through directory, and data predictor, which predicts the access address and prefetches data from remote nodes directly. Second, ICP uses a request replication method to reduce the negative effect of wrong owner prediction operations, thus facilitating overall performance improvement. We present the design and implementation details of the ICP approach. Using detailed full-system simulations, we conclude that the ICP provides a cost-effective solution for designing high-performance multicore processors. © 2014 ACM 1084-4309/2014/06-ART24 $15.00.",Cache coherence; Multicore; Network-on-chip; Prediction,Cache memory; Forecasting; Microprocessor chips; Routers; Software architecture; VLSI circuits; Cache Coherence; Cost-effective solutions; Design and implementations; Full-system simulation; Integrated prediction; Multi core; Multicore architectures; Prediction techniques; Network-on-chip
An Effective floorplan-Guided placement algorithm for large-Scale mixed-Size designs,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902976955&doi=10.1145%2f2611761&partnerID=40&md5=7dcc66d26665b64d238a839f29a283e0,"In this article we propose an effective algorithm flow to handle modern large-scale mixed-size placement, both with and without geometry constraints. The basic idea is to use floorplanning to guide the placement of objects at the global level. The flow consists of four steps: (1) The objects in the original netlist are clustered into blocks; (2) floorplanning is performed on the blocks; (3) the blocks are shifted within the chip region to further optimize the wirelength; (4) with large macro-locations fixed, incremental placement is applied to place the remaining objects. There are several advantages to handling placement at the global level with a floorplanning technique. First, the problem size can be significantly reduced. Second, exact Half-Perimeter WireLength (HPWL) can be minimized. Third, better object distribution can be achieved so that legalization only needs to handle minor overlaps among small objects in a block. Fourth, macro-rotation and various geometry constraints can be handled. To demonstrate the effectiveness of this new flow, we implement a high-quality and efficient floorplan-guided placer called FLOP. We also construct the Modern Mixed-Size (MMS) placement benchmarks that can effectively represent the complexities of modern mixed-size designs and the challenges faced by modern mixed-size placers. Compared with most state-of-the-art mixed-size placers and leading macroplacers, experimental results show that FLOP achieves the best HPWL and easily obtains legal solutions on all circuits with all geometry constraints satisfied. ©2014 ACM 1084-4309/2014/06-ART26 $15.00.",Floorplanning; Geometry constraint; Mixed-size design; Placement,Algorithms; Design; Placers; Effective algorithms; Floor-planning; Geometry constraints; Incremental placement; Mixed-size designs; Mixed-size placement; Placement; Placement algorithm; Geometry
Diagnosability of component-Composition graphs in the mm* model,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902990922&doi=10.1145%2f2611759&partnerID=40&md5=cd446b6e4a3a15c95983641f5ed2c547,"Diagnosability is an important metric for measuring the reliability of multiprocessor systems. This article adopts the MM* model and outlines the common properties of a wide class of interconnection networks, called component-composition graphs (CCGs), to determine their diagnosability by using their obtained properties. By applying the results to multiprocessor systems, the diagnosability of hypercube-like networks (including hypercubes, crossed cubes, Möbius cubes, twisted cubes, locally twisted cubes, generalized twisted cubes,and recursive circulants), star graphs, pancake graphs, bubble-sort graphs, and burnt pancake graphs, all of which belong to the class of CCGs, can also be computed. ©2014 ACM 1084-4309/2014/06-ART26 $15.00.",Comparison diagnosis model; Component-composition graphs; Diagnosability; Graph theory; MM* model; Multiprocessor systems,Geometry; Graph theory; Hypercube networks; Common property; Comparison diagnosis model; Component-composition graphs; Diagnosability; Hypercube-like networks; Locally twisted cubes; Multi processor systems; Recursive circulants; Graphic methods
Power modeling for GPU architectures using McPAT,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902969245&doi=10.1145%2f2611758&partnerID=40&md5=1ee5db56aebec2597461205236ca22f9,"Graphics Processing Units (GPUs) are very popular for both graphics and general-purpose applications. Since GPUs operate many processing units and manage multiple levels of memory hierarchy, they consume a significant amount of power. Although several power models for CPUs are available, the power consumption of GPUs has not been studied much yet. In this article we develop a new power model for GPUs by utilizing McPAT, a CPU power tool. We generate initial power model data from McPAT with a detailed GPU configuration, and then adjust the models by comparing them with empirical data. We use the NVIDIA's Fermi architecture for building the power model, and our model estimates the GPU power consumption with an average error of 7.7% and 12.8% for the microbenchmarks and Merge benchmarks, respectively. ©2014 ACM 1084-4309/2014/06-ART26 $15.00.",Design space exploration; Fermi architecture; Simulation; Validation,Architecture; Computer graphics; Computer graphics equipment; Design space exploration; Graphics processing units; Memory hierarchy; Micro-benchmarks; Model estimates; Processing units; Simulation; Validation; Program processors
BLAS: Block-level adaptive striping for solid-state drives,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897015705&doi=10.1145%2f2555616&partnerID=40&md5=109ebf629800501d62101a4dc0998899,"Increasing the degree of parallelism and reducing the overhead of garbage collection (GC overhead) are the two keys to enhancing the performance of solid-state drives (SSDs). SSDs employmultichannel architectures, and a data placement scheme in an SSD determines how the data are striped to the channels. Without considering the data access pattern, existing fixed and device-level data placement schemes may have either high GC overhead or poor I/O parallelism, resulting in degraded performance. In this article, an adaptive block-level data placement scheme called BLAS is proposed to maximize the I/O parallelism while simultaneously minimizing the GC overhead. In contrast to existing device-level schemes, BLAS allows different data placement policies for blocks with different access patterns. Pages in read-intensive blocks are scattered over various channels to maximize the degree of read parallelism, while pages in each of the remaining blocks are attempted to be gathered in the same physical block to minimize the GC overhead. Moreover, BLAS allows the placement policy for a logical block to be changed dynamically according to the access pattern changes of that block. Finally, a parallelism-aware write buffer management approach is adopted in BLAS to maximize the degree of write parallelism. Performance results show that BLAS yields a significant improvement in the SSD response time when compared to existing device-level schemes. In particular, BLAS outperforms device-level page striping and device-level block striping by factors of up to 8.75 and 7.41, respectively. Moreover, BLAS achieves low GC overhead and is effective in adapting to workload changes. © 2014 ACM.",Data placement; NAND flash memory; SSD; Write buffer,Automation; Data access patterns; Data placement; Degraded performance; Degree of parallelism; Garbage collection; NAND flash memory; SSD; Write buffer; Computer applications
Accelerating FPGA debug: Increasing visibility using a runtime reconfigurable observation and triggering network,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896989466&doi=10.1145%2f2566668&partnerID=40&md5=1a82e85e1bc9f5d47ca45c873206461a,"FPGA technology is commonly used to prototype new digital designs before entering fabrication. Whilst these physical prototypes can operate many orders of magnitude faster than through a logic simulator, a fundamental limitation is their lack of on-chip visibility when debugging. To counter this, trace-bufferbased instrumentation can be installed into the prototype, allowing designers to capture a predetermined window of signal data during live operation for offline analysis. However, instead of requiring the designer to recompile their entire circuit every time the window is modified, this article proposes that an overlay network is constructed using only spare FPGA routing multiplexers to connect all circuit signals through to the trace instruments. Thus, during debugging, designers would only need to reconfigure this network instead of finding a new place-and-route solution. Furthermore, we describe how this network can deliver signals to both the trigger and trace units of these instruments, which are implemented simultaneously using dual-port RAMs. Our results show that new network configurations connecting any subset of signals to 80-90% of the available RAM capacity can be computed in less than 70 seconds, for a 100,000 LUT circuit, as many times as necessary. Our tool-QuickTrace-is available for download. © 2014 ACM.",FPGA debug; FPGA prototyping; Overlay network; Trace buffers; Verification,Design; Instruments; Overlay networks; Verification; Visibility; Digital designs; FPGA prototyping; FPGA technology; Fundamental limitations; Network configuration; Off-line analysis; Orders of magnitude; Run-time reconfigurable; Reconfigurable hardware
A comparative evaluation of multi-objective exploration algorithms for high-level design,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897007856&doi=10.1145%2f2566669&partnerID=40&md5=fc56210e570dacc7f59034b7be707158,"This article presents a detailed overview and the experimental comparison of 15 multi-objective design-space exploration (DSE) algorithms for high-level design. These algorithms are collected from recent literature and include heuristic, evolutionary, and statistical methods. To provide a fair comparison, the algorithms are classified according to the approach used and examined against a large set of metrics. In particular, the effectiveness of each algorithm was evaluated for the optimization of a multiprocessor platform, considering initial setup effort, rate of convergence, scalability, and quality of the resulting optimization. Our experiments are performed with statistical rigor, using a set of very diverse benchmark applications (a video converter, a parallel compression algorithm, and a fast Fourier transformation algorithm) to take a large spectrum of realistic workloads into account. Our results provide insights on the effort required to apply each algorithm to a target design space, the number of simulations it requires, its accuracy, and its precision. These insights are used to draw guidelines for the choice of DSE algorithms according to the type and size of design space to be optimized. © 2014 ACM.",Computer-aided manufacturing; Design automation; Guidelines; Pareto optimization,Computer aided design; Computer aided manufacturing; Heuristic algorithms; Optimization; Quality control; Space research; Comparative evaluations; Design automations; Design space exploration; Experimental comparison; Fast-Fourier transform algorithm; Guidelines; Multi-processor platforms; Pareto optimization; Evolutionary algorithms
Performance and power profiling for emulated android systems,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896996102&doi=10.1145%2f2566660&partnerID=40&md5=db4c99f60d55cc1b117cfb41a2e5c658,"Simulation is a common approach for assisting system design and optimization. For system-wide optimization, energy and computational resources are often the two most critical issues. Monitoring the energy state of each hardware component and measuring the time spent in each state is needed for accurate energy and performance prediction. For software optimization, it is important to profile the energy and the time consumed by each software construct in a realistic operating environment with a proper workload. However, the conventional approaches of simulation often fail to produce satisfying data. First, building a cycle-accurate simulation environment for a complex system, such as an Android smartphone, is difficult and can take a long time. Second, a slow simulation can significantly alter the behavior of multithreaded, I/O-intensive applications and can affect the accuracy of profiles. Third, existing software-based profilers generally do not work on simulators, which makes it difficult for performance analysis of complicated software, for example, Java applications executed by the Dalvik VM in an Android system. To address these aforementioned problems, we proposed and prototyped a framework, called virtual performance analyzer (VPA). VPA takes advantage of an existing emulator or virtual machine monitor to reduce the complexity of building a simulator. VPA allows the user to selectively and incrementally integrate timing models and power models into the emulator with our carefully designed performance/power monitors, tracing facility, and profiling tools to evaluate and analyze the emulated system. The emulated system can perform at different levels of speed to help verify if the profile data are impacted by the emulation speed. Finally, VPA supports existing software-based profiles and enables non-intrusive tracing/profiling by minimizing the probe effect. Our experimental results show that the VPA framework allows users to quickly establish a performance/power evaluation environment and gather useful information to support system design and software optimization for Android smartphones. © 2014 ACM.",Android system emulation; Full system emulation; Performance profiling; Performance tracing; Power model; Timing estimation,Optimization; Smartphones; Systems analysis; Virtual addresses; Android systems; Performance profiling; Performance tracing; Power model; System emulation; Timing estimation; Robots
Cost-effective lifetime and yield optimization for NoC-based MPSoCs,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897011432&doi=10.1145%2f2535575&partnerID=40&md5=08205938c13dca153010dfee95c9cc3b,"As manufacturing processes scale, designers are increasingly dependent on techniques to mitigate manufacturing defect and permanent failure. In embedded systems-on-chip, system lifetime and yield can be increased using slack-under-utilization in execution and storage resources-so that when components are defective, data and tasks can be remapped and rescheduled. For any given system, the design space of possible slack allocations is both large and complex, consisting of every possible way to replace each component in the initial system with another from the component library. Based on the observation that useful slack is often quantized, we have developed Critical Quantity Slack Allocation (CQSA), an approach that effectively and efficiently allocates execution and storage slack to jointly optimize system yield and cost. While exploring less than 1.4% of the slack allocation design space, our approach consistently outperforms alternative slack allocation techniques to find sets of designs within 1.4% of the lifetime-cost Pareto-optimal front.When applied to yield-cost optimization, our approach again outperforms alternative techniques, exploring less than 1.62% of the design space to find sets of designs within 4.27% of the yield-cost Pareto-optimal front. One advantage of managing failure at the system level is that the same techniques that improve lifetime often also improve yield. As a result, with little modification, CQSA is further able to perform effective joint optimization of lifetime and yield, finding designs within 1.6% of the Pareto-optimal front. © 2014 ACM.",Lifetime optimization; Network-on-chip; Slack allocation; System-level design; Yield optimization,Costs; Digital storage; Pareto principle; VLSI circuits; Component libraries; Manufacturing defects; Manufacturing process; Network on chip; Pareto-optimal front; Slack allocation; System level design; Yield optimization; Design
Techniques for scalable and effective routability evaluation,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896981643&doi=10.1145%2f2566663&partnerID=40&md5=b6a37db09d97ce40050a362818c73a89,"Routing congestion has become a critical layout challenge in nanoscale circuits since it is a critical factor in determining the routability of a design. An unroutable design is not useful even though it closes on all other design metrics. Fast design closure can only be achieved by accurately evaluating whether a design is routable or not early in the design cycle. Lately, it has become common to use a light mode version of a global router to quickly evaluate the routability of a given placement. This approach suffers from three weaknesses: (i) it does not adequately model local routing resources, which can cause incorrect routability predictions that are only detected late, during detailed routing; (ii) the congestion maps obtained by it tend to have isolated hotspots surrounded by noncongested spots, called noisy hotspots, which further affects the accuracy in routability evaluation; and (iii) the metrics used to represent congestion may yield numbers that do not provide sufficient intuition to the designer, and moreover, they may often fail to predict the routability accurately. This article presents solutions to these issues. First, we propose three approaches to model local routing resources. Second, we propose a smoothing technique to reduce the number of noisy hotspots and obtain a more accurate routability evaluation result. Finally, we develop a new metric which represents congestion maps with higher fidelity. We apply the proposed techniques to several industrial circuits and demonstrate that one can better predict and evaluate design routability and that congestion mitigation tools can perform much better to improve the design routability. © 2014 ACM.",Congestion metric; Local resource modeling; Physical design; Routability evaluation; Routing,Integrated circuits; Models; Network routing; Routers; Congestion metric; Local resources; Physical design; Routability; Routing; Design
Critical-path-aware high-level synthesis with distributed controller for fast timing closure,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896982971&doi=10.1145%2f2566670&partnerID=40&md5=d08c94e011e14eaeabd76d0c8f9e227b,"Centralized controllers commonly used in high-level synthesis often require long wires and cause high load capacitance, and that is why critical paths typically occur on paths from controllers to data registers instead of paths from data registers to data registers. However, conventional high-level synthesis has focused on delays within a datapath, making it difficult to solve the timing closure problem during physical synthesis. This article presents hardware architecture with a distributed controller, which makes the timing closure problem much easier. A novel critical-path-aware high-level synthesis flow is also presented for synthesizing such hardware through datapath partitioning, register binding, and controller optimization. We explore the design space related to the number of partitions, which is an important design parameter for target architecture. According to our experiments, the proposed approach reduces the critical path delay excluding FUs by 29.3% and that including FUs by 10.0%, with 2.2% area overhead on average compared to centralized controller architecture. © 2014 ACM.",Controller optimization; Distributed controller architecture; High-level synthesis; Register binding,Architecture; Hardware; Integrated control; Optimization; Centralized controllers; Controller optimization; Critical path delays; Distributed controller; Hardware architecture; High Level Synthesis; Register binding; Target architectures; Controllers
Performance-driven dynamic thermal management of MPSoC based on task rescheduling,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896952866&doi=10.1145%2f2566661&partnerID=40&md5=b1f3fc03733812a5c2237177d906b067,"High level of integration has led to the advent of Multiprocessor System-on-Chip (MPSoC) which consists of multiple processor cores and accelerators on the same die. A MPSoC programming model is based on a task graph where tasks are assigned to cores to maximize performance. To address thermal hotspots in MPSoCs, coarse-grain power management techniques based on Dynamic Frequency Scaling (DFS) are widely used. DFS is reactive in nature and has detrimental effects on performance. We propose an alternative solution based on dynamic task rescheduling where a temperature prediction scheme is built into the scheduler. The temperature look-ahead scheme is used for task reassignment or delay insertion in scheduling. Since temperature prediction and task assignment are done at runtime, both must be simple and extremely fast. To that end, we propose a heuristic solution based on a limited branch-and-bound search and compare results against an optimal Integer Linear Programming (ILP)-based solution. The proposed approach is shown to be superior to frequency scaling, and the resulting schedule length is within 5% to 10% of the optimal solution as obtained from ILP formulation. © 2014 ACM.",Multiprocessor system on chip; Runtime task scheduling; Scheduling; Task graph; Temperature prediction; Thermal management; Wavelet transform,Forecasting; Integer programming; Multiprocessing systems; Scheduling; Temperature control; Wavelet transforms; Alternative solutions; Dynamic thermal management; Integer Linear Programming; Multiprocessor system on chips; Power management techniques; Task graph; Task-scheduling; Temperature prediction; Microprocessor chips
Design-for-testability for multi-cycle broadside tests by holding of state variables,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896908237&doi=10.1145%2f2566665&partnerID=40&md5=cbc88fbb58cc82e93e5e0d83122c666f,"This article describes a design-for-testability approach for increasing the transition fault coverage of multicycle broadside tests. Earlier methods addressed two-cycle tests. The importance of multi-cycle tests results from the ability to produce more compact test sets than possible with two-cycle tests, from the fact that when multi-cycle tests are applied at-speed, they can detect defects that are not detected by two-cycle tests and from their ability to avoid overtesting of delay faults. The approach described in this article is based on holding the values of selected state variables constant during the functional clock cycles of a multi-cycle broadside test. This allows new tests to be produced, which are different from broadside tests, without relying on nonfunctional toggling of state variables as in earlier methods for two-cycle tests. Experimental results show significant improvements in transition fault coverage using a fixed set of hold configurations for two types of multi-cycle broadside test sets: (1) test sets that are stored and applied from an external tester, and (2) functional broadside test sets that are generated using on-chip hardware. © 2014 ACM.",Design-for-testability; Multi-cycle broadside tests; Offline test generation; On-chip test generation; Transition faults,Dynamic random access storage; Broadside tests; Functional broadside tests; Multi cycle tests; Offline test; On-chip tests; State variables; Transition fault coverage; Transition faults; Design for testability
"Reducing test cost of integrated, heterogeneous systems using pass-fail test data analysis",2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896955191&doi=10.1145%2f2566666&partnerID=40&md5=4b12bba6ea10001d8553ccac6ef7220c,"Stringent quality requirements for integrated, heterogeneous systems have led designers and test engineers to mandate large sets of tests to be applied to these systems, which, in turn, have resulted in increased test cost. However, many of these tests are unnecessary (i.e., redundant), since their outcomes can be reliably predicted using results from other applied tests. A methodology for identifying the redundant tests of an integrated, heterogeneous system that has only binary pass-fail test data is described. This methodology uses decision trees, Boolean minimization, and satisfiability as core components. Feasibility is empirically demonstrated using test data from two commercially fabricated systems, namely, a high-speed serializer/deserializer (HSS) and a phase-locked loop (PLL). Our analysis of test data from >38,000 HSS and >22,000 PLL circuits show that 14 out of 40 HSS tests and 11 out of 36 PLL tests are redundant. © 2014 ACM.",Integrated system test; Minimum consistent subset covering; Positive unate product test; Statistical learning; Test compaction,Decision trees; Phase locked loops; Boolean minimization; Consistent subset; Heterogeneous systems; Integrated systems; Phase Locked Loop (PLL); Serializer/deserializer; Statistical learning; Test Compaction; Cost benefit analysis
Configurable range memory for effective data reuse on programmable accelerators,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896931906&doi=10.1145%2f2566662&partnerID=40&md5=0626c03d0f1823565dd9f1f1713ec874,"While programmable accelerators such as application-specific processors and reconfigurable architectures can dramatically speed up compute-intensive kernels of an application, application performance can still be severely limited by the communication between processors. To minimize the communication overhead, a shared memory such as a scratchpad memory may be employed between the main processor and the accelerator coprocessor. However, this setup poses a significant challenge to the main processor, which now must manage data on the scratchpad explicitly, resulting in superfluous data copying due to the inflexibility of a scratchpad. In this article, we present an enhancement of a scratchpad, Configurable Range Memory (CRM), whose address range can be reprogrammed to minimize unnecessary data copying between processors and therefore promote data reuse on the accelerator, and also present a software management algorithm for the CRM. Our experimental results involving detailed simulation of full multimedia applications demonstrate that our CRM architecture can reduce the communication overhead quite effectively, reducing the kernel execution time by up to 28% and the application runtime by up to 12.8%, in addition to considerable system energy reduction, compared to the conventional architecture based on a scratchpad. © 2014 ACM.",Array mapping; Coarse-grained reconfigurable architecture; Compilercontrolled memories; On-chip memory architectures and management; Scratchpads,Communication; Embedded systems; Multiprocessing systems; Reconfigurable architectures; Application performance; Application specific processors; Array mapping; Coarse grained reconfigurable architecture; Communication overheads; Multimedia applications; Scratchpads; Software management; Memory architecture
Low-power skewed-load tests based on functional broadside tests,2014,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896913800&doi=10.1145%2f2566664&partnerID=40&md5=76004d639d65107b1f4088676abb7fe2,"This article studies the generation of low-power skewed-load tests such that the signal transitions (and line values) they create during their fast functional clock cycles match those of functional broadside tests. Functional broadside tests create functional operation conditions during their fast functional clock cycles. As a result, the signal transitions that occur during these clock cycles can also occur during functional operation. The procedure described in this article matches these signal-transitions on a line-by-line basis when generating low-power skewed-load tests. The procedure accepts a functional broadside test set for transition faults. In one of its basic steps, the procedure modifies a functional broadside test into a skewedload test. This allows it to retain many of the signal transitions (and line values) of the functional broadside test in the skewed-load test. Experimental results for benchmark circuits demonstrate the extent to which it is possible to match the signal-transitions of skewed-load tests with those of functional broadside tests while achieving the high transition fault coverage that is typical of skewed-load tests. © 2014 ACM.",Functional broadside tests; Low-power test generation; Skewed-load tests; Switching activity; Transition faults,Clocks; Dynamic random access storage; Functional broadside tests; Low-power test generation; Skewed-load tests; Switching activities; Transition faults; Testing
Near-optimal and scalable intrasignal in-place optimization for non-overlapping and irregular access schemes,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891820420&doi=10.1145%2f2534383&partnerID=40&md5=6e0ac7aa673d773766aa93db2a1d36c0,"Storage-size management techniques aim to reduce the resources required to store elements and to concurrently provide efficient addressing during element accessing. Existing techniques are less appropriate for large iteration spaces with increased numbers of irregularly spread holes. They either have to approximate the accessed regions, leading to overestimation of the final resources, or they require prohibited exploration time to find the storage size. In this work, we present a near-optimal and scalable methodology for storage-size, intrasignal, in-place optimization, that is, to compute the minimum amount of resources required to store the elements of a group (array), for irregular complex access schemes in the target domain of non-overlapping store and load accesses. © 2013 ACM.",Iteration space; Near optimality; Scalability; Storage size,Iterative methods; Scalability; Access schemes; Iteration spaces; Management techniques; Near optimality; Near-optimal; Target domain; Optimization
Optimal common-centroid-based unit capacitor placements for yield enhancement of switched-capacitor circuits,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891790058&doi=10.1145%2f2534394&partnerID=40&md5=d1afeaa3723c31e3ee4571716c832ef8,"Yield is defined as the probability that the circuit under consideration meets with the design specification within the tolerance. Placement with higher correlation coefficients has fewer mismatches and lower variation of capacitor ratio, thus achieving higher yield performance. This study presents a new optimization criterion that quickly determines if the placement is optimal. The optimization criterion leads to the development of the concepts of C-entries and partitioned subarrays which can significantly reduce the searching space for finding the optimal/near-optimal placements on a sufficiently large array size. © 2013 ACM.",Common centroid; Mismatch; Placement optimization; Process variation; Spatial correlation; Variance of ratio; Yield enhancement,Capacitors; Common centroid; Mismatch; Placement optimization; Process Variation; Spatial correlations; Variance of ratio; Yield enhancement; Optimization
Built-in generation of multicycle functional broadside tests with observation points,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891823853&doi=10.1145%2f2534396&partnerID=40&md5=60ba5aa02c67a0b8ec4de86c3c7c3ec0,"Functional broadside tests allow overtesting to be avoided as part of a scheme that considers both test generation and the analysis of output responses, by ensuring that delay faults are detected under functional operation conditions. Compared with two-cycle tests, multicycle tests allow more faults to be detected with each test, thus reducing the number of tests that need to be applied. They also provide an opportunity for nonfunctional electrical effects, which are caused by switching between modes of operation, to subside before the clock cycles where delay faults are detected. Built-in test generation facilitates at-speed testing and reduces the test data volume. Motivated by these observations, this article describes the modification of a built-in test generation method for two-cycle functional broadside tests so as to generate multicycle functional broadside tests. The size of the hardware is not increased by the modification. The article investigates the following issues related to this method: (1) the effect of using multicycle tests on the number of tests that need to be applied; (2) fault simulation for tailoring the test generation hardware to a circuit that takes into account, to different extents, the need to allow nonfunctional electrical effects to subside; (3) the insertion of observation points in order to increase the transition fault coverage. © 2013 ACM.",Built-in test generation; Functional broadside tests; Multicycle tests; Observation points; Transition faults,Hardware; Built-in tests; Functional broadside tests; Multi cycle tests; Observation point; Transition faults; Dynamic random access storage
Low-energy volatile STT-RAM cache design using cache-coherence-enabled adaptive refresh,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891780203&doi=10.1145%2f2534393&partnerID=40&md5=7671d7648ca5a45132aa903cef35d11b,"Spin-Torque Transfer RAM (STT-RAM) is a promising candidate for SRAM replacement because of its excellent features, such as fast read access, high density, low leakage power, and CMOS technology compatibility. However, wide adoption of STT-RAM as cache memories is impeded by its long write latency and high write power. Recent work proposed improving the write performance through relaxing the retention time of STTRAM cells. The resultant volatile STT-RAM needs to be periodically refreshed to prevent data loss. When volatile STT-RAM is applied as the last-level cache (LLC) in chip multiprocessor (CMP) systems, frequent refresh operations could dissipate significant extra energy. In addition, refresh operations could severely conflict with normal read/write operations to degrade overall system performance. Therefore, minimizing the performance impact caused by refresh operations is crucial for the adoption of volatile STT-RAM. In this article, we propose Cache-Coherence-Enabled Adaptive Refresh (CCear) tominimize the number of refresh operations for volatile STT-RAM, adopted as the LLC for CMP systems. Specifically, CCear interacts with cache coherence protocol and cache management policy tominimize the number of refresh operations on volatile STT-RAM caches. Full-system simulation results show that CCear performs close to an ideal refresh policy with low overhead. Compared with state-of-the-art refresh policies, CCear simultaneously improves the system performance and reduces the energy consumption. Moreover, the performance of CCear could be further enhanced using small filter caches to accommodate the not-refreshed private STT-RAM blocks. © 2013 ACM.",Cache coherence; Embedded DRAM; Energy efficiency; Nonvolatile memory; Refresh; Spin-torque transfer RAM,Cache memory; CMOS integrated circuits; Dynamic random access storage; Energy efficiency; Energy utilization; Flash memory; Static random access storage; Cache Coherence; Embedded DRAM; Non-volatile memory; Refresh; Spin torque; Energy policy
CoMETC: Coordinated management of energy/thermal/cooling in servers,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891765255&doi=10.1145%2f2534381&partnerID=40&md5=992ff0b327623fc239ec105602475b2b,"We introduce a CoordinatedManagement of Energy, Thermal, and Cooling (CoMETC) technique tominimize cooling and memory energy of server machines. State-of-the-art solutions decouple the optimization of cooling energy costs and energy consumption of CPU and memory subsystems. This results in suboptimal solutions due to thermal dependencies between CPU and memory and the nonlinearity in energy costs of cooling. In contrast, we develop a unified solution that integrates energy, thermal, and coolingmanagement for CPU and memory subsystems to maximize energy savings. CoMETC reduces the operational energy of the memory by clustering active memory pages to a subset of memory modules while accounting for thermal and cooling aspects. At the same time, CoMETC removes hotspots between and within the CPU sockets and reduces the effects of thermal coupling with memory in order to minimize cooling energy costs. We design CoMETC using a control-theoretic approach to guarantee meeting these objectives.We introduce a formal thermal and cooling model to be used for online decisions inside CoMETC. Our experimental results show that CoMETC achieves average cooling and memory energy savings of 58% compared to state-of-the-art techniques at a performance overhead of less than 0.3%. © 2013 ACM.",Algorithms; Design; Measurement; Performance,Algorithms; Design; Energy conservation; Energy utilization; Measurements; Memory architecture; Control-theoretic approach; Memory subsystems; Operational energy; Performance; State-of-the-art techniques; Suboptimal solution; Thermal coupling; Unified solutions; Cooling
Test compaction techniques for assertion-based test generation,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891788331&doi=10.1145%2f2534397&partnerID=40&md5=945dfb050331053956c062a1e0e53286,"Assertions are now widely used in verification as a means to help convey designer intent and also to simplify the detection of erroneous conditions by the firing of assertions. With this expressive modeling power, assertions could also be used for tasks such as helping to assess test coverage and even as a source for test generation. Our work deals with this last aspect, namely, assertion-based test generation. In this article, we present our compacted test generation scheme based on assertions. Novel compaction techniques are presented based on assertion clustering, test-path overlap detection and parallel-path removal. Our compaction approach is experimentally evaluated using nearly 300 assertions to show the amount of reduction that can be obtained in the size of the test sets. This ultimately has a positive impact on verification time in the quest for bugfree designs. ©2013 ACM.",Assertion-based verification; Directed test generation; Functional validation; SVA; Test compaction,Compaction; Assertion-based verification; Functional validation; SVA; Test Compaction; Test generations; Testing
Improving the performance of port range check for network packet filtering,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891808587&doi=10.1145%2f2523069&partnerID=40&md5=49d682765063b4e3b02d47f8b6e66e94,"This article introduces a high-performance packet filter design in which we propose the partial parallel range check (PPRC) technique for speeding up port range check. Unlike the conventional serial design that uses cascading cells to perform the serial check, PPRC divides the single path into several segments. All PPRC segments perform the range compare simultaneously, that is, parallel check, and then the results of each segment are serialized to generate the final check result. Besides theoretical analyses, we also use UMC 90nm CMOS process to implement the PPRC design and verify its effect on the check performance. Compared to state-of-the-art range check techniques, the results show that the PPRC design with the best configuration can improve check performance by 28%, at least. In addition, the PPRC design is more stable and energy efficient than related designs, even though it requiresmore transistors to implement the peripheral circuitry. The range of energy improvement achieved by the PPRC design is about 35%-70%. © 2013 ACM.",High-performance; Packet classification; Packet filter; Port range check,CMOS integrated circuits; Energy efficiency; Energy efficient; High-performance; Network packets; Packet classification; Packet filters; Peripheral circuitry; Port range check; Single path; Design
Performance bound analysis of analog circuits in frequencyand time-domain considering process variations,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891804385&doi=10.1145%2f2534395&partnerID=40&md5=66b477bbf517c5433423a376f1d75019,"In this article, we propose a new performance bound analysis of analog circuits considering process variations. We model the variations of component values as intervals measured from tested chips and manufacture processes. The new method first applies a graph-based analysis approach to generate the symbolic transfer function of a linear(ized) analog circuit. Then the frequency response bounds (maximum and minimum) are obtained by performing nonlinear constrained optimization in which magnitude or phase of the transfer function is the objective function to be optimized subject to the ranges of process variational parameters. The response bounds given by the optimization-based method are very accurate and do not have the overconservativeness issues of existing methods. Based on the frequency-domain bounds, we further develop a method to calculate the time-domain response bounds for any arbitrary input stimulus. Experimental results from several analog benchmark circuits show that the proposed method gives the correct bounds verified by Monte Carlo analysis while it delivers one order of magnitude speedup over Monte Carlo for both frequency-domain and time-domain bound analyses. We also show analog circuit yield analysis as an application of the frequency-domain variational bound analysis. © 2013 ACM.",Bound analysis; Circuit simulation; Process variation; Worst-case analysis,Analog circuits; Circuit simulation; Frequency response; Monte Carlo methods; Optimization; Transfer functions; Bound analysis; Manufacture process; Monte carlo analysis; Nonlinear constrained optimizations; Process Variation; Time-domain response; Variational parameters; Worst-case analysis; Time domain analysis
Destination-based congestion awareness for adaptive routing in 2d mesh networks,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887838741&doi=10.1145%2f2505055&partnerID=40&md5=488804f7e2f6e63ef4ca2244aeb39a18,"The choice of routing algorithm plays a vital role in the performance of on-chip interconnection networks. Adaptive routing is appealing because it offers better latency and throughput than oblivious routing, especially under nonuniform and bursty traffic. The performance of an adaptive routing algorithm is determined by its ability to accurately estimate congestion in the network. In this regard, maintaining global congestion state using a separate monitoring network offers better congestion visibility into distant parts of the network compared to solutions relying only on local congestion. However, the main challenge in designing such routing schemes is to keep the logic and bandwidth overhead as low as possible to fit into the tight power, area, and delay budgets of on-chip routers. In this article, we propose a minimal destination-based adaptive routing strategy (DAR), where every node estimates the delay to every other node in the network, and routing decisions are based on these per-destination delay estimates. DAR outperforms Regional Congestion Awareness (RCA), the best previously known adaptive routing algorithm that uses nonlocal congestion state. The performance improvement is brought about by maintaining fine-grained per-destination delay estimates in DAR that are more accurate than regional congestion metrics measured in RCA. The increased accuracy is a consequence of the fact that the per-destination delay estimates are not corrupted by congestion on links outside the admissible routing paths to the destination. A scalable version of DAR, referred to as SDAR, is also proposed for minimizing the overheads associated with DAR in large network topologies. We show that DAR outperforms local adaptive routing by up to 79% and RCA by up to 58% in terms of latency on SPLASH-2 benchmarks. DAR and SDAR also outperform existing adaptive and oblivious routing algorithms in latency and throughput under synthetic traffic patterns on 88 and 1616 mesh topologies, respectively. © 2013 ACM.",Adaptive routing; On-chip networks,Electric network topology; Routing algorithms; Adaptive routing; Adaptive routing algorithm; Bandwidth overheads; Monitoring network; Oblivious routing; On-chip interconnection network; On-chip networks; Routing decisions; Estimation
Deflection routing in 3d network-on-chip with limited vertical bandwidth,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887838181&doi=10.1145%2f2505011&partnerID=40&md5=a5a55e60f20447ca27cf234c4d5335c6,"This article proposes a deflection routing for 3D NoC with serialized TSVs for vertical links. Compared to buffered routing, deflection routing provides area-and power-efficient communication and little loss of performance under low to medium traffic load. Under 3D environments, the deflection routing can yield even better performance than buffered routing when key aspects are properly taken into account. However, the existing deflection routing technique cannot be directly applied because the serialized TSV links will take longer time to send data than ordinary planar links and cause many problems. A naive deflection through a TSV link can cause significantly longer latency and more energy consumption even for communications through planar links. This article proposes a method to mitigate the effect and also solve arising deadlock and livelock problems. Evaluation of the proposed scheme shows its effectiveness in throughput, latency, and energy consumption. © 2013 ACM.",3D NoC; Deflection routing; TSV serialization,Energy utilization; Three dimensional; VLSI circuits; 3-D environments; 3D NoC; Better performance; Deflection routings; Livelock problems; Loss of performance; Power-efficient communications; TSV serialization; Deflection (structures)
Enabling energy efficient reliability in embedded systems through smart cache cleaning,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887829349&doi=10.1145%2f2505012&partnerID=40&md5=56b26557abf86615044d20c2a481058a,"Incessant and rapid technology scaling has brought us to a point where today's, and future transistors are susceptible to transient errors induced by energy carrying particles, called soft errors.Within a processor, the sheer size and nature of data in the caches render it most vulnerable to electrical interference on data stored in the cache. Data in the cache is vulnerable to corruption by soft errors, for the time it remains actively unused in the cache. Write-through and early-write-back [Li et al. 2004] cache configurations reduce the time for vulnerable data in the cache, at the cost of increased memory writes and thereby energy. We propose a smart cache cleaning methodology, that enables copying of only specific vulnerable cache blocks into the memory at chosen times, thereby ensuring data cache protection with minimal memory writes. In this work, we first propose a hybrid (software-hardware) methodology. We then propose an improved software solution that utilizes cache write-back functionality available in commodity processors; thereby reducing the hardware overhead required to implement smart cache cleaning for such systems. The parameters involved in the implementation of our Smart Cache Cleaning (SCC) technique enable a means to provide for customizable energy-efficient soft error reduction in the L1 data cache. Given the system requirements of reliability, power-budget and runtime priority of the application, appropriate parameters of the SCC can be customized to trade-off power consumption and L1 data cache reliability. Our experiments over LINPACK and Livermore benchmarks demonstrate 26% reduced energy-vulnerability product (energy-efficient vulnerability reduction) compared to that of hardware based cache reliability techniques. Our software-only solution achieves same levels of reliability with an additional 28% performance improvement. © 2013 ACM.",Cache cleaning; Embedded system; Power efficiency; Reliability; Smart cache; Soft error,Cleaning; Cost reduction; Embedded systems; Energy efficiency; Error correction; Hardware; Microprocessor chips; Reliability; Software reliability; Cache configurations; Commodity processors; Electrical interference; Energy-carrying particles; Power efficiency; Smart cache; Soft error; Vulnerability reductions; Cache memory
Exploiting workload dynamics to improve ssd read latency via differentiated error correction codes,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887834378&doi=10.1145%2f2489792&partnerID=40&md5=839653bf70a3d40021784c6667337419,"This article presents a cross-layer codesign approach to reduce SSD read response latency. The key is to cohesively exploit the NAND flash memory device write speed vs. raw storage reliability trade-off at the physical layer and runtime data access workload dynamics at the system level. Leveraging runtime data access workload variation, we can opportunistically slow down NAND flash memory write speed and hence improve NAND flash memory raw storage reliability. This naturally enables an opportunistic use of weaker error correction schemes that can directly reduce SSD read access latency.We develop a disk-level scheduling scheme to effectively smooth the write workload in order to maximize the occurrence of runtime opportunistic NAND flash memory write slowdown. Using 2 bits/cell NAND flash memory with BCH-based error correction correction as a test vehicle, we carry out extensive simulations over various workloads and demonstrate that this developed cross-layer co-design solution can reduce the average SSD read latency by up to 59.4% without sacrificing the write throughput performance. © 2013 ACM.",Data storage; Error correction code; NAND flash memory; Solid state drive,Digital storage; Error correction; Network layers; Program processors; Data storage; Error correction codes; Error-correction schemes; Extensive simulations; NAND flash memory; Scheduling schemes; Solid state drives; Storage reliability; NAND circuits
A fast and scalable multidimensional multiple-choice knapsack heuristic,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887845403&doi=10.1145%2f2541012.2541014&partnerID=40&md5=e4a274704bfcda37d59027fa1b9dbb2a,"Many combinatorial optimization problems in the embedded systems and design automation domains involve decision making in multidimensional spaces. The multidimensional multiple-choice knapsack problem (MMKP) is among themost challenging of the encountered optimization problems. MMKP problem instances appear for example in chip multiprocessor runtime resource management and in global routing of wiring in circuits. Chip multiprocessor resource management requires solving MMKP under real-time constraints, whereas global routing requires scalability of the solution approach to extremely large MMKP instances. This article presents a novel MMKP heuristic, CPH (for Compositional Pareto-algebraic Heuristic), which is a parameterized compositional heuristic based on the principles of Pareto algebra. Compositionality allows incremental computation of solutions. The parameterization allows tuning of the heuristic to the problem at hand. These aspects make CPH a very versatile heuristic. When tuning CPH for computation time, MMKP instances can be solved in real time with better results than the fastest MMKP heuristic so far. When tuning CPH for solution quality, it finds several new solutions for standard benchmarks that are not found by any existing heuristic. CPH furthermore scales to extremely large problem instances. We illustrate and evaluate the use of CPH in both chip multiprocessor resource management and in global routing. © 2013 ACM.",Chip multiprocessors; Combinatorial optimization; Design automation; Knapsack problems; Runtime management and design-time optimization for embedded systems; VLSI routing,Algebra; Benchmarking; Computer aided design; Embedded systems; Microprocessor chips; Multiprocessing systems; Natural resources management; Pareto principle; Resource allocation; Tuning; Chip Multiprocessor; Design automations; Knapsack problems; Runtime management; VLSI routing; Combinatorial optimization
A parallel dual-scanline algorithm for partitioning parameterized 45-degree polygons,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887843876&doi=10.1145%2f2505015&partnerID=40&md5=1eb96f7d967e6989c49075c085a42052,"In order to use rectangular corner stitching data structures in storing parameterized orthogonal layouts, parameterized polygons in the layouts must be partitioned into rectangles. Likewise, in order to use trapezoidal corner stitching data structures in storing parameterized 45-degree layouts, parameterized polygons in the layouts have to be partitioned into trapezoids. In this article, a parallel polygon partitioning algorithm is proposed; the algorithm is capable of partitioning parameterized orthogonal polygons into parameterized rectangles as well as partitioning parameterized 45-degree polygons into parameterized trapezoids. Additionally, the algorithm can be used to partition fixed-coordinate polygons. By adopting the dual-scanline technique, which involves using two scanlines to concurrently sweep an input polygon, the parallel partitioning algorithm can process vertices and edges of the input polygon efficiently. The parallel polygon partitioning algorithm has been implemented in C++ with the use of OpenMP. Compared with a sequential partitioning program which uses a single scanline, our parallel partitioning program can achieve 20% to 30% speedup while partitioning large parameterized polygons or partitioning parameterized polygons with complex constraints. © 2013 ACM.",Parameterized layouts; Parameterized polygons; Polygon decomposition; Trapezoidal corner stitching,Algorithms; Application programming interfaces (API); Axial flow; Data structures; Parameterization; Complex constraints; Orthogonal layout; Parameterized layouts; Parameterized polygons; Partitioning algorithms; Polygon decomposition; Polygon partitioning; Trapezoidal corner stitching; Geometry
Ordering circuit establishment in multiplane nocs,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887845938&doi=10.1145%2f2500752&partnerID=40&md5=d2751c68cfc90f5192ec7b1b8da18d34,"Segregating networks-on-chips (NoCs) into data and control planes yields several opportunities for improving power and performance in chip-multiprocessor systems (CMPs). This article describes a hybrid packet/circuit switched multiplane network optimized to reduce latency in order to improve system performance and/or reduce system energy. Unlike traditional circuit preallocation techniques which require timestamps to reserve circuit resources, this article proposes an order-based preallocation scheme. By enforcing the order in which resources are scheduled and utilized rather than a fixed time, the NoC can take advantage of messages that arrive early while naturally tolerating message delays due to contention. Ordered circuit establishment is presented using two techniques. First, Dej a Vu switching preestablishes circuits for data messages once a cache hit is detected and prior to the requested data becoming available. Second, using Red Carpet Routing, circuits are proactively reserved for a return data message as a request message traverses the NoC. The reduced communication latency over configured circuits enable system performance improvement or saving NoC energy by reducing voltage and frequency without sacrificing performance. In simulations of 16 and 64 core CMPs, Dej a Vu switching enabled average NoC energy savings of 43% and 53% respectively. On the other hand, simulations of communication sensitive benchmarks using Red Carpet Routing show speedup in execution time of up to 16%, with an average of 10% over a purely packet switched NoC and an average of 8% over preconfiguring circuits using Dej a Vu switching. © 2013 ACM.",Algorithms; Design; Performance,Algorithms; Communication; Design; Chip-multiprocessor; Communication latency; Control planes; Improving power; Networks on chips; Packet-switched; Performance; Request messages; Switching circuits
Common-source-line array: An area efficient memory architecture for bipolar nonvolatile devices,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887847678&doi=10.1145%2f2500459&partnerID=40&md5=129ba86845c6a6fd00fe5b65a698a95f,"Traditional array organization of bipolar nonvolatile memories such as STT-MRAM and memristor utilizes two bitlines for cell manipulations.With technology scaling, such bitline pair will soon become the bottleneck for further density improvement. In this article we propose a novel common-source-line array architecture, which uses a shared source-line along the row, leaving only one bitline per column. We elaborate the array design to ensure reliability, and demonstrate its effectiveness on STT-MRAM and memristor memory arrays. Our study results show that with comparable latency and energy, the proposed common-source-line array can save 34% and 33% area for Memristor-RAM and STT-MRAM respectively, compared with corresponding dual-bitline arrays. © 2013 ACM.",Bitline; Memory access; Memory array; Memristor; Nonvolatile memory; STT-MRAM,Memory architecture; Memristors; Passive filters; Random access storage; Bit lines; Memory access; Memory array; Memristor; Non-volatile memory; STT-MRAM; MRAM devices
A routing algorithm for graphene nanoribbon circuit,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887846864&doi=10.1145%2f2505056&partnerID=40&md5=4d1ca1002d9c1bc744d83e2fca239414,"Conventional CMOS devices are facing an increasing number of challenges as their feature sizes scale down. Graphene nanoribbon (GNR) based devices are shown to be a promising replacement of traditional CMOS at future technology nodes. However, all previous works on GNRs focus at the device level. In order to integrate these devices into electronic systems, routing becomes a key issue. In this article, the GNR routing problem is studied for the first time.We formulate the GNR routing problem as a minimum hybrid-cost shortest path problem on triangular mesh (hybrid means that we need to consider both the length and the bending of the routing path). We show that by graph expansion, this minimum hybrid-cost shortest path problem can be solved by applying the conventional shortest path algorithm on the expanded graph. Experimental results show that our GNR routing algorithm effectively handles the hybrid cost. © 2013 ACM.",Graphene nanoribbons; Routing,CMOS integrated circuits; Costs; Graph theory; Routing algorithms; Electronic systems; Future technologies; Graphene nano-ribbon; Graphene nanoribbon (GNR); Graphene nanoribbons; Routing; Shortest path algorithms; Shortest path problem; Nanoribbons
Dynamic power management for multidomain system-on-chip platforms: An optimal control approach,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887830830&doi=10.1145%2f2504904&partnerID=40&md5=8e5be2649e26d086554cc0d7e914625c,"Reducing energy consumption in multiprocessor systems-on-chip (MPSoCs) where communication happens via the network-on-chip (NoC) approach calls for multiple voltage/frequency island (VFI)-based designs. In turn, such multi-VFI architectures need efficient, robust, and accurate runtime control mechanisms that can exploit the workload characteristics in order to save power. Despite being tractable, the linear control models for power management cannot capture some important workload characteristics (e.g., fractality, nonstationarity) observed in heterogeneous NoCs; if ignored, such characteristics lead to inefficient communication and resources allocation, as well as high power dissipation in MPSoCs. To mitigate such limitations, we propose a new paradigm shift from power optimization based on linear models to control approaches based on fractal-state equations. As such, our approach is the first to propose a controller for fractal workloads with precise constraints on state and control variables and specific time bounds. Our results show that significant power savings can be achieved at runtime while running a variety of benchmark applications. © 2013 ACM.",Finite horizon optimal control; Fractal workloads; Network-on-chip; Power management,Application specific integrated circuits; Communication; Control; Energy management; Equations of state; Linear control systems; Multiprocessing systems; Servers; VLSI circuits; Dynamic power management; Finite horizon optimal control; Multiprocessor systems on chips; Network on chip; Power managements; Reducing energy consumption; System-on-chip platforms; Workload characteristics; Fractals
An index-based management scheme with adaptive caching for huge-scale low-cost embedded flash storages,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887831801&doi=10.1145%2f2505013&partnerID=40&md5=09cf74c1318a500b759189aad065ae8c,"Due to its remarkable access performance, shock resistance, and costs, NAND flash memory is now widely adopted in a variety of computing environments, especially in mobile devices such as smart phones, media players and electronic book readers. For the consideration of costs, low-cost embedded flash storages such as flash memory cards are often employed on such devices. Different from solid-state disks, the RAM buffer equipped on low-cost embedded flash storages are very small, for example, limited under several dozens of kilobytes, despite of the rapidly growing capacity of the storages. The significance of effectively utilizing the very limited on-device RAM buffers of embedded flash storages is therefore highlighted, and a novel design of scalable flash management schemes is needed to tackle the new access constraints of MLC NAND flash memory. In this work, a highly scalable design of the flash translation layer is presented with the considerations of the on-device RAM size, user access patterns, address-mapping-information caching and MLC access constraints. Through a series of experiments, it is verified that, with appropriate settings of cache sizes, the proposed management scheme provides comparable performance results to prior arts with much lower requirements on the on-device RAM. In other words, the proposed scheme suggests a strategy to make better use of the on-device RAM, and is suitable for embedded flash storages. © 2013 ACM.",Flash memory; Performance; Scalability; Storage,Costs; Embedded systems; Energy storage; Flash memory; Information systems; Mobile devices; NAND circuits; Scalability; Smart cards; Storage management; Adaptive caching; Computing environments; Flash Memory Card; Flash translation layer; Management scheme; NAND flash memory; Performance; User access patterns; Random access storage
Adaptive virtual channel partitioning for network-on-chip in heterogeneous architectures,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887851142&doi=10.1145%2f2504906&partnerID=40&md5=92549337f44236951265a1c5fa1e52cf,"Current heterogeneous chip-multiprocessors (CMPs) integrate a GPU architecture on a die. However, the heterogeneity of this architecture inevitably exerts different pressures on shared resource management due to differing characteristics of CPU and GPU cores. We consider how to efficiently share on-chip resources between cores within the heterogeneous system, in particular the on-chip network. Heterogeneous architectures use an on-chip interconnection network to access shared resources such as last-level cache tiles and memory controllers, and this type of on-chip network will have a significant impact on performance. In this article, we propose a feedback-directed virtual channel partitioning (VCP) mechanism for on-chip routers to effectively share network bandwidth between CPU and GPU cores in a heterogeneous architecture. VCP dedicates a few virtual channels to CPU and GPU applications with separate injection queues. The proposed mechanism balances on-chip network bandwidth for applications running on CPU and GPU cores by adaptively choosing the best partitioning configuration. As a result, our mechanism improves system throughput by 15% over the baseline across 39 heterogeneous workloads. © 2013 ACM.",Heterogeneous architecture; On-chip network; Quality-of-service,Communication channels (information theory); Computer architecture; Embedded systems; Quality of service; VLSI circuits; Chip Multiprocessor; Different pressures; Heterogeneous architectures; Heterogeneous systems; Heterogeneous workloads; Memory controller; On-chip interconnection network; On-chip networks; Routers
Hardware/software approaches for reducing the process variation impact on instruction fetches,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887853325&doi=10.1145%2f2489778&partnerID=40&md5=2c070312f1f3b4bfe5e9be5a4a8647d3,"As technology moves towards finer process geometries, it is becoming extremely difficult to control critical physical parameters such as channel length, gate oxide thickness, and dopant ion concentration. Variations in these parameters lead to dramatic variations in access latencies in Static Random Access Memory (SRAM) devices. This means that different lines of the same cache may have different access latencies. A simple solution to this problem is to adopt the worst-case latency paradigm. While this egalitarian cache management is simple, it may introduce significant performance overhead during instruction fetches when both address translation (instruction Translation Lookaside Buffer (TLB) access) and instruction cache access take place, making this solution infeasible for future high-performance processors. In this study, we first propose some hardware and software enhancements and then, based on those, investigate several techniques to mitigate the effect of process variation on the instruction fetch pipeline stage in modern processors. For address translation, we study an approach that performs the virtual-to-physical page translation once, then stores it in a special register, reusing it as long as the execution remains on the same instruction page. To handle varying access latencies across different instruction cache lines, we annotate the cache access latency of instructions within themselves to give the circuitry a hint about how long to wait for the next instruction to become available. © 2013 ACM.",Address translation; Encoding; Instruction cache; Process variation,Cache memory; Encoding (symbols); Hardware; Address translation; Gate oxide thickness; Hardware and software; High performance processors; Instruction caches; Process Variation; Static random access memory; Translation lookaside buffer; Pipeline processing systems
A novel differential scan attack on advanced DFT structures,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887840141&doi=10.1145%2f2505014&partnerID=40&md5=10f017128e310441ea013a5d54a28317,"Scan chains insertion is the most common technique to ensure the testability of digital cores, providing high fault coverage. However, for ICs dealing with secret information, scan chains can be used as back doors for accessing secret data thus becoming a threat to system security. So far, advanced test structures used to reduce test costs (e.g., response compaction) and achieve high fault coverage (e.g., X's masking decoder) have been considered as intrinsic countermeasures against these threats. This work proposes a new generic scanbased attack demonstrating that these test structures are not sufficiently effective to prevent leakage through the test infrastructure. This generic attack can be easily adapted to several cryptographic implementations for both symmetric and public key algorithms. The proposed attack is demonstrated on several ciphers. © 2013 ACM.",Scan-based DFT; Side-channel attacks,Automation; Computer applications; Cryptographic implementation; Public key algorithms; Response compaction; Scan-based attacks; Scan-based DFT; Secret information; Side channel attack; Test infrastructures; Design for testability
Call for Nominations for Editor-in-Chief,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025403940&doi=10.1145%2f2541012.2541672&partnerID=40&md5=96473ad63ca7c8d7cd80885377fbbee3,[No abstract available],,
"Editorial to special section on networks on chip: Architecture, tools, and methodologies",2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887848716&doi=10.1145%2f2541012.2541013&partnerID=40&md5=ab79ee3553f9f92c343c1da6bbf977d5,[No abstract available],,
In-network monitoring and control policy for dvfs of cmp networks-on-chip and last level caches,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887832821&doi=10.1145%2f2504905&partnerID=40&md5=25dc3e07628e1d7c3cc5d8f042490a1d,"In chip design today and for a foreseeable future, the last-level cache and on-chip interconnect is not only performance critical but also a substantial power consumer. This work focuses on employing dynamic voltage and frequency scaling (DVFS) policies for networks-on-chip (NoC) and shared, distributed last-level caches (LLC). In particular, we consider a practical system architecture where the distributed LLC and the NoC share a voltage/frequency domain that is separate from the core domain. This architecture enables the control of the relative speed between the cores and memory hierarchy without introducing synchronization delays within the NoC. DVFS for this architecture is more complex than individual link/core-based DVFS since it involves spatially distributed monitoring and control. We propose an average memory access time (AMAT)-based monitoring technique and integrate it with DVFS based on PID control theory. Simulations on PARSEC benchmarks yield a 27% energy savings with a negligible impact on system performance. © 2013 ACM.",Chip multiprocessor; Dynamic voltage/frequency scaling; Last-level cache; Networks-on-chip,Three term control systems; Average memory access time; Chip Multiprocessor; Dynamic voltage and frequency scaling; Dynamic voltage/frequency scaling; Last-level caches; Lastlevel caches (LLC); Monitoring and control; Networks on chips; Microprocessor chips
Architecture customization of on-chip reconfigurable accelerators,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887830526&doi=10.1145%2f2493384&partnerID=40&md5=830af32083a31502be6eac933c6c3872,"Integrating coarse-grained reconfigurable architectures (CGRAs) into a System-on-a-Chip (SoC) presents many benefits as well as important challenges. One of the challenges is how to customize the architecture for the target applications efficiently and effectively without performing explicit design space exploration. In this article we present a novel methodology for incremental interconnect customization of CGRAs that can suggest a new interconnection architecture which is able to maximize the performance for a given set of application kernels while minimizing the hardware cost. In our methodology, we translate the problem of interconnect customization into that of inexact graph matching, and we devised a heuristic for A search algorithm to efficiently solve the inexact graph matching problem. Our experimental results demonstrate that our customization method can quickly find application-optimized interconnections that exhibit 80% higher performance on average compared to the base architecture which has mesh interconnections, with little energy and hardware increase in interconnections and muxes. © 2013 ACM.",Coarse-grained reconfigurable array; Customization; Graph edit; Inexact matching; Interconnect architecture customization,Application specific integrated circuits; Hardware; Coarse-grained reconfigurable arrays; Customization; Graph edit; Inexact matching; Interconnect architectures; Pattern matching
Resource-aware architectures for adaptive particle filter based visual target tracking,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878514743&doi=10.1145%2f2442087.2442093&partnerID=40&md5=4403c18cd64105f72c0c76cc604be1e8,"There are a growing number of visual tracking applications now being envisioned for mobile devices. However, since computer vision algorithms such as particle filtering have large computational demands, they can result in high energy consumption and temperatures in mobile devices. Conventional approaches for distributed target tracking with a camera node and a receiver node are either sender-based (SB) or receiver-based (RB). The SB approach uses little energy and bandwidth, but requires a sender with large computational resources. The RB approach fits applications where computational resources are completely unavailable to the sender, but requires very large energy and bandwidth. In this article, we propose three architectures for distributed particle filtering that (i) reduce particle filtering workload and (ii) allow for dynamic migration of workload between nodes participating in tracking.We also discuss an adaptive particle filtering extension that adapts particle filter computational complexity and can be applied to both the conventional and proposed architectures for improved energy efficiency. Results show that the proposed solutions require low additional overhead, improve on tracking system lifetime, balance node temperatures, maintain track of the desired target, and are more effective than conventional approaches in many scenarios. © 2013 ACM.",Adaptive Particle Filter; Energy management; Particle Filter; Target tracking; Thermal management,Architecture; Bandwidth; Computer vision; Energy efficiency; Energy management; Energy utilization; Mobile devices; Monte Carlo methods; Target tracking; Temperature control; Adaptive particle filters; Computational resources; Computer vision algorithms; Distributed target tracking; High energy consumption; Particle filter; Proposed architectures; Visual target tracking; Signal filtering and prediction
Synthesis of networks of custom processing elements for real-time physical system emulation,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878498341&doi=10.1145%2f2442087.2442092&partnerID=40&md5=632c3740375c25d5263edcc7c125c9d5,"Emulating a physical system in real-time or faster has numerous applications in cyber-physical system design and deployment. For example, testing of a cyber-device's software (e.g., a medical ventilator) can be done via interaction with a real-time digital emulation of the target physical system (e.g., a human's respiratory system). Physical system emulation typically involves iteratively solving thousands of ordinary differential equations (ODEs) that model the physical system. We describe an approach that creates custom processing elements (PEs) specialized to the ODEs of a particular model while maintaining some programmability, targeting implementation on field-programmable gate arrays (FPGAs). We detail the PE micro-architecture and accompanying automated compilation and synthesis techniques. Furthermore, we describe our efforts to use a high-level synthesis approach that incorporates regularity extraction techniques as an alternative FPGA-based solution, and also describe an approach using graphics processing units (GPUs). We perform experiments with five models: a Weibel lung model, a Lutchen lung model, an atrial heart model, a neuron model, and a wave model; each model consists of several thousand ODEs and targets a Xilinx Virtex 6 FPGA. Results of the experiments show that the custom PE approach achieves 4X-9X speedups (average 6.7X) versus our previous general ODE-solver PE approach, and 7X-10X speedups (average 8.7X) versus highlevel synthesis, while using approximately the same or fewer FPGA resources. Furthermore, the approach achieves speedups of 18X-32X (average 26X) versus an Nvidia GTX 460 GPU, and average speedups of more than 100X compared to a six-core TI DSP processor or a four-core ARM processor, and 24X versus an Intel I7 quad core processor running at 3.06 GHz. While an FPGA implementation costs about 3X-5X more than the non-FPGA approaches, a speedup/dollar analysis shows 10X improvement versus the next best approach, with the trend of decreasing FPGA costs improving speedup/dollar in the future. © 2013 ACM.",Custom processor; Field-programmable gate array (FPGA); High-level synthesis; Ordinary differential equation (ODE) solving; Physical models; Real-time emulation,ARM processors; Computer graphics; Cost benefit analysis; Embedded systems; Experiments; Field programmable gate arrays (FPGA); Iterative methods; Ordinary differential equations; Program processors; Respiratory system; Software testing; Custom processors; High Level Synthesis; Ordinary differential equation (ODE) solving; Physical model; Real-time emulation; Models
Low-power anti-aging zero skew clock gating,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878488357&doi=10.1145%2f2442087.2442098&partnerID=40&md5=176a0a12d1c5dc24c3eec5d0d81728bb,"In advanced CMOS technology, the NBTI (negative bias temperature instability) effect results in delay degradations of PMOS transistors. Further, because of clock gating, PMOS transistors in a clock tree often have different active probabilities, leading to different delay degradations. If the degradation difference is not properly controlled, this clock skew may cause the circuit fails to function at some point later in time. Intuitively, the degradation difference can be eliminated, if we increase the active probability of the lowprobability clock gates to ensure the clock gates at the same level always having the same active probability. However, this intuitive method may suffer from large power consumption overhead. In this article, we point out, by carefully planning the transistor-level clock signal propagation path, we can have many clock gates whose active probabilities do not affect the degradation difference. Based on that observation, we propose a critical-PMOS-aware clock tree design methodology to eliminate the degradation difference with minimum power consumption overhead. Benchmark data consistently show our approach achieves very good results in terms of both the NBTI-induced clock skew (i.e., the degradation difference) and the power consumption overhead. © 2013 ACM.",Clock skew minimization; Clock tree; Gated clock design; Power minimization; Reliability,Clocks; Electric Circuits; Forestry; Probability; Reliability; CMOS integrated circuits; Electric clocks; Electric power supplies to apparatus; Forestry; Probability; Reliability; Benchmark data; Clock skews; Clock tree; Gated clocks; Negative bias temperature instability; pMOS transistors; Power minimization; Zero-skew clock; Integrated circuits
Coverage-directed observability-based validation for embedded software,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878476084&doi=10.1145%2f2442087.2442090&partnerID=40&md5=8cfdc3bdab9e2cb9d2b6e22c10b5cc6f,"Motivated by the need for validation methodologies for embedded systems we propose a method for embedded software testing that can be integrated with existing hardware methods. Existing coverage-directed validation methods guarantee the execution of a certain percentage of the program code under test. Yet they do not generally verify whether the statements executed have any influence on the program's output. In the proposed method, a program statement is considered covered not simply for belonging to the executed path, but only if its execution has influence in some observable output. The paths are generated by searching the longest path in terms of the number of statements in the path. Given that not all paths are valid, we check their feasibility using a method based on Mixed Integer Linear Programming (MILP). Variable aliasing is accounted for by representing variables by their memory addresses when building this MILP problem. In this manner, for feasible paths, we obtain immediately the input values that allow the execution of the path. Using these inputs, we determine the statements actually observed. We repeat this process until a user-specified level of coverage has been achieved. In the generation of each new path, the statement coverage obtained so far and the feasibility of previous paths is taken into account. We present results that demonstrate the effectiveness of this methodology. © 2013 ACM.",Coverage; Embedded software; Observability; Validation,Embedded software; Embedded systems; Observability; Coverage; Input values; Memory address; Mixed-integer linear programming; Program statements; Statement coverage; Validation; Validation methodologies; Software testing
Reducing instruction bit-width for low-power vliw architectures,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878476763&doi=10.1145%2f2442087.2442096&partnerID=40&md5=f91424edcf246ae0c78daee3724aabaa,"VLIW (very long instruction word) architectures have proven to be useful for embedded applications with abundant instruction level parallelism. But due to the long instruction bus width it often consumes more power and memory space than necessary. One way to lessen this problem is to adopt a reduced bit-width instruction set architecture (ISA) that has a narrower instruction word length. This facilitates a more efficient hardware implementation in terms of area and power by decreasing bus-bandwidth requirements and the power dissipation associated with instruction fetches. In practice, however, it is impossible to convert a given ISA fully into an equivalent reduced bit-width one because the narrow instruction word, due to bitwidth restrictions, can encode only a small subset of normal instructions in the original ISA. Consequently, existing processors provide narrow instructions in very limited cases along with severe restrictions on register accessibility. The objective of this work is to explore the possibility of complete conversion, as a case study, of an existing 32-bit VLIW ISA into a 16-bit one that supports effectively all 32-bit instructions. To this objective, we attempt to circumvent the bit-width restrictions by dynamically extending the effective instruction word length of the converted 16-bit operations. Further, we will show that our proposed ISA conversion can create a synergy effect with a VLES (variable length execution set) architecture that is adopted in most recent VLIW processors. According to our experiment, the code size becomes significantly smaller after the conversion to 16-bit VLIW code. Also at a slight run time cost, the machine with the 16-bit ISA consumes much less energy than the original machine. © 2013 ACM.",Code generation; Code size; Power consumption; Reduced bit-width ISA; VLIW architecture,Digital signal processing; Electric power utilization; Hardware; Program compilers; Bit-Width; Code Generation; Code size; Hardware implementations; Instruction level parallelism; Instruction set architecture; Variable length execution set; Vliw(very long instruction word); Very long instruction word architecture
Low-power resource binding by postsilicon customization,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878506580&doi=10.1145%2f2442087.2442097&partnerID=40&md5=147ac359f59beadb676709ec6fd52be5,"This article proposes the first postsilicon customization method for resource binding to achieve power reduction application specific integrated circuits (ASICs) design. Instead of committing to one configuration of resource binding during synthesis, our new synthesis method produces a diverse set of candidate bindings for the design. To ensure diversity of the resource usage patterns, we introduce a binding candidate formation method based on the orthogonal arrays. Additional control components are added to enable post manufacturing selection of one of the binding candidates. The resource binding candidate that minimizes the power consumption is selected by considering the specific power characteristics of each chip. An efficient methodology for embedding several binding candidates in one design is developed. Evaluations on benchmark designs show the low overhead and the effectiveness of the proposed methods. As an example, applying our method results in an average of 14.2% (up to 24.0%) power savings on benchmark circuits for a variation model in 45nm CMOS technology. The power efficiency of our customized postsilicon binding is expected to improve with scaling of the technology and the likely resulting higher process variations. © 2013 ACM.",High level synthesis; Low power; Postsilicon optimization; Resource binding customization,Application specific integrated circuits; Benchmarking; CMOS integrated circuits; Additional control; Benchmark designs; High Level Synthesis; Low Power; Post-silicon; Process Variation; Resource binding; Resource usage patterns; Design
The survivability of design-specific spare placement in fpga architectures with high defect rates,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878494030&doi=10.1145%2f2442087.2442104&partnerID=40&md5=9fe0e314f0d5e0db1f0024477bea974b,"We address the problem of optimizing fault tolerance in FPGA architectures with high defect rates (such as nano-FPGAs) without significantly degrading performance. Our methods address fault tolerance during the placement and reconfiguration stages of FPGA programming. First, we provide several complexity results for both the fault reconfiguration and fault-tolerance placement problems. Then, we propose a placement algorithm which, in the presence of randomly generated faults, optimizes spare placement to maximize the probability that the FPGA can be reconfigured to meet a specified timing constraint. We also give heuristics for reconfiguration after faults have been detected. Despite the hardness results for both the placement and reconfiguration problems, we show our heuristics perform well in simulation (in one scenario, increasing the probability of successful reconfiguration by as much as 55% compared to a uniform spare placement). © 2013 ACM.",Complexity; Fault Tolerance; FPGA; Hardness; Nanoelectronics; NPcomplete; Placement; Reconfiguration; Survivability,Defects; Fault tolerance; Hardness; Nanoelectronics; Complexity; NP Complete; Placement; Reconfiguration; Survivability; Field programmable gate arrays (FPGA)
A study of row-based area-array I/O design planning in concurrent chip-package design flow,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878506179&doi=10.1145%2f2442087.2442101&partnerID=40&md5=dae0878d0a91632eb17f24c5fb60f090,"IC-centric design flow has been a common paradigm when designing and optimizing a system. Package and board/system designs are usually followed by almost-ready chip designs, which causes long turn-around time communicating with package and system houses. In this article, the realizations of area-array I/O design methodologies are studied. Different from IC-centric flow, we propose a chip-package concurrent design flow to speed up the design time. Along with the flow, we design the I/O-bump (and P/G-bump) tile that combines I/O (and P/G) and bump into a hard macro with the considerations of I/O power connection and electrostatic discharge (ESD) protection. We then employ an I/O-row based scheme to place I/O-bump tiles with existed metal layers. By such a scheme, it reduces efforts in I/O placement legalization and the redistribution layer (RDL) routing. With the emphasis on package design awareness, the proposed methods map package balls onto chip I/Os, thus providing an opportunity to design chip and package in parallel. Due to this early study of I/O and bump planning, faster convergence can be expected with concurrent design flow. The results are encouraging and the merits of this flow are reassuring. © 2013 ACM.",Area-array IC design; Chip-package feasibility study; Concurrent IC design flow; I/O-bump planning,Electrostatic devices; Electrostatic discharge; Chip-package design; Design Methodology; Electrostatic discharge protection; Faster convergence; Feasibility studies; IC designs; Placement legalization; Redistribution layers; Design
Revisiting automated physical synthesis of high-performance clock networks,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878501046&doi=10.1145%2f2442087.2442102&partnerID=40&md5=e809a0f48efaa63d3b263e69aa3c4c9b,"High-performance clock distribution has been a challenge for nearly three decades. During this time, clock synthesis tools and algorithms have strove to address a myriad of important issues helping designers to create faster, more reliable, and more power efficient chips. This work provides a complete discussion of the high-performance ASIC clock distribution using information gathered from both leading industrial clock designers and previous research publications. While many techniques are only briefly explained, the references summarize the most influential papers on a variety of topics for more in-depth investigation. This article also provides a thorough discussion of current issues in clock synthesis and concludes with insight into future research and design challenges for the community at large. © 2013 ACM.",Clock optimization; Physical synthesis; Robust design,Design; Industrial research; Clock distribution; Clock network; Design challenges; Physical synthesis; Power efficient; Robust designs; Synthesis tool; Clocks
Composable thermal modeling and simulation for architecture-level thermal designs of multicore microprocessors,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878497566&doi=10.1145%2f2442087.2442099&partnerID=40&md5=c0dc710a81af83f18a4b0b28e6ec7b74,"Efficient temperature estimation is vital for designing thermally efficient, lower power and robust integrated circuits in nanometer regime. Thermal simulation based on the detailed thermal structures no longer meets the demanding tasks for efficient design space exploration. The compact and composable model-based simulation provides a viable solution to this difficult problem. However, building such thermal models from detailed thermal structures was not well addressed in the past. In this article, we propose a new compact thermal modeling technique, called ThermComp, standing for thermal modeling with composable modules. ThermComp can be used for fast thermal design space exploration for multicore microprocessors. The new approach builds the composable model from detailed structures for each basic module using the finite difference method and reduces the model complexity by the sampling-based model order reduction technique. These composable models are then used to assemble different multicore architecture thermal models and realized into SPICE-like netlists. The resulting thermal models can be simulated by the general circuit simulator SPICE. ThermComp tries to preserve the accuracy of fine-grained models with the speed of coarse-grained models. Experimental results on a number of multicore microprocessor architectures show the new approach can easily build accurate thermal systems from compact composable models for fast architecture thermal analysis and optimization and is much faster than the existing HotSpot method with similar accuracy. © 2013 ACM.",Composable; Model reduction; Multicore; Thermal modeling,Circuit simulation; Microprocessor chips; Software architecture; SPICE; Thermoanalysis; Thermography (temperature measurement); Compact thermal modeling; Composable; Efficient design space explorations; Microprocessor architectures; Model reduction; Model-based simulations; Multi core; Thermal modeling; Computer simulation
Runtime verification for multicore SoC with high-quality trace data,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878509619&doi=10.1145%2f2442087.2442089&partnerID=40&md5=aeb38b0682d27418f5740a2af4e20d39,"Multicore System-on-Chip (SoC) implementations of embedded systems are becoming very popular. In these systems it is possible to spread out computations over many cores. On one hand this leads to better energy efficiency if clock frequencies and core voltages are reduced. On the other hand this delivers very high performance to the software developer and thus enables complex software systems to be implemented. Unfortunately, debugging and validation of these systems becomes extremely difficult. Various technological approaches try to solve this dilemma. In this contribution we will show a new approach to observe multicore SoCs and make their internal operations visible to external analysis tools. Also, we show that runtime verification can be employed to analyze and validate these internal operations while the system operates in its normal environment. The combination of these two approaches delivers unprecedented options to the developer to understand and verify system behavior even in complex multicore SoCs. © 2013 ACM.",Embedded system; Multicore SoC; Runtime verification; Synchronisation; Test driven development; Trace data,Application specific integrated circuits; Digital storage; Embedded systems; Energy efficiency; Synchronization; Complex software systems; Internal operations; Multicore soc; Run-time verification; Software developer; System behaviors; Test driven development; Trace data; Program debugging
Shared recovery for energy efficiency and reliability enhancements in real-time applications with precedence constraints,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878516822&doi=10.1145%2f2442087.2442094&partnerID=40&md5=59fd0de4aeea41af98b60f0d8fca793c,"While Dynamic Voltage Scaling (DVS) remains as a popular energy management technique for modern computing systems, recent research has identified significant and negative impacts of voltage scaling on system reliability. To preserve system reliability under DVS settings, a number of reliability-aware power management (RA-PM) schemes have been recently studied. However, the existing RA-PM schemes normally schedule a separate recovery for each task whose execution is scaled down and are rather conservative. To overcome such conservativeness, we study in this article novel RA-PM schemes based on the shared recovery (SHR) technique. Specifically, we consider a set of frame-based real-time tasks with individual deadlines and a common period where the precedence constraints are represented by a directed acyclic graph (DAG). We first show that the earliest deadline first (EDF) algorithm can always yield a schedule where all timing and precedence constraints are met by considering the effective deadlines of tasks derived from as late as possible (ALAP) policy, provided that the task set is feasible. Then, we propose a shared recovery based frequency assignment technique (namely SHR-DAG) and prove its optimality to minimize energy consumption while preserving the system reliability. To exploit additional slack that arises from early completion of tasks, we also study a dynamic extension for SHR-DAG to improve energy efficiency and system reliability at runtime. The results from our extensive simulations show that, compared to the existing RA-PM schemes, SHR-DAG can achieve up to 35% energy savings, which is very close to the maximum achievable energy savings. More interestingly, our extensive evaluation also indicates that the new schemes offer non-trivial improvements on system reliability over the existing RA-PM schemes as well. © 2013 ACM.",DVS; Real-time systems; Reliability-aware power management,Energy efficiency; Energy utilization; Real time systems; Recovery; Reliability; Response time (computer systems); Directed acyclic graph (DAG); DVS; Dynamic voltage scaling; Earliest deadline first algorithm; Efficiency and reliability; Frequency assignments; Power managements; Precedence constraints; Voltage stabilizing circuits
T/t-diagnosability of regular graphs under the PMC model,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878506651&doi=10.1145%2f2442087.2442091&partnerID=40&md5=eb9a35f2af5e7cbcb0e0f9da7665b70e,"A system is t/t-diagnosable if, given any collection of test results, the faulty nodes can be isolated to within a set of at most t nodes provided that the number of faulty nodes does not exceed t. Given an N-vertex graph G that is regular with the common degree d and has no cycle of three or four vertices, this study shows that G is (2d- 2)/(2d- 2)-diagnosable if N ≥ 4d- 3 > 0. Based on this result, the t/t-diagnosabilities of several classes of graphs can be computed efficiently. © 2013 ACM.",Diagnosability; Multiprocessor systems; Pessimistic diagnosis strategy; The PMC model,Multiprocessing systems; Diagnosability; Faulty node; Multi processor systems; N-vertex graph; Pessimistic diagnosis strategy; PMC model; Regular graphs; Graph theory
Bonnroute: Algorithms and data structures for fast and good vlsi routing,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878500313&doi=10.1145%2f2442087.2442103&partnerID=40&md5=9afcec41131db8bae104ce496f0ad7f8,"We present the core elements of BonnRoute: advanced data structures and algorithms for fast and highquality routing in modern technologies. Global routing is based on a combinatorial approximation scheme for min-max resource sharing. Detailed routing uses exact shortest path algorithms, based on a shape-based data structure for pin access and a two-level track-based data structure for long-distance connections. All algorithms are very fast. Compared to an industrial router (on 32nm and 22nm chips), BonnRoute is over two times faster, has 5% less netlength, 20% less vias, and reduces detours by more than 90 %. © 2013 ACM.",Detailed routing; Global routing; Routing optimization,Algorithms; Algorithms and data structures; Approximation scheme; Detailed routing; Global routing; Long-distance connection; Modern technologies; Routing optimization; Shortest path algorithms; Data structures
Achieving autonomous power management using reinforcement learning,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878515224&doi=10.1145%2f2442087.2442095&partnerID=40&md5=1101cf53195b3e008cbee21cf7718b81,"System level power management must consider the uncertainty and variability that come from the environment, the application and the hardware. A robust power management technique must be able to learn the optimal decision from past events and improve itself as the environment changes. This article presents a novel on-line power management technique based on model-free constrained reinforcement learning (Q-learning). The proposed learning algorithm requires no prior information of the workload and dynamically adapts to the environment to achieve autonomous power management. We focus on the power management of the peripheral device and the microprocessor, two of the basic components of a computer. Due to their different operating behaviors and performance considerations, these two types of devices require different designs of Q-learning agent. The article discusses system modeling and cost function construction for both types of Q-learning agent. Enhancement techniques are also proposed to speed up the convergence and better maintain the required performance (or power) constraint in a dynamic system with large variations. Compared with the existing machine learning based power management techniques, the Q-learning based power management is more flexible in adapting to different workload and hardware and provides a wider range of power-performance tradeoff. © 2013 ACM.",Computer; Machine learning; Power management; Thermal management,Computer peripheral equipment; Computers; Energy management; Hardware; Industrial management; Learning algorithms; Learning systems; Reinforcement learning; Temperature control; Enhancement techniques; Environment change; Function construction; Operating behavior; Power management techniques; Power managements; System-level power management; Uncertainty and variability; Environmental management
A survey and taxonomy of on-chip monitoring of multicore systems-on-chip,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878484855&doi=10.1145%2f2442087.2442088&partnerID=40&md5=39b7c34d8d1882284c43699b1eec4097,"Billion transistor systems-on-chip increasingly require dynamic management of their hardware components and careful coordination of the tasks that they carry out. Diverse real-time monitoring functions assist towards this objective through the collection of important system metrics, such as throughput of processing elements, communication latency, or resource utilization for each application. The online evaluation of these metrics can result in localized or global decisions that attempt to improve aspects of system behavior, system performance, quality-of-service, power and thermal effects under nominal conditions. This work provides a comprehensive categorization of monitoring approaches used in multiprocessor SoCs. As adaptive systems are encountered in many disciplines, it is imperative to present the prominent research efforts in developing online monitoring methods. To this end we offer a taxonomy that groups strongly related techniques that designers increasingly use to produce more efficient and adaptive chips. The provided classification helps to understand and compare architectural mechanisms that can be used in systems, while one can envisage the innovations required to build real adaptive and intelligent systems-on-chip. © 2013 ACM.",Adaptive SoC; Diagnosis; Dynamic management; Fault tolerance; Multicore; Network-on-chip; On-chip monitoring; On-line debugging; Proactive SoC management; Reconfiguration; Run-time optimization; Security,Adaptive systems; Diagnosis; Fault tolerance; Intelligent systems; Program diagnostics; Quality of service; VLSI circuits; Adaptive SoC; Dynamic management; Multi core; Network on chip; On-chip monitoring; Reconfiguration; Runtime optimization; Security; Taxonomies
"IC Power delivery: Voltage regulation and conversion, system-level cooptimization and technology implications",2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878494126&doi=10.1145%2f2442087.2442100&partnerID=40&md5=5d2dae1dabf1dc1f6e6f0bf854d20297,"Modern IC power delivery systems encompass large on-chip passive power grids and active on-chip or off-chip voltage converters and regulators. While there exists little work targeting on holistic design of such complex IC subsystems, the optimal system-level design of power delivery is critical for achieving power integrity and power efficiency. In this article, we conduct a systematic design analysis on power delivery networks that incorporate Buck Converters (BCs) and on-chip Low-Dropout voltage regulators (LDOs) for the entire chip power supply. The electrical interactions between active voltage converters, regulators as well as passive power grids and their influence on key system design specifications are analyzed comprehensively. With the derived design insights, the system-level codesign of a complete power delivery network is facilitated by a proposed automatic optimization flow in which key design parameters of buck converters and on-chip LDOs as well as on-chip decoupling capacitance are jointly optimized. The experimental results demonstrate significant performance improvements resulted from the proposed system cooptimization in terms of achievable area overhead, supply noise and power efficiency. Impacts of different decoupling capacitance technologies are also investigated. © 2013 ACM.",Cooptimization; DC-DC conversion; Distributive voltage regulation; Power delivery network,Complex networks; DC-DC converters; Electric current regulators; Electric power distribution; Integrated circuit testing; Optimization; Voltage regulators; Co-optimization; Dc-Dc conversion; De-coupling capacitance; Electrical interaction; Low-dropout voltage regulators; Performance improvements; Power delivery network; Power delivery systems; Electric power transmission
Power-safe application of TDF patterns to flip-chip designs during wafer test,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897015850&doi=10.1145%2f2491477.2491487&partnerID=40&md5=948e412fdc13cdeb17f43c725389b770,"Due to high switching activities in test mode, circuit power consumption is higher than its functional operation. Large switching in the circuit during launch-to-capture cycles not only negatively impacts circuit performance causing overkill, but could also burn tester probes during wafer test due to the excessive current they must drive. It is necessary to develop a quick and effective method for evaluating each pattern, identifing high-power patterns considering functional and tester probes' current limits and making the final pattern set power-safe. Compared with previous low-power methods that deal with scan structure modification or pattern filling techniques, the new proposed method takes into account layout information and resistance in the power distribution network and can identify peak current among C4 power bumps. Post-processing steps replace power-unsafe patterns with low-power ones. The final pattern set provides considerable peak current reduction while fault coverage is maintained. © 2013 ACM.",Flip-chip design; Layout-aware; Low-power test; Peak current; Transition delay faults,Electric network analysis; Flip chip devices; Probes; Flip chip; Layout-aware; Low-power tests; Peak currents; Transition delay faults; Low power electronics
Test compaction for small-delay defects using an effective path selection scheme,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896981184&doi=10.1145%2f2491477.2491488&partnerID=40&md5=b2ab11624a1f10ef3b35974b11a9eb14,"Testing for small-delay defects (SDDs) requires fault-effect propagation along the longest testable paths. However, identification of the longest testable paths requires high CPU time, and the sensitization of all such paths leads to large pattern counts. Dynamic test compaction for small-delay defects is therefore necessary to reduce test-data volume. We present a new technique for identifying the longest testable paths through each gate in order to accelerate test generation for SDDs. The resulting test patterns sensitize the longest testable paths that pass through each SDD site. An efficient dynamic test compaction method based on structural analysis is presented to reduce the pattern count substantially, while ensuring that all the longest paths for each SDD are sensitized. Simulation results for a set of ISCAS 89 and IWLS 05 benchmark circuits demonstrate the effectiveness of this method. © 2013 ACM.",Broadside scan testing; Critical testable path; Dynamic compaction; Selected testable path circuit; Small-delay defects,Compaction; Software testing; Accelerate test; Benchmark circuit; Critical testable path; Dynamic compaction; Dynamic test compaction; Scan testing; Test Compaction; Test-data volume; Electron device testing
Routability optimization for crossbar-switch structured ASIC design,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896972915&doi=10.1145%2f2491477.2491483&partnerID=40&md5=128d6a50a06910db791387a053b00727,"In the routing architecture of a structured application-specific integrated circuit (ASIC), the crossbar is one of the most area-efficient switch blocks. Nevertheless, a dangling wire occurs when there is a routing bend in a crossbar switch. Dangling wires incur longer wire lengths as well as a higher interconnection capacitance. In this article, we tackle dangling wire issues for structured ASIC routability optimization. We first propose a compact graph model for crossbar-switch routing. With our graph model, switch connectivity relations can be removed to keep the 2D structured ASIC routing graph efficient and to speed up the runtime of our routing algorithm. Furthermore, we propose a heuristic dangling-wire-avoidance routing framework containing deferred pin assignment, Steiner point reassignment, and anchor pair insertion in order to minimize dangling wires and channel width. Finally, in order to take routing bends and channel width into account simultaneously, we propose concurrent and sequential integer linear programming (ILP) formulations and ILP variable/constraint degeneration techniques. The experimental results demonstrate that our proposed heuristic routing framework reduces dangling wires by 19%, channel width by 38%, and wire length by 13% to VPR using the crossbar switch (VPR-C). In addition, our sequential ILP router reduces dangling wires by 38%, channel width by 40%, and wire length by 15% compared to VPR-C. Thus, the runtime efficiency of our sequential ILP router is attractive for crossbar-switch structured ASIC routing. © 2013 ACM.",Crossbar switch; Structured ASIC,Application specific integrated circuits; Crossbar equipment; Electronic equipment; Graph theory; Integer programming; Integrated circuits; Optimization; Switching networks; Crossbar switch; Interconnection capacitance; Routing architecture; Routing frameworks; Run-time efficiency; Sequential integer linear programming; Structured ASICs; Switch connectivity; Wire
Analysis and minimization of power-transmission loss in locally daisy-chained systems by local energy buffering,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896922649&doi=10.1145%2f2491477.2491481&partnerID=40&md5=e1ee5662b4c9aabdcd0e25e12fd74f0d,"Power-transmission loss can be a severe problem for low-power embedded systems organized in a daisy-chain topology. The loss can be so high that it can result in failure to power the load in the first place. The first contribution of this article is a recursive algorithm for solving the transmission current on each segment of the daisy chain at a given supply voltage. It enables solving not only the transmission loss but also reports infeasible configurations if the voltage is too low. Using this core algorithm, our second contribution is to find energy-efficient configurations that use local energy buffers (LEBs) to eliminate peak load on the bus without relying on high voltage. Experimental results confirm that our proposed techniques significantly reduce the total energy consumption and enable the deployed system to operate for significantly longer. © 2013 ACM.",Daisy-chaining; Distribution; Embedded systems; Energy storage; Power management; Sensors,Algorithms; Embedded systems; Energy storage; Energy utilization; Sensors; Daisy-chaining; Deployed systems; Distribution; Energy efficient; Power managements; Recursive algorithms; Total energy consumption; Transmission loss; Wave transmission
How to efficiently implement dynamic circuit specialization systems,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895471886&doi=10.1145%2f2491477.2491479&partnerID=40&md5=cbc1563fb049ca86f27da985d1bfa8b1,"Dynamic circuit specialization (DCS) is a technique used to implement FPGA applications where some of the input data, called parameters, change slowly compared to other inputs. Each time the parameter values change, the FPGA is reconfigured by a configuration that is specialized for those new parameter values. This specialized configuration is much smaller and faster than a regular configuration. However, the overhead associated with the specialization process should be minimized to achieve the desired benefits of using the DCS technique. This overhead is represented by both the FPGA resources needed to specialize the FPGA at runtime and by the specialization time. The introduction of parameterized configurations [Bruneel and Stroobandt 2008] has improved the efficiency of DCS implementations. However, the specialization overhead still takes a considerable amount of resources and time. In this article, we explore how to efficiently build DCS systems by presenting a variety of possible solutions for the specialization process and the overhead associated with each of them. We split the specialization process into two main phases: the evaluation and the configuration phase. The PowerPC embedded processor, the MicroBlaze, and a customized processor (CP) are used as alternatives in the evaluation phase. In the configuration phase, the ICAP and a custom configuration interface (SRL configuration) are used as alternatives. Each solution is used to implement a DCS system for three applications: an adaptive finite impulse response (FIR) filter, a ternary content-addressable memory (TCAM), and a regular expression matcher (RegEx). The experiments show that the use of our CP along with the SRL configuration achieves minimum overhead in terms of resources and time. Our CP is 1.8 and 3.5 times smaller than the PowerPC and the area-optimized implementation of the MicroBlaze, respectively. Moreover, the use of the CP enables a more compact representation for the parameterized configuration in comparison to both the PowerPC and the MicroBlaze processors. For instance, in the FIR, the parameterized configuration compiled for our CP is 6-7 times smaller than that for the embedded processors. © 2013 ACM.",Boolean Network evaluation; Dynamic circuit specialization; FPGA; Runtime reconfiguration,Parameterization; Pattern matching; Boolean Networks; Compact representation; Dynamic Circuits; Embedded processors; Regular configuration; Regular expressions; Run time reconfiguration; Ternary content addressable memory; Field programmable gate arrays (FPGA)
Thread-based multi-engine model checking for multicore platforms,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896994751&doi=10.1145%2f2491477.2491480&partnerID=40&md5=413473613835d2ec2d0a7b7603a34628,"This article describes a multithreaded, portfolio-based approach to model checking, where multiple cores are exploited as the underlying computing framework to support concurrent execution of cooperative engines. We introduce a portfolio-based approach to model checking. Our portfolio is first driven by an approximate runtime predictor that provides a heuristic approximation to a perfect oracle and suggests which engines are more suitable for each verification instance. Scalability and robustness of the overall model-checking effort highly rely on a concurrent, multithreaded model of execution. Following similar approaches in related application fields, we dovetail data partitioning, focused on proving several properties in parallel, and engine partitioning, based on concurrent runs of different modelchecking engines competing for completion of the same problem. We investigate concurrency not only to effectively exploit several available engines, which operate independently, but also to show that a cooperative effort is possible. In this case, we adopt a straightforward, light-weight, model of inter-engine communication and data sharing. We provide a detailed description of the ideas, algorithms, and experimental results obtained on the benchmarks from the Hardware Model Checking Competition suites (HWMCC'10 and HWMCC'11). © 2013 ACM.",Model checking; Multi-engine verification; Multi-threaded verification,Engines; Application fields; Computing frameworks; Concurrent execution; Data partitioning; Heuristic approximations; Multi-core platforms; Multithreaded; Multithreaded model; Model checking
Employing circadian rhythms to enhance power and reliability,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897005539&doi=10.1145%2f2491477.2491482&partnerID=40&md5=b60760ea2173b9374b9df66fe5472356,"This article presents a novel scheme for saving architectural power by mitigating delay degradations in digital circuits due to bias temperature instability (BTI), inspired by the notion of human circadian rhythms. The method works in two alternating phases. In the first, the compute phase, the circuit is awake and active, operating briskly at a greater-than-nominal supply voltage which causes tasks to complete more quickly. In the second, the idle phase, the circuit is power-gated and put to sleep, enabling BTI recovery. Since the wakeful stage works at an elevated supply voltage, it results in greater aging than operation at the nominal supply voltage, but the sleep state involves a recovery that more than compensates for this differential. We demonstrate, both at the circuit and the architectural levels, that at about the same performance, this approach can result in appreciable BTI mitigation, thus reducing the guardbands necessary to protect against aging, which results in power savings over the conventional design. © 2013 ACM.",Aging guardbands; Architectures; BTI; Digital circuits; Low power; Low-power design; Power dissipation; Power gating; Supply voltage,Architecture; Digital circuits; Energy dissipation; Integrated circuits; BTI; Low Power; Low-power design; Power gatings; Supply voltages; Low power electronics
Dynamic programming-based runtime thermal management (DPRTM): An online thermal control strategy for 3D-NoC systems,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891763079&doi=10.1145%2f2534382&partnerID=40&md5=81313385f298f3fbe2cdfe3dd8673bd3,"Complex thermal behavior inhibits the advancement of three-dimensional (3D) very-large-scale-integration (VLSI) system designs, as it could lead to ultra-high temperature hotspots and permanent silicon device damage. This article introduces a new runtime thermal management strategy to effectively diffuse and manage heat throughout 3D chip geometry for a better throughput performance in networks on chip (NoC). This strategy employs a dynamic programming-based runtime thermal management (DPRTM) policy to provide online thermal regulation. Reactive and proactive adaptive schemes are integrated to optimize the routing pathways depending on the critical temperature thresholds and traffic developments. Also, when the critical system thermal limit is violated, an urgent throttling will take place. The proposed DPRTM is rigorously evaluated through cycle-accurate simulations, and results show that the proposed approach outperforms conventional approaches in terms of computational efficiency and thermal stability. For example, the system throughput using theDPRTM approach can be improved by 33% when compared to other adaptive routing strategies for a given thermal constraint. Moreover, the DPRTM implementation presented in this article demonstrates that the hardware overhead is insignificant. This work opens a new avenue for exploring the on-chip adaptability and thermal regulation for future large-scale and 3D many-core integrations. © 2013 ACM.",3D-IC; Adaptive routing; Dynamic programming; Networks on chip; Performance analysis; Thermal management; Traffic-thermal cosimulation,Computational efficiency; Network-on-chip; Online systems; Temperature control; Thermal management (electronics); Three dimensional integrated circuits; VLSI circuits; Adaptive routing; Cosimulation; Cycle-accurate simulation; Networks on chips; Performance analysis; Thermal control strategies; Thermal management strategy; Very large scale integrations (VLSI); Dynamic programming
An efficient method for analyzing on-chip thermal reliability considering process variations,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896950621&doi=10.1145%2f2491477.2491485&partnerID=40&md5=9d7ff969d837e5c1190510ae6a58cc7d,"This work provides an efficient statistical electrothermal simulator for analyzing on-chip thermal reliability under process variations. Using the collocation-based statistical modeling technique, first, the statistical interpolation polynomial for on-chip temperature distribution can be obtained by performing deterministic electrothermal simulation very few times and by utilizing polynomial interpolation. After that, the proposed simulator not only provides the mean and standard deviation profiles of on-chip temperature distribution, but also innovates the concept of thermal yield profile to statistically characterize the on-chip temperature distribution more precisely, and builds an efficient technique for estimating this figure of merit. Moreover, a mixed-mesh strategy is presented to further enhance the efficiency of the developed statistical electrothermal simulator. Experimental results demonstrate that (1) the developed statistical electrothermal simulator can obtain accurate approximations with orders of magnitude speedup over the Monte Carlo method; (2) comparing with a well-known cumulative distribution function estimation method, APEX [Li et al. 2004], the developed statistical electrothermal simulator can achieve 215× speedup with better accuracy; (3) the developed mixedmesh strategy can achieve an order of magnitude faster over our baseline algorithm and still maintain an acceptable accuracy level. © 2013 ACM.",Chip temperature; Electrothermal simulation; Process variation; Simulation; Thermal analysis; Thermal reliability,Estimation; Interpolation; Monte Carlo methods; Reliability; Simulators; Statistics; Temperature distribution; Thermoanalysis; Chip temperature; Electro-thermal simulation; Process Variation; Simulation; Thermal reliability; Electric heating
Order statistics for correlated random variables and its application to at-speed testing,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896924644&doi=10.1145%2f2491477.2491486&partnerID=40&md5=ad0902b578318c38f38246f7a9042bf8,"Although order statistics have been studied for several decades, most of the results are based on the assumption of independent and identically distributed (i.i.d.) random variables. In the literature, how to compute the mth order statistics of n correlated random variables is still a problem. This article proposes a recursive algorithm based on statistical min/max operations to compute order statistics for general correlated and not necessarily identically distributed random variables. The algorithm has an O(mn) time complexity and O(m+ n) space complexity. A binary tree-based data structure is further developed to allow selective update of the order statistics with O(nm2) time. As a vehicle to demonstrate the algorithm, we apply it to the path selection algorithm in at-speed testing. A novel metric multilayer process space coverage metric is proposed to quantitatively gauge the quality of path selection. We then show that such a metric is directly linked to the order statistics, and our recursive algorithm can thus be applied. By employing a branch-and-bound path selection algorithm with these techniques, this article shows that selecting an optimal set of paths for a multimillion-gate design can be performed efficiently. Compared to the state of the art, experimental results show both the efficiency of our algorithms and better quality of our path selection. © 2013 ACM.",At-speed testing; Order statistics; Path selection; Process space coverage,Random variables; At-speed testing; Identically distributed random variables; Order statistics; Path selection; Path selection algorithms; Recursive algorithms; Space coverage; Tree-based data structures; Algorithms
On bottleneck analysis in stochastic stream processing,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896990463&doi=10.1145%2f2491477.2491478&partnerID=40&md5=48911ea7e9005554b31df7af6920fba2,"Past improvements in clock frequencies have traditionally been obtained through technology scaling, but most recent technology nodes do not offer such benefits. Instead, parallelism has emerged as the key driver of chip-performance growth. Unfortunately, efficient simultaneous use of on-chip resources is hampered by sequential dependencies, as illustrated by Amdahl's law. Quantifying achievable parallelism in terms of provable mathematical results can help prevent futile programming efforts and guide innovation in computer architecture toward the most significant challenges. To complement Amdahl's law, we focus on stream processing and quantify performance losses due to stochastic runtimes. Using spectral theory of random matrices, we derive new analytical results and validate them by numerical simulations. These results allow us to explore unique benefits of stochasticity and show how and when they outweigh the costs for software streams. © 2013 ACM.",Random matrices; Stochasticity; Stream processing,Computer architecture; Mathematical programming; Analytical results; Bottleneck analysis; Random matrices; Sequential dependencies; Simultaneous use; Stochasticity; Stream processing; Technology scaling; Stochastic systems
Agglomerative-based flip-flop merging and relocation for signal wirelength and clock tree optimization,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896920737&doi=10.1145%2f2491477.2491484&partnerID=40&md5=33838757326be1c274007ae0b4500052,"In this article, we propose a flip-flop merging algorithm based on agglomerative clustering. Compared to previous state-of-the-art on flip-flop merging, our proposed algorithm outperforms that of Chang et al. [2010] and Wang et al. [2011] in all aspects, including number of flip-flop reductions, increase in signal wirelength, displacement of flip-flops, and execution time. Our proposed algorithm also has minimal disruption to original placement. In comparison with Jiang et al. [2011], Wang et al. [2011], and Chang et al. [2010], our proposed algorithm has the least displacement when relocating merged flip-flops. While previous works on flip-flop merging focus on the number of flip-flop reduction, we further evaluate the power consumption of clock tree after flip-flop merging. To further minimize clock tree wirelength, we propose a framework that determines a preferable location for relocated merged flip-flops for clock tree synthesis (CTS). Experimental results show that our CTS-driven flip-flop merging can reduce clock tree wirelength by an average of 7.82% with minimum clock network power consumption compared to all of the previous works. © 2013 ACM.",,Algorithms; Electric Circuits; Forestry; Algorithms; Electric clocks; Forestry; Merging; Microprocessor chips; Agglomerative clustering; Clock network; Clock tree; Clock tree optimization; Clock tree synthesis; Merging algorithms; Wire length; Flip flop circuits
Online thermal control methods for multiprocessor systems,2013,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872378667&doi=10.1145%2f2390191.2390197&partnerID=40&md5=30676105791684a280b2381b28e70f89,"With technological advances, the number of cores integrated on a chip is increasing. This in turn is leading to thermal constraints and thermal design challenges. Temperature gradients and hotspots not only affect the performance of the system but also lead to unreliable circuit operation and affect the lifetime of the chip. Meeting temperature constraints and reducing hotspots are critical for achieving reliable and efficient operation of complex multi-core systems. In this article, we analyze the use of four of the most promising families of online control techniques for thermal management of multiprocessors system-on-chip (MPSoC). In particular, in our exploration, we aim at achieving an online smooth thermal control action that minimizes the performance loss as well as the computational and hardware overhead of embedding a thermal management system inside the MPSoC. The definition of the optimization problem to tackle in this work considers the thermal profile of the system, its evolution over time, and current time-varying workload requirements. Thus, this problem is formulated as a finite-horizon optimal control problem, and we analyze the control features of different online thermal control approaches. In addition, we implemented the policies on an MPSoC hardware simulation platform and performed experiments on a cycle-accurate model of the eight-core Niagara multi-core architecture using benchmarks ranging from Web-accessing to playing multimedia. Results show different trade-offs among the analyzed techniques regarding the thermal profile, the frequency setting, the power consumption, and the implementation complexity. © 2012 ACM.",Convex optimization; DVFS; Hot spots; MPC; MPSoC; Online; Thermal management,Convex optimization; Economic and social effects; Hardware; Multiprocessing systems; Online systems; Optimal control systems; Optimization; System-on-chip; Temperature control; Thermal variables control; DVFS; Finite horizon optimal control; Hot spot; Implementation complexity; MPSoC; Multicore architectures; Online; Thermal management systems; Computer architecture
ECO cost measurement and incremental gate sizing for late process changes,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872386632&doi=10.1145%2f2390191.2390207&partnerID=40&md5=f14368ae33b71b0b3a76f573ee5a8117,"Changes in the manufacturing process parameters may create timing violations in a design, making it necessary to perform an engineering change order (ECO) to correct these problems.We present a framework for performing incremental gate sizing for process changes late in the design cycle, and a method for creating initial designs that are robust to late process changes. This includes a method for measuring and estimating ECO cost and for transforming these costs into linear programming optimization problems. In the case of ECOs, the method reduces ECO costs on average, by 89% in changed area compared to a leading commercial tool. Furthermore, the robust initial designs are, on average, 55% less likely to need redesign in the future. © 2012 ACM.",ECO; Gate sizing; Incremental algorithms; Linear programming,Design; Gates (transistor); Linear programming; Commercial tools; Design cycle; ECO; Eco-costs; Engineering change orders; Gate sizing; Incremental algorithm; Initial design; Linear programming optimization; Manufacturing process parameters; Process change; Costs
Using implications to choose tests through suspect fault identification,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872391657&doi=10.1145%2f2390191.2390205&partnerID=40&md5=54413c104fe6b28549cd99a02b2ec2f8,"As circuits continue to scale to smaller feature sizes, wearout and latent defects are expected to cause an increasing number of errors in the field. Online error detection techniques, including logic implication-based checker hardware, are capable of detecting at least some of these errors as they occur. However, recovery may be expensive, and the underlying problem may lead to multiple failures of a core over time. In this article, we will investigate the diagnostic capability of logic implications to identify possible failure locations when an error is detected online. We will then utilize this information to select a highly efficient test set that can be used to effectively test the identified suspect locations in both the failing core and in other identical cores in the system. © 2012 ACM.",Implications; Invariance; On-chip diagnosis; Online error detection; Suspect fault identification,Automation; Computer applications; Invariance; Diagnostic capabilities; Failure locations; Fault identifications; Feature sizes; Implications; Latent defects; Multiple failures; On chips; On-line error detection; Test sets; Error detection
A self-tuning design methodology for power-efficient multi-core systems,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872380863&doi=10.1145%2f2390191.2390195&partnerID=40&md5=f27e7c3d7a543ab54bf140a2b5479620,"This article aims to achieve computational reliability and energy efficiency through codevelopment of algorithms, device, and circuit designs for application-specific, reconfigurable architectures. The new methodology characterizes aging-switching activity and aging-supply voltage relationships that are applicable for minimizing power consumption and task execution efficiency in order to achieve low bit energy ratio (BER). In addition, a new dynamic management algorithm (DMA) is proposed to alleviate device degradation and to extend system lifespan. In contrast to traditional workload balancing schemes in which cores are regarded as homogeneous, the new algorithm ranks cores as 'highly competitive,' 'less competitive,' and 'not competitive' according to their various competitiveness. Core competitiveness is evaluated based upon their reliability, temperature, and timing requirements. Consequently, 'competitive' cores will take charge of the majority of the tasks at relatively high voltage/frequency without violating power and timing budgets, while 'not competitive' cores will have light workloads to ensure their reliability. The new approach combines intrinsic device characteristics (aging-switching activity and aging-supply voltage curves) into an integrated framework to achieve high reliability and low energy level with graceful degradation of system performance. Experimental results show that the proposed method has achieved up to 20% power reduction, with about 4% performance degradation (in terms of accomplished workload and system throughput), compared with traditional workload balancing methods. The new method also improves system mean-time-to-failure (MTTF) by up to 25%. © 2012 ACM.",Competitive index; Dynamic management algorithm; Multi-core systems; Negative bias temperature instability; Self-tuning design,Algorithms; Competition; Energy efficiency; Reconfigurable architectures; Reliability; Tuning; Competitive index; Dynamic management; Multi-core systems; Negative bias temperature instability; Selftuning; Integrated circuits
SEU fault evaluation and characteristics for SRAM-based fpga architectures and synthesis algorithms,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872408131&doi=10.1145%2f2390191.2390204&partnerID=40&md5=26fbee211a5b0cca48c983cead83bb59,"Reliability has become an increasingly important concern for SRAM-based field programmable gate arrays (FPGAs). Targeting SEU (single event upset) in SRAM-based FPGAs, this article first develops an SEU evaluation framework that can quantify the failure sensitivity for each configuration bit during design time. This framework considers detailed fault behavior and logic masking on a post-layout FPGA application and performs logic simulation on various circuit elements for fault evaluation. Applying this framework onMCNC benchmark circuits, we first characterize SEUs with respect to different FPGA circuits and architectures, for example, bidirectional routing and unidirectional routing. We show that in both routing architectures, interconnects not only contribute to the lion's share of the SEU-induced functional failures, but also present higher failure rates per configuration bits than LUTs. Particularly, local interconnect multiplexers in logic blocks have the highest failure rate per configuration bit. Then, we evaluate three recently proposed SEU mitigation algorithms, IPD, IPF, and IPV, which are all logic resynthesis-based with little or no overhead on placement and routing. Different fault mitigating capabilities at the chip level are revealed, and it demonstrates that algorithms with explicit consideration for interconnect significantly mitigate the SEU at the chip level, for example, IPV achieves 61% failure rate reduction on average against IPF with about 15%. In addition, the combination of the three algorithms delivers over 70% failure rate reduction on average at the chip level. The experiments also reveal that in order to improve fault tolerance at the chip level, it is necessary for future fault mitigation algorithms to concern not only LUT or interconnect faults, but also their interactions. We envision that our framework can be used to cast more useful insights for more robust FPGA circuits, architectures, and better synthesis algorithms. © 2012 ACM.",Architecture; FPGA; Interconnect; Routing; SER; Soft error; Synthesis,Algorithms; Architecture; Failure analysis; Fault tolerance; Field programmable gate arrays (FPGA); Logic circuits; Synthesis (chemical); Benchmark circuit; Chip-level; Circuit elements; Design time; Evaluation framework; Failure rate; Fault evaluation; FPGA applications; FPGA circuits; Functional failure; Interconnect; Interconnect faults; Logic blocks; Logic simulations; Placement and routing; Routing; Routing architecture; SER; Single event upsets; Soft error; SRAM-based FPGA; Synthesis algorithms; Unidirectional routing; Computer control systems
Hybrid nonvolatile disk cache for energy-efficient and high-performance systems,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872415325&doi=10.1145%2f2390191.2390199&partnerID=40&md5=94bd60c2350f35971f0c34322e5a809f,"NAND flash memory has been employed as disk cache in recent years. It has the advantages of high performance, low leakage power, and cost efficiency. However, flash memory's performance is limited by the inability of in-place updates, coarse access granularity, and a limited number of write/erase times. In this article, we propose a hybrid nonvolatile disk cache architecture for high-performance and energy-efficient systems, where the disk cache is implemented with a small-size phase change memory (PCM) and a largesize NAND flash memory. Compared with current flash memory-based disk cache, it has the following advantages. (1) System performance is improved as requests are carefully directed between PCM and flash memory; (2) the energy consumption of disk cache is substantially reduced with significant reduction of additional operations, such as garbage collections; (3) the efficiency of flash memory is improved with the reduction of write activities on flash memory; and (4) lifetime of NAND flash memory is increased with most of the write operations assigned to PCM, where PCM's lifetime is guaranteed to be longer than the lifetime of flash memory. Simulation results show that the proposed methods can substantially improve the system performance, energy consumption, and lifetime of the hybrid disk cache. © 2012 ACM.",Flash memory; Hybrid disk cache; Phase change memory,Energy efficiency; Energy utilization; NAND circuits; Phase change memory; Cost efficiency; Disk cache; Energy efficient; Garbage collection; High performance systems; In-place update; Low leakage power; NAND flash memory; Non-volatile; Write operations; Flash memory
Improving performance per watt of asymmetric multi-core processors via online program phase classification and adaptive core morphing,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872367186&doi=10.1145%2f2390191.2390196&partnerID=40&md5=609d5f65b6b4061ced7faad4405ff13d,"Asymmetric multi-core processors (AMPs) have been shown to outperform symmetric ones in terms of performance and performance/watt. Improved performance and power efficiency are achieved when the program threads are matched to their most suitable cores. Since the computational needs of a program may change during its execution, the best thread to core assignment will likely change with time. We have, therefore, developed an online program phase classification scheme that allows the swapping of threads when the current needs of the threads justify a change in the assignment. The architectural differences among the cores in an AMP can never match the diversity that exists among different programs and even between different phases of the same program. Consider, for example, a program (or a program phase) that has a high instruction-level parallelism (ILP) and will exhibit high power efficiency if executed on a powerful core. We can not, however, include such powerful cores in the designed AMP, since they will remain underutilized most of the time, and they are not power efficient when the programs do not exhibit a high degree of ILP. Thus, we must expect to see program phases where the designed cores will be unable to support the ILP that the program can exhibit. We, therefore, propose in this article a dynamic morphing scheme. This scheme will allow a core to gain control of a functional unit that is ordinarily under the control of a neighboring core during periods of intense computation with high ILP. This way, we dynamically adjust the hardware resources to the current needs of the application. Our results show that combining online phase classification and dynamic core morphing can significantly improve the performance/watt of most multithreaded workloads. © 2012 ACM.",Asymmetric multi-cores; Dynamic core morphing (DCM); Hardwareassisted core reconfiguration and thread scheduling,Embedded systems; Hardware; Java programming language; Program processors; Dynamic cores; Functional units; Hardware resources; Improving performance; Instruction-level parallelism; Morphing; Multi-core processor; Multi-cores; Multithreaded; Online programs; Phase classification; Power efficiency; Power efficient; Program phasis; Thread scheduling; Multicore programming
Discrete sizing for leakage power optimization in physical design: A comparative study,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872407103&doi=10.1145%2f2390191.2390206&partnerID=40&md5=368669211fd89d5c4c6c9ebe076f7591,"While sizing has been studied for over three decades, the absence of a common framework with which to compare methods has made progress difficult to measure. In this article, we compare popular sizing techniques in which gates are chosen from a discrete standard cell library and slew and interconnect effects are accounted for. The difference between sizing methods reduces from roughly 53% to 8% between best and worst case after slew propagation is taken into account. In our benchmarks, no one sizing technique consistently outperforms the others. © 2012 ACM.",Discrete sizing; Gate sizing algorithms; Greedy; Lagrangian relaxation; Leakage power optimization; Linear programming; Peephole; Sensitivity-based sizing,Electric batteries; Linear programming; Discrete sizing; Gate sizing; Greedy; LaGrangian relaxation; Leakage power optimization; Peephole; Sensitivity-based sizing; Gates (transistor)
Introduction to the special section on adaptive power management for energy and temperature-aware computing systems,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872403384&doi=10.1145%2f2390191.2390192&partnerID=40&md5=58fbcb84fc0e4e8cdcf1fadeaeff429d,[No abstract available],,
"Design of energy-efficient, adaptable throughput systems at near/sub-threshold voltage",2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872383537&doi=10.1145%2f2390191.2390194&partnerID=40&md5=67f0f4acd59197138deb07a5fea3ac7d,"Voltage scaling has been a prevalent method of saving energy for energy-constrained applications. However, current technology trends which shrink transistors sizes exacerbate process variation effects in voltagescaled systems. Large variations in transistor parameters result in high variation in performance and power across the chip. These effects, if ignored at the design, stage, will result in unpredictable behavior when deployed in the field. In this article, we leverage the benefits of voltage scaling methodology for obtaining energy efficiency and compensate for the loss in throughput by exploiting parallelism present in the various DSP designs.We show that such a hybrid method consumes 8%-77% less power, compared to simple dynamic voltage scaling over different throughputs. We study this system architecture in two different workload environments: static and dynamic.We show that to achieve the highest level of energy efficiency, the number of cores and the operating voltages vary widely between a BASE design versus a process variation-aware (PVA) design. We further demonstrate that the PVA design enjoys an average of 26.9% and 51.1% reduction in energy consumption for the static and dynamic designs, respectively. Since different cores will have a wide range of speeds at operating voltages close to near/sub-thresholds due to process variation, we gather characteristic behavior of each core. With knowledge of the core speeds, we can further increase the energy efficiency. Furthermore, in this article, we show that of this methodology will be 49.3% more energy efficient, compared to that building the system with no knowledge about the characteristics of each core. © 2012 ACM.",45 nm; Active unit scaling; DVFS; Energy-efficient; Low-power; Multicore; Process variation; Voltage scaling,Energy efficiency; Throughput; 45 nm; Active unit scaling; DVFS; Energy efficient; Low Power; Multi core; Process Variation; Voltage-scaling; Design
Thermal prediction and adaptive control through workload phase detection,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872374788&doi=10.1145%2f2390191.2390198&partnerID=40&md5=2d8328761cf920abf2ac5485f2e3f57d,"Elevated die temperature is a true limiter to the scalability of modern processors. With continued technology scaling in order to meet ever-increasing performance demands, it is no longer cost effective to design cooling systems that handle the worst-case thermal behaviors. Instead, cooling systems are designed to handle typical chip operation, while processors must detect and handle rare thermal emergencies. Most processors rely on measurements from integrated thermal sensors and dynamic thermal management (DTM) techniques in order to manage the trade-off between performance and thermal risk. Optimal management requires advanced knowledge of the thermal trajectory based on the current workload behaviors and operating conditions. In this work, we devise novel workload phase classification strategies that automatically discriminate among workload behaviors with respect to the thermal control response. We incorporate workload phase-detection and thermal models into a dynamic voltage and frequency scaling (DVFS) technique that can optimally control temperature during runtime based on thermal predictions. We demonstrate the effectiveness of our proposed techniques in predicting and adaptively controlling the thermal behavior of a real quad-core processor in response to a wide range of workloads. In comparison with state-of-the-art model predictive control (MPC) techniques in previous works on thermal prediction, we demonstrate a 5.8% improvement in instruction throughput with the same number of thermal violations. In comparison with simple proportional-integral (PI) feedback control techniques, we improve instruction throughput by 3.9%, while significantly reducing the number of thermal violations. © 2012 ACM.",,Cooling systems; Forecasting; Model predictive control; Predictive control systems; Thermoelectric equipment; Adaptive Control; Control techniques; Control temperatures; Cost effective; Die temperatures; Dynamic thermal management; Dynamic voltage and frequency scaling; Modern processors; Operating condition; Optimal management; Phase classification; Phase detection; Proportional-integral; Runtimes; Technology scaling; Thermal behaviors; Thermal control; Thermal emergencies; Thermal model; Thermal prediction; Thermal sensors; Trajectory-based; Temperature control
Hierarchical power management for adaptive tightly-coupled processor arrays,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872392538&doi=10.1145%2f2390191.2390193&partnerID=40&md5=13c8f34f3f1006574cdfd8c80690fa9e,"We present a self-adaptive hierarchical power management technique for massively parallel processor architectures, supporting a new resource-aware parallel computing paradigm called invasive computing. Here, an application can dynamically claim, execute, and release the resources in three phases: resource acquisition (invade), program loading/configuration and execution (infect), and release (retreat). Resource invasion is governed by dedicated decentralized hardware controllers, called invasion controllers (iCtrls), which are integrated into each processing element (PE). Several invasion strategies for claiming linearly connected or rectangular regions of processing resources are implemented. The key idea is to exploit the decentralized resource management inherent to invasive computing for power savings by enabling applications themselves to control the power for processing resources and invasion controllers using a hierarchical power-gating approach. We propose analytical models for estimating various components of energy consumption for faster design space exploration and compare them with the results obtained from a cycle-accurate C++ simulator of the processor array. In order to find optimal design trade-offs, various parameters like (a) energy consumption, (b) hardware cost, and (c) timing overheads are compared for different sizes of power domains. Experimental results show significant energy savings (up to 73%) for selected characteristical algorithms and different resource utilizations. In addition, we demonstrate the accuracy of our proposed analytical model. Here, estimation errors less than 3.6% can be reported. © 2012 ACM.",Adaptive power optimization; Parallel computing; Resource awareness; Runtime resource management; Timing overhead minimization,Analytical models; Computer architecture; Energy utilization; Hardware; Models; Natural resources management; Parallel algorithms; Parallel processing systems; Resource allocation; Adaptive tightly-coupled; Cycle accurate; Design space exploration; Different sizes; Estimation errors; Hardware cost; Hierarchical power management; Massively parallel processors; Optimal design; Power domain; Power Optimization; Power savings; Power-gating; Processing elements; Processing resources; Processor array; Rectangular regions; Resource acquisition; Resource aware; Resource awareness; Resource management; Resource utilizations; Self-adaptive; Three phasis; Parallel architectures
Concurrency-aware compiler optimizations for hardware description languages,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872422065&doi=10.1145%2f2390191.2390201&partnerID=40&md5=73ca4338be860878159518ad13225992,"In this article, we discuss the application of compiler technology for eliminating redundant computation in hardware simulation. We discuss how concurrency in hardware description languages (HDLs) presents opportunities for expression reuse across different threads. While accounting for discrete event simulation semantics, we extend the data flow analysis framework to concurrent threads. In this process, we introduce a rewriting scheme named ?VF and a graph representation to model sensitivity relationships among threads. An algorithm for identifying common subexpressions as applied to HDLs is presented. Related issues, such as scheduling correctness, are also considered. © 2012 ACM.",Common sub-expression elimination; VHDL verilog hardware verification,Discrete event simulation; Hardware; Program compilers; Semantics; Common subexpression elimination; Compiler optimizations; Compiler technology; Concurrent threads; Graph representation; Hardware simulation; Hardware verification; Model sensitivity; Redundant computation; Sub-expressions; Computer hardware description languages
Verification work reduction methodology in low-power chip implementation,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872405206&doi=10.1145%2f2390191.2390203&partnerID=40&md5=fd9b939b45ad5149118d9c55a4cf01c4,"In order to achieve satisfactory verification for complicated low-power demands in green products, we propose a verification work reduction methodology. It consists of three step, namely virtual, direct actual, and actual model simulations. Virtual low-power simulation inserts low-power cells, such as isolators or level shifters, virtually and simulates logical behavior for design under test (DUT) based on user-defined power mode. Direct actual low-power simulation replaces behavior models without non-logical pins for some of modules with actual models with non-logical pins, which are Vdd and Gnd, and simulates DUT in mixed level. Actual low-power simulation simulates DUT by using actual models with non-logical pins for all cells and hard macros. We introduce techniques which classify the type of the bugs on which we focus at each verification step and prevent the concerned bugs from leaking to the latter verification step as much as possible. We applied our methodology to an actual chip and could reduce the total simulation period until tape-out by 38.8% and the total chip development period by 10%, compared with the conventional methodology. © 2012 ACM.",Logic simulation; Low power; Power intent; Power shutdown; Verification efficiency,Automation; Computer applications; Actual chips; Behavior model; Chip implementation; Design under tests; Green products; Hard macro; Level shifter; Logic simulations; Logical behavior; Low Power; Mixed levels; Model simulation; Power intent; Power modes; Power shutdown; Computer simulation
Compiler-in-the-loop exploration during datapath synthesis for higher quality delay-area trade-offs,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872381353&doi=10.1145%2f2390191.2390202&partnerID=40&md5=717cb8f4c0e5a34ee9b77bfef8cc55d9,"Design space exploration during high-level synthesis targets the computation of those design solutions which form optimal trade-off points. This quest for optimal trade-offs has been focused on studying the impact of various architectural-level parameters during high-level synthesis algorithms, silently neglecting the tradeoffs produced from the combined impact of behavioral-level together with architectural-level parameters. We propose a novel design space, exploration methodology that studies an extended instance of the solution space considering the effects of combining compiler- and architectural-level transformations. It is shown that exploring the design space in a global manner reveals new trade-off points, thus shifting towards higher quality design solutions. We use a combination of upper-bounding conditions together with gradient-based heuristic pruning to efficiently traverse the extended search space. Our exploration framework delivers significant quality improvements without compromising the optimality (Pareto accuracy) of the discovered solutions, together with significant runtime reductions compared to exploring exhaustively the solution space at every allocation scenario. © 2012 ACM.",Delay-area trade-offs; Design space exploration; High-level synthesis,Design; Optimization; Program compilers; Data paths; Delay-area trade-offs; Design solutions; Design space exploration; Design spaces; Gradient based; High Level Synthesis; Novel design; Optimality; Quality design; Quality improvement; Runtimes; Search spaces; Solution space; Trade-off point; Economic and social effects
Accelerating throughput-aware runtime mapping for heterogeneous MPSoCs,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872421259&doi=10.1145%2f2390191.2390200&partnerID=40&md5=5d8c8b23b8d7e74574fcf54291678090,"Modern embedded systems need to support multiple time-constrained multimedia applications that often employ multiprocessor-systems-on-chip (MPSoCs). Such systems need to be optimized for resource usage and energy consumption. It is well understood that a design-time approach cannot provide timing guarantees for all the applications due to its inability to cater for dynamism in applications. However, a runtime approach consumes large computation requirements at runtime and hence may not lend well to constrained-aware mapping. In this article, we present a hybrid approach for efficient mapping of applications in such systems. For each application to be supported in the system, the approach performs extensive design-space exploration (DSE) at design time to derive multiple design points representing throughput and energy consumption at different resource combinations. One of these points is selected at runtime efficiently, depending upon the desired throughput while optimizing for energy consumption and resource usage. While most of the existing DSE strategies consider a fixed multiprocessor platform architecture, our DSE considers a generic architecture, making DSE results applicable to any target platform. All the compute-intensive analysis is performed during DSE, which leaves for minimum computation at runtime. The approach is capable of handling dynamism in applications by considering their runtime aspects and providing timing guarantees. The presented approach is used to carry out a DSE case study for models of real-life multimedia applications: H.263 decoder, H.263 encoder, MPEG-4 decoder, JPEG decoder, sample rate converter, and MP3 decoder. At runtime, the design points are used to map the applications on a heterogeneous MPSoC. Experimental results reveal that the proposed approach provides faster DSE, better design points, and efficient runtime mapping when compared to other approaches. In particular, we show that DSE is faster by 83% and runtime mapping is accelerated by 93% for some cases. Further, we study the scalability of the approach by considering applications with large numbers of tasks. © 2012 ACM.",Design-space exploration; Embedded systems; Energy consumption; Multimedia applications; Multiprocessor systems-on-chip; Runtime mapping; Synchronous data-flow graphs; Throughput,Data flow analysis; Design; Embedded systems; Energy utilization; Fixed platforms; Microprocessor chips; Motion Picture Experts Group standards; Multiprocessing systems; Optimization; Space research; Throughput; Design space exploration; Multimedia applications; Multiprocessor systems on chips; Run-time mapping; Synchronous dataflow graphs; Mapping
Symbolic-event-propagation-based minimal test set generation for robust path delay faults,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870158357&doi=10.1145%2f2348839.2348851&partnerID=40&md5=f219c78112df6abf2ee20eb498255f62,We present a symbolic-event-propagation-based scheme to generate hazard-free tests for robust path delay faults. This approach identifies all robustly testable paths in a circuit and the corresponding complete set of test vectors. We address the problem of finding a minimal set of test vectors that covers all robustly testable paths. We propose greedy and simulated-annealing- based algorithms to find the same. Results on ISCAS89 benchmark circuits show a considerable reduction in test vectors for covering all robustly testable paths. © 2012 ACM.,Fault coverage; Minimal test set; Robust delay faults; Symbolic methods,Automation; Computer applications; Benchmark circuit; Delay faults; Fault coverages; Path delay fault; Symbolic methods; Test set generation; Test sets; Test vectors; Simulated annealing
Static NBTI reduction using internal node control,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870177833&doi=10.1145%2f2348839.2348849&partnerID=40&md5=cfc7bf8be89e296ad33951fa6970d828,"Negative Bias Temperature Instability (NBTI) is a significant reliability concern for nanoscale CMOS circuits. Its effects on circuit timing can be especially pronounced for circuits with standby-mode equipped functional units, because these units can be subjected to static NBTI stress for extended periods of time. This article describes Internal Node Control (INC), in which the inputs to some individual gates are directly manipulated to prevent this static NBTI fatigue. We prove that the INC selection problem is NP-complete and present a linear-time heuristic that can quickly determine near-optimal placements. This near-optimality is confirmed by comparing results for small benchmarks against optimal solutions from a mixed integer linear programming formulation of our problem. We evaluate the heuristic on the ISCAS85 benchmarks and the Synopsys DesignWare Library. Our heuristic reduces static NBTI-induced delay over a ten year period by 30-60% and can reduce total path delay by an average 9.4% when NBTI degradation is severe. The INC placements and sleep signal routing require only a 1.6% increase in area. © 2012 ACM.",Input vector control; Internal node control; NBTI; Negative bias temperature instability,CMOS integrated circuits; Negative temperature coefficient; Thermodynamic stability; Circuit timing; DesignWare; Functional units; Input vector control; Internal nodes; Mixed integer linear programming; Nanoscale CMOS; NBTI; Negative bias temperature instability; NP Complete; Optimal solutions; Path delay; Selection problems; Sleep signals; Synopsys; Integrated circuits
Buffer optimization and dispatching scheme for embedded systems with behavioral transparency,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870219157&doi=10.1145%2f2348839.2348845&partnerID=40&md5=98f6a0afc7e3b4e5b13592af46a7e4c7,"This article presents a buffer minimization scheme with low dispatching overhead for embedded software processes. To accomplish this, we exploit behavioral transparency in the model of computation. In such a model (e.g., synchronous dataflow), the state of buffer requirements is determined completely by the firing sequence of the actors without requiring functional simulation of the actors. Fine-grained buffer allocation incurs high and code pointer overhead while coarse-grained allocation suffers from memory fragmentation. Instead, we propose a medium-grained, ""access-contiguous"" buffer allocation scheme that minimizes the total buffer space and pointer overhead. We formulate the buffer allocation problem as 2D tiles that represent the lifetime of the buffers to minimize their memory occupation spatially and temporally. Experimental results show that our scheme uses less data memory than existing techniques by 26% on average, or up to 57% in the best case. Our technique retains code modularity for dynamic configuration and, more importantly, enables many more applications that otherwise would not fit if implemented using previous state-of-the-art techniques. © 2012 ACM.",Behavioral transparency; Buffer allocation; Component software; Memory optimization; Model of computation; Runtime system,Computer simulation; Data flow analysis; Embedded software; Optimization; Buffer allocation; Component software; Memory optimization; Model of computation; Runtime systems; Transparency
Synchronizing AMS assertions with AMS simulation: From theory to practice,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870175719&doi=10.1145%2f2348839.2348842&partnerID=40&md5=7095a81a056ab5a62df657c6f5eb26a0,"The verification community anticipates the adoption of assertions in the Analog and Mixed-Signal (AMS) domain in the near future. Several questions need to be answered before AMS assertions are brought into practice, such as: (a) How will the languages for AMS assertions be different from the ones in the digital domain? (b) Does the analog simulator have to be assertion aware? (c) If so, then how and where on the time line will the AMS assertion checker synchronize with the analog simulator? and (d) What will be the performance penalty for monitoring AMS assertions accurately over analog simulation? This article attempts to answer these questions through theoretical analysis and empirical results obtained from industrial test cases. We study logics which extend Linear Temporal Logic (LTL) with predicates over real variables, and show that further extensions allowing the binding of real-valued variables across time makes the logic undecidable. We present a toolkit which can integrate with existing AMS simulators for checking AMS assertions on practical designs. We study the problem of synchronizing the AMS simulator with the AMS assertion checker and demonstrate the performance penalty of different synchronization options. © 2012 ACM.",Mixed-signal; Satisfiability; Simulation; Temporal logic,Synchronization; Temporal logic; Across time; Analog simulations; Analog simulators; Assertion checkers; Digital domain; Industrial Test Case; Linear temporal logic; Mixed signal; Performance penalties; Real-valued variables; Satisfiability; Simulation; Time line; Simulators
Synthesis of adaptable hybrid adders for area optimization under timing constraint,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870227763&doi=10.1145%2f2348839.2348847&partnerID=40&md5=a995f058271be3261a34f3a0bae72f95,"Satisfying the timing constraint is the utmost concern in the integrated circuit design and it is true that most critical timing paths in a circuit cover one or more arithmetic components such as adder, subtractor, and multiplier of which addition logic is commonly involved. This work addresses the problem of redesigning the addition logic (in a form of hybrid adder) on a critical timing path to meet the timing constraint while minimally allocating the required addition logic. Unlike the conventional hybrid adder design schemes in which they assume uniform or specific patterns of input signal arrival times and minimize the latest timing of the output signals, our work extracts the required timing of each output signal as well as the input arrival times directly from the circuit and resynthesizes the addition logic by creating a customized hybrid adder that is best suited, in terms of logic area, for meeting the timing constraint of the circuit. Specifically, we propose a systematic approach of hybrid adder design exploration, basically following the principle of dynamic programming with well-controlled pruning techniques. This work is realistic and practically very useful in that it can be used as a timing optimizer to the computation-intensive circuits with a tight timing budget. We provide a set of diverse experimental data to show how much the proposed hybrid adder scheme is effective in meeting or reducing timing while maintaining the circuit area as minimal as possible. © 2012 ACM.",Arithmetic optimization; Hybrid adder; RTL resynthesis; Timing optimization,Adders; Logic circuits; Optimization; Area optimization; Arrival time; Hybrid Adder; Integrated circuit designs; Optimizers; Output signal; Pruning techniques; RTL resynthesis; Signal arrival time; Subtractor; Timing constraints; Timing optimization; Timing circuits
Conditional diagnosability of k-ary n-cubes under the PMC model,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870177215&doi=10.1145%2f2348839.2348850&partnerID=40&md5=ef03dd2ebcca4084bbd0d8a940adcc6c,"Processor fault diagnosis plays an important role in measuring the reliability of multiprocessor systems and the diagnosis of many well-known interconnection networks. The conditional diagnosability, which is more general than the classical diagnosability, is to measure the diagnosability of a multiprocessor system under the assumption that all of the neighbors of any node in the system cannot fail at the same time. This study shows that the conditional diagnosability for k-ary n-cubes under the PMC model is 8n-7 for k ≥ 4 and n ≥ 4. © 2012 ACM.",Conditional diagnosability; K-ary n-cubes; Multiprocessor systems; PMC diagnosis model,Fault detection; Fault tolerance; Geometry; Conditional diagnosability; Diagnosability; Diagnosis model; K-ary n-cubes; Multi processor systems; PMC model; Multiprocessing systems
Resource sharing of pipelined custom hardware extension for energy-efficient application-specific instruction set processor design,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870224387&doi=10.1145%2f2348839.2348843&partnerID=40&md5=19793e54b557df6de5835b990de95c7e,"Application-Specific Instruction set Processor (ASIP) has become an increasingly popular platform for embedded systems because of its high performance, flexibility, and short turn-around time. The hardware extension in ASIPs can speed-up program execution. However, it also incurs area overhead and extra static energy consumption. Traditional datapath merging techniques reduce the circuit overhead by reusing hardware modules for executing multiple operations. However, they introduce structural hazard for multiple custom instructions in sequence, and hence reduce the performance improvement. In this article, we introduce a pipelined configurable structure for the hardware extension in ASIPs, so that structural hazards can be remedied.With multiple subgraphs of operations selected, we design a novel operation-to-hardware mapping algorithm based on Integer Linear Programming (ILP) to automatically construct a resource-efficient pipelined configurable functional unit. Different resource sharing schemes would affect both the hardware overhead and the overall performance improvement. We analyze the design trade-offs between resource efficiency and performance improvement. At the end, we present our design space exploration results by setting the optimization objective to area, area and delay, and delay respectively. © 2012 ACM.",Application-specific instruction set processor; Configurable functional unit; Energy efficiency; Resource sharing,Conformal mapping; Embedded systems; Energy efficiency; Energy utilization; Hazards; Integer programming; Application-specific instruction set; Area overhead; Configurable functional units; Custom hardwares; Custom instruction; Data paths; Design space exploration; Design tradeoff; Energy efficient; Hardware extension; Hardware modules; Hardware overheads; Integer Linear Programming; Mapping algorithms; Merging techniques; Multiple operations; Performance improvements; Popular platform; Program execution; Resource efficiencies; Resource sharing; Resource sharing schemes; Structural hazards; Subgraphs; Hardware
Formal verification and debugging of precise interrupts on high performance microprocessors,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870169567&doi=10.1145%2f2348839.2348841&partnerID=40&md5=37c47c97d6a3f9dc339163690d55ae47,"The increased parallelism provided by Out-Of-Order (OOO) and superscalar mechanisms have made the control portion of advanced processors more complicated so that the state-of-the-art formal verification techniques for Register-Transfer-Level (RTL) and gate-level designs cannot scale to the complexity of such complicated processors. Moreover, verification and debugging of exceptions and external interrupts on such processors are nontrivial tasks. Because the exceptions arrival time, the external interrupt arrival time, as well as the microprocessor response time must be precise, verification and debugging require sophisticated hardware and software capabilities. This article proposes techniques for effective verification and debugging of cycle-accurate OOO processors in the event of exceptions and external interrupts. The results show that our techniques reduce the complexity of the verification and debugging processes by reducing the number of simulation cycles (3.3 × average reduction) and the number of state variables (8.7 × average reduction) to be traced for localizing bugs. © 2012 ACM.",Debugging; Formal verification; Out-of-order processors; Precise interrupt,Computer debugging; Design; Microprocessor chips; Verification; Arrival time; Cycle accurate; Debugging process; Formal verifications; Gate-level designs; Hardware and software; High-performance microprocessors; Non-trivial tasks; Number of state; Out of order; Out-of-order processors; Precise interrupt; Register transfer level; Superscalar; Program debugging
A hardware/software cooperative custom register binding approach for register spill elimination in application-specific instruction set processors,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870179882&doi=10.1145%2f2348839.2348844&partnerID=40&md5=416a87bfb59e058900d1a970f9324d9e,"Application-Specific Instruction set Processor (ASIP) has become an important design choice for embedded systems. It can achieve both high flexibility offered by the base processor core and high performance and energy efficiency offered by the dedicated hardware extensions. Although a lot of efforts have been devoted to computation acceleration, for example, automatic custom instruction identification and synthesis, limited on-chip data storage elements including the register file and data cache have become a potential performance bottleneck. For custom instructions that have more inputs and/or outputs than the generic register file I/O ports, custom registers are added in ASIPs to satisfy the need of additional inputs and outputs, and traditionally they are used only by custom instructions. In this article, we propose a hardware/software cooperative approach with a linear scan register allocation algorithm, which allows base instructions to utilize the existing custom registers in ASIPs for eliminating register spills of the program. The data traffic between the base processor and off-chip memory can be replaced with energy-efficient on-chip communications between the processor core and custom hardware extensions. Our experimental results demonstrate that a significant performance gain can be achieved, orthogonal to improvements by other techniques in ASIP design. © 2012 ACM.",Application-specific instruction set processor; Custom registers; Memory traffic reduction; Register spills,Energy efficiency; Hardware; Program processors; Application-specific instruction set; Custom hardwares; Custom instruction; Custom registers; Data caches; Data storage; Data traffic; Dedicated hardware; Energy efficient; Hardware/software; High flexibility; I/O ports; Off-chip memories; On chip communication; On chips; Performance bottlenecks; Performance Gain; Processor cores; Register allocation; Register binding; Register files; Traffic reduction; Embedded systems
Fast statistical full-chip leakage analysis for nanometer VLSI systems,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870222263&doi=10.1145%2f2348839.2348855&partnerID=40&md5=ce5d43e1ee72417a4a35fd56e1772b98,"In this article, we present a new full-chip statistical leakage estimation considering the spatial correlation condition (strong or weak). The new algorithm can deliver linear time, O(N), time complexity, where N is the number of grids on chip. The proposed algorithm adopts a set of uncorrelated virtual variables over grid cells to represent the original physical random variables and the cell size is determined by the spatial correlation length. In this way, each physical variable is always represented by virtual variables locally. We prove the number of neighbor cells for each grid cell is not related to the condition of spatial correlation (from no correlation to 100% correlated), which leads to linear time complexity in terms of number of gates. We compute the gate leakage by the orthogonal polynomials-based collocation method. The total leakage of a whole chip can be computed by simply summing up the coefficients of corresponding orthogonal polynomials in each grid cell. Furthermore, we develop a look-up table to cache statistical information for each type of gate instead of calculating leakage for every single instance of gate on a chip. As a result, a new statistical leakage characterization in Standard Cell Library (SCL) is put forward. Furthermore, an incremental analysis algorithm is proposed to update the chip-level statistical leakage information efficiently after a few changes are made. The proposed method has no restrictions on static leakage models, or types of leakage distributions. The large circuit examples in 45nm CMOS process demonstrate the proposed algorithm is 1000X faster than a recently proposed grid-based method with similar accuracy and many orders of magnitude times speedup over the Monte Carlo method. Experimental results also show the incremental analysis provides about 10X further speedup. We expect the incremental analysis could achieve more speedup over the full leakage analysis for larger problem sizes. © 2012 ACM.",Leakage power analysis; Spatial correlation; Statistical analysis,Algorithms; CMOS integrated circuits; Electric batteries; Monte Carlo methods; Nanoelectronics; Orthogonal functions; Statistical methods; Table lookup; Threshold voltage; Cell size; Chip-level; CMOS processs; Collocation method; Gate leakages; Grid cells; Grid-based methods; Incremental analysis; Large circuits; Leakage characterization; Leakage distribution; Leakage estimation; Leakage information; Leakage power analysis; Linear time; Linear time complexity; Look up table; Nanometer VLSI; Number of gates; Number of Grids; On chips; Orders of magnitude; Orthogonal polynomial; Physical variables; Problem size; Spatial correlations; Standard cell; Static leakage; Statistical information; Time complexity; Leakage currents
Migration-resistant policies for probe-wear leveling in MEMS storage devices,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870183422&doi=10.1145%2f2348839.2348853&partnerID=40&md5=0419c2f46b693c63c9b497e8645027f5,"Probes (read/write heads) in a MEMS storage device are susceptible to wear. We study probe wear, and analyze the causes of uneven wear. We show that under real-world workloads some probes can wear one order of magnitude faster than others. This premature expiry has severe consequences for reliability, timing performance, energy efficiency, and lifetime. Wear leveling precludes premature expiry and is thus necessary. We discuss the fundamental differences between probe wear in MEMS storage and medium wear in Flash, calling for a different treatment. We devise three policies to level probe wear. The policies provide a spectrum between best lifetime and least influence on the response time and energy efficiency of a MEMS storage device. We make the case that data migration can be prevented by augmenting the policies with a simple rule. We study the influence of the data layout configuration on the leveling performance of the policies. © 2012 ACM.",Lifetime; Probe data storage; Wear-leveling algorithms,Energy efficiency; Virtual storage; Data layouts; Data migration; Data storage; Lifetime; Simple rules; Timing performance; Wear leveling; Wear-leveling algorithms; Probes
An algorithm for jointly optimizing quantization and multiple constant multiplication,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870194039&doi=10.1145%2f2348839.2348846&partnerID=40&md5=9b4141a5d7085c262e241606e6b18d50,"This article presents a joint framework for quantization and Multiple Constant Multiplication (MCM) optimization, which yields a computationally efficient implementation of multiplierless multiplication in hardware and software. Frameworks of this nature have been developed in the context of Finite Impulse Response (FIR) filters, where frequency response specifications are used to drive the design. In this work, we look at a general case, considering as given a vector of ideal, real constants, which may come from any application and do not necessarily represent FIR filter coefficients. We first formulate a joint optimization problem for finding a fixed-point vector and a shift-add network that are optimal in terms of quantization error and MCM complexity. We then describe ways to finitize and prune the search space, leading to an efficient algorithm called JOINT SOLVE that solves the problem. Finally, via extensive randomized experiments, we show that our joint framework is notably more computationally efficient than a disjointed one, reducing the MCM cost by 15%-60% on moderate size problems. © 2012 ACM.",Digital circuits; Digital signal processing; Finite impulse response filters; Fixed-point arithmetic; Multiple constant multiplication; Multiplying circuits; Quantization,Algorithms; Computational efficiency; Digital circuits; Digital signal processing; Dividing circuits (arithmetic); FIR filters; Frequency response; Multiplying circuits; Signal processing; Computationally efficient; Filter coefficients; Hardware and software; Joint optimization; Multiple constant multiplications; Multiplierless; Quantization; Quantization errors; Real constants; Search spaces; Optimization
ECR: A powerful and low-complexity error cancellation rewiring scheme,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870222600&doi=10.1145%2f2348839.2348854&partnerID=40&md5=c172dbbaab43b665fc518be85f60e456,"Rewiring is known to be a class of logic restructuring technique that is at least equally powerful in flexibility compared to other logic transformation techniques. Especially it is wiring sensitive and is particularly useful for interconnect-based circuit synthesis processes. One of the most well-studied rewiring techniques is the ATPG-based Redundancy Addition and Removal (RAR) technique which adds a redundant alternative wire to make an originally irredundant target wire become redundant and thus removable. In this article, we propose a new Error-Cancellation-based Rewiring scheme (ECR) which can also identify non-RAR-based rewiring operations with high efficiency. In ECR scheme, it is not necessary for alternative wires to be redundant. Based on the notion of error cancellation, we analyze and reformulate the rewiring problem, and a more generalized rewiring scheme is developed to detect more rewiring cases which are not obtainable by existing schemes while it still maintains a low runtime complexity. Comparing with the most recent non-RAR rewiring tool IRRA, the total number of alternative wires found by our approach is about doubled (202%) while the CPU time used is just slightly more (8%) upon benchmarks preoptimized by ABC's rewriting. Our experimental results also suggest that the ECR engine is more powerful than IRRA in FPGA technology mapping. © 2012 ACM.",ATPG; Error cancellation; Rewiring,Automation; Computer applications; ATPG; Circuit synthesis; CPU time; Error cancellation; FPGA technology; Logic restructuring; Low-complexity; Rewiring; Run time complexity; Transformation techniques; Wire
MCEmu: A framework for software development and performance analysis of multicore systems,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870188304&doi=10.1145%2f2348839.2348840&partnerID=40&md5=2bce79944650e9d2f9d7addf1db2af78,"Developing software for heterogeneous multicore systems is particularly challenging even for experienced developers. While emulators have proven useful to application development, very few heterogeneous multicore emulators have been made available by vendors so far, as building an emulator for a heterogeneous multicore system has been a time-consuming and difficult task. Thus, we proposed a framework, called MCEmu, to speed up the process of building a heterogeneous multicore emulator by integrating existing and/or new processor emulators. MCEmu is designed to help system and application development, with a basic multicore board support package, an interprocessor communication library, and tools for debugging, tracing, and performance monitoring. In addition, MCEmu can run on a multicore host system to accelerate the emulation of data parallel applications. We show that MCEmu can be very useful for developing system software before the system becomes available, as it has helped us catch numerous functional and performance bugs which could have been hard to find. In this article, we present the design of MCEmu and demonstrate its capabilities with our case studies. © 2012 ACM.",Embedded systems; Heterogeneous multicore systems; Parallel simulation; Performance estimation,Computer simulation; Embedded systems; Application development; Board support packages; Data parallel; Help systems; Heterogeneous multicore; Inter processor communication; Multi core; Multi-core systems; Parallel simulations; Performance analysis; Performance estimation; Performance monitoring; System softwares; Program debugging
The synthesis of cyclic dependencies with boolean satisfiability,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870209092&doi=10.1145%2f2348839.2348848&partnerID=40&md5=662afe8648da20002e75e85259010c15,"The accepted wisdom is that combinational circuits must have acyclic (i.e., feed-forward) topologies. Yet simple examples suggest that this is incorrect. In fact, introducing cycles (i.e., feedback) into combinational designs can lead to significant savings in area and in delay. Prior work described methodologies for synthesizing cyclic circuits with Sum-Of-Product (SOP) and Binary-Decision Diagram (BDD)-based formulations. Recently, techniques for analyzing and mapping cyclic circuits based on Boolean satisfiability (SAT) were proposed. This article presents a SAT-based methodology for synthesizing cyclic dependencies. The strategy is to generate cyclic functional dependencies through a technique called Craig interpolation. Given a choice of different functional dependencies, a branch-and-bound search is performed to pick the best one. Experiments on benchmark circuits demonstrate the effectiveness of the approach. © 2012 ACM.",Boolean satisfiability; Circuit verification; Cyclic circuits; Logic design; Logic synthesis,Logic design; Benchmark circuit; Boolean satisfiability; Branch and bounds; Circuit verification; Craig interpolation; Cyclic circuits; Cyclic dependencies; Feed-Forward; Functional dependency; Logic synthesis; Sum of products; Combinatorial circuits
Targeted random test generation for power-aware multicore designs,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878512978&doi=10.1145%2f2209291.2209298&partnerID=40&md5=03d0f203306540298b2d8adff7888060,"Multicore Register Transfer Level (RTL) model simulations are indispensable in exposing subtle memory subsystem bugs. Validating memory consistency, coherency, and atomicity is a crucial design verification task. Random MultiProcessor (MP) test generators play critical roles in pre- and post-silicon validation. The Advanced Configuration and Power Interface (ACPI) standard supports dynamic frequency and voltage scaling by controlling performance states (P-States), yet multicore verification is generally conducted with cores operating at the P0-State. Independently varying core frequencies introduces new sets of intracore and intercore traffic latencies. The article introduces targeted random MP test generation techniques for multicore P-State functional verification. It develops a simple coverage metric to evaluate MP test effectiveness. The metric is demonstrated on MIP's instruction-set-based random MP tests. A novel technique is introduced to modulate the test address space by the spherical Bessel function. The technique delivers an order of magnitude coverage improvement over completely random tests. The article then outlines minimal P-State combinations to be exercised by MP tests. It also formulates two new methodologies to set up and apply MP tests for effective multicore P-State coverage. The methodologies are termed SimInit and SimTransition. First-level analyses indicate that these methods can deliver 97% to 100% improvement over random MP test coverage. © 2012 ACM.",ACPI; Chip multiprocessing; Design verification; Dynamic power management; Functional verification; Multiprocessing; P-states; Power management; Random test generation; Test generation,Computer simulation; Design; Energy management; Multiprocessing systems; ACPI; Chip multiprocessing; Design verification; Dynamic power management; Functional verification; Multiprocessing; P-states; Power managements; Random test generation; Test generations; Testing
Verification and coverage of message passing multicore applications,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878513737&doi=10.1145%2f2209291.2209296&partnerID=40&md5=1c1befe93157b9e285cdb33677a5d086,"We describe verification and coverage methods for multicore software that uses message passing libraries for communication. Specifically, we provide techniques to improve reliability of software using the new industry standard MCAPI by the Multicore Association. We develop dynamic predictive verification techniques that allow us to find actual and potential errors in a multicore software. Some of these error types are deadlocks, race conditions, and violation of temporal assertions. We complement our verification techniques with a mutation-testing-based coverage metric. Coverage metrics enable measuring the quality of verification tests. We implemented our techniques in tools and validated them on several multicore programs that use the MCAPI standard. We implement our techniques in tools and experimentally show the effectiveness of our approach. We find errors that are not found using traditional dynamic verification techniques and we can potentially explore execution schedules different than the original program with our coverage tool. This is the first time such predictive verification and coverage metrics have been developed for MCAPI. © 2012 ACM.",Coverage; Message passing communication; Multicore software; Mutation testing; Predictive verification,Communication; Errors; Software reliability; Coverage; Dynamic verifications; Message passing libraries; Message-passing communication; Multi core; Multi-core programs; Mutation testing; Verification techniques; Verification
Towards the formal verification of cache coherency at the architectural level,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878501175&doi=10.1145%2f2209291.2209293&partnerID=40&md5=c5b19c5a70fe041952efcba3dddfd62f,"Cache coherency is one of the major issues in multicore systems. Formal methods, in particular model-checking, have been successful at verifying high-level protocols, but, to the best of our knowledge, the verification of cache coherency at the architectural level is still an open issue. All existing verification efforts assume a reliable interconnect, that is, messages eventually reach their destination. We discuss the challenge of discharging this assumption at the architectural level where implementation details of the interconnect are mixed with a cache coherency protocol. Our automatic approach is based on a well-defined set of primitives to express architectural models, a generic model of communication fabrics expressed in an automated theorem proving system, and a dedicated algorithm for deadlock and livelock detection. We argue that reliability depends on the interaction between the interconnect and the cache coherency protocol. They must be verified altogether as their combination creates intricate message dependencies. We sketch our verification approach and apply it to a simple write-invalidate protocol on the Spidergon network-on-chip from STMicroelectronics. Our approach is promising. For this simple protocol, networks with tens of agents and hundreds of components can be analyzed within seconds. © 2012 ACM.",Cache coherency; Deadlock; Formal methods; Interactive theorem proving; Nocs,Formal methods; Model checking; VLSI circuits; Architectural levels; Automated theorem proving; Cache coherency; Cache coherency protocol; Deadlock; Formal verifications; Interactive theorem proving; Nocs; Theorem proving
Deterministic replay for message-passing-based concurrent programs,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878217311&doi=10.1145%2f2209291.2209295&partnerID=40&md5=c92b37f2d7188f89beb2b26a036d54bf,"The Multicore Communications API (MCAPI) is a new message-passing API that was released by the Multicore Association. MCAPI provides an interface designed for closely distributed embedded systems with multiple cores on a chip and/or chips on a board. Similar to parallel programs in other domains, debugging MCAPI programs is a challenging task due to their nondeterministic behavior. In this article we present a tool that is capable of deterministically replaying MCAPI program executions, which provides valuable insight for MCAPI developers in case of failure. © 2012 ACM.",Debugging; Deterministic replay; MCAPI; Message race; Multicore programs,Computer debugging; Message passing; Parallel architectures; Concurrent program; Deterministic replay; Distributed embedded system; MCAPI; Message race; Multi-core programs; Nondeterministic behavior; Program execution; Program debugging
Model-driven automation for simulation-based functional verification,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878478711&doi=10.1145%2f2209291.2209304&partnerID=40&md5=57e1cb9afb8a7179805b5241f3bb3e9f,"Developing testbenches for dynamic functional verification of hardware designs is a software-intensive process that lies on the critical path of electronic system design. The increasing capabilities of electronic components is contributing to the construction of complex verification environments that are increasingly difficult to understand, maintain, extend, and reuse across projects. Model-driven software engineering addresses issues of complexity, productivity, and code quality through the use of high-level system models and subsequent automatic transformations. Reasoning about verification testbench decomposition becomes simpler at higher levels of abstraction. In particular, the aspect-oriented paradigm, when applied at the model level, canminimize the overlap in functionality between modules, improving-maintainability and reusability. This article presents an aspect-oriented model-driven engineering process and toolset for the development of hardware verification testbenches. We illustrate how this process and toolset supports modularized design and automatic transformation to verification environment-specific models and source code through an industry case study. © 2012 ACM.",Aspectoriented; Code generation; E hardware verification language; Function verification; Model-based software engineering; Theme/UML,Hardware; Reusability; Software engineering; Aspect-oriented; Code Generation; Function verifications; Hardware verification languages; Model based software engineering; Theme/UML; Systems analysis
High-performance clock mesh optimization,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872318080&doi=10.1145%2f2209291.2209306&partnerID=40&md5=d8036d9e5e8d28a4b5029ebed20737fa,"Clock meshes are extremely effective at producing low-skew regional clock networks that are tolerant of environmental and process variations. For this reason, clock meshes are used in most high-performance designs, but this robustness consumes significant power. In this work, we present two techniques to optimize high-performance clock meshes. The first technique is a mesh perturbation methodology for nonuniform mesh routing. The second technique is a skew-aware buffer placement through iterative buffer deletion. We demonstrate how these optimizations can achieve significant power reductions and a near elimination of short-circuit power. In addition, the total wire length is decreased, the number of required buffers is decreased, and both skew and robustness are improved on average when variation is considered. © 2012 ACM.",Clock mesh optimization; Robust design,Iterative methods; Optimization; High-performance design; Mesh optimization; Non-uniform mesh; Power reductions; Process Variation; Robust designs; Short-circuit power; Total wire length; Clocks
Postscheduling buffer management trade-offs in streaming software synthesis,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877758026&doi=10.1145%2f2209291.2209300&partnerID=40&md5=1ede19a69c26e0f03c87d85b9013612e,"Streaming applications, which are abundant in many disciplines such as multimedia, networking, and signal processing, require efficient processing of a seemingly infinite sequence of input data. In the context of streaming software synthesis from data flow graphs, we study the inherent trade-off between memory requirement and compilation runtime, under a given task firing schedule. We utilize postscheduling analysis granularity to control the amount of details in characterization of buffer's spatio-temporal footprints. Subsequently, we transform the buffer allocation problem to two-dimensional packing of polygons, where complexity of the packing problem (e.g., polygon shapes) is determined by the analysis granularity. We develop an evolutionary packing optimization algorithm which readily yields buffer allocations. Experimental results highlight the trade-off between complexity of the analysis and the total buffer size of generated implementations. In addition, they show dramatic improvements in total buffer size, if one is willing to pay the additional cost in optimization runtime. © 2012 ACM.",Buffer management; Optimization; Postscheduling; Software synthesis; Streaming applications; Synchronous data flow,Algorithms; Application programs; Data flow analysis; Data flow graphs; Economic and social effects; Optimization; Signal processing; Buffer management; Post-scheduling; Software synthesis; Streaming applications; Synchronous data flow; Multitasking
Introduction to special section on verification challenges in the concurrent world,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878494279&doi=10.1145%2f2209291.2209292&partnerID=40&md5=fa86f30030f2251199d49de7e860b000,[No abstract available],,
A fast heuristic approach for parametric yield enhancement of analog designs,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878496735&doi=10.1145%2f2209291.2209308&partnerID=40&md5=cf18bf4a7b42781964b74fdb0bde5197,"In traditional yield enhancement approaches, a lot of computation efforts have to be paid first to find the feasible regions and the Pareto fronts, which will become a heavy cost for large analog circuits. In order to reduce the computation efforts, this article proposes a fast heuristic approach that tries to finish all iteration steps of the yield enhancement flow at behavior level. First, a novel force-directed Nominal Point Moving (NPM) algorithm is proposed to find a better nominal point without building the feasible regions. Then, an equation-based behavior-level sizing approach is proposed to map the NPM results at performance level to behavior-level parameters. A fast behavior-level Monte Carlo simulation is also proposed to shorten the iterative yield enhancement flow. Finally, using the obtained behavioral parameters as the sizing targets of each subblock, the device sizing time is significantly reduced instead of sizing from the system-level specifications directly. As demonstrated on several analog circuits, this heuristic approach could be another efficient methodology to help designers improve their analog circuits toward better yield. © 2012 ACM.",Analog circuits; Process variation; Yield enhancement,Analog circuits; Heuristic methods; Monte Carlo methods; Behavioral parameters; Computation effort; Heuristic approach; Parametric yield; Performance level; Process Variation; System-level specifications; Yield enhancement; Iterative methods
Fast poisson solvers for thermal analysis,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878494205&doi=10.1145%2f2209291.2209305&partnerID=40&md5=70f0445147e5c2b16d0680b933e91370,"Accurate and efficient thermal analysis for a VLSI chip is crucial, both for sign-off reliability verification and for design-time circuit optimization. To determine an accurate temperature profile, it is important to simulate a die together with its thermalmounts: this requires solving Poisson's equation on a nonrectangular 3D domain. This article presents a class of eigendecomposition- based Fast Poisson Solvers (FPS) for chip-level thermal analysis. We start with a solver that solves a rectangular 3D domain with mixed boundary conditions in O(N· logN) time, where N is the dimension of the finite difference matrix. Then we reveal, for the first time in the literature, a strong relation between fast Poisson solvers and Green-function-based methods. Finally, we propose an FPS method that leverages the preconditioned conjugate gradient method to solve nonrectangular 3D domains efficiently. We demonstrate this approach on thermal analysis of an industrial microprocessor, showing accurate results verified by a commercial tool, and that it solves a system of dimension 4.54e6 in only 13 conjugate gradient iterations, with a runtime of 65 seconds, a 15X speedup over the popular ICCG solver. © 2012 ACM.",Fast poisson solver; Green function; Thermal analysis,Conjugate gradient method; Green's function; Poisson equation; Three dimensional computer graphics; Circuit optimization; Conjugate gradient iteration; Eigen decomposition; Fast Poisson solver; Mixed boundary condition; Poisson's equation; Preconditioned conjugate gradient method; Temperature profiles; Thermoanalysis
Directed test generation for validation of multicore architectures,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864150696&doi=10.1145%2f2209291.2209297&partnerID=40&md5=66e7a643517c20391b343f7ce15c2404,"Functional validation is widely acknowledged as a major challenge for multicore architectures. Directed tests are promising since a significantly smaller number of directed tests can achieve the same coverage goal compared to constrained-random tests. SAT-based bounded model checking is effective for automated generation of directed tests (counterexamples). While existing approaches focus on clause forwarding between different bounds to reduce the test generation time, this article proposes a novel technique that exploits temporal, structural, and spatial symmetry in multicore designs at the same time. Our proposed technique enables the reuse of the knowledge learned from one core to the remaining cores in multicore architectures (structural symmetry), from one bound to the next for a give property (temporal symmetry), as well as from one property to other properties (spatial symmetry). The experimental results demonstrate that our approach can significantly (3-10 times) reduce overall test generation time compared to existing approaches. © 2012 ACM.",Bounded model checking; Multicore architecture; SAT solving; Test Generation,Software architecture; Automated generation; Bounded model checking; Functional validation; Multicore architectures; SAT-solving; Structural symmetry; Temporal symmetry; Test generations; Model checking
A3MAP: Architecture-aware analytic mapping for networks-on-chip,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877917626&doi=10.1145%2f2209291.2209299&partnerID=40&md5=48e17967cdc721464f49fcd49ca9ca62,"In this article, we propose novel and global Architecture-Aware Analytic MAPping (A3MAP) algorithms applied to Networks-on-Chip (NoCs) not only with homogeneous Processing Elements (PEs) on a regular mesh network as done by most previous application mapping algorithms but also with heterogeneous PEs on an irregular mesh or custom network. As the main contributions, we develop a simple yet efficient interconnection matrix that can easily model any core graph and network. Then, an application mapping problem is exactly formulated to Mixed Integer Quadratic Programming (MIQP). Since MIQP is NP-hard, we propose two effective heuristics, a successive relaxation algorithm achieving short runtime, called A3MAP-SR and a genetic algorithm achieving highmapping quality, called A3MAP-GA. We also propose a partition-based application mapping approach for large-scale NoCs, which provides better trade-off between performance and runtime. Experimental results show that A3MAP algorithms reduce total hop count, compared to the previous application mapping algorithms optimized for a regular mesh network, called NMAP [Murali and Micheli 2004] and for an irregular mesh and custom network, called CMAP [Tornero et al. 2008]. Furthermore, A3MAP algorithms make packets travel shorter distance than CMAP, which is related to energy consumption. © 2012 ACM.",Application mapping; Genetic algorithm; Homogeneous/heterogeneous processing element; Mixed integer quadratic programming; Network-on-chip; Successive relaxation algorithm,Conformal mapping; Energy utilization; Genetic algorithms; VLSI circuits; Application mapping; Mixed integer quadratic programming; Network on chip; Processing elements; Relaxation algorithm; Network architecture
A full lifecycle performance verification methodology for multicore systems-on-chip,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878488682&doi=10.1145%2f2209291.2209294&partnerID=40&md5=beb92001512ad1b93f18681c606bd503,"Multicore Systems-on-Chip (MCSoC) are comprised of a rich set of processor cores, specialized hardware accelerators, and I/O interfaces. Functional verification of these complex designs is a critical and demanding task, however, focusing only on functional verification is very risky because the motivation for building such systems in the first place is to achieve high levels of system throughput. Therefore a functionally correct MCSoC that does not exhibit sufficient performance will fail in the market. In addition, limiting performance verification efforts to analyzing individual system components in isolation is insufficient due to: (1) the degree of system-level resource contention that an application domain imposes on the MCSoC, and (2) the degree of configuration flexibility that is typically afforded by an MCSoC. These factors motivate system-level performance verification of MCSoC. This article presents an important industrial case study of MCSoC performance verification involving both pre- and postsilicon analysis, highlighting the methodology used, the lessons learned, and recommendations for improvement. © 2012 ACM.",Multicore; System-on-chip,Application specific integrated circuits; Industrial applications; Configuration flexibility; Functional verification; Industrial case study; Life-cycle performance; Multi core; Performance verification; System on chips; System-level performance; Microprocessor chips
Formal verification of code motion techniques using data-flow-driven equivalence checking,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878487106&doi=10.1145%2f2209291.2209303&partnerID=40&md5=21a29dedcda28e18b17d8e42f33f9abc,"A formal verification method for checking correctness of code motion techniques is presented in this article. Finite State Machine with Datapath (FSMD) models have been used to represent the input and the output behaviors of each synthesis step. The method introduces cutpoints in one FSMD, visualizes its computations as concatenation of paths from cutpoints to cutpoints, and then identifies equivalent finite path segments in the other FSMD; the process is then repeated with the FSMDs interchanged. Unlike many other reported techniques, the method is capable of verifying both uniform and nonuniform code motion techniques. It has been underlined in this work that for nonuniform code motions, identifying equivalent path segments involves model checking of some data-flow properties. Our method automatically identifies the situations where such properties are needed to be checked during equivalence checking, generates the appropriate properties, and invokes the model checking tool NuSMV to verify them. The correctness and the complexity of the method have been dealt with. Experimental results demonstrate the effectiveness of the method. © 2012 ACM.",Code motion; Equivalence checking; Formal verification; FSMD models; High-level synthesis; Model checking,Automation; Computer applications; Code motion; Equivalence checking; Formal verification methods; Formal verifications; High Level Synthesis; Model checking tools; Motion techniques; Path segments; Model checking
An ILP solution to address code generation for embedded applications on digital signal processors,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878472494&doi=10.1145%2f2209291.2209301&partnerID=40&md5=47fde84afc8d28ec3363936df2bd5a9d,"Digital Signal Processors (DSPs) are a family of embedded processors designed under tightmemory, area, and cost constraints. Many DSPs use irregular addressing modes where base-plus-offset mode is not supported. However, they often have Address Generation Units (AGUs) that can perform auto-increment/ decrement address arithmetic instructions in parallel with Load/Store instructions. This feature can be utilized to reduce the number of explicit address arithmetic instructions and thus reduce the embedded application code size. This code size reduction is essential for this family of DSP as the code usually resides in the ROM and hence the code size directly translates into silicon area. An effective technique for optimized code generation is offset assignment. This is a well-used technique in the literature to decrease the code size by finding an offset assignment that can effectively utilize auto-increment/decrement. This problem is known as simple offset assignment when there is only one address register and as General Offset Assignment (GOA) for multiple available address registers. In this article, we present an optimal Integer Linear Programming (ILP) solution to the offset assignment problem with variable coalescing where more than one variable can share the same memory location. Variable permutation is also formulated to find the best access sequence to achieve the best offset assignment that decreases the code size the most. Experimental results on several benchmarks show the effectiveness of our variable permutation technique as well as the large improvement from the ILP-based solutions compared to heuristics. © 2012 ACM.",AGU; DSP; General offset assignment; Simple offset assignment; Variable coalescing,Flocculation; Integer programming; Optimization; Signal processing; AGU; DSP; General offset assignment; Simple offset assignments; Variable coalescing; Network components
Load-balanced clock tree synthesis with adjustable delay buffer insertion for clock skew reduction in multiple dynamic supply voltage designs,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878207689&doi=10.1145%2f2209291.2209307&partnerID=40&md5=401af0b7b26a5534dc0f64f3005c5c2a,"Power consumption is known to be a crucial issue in current IC designs. To tackle this problem, Multiple Dynamic Supply Voltage (MDSV) designs are proposed as an efficient solution for power savings. However, the increasing variability of clock skew during the switching of power modes leads to an increase in the complication of clock skew reduction in MDSV designs. In this article, we propose a load-balanced clock tree synthesizer with Adjustable Delay Buffer (ADB) insertion for clock skew reduction in MDSV designs. The clock tree synthesizer adopts the Minimum Spanning Tree (MST) metric to estimate the interconnect capacitance and execute the graph-theoretic clustering. The power-mode-guided optimization is also embedded into the clock tree synthesizer for improving additional area overhead in the step of ADB insertion. After constructing the initial buffered clock tree, we insert the ADBs with delay value assignments to reduce clock skew in MDSV designs. The ADBs can be used to produce additional delays, hence the clock latencies and skew become tunable in a clock tree. An efficient algorithm of ADB insertion for the minimization of clock skew, area, and runtime in MDSV designs has been presented. Comparing with the state-of-the-art algorithm of ADB insertion, experimental results show maximum 42.40% area overhead improvement. With the power-mode-guided optimization, the maximum improvement of area overhead can increase to 47.87%. © 2012 ACM.",Adjustable delay buffer; Clock tree synthesis; Load balanced; Low power; Multiple dynamic supply voltage,Algorithms; Design; Graph theory; Light transmission; Optimization; Adjustable delay buffers; Clock tree synthesis; Load-balanced; Low Power; Supply voltages; Electric clocks
Divide and conquer high-level synthesis design space exploration,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878489070&doi=10.1145%2f2209291.2209302&partnerID=40&md5=59b3d6658992ae9717ffde4c695b458d,"A method to accelerate the Design Space Exploration (DSE) of behavioral descriptions for high-level synthesis based on a divide and conquer method called Divide and Conquer Exploration Algorithm (DC-ExpA) is presented. DC-ExpA parses an untimed behavioral description given in C or SystemC and clusters interdependent operations which are in turn explored independently by inserting synthesis directives automatically in the source code. The method then continues by combining the exploration results to obtain only Pareto-optimal designs. This method accelerates the design space exploration considerably and is compared against two previous methods: an Adaptive Simulated Annealer Exploration Algorithm (ASA-ExpA) that shows good optimality at high runtimes, and a pattern matching method called Clustering Design Space Exploration Acceleration (CDS-ExpA) that is fast but suboptimal. Our proposed method is orthogonal to previous exploration methods that focus on the exploration of resource constraints, allocation, binding, and/or scheduling. Our proposed method on contrary sets local synthesis directives that decide upon the overall architectural structure of the design (e.g., mapping certain arrays to memories or registers). Results show that DC-ExpA explores the design space on average 61% faster than ASA-ExpA, obtaining comparable results indicated by several quality indicators, for example, distance to reference Pareto-front, hypervolume, and Pareto dominance. Compared to CDS-ExpA it is 69% slower, but obtains much betters results compared to the same quality indicators. © 2012 ACM.",Acceleration; Design space exploration; High-level synthesis,Acceleration; Algorithms; Pattern matching; Architectural structure; Behavioral descriptions; Design space exploration; Divide and conquer methods; Exploration algorithms; High Level Synthesis; Pareto-optimal design; Resource Constraint; Pareto principle
Computer generation of hardware for linear digital signal processing transforms,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860271858&doi=10.1145%2f2159542.2159547&partnerID=40&md5=b77e2d102fef241c1b9dce3a9c159a9c,"Linear signal transforms such as the discrete Fourier transform (DFT) are very widely used in digital signal processing and other domains. Due to high performance or efficiency requirements, these transforms are often implemented in hardware. This implementation is challenging due to the large number of algorithmic options (e.g., fast Fourier transform algorithms or FFTs), the variety of ways that a fixed algorithm can be mapped to a sequential datapath, and the design of the components of this datapath. The best choices depend heavily on the resource budget and the performance goals of the target application. Thus, it is difficult for a designer to determine which set of options will best meet a given set of requirements. In this article we introduce the Spiral hardware generation framework and system for linear transforms. The system takes a problem specification as input as well as directives that define characteristics of the desired datapath. Using a mathematical language to represent and explore transform algorithms and datapath characteristics, the system automatically generates an algorithm, maps it to a datapath, and outputs a synthesizable register transfer level Verilog description suitable for FPGA or ASIC implementation. The quality of the generated designs rivals the best available handwritten IP cores. © 2012 ACM 1084-4309/2012/04-ART15 $10.00.",,Algorithms; Computer hardware description languages; Design; Digital signal processing; Discrete Fourier transforms; Hardware; Multiprocessing systems; Best choice; Computer generation; Data paths; Discrete fourier transform (DFT); Efficiency requirements; Fast Fourier transform algorithm; IP core; Linear signal transforms; Linear transform; Mathematical languages; Problem specification; Register transfer level; Resource budget; Target application; Transform algorithm; Verilog; Mathematical transformations
A yield and reliability improvement methodology based on logic redundant repair with a repairable scan flip-flop designed by push rule,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860266523&doi=10.1145%2f2159542.2159549&partnerID=40&md5=d89b5222f5f22ce6d7cc5b4b13b2d711,"We propose a yield improvement methodology which repairs a faulty chip due to logic defect by using a repairable scan flip-flop (R-SFF). Our methodology improves area penalty, which is a large issue for logic repair technology in actual products, by using repair grouping and a redundant cell insertion algorithm and by pushing the design rule for the repairable area of R-SFF. Additionally, compared with the conventional method, we reduce the number of wire connections around redundant cells by improving the replacement method of the faulty cell by the redundant cell. The proposed methodology reduces the total area penalty caused by the logic redundant repair to 3.6% and improves the yield, that is the number of good chips on a wafer, by 4.7% when the defect density is 1.0[1/cm and2]. Furthermore, we propose the strategy to repair the in-field failures due to latent defect for the chip whose repair function had not been used in the shipment test. © 2012 ACM 1084-4309/2012/04-ART17 $10.00.",,Cells; Cytology; Defects; Scanning; Area penalty; Conventional methods; Design rules; Faulty cells; In-field; Insertion algorithms; Latent defects; Reliability improvement; Repair functions; Repair technology; Replacement methods; Scan flip-flops; Yield Improvement; Repair
Timing analysis of system initialization and crash recovery for a segment-based flash translation layer,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860263536&doi=10.1145%2f2159542.2159546&partnerID=40&md5=30fd30147792ff1f9c1a2a030d736089,"Recently, the capacity of flash-memory storage systems has grown rapidly, and flash-memory technology has advanced along with the wave of consumer electronics and embedded systems. In order to properly manage product cost and initialization performance, vendors face serious challenges in system design and analysis. Thus, the timing analysis of system initialization and crash recovery for a segment-based flash translation layer has become an important research topic. This article focuses on system initialization, crash recovery, and timing analysis. The timing analysis of system initialization involves the relationship between the size of the main memory and the system initialization time. The timing analysis of crash recovery explains the worst case recovery time. The experiments in this study show that the timing analysis of system initialization and crash recovery can be applied to the segment-based flash translation layer. © 2012 ACM 1084-4309/2012/04-ART14 $10.00.",,Consumer electronics; Cost accounting; Embedded systems; Monolithic microwave integrated circuits; Design and analysis; Flash translation layer; Main memory; Product cost; Recovery time; Research topics; Segment-based; Storage systems; Timing Analysis; Recovery
Optimized 3D Network-on-Chip design using simulated allocation,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860306896&doi=10.1145%2f2159542.2159544&partnerID=40&md5=b16aa0527f27dbd9748bef50c4bc0b7b,"Three-dimensional (3D) silicon integration technologies have provided new opportunities for Network-on-Chip (NoC) architecture design in Systems-on-Chip (SoCs). In this article, we consider the applicationspecific NoC architecture design problem in a 3D environment. We present an efficient floorplan-aware 3D NoC synthesis algorithm based on simulated allocation (SAL), a stochastic method for traffic flow routing, and accurate power and delay models for NoC components. We demonstrate that this method finds greatly improved solutions compared to a baseline algorithm reflecting prior work. To evaluate the SAL method, we compare its performance with the widely used simulated annealing (SA) method and show that SAL is much faster than SA for this application, while providing solutions of very similar quality. We then extend the approach from a single-path routing to a multipath routing scheme and explore the trade-off between power consumption and runtime for these two schemes. Finally, we study the impact of various factors on the network performance in 3D NoCs, including the TSV count and the number of 3D tiers. Our studies show that link power and delay can be significantly improved when moving from a 2D to a 3D implementation, but the improvement flattens out as the number of 3D tiers goes beyond a certain point. © 2012 ACM 1084-4309/2012/04-ART12 $10.00.",,Algorithms; Microprocessor chips; Network performance; Simulated annealing; Three dimensional computer graphics; VLSI circuits; 3-D environments; Delay models; Multi-path routing schemes; Network-on-chip architectures; Network-on-chip design; NoC architectures; Runtimes; Silicon integration; Simulated allocation; Single-path routing; Stochastic methods; Synthesis algorithms; Systems on chips; Traffic flow; Three dimensional
Performance/thermal-aware design of 3D-stacked L2 caches for CMPs,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860295519&doi=10.1145%2f2159542.2159545&partnerID=40&md5=fc22a58548754af1dc3be2eaf9ca1fd0,"Three-dimensional (3D) stacking technology enables integration of more memory on top of chip multiprocessors (CMPs). As the number of cores and the capacity of on-chip memory increase, the Non-Uniform Cache Architecture (NUCA) becomes more attractive. Compared to 2D cases, 3D stacking provides more options for the design of on-chip memory due to numerous advantages, such as the extra layout dimension, low latency across layers, etc. On the other hand, 3D stacking aggravates the thermal problem due to the increase of power density. In this work, we first study the design of 3D-stacked set-associative L2 caches through managing the placement of cache ways. The evaluation results show that the placement and corresponding management of 3D cache ways have an impact on the performance of CMPs. Then, we show that the efficiency of thermal control is also related to the placement of cache ways. For caches implemented with different memory technologies, the placement and management of cache ways have different effects on power consumption and power distribution. Consequently, we propose techniques to improve the efficiency of thermal control for different memory technologies. The evaluation results show the trade-off between performance and thermal control efficiency. © 2012 ACM 1084-4309/2012/04- ART13 $10.00.",,Design; Efficiency; Temperature control; Three dimensional; 3D stacking; Chip Multiprocessor; Different effects; Evaluation results; L2 Cache; Low latency; Memory technology; Non-uniform cache architectures; On chip memory; Power densities; Power distributions; Set-associative; Stacking technology; Thermal control; Cache memory
An extended SystemC framework for efficient HW/SW co-simulation,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860295142&doi=10.1145%2f2159542.2159543&partnerID=40&md5=384c0de33559677df18c3fede03e6dcf,"In this article, we propose an extended SystemC framework that directly enables software simulation in SystemC. Although SystemC has been widely adopted for system-level simulation of hardware designs nowadays, to complete HW/SW co-simulation, it still requires an additional instruction set simulator (ISS) for software execution. However, the heavy intercommunication overheads between the two heterogeneous simulators would significantly slow down simulation performance. To deal with this issue, our proposed approach automatically generates high-speed and equivalent SystemC models for target software applications that can be directly integrated with hardware models for complete HW/SW co-simulation. In addition, to properly handle multitasking, an efficient OS model is devised to support accurate preemptive scheduling. Since both the generated application model and the OS model are constructed in SystemC modules, our approach avoids heavy intercommunication overheads and achieves over 1,000 times faster simulation than that of the conventional ISS-SystemC approach. Experimental results demonstrate that our extended SystemC approach can perform at 50 to 220 MIPS while offering accurate simulation results. © 2012 ACM 1084-4309/2012/04-ART11 $10.00.",,Computer software; Hardware; Application models; Hardware design; Hardware models; High-speed; HW/SW co-simulation; Instruction set simulators; Pre-emptive scheduling; Simulation performance; Software applications; Software execution; Software simulation; System level simulation; SystemC; Computer simulation
Scan flip-flop grouping to compress test data and compact test responses for launch-on-capture delay testing,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860266335&doi=10.1145%2f2159542.2159550&partnerID=40&md5=932f613d25f79615ad94d84da386630a,"Test data compression is a much more difficult problem for launch-on-capture (LOC) delay testing, because test data for LOC delay testing is much more than that of stuck-at fault testing, and LOC delay fault test generation in the two-frame circuit model can specify many more inputs. A new scan architecture is proposed to compress test stimulus data, compact test responses, and reduce test application time for LOC delay fault testing. The new scan architecture merges a number of scan flip-flops into the same group, where all scan flip-flops in the same group are assigned the same values for all test pairs. Sufficient conditions are presented for including any pair of scan flip-flops into the same group for LOC transition, non-robust path delay, and robust path delay fault testing. Test data for LOC delay testing based on the new scan architecture can be compressed significantly. Test application time can also be reduced greatly. Sufficient conditions are presented to construct a test response compactor for LOC transition, non-robust, and robust path delay fault testing. Folded scan forest and test response compactor are constructed for further test data compression. Sufficient experimental results are presented to show the effectiveness of the method. © 2012 ACM 1084-4309/2012/04- ART18 $10.00.",,Data compression; Scanning; Circuit models; Delay fault test; Delay testing; Delay-fault testing; Launch-on-capture; Non-robust path; Path delay fault testing; Scan architecture; Scan flip-flops; Scan forest; Stuck-at faults; Sufficient conditions; Test application time; Test data; Test Data Compression; Test response; Integrated circuit testing
Timing optimization in sequential circuit by exploiting clock-gating logic,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860279918&doi=10.1145%2f2159542.2159548&partnerID=40&md5=dec2aacfe4bc6f2bbc0da3fcf3be7eb8,"Clock gating is a popular technique for reducing power dissipation. In a circuit with clock gating, the clock signal can be shut off without changing the functionality under certain clock-gating conditions. In this article, we observe that the clock-gating conditions and the next-state function of a Flip-Flop (FF) are correlated and can be used for sequential circuit optimization. We also show that the implementation of the next-state function of any FF can be just an inverter if the clock signal is appropriately gated. By exploiting the flexibility between the clock-gating conditions and the next-state function, we propose an iterative optimization algorithm to improve the timing of sequential circuits. We present experimental results of a set of benchmark circuits with a timing improvement of 10.20% on average. © 2012 ACM 1084-4309/2012/04-ART16 $10.00.",,Algorithms; Electric clocks; Sequential circuits; Benchmark circuit; Circuit optimization; Clock gating; Clock signal; Iterative optimization algorithms; Next state functions; Timing optimization; Timing circuits
Error rate estimation for defective circuits via ones counting,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857810764&doi=10.1145%2f2071356.2071364&partnerID=40&md5=00d9467cc183867fa8c7f0e24d7b8359,"With VLSI circuit feature size scaling down, it is becoming more difficult and expensive to achieve a desired level of yield. Error-tolerance employs defective chips that occasionally produce erroneous yet acceptable results in targeted applications, and has been proposed as one way to increase effective yield. These chips are characterized by criteria set by specific applications. Error rate, an upper-bound on how frequent errors occur at an output, is one such criterion. In this article we focus on the following problem: given a combinational logic circuit that is defective, and hence occasionally produces an erroneous output, how can we determine the error rate of each output line by using ones counting? The results of this work can also be used for runtime error estimation in aging systems and in environments where soft-errors are produced. © 2012 ACM.",Binning integrated circuits; BIST; Error rate; Ones counting; Yield,Built-in self test; Logic circuits; VLSI circuits; Aging systems; Circuit features; Combinational logic circuits; Defective chips; Defective circuits; Error rate; Error rate estimation; Following problem; Ones counting; Output lines; Run-time errors; Soft error; Yield; Errors
System-level synthesis for wireless sensor node controllers: A complete design flow,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857878551&doi=10.1145%2f2071356.2071358&partnerID=40&md5=5b82207eca527721b7203489398062b7,"Wireless sensor networks (WSN) is a new and very challenging research field for embedded system design automation. Engineering a WSN node hardware platform is known to be a tough challenge, as the design must enforce many severe constraints, among which energy dissipation is by far the most important one. WSN node devices have until now been designed using off-the-shelf low-powermicrocontroller units (MCUs), even if their power dissipation is still an issue and hinders the widespread use of this new technology. In this work, we propose a complete system-level flow for an alternative approach based on the concept of hardware microtasks, which relies on hardware specialization and power gating to drastically improve the energy efficiency of the computational/control part of the node. Our case study shows that power savings between one to two orders of magnitude are possible w.r.t. MCU-based implementations. © 2012 ACM.",Design space exploration; Domain-specific language; Hardware specialization; Low-power design; Microcoded FSM; Power gating; System-level design-flow; WSN node controller,Computer aided design; Electric network synthesis; Embedded software; Energy dissipation; Energy efficiency; Problem oriented languages; Space research; Systems analysis; Design space exploration; Domain specific languages; Low-power design; Microcoded FSM; Node controllers; Power gatings; System levels; Sensor nodes
Launch-on-shift test generation for testing scan designs containing synchronous and asynchronous clock domains,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870205409&doi=10.1145%2f2348839.2348852&partnerID=40&md5=12f2c14a65f129c564efa0e4de6ef9b8,"This article presents a hybrid Automatic Test Pattern Generation (ATPG) technique using the staggered Launch-On-Shift (LOS) scheme followed by the one-hot launch-on-shift scheme for testing delay faults in a scan design containing asynchronous clock domains. Typically, the staggered scheme produces small test sets but needs long ATPG runtime, whereas the one-hot scheme takes short ATPG runtime but yields large test sets. The proposed hybrid technique is intended to reduce test pattern count with acceptable ATPG runtime for multimillion-gate scan designs. In case the scan design contains multiple synchronous clock domains, and each group of synchronous clock domains is treated as a clock group and tested using a launch-aligned or a capture-aligned LOS scheme. By combining these schemes together, we found the pattern counts for two large industrial designs were reduced by approximately 1.6X to 1.8X, while the ATPG runtime was increased by 40% to 50%, when compared to the one-hot clocking scheme alone. © 2012 ACM.",At-speed scan testing; Delay fault testing; Hybrid launch-on-shift,Electric clocks; Integrated circuit testing; Asynchronous clock domains; At-speed scan testing; Delay-fault testing; Hybrid techniques; Launch-on-shift; Staggered schemes; Synchronous clocks; Test generations; Automatic test pattern generation
Easy formal specification and validation of unbounded networks-on-chips architectures,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857873168&doi=10.1145%2f2071356.2071357&partnerID=40&md5=e58d7e65d9ac18226e491aa64fe4ca8e,"This article presents a formal specification and validation environment to prove safety and liveness properties of parametric - unbounded - NoCs architectures described at a high-level of abstraction. The environment improves the GeNoC approach with two new theorems, proving evacuation and starvation freedom. The application of the validation methodology is illustrated on a HERMES NoC with adaptive west-first routing and wormhole switching. This case study illustrates the strong compositional aspect of the GeNoC environment. The complete specification of this HERMES instance, together with the proof that the specification is deadlock-free, starvation free, and all messages eventually leave the network at their correct destination, could be achieved in about a week. Approximately 86% of this proof is automatically derived from the GeNoC model. © 2012 ACM.",Deadlock; Evacuation; Formal methods; Interactive theorem proving; NoCs; Starvation,Formal methods; Network architecture; Theorem proving; Deadlock; Evacuation; Interactive theorem proving; NoCs; Starvation; Specifications
Statistical soft error rate (SSER) analysis for scaled CMOS designs,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863265490&doi=10.1145%2f2071356.2071365&partnerID=40&md5=18c9e73877c38cc197fdd614cab756f5,"This article re-examines the soft error effect caused by radiation-induced particles beyond the deep submicron regime. Considering the impact of process variations, voltage pulse widths of transient faults are found no longer monotonically diminishing after propagation, as they were formerly. As a result, the soft error rates in scaled electronic designs escape traditional static analysis and are seriously underestimated. In this article we formulate the statistical soft error rate (SSER) problem and present two frameworks to cope with the aforementioned sophisticated issues. The table-lookup framework captures the change of transient-fault distributions implicitly by using a Monte-Carlo approach, whereas the SVR-learning framework does the task explicitly by using statistical learning theory. Experimental results show that both frameworks can more accurately estimate SERs than static approaches do. Meanwhile, the SVR-learning framework outperforms the table-lookup framework in both SER accuracy and runtime. © 2012 ACM.",Monte Carlo method; Soft error; Statistical learning; Support vector machine; Transient fault,Electric potential; Fault tree analysis; Monte Carlo methods; Deep sub-micron; Electronic design; MONTE CARLO; Process Variation; Radiation-induced; Runtimes; Scaled CMOS; Soft error; Soft error effects; Soft error rate; Static approach; Statistical learning; Statistical learning theory; Support vector machine (SVM); Transient fault; Transient faults; Voltage pulse; Table lookup
Compact modeling of interconnect circuits over wide frequency band by adaptive complex-valued sampling method,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863272103&doi=10.1145%2f2071356.2071361&partnerID=40&md5=b1580dc04c28d693bda3d0c89761b835,"In this article, we propose a new model order-reduction method for compact modeling of interconnect circuits over wide frequency band using a novel complex-valued adaptive sampling and error estimation scheme. We address the outstanding error control problems in the existing sampling-based reduction framework over a frequency band. Our new method, WBMOR, explicitly and efficiently computes the exact residual errors to guide the sampling process. We show by sampling along the imaginary axis and performing a new complexvalued reduction that the reduced model will match exactly with the original model at the sample points. Additionally, we show in theory that the proposed method can achieve the error bound over a given frequency range. In practice, the new algorithm can help designers choose the best order of the reduced model for the given frequency range and error bound via the adaptive sampling scheme. In addition, WBMORcan perform wideband accurate reductions of interconnect circuits for analog and RF applications where model accuracy needs to be maintained over a wide frequency range. We compare several sampling schemes such as Monte Carlo, logarithmic, recently proposed resampling, and ARMS methods. Experimental results on a number of RLC circuits show that WBMOR is much more efficient than all the other sampling methods, including the recently proposed resampling and ARMS schemes with the same reduction orders. Compared with the traditional real-valued sampling methods, the complex-valued sampling method is more accurate for the same computational cost. © 2012 ACM.",Adaptive; Complex-valued sampling; Model reduction,Analog circuits; Frequency bands; Adaptive; Adaptive sampling; Compact modeling; Computational costs; Error bound; Error control; Frequency ranges; Imaginary axis; Interconnect circuits; Model accuracy; Model reduction; MONTE CARLO; New model; Original model; Reduced model; Reduction orders; Resampling; Residual error; RF applications; RLC circuit; Sample point; Sampling method; Sampling process; Sampling schemes; Sampling-based; Wide frequency bands; Wide frequency range; Wide-band; Monte Carlo methods
Postplacement voltage island generation,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857834804&doi=10.1145%2f2071356.2071360&partnerID=40&md5=a142d17d1e6dc4616bda1ea46405ef54,"High power consumption will not only shorten the battery life of handheld devices, but also cause thermal and reliability problems. To lower power consumption, one way is to reduce the supply voltage as in multisupply voltage (MSV) designs. In region-based MSV, a circuit will be partitioned into ""voltage islands"" where each island occupies a contiguous physical space and operates at one supply voltage. In the work of Wu et al. [2005], this voltage supply problem is addressed, and the input placement is partitioned into a set of rectangular voltage islands by a slicing structure. However, the constraint of using a slicing structure prohibits better solutions in their approach. In the work of Ching et al. [2006], the constraint of obtaining rectangular shapes is relaxed; their method forms islands of very irregular shapes. In this article, we propose a method that focuses on forming rectangular voltage islands to minimize the power consumption, while at the same time favoring the power routing step. It is found that, even with this reduced flexibility on island shapes, we can still perform as well as, or in some cases, even better than the previous work of Ching et al. [2006] that does not control the shapes of the islands, in terms of power saving and island number. © 2012 ACM.",Low power; Multisupply voltage design; Voltage island,Automation; Battery life; Hand held device; High power consumption; Irregular shape; Island shape; Low Power; Lower-power consumption; Power routing; Power savings; Rectangular shapes; Region-based; Reliability problems; Slicing structure; Supply voltages; Voltage island; Voltage supply; Computer applications
Optimization algorithms for the multiplierless realization of linear transforms,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857851929&doi=10.1145%2f2071356.2071359&partnerID=40&md5=e26b61ad3f24af855eedfec00e5a0fe9,"This article addresses the problem of finding the fewest numbers of addition and subtraction operations in the multiplication of a constant matrix with an input vector-a fundamental operation in many linear digital signal processing transforms. We first introduce an exact common subexpression elimination (CSE) algorithm that formalizes the minimization of the number of operations as a 0-1 integer linear programming problem. Since there are still instances that the proposed exact algorithm cannot handle due to the NP-completeness of the problem, we also introduce a CSE heuristic algorithm that iteratively finds themost common 2-term subexpressions with the minimum conflicts among the expressions. Furthermore, since the main drawback of CSE algorithms is their dependency on a particular number representation, we propose a hybrid algorithm that initially finds promising realizations of linear transforms using a numerical difference method, and then applies the proposed CSE algorithmto utilize the common subexpressions iteratively. The experimental results on a comprehensive set of instances indicate that the proposed approximate algorithms find competitive results with those of the exact CSE algorithm and obtain better solutions than the prominent, previously proposed, heuristics. It is also observed that our solutions yield significant area reductions in the design of linear transforms after circuit synthesis, compared to direct realizations of linear transforms. © 2012 ACM.",0-1 integer linear programming; Common subexpression elimination; Constant matrix-vector multiplication; Numerical difference method,Digital signal processing; Heuristic algorithms; Iterative methods; Numerical methods; Approximate algorithms; Area reduction; Circuit synthesis; Common subexpression elimination; Constant matrix; Difference method; Exact algorithms; Fundamental operations; Hybrid algorithms; Integer Linear Programming; Linear transform; Matrix vector multiplication; Multiplierless; Np-completeness; Number representation; Optimization algorithms; Sub-expressions; Mathematical transformations
Reliability-driven power/ground routing for analog ICs,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863250786&doi=10.1145%2f2071356.2071362&partnerID=40&md5=d4a3d8ed8bc45e4c11013668c4da99c0,"Electromigration and voltage drop (IR-drop) are two major reliability issues in modern IC design. Electromigration gradually creates permanently open or short circuits due to excessive current densities; IR-drop causes insufficient power supply, thus degrading performance or even inducing functional errors because of nonzero wire resistance. Both types of failure can be triggered by insufficient wire widths. Although expanding the wire width alleviates electromigration and IR-drop, unlimited expansion not only increases the routing cost, but may also be infeasible due to the limited routing resource. In addition, electromigration and IR-drop manifest mainly in the power/ground (P/G) network. Therefore, taking wire widths into consideration is desirable to prevent electromigration and IR-drop at P/G routing. Unlike mature digital IC designs, P/G routing in analog ICs has not yet been well studied. In a conventional design, analog designers manually route P/G networks by implementing greedy strategies. However, the growing scale of analog ICs renders manual routing inefficient, and the greedy strategies may be ineffective when electromigration and IR-drop are considered. This study distances itself from conventional manual design and proposes an automatic analog P/G router that considers electromigration and IR-drops. First, employing transportation formulation, this article constructs an electromigration-aware rectilinear Steiner tree with the minimum routing cost. Second, without changing the solution quality, wires are bundled to release routing space for enhancing routability and relaxing congestion. A wire width extension method is subsequently adopted to reduce wire resistance for IR-drop safety. Compared with high-tech designs, the proposed approach achieves equally optimal solutions for electromigration avoidance, with superior efficiencies. Furthermore, via industrial design, experimental results also show the effectiveness and efficiency of the proposed algorithm for electromigration prevention and IR-drop reduction. © 2012 ACM.",Analog; Electromigration; IR-drop; Power/ground network; Routing; Steiner tree,Algorithms; Analog Computers; Networks; Power; Product Design; Resistance; Wire; Algorithms; Forestry; Integrated circuits; Product design; Routers; Wire; Analog; IR-drop; Power/ground network; Routing; Steiner trees; Electromigration
Coverage-directed test generation automated by machine learning - A review,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857846062&doi=10.1145%2f2071356.2071363&partnerID=40&md5=c9812111b3cb6cbb3e5ecc5941f6bcd5,"The increasing complexity and size of digital designs, in conjunction with the lack of a potent verification methodology that can effectively cope with this trend, continue to inspire engineers and academics in seeking ways to further automate design verification. In an effort to increase performance and to decrease engineering effort, research has turned to artificial intelligence (AI) techniques for effective solutions. The generation of tests for simulation-based verification can be guided by machine-learning techniques. In fact, recent advances demonstrate that embedding machine-learning (ML) techniques into a coverage-directed test generation (CDG) framework can effectively automate the test generation process, making it more effective and less error-prone. This article reviews some of the most promising approaches in this field, aiming to evaluate the approaches and to further stimulate more directed research in this area. © 2012 ACM.",Bayesian networks; Coverage-directed test generation (CDG); Genetic algorithms; Genetic programming; Inductive logic programming (ILP); Markov models,Artificial intelligence; Bayesian networks; Genetic algorithms; Genetic programming; Learning systems; Markov processes; Design verification; Digital designs; Effective solution; Error prones; Inductive logic; Machine learning techniques; Machine-learning; Markov model; Simulation-based; Test generations; Verification methodology; Inductive logic programming (ILP)
A fast non-Monte-Carlo yield analysis and optimization by stochastic orthogonal polynomials,2012,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863264862&doi=10.1145%2f2071356.2071366&partnerID=40&md5=87346a85c2ed272f5499f77db97d2f52,"Performance failure has become a significant threat to the reliability and robustness of analog circuits. In this article, we first develop an efficient non-Monte-Carlo (NMC) transient mismatch analysis, where transient response is represented by stochastic orthogonal polynomial (SOP) expansion under PVT variations and probabilistic distribution of transient response is solved. We further define performance yield and derive stochastic sensitivity for yield within the framework of SOP, and finally develop a gradient-based multiobjective optimization to improve yield while satisfying other performance constraints. Extensive experiments show that compared to Monte Carlo-based yield estimation, our NMC method achieves up to 700X speedup and maintains 98% accuracy. Furthermore, multiobjective optimization not only improves yield by up to 95.3% with performance constraints, it also provides better efficiency than other existing methods. © 2012 ACM.",Circuit simulation; Monte Carlo; Yield analysis; Yield optimization,Circuit simulation; Multiobjective optimization; Orthogonal functions; Probability distributions; Stochastic systems; Transient analysis; Gradient based; Mismatch analysis; MONTE CARLO; Orthogonal polynomial; Performance constraints; Performance failure; Probabilistic distribution; PVT variations; Reliability and robustness; Yield analysis; Yield estimation; Yield optimization; Monte Carlo methods
Mitigating the effects of large Multiple Cell Upsets (MCUs) in Memories,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150211&doi=10.1145%2f2003695.2003705&partnerID=40&md5=62e1eec9435601b9a1c4030ce7b54325,"Reliability is a critical issue for memories. Radiation particles that hit the device can cause errors in some cells, which can lead to data corruption. To avoid this problem, memories are protected with per-word error correction codes (ECCs). Typically, single-error correction and double-error detection (SEC-DED) codes are used. As technology scales, errors caused by radiation particles on memories tend to affect more than one cell-what is known as a multiple cell upset (MCU). To ensure that only a single cell is affected in each word, interleaving is used. With interleaving, cells that belong to the same word are placed at a sufficient distance such that an MCU will only affect a single cell on each word. The use of interleaving significantly increases the cost of the device. Also, determining the interleaving distance (ID) required to avoid MCUs causing double errors is not trivial. Typically, accelerated radiation experiments with a limited number of particle hits are used. They provide a lower bound on the required ID, but larger MCUs may occur with a low probability. But even if the percentage of such large MCUs is very low, the impact on reliability can be significant. This article presents a technique to mitigate the effects of large MCUs that is, those that exceed the ID, on memory reliability. The proposed approach is able to correct most double errors caused by large MCUs by exploiting the locality of the errors within an MCU. © 2011 ACM.",Error-correcting codes; Fault-tolerant memory; High-level protection technique; Protection against radiation,Error detection; Reliability; Critical issues; Data corruption; Error correcting code; Error correction codes; Fault-tolerant; Low probability; Lower bounds; Memory reliability; Multiple cell upset; Protection against radiation; Protection techniques; Radiation experiment; Radiation particles; Single cells; Cells
Analog layout retargeting using geometric programming,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150210&doi=10.1145%2f2003695.2003710&partnerID=40&md5=260da58284afaeb2f2d1da4d117ad911,"To satisfy the requirements of complex and special analog layout constraints, a new analog layout retargeting method is presented in this article. Our approach uses geometric programming (GP) to achieve new technology design rules, implement device symmetry and matching constraints, and manage parasitics optimization. The GP, a class of nonlinear optimization problem, can be transferred or fitted into a convex optimization problem. Therefore, a global optimum solution can be achieved. Moreover, the GP can address problems with large-scale variables and constraints without setting an initialization variable range. To meet the prerequisites of the GP methodology for analog layout automation, we propose three kinds of mathematical transformations, including negative coefficient transformation, fraction transformation, and maximum of posynomial transformation. The efficiency and effectiveness of the proposed algorithm, as compared with the other existing methods, are demonstrated by a basic case-study example: a two-stage Miller-compensated operational amplifier and a single-ended folded cascode operational amplifier. © 2011 ACM.",Geometric Programming; Global optimization; Layout; Retargeting; Transformation,Algorithms; Convex optimization; Geometry; Global optimization; Operational amplifiers; Optimization; Analog layout; Analog layout automation; Convex optimization problems; Folded-cascode; Geometric programming; Global optimum solutions; Layout; Matching constraints; Negative coefficients; Non-linear optimization problems; Parasitics; Posynomial; Retargeting; Single-ended; Technology designs; Transformation; Two stage; Variable range; Mathematical transformations
Resource-constrained multiprocessor synthesis for floating-point applications on FPGAs,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150217&doi=10.1145%2f2003695.2003701&partnerID=40&md5=3a992960b3e976902e6fc7e485f49146,"Although state-of-the-art field-programmable gate arrays offer exciting new opportunities in exploring lowcost high-performance architectures for data-intensive scientific applications, they also present serious challenges. Multiprocessor-on-programmable-chip, which integrates software programmability and hardware reconfiguration, provides substantial flexibility that results in shorter design cycles, higher performance, and lower cost. In this article, we present an application-specific design methodology for multiprocessoron- programmable-chip architectures that target applications involving large matrices and floating-point operations. Given an application with specific energy-performance and resource constraints, our methodology aims to customize the architecture to match the diverse computation and communication requirements of the application tasks. Graph-based analysis of the application drives system synthesis that employs a precharacterized, parameterized hardware component library of functional units. Extensive experimental results for three diverse applications are presented to demonstrate the efficacy of our design methodology. © 2011 ACM.",FPGA design and synthesis; Heterogeneous multiprocessors; Mixed-mode parallel processing; Multiprocessor-on-programmable-chip; Resource-constrained optimization,Constrained optimization; Design; Digital arithmetic; Field programmable gate arrays (FPGA); Microprocessor chips; FPGA design and synthesis; Heterogeneous multiprocessors; Mixed mode; Multiprocessor-on-programmable-chip; Resource-constrained; Multiprocessing systems
GALS-designer: A design framework for GALS software systems,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150218&doi=10.1145%2f2003695.2003699&partnerID=40&md5=66eb3464e4b2936aab528a9ec0cffe07,"GALS-Designer is a framework for the design of software systems which comply with the formal Globally Asynchronous Locally Synchronousmodel of computation (GALS). Those systems consist of single or multiple GALS programs and their immediate environment, which can be other programs and any other modules described in SystemC. The framework integrates our libGALS library for writing GALS programs and SystemC. It enables modeling and simulation of single and multiple GALS programs within the single SystemC executable model on the host (simulation) operating system. The same GALS programs can then be run without SystemC on a target operating system for which the libGALS runtime library is available. The use of the GALS-Designer is demonstrated on an example of a complex embedded system. As libGALS can execute on multiprocessor platforms both the simulation and target models of the GALS system can take advantage of multiprocessor and multicore systems, which is not possible when using standard SystemC. Results of running simulation models of GALS programs demonstrate simulation performance improvement when executing on multicore platforms. © 2011 ACM.",GALS; Globally asynchronous; Implementation; LibGALS; Locally synchronous; Modeling; Simulation; Software concurrency; Specification; System-level design; SystemC; SystemJ,Computer simulation; Design; Multicore programming; Multiprocessing systems; Software design; GALS; Globally asynchronous; Implementation; LibGALS; Locally synchronous; Simulation; System level design; SystemC; SystemJ; Computer software
A 36μW heartbeat-detection processor for a wireless sensor node,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150204&doi=10.1145%2f2003695.2003711&partnerID=40&md5=b92b49b1843d9771c472ec912d9f8c8d,"In order to provide better services to elderly people, home healthcare monitoring systems have been increasingly deployed. Typically, these systems are based on wireless sensor nodes, and should utilize very low energy during their lifetimes, as they are powered by scavengers. In this article, we present an ultra-low power processing system for a wireless sensor node for very low duty cycle applications. In the CoolBio system-on-chip, we utilized several power reduction techniques at both the architecture level and the circuit level. These techniques include feature extraction, voltage and frequency scaling, clock and power gating and a redesign of key standard cells. In the design of the ultra-low power processing system, we paid special attention to the memory subsystem, as it is one of the most power-consuming modules in a design. We also designed a clock manager in order to reduce the power consumed by clocking, and a power manager that is able to power-off unutilized modules. The proposed wireless sensor node processing system consumes 36.4μW at 100MHz and 1.2V supply voltage, for a heartbeat-detection algorithm with a 0.01% duty cycle. © 2011 ACM.",CoolBio; Coolflux; ECG; Heartbeat detection; Low duty cycle; Power reduction; System-on-chip; Ultra-low power; Wireless sensor node; WSN,Application specific integrated circuits; Electric batteries; Electric network synthesis; Feature extraction; Health care; Management; Managers; Programmable logic controllers; Sensor nodes; Telecommunication equipment; CoolBio; Coolflux; Heartbeat detection; Low duty-cycles; Power reductions; System on chips; Ultra-low power; Wireless sensor node; WSN; Sensors
Clock tree synthesis for TSV-based 3D IC designs,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150202&doi=10.1145%2f2003695.2003708&partnerID=40&md5=6c83792abcf55afc2a364455df96c62c,"For the cost-effective implementation of clock trees in through-silicon via (TSV)-based 3D IC designs, we propose core algorithms for 3D clock tree synthesis. For a given abstract tree topology, we propose DLE-3D (deferred layer embedding for 3D ICs), which optimally finds the embedding layers of tree nodes, so that the TSV cost required for a tree topology is minimized, and DME-3D (deferred merge embedding for 3D ICs), which is an extended algorithm of the 2D merging segment, to minimize the total wirelength in 3D design space, with the consideration of the TSV effect on delay. In addition, when an abstract tree topology is not given, we propose NN-3D (nearest neighbor selection for 3D ICs), which constructs a (TSV and wirelength) cost-effective abstract tree topology for 3D ICs. Through experimentation, we have confirmed that the clock tree synthesis flow using the proposed algorithms is very effective, outperforming the existing 3D clock tree synthesis in terms of the number of TSVs, total wirelength, and clock power consumption. © 2011 ACM.",3D ICs; Clock tree synthesis; Optimization; Routing; TSV,Abstracting; Adaptive systems; Algorithms; Cost effectiveness; Design; Electric clocks; Microprocessor chips; Plant extracts; Telecommunication networks; Topology; Trees (mathematics); 3-D ICs; 3D design; 3D IC design; Abstract trees; Clock power consumption; Clock Tree; Clock tree synthesis; Core algorithms; Deferred-merge embedding; Nearest neighbors; Not given; Routing; Tree nodes; Tree topology; TSV; Wire length; Three dimensional
Memory access optimization in compilation for coarse-grained reconfigurable architectures,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155177632&doi=10.1145%2f2003695.2003702&partnerID=40&md5=d1a19c5b9f89062c81dbc77c8d951654,"Coarse-grained reconfigurable architectures (CGRAs) promise high performance at high power efficiency. They fulfil this promise by keeping the hardware extremely simple, andmoving the complexity to application mapping. One major challenge comes in the form of data mapping. For reasons of power-efficiency and complexity, CGRAs use multibank local memory, and a row of PEs share memory access. In order for each row of the PEs to access any memory bank, there is a hardware arbiter between the memory requests generated by the PEs and the banks of the local memory. However, a fundamental restriction remains in that a bank cannot be accessed by two different PEs at the same time. We propose to meet this challenge by mapping application operations onto PEs and data into memory banks in a way that avoids such conflicts. To further improve performance on multibank memories, we propose a compiler optimization for CGRA mapping to reduce the number of memory operations by exploiting data reuse. Our experimental results on kernels from multimedia benchmarks demonstrate that our local memory-aware compilation approach can generate mappings that are up to 53% better in performance (26% on average) compared to a memoryunaware scheduler. © 2011 ACM.",Array mapping; Bank conflict; Coarse-grained reconfigurable architecture; Compilation; Multibank memory,Architecture; Computer hardware; Embedded systems; Mapping; Optimization; Array mapping; Bank conflict; Coarse grained reconfigurable architecture; Compilation; Multibank memory; Memory architecture
IO connection assignment and RDL routing for flip-chip designs,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150196&doi=10.1145%2f2003695.2003707&partnerID=40&md5=a0bd1add77d2d62f13e130d7aa4e52d1,"Given a set of IO buffers and a set of bump balls with the capacity constraints between two adjacent bump balls, based on the construction of the Delaunary triangulation and a Manhattan Voronoi diagram, an O(n2) assignment algorithm is proposed to assign all the IO connections in a single redistribution layer for IO connection assignment, where n is the number of bump balls in a flip-chip design. Furthermore, based on the computation of the probabilistic congestion for the assigned IO connections, an O(n2) routing algorithm is proposed to minimize the total wirelength to route all the assigned IO connections while satisfying the capacity constraints for single-layer RDL routing. Compared with the combination of a greedy IO assignment and our RDL routing, our IO assignment reduces the total wirelength by 9.9% and improves the routability by 8.8% on the average for 6 tested circuits. Compared with the combination of a greedy IO assignment, the single-layer BGA global router [Tomioka and Takahashi 2006] and our RDL detailed routing, our IO connection assignment and RDL routing reduces the total wirelength by 12.9% and improve the routability by 10.2% on the average for 6 tested circuits. Besides that the experimental results show that our IO connection assignment and RDL routing can reduce 52.1% of the total wirelength on the average to achieve 100% routability for 12 tested industrial circuits under reasonable CPU time. © 2011 ACM.",Flip-chip designs; RDL routing; Single-layer routing,Algorithms; Design; Assignment algorithms; Capacity constraints; CPU time; Detailed routing; Flip chip; Flip-chip designs; Global routers; Industrial circuits; Manhattans; RDL routing; Redistribution layers; Routability; Single layer; Voronoi diagrams; Wire length; Flip chip devices
Dynamic data folding with parameterizable FPGA configurations,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155142214&doi=10.1145%2f2003695.2003703&partnerID=40&md5=ddea9809ac6e165ab81f02057e35e0d7,"In many applications, subsequent data manipulations differ only in a small set of parameter values. Because of their reconfigurability, FPGAs (field programmable gate arrays) can be configured with a specialized circuit each time the parameter values change. This technique is called dynamic data folding. The specialized circuits are smaller and faster than their generic counterparts. However, the overhead involved in generating the configurations for the specialized circuits at runtime is very large when conventional tools are used, and this overhead will in many cases negate the benefit of using optimized configurations. This article introduces an automatic method for generating runtime parameterizable configurations from arbitrary Boolean circuits. These configurations, in which some of the configuration bits are expressed as a closed-form Boolean expression of a set of parameters, enable very fast run-time specialization, since specialization only involves evaluating these expressions. Our approach is validated on a ternary contentaddressable memory (TCAM). We show that the specialized configurations, produced by our method use 2.82 times fewer LUTs than the generic configuration, and even 1.41 times fewer LUTs than the implementation generated by Xilinx Coregen. Moreover, while Coregen needs hand-crafted generators for each type of circuit, our toolflow can be applied to any VHDL design. Using our automatic and generally applicable method, run-time hardware optimization suddenly becomes feasible for a large class of applications. © 2011 ACM.",Automatic hardware synthesis; Dynamic data folding; FPGA; Run-time reconfiguration,Logic gates; Optimization; Ternary content adressable memory; Automatic hardware synthesis; Automatic method; Boolean circuit; Boolean expressions; Closed form; Data manipulations; Dynamic data; Dynamic data folding; FPGA configuration; Generic configuration; Hardware optimization; Large class; Optimized configuration; Parameter values; Reconfigurability; Run time reconfiguration; Runtimes; Ternary content addressable memories; Tool-flow; Field programmable gate arrays (FPGA)
Integrated microarchitectural floorplanning and run-time controller for inductive noise mitigation,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155142216&doi=10.1145%2f2003695.2003706&partnerID=40&md5=7263c2e6ed8d08112f20e955a71e9472,"In this article, we propose a design methodology using two complementary techniques to address highfrequency inductive noise in the early design phase of a microprocessor. First, we propose a noise-aware floorplanning technique that uses microarchitectural profile information to create noise-aware floorplans. Second, we present the design of a dynamic inductive-noise controlling mechanism at the microarchitectural level, which limits the on-die current demand within predefined bounds, regardless of the native power and current characteristics of running applications. By dynamically monitoring the access patterns ofmicroarchitectural modules, our mechanism can effectively limit simultaneous switching activity of close-by modules, thereby leveling voltage ringing at local power-pins. Compared to prior aRt, Our Di/Dt Alleviation Technique Is The First That Takes The Processor's Floorplan, As Well As Its Power-Pin Distribution, Into Account To Provide A Finer-Grained Control With Minimal Performance Degradation. Based On The Evaluation Results Using 2D Floorplans, We Show That Our Techniques Can Significantly Improve Inductive Noise Induced By Current Demand Variation And Reduce The Average Current Variability By Up To 7 Times, With An Average Performance Overhead Of 4.0%. In Addition, Our Floorplan Reduces The Noise Margin Violations Using Our Noise-Aware Floorplan By An Average Of 56.3% While Reducing The Decap Budget By 28%. © 2011 ACM.",Floorplanning; Microarchitecture; Power supply noise,Design; Access patterns; Average currents; Controlling mechanism; Current characteristic; Demand variations; Design Methodology; Early design phase; Evaluation results; Floor-planning; Floorplans; High frequency HF; Inductive noise; Micro architectures; Noise margins; Performance degradation; Power-supply noise; Running applications; Runtimes; Switching activities; Voltage ringing; Embedded systems
Concurrency-oriented verification and coverage of system-level designs,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150223&doi=10.1145%2f2003695.2003697&partnerID=40&md5=fc99793d045682c292f5224a593c344a,"Correct concurrent System-on-Chips (SoCs) are very hard to design and reason about. In this work, we develop an automated framework complete with concurrency-oriented verification and coverage techniques for system-level designs. Our techniques are different from traditional simulation-based reliability techniques, since concurrency information is often lost in traditional techniques. We preserve concurrency information to obtain unique verification techniques that allow us to predict potential errors (formulated as transactionlevel assertions) from error-free simulations. In order to do this, we exploit the inherent concurrency in the designs to generate and analyze novel partial-order simulation traces. Additionally, to evaluate the confidence on verification results and the gauge progress of verification, we develop novel mutation testing based on concurrent coverage metrics. Mutation testing is a fault insertion-based simulation technique that has been successfully applied in software testing. We present a comprehensive list of mutation operators for SystemC, similar to behavioral fault models, and show the effectiveness of these operators by relating them to actual bug patterns. We have successfully applied our verification and coverage techniques on industrial systems and demonstrated that current verification test suites need to be improved for concurrent designs, and we have found errors in systems that were tested previously. © 2011 ACM.",Assertion-based verification; Concurrency; Coverage; Mutation testing; Partial-orders; Predictive verification; Simulation; SystemC,Computer software selection and evaluation; Design; Errors; Microprocessor chips; Software testing; Assertion-based verification; Concurrency; Coverage; Mutation testing; Partial-orders; Predictive verification; Simulation; SystemC; Mathematical models
A probabilistic analysis of coverage methods,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155150220&doi=10.1145%2f2003695.2003698&partnerID=40&md5=6f64ba74e987c04bd5273b954acd7696,"Coverage is an importantmeasure for the quality and completeness of the functional verification of hardware logic designs. Verification teams spend a significant amount of time looking for bugs in the design and in providing high-quality coverage. This process is performed through the use of various sampling strategies for selecting test inputs. The selection of sampling strategies to achieve the verification goals is typically carried out in an intuitive manner. We studied several commonly used sampling strategies and provide a probabilistic framework for assessing and comparing their relative values. For this analysis, we derived results for two measures of interest: first, the probability of finding a bug within a given number of samplings; and second, the expected number of samplings until a bug is detected. These results are given for both recurring sampling schemes, in which the same inputs might be selected repeatedly, and for nonrecurring sampling schemes, in which already sampled inputs are never selected again. By considering results from the theory of search, and more specifically, from the well-known multiarmed bandit problem, we demonstrate the optimality of a greedy sampling strategy within our defined framework. © 2011 ACM.",Coverage analysis; Functional verification; Search theory,Logic design; Coverage analysis; Functional verification; High quality; Multi-armed bandit problem; Optimality; Probabilistic analysis; Probabilistic framework; Relative value; Sampling schemes; Sampling strategies; Search theory; Test inputs; Telecommunication networks
Clock buffer polarity assignment with skew tuning,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155142208&doi=10.1145%2f2003695.2003709&partnerID=40&md5=69611efce1cad24643f2ee58114d81a0,"A clock polarity assignment method is proposed that reduces the peak current on the vdd/gnd rails of an integrated circuit. The impacts of (i) the output capacitive load on the peak current drawn by the sink-level clock buffers, and (ii) the buffer/inverter replacement scheme of polarity assignment on timing accuracy are considered in the formulation. The proposed sink-level-only polarity assignment is performed by a lexi-search algorithm in order to balance the peak current on the clock tree. Most of the previous polarity assignment methods that do not include clock tree resynthesis lead to an undesirable increase in the worst corner clock skew. Hence, a skew-tuning scheme is proposed that reduces the clock skew through polarity refinement and not through clock tree resynthesis. The proposed polarity assignment method with the skew-tuning scheme is implemented within an industrial design flow for practicality. Experimental results show that the worst-case peak current drawn by the clock tree can be reduced by an average of 36.5%. The worst corner clock skew is increased from 60.7ps to 76.2ps by applying the proposed polarity assignment method. The proposed skew-tuning scheme reduces the worst-case clock skew from 76.2ps to 61.5ps, on average, with a limited degradation in the peak current improvement (36.5% to 31.2%, on average). © 2011 ACM.",Clock network synthesis; Clock skew; Physical design; Polarity assignment,Electric clocks; Plant extracts; Product design; Trees (mathematics); Capacitive loads; Clock buffer; Clock network synthesis; Clock skew; Clock skews; Clock Tree; Peak currents; Physical design; Polarity assignment; Replacement scheme; Tuning
Timing variation-aware scheduling and resource binding in high-level synthesis,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155142231&doi=10.1145%2f2003695.2003700&partnerID=40&md5=f489e6da352ce372f81f2249b9bb86f6,"Due to technological scaling, process variations have increased significantly, resulting in large variations in the delay of the functional units. Hence, the worst-case approach is becoming increasingly pessimistic in meeting a certain performance yield. The problem therefore is to increase the performance as much as possible while maintaining the desired yield. In this work, we introduce an integer linear programming (ILP) formulation for scheduling and resource binding in high-level synthesis (HLS) which tries to mitigate the effect of timing variations. In the presence of delay variations of resources, as chained resources can give a better latency and performance yield tradeoff, instead of considering them independently, we consider external chaining of resources, that is, two or more resources are connected by external wiring, and exploit operation chaining. Without violating the yield constraints, the proposed ILP formulation chains two consecutive operations and binds these chained operations to chained resources for minimizing the overall latency of the schedule. Our ILP formulation also makes sure that two consecutive operations can be chained over multiple clock cycles so that it becomes possible to access the data in the middle of the chained operations at the start of the clock steps over which the operations are chained. By solving our ILP formulation using ILOG CPLEX, we show that our mechanism achieves lesser latency in most cases, compared to the no-chaining case. Significant performance improvement is achieved even for the 100% yield case, which has never been demonstrated in any published work, to the best of our knowledge. © 2011 ACM.",Binding; Chaining; HLS; ILP; Scheduling; Variation-aware,Integer programming; Binding; Chaining; HLS; ILP; Variation-aware; Scheduling
Parallel circuit simulation with adaptively controlled projective integration,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155177629&doi=10.1145%2f2003695.2003704&partnerID=40&md5=d84e8c6bcfe2f2291301ba8d16da899c,"In this article, a parallel transient circuit simulation approach based on an adaptively-controlled timestepping scheme is proposed. Different from the widely-used implicit numerical integration techniques in most transient simulators, this work exploits the recently-developed explicit telescopic projective numerical integration method for efficient parallel circuit simulation. Because telescopic projective integration addresses the well-known stability issue of explicit numerical integrations by adopting combinations of inner integrators and outer integrators in a multilevel fashion, the simulation time-step is no longer limited by the smallest time constant in the circuit. With dynamic control of telescopic projective integration, the proposed projective integration framework not only leads to noticeable efficiency improvement in circuit simulation, it also lends itself to straightforward parallelization due to its explicit nature. The latter has led to encouraging runtime efficiencies, observed on shared-memory parallel platforms. In addition to solving standard initial-value problems (IVPs) of differential equations, the same telescopic integration framework is adopted for solving final-value problems (FVPs), where the system is integrated backwards in time. Through a new elegant formulation, we show how an IVP and FVP can be simultaneously solved to allow for a coarse-grained bidirectional parallel circuit simulation scheme. Such a bidirectional approach is demonstrated in the context of parallel shooting-Newton-based steady-state circuit analysis. The proposed bidirectional approach has unique and favorable properties: the solutions of the two ODE problems are completely data-independent with built-in automatic load balancing. © 2011 ACM.",Explicit numerical integration; Parallel computing; Transient simulation,Differential equations; Electric network analysis; Initial value problems; Integration; Numerical methods; Parallel architectures; Coarse-grained; Dynamic controls; Efficiency improvement; Final-value problems; Implicit numerical integration; Integration frameworks; Numerical integration methods; Numerical integrations; Parallelizations; Run-time efficiency; Shared-memory parallels; Simulation approach; Stability issues; Time constants; Time step; Time-stepping schemes; Transient simulation; Circuit simulation
Multithreaded simulation for synchronous dataflow graphs,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960659084&doi=10.1145%2f1970353.1970358&partnerID=40&md5=b6f8a106241a917fca1dd15173e23a70,"For system simulation, Synchronous DataFlow (SDF) has been widely used as a core model of computation in design tools for digital communication and signal processing systems. The traditional approach for simulating SDF graphs is to compute and execute static schedules in single-processor desktop environments. Nowadays, however, multicore processors are increasingly popular desktop platforms for their potential performance improvements through thread-level parallelism. Without novel scheduling and simulation techniques that explicitly explore thread-level parallelism for executing SDF graphs, current design tools gain only minimal performance improvements on multicore platforms. In this article, we present a new multithreaded simulation scheduler, called MSS, to provide simulation runtime speedup for executing SDF graphs on multicore processors. MSS strategically integrates graph clustering, intracluster scheduling, actor vectorization, and intercluster buffering techniques to construct InterThread Communication (ITC) graphs at compile-time. MSS then applies efficient synchronization and dynamic scheduling techniques at runtime for executing ITC graphs in multithreaded environments. We have implemented MSS in the Advanced Design System (ADS) from Agilent Technologies. On an Intel dual-core, hyper-threading (4 processing units) processor, our results from this implementation demonstrate up to 3.5 times speedup in simulating modern wireless communication systems (e.g., WCDMA3G, CDMA 2000, WiMax, EDGE, and Digital TV). © 2011 ACM.",Multithreaded simulation; Scheduling; Synchronous dataflow,Communication systems; Computer simulation; Design; Digital communication systems; Digital television; Global system for mobile communications; Graphic methods; Parallel processing systems; Response time (computer systems); Satellites; Scheduling; Signal processing; Wimax; Advanced design system; Agilent technologies; Cdma2000; Compile time; Core model; Design tool; Desktop environment; Digital communications; Digital TV; Dual-core; Dynamic scheduling; Graph clustering; Hyper-threading; Intra-cluster; Multi-core platforms; Multi-core processor; Multithreaded; Multithreaded environments; Performance improvements; Processing units; Runtimes; Signal processing systems; Simulation technique; Synchronous Dataflow; Synchronous dataflow graphs; System simulations; Thread level parallelism; Vectorization; Wireless communication system; Data flow analysis
40nm CMOS 0.35V-optimized standard cell libraries for ultra-low power applications,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960657621&doi=10.1145%2f1970353.1970369&partnerID=40&md5=ee2c346ce04d8deb05aa5f337e91ab36,"Ultra-low voltage is now a well-known solution for energy constrained applications designed using nanometric process technologies. This work is focused on setting up an automated methodology to enable the design of ultra-low voltage digital circuits exclusively using standard EDA tools. To achieve this goal, a 0.35V energy-delay optimized library was developed. This library, fully compliant with standard library design flow and characterization, was verified through the design and fabrication of a BCH decoder circuit, following a standard front-end to back-end flow. At 0.33V, it performs at 600 kHz with a dynamic energy consumption reduced by a factor 14x from nominal 1.1V. Based on this design, experiments, and preliminary silicon results, two additional libraries were developed in order to enhance future ultra-low voltage circuit performance. © 2011 ACM.",Bose choudhury hocquenghem; Circuit; CMOS; Design; Energy; Library; Logic; Low power; Methodology; Subthreshold; Ultra low voltage,Design; Digital circuits; Electric batteries; Energy utilization; Libraries; Optimization; Bose choudhury hocquenghem; Circuit; CMOS; Energy; Logic; Low Power; Methodology; Subthreshold; Ultra-low-voltage; Standards
Thread warping: Dynamic and transparent synthesis of thread accelerators,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960671131&doi=10.1145%2f1970353.1970365&partnerID=40&md5=cca7c9f62ca3c832c06ba1c28e7a7798,"We introduce thread warping, a dynamic optimization technique that customizes multicore architectures to a given application by dynamically synthesizing threads into custom accelerator circuits on FPGAs (Field-Programmable Gate Arrays). Thread warping builds upon previous dynamic synthesis techniques for single-threaded applications, enabling dynamic architectural adaptation to different amounts of thread-level parallelism, while also exploiting parallelism within each thread to further improve performance. Furthermore, thread warping maintains the important separation of function from architecture, enabling portability of applications to architectures with different quantities of microprocessors and FPGAs, an advantage not shared by static compilation/synthesis approaches. We introduce an approach consisting of CAD tools and operating system support that enables thread warping on potentially any microprocessor/FPGA architecture. We evaluate thread warping using a simulator for high-performance computing systems with different interconnections in addition to multicore embedded systems having between 4 and 64 ARM11 microprocessors. On average, thread warping achieved approximately 3x speedup compared to a high-performance quad-core Intel Xeon and 109x compared to an embedded system consisting of 4 ARM11 cores, with a size cost approximately equal to 36 ARM11 cores. © 2011 ACM.",Embedded systems; FPGA; Reconfigurable computing; Runtime optimizations; Synthesis,Architecture; Computer aided design; Computer software selection and evaluation; Embedded systems; Field programmable gate arrays (FPGA); Microprocessor chips; Optimization; Parallel processing systems; Reconfigurable hardware; Software architecture; CAD tool; Dynamic optimization; Dynamic synthesis; High performance computing systems; Multicore architectures; Multicore embedded system; Operating system support; Reconfigurable computing; Runtime optimization; Single-threaded; Static compilation; Thread level parallelism; Weaving
A special section on multicore parallel CAD: Algorithm design and programming,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960665878&doi=10.1145%2f1970353.1970354&partnerID=40&md5=a45500505e9a3521c3c9b4efab5bf70e,[No abstract available],,
Hardware-software codesign of an embedded multiple-supply power management unit for multicore SoCs using an adaptive global/local power allocation and processing scheme,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960663291&doi=10.1145%2f1970353.1970364&partnerID=40&md5=32cad414660e42e2f4df73ae0806b3cc,"Power dissipation has become a critical design constraint for the growth of modern multicore systems due to increasing clock frequencies, leakage currents, and system parasitics. To overcome this urgent crisis, this article presents an embedded platform for on-chip power management of a multicore System-on-Chip (SoC). The design involves the development of two key components, from the hardware to the software level. From the hardware perspective, a multiple-supply power management unit is proposed and is implemented using a Single-Inductor Multiple-Output (SIMO) DC-DC converter. To dynamically respond to the sensed instantaneous power demands and to accurately control the power delivery to the processor cores, the power management unit employs a software-defined adaptive global/local power allocation feedback controller. The proposed controller is designed using the hardware-software codesign methodology to uniquely control the SIMO converter during various operation scenarios. This is achieved using several embedded software control algorithms that operate synergetically to ensure efficient and reliable system operation. The hardware-software codesign technique also allows the SIMOcontroller to be integrated with future microprocessor cores. Therefore, by employing the vast amount of on-chip resources, the converter can perform effective power processing to provide the most power-optimal voltages at the hardware level. Such an embedded power management module leads to an integrated, power-aware, and autonomous SoC design that is independent of additional external hardware control, thereby reducing on-chip area and system complexity. In this design, each power output from the SIMO converter provides a step-up/down voltage conversion, thereby enabling a wide range of variable supply voltage. An adaptive global/local power allocation control algorithm is employed to significantly improve Dynamic Voltage and Frequency Scaling (DVFS) tracking speed and line/load regulation, while still retaining low cross-regulation. Designed with a 180nm CMOS process, the converter precisely provides three independently variable power outputs from 0.9 V to 3.0 V, with a total power range from 33 mW to 900 mW. A very fast load transient response of 3.25 μs is achieved, in response to a 67.5-mA full-step load current change. The design thus provides a cost-effective power management solution to achieve a robust, fast-transient, DVFS-compatible multicore SoC. © 2011 ACM.",Adaptive power allocation control scheme; Dynamic voltage scaling (DVS); Multicore systems; Single-inductor multiple-output (SIMO) DC-DC converter; Switching converter,Algorithms; Application specific integrated circuits; CMOS integrated circuits; Computer control; Controllers; DC-DC converters; Design; Electric power transmission; Embedded software; Embedded systems; Energy management; Hardware; HVDC power transmission; Leakage currents; Management; Microprocessor chips; Programmable logic controllers; Switching circuits; Voltage stabilizing circuits; Control schemes; Dynamic voltage scaling (DVS); Multi-core systems; Single-inductor multiple-output (SIMO) DC-DC converter; Switching converter; Electric inductors
Chassis: A platform for verifying PMU integration using autogenerated behavioral models,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960682424&doi=10.1145%2f1970353.1970367&partnerID=40&md5=c27a8c15abc8581c710871ef265f69cc,"PowerManagement Units (PMUs) are large integrated circuits consisting of many predesigned mixed-signal components. PMU integration poses a serious verification problem considering the size of the integrated circuit and the complexity of analog simulation. In this article we present an approach for automatic generation of behavioral models for PMU components from top-down skeleton models, fitted with parameter values estimated by bottom-up parameter extraction algorithms. It is shown that replacing PMU components with these autogenerated hybrid automata-based abstract behavioral models enables significant simulation speedup (> 20X on our industrial test cases) and helps in early detection of integration errors. The article also justifies the level of accuracy in our models with respect to the goal of verifying integrated PMUs. The approach presented in this work is implemented in the form of a tool suite called Chassis. © 2011 ACM.",Behavioral modeling; Verification; Voltage regulators,Chassis; Electric potential; Integrated circuits; Integration; Parameter extraction; Voltage regulators; Analog simulations; Automatic Generation; Behavioral model; Behavioral modeling; Early detection; Extraction algorithms; Industrial Test Case; Integration error; Large integrated circuits; Mixed signal; Parameter values; Power managements; Simulation speed-up; Toolsuite; Topdown; Verification problems; Computer simulation
A metric for quantifying similarity between timing constraint sets in real-time systems,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960680883&doi=10.1145%2f1970353.1970368&partnerID=40&md5=c9a3db655189a1c9f206f1514aea2f04,"Real-time systems are systems in which their timing behaviors must satisfy a specified set of timing constraints and they often operate in a real-world environment with scarce resources. As a result, the actual runtime performance of these systems may deviate from the design, either inevitably due to unpredictable factors or by intention in order to improve system's other Quality-of-Service (QoS) properties. In this article, we first introduce a new metric, timing constraint set similarity, to quantify the resemblance between two different timing constraint sets. Because directly calculating the exact value of the metric involves calculating the size of a polytope which is a #P-hard problem, we instead introduce an efficientmethod for estimating its bound. We further illustrate how this metric can be exploited for improving system predictability and for evaluating trade-offs between timing constraint compromises and the system's other QoS property gains. © 2011 ACM.",Constraint satisfaction; Quality of service; Real-time systems; Scheduling; Timing constraint feasibility region; Timing constraint similarities; Timing constraints,Discrete cosine transforms; Quality of service; Timing circuits; Constraint Satisfaction; Improving systems; Polytopes; QoS properties; Real world environments; Runtime performance; Scarce resources; Timing constraints; Real time systems
Efficient and deterministic parallel placement for FPGAs,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960675590&doi=10.1145%2f1970353.1970355&partnerID=40&md5=1ef0d9df3255d3cab607a44c29cbb0eb,"We describe a parallel simulated annealing algorithm for FPGA placement. The algorithm proposes and evaluates multiple moves in parallel, and has been incorporated into Altera's Quartus II CAD system. Across a set of 18 industrial benchmark circuits, we achieve geometric average speedups during the quench of 2.7x and 4.0x on four and eight processors, respectively, with individual circuits achieving speedups of up to 3.6x and 5.9x. Over the course of the entire anneal, we achieve speedups of up to 2.8x and 3.7x, with geometric average speedups of 2.1x and 2.4x. Our algorithm is the first parallel placer to optimize for criteria other than wirelength, such as critical path length, and is one of the few deterministic parallel placement algorithms. We discuss the challenges involved in combining these two features and the new techniques we used to overcome them. We also quantify the impact of maintaining determinism on eight cores, and find that while it reduces performance by approximately 15% relative to an ideal speedup of 8.0x, hardware limitations are a larger factor and reduce performance by 30-40%. We then suggest possible enhancements to allow our approach to scale to 16 cores and beyond. © 2011 ACM.",FPGAs; Parallel placement; Timing-driven placement,Algorithms; Timing circuits; Benchmark circuit; CAD system; Critical path lengths; FPGAs; Parallel placement; Parallel simulated annealing algorithm; Placement algorithm; Quartus II; Timing driven placement; Wire length; Simulated annealing
GPU-based parallelization for fast circuit optimization,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960665047&doi=10.1145%2f1970353.1970357&partnerID=40&md5=706fa73939a19f6fb164c9d86fc584cf,"The progress of GPU (Graphics Processing Unit) technology opens a new avenue for boosting computing power. This work is an attempt to exploit the GPU for accelerating VLSI circuit optimization. We propose GPU-based parallel computing techniques and apply them on simultaneous gate sizing and threshold voltage assignment, which is a popular method for VLSI performance and power optimization. These techniques include efficient task scheduling and memory organization, all of which are aimed to fully utilize the advantages of GPUs. Compared to conventional sequential computation, our techniques can provide up to 56× (39× on average) speedup without any sacrifice on solution quality. © 2011 ACM.",Circuit optimization; General-purpose graphics computing unit (GPGPU); Parallelization,Gates (transistor); Optimization; Parallel architectures; Program processors; VLSI circuits; Circuit optimization; Computing power; Gate sizing; General-purpose graphics computing unit (GPGPU); Graphics Processing Unit; Memory organizations; Parallel computing techniques; Parallelizations; Power Optimization; Sequential computations; Solution quality; Task-scheduling; Voltage assignment; Computer graphics equipment
A new algorithm for VHDL parallel simulation,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960678003&doi=10.1145%2f1970353.1970360&partnerID=40&md5=871213306a21f11e0d1a8e9b6d2adc28,"This article proposes a new algorithm for parallel synchronous simulation of VHDL designs to be executed on desktop computers. Besides executing VHDL processes in parallel, the algorithm focuses on parallelizing the simulation kernel with special emphasis on signal grouping while maintaining language semantics. Synchronous approaches are the most suitable for shared memory multiprocessor (SMP) desktop computers but may be difficult to parallelize because of the low activity detected in most of the designs. The degree of parallelism is increased in this approach by performing an exhaustive VHDL signal dependencies analysis and avoiding any sequential phase in the simulator. VHDL semantics impose a synchronization barrier after each phase, that is, the process and the kernel simulation phase, as the language definition does not allow simultaneous execution of kernel and processes. These barriers have been relaxed in order to increase the level of parallelism and obtain better performance. Another aspect the new algorithm takes into account is to improve load balancing and locality of references, both critical issues in synchronous simulators, by introducing a new load balancing algorithm that exploits the cyclic characteristics of circuit simulators. These developments make the algorithm suitable for commodity hardware, that is, SMP that are currently used as desktop personal computers. © 2011 ACM.",Parallel; Simulation; VHDL,Algorithms; Computer simulation languages; Parallel architectures; Personal computers; Semantics; Simulators; Circuit simulators; Commodity hardware; Critical issues; Degree of parallelism; Kernel simulations; Language semantics; Load balancing algorithms; Locality of reference; Parallel; Parallel simulations; Parallelizing; Shared memory multiprocessor; Simulation; Simulation Kernel; Synchronization barriers; Synchronous simulation; VHDL; Computer hardware description languages
Massively parallel logic simulation with GPUs,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960691849&doi=10.1145%2f1970353.1970362&partnerID=40&md5=bbaa22fd592b59d8d06a88943f3aad17,"In this article, we developed a massively parallel gate-level logical simulator to address the ever-increasing computing demand for VLSI verification. To the best of the authors' knowledge, this work is the first one to leverage the power of modern GPUs to successfully unleash the massive parallelism of a conservative discrete event-driven algorithm, CMB algorithm. A novel data-parallel strategy is proposed to manipulate the fine-grain message passing mechanism required by the CMB protocol. To support robust and complete simulation for real VLSI designs, we establish both a memory paging mechanism and an adaptive issuing strategy to efficiently utilize the GPU memory with a limited capacity. A set of GPU architecture-specific optimizations are performed to further enhance the overall simulation performance. On average, our simulator outperforms a CPU baseline event-driven simulator by a factor of 47.4X. This work proves that the CMB algorithm can be efficiently and effectively deployed on modern GPUs without the performance overhead that had hindered its successful applications on previous parallel architectures. © 2011 ACM.",CMB algorithm; Discrete event-driven; Gate-level logic simulation; GPU,Algorithms; Knowledge management; Message passing; Parallel architectures; Simulators; CMB algorithm; Computing demands; Data parallel; Discrete event-driven; Event-driven simulators; GPU; Limited capacity; Logic simulations; Massive parallelism; Parallel logic; Simulation performance; VLSI design; Program processors
Gate-level simulation with GPU computing,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960659315&doi=10.1145%2f1970353.1970363&partnerID=40&md5=3bf78ad2e11a6636190943f3d977cc1e,"Functional verification of modern digital designs is a crucial, time-consuming task impacting not only the correctness of the final product, but also its time to market. At the heart of most of today's verification efforts is logic simulation, used heavily to verify the functional correctness of a design for a broad range of abstraction levels. In mainstream industry verification methodologies, typical setups coordinate the validation effort of a complex digital system by distributing logic simulation tasks among vast server farms for months at a time. Yet, the performance of logic simulation is not sufficient to satisfy the demand, leading to incomplete validation processes, escaped functional bugs, and continuous pressure on the EDA industry to develop faster simulation solutions. In this work we propose GCS, a solution to boost the performance of logic simulation, gate-level simulation in particular, by more than a factor of 10 using recent hardware advances in Graphic Processing Unit (GPU) technology. Noting the vast available parallelism in the hardware of modern GPUs and the inherently parallel structures of gate-level netlists, we propose novel algorithms for the efficient mapping of complex designs to parallel hardware. Our novel simulation architecture maximizes the utilization of concurrent hardware resources while minimizing expensive communication overhead. The experimental results show that our GPU-based simulator is capable of handling the validation of industrial-size designs while delivering more than an order-ofmagnitude performance improvements on average, over the fastest multithreaded simulators commercially available. © 2011 ACM.",Gate-level simulation; General-purpose graphics processing unit (GP-GPU); GPU computing; High-performance simulation; Parallel CAD,Algorithms; Computer hardware; Design; Industry; Program processors; Gate-level simulation; GPU computing; Graphics Processing Unit; High-performance simulation; Parallel CAD; Computer graphics equipment
Accelerating UNISIM-based cycle-level microarchitectural simulations on multicore platforms,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960682986&doi=10.1145%2f1970353.1970359&partnerID=40&md5=8c29700178bd03d7d2d56589bf22f4f0,"UNISIM has been shown to ease the development of simulators for multi-/many-core systems. However, UNISIM cycle-level simulations of large-scale multiprocessor systems could be very time consuming. In this article, we propose a systematic framework for accelerating UNISIM cycle-level simulations on multicore platforms. The proposed framework relies on exploiting the fine-grained parallelism within the simulated cycles using POSIX threads. A multithreaded simulation engine has been devised from the single-threaded UNISIM SystemC engine to facilitate the exploitation of inherent parallelism. An adaptive technique that manages the overall computation workload by adjusting the number of threads employed at any given time is proposed. In addition, we have introduced a technique to balance the workloads of multithreaded executions. This load balancing involves the distributions of SystemC objects among threads. A graphpartitioning- based technique has been introduced to automate such distributions. Finally, two strategies are proposed for realizing nonautomated and fully automated adaptive multithreaded simulations, respectively. Our investigations show that notable acceleration can be achieved by deploying the proposed framework. In particular, we show that simulations on an 8-core multicore platform can provide for close to 6X speedups when simulating many-core systems with large number of cores. © 2011 ACM.",Cyclelevel microarchitectural simulation; Multi-/many-core systems; Parallelization; Parallelization of system C engine; Simulation; UNISIM,Microarchitectural simulation; Multi-/many-core systems; Parallelizations; Simulation; UNISIM
Design and implementation of a throughput-optimized GPU floorplanning algorithm,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960672154&doi=10.1145%2f1970353.1970356&partnerID=40&md5=740887fb69b0465defb333c667fd4620,"In this article, we propose a novel floorplanning algorithm for GPUs. Floorplanning is an inherently sequential algorithm, far from the typical programs suitable for Single-Instruction Multiple-Thread (SIMT)-style concurrency in a GPU. We propose a fundamentally different approach of exploring the floorplan solution space, where we evaluate concurrent moves on a given floorplan. We illustrate several performance optimization techniques for this algorithm in GPUs. To improve the solution quality, we present a comprehensive exploration of the design space, including various techniques to adapt the annealing approach in a GPU. Compared to the sequential algorithm, our techniques achieve 6-188X speedup for a range of MCNC and GSRC benchmarks, while delivering comparable or better solution quality. © 2011 ACM.",Floorplanning; GPU; Parallel CAD,Benchmarking; Optimization; Program processors; Sequential switching; Design spaces; Floor-planning; Floorplans; GPU; Parallel CAD; Performance optimizations; Sequential algorithm; Solution quality; Solution space; Algorithms
Locality-driven parallel static analysis for power delivery networks,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960696185&doi=10.1145%2f1970353.1970361&partnerID=40&md5=c3e863ebb6baec7336f071d6b75ecf55,"Large VLSI on-chip Power Delivery Networks (PDNs) are challenging to analyze due to the sheer network complexity. In this article, a novel parallel partitioning-based PDN analysis approach is presented. We use the boundary circuit responses of each partition to divide the full grid simulation problem into a set of independent subgrid simulation problems. Instead of solving exact boundary circuit responses, a more efficient scheme is proposed to provide near-exact approximation to the boundary circuit responses by exploiting the spatial locality of the flip-chip-type power grids. This scheme is also used in a block-based iterative error reduction process to achieve fast convergence. Detailed computational cost analysis and performance modeling is carried out to determine the optimal (or near-optimal) number of partitions for parallel implementation. Through the analysis of several large power grids, the proposed approach is shown to have excellent parallel efficiency, fast convergence, and favorable scalability. Our approach can solve a 16-million-node power grid in 18 seconds on an IBM p5-575 processing node with 16 Power5+ processors, which is 18.8X faster than a state-of-the-art direct solver. © 2011 ACM.",Locality; Parallel; Partitioning based; Power delivery network; Static analysis,Cost accounting; Electric power distribution; Electric power transmission; Optimization; Static analysis; VLSI circuits; Analysis approach; Circuit response; Computational costs; Direct solvers; Fast convergence; Grid simulations; Iterative error; Large power; Locality; Network complexity; On chips; Parallel; Parallel efficiency; Parallel implementations; Partitioning based; Performance Modeling; Power delivery network; Power grids; Processing nodes; Spatial locality; Sub-grids; Electric network analysis
Scan-based attacks on linear feedback shift register based stream ciphers,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953889852&doi=10.1145%2f1929943.1929952&partnerID=40&md5=491da5c322a6bea15d27cc0bc43352d3,"Stream cipher is an important class of encryption algorithm that encrypts plaintext messages one bit at a time. Various stream ciphers are deployed in wireless telecommunication applications because they have simple hardware circuitry, are generally fast and consume very low power. On the other hand, scan-based Design-for-Test (DFT) is one of the most popular methods to test IC devices. All flip-flops in the Design Under Test are connected to one or more scan chains and the states of the flip-flops can be scanned out through these chains. In this paper, we present an attack on stream cipher implementations by determining the scan chain structure of the Linear Feedback Shift Registers in their implementations. Although scan-based DFT is a powerful testing scheme, we show that it can be used to retrieve the information stored in a crypto chip thus compromising its theoretically proven security. © 2011 ACM.",LFSR; RFID; Scan-based DFT; Side-channel attack; Stream Cipher,Computer hardware description languages; Cryptography; Flip flop circuits; Hydraulics; Integrated circuit testing; LFSR; RFID; Scan-based DFT; Side-channel attack; Stream Cipher; Shift registers
A gridless routing system with nonslicing floorplanning-based crosstalk reduction on gridless track assignment,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953899927&doi=10.1145%2f1929943.1929951&partnerID=40&md5=a2d5e0e82a6b9deeb89c67c7471db067,"Track assignment, which is an intermediate stage between global routing and detailed routing, provides a good platform for promoting performance, and for imposing additional constraints during routing, such as crosstalk. Gridless track assignment (GTA) has not been addressed in public literature. This work develops a gridless routing system integrating a congestion-driven global router, crosstalk-driven GTA and an enhanced implicit connection-graph-based router. Initial assignment is produced rapidly with a left-edge like algorithm. Crosstalk reduction on the assignment is then transformed to a restricted nonslicing floorplanning problem, and a deterministic O-Tree based algorithm is employed to reassign each net segment. Finally, each panel is partitioned into several subpanels, and the subpanels are reordered using branch and bound algorithm to decrease the crosstalk further. Before detailed routing, routing tree construction is undertaken for placed IRoutes and other pins; many original point-to-point routings are set to connect to IRoutes, and can be accomplished simply with pattern routing. For detailed routing, this work proposes a rapid extraction method for pseudomaximum stripped tiles to boost path propagation. Experimental results demonstrate that the proposed gridless routing system has over 2.02 times the runtime speedup in average for fixed-and variablerule routings of an implicit connection-graph-based router, NEMO. As compared with a commercial routing tool, this work yields an average reduction rate of 13.8% in coupling capacitance calculated using its built-in coupling capacitance estimator. © 2011 ACM.",Crosstalk reduction; Detailed routing; Full-chip routing; Gridless routing; Implicit connection-graph based router; Non-slicing floorplanning,Algorithms; Capacitance; Crosstalk; Linear programming; Crosstalk reduction; Detailed routing; Full-chip routing; Gridless; Implicit connection-graph based router; Non-slicing floorplanning; Trees (mathematics)
Automatic memory partitioning and scheduling for throughput and power optimization,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953855900&doi=10.1145%2f1929943.1929947&partnerID=40&md5=415b12d7565afbca2f79d15521d61cf0,"Memory bottleneck has become a limiting factor in satisfying the explosive demands on performance and cost in modern embedded system design. Selected computation kernels for acceleration are usually captured by nest loops, which are optimized by state-of-the-art techniques like loop tiling and loop pipelining. However, memory bandwidth bottlenecks prevent designs from reaching optimal throughput with respect to available parallelism. In this paper we present an automatic memory partitioning technique which can efficiently improve throughput and reduce energy consumption of pipelined loop kernels for given throughput constraints and platform requirements. Also, our proposed algorithm can handle general array access beyond affine array references. Our partition scheme consists of two steps. The first step considers cycle accurate scheduling information to meet the hard constraints on memory bandwidth requirements specifically for synchronized hardware designs. An ILP formulation is proposed to solve the memory partitioning and scheduling problem optimally for small designs, followed by a heuristic algorithm which is more scalable and equally effective for solving large scale problems. Experimental results show an average 6x throughput improvement on a set of real-world designs with moderate area increase (about 45% on average), given that less resource sharing opportunities exist with higher throughput in optimized designs. The second step further partitions the memory banks for reducing the dynamic power consumption of the final design. In contrast to previous approaches, our technique can statically compute memory access frequencies in polynomial time with little or no profiling. Experimental results show about 30% power reduction on the same set of benchmarks. © 2011 ACM.",Behavioral synthesis; Memory partition,Cache memory; Embedded software; Embedded systems; Energy utilization; Heuristic algorithms; Optimization; Polynomial approximation; Systems analysis; Throughput; Array references; Behavioral synthesis; Computation kernel; Cycle accurate; Dynamic power consumption; Energy consumption; Hard constraints; Hardware design; ILP formulation; Large-scale problem; Limiting factors; Loop pipelining; Loop tiling; Memory access; Memory bandwidths; Memory banks; Memory bottleneck; Memory partition; Modern embedded systems; Optimized designs; Partition schemes; Partitioning techniques; Polynomial-time; Power Optimization; Power reductions; Real-world designs; Resource sharing; Scheduling information; Scheduling problem; Throughput constraints; Throughput improvement; Design
A parallel branch-and-cut approach for detailed placement,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953848782&doi=10.1145%2f1929943.1929950&partnerID=40&md5=1368ae9b106fa97472b45343ece1c234,"We introduce a technique that utilizes distributing computing resources for the efficient optimization of a traditional physical design problem. Specifically, we present a detailed placement strategy designed to exploit distributed computing environments, where the additional computing resources are employed in parallel to improve the optimization time. A Mixed Integer Programming (MIP) model and branch-and-cut optimization strategy are employed to solve the standard cell placement problem. By exploiting the problem structure, our algorithm improves upon the solutions afforded by existing optimization algorithms. First, an efficient batch-branching technique can eliminate several integer decision variables during each step of the optimization procedure. This batch-branching scheme can be performed serially or in parallel. In addition, custom cutting-planes are shown to significantly reduce the run time for optimizations as they efficiently refine the feasible region in order to quickly produce integer solutions. Our serial branch-and-cut strategies allow for significant reductions in wirelength, relative to the state-of-the-art commercial software package CPLEX, assuming a fixed allotment of time. Furthermore, we show that distributed computing resources can be used to significantly reduce the time required to achieve reductions in wirelength. © 2011 ACM.",Detailed placement; MIP; Parallel,Algorithms; Electric batteries; Optimization; Branch-and-cut; Commercial software; Computing resource; Cutting planes; Decision variables; Detailed placement; Distributed computing environment; Distributed computing resources; Distributing computing; Feasible regions; Integer solutions; MIP; Mixed integer programming model; Optimization algorithms; Optimization procedures; Optimization strategy; Parallel; Physical design; Placement strategy; Problem structure; Runtimes; Standard-cell placement; Wire length; Integer programming
Dimension-reducible boolean functions based on affine spaces,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953856940&doi=10.1145%2f1929943.1929945&partnerID=40&md5=a26423d30ba59a65e2cde54f3f7d8adb,"We define and study a new class of regular Boolean functions called D-reducible. A D-reducible function, depending on all its n input variables, can be studied and synthesized in a space of dimension strictly smaller than n. We show that the D-reducibility property can be efficiently tested, in time polynomial in the representation of f, that is, an initial SOP form of f. A D-reducible function can be efficiently decomposed, giving rise to a new logic form, that we have called DredSOP. This form is shown here to be generally smaller than the corresponding minimum SOP form. Our experiments have also shown that a great number of functions of practical importance are indeed D-reducible, thus validating the overall interest of our approach. © 2011 ACM.",Automatic synthesis; Logic synthesis; Optimization; Regular Boolean functions,Optimization; Affine space; Automatic synthesis; Input variables; Logic synthesis; Practical importance; Regular Boolean functions; Time polynomials; Boolean functions
Overhead-aware energy optimization for real-time streaming applications on multiprocessor system-on-chip,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953862857&doi=10.1145%2f1929943.1929946&partnerID=40&md5=597c5ee07546e175e6771bb2d5f1ab36,"In this article, we focus on solving the energy optimization problem for real-time streaming applications on multiprocessor System-on-Chip by combining task-level coarse-grained software pipelining with DVS (Dynamic Voltage Scaling) and DPM (Dynamic Power Management) considering transition overhead, inter-core communication and discrete voltage levels. We propose a two-phase approach to solve the problem. In the first phase, we propose a coarse-grained task parallelization algorithm called RDAG to transform a periodic dependent task graph into a set of independent tasks by exploiting the periodic feature of streaming applications. In the second phase, we propose a scheduling algorithm, GeneS, to optimize energy consumption. GeneS is a genetic algorithm that can search and find the best schedule within the solution space generated by gene evolution. We conduct experiments with a set of benchmarks from E3S and TGFF. The experimental results show that our approach can achieve a 24.4% reduction in energy consumption on average compared with the previous work. © 2011 ACM.",Energy optimization; MPSoC; Overhead-aware; Real-time; Streaming applications; Task scheduling,Application specific integrated circuits; Biology; Embedded systems; Energy utilization; Genes; Microprocessor chips; Multiprocessing systems; Multitasking; Optimization; Programmable logic controllers; Voltage stabilizing circuits; Energy optimization; MPSoC; Overhead-aware; Real-time; Streaming applications; Task scheduling; Scheduling algorithms
MicroFix: Using timing interpolation and delay sensors for power reduction,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953896623&doi=10.1145%2f1929943.1929948&partnerID=40&md5=b0343f214b0b2c8e84d2ef51bec773b6,"Taditional DVFS schemes are oblivious to fine-grained adaptability resulting from path-grained timing imbalance. With the awareness of such fine-grained adaptability, better power-performance efficiency can be obtained. We propose a new scheme, MicroFix, to exploit such fine-grained adaptability. We first show the potential resulted from the path-grained timing imbalance and then present a new technique, Timing Interpolation, to reap the fine-grained adaptability for power reduction. Moreover, to eliminate the conservative margins of traditional DVFS, unlike the previous approaches such as Razor that reactively handle the delay errors (induced by aggressively scaled voltage/frequcncy) by enabling error detection and recovery, we propose a proactive approach by error prediction, thereby obviate the high-cost recovery routines. MicroFix was evaluated based on ISCAS89 benchmarks and the floating-point unit adopted by OpenSPARC T1 processor. Compared to ideal traditional DVFS schemes, the experimental results show that for most of the evaluated circuits, MicroFix can help saving up to 20% power consumption without compromising with frequency, at the expense of less than 5% area overhead. Compared to nonideal DVFS schemes (with 10% voltage margin), the power reduction can even reach up to 38% on average. © 2011 ACM.",Delay sensor; DVFS; Fine-grained adaptability; Power reduction; Timing interpolation,Depreciation; Interpolation; Sensors; Delay sensor; DVFS; Fine-grained adaptability; Power reduction; Timing interpolation; Error detection
Reducing the switching activity of test sequences under transparent-scan,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953869938&doi=10.1145%2f1929943.1929949&partnerID=40&md5=ded164eed0893d3edc2150af8c79e32b,"Transparent-scan is a test application scheme for scan circuits. It provides unique opportunities for test compaction that do not exist with the standard test application scheme. We show that it also provides unique opportunities for reducing the power dissipation of a scan-based test set. After translating a standard scanbased test set into a transparent-scan sequence, we apply two operations for reducing the power dissipation of the sequence. The first operation attempts to remove a test vector that causes high power dissipation. The second operation attempts to replace a scan clock cycle with a functional clock cycle, or a functional clock cycle with a scan clock cycle, in order to reduce the power dissipation. Both operations are implemented such that they reduce the power dissipation without reducing the fault coverage. We also consider a third operation that attempts to complement arbitrary values in the transparent-scan sequence in order to further reduce the power dissipation. © 2011 ACM.",Power dissipation; Scan design; Stuck-at faults; Switching activity; Transparent-scan,Clocks; Testing; Power dissipation; Scan designs; Stuck-at faults; Switching activity; Transparent-scan; Electric losses
ACM Transactions on Design Automation of Electronic Systems,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024257134&doi=10.1145%2f1929943.1929944&partnerID=40&md5=3f4904a6a1ac370ece4b3751379a5d2f,[No abstract available],,
2011 ACM TODAES Best Paper Award,2011,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024265952&doi=10.1145%2f2003695.2003696&partnerID=40&md5=cfce77e9e50def32705dc906a3aba466,"In high-level synthesis for real-time embedded systems using heterogeneous functional units (FUs), it is critical to select the best FU type for each task. However, some tasks may not have fixed execution times. This article models each varied execution time as a probabilistic random variable and solves the heterogeneous assignment with probability (HAP) problem. The solution of the HAP problem assigns a proper FU type to each task such that the total cost is minimized while the timing constraint is satisfied with a guaranteed confidence probability. The solutions to the HAP problem are useful for both hard real-time and soft real-time systems. Optimal algorithms are proposed to find the optimal solutions for the HAP problem when the input is a tree or a simple path. Two other algorithms, one is optimal and the other is near-optimal heuristic, are proposed to solve the general problem. The experiments show that our algorithms can effectively reduce the total cost while satisfying timing constraints with guaranteed confidence probabilities. For example, our algorithms achieve an average reduction of 33.0% on total cost with 0.90 confidence probability satisfying timing constraints compared with the previous work using the worst-case scenario. © 2011, ACM. All rights reserved.",,
Low-power hypercube divided memory FFT engine using 3D integration,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650269268&doi=10.1145%2f1870109.1870114&partnerID=40&md5=e181d5de559a54d2381c606d722776f8,In this article we demonstrate a floating point FFT processor that leverages both 3D integration and a unique hypercube memory division scheme to reduce the power consumption of a 1024 point FFT down to 4.227μJ. The hypercube memory division scheme lowers the energy per memory access by 59.2% and increases the total required area by 16.8%. The use of 3D integration reduces the logic power by 5.2%. We describe the tool flow required to realize the 3D implementation and perform a thermal analysis of it. © 2010 ACM.,3DIC; FFT; Scaling; TSV,Geometry; Thermoanalysis; Three dimensional; 3-D integration; 3DIC; A-thermal; FFT; Floating point FFT processor; Hypercube; Low Power; Memory access; Power Consumption; Scaling; Tool flow; TSV; Fast Fourier transforms
A simultaneous input vector control and circuit modification technique to reduce leakage with zero delay penalty,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650282763&doi=10.1145%2f1870109.1870118&partnerID=40&md5=f9122cfb472a3f8e30bccc3ac725377d,"Leakage power currently comprises a large fraction of the total power consumption of an IC. Techniques to minimize leakage have been researched widely. However, most approaches to reducing leakage have an associated performance penalty. In this article, we present an approach which minimizes leakage by simultaneously modifying the circuit while deriving the input vector that minimizes leakage. In our approach, we selectively modify a gate so that its output (in sleep mode) is in a state which helps minimize the leakage of other gates in its transitive fanout. Gate replacement is performed in a slack-aware manner, to minimize the resulting delay penalty. One of the major advantages of our technique is that we achieve a significant reduction in leakage without increasing the delay of the circuit. © 2010 ACM.",ASIC; Input vector control; Leakage; Low power; Subthreshold leakage,Combinatorial circuits; Electric power supplies to apparatus; Vectors; ASIC; Input vector control; Leakage; Low Power; Sub-threshold leakage; Leakage currents
Energy- and performance-efficient communication framework for embedded MPSoCs through application-driven release consistency,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650256244&doi=10.1145%2f1870109.1870117&partnerID=40&md5=62c801256913b8d5fc766aaf9484628f,"We present a framework for performance-, bandwidth-, and energy-efficient intercore communication in embedded MultiProcessor Systems-on-a-Chip (MPSoC). The methodology seamlessly integrates compiler, operating system, and hardware support to achieve a low-cost communication between synchronized producers and consumers. The technique is especially beneficial for datastreaming applications exploiting pipeline parallelism with computational phases mapped to separate cores. Code transformations utilizing a simple ISA support ensure that producer writes are propagated to consumers with a single interconnect transaction per cache block just prior to the producer exiting its synchronization region. Furthermore, in order to completely eliminate misses to shared data caused by interference with private data and also to minimize the cache energy, we integrate to the proposed framework a cache way partitioning policy based on a simple cache configurability support, which isolates the shared buffers from other cache traffic. This mechanism results in significant power savings since only a subset of the cache ways needs to be looked up for each cache access. The end result of the proposed framework is a single communication transaction per shared cache block between a producer and a consumer with no coherence misses on the consumer caches. Our experiments demonstrate significant reductions in interconnect traffic, cache misses, and energy for a set of multiprocessor benchmarks. © 2010 ACM.",Multicore,Communication; Computer operating systems; Cosine transforms; Embedded systems; Multiprocessing systems; Cache access; Cache blocks; Cache energy; Cache Miss; Code transformation; Coherence miss; Communication framework; Configurability; Embedded multiprocessors; Energy efficient; Hardware supports; Inter-core communications; Interconnect traffics; Multi core; Operating systems; Power savings; Private data; Shared buffer; Shared cache; Shared data; Telecommunication systems
Behavior-level observability analysis for operation gating in low-power behavioral synthesis,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650268767&doi=10.1145%2f1870109.1870113&partnerID=40&md5=a0cb3b90db8fa1e17dbcc95d5d7e9472,"Many techniques for power reduction in advanced RTL synthesis tools rely explicitly or implicitly on observability don't-care conditions. In this article we propose a systematic approach to maximize the effectiveness of these techniques by generating power-friendly RTL descriptions in behavioral synthesis. This is done using operation gating, that is, explicitly adding a predicate to an operation based on its observability condition, so that the operation, once identified as unobservable at runtime, can be avoided using RTL power optimization techniques such as clock gating. We first introduce the concept of behavior-level observability and its approximations in the context of behavioral synthesis. We then propose an efficient procedure to compute an approximated behavior-level observability of every operation in a dataflow graph. Unlike previous techniques which work at the bit level in Boolean networks, our method is able to perform analysis at the word level, and thus avoids most computation effort with a reasonable approximation. Our algorithm exploits the observability-masking nature of some Boolean operations, as well as the select operation, and allows certain forms of other knowledge to be considered for stronger observability conditions. The approximation is proved exact for (acyclic) dataflow graphs when non-Boolean operations other than select are treated as black boxes. The behavior-level observability condition obtained by our analysis can be used to guide the operation scheduler to optimize the efficiency of operation gating. In a set of experiments on real-world designs, our method achieves an average of 33.9% reduction in total power; it outperforms a previous method by 17.1% on average and gives close-to-optimal solutions on several designs. To the best of our knowledge, this is the first time behavior-level observability analysis and optimization are performed during behavioral synthesis in a systematic manner. We believe that our idea can be applied to compiler transformations in general. © 2010 ACM.",Behavioral synthesis; Low power; Observability; Operation gating; Scheduling,Boolean algebra; Data flow analysis; Optimization; Program compilers; Behavioral synthesis; Bit level; Black boxes; Boolean Networks; Boolean operations; Clock gating; Compiler transformations; Computation effort; Data-flow graphs; Low Power; Observability analysis; Operation gating; Operation scheduler; Optimal solutions; Power Optimization; Power reductions; Real-world designs; RTL synthesis; Runtimes; Time behavior; Total power; Unobservable; Word level; Observability
VGreen: A system for energy-efficient management of virtual machines,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650280026&doi=10.1145%2f1870109.1870115&partnerID=40&md5=ddfd3e955167cff7d454da9dcec73cfa,"In this article, we present vGreen, a multitiered software system for energy-efficient virtual machine management in a clustered virtualized environment. The system leverages the use of novel hierarchical metrics that work across the different abstractions in a virtualized environment to capture power and performance characteristics of both the virtual and physical machines. These characteristics are then used to implement policies for scheduling and power management of virtual machines across the cluster. We show through real implementation of the system on a state-of-the-art testbed of server machines that vGreen improves both average performance and system-level energy savings by close to 40% across benchmarks with varying characteristics. © 2010 ACM.",Energy; Migration; Virtualization; Workload characterization,Energy management; Virtual reality; Art testbed; Energy; Energy efficient; Energy saving; Migration; Multi-tiered; Performance characteristics; Power managements; Server machines; Show through; Software systems; System levels; Virtual machine management; Virtual machines; Virtualizations; Virtualized environment; Workload characterization; Scheduling
Guest editorial: Current trends in low-power design,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650274085&doi=10.1145%2f1870109.1870110&partnerID=40&md5=fc56370d4aefa8a0cc0a665d4d02e9ed,[No abstract available],,
Scan-cell reordering for minimizing scan-shift power based on nonspecified test cubes,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650295633&doi=10.1145%2f1870109.1870119&partnerID=40&md5=a5c42ba1d2e73f6de492386c67be1246,"This article presents several scan-cell reordering techniques to reduce the signal transitions during the test mode while preserving the don't-care bits in the test patterns for a later optimization. Combined with a pattern-filling technique, the proposed scan-cell reordering techniques can utilize both high response correlations and pattern correlations to simultaneously minimize scan-out and scan-in transitions. Those scan-shift transitions can be further reduced by selectively using the inverse connections between scan cells. In addition, the trade-off between routing overhead and power consumption can also be controlled by the proposed scan-cell reordering techniques. A series of experiments are conducted to demonstrate the effectiveness of each of the proposed techniques individually. © 2010 ACM.",DFT; Low-power testing; Scan testing,DFT; High response; Low Power; Pattern correlation; Power Consumption; Routing overheads; Scan cells; Scan testing; Shift transitions; Signal transition; Test cube; Test Pattern; Optimization
Energy-efficient progressive remote update for flash-based firmware of networked embedded systems,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650296816&doi=10.1145%2f1870109.1870116&partnerID=40&md5=8101a840aba3a2e9603feed0bbb1b18c,"Firmware update over a network connection is an essential but expensive feature for many embedded systems due to not only the relatively high power consumption and limited bandwidth, but also page-granular erasure before rewriting to flash memory. This work proposes a page-level, link-time technique that minimizes not only the size of patching scripts but also perturbation to the firmware memory, over the entire sequence of updates in the system's lifetime. We propose a tool that first clusters functions to minimize caller-callee dependency across pages, and then orders the functions within each page to minimize intrapage perturbation. Experimental results show our technique to reduce the energy consumption of firmware update by 30-42% over the state-of-the-art. Most importantly, this is the first work that has ever shown to evolve well over 41 revisions of a real-world open-source real-time operating system. © 2010 ACM.",Clycomatic complexity; Diff; Embedded systems; High-level analysis; NOR flash memory; Page; Progressive code update,Computer operating systems; Energy utilization; Firmware; Flash memory; Clycomatic complexity; Diff; High-level analysis; NOR flash memory; Page; Progressive code update; Embedded systems
Nanometer MOSFET effects on the minimum-energy point of sub-45nm subthreshold logic-mitigation at technology and circuit levels,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650293307&doi=10.1145%2f1870109.1870111&partnerID=40&md5=958ac2f78c4db339f7929eda4816f81a,"Subthreshold operation of digital circuits enables minimum energy consumption. In this article, we observe that minimum energy Emin of subthreshold logic dramatically increases when reaching 45nm CMOS node. We demonstrate by circuit simulation and analytical modeling that this increase comes from the combined effects of variability, gate leakage, and Drain-Induced Barrier Lowering (DIBL) effect. We then investigate the new impact of individual MOSFET parameters Vg, Vt, and Tox on Emin in sub-45nm technologies. We further propose an optimum MOSFET selection, which favors low-Vt mid-Vg devices in 45nm CMOS technology. The use of such optimum MOSFETs yields 35% Emin reduction for a benchmark multiplier with good speed performances and negligible area overhead. This optimum MOSFET selection can easily be integrated into a standard EDA tool flow by appropriate selection of the standard cell library. We finally demonstrate that undoped-channel fully-depleted Silicon-On-Insulator (SOI) technology brings 60% Emin reduction with baseline MOSFETs thanks to strong mitigation of variability and short-channel effects. This study reveals a new ( à priori counterintuitive) paradigm in device optimization for subthreshold logic: relaxing gate leakage constraints to improve robustness against short-channel effects and variability. Additionally, we propose pre-Silicon BSIM4 MOSFET model cards for realistic subthreshold circuit simulations including variability in bulk and fully depleted SOI technologies, which are made available online. © 2010 ACM.",CMOS digital integrated circuits; Leakage current; Short-channel effects; Silicon-On-Insulator (SOI) technology; Subthreshold logic; Ultra-low power; Variability,Circuit simulation; CMOS integrated circuits; Digital integrated circuits; Electric batteries; Energy utilization; Integrated circuits; Leakage currents; MOSFET devices; Semiconducting silicon compounds; Silicon on insulator technology; Technology; Threshold elements; CMOS digital integrated circuits; Short-channel effect; Silicon-On-Insulator (SOI) technology; Subthreshold logic; Ultra-low power; Variability; Logic circuits
NBTI-aware clustered power gating,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650271386&doi=10.1145%2f1870109.1870112&partnerID=40&md5=ef99cc9b27f61491b79bc3b9e6b8f7ee,"The emergence of Negative Bias Temperature Instability (NBTI) as the most relevant source of reliability in sub-90nm technologies has led to a new facet of the traditional trade-off between power and reliability. NBTI effects in fact manifest themselves as an increase of the propagation delay of the devices over time, which adds up to the delay penalty incurred by most low-power design solutions. This implies that, given a desired lifetime of a circuit (i.e., a given performance target at some point in time), a power-managed component will fail earlier than a nonpowermanaged one. In this work, we show how it is possible to partially overcome this conflict, by leveraging the benefits in terms of aging provided by power-gating (i.e., by using switches that disconnect a logic block from the ground). Thanks to some electrical properties, it is possible to nullify aging effects during standby periods. Based on this important property, we propose a methodology for a NBTI-aware power gating that allows synthesizing low-leakage circuits with maximum lifetime. © 2010 ACM.",Aging; Leakage power; Low-power design; NBTI; Power gating; Reliability,Design; Leakage currents; Reliability; Aging; Leakage power; Low-power design; NBTI; Power gatings; Electric properties
Partitioning techniques for partially protected caches in resource-constrained embedded systems,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958032912&doi=10.1145%2f1835420.1835423&partnerID=40&md5=4cc2f988a9153c19c9372741d5d248f3,"Increasing exponentially with technology scaling, the soft error rate even in earth-bound embedded systems manufactured in deep subnanometer technology is projected to become a serious design consideration. Partially protected cache (PPC) is a promising microarchitectural feature to mitigate failures due to soft errors in power, performance, and cost sensitive embedded processors. A processor with PPC maintains two caches, one protected and the other unprotected, both at the same level of memory hierarchy. The intuition behind PPCs is that not all data in the application is equally prone to soft errors. By finding and mapping the data that is more prone to soft errors to the protected cache, and error-resilient data to the unprotected cache, failures induced by soft errors can be significantly reduced at aminimal power and performance penalty. Consequently, the effectiveness of PPCs critically hinges on the compiler's ability to partition application data into error-prone and error-resilient data. The effectiveness of PPCs has previously been demonstrated on multimedia applications-where an obvious partitioning of data exists, the multimedia data is inherently resilient to soft errors, and the rest of the data and the entire code is assumed to be error-prone. Since the amount of multimedia data is a quite significant component of the entire application data, this obvious partitioning is quite effective. However, no such obvious data and code partitioning exists for general applications. This severely restricts the applicability of PPCs to data caches and instruction caches in general. This article investigates vulnerability-based partitioning schemes that are applicable to applications in general and effectively reduce failures due to soft errors at minimal power and performance overheads. Our experimental results on an HP iPAQ-like processor enhanced with PPC architecture, running benchmarks from the MiBench suite demonstrate that our partitioning heuristic efficiently finds page partitions for data PPCs that can reduce the failure rate by 48% at only 2% performance and 7% energy overhead, and finds page partitions for instruction PPCs that reduce the failure rate by 50% at only 2% performance and 8% energy overhead, on average. © 2010 ACM.",Embedded systems; Page partitioning technique; Partially protected cache; Soft error; Vulnerability,Cache memory; Computer architecture; Cost reduction; Error correction; Microprocessor chips; Application data; Code partitioning; Cost-sensitive; Data caches; Design considerations; Embedded processors; Error prones; Error-resilient; Failure rate; General applications; HP iPAQ; Instruction caches; Memory hierarchy; Minimal power; Multimedia applications; Multimedia data; Partially protected cache; Partitioning techniques; Performance penalties; Resource-constrained; Soft error; Soft error rate; Subnanometers; Technology scaling; Vulnerability; Embedded systems
"Power gating: Circuits, design methodologies, and best practice for standard-cell VLSI designs",2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958043152&doi=10.1145%2f1835420.1835421&partnerID=40&md5=c7ad44e71fcd3ff06ee5104a17bb26e6,"Power gating has become one of the most widely used circuit design techniques for reducing leakage current. Its concept is very simple, but its application to standard-cell VLSI designs involves many careful considerations. The great complexity of designing a power-gated circuit originates from the side effects of inserting current switches, which have to be resolved by a combination of extra circuitry and customized tools and methodologies. In this tutorial we survey these design considerations and look at the best practice within industry and academia. Topics include output isolation and data retention, current switch design and sizing, and physical design issues such as power networks, increases in area and wirelength, and power grid analysis. Designers can benefit from this tutorial by obtaining a better understanding of implications of power gating during an early stage of VLSI designs. We also review the ways in which power gating has been improved. These include reducing the sizes of switches, cutting transition delays, applying power gating to smaller blocks of circuitry, and reducing the energy dissipated in mode transitions. Power gating has also been combined with other circuit techniques, and these hybrids are also reviewed. Important open problems are identified as a stimulus to research. © 2010 ACM.",Design methodology; Leakage current; Low power; Power gating; Standard-cell; VLSI,Design; Electric network analysis; Electric power systems; Integrated circuit manufacture; Leakage currents; Management; Power generation; Standards; Design Methodology; Low Power; Power gatings; Standard-cell; VLSI; VLSI circuits
On the completeness of the polymorphic gate set,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958042565&doi=10.1145%2f1835420.1835425&partnerID=40&md5=7d454e429390272c675fdcc308ca0031,"Polymorphic gates are special kinds of logic gates that can exhibit different functions under the control of environmental parameters, such as light, temperature, and VDD. These polymorphic gates can be used to build polymorphic circuits that perform different functions under different environments. Because polymorphic gates are different from traditional logic gates, the existent completeness theory for the traditional logic gate set is not suitable for the polymorphic gate set. So far, only the definition of the complete polymorphic gate set is given. There is no approach to judging whether a given polymorphic gate set is complete. The contributions of this article include three aspects. First, the impact of logic-1 and logic-0 on the completeness of the polymorphic gate set is discussed. Second, the theory and two related algorithms for judging the completeness of polymorphic gate sets with two modes are given. Finally, the theory and related algorithms for complete polymorphic gate sets with more than two modes are proposed. © 2010 ACM.",Completeness theory; Polymorphic circuit; Polymorphic electronics; Polymorphic gate,Completeness theory; Environmental parameter; Gate sets; Polymorphic circuit; Polymorphic electronics; Polymorphic gate; Logic gates
An in-place search algorithm for the resource constrained scheduling problem during high-level synthesis,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958069365&doi=10.1145%2f1835420.1835422&partnerID=40&md5=6bd473414f33adcf9340cf9db691eb46,"We propose an in-place search algorithm for computing the exact solutions to the resource constrained scheduling problem. This algorithm supports operation chaining, pipelining and multicycling in the underlying scheduling problem. Based on two lower-bound estimation mechanisms that are capable of predicting the criterion values of search nodes represented by partially scheduled data flow graphs, the proposed algorithm can effectively prune the nonpromising search space and finds the optimum usually several times faster than existing techniques. As opposed to existing search-based scheduling techniques whose space complexity is squared or exponential in the search depth, our approach requires only a constant storage space during the traversal of the search tree. The low space complexity is accomplished by using a combination-generating algorithm, which leads our approach to visit search nodes in such a way that each one is obtained by making only a small change to its sibling without keeping any parent nodes in memory. Experimental results on several well known benchmarks with varying resource constraints show the effectiveness of the proposed algorithm. © 2010 ACM.",Design automation; Exact scheduling; High-level synthesis; Optimal scheduling; Resource-constrained scheduling,Computer aided design; Data flow analysis; Learning algorithms; Optimization; Design automations; Exact scheduling; High Level Synthesis; Optimal scheduling; Resource constrained scheduling; Scheduling algorithms
Huffman-based code compression techniques for embedded processors,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958071099&doi=10.1145%2f1835420.1835424&partnerID=40&md5=b41dc385dbfa49dec00a8cdfbe772e4a,"The size of embedded software is increasing at a rapid pace. It is often challenging and time consuming to fit an amount of required software functionality within a given hardware resource budget. Code compression is a means to alleviate the problem by providing substantial savings in terms of code size. In this article we introduce a novel and efficient hardware-supported compression technique that is based on Huffman Coding. Our technique reduces the size of the generated decoding table, which takes a large portion of the memory. It combines our previous techniques, Instruction Splitting Technique and Instruction Re-encoding Technique into new one called Combined Compression Technique to improve the final compression ratio by taking advantage of both previous techniques. The instruction Splitting Technique is instruction set architecture (ISA)-independent. It splits the instructions into portions of varying size (called patterns) before Huffman coding is applied. This technique improves the final compression ratio by more than 20% compared to other known schemes based on Huffman Coding. The average compression ratios achieved using this technique are 48% and 50% for ARM and MIPS, respectively. The Instruction Re-encoding Technique is ISA-dependent. It investigates the benefits of reencoding unused bits (we call them reencodable bits) in the instruction format for a specific application to improve the compression ratio. Reencoding those bits can reduce the size of decoding tables by up to 40%. Using this technique, we improve the final compression ratios in comparison to the first technique to 46% and 45% for ARM and MIPS, respectively (including all overhead that incurs). The Combined Compression Technique improves the compression ratio to 45% and 42% for ARM and MIPS, respectively. In our compression technique, we have conducted evaluations using a representative set of applications and we have applied each technique to two major embedded processor architectures, namely ARM and MIPS. © 2010 ACM.",Code compression; Code density; Embedded systems; Huffman coding,Computer architecture; Decoding; Embedded software; Embedded systems; Encoding (symbols); Plasma waves; Code compression; Code density; Code size; Compression ratios; Compression techniques; Embedded processor architecture; Embedded processors; Hardware resources; Huffman; Huffman coding; Instruction set architecture; Re-encoding; Software functionality; Splitting techniques; Data compression
Complexity of 3-D floorplans by analysis of graph cuboidal dual hardness,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958024352&doi=10.1145%2f1835420.1835426&partnerID=40&md5=4dfa004b9f580ebebd2b849d75f25b87,"Interconnect dominated electronic design stimulates a demand for developing circuits on the third dimension, leading to 3-D integration. Recent advances in chip fabrication technology enable 3-D circuit manufacturing. However, there is still a possible barrier of design complexity in exploiting 3-D technologies. This article discusses the impact of migrating from 2-D to 3-D on the difficulty of floorplanning and placement. By looking at a basic formulation of the graph cuboidal dual problem, we show that the 3-D cases and the 3-layer 2.5-D cases are fundamentally more difficult than the 2-D cases in terms of computational complexity. By comparison among these cases, the intrinsic complexity in 3-D floorplan structures is revealed in the hard-to-decide relations between topological connections and geometrical contacts. The results show possible challenges in the future for physical design and CAD of 3-D integrated circuits. © 2010 ACM.",3-D integration; Cuboidal dual; Floorplanning; Hardness,Computational complexity; Design; Hardness; Topology; 3-D integrated circuit; 3-D integration; Basic formulation; Cuboidal dual; Design complexity; Dual problem; Electronic design; Floor-planning; Floorplanning and placement; Floorplans; In-chip; Physical design; Three dimensional
Concept-based partitioning for large multidomain multifunctional embedded systems,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953582614&doi=10.1145%2f1754405.1754407&partnerID=40&md5=057b6431ec66e4aa0966fb0f7344580a,"Hardware-software partitioning is an important phase in embedded systems. Decisions made during this phase impact the quality, cost, performance, and the delivery date of the final product. Over the past decade or more, various partitioning approaches have been proposed. A majority operate at a relatively fine granularity and use a low-level executable specification as the starting point. This presents problems if the context is families of industrial products with frequent release of upgraded or new members. Managing complexity using a low-level specification is extremely challenging and impacts developer productivity. Designing using a high-level specification and component-based development, although a better option, imposes component integration and replacement problems during system evolution and new product release. A new approach termed Concept-Based Partitioning is presented that focuses on system evolution, product lines, and largescale reuse when partitioning. Beginning with information from UML 2.0 sequence diagrams and a concept repository concepts are identified and used as the unit of partitioning within a specification. A methodology for the refinement of interpart communication in the system specification using sequence diagrams is also presented. Change localization during system evolution, composability during large-scale reuse, and provision for configurable feature variations for a product line are facilitated by a Generic Adaptive Layer (GAL) around selected concepts. The methodology was applied on a subsystem of an Unmanned Aerial Vehicle (UAV) using various concepts which improved the composability of concepts while keeping performance and size overhead within the 2% range. © 2010 ACM.",Codesign; Embedded system design; Product families; System evolution; System partitioning; UML,Biology; Embedded software; Product design; Remotely operated vehicles; Specifications; Unmanned aerial vehicles (UAV); Co-designs; Component integration; Component-Based Development; Composability; Concept-based; Configurable; Embedded system design; Executable specifications; Feature variations; Fine granularity; Hardware software partitioning; High level specification; Industrial product; Multi domains; New approaches; New members; New product; Product-lines; Sequence diagrams; System evolution; System specification; UML 2.0; Embedded systems
Effective congestion reduction for IC package substrate routing,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953563055&doi=10.1145%2f1754405.1754412&partnerID=40&md5=b2f3432728a731e1099ec171881715cd,"Off-chip substrate routing for high-density packages is challenging due to requirements such as high density, lack of vertical detour, non-Manhattan routing, and primarily planar routing. The existing substrate routing algorithms often result in a large number of unrouted nets that have to be routed manually. This article develops an effective yet efficient diffusion-driven method D-Router to reduce congestion. Starting with an initial routing, we develop an effective diffusion-based congestion reduction. We iteratively find a congested window and spread out connections to reduce congestion inside the window by a simulated diffusion process based on the duality between congestion and concentration. The window is released after the congestion is eliminated. Compared with the state-of-the-art substrate routing method that leads to 480 nets unrouted for ten industrial designs with a total of 6415 nets, the D-Router reduces the amount of unrouted netsto 104, a reduction to the 4.6 multiple. In addition, the D-Router obtains a similar reduction on unrouted nets but runs up to 94 times faster when compared with a negotiation-based substrate routing. © 2010 ACM.",Congestion reduction; IC package; Routability; Routing; Substrate,Diffusion; Packaging; Routers; Substrates; Congestion reduction; Diffusion process; Effective diffusion; High density; IC package; Industrial design; Non-Manhattan routing; Off-chip; Routability; Routing methods; Traffic congestion
Logic synthesis and circuit customization using extensive external don't-cares,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953561684&doi=10.1145%2f1754405.1754411&partnerID=40&md5=141b514989ba07f43670d74678ae4991,"Traditional digital circuit synthesis flows start from an HDL behavioral definition and assume that circuit functions are almost completely defined, making don't-care conditions rare. However, recent design methodologies do not always satisfy these assumptions. For instance, third-party IP blocks used in a system-on-chip are often overdesigned for the requirements at hand. By focusing only on the input combinations occurring in a specific application, one could resynthesize the system to greatly reduce its area and power consumption. Therefore we extend modern digital synthesis with a novel technique, called SWEDE, that makes use of extensive external don't-cares. In addition, we utilize such don't-cares present implicitly in existing simulation-based verification environments for circuit customization. Experiments indicate that SWEDE scales to large ICs with half-million input vectors and handles practical cases well. © 2010 ACM.",Circuit customization; Don't-care optimization; Logic synthesis,Application specific integrated circuits; Digital integrated circuits; Microprocessor chips; Optimization; Care optimization; Circuit functions; Design Methodology; Digital synthesis; Input vector; IP block; Logic synthesis; Novel techniques; Power Consumption; Simulation-based; System on chips; Verification environment; Logic circuits
On-chip sensor-driven efficient thermal profile estimation algorithms,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953582947&doi=10.1145%2f1754405.1754410&partnerID=40&md5=28775fa0c8c07a074ac0ba6d885f1295,"This article addresses the problem of chip-level thermal profile estimation using runtime temperature sensor readings. We address the challenges of: (a) availability of only a few thermal sensors with constrained locations (sensors cannot be placed just anywhere); (b) random chip power density characteristics due to unpredictable workloads and fabrication variability. Firstly we model the random power density as a probability density function. Given such statistical characteristics and the runtime thermal sensor readings, we exploit the correlation in power dissipation among different chip modules to estimate the expected value of temperature at each chip location. Our methods are optimal if the underlying power density has Gaussian nature. We give a heuristic method to estimate the chip-level thermal profile when the underlying randomness is non-Gaussian. An extension of our method has also been proposed to address the dynamic case. Several speedup strategies are carefully investigated to improve the efficiency of the estimation algorithm. Experimental results indicated that, given only a few thermal sensors, our method can generate highly accurate chip-level thermal profile estimates within a few milliseconds. © 2010 ACM.",Estimation; On-chip sensor; Statistical; Thermal profile,Estimation; Heuristic methods; Location; Sensors; Chip power; Chip-level; Estimation algorithm; Expected values; Gaussians; Non-Gaussian; On-chip sensors; Power densities; Power dissipation; Runtimes; Statistical; Statistical characteristics; Thermal profiles; Thermal sensors; Probability density function
Race analysis for systemc using model checking,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953587427&doi=10.1145%2f1754405.1754406&partnerID=40&md5=35257fcfdfcb070668545bd9cc55990a,"SystemC is a system-level modeling language that offers a wide range of features to describe concurrent systems at different levels of abstraction. The SystemC standard permits simulators to implement a deterministic scheduling policy, which often hides concurrency-related design flaws. We present a novel compiler for SystemC that integrates a very precise formal race analysis by means of model checking. Our compiler produces a simulator that uses the outcome of the analysis to perform partial order reduction. The key insight to make the model checking engine scale is to apply it only to tiny fractions of the SystemC model. We show that the outcome of the analysis is not only valuable to eliminate redundant context switches at runtime, but can also be used to diagnose race conditions statically. In particular, our analysis is able to reveal races that can remain undetected during simulation and is able to formally prove the absence of races. © 2010 ACM.",Formal analysis; Model checking; Partial-order reduction; Simulation; SystemC,Computer simulation; Logic design; Program compilers; Concurrent systems; Context switch; Design flaws; Deterministic scheduling; Formal analysis; Levels of abstraction; Partial order reductions; Partial-order reduction; Race condition; Runtimes; System-level modeling; SystemC; Model checking
Low-power tinyos tuned processor platform for Wireless Sensor Network motes,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953546678&doi=10.1145%2f1754405.1754408&partnerID=40&md5=aaa4da1672df44888a17d9a39136b1e3,"In this article we describe a low-power processor platform for use in Wireless Sensor Network (WSN) nodes (motes). WSN motes are small, battery-powered devices comprised of a processor, sensors, and a radio frequency transceiver. It is expected that WSNs consisting of large numbers of motes will offer long-term, distributed monitoring, and control of real-world equipment and phenomena. A key requirement for these applications is long battery life. We investigate a processor platform architecture based on an application-specific programmable processor core, System-On-Chip bus, and a hardware accelerator. The architecture improves on the energy consumption of a conventional microprocessor design by tuning the architecture for a suite of TinyOS-based WSN applications. The tuning method used minimizes changes to the instruction set architecture facilitating rapid software migration to the new platform. The processor platform was implemented and validated in an FPGA-based WSN mote. The benefits of the approach in terms of energy consumption are estimated to be a reduction of 48% for ASIC implementation relative to a conventional programmable processor for a typical TinyOS application suite without use of voltage scaling. © 2010 ACM.",Embedded system design; Hardware-software codesign; Low power processor; Wireless Sensor Network,Application specific integrated circuits; Design; Embedded software; Embedded systems; Microprocessor chips; Nanotechnology; Sensor networks; Tuning; Wireless sensor networks; Application-Specific; Battery life; Battery powered devices; Distributed monitoring; Embedded system design; Energy consumption; Hardware accelerators; Hardware software codesign; Instruction set architecture; Low Power; Low power processor; Low power processors; Microprocessor designs; Platform architecture; Programmable processors; Radio frequency transceiver; Real-world; Software migration; System on chips; Tuning method; Voltage-scaling; Computer architecture
Register file partitioning and recompilation for register file power reduction,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953597369&doi=10.1145%2f1754405.1754409&partnerID=40&md5=55db14d916c9a6bb4352f77bf41e0c7f,"Register files in modern embedded processors contribute a substantial budget in the energy consumption due to their large switching capacitance and long working time. For some embedded processors, on average 25% of registers account for 83% of register file accessing time. This motivates us to partition the register file into hot and cold regions, with the most frequently used registers placed in the hot region, and the rarely accessed ones in the cold region. We employ the bit-line splitting and drowsy register cell techniques to reduce the overall register file accessing power. We propose a novel approach to partition the register in a way that can achieve the largest power saving. We formulate the register file partitioning process into a graph partitioning problem, and apply an effective algorithm to obtain the optimal result. We evaluate our algorithm for MiBench and SPEC2000 applications on the SimpleScalar PISA system, and an average saving of 58.3% and 54.4% over the nonpartitioned register file accessing power is achieved. The area overhead is negligible, and the execution time overhead is acceptable (5.5% for MiBench 2.4% for SPEC2000). Further evaluation for MiBench applications is performed on Alpha and X86 system. © 2010 ACM.",Compilers; Low-power design; Processor architectures; Register file partitioning,Computer architecture; Electric load forecasting; Graph theory; Interactive computer systems; Nanotechnology; Area overhead; Bit lines; Cold regions; Effective algorithms; Embedded processors; Energy consumption; Execution time; Graph partitioning problems; Hot regions; Low-power design; Optimal results; Power reductions; Power savings; Processor architectures; Recompilation; Register files; Simplescalar; Working time; Program compilers
Serialized parallel code generation framework for MPSoC,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749271051&doi=10.1145%2f1698759.1698761&partnerID=40&md5=5b6b0f308424ee559c621f771fb7ad5e,"The models of computations that express concurrency naturally are preferred for initial specification of MPSoC system, since popular programming languages such as C and C++ are designed for sequential execution. In our previous work, we proposed a design framework where two models are used for the initial specification of the system behavior; task model at the top level and dataflow model inside each task. After the partition and mapping process is performed with each architecture candidate, the target code is automatically generated for both Design-Space Exploration (DSE) and final implementation. In this article, we focus on parallel code generation for MPSoC, proposing two main techniques. The first is to express functional and data parallelism differently following the partition and mapping decision. In the proposed technique, the generated code consists of multiple tasks running concurrently, which achieves functional parallelism. On the other hand, we use OpenMP directives to express data parallelism inside a task. Second is to adopt the code serialization technique to execute a multitasking application without OS scheduler, aiming to generate the highly portable code on various platforms for an efficient DSE process. We extend the previous code serialization techniques to multiprocessor systems and utilize the formal properties of the dataflow model for efficient code generation. The experiments including H.263 codec example show the viability of the proposed technique and the efficiency of the generated code. © 2010 ACM.",Design-space exploration; Embedded software; Multiprocessor system on chip; Parallel programming; Software generation,C (programming language); Data flow analysis; Design; Embedded software; Fault tolerance; Formal logic; Multiprocessing systems; Network components; Parallel programming; Program compilers; Quality assurance; Space research; Specifications; Automatically generated; Code Generation; Data parallelism; Dataflow model; Design frameworks; Design-space exploration; Formal properties; H.263 codec; Highly-portable; Mapping process; Models of computation; Multi processor systems; Multiple tasks; Multiprocessor system on chip; Multiprocessor system on chips; Parallel code; Programming language; Sequential execution; System behaviors; Target codes; Task models; Mathematical models
Parameterized architecture-level dynamic thermal models for multicore microprocessors,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749243024&doi=10.1145%2f1698759.1698766&partnerID=40&md5=71ed9376f76f71fe7fffe6ecaa9076d6,"In this article, we propose a new architecture-level parameterized dynamic thermal behavioral modeling algorithm for emerging thermal-related design and optimization problems for high-performance multicore microprocessor design. We propose a new approach, called ParThermPOF, to build the parameterized thermal performance models from the given accurate architecture thermal and power information. The new method can include a number of variable parameters such as the locations of thermal sensors in a heat sink, different components (heat sink, heat spreader, core, cache, etc.), thermal conductivity of heat sink materials, etc. The method consists of two steps: first, a response surface method based on low-order polynomials is applied to build the parameterized models at each time point for all the given sampling nodes in the parameter space. Second, an improved Generalized Pencil-Of-Function (GPOF) method is employed to build the transfer-function-based behavioral models for each time-varying coefficient of the polynomials generated in the first step. Experimental results on a practical quad-core microprocessor show that the generated parameterized thermal model matches the given data very well. The compact models by ParThermPOF offer two order of magnitudes speedup over the commercial thermal analysis tool FloTHERM on the given examples. ParThermPOF is very suitable for design space exploration and optimization where both time and system parameters need to be considered. © 2010 ACM.",Architecture; Behavioral modeling; Chip-multiprocessor; Multicore; Thermal modeling,Design; Heat sinks; Multiprocessing systems; Parameterization; Space research; Thermoanalysis; Thermography (temperature measurement); Behavioral model; Behavioral modeling; Chip-multiprocessor; Compact model; Design and optimization; Design space exploration; Generalized pencil-of-function methods; Heat sink materials; Heat spreaders; Low-order polynomials; Microprocessor designs; Multi core; New approaches; Order of magnitude; Parameter spaces; Parameterized; Parameterized model; Response surface method; Thermal analysis; Thermal model; Thermal modeling; Thermal Performance; Thermal sensors; Time points; Time-varying coefficient; Variable parameters; Microprocessor chips
Reliability analysis of memories protected with BICS and a per-word parity bit,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749295450&doi=10.1145%2f1698759.1698768&partnerID=40&md5=5360016fe72b689e3094cc506acc77fc,"This article presents an analysis of the reliability of memories protected with Built-in Current Sensors (BICS) and a per-word parity bit when exposed to Single Event Upsets (SEUs). Reliability is characterized by Mean Time to Failure (MTTF) for which two analytic models are proposed. A simple model, similar to the one traditionally used for memories protected with scrubbing, is proposed for the low error rate case. A more complex Markov model is proposed for the high error rate case. The accuracy of the models is checked using a wide set of simulations. The results presented in this article allow fast estimation of MTTF enabling design of optimal memory configurations to meet specified MTTF goals at minimum cost. Additionally the power consumption of memories protected with BICS is compared to that of memories using scrubbing in terms of the number of read cycles needed in both configurations. © 2010 ACM.",Built-in current sensors; Error correcting codes; Fault-tolerant memory; High-level protection technique,Binary codes; Markov processes; Quality assurance; Sensors; Analytic models; Built-in current sensors; Error correcting code; Error rate; Fast estimation; Fault-tolerant; Markov model; Mean time to failure; Memory configuration; Minimum cost; Parity bits; Power Consumption; Protection techniques; Read-cycle; Simple model; Single event upsets; Reliability analysis
"Speeding-up heuristic allocation, scheduling and binding with SAT-based abstraction/refinement techniques",2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749264850&doi=10.1145%2f1698759.1698762&partnerID=40&md5=e0c1a2ef74f58220c4e36a663433b89b,"Hardware synthesis is the process by which system-level, Register Transfer (RT)-level, or behavioral descriptions can be turned into real implementations, in terms of logic gates. Scheduling is one of the most time-consuming steps in the overall design flow, and may become much more complex when performing hardware synthesis from high-level specifications. Exploiting a single scheduling strategy on very large designs is often reductive and potentially inadequate. Furthermore, finding the best single candidate among all possible scheduling algorithms is practically infeasible. In this article we introduce a hybrid scheduling approach that is a preliminary step towards a comprehensive solution not yet provided by industrial or by academic solutions. Our method relies on an abstract symbolic representation of data flow nodes (operations) bound to control flow paths: it produces a more realistic lower bound during the prescheduling resource estimation step and speeds up slower but accurate heuristic scheduling techniques, thus achieving a globally improved result. © 2010 ACM.",Allocation; Binding; High level synthesis; Resource estimation; Satisfiability; Scheduling,Estimation; Heuristic methods; Behavioral descriptions; Control flows; Data flow; Hardware synthesis; Heuristic scheduling; High level specification; High level synthesis; Hybrid scheduling; Large designs; Lower bounds; Overall design; Pre-scheduling; Register transfer; Resource estimation; Satisfiability; Scheduling strategies; Symbolic representation; System levels; Scheduling algorithms
Hardware/software partitioning and pipelined scheduling on runtime reconfigurable FPGAs,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749271029&doi=10.1145%2f1698759.1698763&partnerID=40&md5=253c8783777c7b20663e1b7fa17b977c,"FPGAs are widely used in today's embedded systems design due to their low cost, high performance, and reconfigurability. Partially RunTime-Reconfigurable (PRTR) FPGAs, such as Virtex-2 Pro and Virtex-4 from Xilinx, allow part of the FPGA area to be reconfigured while the remainder continues to operate without interruption, so that HW tasks can be placed and removed dynamically at runtime. We address two problems related to HW task scheduling on PRTR FPGAs: (1) HW/SW partitioning. Given an application in the form of a task graph with known execution times on the HW (FPGA) and SW (CPU), and known area sizes on the FPGA, find an valid allocation of tasks to either HW or SW and a static schedule with the optimization objective of minimizing the total schedule length (makespan). (2) Pipelined scheduling. Given an input task graph, construct a pipelined schedule on a PRTR FPGA with the goal of maximizing system throughput while meeting a given end-to-end deadline. Both problems are NP-hard. Satisfiability Modulo Theories (SMT) is an extension to SAT by adding the ability to handle arithmetic and other decidable theories. We use the SMT solver Yices with Linear Integer Arithmetic (LIA) theory as the optimization engine for solving the two scheduling problems. In addition, we present an efficient heuristic algorithm based on kernel recognition for the pipelined scheduling problem, a technique borrowed from SW pipelining, to overcome the scalability problem of the SMT-based optimal solution technique. © 2010 ACM.",HW/SW partitioning; Runtime reconfigurable FPGA; Scheduling,Computational complexity; Embedded software; Embedded systems; Heuristic algorithms; Optimization; Scheduling algorithms; Decidable theory; Embedded systems design; End-to-end deadline; Execution time; Hardware/software partitioning; HW/SW partitioning; Integer arithmetic; Low costs; Makespan; NP-hard; Optimal solutions; Optimization engine; Pipelined schedules; Pipelined scheduling; Re-configurable; Reconfigurability; Run-time reconfigurable; Runtime reconfigurable FPGA; Runtimes; Satisfiability modulo Theories; Scalability problems; Schedule length; Scheduling problem; System throughput; Task graph; Task-scheduling; Field programmable gate arrays (FPGA)
Thermal analysis of multiprocessor SoC applications by simulation and verification,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749295470&doi=10.1145%2f1698759.1698765&partnerID=40&md5=7753d02ac2fcbd7887adc614959bc748,"Overheating of computer chips leads to degradation of performance and reliability. Therefore, preventing chips from overheating in spite of increased performance requirements has emerged as a major challenge. Since the cost of cooling has been rising steadily, various architecture and application design techniques are used to prevent chip overheating. Temperature-aware task scheduling has emerged as an important application design methodology for addressing this problem in multiprocessor SoC systems. In this work we present the formulation and implementation of a method for analyzing the thermal (chip heating) behavior of a MPSoC task schedule, during the early stages of the design. We highlight the challenges in developing such a framework and propose solutions for tackling them. Due to nondeterminism in task execution times and decision branches, multiprocessor applications cannot be evaluated accurately by the current state-of-the-art thermal simulation and steady-state analysis methods. Hence an analysis covering nondeterministic execution behaviors is required for thermal analysis of MPSoC task schedules. To address this issue we propose a model checking-based approach for solving the thermal analysis problem and formulate it as a hybrid automata reachability verification problem. We present an algorithm for constructing this hybrid automata given the task schedule, a set of power profiles of tasks, and the Compact Thermal Model (CTM) of the chip. Information about task power consumption is inferred from Markov chains which are learned from power profiles of tasks, obtained from simulation or emulation runs. A numerical analysis-based algorithm which uses CounterExample-Guided Abstraction Refinement (CEGAR) is developed for reachability analysis of this hybrid automata. We propose a directed simulation methodology which uses results of a time-bounded analysis of the hybrid automata modeling thermal behavior of the application, to simulate the expected worst-case execution runs of the same. The algorithms presented in this work have been implemented in a prototype tool called HeatCheck. We present experimental results and analysis of thermal behavior of a set of task schedules executing on a MPSoC system. © 2010 ACM.",Chip temperature; Hybrid automata; Markov chain; Multiprocessor system-on-chip; Thermal analysis,Application specific integrated circuits; Automata theory; Design; Markov processes; Model checking; Numerical analysis; Programmable logic controllers; Robots; Thermoanalysis; Translation (languages); Application design; Chip heating; Chip overheating; Chip temperature; Compact thermal models; Computer chips; Counterexample-guided abstraction refinement; Hybrid automatons; Markov Chain; Multi-processor SoC; Multiprocessor system on chips; Non-determinism; Performance requirements; Power Consumption; Power profile; Prototype tools; Reachability; Reachability analysis; Simulation methodology; Steady-state analysis; Task executions; Task schedule; Task-scheduling; Thermal analysis; Thermal behaviors; Thermal simulations; Verification problems; Worst-case execution; Multiprocessing systems
Benchmarking and evaluating reconfigurable architectures targeting the mobile domain,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749271077&doi=10.1145%2f1698759.1698764&partnerID=40&md5=c88a14100dd91fbdfbc8a228aa7c49f3,"We present the GroundHog 2009 benchmarking suite that evaluates the power consumption of reconfigurable technology for applications targeting the mobile computing domain. This benchmark suite includes seven designs; one design targets fine-grained FPGA fabrics allowing for quick state-of-the-art evaluation, and six designs are specified at a high level allowing them to target a range of existing and future reconfigurable technologies. Each of the six designs can be stimulated with the help of synthetically generated input stimuli created by an open-source tool included in the downloadable suite. Another tool is included to help verify the correctness of each implemented design. To demonstrate the potential of this benchmark suite, we evaluate the power consumption of two modern industrial FPGAs targeting the mobile domain. Also, we show how an academic FPGA framework, VPR 5.0, that has been updated for power estimates can be used to estimates the power consumption of different FPGA architectures and an open-source CAD flow mapping to these architectures. © 2010 ACM.",Benchmark; Benchmarking; FPGAs; Mobile; Power,Design; Field programmable gate arrays (FPGA); Benchmark; Benchmark suites; CAD flow; FPGA architectures; FPGA fabric; Mobile computing domain; Mobile domains; Open source tools; Open-source; Power Consumption; Reconfigurable architecture; Reconfigurable technologies; Benchmarking
"Phase-adjustable error detection flip-flops with 2-stage hold-driven optimization, slack-based grouping scheme and slack distribution control for dynamic voltage scaling",2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749243047&doi=10.1145%2f1698759.1698767&partnerID=40&md5=457ad4e11c522fb11a2c22e464ac9e43,"For Dynamic Voltage Scaling (DVS), we propose a novel design methodology. This methodology is composed of an error detection circuit and three technologies to reduce the area and power penalties which are the large issues for the conventional DVS with error detection. The proposed circuit, Phase-Adjustable Error Detection Flip-Flip (PEDFF), adjusts the clock phase of an additional FF for the timing error detection, based on the timing slack. 2-Stage Hold-Driven Optimization (2-SHDO) technology splits the hold-driven optimization in two stages. Slack-Based Grouping Scheme (SBGS) technology divides each timing path into appropriate groups based on the timing slack. Slack Distribution Control (SDC) technology improves the sharp distribution of the path delay at which the logic synthesis tool has relaxed the delay. We evaluate the methodology by simulating a 32-bit microprocessor in 90 nm CMOS technology. The proposed methodology reduces the energy consumption by 19.8% compared to non-DVS. The OR-tree's latency is shortened to 16.3% compared to the conventional DVS. The area and power penalties for delay buffers on short paths are reduced to 35.0% and 40.6% compared to the conventional DVS, respectively. The proposed methodology with SDC reduces the energy consumption by 17.0% on another example with the sharp slack distribution by the logic synthesis compared to non-DVS. © 2010 ACM.",CTS; DVS; Error detection flip-flop; P & R; STA,CMOS integrated circuits; Flip flop circuits; Optimization; Storm sewers; Synthetic apertures; Technology; Time measurement; Timing circuits; Voltage stabilizing circuits; 90nm CMOS; Clock phase; CTS; Delay buffers; Distribution control; Dynamic voltage scaling; Energy consumption; Logic synthesis; Logic synthesis tools; Novel design methodology; Path delay; Power penalty; Short-path; Timing error detection; Timing slack; Two stage; Error detection
Fast and accurate processor models for efficient MPSoC design,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749243044&doi=10.1145%2f1698759.1698760&partnerID=40&md5=d0d4ce1166a7f16ac613f14ce702659f,"With growing system complexity and ever-increasing software content, the development of embedded software for upcoming MPSoC architectures is a tremendous challenge. Traditional ISS-based validation becomes infeasible due to the large complexity. Addressing the need for flexible and fast simulating models, we introduce in this article our approach of abstract processor modeling in the context of multiprocessor architectures. We combine modeling of computation on processors with an abstract RTOS and accurate interrupt handling into a versatile, multifaceted processor model with several levels of features. Our processor models are utilized in a framework allowing designers to develop a system in a top-down manner using automatic model generation and compilation down to a given MPSoC architecture. During generation, instances of our processor models are integrated into a system model combining software, hardware, and bus communication. The generated system model serves for rapid design space exploration and a fast and accurate system validation. Our experimental results show the benefits of our processor modeling using an actual multiprocessor mobile phone baseband platform. Our abstract models of this complex system reach a simulation speed of 300MCycles/s within a high accuracy of less than 3% error. In addition, our results quantify the speed/accuracy trade-off at varying abstraction levels of our models to guide future processor model designers. © 2010 ACM.",MPSoC; Multi-processor system-on-chip; Performance prediction/estimation; Processor modeling; System-level design; TLM; Transactionlevel model,Abstracting; Application specific integrated circuits; Design; Embedded software; Model structures; Multiprocessing systems; Programmable logic controllers; Space research; Systems analysis; Telecommunication equipment; Abstract models; Abstraction level; Automatic model generation; Base bands; Bus communications; Complex systems; Interrupt handling; Modeling systems; Multi processor architecture; Multiprocessor-system; Processor models; Rapid design; Simulation speed; Software contents; System complexity; System models; System validation; Top-down manner; Transaction-level model; Computer architecture
Call for Papers ACM Transactions on Design Automation of Electronic Systems (TODAES) Special Section on Low-Power Electronics and Design,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025398874&doi=10.1145%2f1698759.1698770&partnerID=40&md5=96d4f661521ecd25d39278b988eba3ad,[No abstract available],,
Low-overhead Fmax calibration at multiple operating points using delay-sensitivity-based path selection,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749264817&doi=10.1145%2f1698759.1698769&partnerID=40&md5=6c878962e1280e5cebdf9be456807af2,"Maximum operating frequency (Fmax) of a system often needs to be determined at multiple operating points, defined by voltage and temperatures. Such calibration is important for the speed binning process, where the voltage-frequency (V-Fmax) relation needs to be accurately determined to sort chips into different bins that can be used for different applications. Moreover, adaptive systems typically require Fmax calibration at multiple operating points in order to dynamically change operating condition such as supply voltage or body bias for power, temperature, or throughput management. For example, a Dynamic Voltage and Frequency Scaling (DVFS) system requires accurate delay calibration at multiple operating voltages in order to apply the correct operating frequency corresponding to a scaled supply. In this article, we propose a low-overhead design technique that allows efficient characterization of Fmax at different operating voltages and temperatures. The proposed method selects a set of representative timing paths in a circuit based on their temperature and voltage sensitivities and dynamically configures them into a ring oscillator to compute the critical path delay. Compared to existing Fmax calibration approaches, the proposed approach provides the following two main advantages: (1) it introduces a delay sensitivity metric to isolate few representative timing paths; (2) it considers actual timing paths instead of critical path replicas, thereby accounting for local within-die delay variations. The all-digital calibration method is robust under process variations and achieves high delay estimation accuracy (< 4% error) at the cost of negligible design overhead (1.7% in delay, 0.3% in power, and 3.5% in die-area). © 2010 ACM.",F<sub>max</sub> calibration; Frequency binning; Temperature adaptation,Adaptive systems; Dies; Microprocessor chips; Multistable circuits; Oscillators (electronic); Sensitivity analysis; Time delay; Time measurement; Timing circuits; Body bias; Critical path delays; Critical Paths; Delay estimation; Delay variation; Design technique; Digital calibrations; Dynamic voltage and frequency scaling; Frequency binning; Maximum operating frequency; Multiple operating points; Operating condition; Operating frequency; Operating voltage; Path selection; Process Variation; Ring oscillator; Speed binning; Supply voltages; Voltage sensitivity; Within dies; Calibration
ACM Journal on Emerging Technologies in Computing Systems,2010,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024276091&doi=10.1145%2f1870109.1870120&partnerID=40&md5=d7bcf8f4f2729fa978042a4a79807285,[No abstract available],,
Autonomous hardware/software partitioning and voltage/frequency scaling for low-power embedded systems,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049151099&doi=10.1145%2f1640457.1640459&partnerID=40&md5=382f9fc2f8ab42164ca9506c535fa718,"Warp processing is a recent computing technology capable of autonomously partitioning the critical kernels within an executing software application to hardware circuits implemented within an on-chip FPGA. While previous performance-driven warp processing has been shown to provide significant performance improvements over software only execution, the dynamic performance improvement of warp processors may be lost for certain application domains, such as real-time systems. Alternatively, as power consumption continue to become a dominant design constraint, we present and thoroughly analyze a low-power warp processing methodology that leverages voltage and/or frequency scaling to substantially reduce power consumption without any performance degradationall without requiring designer effort beyond the initial software development. © 2009 ACM.",Dynamically adaptable systems; Hardware/software partitioning; Low-power; Low-power FPGAs; Reconfigurable computing; Warp processing,Computer software; Embedded systems; Field programmable gate arrays (FPGA); Weaving; Adaptable system; Hardware/software partitioning; Low Power; Low-power FPGAs; Reconfigurable computing; Real time systems
Using stuck-at tests to form scan-based tests for transition faults in standard-scan circuits,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049135320&doi=10.1145%2f1640457.1640464&partnerID=40&md5=f22ca46cbbddcb09a15d3a64c2a99a75,"In enhanced-scan circuits, a two-pattern test < t i, t j > for a transition fault can be obtained by using a test t j that detects a stuck-at fault, and preceding it by a test t i that activates another stuck-at fault. Thus, test generation for transition faults can be done by combining pairs of stuck-at tests. This provides an alternative to deterministic test generation, as well as reduces the test storage requirements for transition fault tests. We study the possibility of generating scan-based tests for transition faults in standard-scan circuits in a similar way, by combining pairs of stuck-at tests. Since it is not always possible to obtain a standard-scan test that is equivalent to a two-pattern test < t i, t j > based on stuck-at tests t i and t j, it is not always possible to guarantee that the combination of t i and t j will detect a transition fault. To compensate for this, it is necessary to try combinations of different stuck-at test pairs, resulting in an increased simulation effort to compute effective standard-scan tests. Our focus in this work is on reducing this simulation effort by reducing the number of stuck-at test pairs that need to be considered. © 2009 ACM.",Broadside tests; Scan circuits; Skewed-load tests; Stuck-at faults; Transition faults,Combinatorial circuits; Dynamic random access storage; Integrated circuit testing; Standards; Load test; Scan circuits; Scan tests; Scan-based test; Storage requirements; Stuck-at faults; Test generations; Transition fault tests; Transition faults; Testing
Performance-constrained voltage assignment in multiple supply voltage SoC floorplanning,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049132279&doi=10.1145%2f1640457.1640460&partnerID=40&md5=f343bd3214c76e54acbd4de79694a0f0,"Using voltage island methodology to reduce power consumption for System-on-a-Chip (SoC) designs has become more and more popular recently. Currently this approach has been considered either in system-level architecture or postplacement stage. Since hierarchical design and reusable intellectual property (IP) are widely used, it is necessary to optimize floorplanning/ placement methodology considering voltage islands generation to solve power and critical path delay problems. In this article, we propose a floorplanning methodology considering voltage islands generation and performance constraints. Our method is flexible and can be extended to hierarchical design. The experimental results on some MCNC benchmarks show that our method is effective in meeting performance constraints and can simultaneously consider the tradeoff between power routing cost and total power dissipation. © 2009 ACM.",,Application specific integrated circuits; Critical path delays; Floor-planning; Hierarchical design; Multiple supply voltages; Performance constraints; Power Consumption; Power routing; System-level architectures; System-on-a-chip designs; Total power dissipation; Voltage assignment; Voltage island; Programmable logic controllers
Design and implementation of an efficient wear-leveling algorithm for solid-state-disk microcontrollers,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049137853&doi=10.1145%2f1640457.1640463&partnerID=40&md5=7823da482d2459340166d2741b529963,"Solid-state disks (SSDs) are storage devices that emulate hard drives with flash memory. They have been widely deployed in mobile computers as disk drive replacements. Flash memory is organized in terms of erase blocks. With the current technology, a block can reach the end of its lifetime after thousands of erasure operations. Wear leveling is a technique to evenly erase the entire flash memory so that all blocks remain alive as long as possible. This study introduces a new wear-leveling algorithm based the observation that, under a real-life mobile PC's workload, most erasure operations are contributed by a small fraction of blocks. Our key ideas are 1) moving rarely updated data to a block that is extraordinarily worn and 2) avoiding repeatedly involving a block in wear-leveling activities. This study presents a successful implementation of the proposed wear-leveling algorithm using about 200 bytes of RAM in an SSD controller rated at 33 MHz. Evaluation results show that this algorithm achieves even wear of the entire flash memory while reducing the overheads of extra flash-memory operations. © 2009 ACM.",Embedded systems; Flash memory; Solid-state disks; Wear leveling,Algorithms; Disks (machine components); Disks (structural components); Drives; Embedded systems; Solid state devices; Current technology; Disk drive; Evaluation results; Hard drives; Mobile computers; Storage devices; Updated data; Wear leveling; Wear-leveling algorithms; Flash memory
Spatial locality exploitation for runtime reordering of JPEG2000 wavelet data layouts,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049154730&doi=10.1145%2f1640457.1640465&partnerID=40&md5=3e9cd341f3df84df7786107e3f631205,"Exploitation of spatial locality is essential for memories to increase the access bandwidth and to reduce the access-related latency and energy per word. Spatial locality exploitation of a kernel can be improved by modifying placement of data in memory, but this may be felt not only by the kernel itself, but also in other application components accessing the same data. Thus care is needed to avoid global miss-rate improvements are thwarted by miss-rate increases in other application components. This article examines application-level miss-rate increases due to handling modified Wavelet Transform data layouts by explicitly reordering at runtime, exploiting the execution order freedom within a reordering buffer when the layout of surrounding components is known. For the JPEG2000 application, taking into account the reordering costs still results in 80% net WT miss-rate gains. © 2009 ACM.",Layout transformations; Spatial locality; Wavelet transform,Digital image storage; Wavelet transforms; Data layouts; JPEG 2000; Layout transformations; Miss-rate; Other applications; Runtimes; Spatial locality; Data handling
Automatic design of application-specific reconfigurable processor extensions with UPaK synthesis kernel,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049161644&doi=10.1145%2f1640457.1640458&partnerID=40&md5=e6e644ec6f2e38dc24a0d2bcbde33447,"This article presents a new tool for automatic design of application-specific reconfigurable processor extensions based on UPaK (Abstract Unified Patterns Based Synthesis Kernel for Hardware and Software Systems). We introduce a complete design flow that identifies new instructions, selects specific instructions and schedules a considered application on the newly created reconfigurable architecture. The identified extensions are implemented as specialized sequential or parallel instructions. These instructions are executed on a reconfigurable unit implementing all merged patterns. Our method uses specially developed algorithms for subgraph isomorphism that are implemented as graph matching constraints. These constraints together with separate algorithms are able to efficiently identify computational patterns and carry out application mapping and scheduling. Our methods can handle both time-constrained and resource-constrained scheduling. Experimental results show that the presented method provides high coverage of application graphs with small number of patterns and ensures high application execution speedup both for sequential and parallel application execution with reconfigurable processor extensions implementing selected patterns. © 2009 ACM.",Constraint programming; Reconfigurable architectures; Resource assignment; Scheduling; System-level synthesis,Computational efficiency; Computer programming; Constraint theory; Pattern matching; Set theory; Application execution; Application mapping; Application-Specific; AS graph; Automatic design; Computational patterns; Constraint programming; Design flows; Hardware and software; New tools; Parallel application; Parallel instructions; Re-configurable; Reconfigurable architecture; Reconfigurable processors; Resource assignment; Resource constrained scheduling; Scheduling systems; Separate algorithm; Specific instruction; Subgraph isomorphism; Synthesis kernels; Reconfigurable hardware
Minimizing leakage power of sequential circuits through mixed-Vt flip-flops and multi-Vt combinational gates,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049093998&doi=10.1145%2f1640457.1640461&partnerID=40&md5=853a683a5807a9fad134cbb519bb1c16,"The current use of multi-Vt to control leakage power targets combinational gates, even though sequential elements such as flip-flops and latches also contribute appreciable leakage. We can, nevertheless, apply multi-Vt to flip-flops, but few can take advantage of high-V t, which causes abrupt changes in timing. We combine low- and high-Vt at the transistor level to design mixed-Vt flip-flops with reduced leakage, an unchanged footprint, and a small increase in either setup time or clock-to-Q delay, but not both. An allocation algorithm for two Vts determines the Vt (mixed, high, or low) of each flip-flop and the Vt of each combinational gate (high or low) in a sequential circuit. Experiments with 65-nm technology show an average leakage saving of 42% compared to conventional multi-Vt approaches; the leakage of flip-flops alone is cut by 78%. This saving is largely unaffected by die-to-die or within-die process variations, which we show through simulations. Standard deviation of leakage caused by process variation is also reduced due to less use of low-Vt devices. We also extend our approach to three Vts, and obtain a further 14% reduction in leakage. © 2009 ACM.",Flip-flop; Leakage current; Low power; Mixed-V<sub>t</sub>; Sequential circuit,Dies; Flip flop circuits; Sequential circuits; Abrupt change; Allocation algorithm; Clock-to-Q delay; Leakage power; Low Power; Process Variation; Sequential elements; Set-up time; Show through; Standard deviation; Transistor level; Within-die process; Leakage currents
Circuit optimization techniques to mitigate the effects of soft errors in combinational logic,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049136046&doi=10.1145%2f1640457.1640462&partnerID=40&md5=504a0150f65bf66191d83743ee52d490,"Soft errors in combinational logic circuits are emerging as a significant reliability problem for VLSI designs. Technology scaling trends indicate that the soft error rates (SER) of logic circuits will be dominant factor for future technology generations. SER mitigation in logic can be accomplished by optimizing either the gates inside a logic block or the flipflops present on the block boundaries. We present novel circuit optimization techniques that target these elements separately as well as in unison to reduce the SER of combinational logic circuits. First, we describe the construction of a new class of flip-flop variants that leverage the effect of temporal masking by selectively increasing the length of the latching window thereby preventing faulty transients from being registered. In contrast to previous flip-flop designs that rely on logic duplication and complicated circuit design styles, the new variants are redesigned from the library flip-flop using efficient transistor sizing. We then propose a flip-flop selection method that uses slack information at each primary output node to determine the flip-flop configuration that produces maximum SER savings. Next, we propose a gate sizing algorithm that trades off SER reduction and area overhead. This approach first computes bounds on the maximum achievable SER reduction by resizing a gate. This bound is then used to prune the circuit graph, arriving at a smaller set of candidate gates on which we perform incremental sensitivity computations to determine the gates that are the largest contributors to circuit SER. Third, we propose a unified, co-optimization approach combining flip-flop selection with the gate sizing algorithm. The joint optimization algorithm produces larger SER reductions while incurring smaller circuit overhead than either technique taken in isolation. Experimental results on a variety of benchmarks show average SER reductions of 10.7X with gate sizing, 5.7X with flip-flop assignment, and 30.1X for the combined optimization approach, with no delay penalties and area overheads within 5-6%. The runtimes for the optimization algorithms are on the order of 1-3 minutes. © 2009 ACM.",Circuit optimization; Combinational logic; Sequential circuits; Soft errors,Algorithms; Electric currents; Error correction; Flip flop circuits; Gates (transistor); Logic design; Microprocessor chips; Optimization; Sequential circuits; Switching circuits; VLSI circuits; Area overhead; Average SER; Block boundaries; Circuit graphs; Circuit optimization; Co-optimization; Combinational logic; Combinational logic circuits; Complicated circuits; Delay penalty; Dominant factor; Flip-flop designs; Future technologies; Gate sizing; Joint optimization; Latching window; Logic blocks; Logic duplication; Optimization algorithms; Optimization approach; Primary outputs; Reliability problems; Runtimes; Selection methods; Sensitivity computation; Soft error; Soft error rate; Technology scaling; Temporal masking; Transistor sizing; VLSI design; Integrated circuit manufacture
Thermal sensor allocation and placement for reconfigurable systems,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349144442&doi=10.1145%2f1562514.1562518&partnerID=40&md5=5817ba40caa65b2d2a2a3c2c52ddb89b,"A dynamic monitoring of thermal behavior of hardware resources using thermal sensors is very important to maintain the operation of systems safe and reliable. This article addresses the problem of thermal sensor allocation and placement for reconfigurable systems. For programmable logic arrays, the degree of the use of hardware resources in the systems highly depends on the target application to be implemented, making the allocation of thermal sensors at the manufacturing stage inadequate (or too costly if implemented) due to the unpredictable thermal profile. This means that the thermal sensor allocation could be processed at the time when the reconfigurable logic is implemented (i.e., at the post manufacturing stage). This work proposes an effective solution to the problem of thermal sensor allocation and placement at the post-manufacturing stage. Specifically, we define the Sensor Allocation and Placement Problem (SAPP), and propose a solution which formulates SAPP into the Unate-Covering Problem (UCP) and solves it optimally. Also we combine SAPP with temperature correlation to reduce required sensors more aggressively and propose a solution by applying UCP again. We then provide an extended solution to handle a practical design issue where the hardware resources for the sensor implementation on specific array locations have already been used up by the application logic. Experimental results using MCNC benchmarks show that our proposed technique uses 62.4% and 19.7% less number of sensors to monitor hotspots on the average than that used by the grid-based and the bisection-based approaches while the overhead of auxiliary circuitry is minimized, respectively. © 2009 ACM.",Optimal placement; Reconfigurable system; Thermal sensor; Unate-covering problem,Structural design; Application logic; Covering problems; Design issues; Dynamic monitoring; Effective solution; Grid-based; Hardware resources; Hotspots; Manufacturing stages; Optimal placement; Placement problems; Programmable logic array; Reconfigurable logic; Reconfigurable system; Reconfigurable systems; Sensor allocation; Sensor implementation; Target application; Temperature correlation; Thermal behaviors; Thermal profiles; Thermal sensor; Thermal sensors; Unate-covering problem; Sensors
SUPERB: Simulator utilizing parallel evaluation of resistive bridges,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349125815&doi=10.1145%2f1562514.1596831&partnerID=40&md5=ffb8cec08cf50a38b9476204fb95ff6e,"A high-performance resistive bridging fault simulator SUPERB (Simulator Utilizing Parallel Evaluation of Resistive Bridges) is proposed. It is based on fault sectioning in combination with parallel-pattern or parallel-fault multiple-stuck-at simulation. It outperforms a conventional interval-based resistive bridging fault simulator by three orders of magnitude while delivering identical results. Further competing tools are outperformed by several orders of magnitude. Industrial-size circuits, including a multi-million-gates design, could be simulated with runtimes within an order of magnitude of the runtimes for pattern-parallel stuck-at fault simulation. © 2009 ACM.",Bridging fault simulation; Fault mapping; PPSFP; Resistive bridging faults; SPPFP,Bridging fault simulation; Fault mapping; PPSFP; Resistive bridging faults; SPPFP; Simulators
A hardware platform for efficient worm outbreak detection,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349094834&doi=10.1145%2f1562514.1562517&partnerID=40&md5=7cfdecc8c4fba93b16383bdf10ef469d,"Network Intrusion Detection Systems (NIDS) monitor network traffic to detect attacks or unauthorized activities. Traditional NIDSes search for patterns that match typical network compromise or remote hacking attempts. However, newer networking applications require finding the frequently repeated strings in a packet stream for further investigation of potential attack attempts. Finding frequently repeated strings within a given time frame of the packet stream has been quite efficient to detect polymorphic worm outbreaks. A novel real-time worm outbreak detection system using two-phase hashing and monitoring repeated common substrings is proposed in this article. We use the concept of shared counters to minimize the memory cost while efficiently sifting through suspicious strings. The worm outbreak system has been prototyped on Altera Stratix FPGA. We have tested the system for various settings and packet stream sizes. Experimental results verify that our system can support line speed of gigabit-rates with negligible false positive and negative rates. © 2009 ACM.",False negative; False positive; Hashing; Network Intrusion Detection System; Polymorphic worm; Shared counters; Worm outbreak,Computer crime; Internet; Personal computing; False negative; False positive; Hashing; Network Intrusion Detection System; Polymorphic worm; Shared counters; Worm outbreak; Intrusion detection
Variation-aware multimetric optimization during gate sizing,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349140901&doi=10.1145%2f1562514.1562522&partnerID=40&md5=68ffbe4f6f5d280028c2ca4e9f237bdb,"The aggressive scaling of technology has not only accentuated the effects of intradie parametric variations in devices, but it has also impacted the effects of optimizing a certain performance metric on the optimality of other metrics. Thus, there is a need for optimization methods that can perform the simultaneous optimization of multiple metrics considering the effects of process variations. In this article, a novel variation-aware gate sizing framework has been developed that can perform simultaneous optimization of multiple performance metrics. In this framework, the relationships between the optimization metrics (like dynamic power, leakage power, and crosstalk noise) are modeled as a function of the gate sizes in the objective function. The delay values obtained from unconstrained delay optimization and the noise margins derived from coupling capacitance information form the constraints for the multimetric optimization problem. As an abstract framework, it is independent of the type of mathematical programming approach as well as the metrics chosen to be optimized. The framework has been implemented using a mathematical programming approach and has been tested on ITC'99 benchmarks for different combinations of multimetric and single-metric optimizations of delay, dynamic power, leakage power, and crosstalk noise. The results indicate that the framework identifies good solution points, and is efficient for postlayout optimization via gate sizing. © 2009 ACM.",Crosstalk noise; Delay; Gate sizing; Mathematical programming; Optimization; Power,Crosstalk; Gates (transistor); Metric system; Parametric devices; Abstract framework; Aggressive scaling; Coupling capacitance; Crosstalk noise; Delay; Delay optimization; Delay values; Dynamic Power; Gate sizing; Leakage power; Noise margins; Objective functions; Optimality; Optimization method; Optimization problems; Parametric variation; Performance metrices; Performance metrics; Post-layout optimization; Power; Process Variation; Simultaneous optimization; Mathematical programming
T-trees: A tree-based representation for temporal and three-dimensional floorplanning,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349150078&doi=10.1145%2f1562514.1562519&partnerID=40&md5=9933324dd2b36b3e1914b64e271b09a3,"Improving logic capacity by time-sharing, dynamically reconfigurable FPGAs are employed to handle designs of high complexity and functionality. In this article, we model each task as a 3D-box and deal with the temporal floorplanning/placement problem for dynamically reconfigurable FPGA architectures. We present a tree-based data structure, called T-trees, to represent the spatial and temporal relations among tasks. Each node in a T-tree has at most three children which represent the dimensional relationship among tasks. For the T-tree, we develop an efficient packing method and derive the condition to ensure the satisfaction of precedence constraints which model the temporal ordering among tasks induced by the execution of dynamically reconfigurable FPGAs. Experimental results show that our tree-based formulation can obtain significantly better solution quality with less execution time than the most recent state-of-the-art work. © 2009 ACM.",Partially dynamical reconfiguration; Reconfigurable computing; Temporal floorplanning,Computer science; Data structures; Field programmable gate arrays (FPGA); Art work; Execution time; Floor-planning; Floorplanning/placement; Logic capacity; Packing method; Partially dynamical reconfiguration; Precedence constraints; Re-configurable; Recent state; Reconfigurable computing; Reconfigurable FPGA; Solution quality; T-trees; Temporal floorplanning; Temporal ordering; Temporal relation; Time-sharing; Tree-based; Three dimensional
"Leakage reduction, delay compensation using partition-based tunable body-biasing techniques",2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349149377&doi=10.1145%2f1562514.1562521&partnerID=40&md5=10cf7b086a6d1b1b9df8680cfcd838f8,"In recent years, fabrication technology of CMOS has scaled to nanometer dimensions. As scaling progresses, several new challenges follow. Among them, the most noticeable two are process variations and leakage current of the circuit. To tackle the problems of process variations and leakage current, an effective way is to use a body-biasing technique. In substance, using the RBB technique can minimize leakage current but increase the delay of a gate. Contrary to RBB, the FBB technique decreases the delay but increases leakage current of a gate. In the previous work, a single body-biasing is applied to the whole circuit. In a slow circuit, since the FBB is applied to the whole circuit, the leakage current of all gates in the circuit increases dramatically. On the other hand, in a fast circuit, RBB is applied to decrease the leakage current. However, without violating the timing specification, the value of body-biasing is restricted by the critical paths, and the saving of leakage current is limited. In this article, we propose a design flow to partition the circuit into subcircuits so that each subcircuit can be applied its individual RBB or FBB. Experiments show that our method is able to save leakage current from 42% to 47% as compared to designs not using a body-biasing technique. Under process variations, our method can save 42% to 49% leakage on fast circuits and 20% to 35% on slow circuits. © 2009 ACM.",Body biasing; Leakage current; Low-power design; Process variations,Design; Body biasing; Critical Paths; Delay compensation; Design flows; Fabrication Technologies; Leakage reduction; Low-power design; Nanometer dimensions; Process Variation; Process variations; Sub-circuits; Timing specifications; Leakage currents
Leakage-aware task scheduling for partially dynamically reconfigurable FPGAs,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349121655&doi=10.1145%2f1562514.1562520&partnerID=40&md5=899b1ea1b449fdad39a2222c638c960d,"As technology continues to shrink, reducing leakage power of Field-Programmable Gate Arrays (FPGAs) becomes a critical issue for the practical use of FPGAs. In this article, we address the leakage issue of partially dynamically reconfigurable FPGA architectures with sleep transistors embedded into FPGA fabrics. In particular, we focus on eliminating leakage waste due to the delay between reconfiguration and execution time of a task. For partially dynamically reconfigurable FPGAs, the configuration prefetching technique is commonly used to hide runtime reconfiguration overhead. With prefetching, the configuration of a task is loaded into FPGAs as early as possible. Therefore, there is often a delay between reconfiguration and execution time of a task. In this period of time, the SRAM cells allocated to a task cannot be turned off even though they are not utilized. In this article, we propose a two-stage task scheduling methodology to reduce leakage waste due to the delay between reconfiguration and execution time of a task without sacrificing performance. In the first stage, a performance-driven task scheduler that targets at minimizing the schedule length is invoked to generate an initial placement. In the second stage, a postplacement leakage-aware task scheduling is applied to refine the initial placement such that leakage waste is minimized provided that the schedule length is not increased. To solve the postplacement leakage optimization problem, we propose two algorithms. The first one is an optimal algorithm based on Integer Linear Programming (ILP). The second algorithm is a heuristic approach that iteratively refines the placement to reduce leakage waste. Experimental results on real and synthetic designs show that the efficiency and effectiveness of the proposed postplacement leakage reduction techniques. © 2009 ACM.",Leakage; Partially dynamical reconfiguration; Placement; Reconfigurable computing; Scheduling,Computer science; Heuristic algorithms; Integer programming; Linearization; Multitasking; Optimization; Refining; Scheduling algorithms; Transistors; Critical issues; Execution time; FPGA fabric; Heuristic approach; Integer Linear Programming; Leakage; Leakage optimization; Leakage power; Leakage reduction techniques; Optimal algorithm; Partially dynamical reconfiguration; Performance-driven; Placement; Practical use; Prefetching; Prefetching techniques; Re-configurable; Reconfigurable computing; Reconfigurable FPGA; Run time reconfiguration; Schedule length; Sleep transistors; SRAM Cell; Synthetic design; Task-scheduling; Two stage; Field programmable gate arrays (FPGA)
2009 ACM TODAES Best Paper Award,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024282407&doi=10.1145%2f1562514.1562515&partnerID=40&md5=00ad75b4f28aeaa22add9275e1bc71ed,[No abstract available],,
Power-delay optimization in VLSI microprocessors by wire spacing,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349143098&doi=10.1145%2f1562514.1562523&partnerID=40&md5=04d6c329087ecb8181e0f3541bd410de,"The problem of optimal space allocation among interconnect wires in a VLSI layout, in order to minimize the switching power consumption and the average signal delay, is addressed in this article. We define a Weighted Power-Delay Sum (WPDS) objective function and derive necessary and sufficient conditions for the existence of optimal interwire space allocation, based on the notion of capacitance density. At the optimum, every wire must be in equilibrium of its line-to-line weighted capacitance density on its two opposite sides, and the WPDS of the whole circuit is minimal if and only if capacitance density is uniformly distributed across the entire layout. This condition is shown to be equivalent to all paths of the layout cross-capacitance graph having the same length and all cuts having the same flow. An implementation which has been used in the design of a recent commercial high-end microprocessor and yielded 17% power reduction and 9% delay reduction in top-level interconnects is presented. © 2009 ACM.",Delay-optimization; Interconnect optimization; Power optimization; Wire spacing,Capacitance; Microprocessor chips; Wire; Capacitance density; Delay reduction; Delay-optimization; High-end microprocessors; Interconnect optimization; Interconnect wires; Objective functions; Optimal space; Power optimization; Power reductions; Power-delay optimization; Signal delays; Space allocation; Sufficient conditions; Switching power; VLSI layout; Wire spacing; Optimization
Efficient memory management for hardware accelerated Java Virtual Machines,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349104586&doi=10.1145%2f1562514.1562516&partnerID=40&md5=4bc98a8519c3b4a5f0d96e3396a06d30,"Application-specific hardware accelerators can significantly improve a system's performance. In a Java-based system, we then have to consider a hybrid architecture that consists of a Java Virtual Machine running on a general-purpose processor connected to the hardware accelerator. In such a hybrid architecture, data communication between the accelerator and the general-purpose processor can incur a significant cost, which may even annihilate the original performance improvement of adding the accelerator. A careful layout of the data in the memory structure is therefore of major importance to maintain the acceleration performance benefits. This article addresses the reduction of the communication cost in a distributed shared memory consisting of the main memory of the processor and the accelerator's local memory, which are unified in the Java heap. Since memory access times are highly nonuniform, a suitable allocation of objects in either main memory or the accelerator's local memory can significantly reduce the communication cost. We propose several techniques for finding the optimal location for each Java object's data, either statically through profiling or dynamically at runtime. We show how we can reduce communication cost by up to 86% for the SPECjvm and DaCapo benchmarks. We also show that the best strategy is application dependent and also depends on the relative cost of remote versus local accesses. For a relative cost higher than 10, a self-learning dynamic approach often results in the best performance. © 2009 ACM.",Dynamic memory management; Hardware acceleration; Java Virtual Machine,Communication; Computer architecture; Costs; General purpose computers; Hardware; Storage allocation (computer); Acceleration performance; Application-specific hardware; Best strategy; Communication cost; Data-communication; Distributed shared memory; Dynamic memory management; General purpose processors; Hardware acceleration; Hardware accelerators; Hardware-accelerated; Hybrid architectures; IMPROVE-A; Java objects; Java Virtual Machine; Java virtual machines; Local memories; Main memory; Memory access; Memory management; Memory structure; Nonuniform; Optimal locations; Performance improvements; Runtimes; Self-learning; System's performance; Acceleration
Theories and algorithms on single-detour routing for untangling twisted bus,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650315185&doi=10.1145%2f1529255.1529268&partnerID=40&md5=4367e1af04b87d63bda5adc326115169,"Previous works on PCB bus routing assume matched pin ordering on both sides. But in practice, the pin ordering might be mismatched and the nets become twisted. In this article, we propose a preprocessing step to untangle such twisted nets. We also introduce a practical routing style, which we call single-detour routing, to simplify the untangling problem. We then present a necessary and sufficient condition for the existence of single-detour routing solutions. Furthermore, we present a dynamic-programming-based algorithm to solve the single-detour untangling problem with consideration of wire capacity between adjacent pins. Our algorithm produces an optimal single-detour routing solution that rematches the pin ordering. By integrating our algorithm into the bus router in a previous length-matching router, we show that many routing problems that cannot be solved previously can now be solved with insignificant increase in runtime. © 2009 ACM.",Bus routing; Dynamic programming; Printed circuit board (PCB); Single-detour routing; Twisted bus,Buses; Dynamic programming; Printed circuit boards; Printed circuit manufacture; Routers; Systems engineering; Bus routing; Pre-processing step; Routing problems; Runtime; Single-detour routing; Sufficient conditions; Twisted bus; Routing algorithms
Custom topology rotary clock router with tree subnetworks,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650333855&doi=10.1145%2f1529255.1529266&partnerID=40&md5=edbe2f7d28ed8632c72ff08f622c446b,"Increasing demands on computing power have spurred the development of faster, higher-density Integrated Circuits (ICs), compounding power and complexity concerns in design budgets. The clock distribution network is a significant contributor to such power and complexity concerns. Resonant rotary clocking is a relatively new technology that realizes several benefits over current clocking methods, including power, frequency, and variation tolerance, yet lacks the automation tools to promote increased use. Towards this end, an automated rotary clock routing methodology is presented that generates custom topology rotary ring routes with tree subnetworks. In addition to the benefits of adiabatic clocking, the presented custom topology router permits 38.6% shorter wirelengths on average for register tapping, compared to traditional prescribed skew, binary tree routing. © 2009 ACM.",Clock network design; Clock skew; Multiphase synchronization; Resonant rotary clocking,Binary trees; Distributed parameter networks; Distribution of goods; Integrated circuits; Power converters; Topology; Automation tools; Clock distribution network; Clock network design; Clock skew; Computing power; Design budget; Multiphase synchronization; New technologies; Over current; Prescribed skew; Resonant rotary clocking; Rotary clock; Subnetworks; Tree routing; Variation tolerances; Electric clocks
Scenario-based timing verification of multiprocessor embedded applications,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650312344&doi=10.1145%2f1529255.1529259&partnerID=40&md5=46509a801751dc57d000727dc473a001,"This work presents a static timing-analysis method for verification of scenario-based real-time properties, on graphical task-level models of embedded applications. Scenario-based properties specify timing constraints which must be honored for specific control-flow behaviors and task execution orderings. Static checking of scenario-based properties currently requires computationally expensive model checking methods. Hence the proposed graph-based static timing-analysis algorithm improves upon the state-of-the-art. This is manifested in a significant performance advantage over timed model checking (up to 1000X in several cases), which suffers from state space explosion. The proposed algorithm also employs compositional reasoning and abstraction refinement for handling large problems. We also illustrate methods for using scenario-based timing analysis, which can act as alternatives to traditional timed model checking for verification of timed systems like FDDI and Fischer protocols. We implement this timing verification algorithm as a tool called SymTime and present experimental results for SymTime comparing it with SPIN, UPPAAL, and a TCTL model checker for Time Petri Nets, called Romeo. © 2009 ACM.",Execution scenarios; Real time systems; Static timing analysis; Timing verification,Algorithms; Graph theory; Interactive computer systems; Petri nets; Real time systems; Systems analysis; Time measurement; Time sharing systems; Timing circuits; Abstraction refinement; Analysis algorithms; Analysis method; Compositional reasoning; Control-flow; Embedded application; Execution scenarios; Graph-based; Level model; Model checker; Real-time properties; State-space explosion; Static checking; Static timing analysis; Task executions; Time Petri nets; Timed model checking; Timed systems; Timing Analysis; Timing constraints; Timing verification; Model checking
A memetic approach to the automatic design of high-performance analog integrated circuits,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650327599&doi=10.1145%2f1529255.1529264&partnerID=40&md5=b0402e16a9e7c01432ffd46febc34659,"This article introduces an evolution-based methodology, named memetic single-objective evolutionary algorithm (MSOEA), for automated sizing of high-performance analog integrated circuits. Memetic algorithms may achieve higher global and local search ability by properly combining operators from different standard evolutionary algorithms. By integrating operators from the differential evolution algorithm, from the real-coded genetic algorithm, operators inspired by the simulated annealing algorithm, and a set of constraint handling techniques, MSOEA specializes in handling analog circuit design problems with numerous and tight design constraints. The method has been tested through the sizing of several analog circuits. The results show that design specifications are met and objective functions are highly optimized. Comparisons with available methods like genetic algorithm and differential evolution in conjunction with static penalty functions, as well as with intelligent selection-based differential evolution, are also carried out, showing that the proposed algorithm has important advantages in terms of constraint handling ability and optimization quality. © 2009 ACM.",Analog circuit sizing; Analog design automation; Constrained optimization; Memetic algorithm,Analog circuits; Analog differential analyzers; Automation; Computer aided design; Constrained optimization; Integrated circuit manufacture; Integrated circuits; Linear integrated circuits; Simulated annealing; Analog Circuit Design; Analog circuit sizing; Analog design automation; Analog integrated circuit; Automatic design; Constraint handling; Constraint-handling techniques; Design specification; Differential Evolution; Differential evolution algorithms; Local search; Memetic; Memetic algorithm; Memetic algorithms; Memetic approach; Objective functions; Optimization quality; Penalty function; Real-coded genetic algorithm; Simulated annealing algorithms; Tight design; Evolutionary algorithms
Systemj compilation using the tandem virtual machine approach,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650321551&doi=10.1145%2f1529255.1529256&partnerID=40&md5=9de737ca6dd2a2e0a554f179b6b4ebe7,"SystemJ is a language based on the Globally Asynchronous Locally Synchronous (GALS) paradigm. A SystemJ program is a collection of GALS nodes, also called clock domains, and each clock domain is a synchronous program that extends the Java language. Initial compilation of SystemJ has been to standard Java executing on a Java Virtual Machine (JVM), which is both inefficient and bulky for small embedded systems. This article proposes a new approach for compiling and executing SystemJ using a new type of virtual machine, called a Tandem Virtual Machine (TVM). The TVM approach provides an efficient implementation of SystemJ on both standard processors and resource-constrained embedded processors. The new approach is based on separating the control-driven and data-driven operations for execution on two virtual machines. While the JVM executes the data-driven operations, a Control Virtual Machine (CVM) is introduced to execute the control-driven parts of a SystemJ program. The TVM approach is capable of handling all data-driven and control-driven operations required by the GALS model. The benchmark results show that the TVM has code size improvements of over 60% on average and also a substantial improvement in execution speed over standard Java-based compilation. © 2009 ACM.",Compilation; Esterel; System-level design; SystemJ; Virtual machines,Computer software; Data handling; Embedded systems; Java programming language; Linguistics; Program compilers; Standards; Compilation; Esterel; System-level design; SystemJ; Virtual machines; Machine design
Selective shielding technique to eliminate crosstalk transitions,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650309272&doi=10.1145%2f1529255.1529265&partnerID=40&md5=d89d73fa0c5cb64347dbf3076e91a6d3,"With CMOS process technology scaling to deep submicron level, propagation delay across long on-chip buses is becoming one of the main performance limiting factors in high-performance designs. Propagation delay is very significant when adjacent wires are transitioning in opposite direction as compared to transitioning in the same direction. As opposite transitions on adjacent wires (called as crosstalk transitions) have significant impact on propagation delay, several bus encoding techniques have been proposed in literature to eliminate such transitions. We propose selective shielding technique to eliminate crosstalk transitions. We show that the selective shielding technique requires ⌈3n/2⌉ wires to encode a n-bit bus. SPICE simulations by considering 90nm technology nodes reveal that, for uniformly distributed random data, our technique achieves nearly 39% (21%) delay savings over 10mm-length uncoded 32-bit bus for pipelined (nonpipelined) data transmission at the cost of nearly 7% energy overhead. © 2009 ACM.",Bus encoding; Crosstalk; Power consumption; Switching activity,Crosstalk; Electric power utilization; Encoding (symbols); Shielding; Wire; 90 nm technology node; Bus encoding; CMOS process technology; Data transmission; Deep sub-micron; High-performance design; On-chip bus; Performance limiting factor; Power consumption; Propagation delays; Random data; Shielding techniques; Significant impacts; SPICE simulations; Switching activity; UNCODED; Buses
Allocating power ground vias in 3D ICs for simultaneous power and thermal integrity,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650321548&doi=10.1145%2f1529255.1529263&partnerID=40&md5=d12b1cdd4d16cea072cbda398520f5bf,"The existing work on via allocation in 3D ICs ignores power/ground vias' ability to simultaneously reduce voltage bounce and remove heat. This article develops the first in-depth study on the allocation of power/ground vias in 3D ICs with simultaneous consideration of power and thermal integrity. By identifying principal ports and parameters, effective electrical and thermal macromodels are employed to provide dynamic power and thermal integrity as well as sensitivity with respect to via density. With the use of sensitivity, an efficient via allocation simultaneously driven by power and thermal integrity is developed. Experiments show that, compared to sequential power and thermal optimization using static integrity, sequential optimization using the dynamic integrity reduces nonsignal vias by up to 18%, and simultaneous optimization using dynamic integrity further reduces nonsignal vias by up to 45.5%. © 2009 ACM.",Macromodeling; Parametric 3D-IC design; Thermal and power integrity,Electric potential; Optimization; Three dimensional; 3-D ICs; Dynamic integrity; Dynamic Power; In-depth study; Macro-models; Macromodeling; Parametric 3D-IC design; Sequential optimization; Simultaneous optimization; Thermal and power integrity; Thermal optimization; Via density; Quality assurance
Generating realistic stimuli for accurate power grid analysis,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650284495&doi=10.1145%2f1529255.1529262&partnerID=40&md5=7fa3249a0917cf7e3b742c5df3b23f6f,"Power analysis tools are an integral component of any current power sign-off methodology. The performance of a design's power grid affects the timing and functionality of a circuit, directly impacting the overall performance. Ensuring power grid robustness implies taking into account, among others, static and dynamic effects of voltage drop, ground bounce, and electromigration. This type of verification is usually done by simulation, targeting a worst-case scenario where devices, switching almost simultaneously, could impose stern current demands on the power grid. While determination of the exact worst-case switching conditions from the grid perspective is usually not practical, the choice of simulation stimuli has a critical effect on the results of the analysis. Targetting safe but unrealistic settings could lead to pessimistic results and costly overdesigns in terms of die area. In this article we describe a software tool that generates a reasonable, realistic, set of stimuli for simulation. The approach proposed accounts for timing and spatial restrictions that arise from the circuit's netlist and placement and generates an approximation to the worst-case condition. The resulting stimuli indicate that only a fraction of the gates change in any given timing window, leading to a more robust verification methodology, especially in the dynamic case. Generating such stimuli is akin to performing a standard static timing analysis, so the tool fits well within conventional design frameworks. Furthermore, the tool can be used for hotspot detection in early design stages. © 2009 ACM.",Ground bounce; Power grid; Simulation; Stimuli generation; Verification; Voltage drop,Computer software; Drops; Electric potential; Electric power systems; Electric power transmission networks; Power generation; Time measurement; Timing circuits; Voltage control; Windows; Ground bounce; Power grid; Simulation; Stimuli generation; Voltage drop; Mesh generation
Simultaneous resource binding and interconnection optimization based on a distributed register-file microarchitecture,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650340972&doi=10.1145%2f1529255.1529257&partnerID=40&md5=3da5647e45d207e88e1bb4dd8cbd1d6c,"Behavior synthesis and optimization beyond the register-transfer level require an efficient utilization of the underlying platform features. This article presents a platform-based resource binding approach based on a Distributed Register-File Microarchitecture (DRFM), which makes efficient use of distributed embedded memory blocks as register files in modern FPGAs. DRFM contains multiple islands, each having a local register file, a functional unit pool, and data-routing logic. Compared to the traditional discrete-register counterpart, a DRFM allows use of the platform-featured on-chip memory or register-file IP blocks to implement its local register files, and this results in a substantial saving of multiplexing logic and global interconnects. DRFM provides a useful architectural template and a direct optimization objective for minimizing interisland connections for synthesis algorithms. Given the scheduling solution and resource (functional units) constraints, two novel algorithms in the resource binding stage are developed based on DRFM: (i) a simultaneous DRFM clustering and binding algorithm, which decides the configuration of DRFM and the assignment of operations into islands with the focus on optimizing global connections; (ii) a data-forwarding scheduling algorithm, which takes advantage of the operation slacks to handle the read-port restriction of register files. On the Xilinx Virtex4 FPGA platform, experimental results with a set of real-life test cases show a 50% logic area reduction achieved by applying our approach, with a 14.6% performance improvement, compared to the traditional discrete-register-based approach. Also, experiments on small-size designs show that our algorithm produces the same number of total connections and at most one more maximum feeding-in connection compared to optimal solutions generated by ILP. © 2009 ACM.",Behavioral synthesis; Distributed register file; Resource binding,Clustering algorithms; Digital signal processing; Field programmable gate arrays (FPGA); Microprocessor chips; Optimization; Area reduction; Behavior synthesis; Behavioral synthesis; Data-forwarding; Direct optimization; Distributed register file; Embedded memory blocks; Functional units; Global interconnects; IP block; Micro architectures; Novel algorithm; On chip memory; Optimal solutions; Performance improvements; Real-life test; Register files; Register transfer level; Resource binding; Synthesis algorithms; Scheduling algorithms
Playing the trade-off game: Architecture exploration using coffeee,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650318482&doi=10.1145%2f1529255.1529258&partnerID=40&md5=cdd0f60b03d4a8f5aa3e1e3143a91e17,"Modern mobile devices need to be extremely energy efficient. Due to the growing complexity of these devices, energy-aware design exploration has become increasingly important. Current exploration tools often do not support energy estimation, or require the design to be very detailed before estimation is possible. It is important to get early feedback on both performance and energy consumption during all phases of the design and at higher abstraction levels. This article presents a unified optimization and exploration framework to explore source-level transformation to processor architecture design space. The proposed retargetable compiler and simulator framework can map applications to a range of processors and memory configurations, simulate, and report detailed performance and energy estimates. An accurate and consistent energy modeling approach is introduced which can estimate the energy consumption of processor and memories at a component level, which can help to guide the design process. Fast energy-aware architecture exploration is illustrated by modeling both state-of-the-art processors as well as other architectures. Various design trade-offs are also illustrated on different academic as well as industrial benchmarks from both the wireless communication and multimedia domain. We also illustrate a design space exploration on different applications and show that there is large trade-off space between application performance, energy consumption, and area. We show that the proposed framework is consistent, accurate, and covers a large design space including various novel low-power extensions in a unified framework. © 2009 ACM.",Architecture exploration; Area; Compiler-architecture interaction; Design; Embedded systems; Energy; Loop transformations; Power estimation; Power-performance trade-off; Processors; VLIW,Commerce; Embedded systems; Energy conversion; Energy efficiency; Estimation; Mobile devices; Product design; Program compilers; Space research; Systems analysis; Very long instruction word architecture; Architecture exploration; Area; Compiler-architecture interaction; Energy; Loop transformations; Power estimation; Power-performance trade-off; Processors; VLIW; Architectural design
Methods for power optimization in SOC-based data flow systems,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650290546&doi=10.1145%2f1529255.1529260&partnerID=40&md5=af2d671cf8e00dcfba57a6f8cb5758c8,"Whereas the computing power of DSP or general-purpose processors was sufficient for 3G baseband telecommunication algorithms, stringent timing constraints of 4G wireless telecommunication systems require computing-intensive data-driven architectures. Managing the complexity of these systems within the energy constraints of a mobile terminal is becoming a major challenge for designers. System-level low-power policies have been widely explored for generic software-based systems, but data-flow architectures used for high data-rate telecommunication systems feature heterogeneous components that require specific configurations for power management. In this study, we propose an innovative power optimization scheme tailored to self-synchronized data-flow systems. Our technique, based on the synchronous data-flow modeling approach, takes advantage of the latest low-power techniques available for digital architectures. We illustrate our optimization method on a complete 4G telecommunication baseband modem and show the energy savings expected by this technique considering present and future silicon technologies. © 2009 ACM.",4G base-band modem; Data-driven SOC; Power optimization; Synchronous data-flow graph,Computer architecture; Computer networks; Data communication systems; Data flow analysis; Digital signal processing; Energy management; General purpose computers; Graph theory; Graphic methods; Modems; Optimization; Programmable logic controllers; Telecommunication; Timing circuits; Wireless sensor networks; 4G base-band modem; Base bands; Computing power; Data flow systems; Data-driven architectures; Data-driven SOC; Data-rate; Dataflow; Digital architecture; Energy constraint; Energy saving; General purpose processors; Generic softwares; Heterogeneous component; Low Power; Mobile terminal; Optimization method; Power managements; Power optimization; Silicon Technologies; Synchronized data; Synchronous data-flow graph; System levels; Timing constraints; Wireless telecommunication systems
Word-length selection for power minimization via nonlinear optimization,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650290545&doi=10.1145%2f1529255.1529261&partnerID=40&md5=0ff6e18b0511128ab369a49210151deb,"This article describes the first method for minimizing the dynamic power consumption of a Digital Signal Processing (DSP) algorithm implemented on reconfigurable hardware via word-length optimization. Fast models for estimating the power consumption of the arithmetic components and the routing power of these algorithm implementations are used within a constrained nonlinear optimization formulation that solves a relaxed version of word-length optimization. Tight lower and upper bounds on the cost of the integer word-length problem can be obtained using the proposed solution, with typical upper bounds being 2.9% and 5.1% larger than the lower bounds for area and power consumption, respectively. Heuristics can then use the upper bound as a starting point from which to get even closer to the known lower bound. Results show that power consumption can be improved by up to 40% compared to that achieved when using simple word-length selection techniques, and further comparisons are made between the minimization of different cost functions that give insight into the advantages offered by multiple word-length optimization. © 2009 ACM.",Bitwidth; Power consumption; Signal processing; Synthesis; Word length,Control theory; Cost functions; Electric power utilization; Routing algorithms; Signal processing; Algorithm implementation; Bitwidth; Constrained nonlinear optimization; Digital signal processing algorithms; Dynamic power consumption; FAST model; Lower and upper bounds; Lower bounds; Non-linear optimization; Power consumption; Power minimization; Reconfigurable hardwares; Selection techniques; Synthesis; Upper Bound; Word length; Constrained optimization
High-performance obstacle-avoiding rectilinear steiner tree construction,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650281459&doi=10.1145%2f1529255.1529267&partnerID=40&md5=c93195c083bbdc68d5a63269db12ecdb,"Rectilinear Steiner trees are used to route signal nets by global and detail routers in VLSI design for a long time. However, in current IC industry, there are significantly increasing obstacles to be considered, such as large-scale power networks, pre-routed nets, IP blocks, and antenna jumpers. Accordingly, the obstacle-avoiding rectilinear Steiner minimal tree (OARSMT) problem has become more important. In this article, we propose a new routing graph, obstacle-avoiding routing graph (OARG), for the OARSMT problem. Due to the important properties of OARG, we construct a 3-step algorithm and a local refinement scheme, which both can take advantage of these properties, to find a suboptimal solution efficiently. Furthermore, each step of our 3-step algorithm as well as the local refinement scheme has theoretical or practical benefits. Therefore, each of them can be applicable to other existing works for general or specific considerations such as efficiency or effectiveness. Extensive experimental results show that our method outperforms all existing works in terms of wirelength and achieves the best speed performance. © 2009 ACM.",Obstacle-avoiding; Rectilinear; Routing; Steiner tree,Integrated circuits; IC industry; IP block; Large-scale power networks; Local refinement; Obstacle-avoiding; Performance obstacles; Rectilinear; Rectilinear steiner trees; Routing; Routing graph; Speed performance; Steiner minimal tree; Steiner tree; Step algorithms; Suboptimal solution; VLSI design; Wire length; Integrated circuit layout
BoxRouter 2.0: A hybrid and robust global router with layer assignment for routability,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849350514&doi=10.1145%2f1497561.1497575&partnerID=40&md5=893658151fd47aca502cc6b2b2645793,"In this article, we present BoxRouter 2.0, and discuss its architecture and implementation. As high-performance VLSI design becomes more interconnect-dominant, efficient congestion elimination in global routing is in greater demand. Hence, we propose a global router which has a strong ability to improve routability and minimize the number of vias with blockages, while minimizing wirelength. BoxRouter 2.0 is extended from BoxRouter 1.0, but can perform multi-layer routing with 2D global routing and layer assignment. Our 2D global routing is equipped with two ideas: node shifting for congestion-aware Steiner tree and robust negotiation-based A* search for routing stability. After 2D global routing, 2D-to-3D mapping is done by the layer assignment which is powered by progressive via/blockage-aware integer linear programming. Experimental results show that BoxRouter 2.0 has better routability with comparable wirelength than other routers on ISPD07 benchmark, and it can complete (no overflow) the widely used ISPD98 benchmark for the first time in the literature with the shortest wirelength. We further generate a set of harder ISPD98 benchmarks to push the limit of BoxRouter 2.0, and propose the hardened ISPD98 benchmarks to map state-of-the-art solutions for future routing research. © 2009 ACM.",Congestion; Global routing; Integer linear programming; Layer assignment; Physical design; Routability; VLSI,Dynamic programming; Integer programming; Integrated circuit layout; Linear programming; Linearization; Optimization; Routers; Congestion; Global routing; Integer linear programming; Layer assignment; Physical design; Routability; VLSI; Routing algorithms
Dynamic security domain scaling on embedded symmetric multiprocessors,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66049151339&doi=10.1145%2f1497561.1497567&partnerID=40&md5=5d0d61e5403531ef3909a2c2be4c040e,"We propose a method for dynamic security-domain scaling on SMPs that offers both highly scalable performance and high security for future high-end embedded systems. Its most important feature is its highly efficient use of processor resources, accomplished by dynamically changing the number of processors within a security-domain (i.e., dynamically yielding processors to other security-domains) in response to application load requirements. Two new technologies make this scaling possible without any virtualization software: (1) self-transition management and (2) unified virtual address mapping. Evaluations show that this domain control provides highly scalable performance and incurs almost no performance overhead in security-domains. The increase in OSs in binary code size is less than 1.5%, and the time required for individual state transitions is on the order of a single millisecond. This scaling is the first in the world to make possible the dynamic changing of the number of processors within a security-domain on an ARM SMP. © 2009 ACM.",AMP; Dynamic security-domain scaling; SMP,Binary codes; Embedded systems; Multiprocessing systems; AMP; Dynamic security; Dynamic security-domain scaling; New technologies; Processor resources; Scalable performance; Self-transition; SMP; State transitions; Symmetric multi-processors; Virtual address; Virtualization software; Shape memory effect
Skew-aware polarity assignment in clock tree,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849300676&doi=10.1145%2f1497561.1497574&partnerID=40&md5=467236b0c32d4add93da65584fee416c,"In modern sequential VLSI designs, clock tree plays an important role in synchronizing different components in a chip. To reduce peak current and power/ground noises caused by clock network, assigning different signal polarities to clock buffers is proposed in previous work. Although peak current and power/ground noises are minimized by signal polarities assignment, an assignment without timing information may increase the clock skew significantly. As a result, a timing-aware signal polarities assigning technique is necessary. In this article, we propose a novel signal polarities assigning technique which can not only reduce peak current and power/ground noises simultaneously but also render the clock skew in control. The experimental result shows that the clock skew produced by our algorithm is 94% of original clock skew in average while the clock skews produced by three algorithms (Partition, MST, Matching) in the absence of post clock tuning steps in the previous work are 235%, 272%, and 283%, respectively. Moreover, our algorithm is as efficient as the three algorithms of the previous work in reducing peak current and power/ground noises. © 2009 ACM.",Clock skew; Clock tree; Peak current; Polarity assignment; Power/ground noise,Algorithms; Time measurement; Clock skew; Clock tree; Peak current; Polarity assignment; Power/ground noise; Electric clocks
Temperature-aware register reallocation for register file power-density minimization,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849331797&doi=10.1145%2f1497561.1497569&partnerID=40&md5=5edc2a77bb14b60e18c6f422eceef72e,"Increased chip temperature has been known to cause severe reliability problems and to significantly increase leakage power. The register file has been previously shown to exhibit the highest temperature compared to all other hardware components in a modern high-end embedded processor, which makes it particularly susceptible to faults and elevated leakage power. We show that this is mostly due to the highly clustered register file accesses where a set of few registers physically placed close to each other are accessed with very high frequency. We propose compile-time temperature-aware register reallocation methodologies for breaking such groups of registers and to uniformly distribute the accesses to the register file. This is achieved with no performance and no hardware overheads. We show that the underlying problem is NP-hard, and subsequently introduce and evaluate two efficient algorithmic heuristics. Our extensive experimental study demonstrates the efficiency of the proposed methodology. © 2009 ACM.",Algorithms; Design; Experimentation,Computational complexity; Chip temperature; Compile time; Embedded processors; Experimental studies; Experimentation; Hardware components; Hardware overheads; Highest temperature; Leakage power; NP-hard; Register files; Register re-allocation; Reliability problems; Very high frequency; Leakage currents
Instrumenting AMS assertion verification on commercial platforms,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849238809&doi=10.1145%2f1497561.1497564&partnerID=40&md5=f325c59022ba35880624c602e9acbd7f,"The industry trend appears to be moving towards designs that integrate large digital circuits with multiple analog/RF (radio frequency) interfaces. In the verification of these large integrated circuits, the number of nets that need to be monitored has been growing rapidly. Consequently, the mixed-signal design community has been feeling the need for AMS (Analog and Mixed Signal) assertions that can automatically monitor conformance with expected time-domain behavior and help in debugging deviations from the design intent. The main challenges in providing this support are (a) developing AMS assertion languages or AMS verification libraries, and (b) instrumenting existing commercial simulators to support assertion verification during simulation. In this article, we report two approaches: the first extends the Open Verification Library (OVL) to the AMS domain by integrating a new collection of AMS verification libraries; while the second extends SystemVerilog Assertions (SVA) by augmenting analog predicates into SVA. We demonstrate the use of AMS-OVL on the Cadence Virtuoso environment while emphasizing that our libraries can work in any environment that supports Verilog and Verilog-A. We also report the development of tool support for AMS-SVA using a combination of Cadence NCSIM and Synopsys VCS. We demonstrate the utility of both approaches on the verification of LP3918, an integrated power management unit (PMU) from National Semiconductors. We believe that in the absence of existing EDA (Electronic Design Automation) tools for AMS assertion verification, the proposed approaches of integrating our libraries and our tool sets with existing commercial simulators will be of considerable and immediate practical value. © 2009 ACM.",Assertion; Integrated mixed signal design; OVL; Simulation; SVA; Verification library,Computer aided design; Digital integrated circuits; Digital libraries; Digital radio; Energy management; Libraries; Systems analysis; Assertion; Integrated mixed signal design; OVL; Simulation; SVA; Alpha particle spectrometers
Cost minimization while satisfying hard/soft timing constraints for heterogeneous embedded systems,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849231067&doi=10.1145%2f1497561.1497568&partnerID=40&md5=1a26833d2d14aeb0b5b98893a635de56,"In high-level synthesis for real-time embedded systems using heterogeneous functional units (FUs), it is critical to select the best FU type for each task. However, some tasks may not have fixed execution times. This article models each varied execution time as a probabilistic random variable and solves heterogeneous assignment with probability (HAP) problem. The solution of the HAP problem assigns a proper FU type to each task such that the total cost is minimized while the timing constraint is satisfied with a guaranteed confidence probability. The solutions to the HAP problem are useful for both hard real-time and soft real-time systems. Optimal algorithms are proposed to find the optimal solutions for the HAP problem when the input is a tree or a simple path. Two other algorithms, one is optimal and the other is near-optimal heuristic, are proposed to solve the general problem. The experiments show that our algorithms can effectively reduce the total cost while satisfying timing constraints with guaranteed confidence probabilities. For example, our algorithms achieve an average reduction of 33.0% on total cost with 0.90 confidence probability satisfying timing constraints compared with the previous work using worst-case scenario. © 2009 ACM.",Embedded Systems; Heterogeneous; High-level synthesis; Real-time,Costs; Embedded systems; Hazardous materials; Heuristic algorithms; Optimization; Random variables; Time measurement; Timing circuits; Confidence probability; Cost minimization; Execution time; Hard real-time; Heterogeneous; Heterogeneous embedded system; Heterogeneous functional unit; High-level synthesis; Optimal algorithm; Optimal solutions; Other algorithms; Real-time; Real-time embedded systems; Soft real-time systems; Timing constraints; Total costs; Worst case scenario; Real time systems
A cosimulation methodology for HW/SW validation and performance estimation,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849438669&doi=10.1145%2f1497561.1497566&partnerID=40&md5=d572bca190cb6eeec72b40b43e174901,"Cosimulation strategies allow us to simulate and verify HW/SW embedded systems before the real platform is available. In this field, there is a large variety of approaches that rely on different communication mechanisms to implement an efficient interface between the SW and the HW simulators. However, the literature lacks a comprehensive methodology which addresses the need for integrating and synchronizing heterogeneous simulators, like, for example, the SystemC simulation kernel for HW modules and an instruction set simulator for SW applications, without being intrusive for the HW and SW descriptions involved in the simulation. In this context, this article presents, compares, and integrates in a system-level framework two different co-simulation strategies for modeling, analyzing, and validating the performance of a HW/SW embedded system. Moreover, for both of them, a mechanism is proposed to provide an accurate time synchronization of the HW/SW communication. The first strategy is intended to provide an early cosimulation environment where HW/SW interaction can be validated without involving the operating system. The communication is implemented between a single SW task and a SystemC description of an HW module by exploiting the features of the remote debugging interface of a debugger (the GNU GDB), and by modifying the SystemC simulation kernel. On the other hand, the second strategy is intended to be used in further development steps, when the operating system is introduced to validate the cosimulation between HW modules and multitasking SW applications. In this approach, the communication is implemented via interrupts by using the features offered by the operating system. Experimental results are reported on two different case studies to analyze and compare the effectiveness of both the approaches. © 2009 ACM.",Embedded Systems; HW/SW co-simulation; HW/SW validation,Communication; Computer operating systems; Simulators; Strategic planning; Communication mechanisms; Cosimulation; Debugger; Further development; HW/SW co-simulation; HW/SW validation; Instruction set simulators; Operating systems; Performance estimation; Remote debugging; Simulation Kernel; System levels; SystemC; Time synchronization; Embedded systems
FPGA-based hardware acceleration for Boolean satisfiability,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849160717&doi=10.1145%2f1497561.1497576&partnerID=40&md5=ef39b03b27f5766367fc147e3f7ad7b4,"We present an FPGA-based hardware solution to the Boolean satisfiability (SAT) problem, with the main goals of scalability and speedup. In our approach the traversal of the implication graph as well as conflict clause generation are performed in hardware, in parallel. The experimental results and their analysis, along with the performance models are discussed. We show that an order of magnitude improvement in runtime can be obtained over MiniSAT (the best-in-class software based approach) by using a Virtex-4 (XC4VFX140) FPGA device. The resulting system can handle instances with as many as 10K variables and 280K clauses. © 2009 ACM.",Boolean constant propagation (BCP); Boolean satisfiabilty (SAT); Conflict induced clauses; FPGA; Non-chronological backtrack,Boolean functions; Combinatorial circuits; Hardware; Parallel architectures; Boolean constant propagation (BCP); Boolean satisfiabilty (SAT); Conflict induced clauses; FPGA; Non-chronological backtrack; Field programmable gate arrays (FPGA)
Efficient partial scan cell gating for low-power scan-based testing,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849159162&doi=10.1145%2f1497561.1497571&partnerID=40&md5=e1f833892d7fb40473678c9cd7636823,"Gating of the outputs of a portion of the scan cells (partial gating) has been recently proposed as a method for reducing the dynamic power dissipation during scan-based testing. We present a new systematic method for selecting, under area and performance design constraints, the most suitable for gating subset of scan cells as well as the proper gating value for each one of them, aiming at the reduction of the average switching activity during testing. We show that the proposed method outperforms the corresponding already known methods, with respect to average dynamic power dissipation reduction. © 2009 ACM.",Low-power testing; Partial gating; Scan cell gating; Scan-based testing,Electric power utilization; Scanning; Dynamic power dissipation; Low Power; Low-power testing; Partial gating; Partial scan; Performance design; Scan cell gating; Scan cells; Scan-based testing; Switching activities; Systematic method; Data compression
Provably correct on-chip communication: A formal approach to automatic protocol converter synthesis,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849488781&doi=10.1145%2f1497561.1497562&partnerID=40&md5=d364b4808609fbde3d9543121b3cfc25,"Hardware module reuse is a standard solution to the problems of increasing complexity of chip architectures and pressure to reduce time to market. In the absence of a single module interface standard, predesigned modules for ""plug-and-play"" usually require a converter between incompatible interface protocols. Current approaches to automatic synthesis of protocol converters mostly lack formal foundations and either employ abstractions far removed from the HDL implementation level or grossly simplify the structure of the protocols considered. This work presents a state-machine-based formalism for modeling bus-based communication protocols and a notion of protocol compatibility and of correct conversion between incompatible protocols. This formalism is used to derive algorithms for checking protocol compatibility and for provably correct, automatic converter synthesis. Experiments with automatic converter synthesis between different configurations of widely used commercial bus protocols, such as AMBA AHB, ASB APB, and the Open Core Protocol (OCP) are discussed. The work here is unique in its combination of a completely formal approach and the use of a low abstraction level that enables precise modeling of protocol characteristics that is also close to HDL. © 2009 ACM.",Automatic design; Converter synthesis; Protocol compatibility; System-on-chip,Abstracting; Application specific integrated circuits; Communication; Concurrent engineering; Microprocessor chips; Programmable logic controllers; Abstraction level; Automatic design; Automatic synthesis; Bus protocol; Bus-based communication; Chip architecture; Converter synthesis; Formal approach; Formal foundation; Hardware modules; Interface protocol; On chip communication; Open core; Plug-and-play; Precise modeling; Protocol compatibility; Protocol converters; Single modules; Standard solutions; State-machine; System-on-chip; Time to market; Electric inverters
External memory layout vs. schematic,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849355547&doi=10.1145%2f1497561.1497573&partnerID=40&md5=60a75a7e2a2837dd87f9b9474d01dd78,"The circuit represented by a VLSI layout must be verified by checking it against the schematic circuit as an important part of the functional verification step. This involves two central problems of matching the circuit graphs with each other (graph isomorphism) and extracting a higher level of circuit from a given level by finding subcircuits in the circuit graph (subgraph isomorphism). Modern day VLSI layouts contain millions of devices. Hence the memory requirements of the data structures required by tools for verifying them become huge and can easily exceed the amount of internal memory available on a computer. In such a scenario, a program not aware of the memory hierarchy performs badly because of its unorganized input/output operations (I/Os) as the speed of a disk access is about a million times slower than accessing a main memory location. In this article, we present I/O-efficient algorithms for the graph isomorphism and subgraph isomorphism problems in the context of verification of VLSI layouts. Experimental results show the need and utility of I/O-efficient algorithms for handling problems with large memory requirements. © 2009 ACM.",Design automation; External memory algorithms; Graph; Subgraph isomorphism; Verification of layouts,Algorithms; Computer aided design; Data structures; Central problems; Circuit graphs; Design automation; Efficient algorithm; External memory; External memory algorithms; Functional verification; Graph; Graph isomorphism; Input/output operations; Internal memory; Main memory; Memory hierarchy; Memory requirements; Sub-circuits; Subgraph isomorphism; Subgraph isomorphism problem; VLSI layout; Set theory
Reducing fault dictionary size for million-gate large circuits,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849368967&doi=10.1145%2f1497561.1497570&partnerID=40&md5=743192664fd44050c3f3ce7c52bd718d,"In general, fault dictionary is prevented from practical applications in fault diagnosis due to its extremely large size. Several previous works are proposed for the fault dictionary size reduction. However, some of them fail to bring down the size to an acceptable level, and others might not be able to handle today's million-gate circuits due to their high time and space complexity. In this article, an algorithm is presented to reduce the size of pass-fail dictionary while still preserving high diagnostic resolution. The proposed algorithm possesses low time and space complexity by avoiding constructing the huge distinguishability table, which inevitably boosts up the required computation complexity. Experimental results demonstrate that the proposed algorithm is capable of handling industrial million-gate large circuits in a reasonable amount of runtime and memory. © 2009 ACM.",Diagnostic resolution; Fault diagnosis; Fault dictionary,Computation complexity; Diagnostic resolution; Distinguishability; Fault diagnosis; Fault dictionary; Gate circuit; Large circuits; Large sizes; Runtime; Size reductions; Time and space; Algorithms
System-level PVT variation-aware power exploration of on-chip communication architectures,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849368968&doi=10.1145%2f1497561.1497563&partnerID=40&md5=d95a33f5956a614ea53a9d6c8dd21545,"With the shift towards deep submicron (DSM) technologies, the increase in leakage power and the adoption of power-aware design methodologies have resulted in potentially significant variations in power consumption under different process, voltage, and temperature (PVT) corners. In this article, we first investigate the impact of PVT corners on power consumption at the system-on-chip (SoC) level, especially for the on-chip communication infrastructure. Given a target technology library, we then show how it is possible to ""scale up"" and abstract the PVT variability at the system level, allowing characterization of the PVT-aware design space early in the design flow. We conducted several experiments to estimate power for PVT corner cases, at the gate level, as well as at the higher system level. Our preliminary results are very interesting, and indicate that (i) there are significant variations in power consumption across PVT corners; and (ii) the PVT-aware power estimation problem may be amenable to a reasonably simple abstraction at the system level. © 2009 ACM.",Digital systems; High-level synthesis; On-chip communication architectures; Performance exploration; Power estimation; PVT variation,Abstracting; Application specific integrated circuits; Communication; Electric power utilization; Estimation; Programmable logic controllers; Digital systems; High-level synthesis; On-chip communication architectures; Performance exploration; Power estimation; PVT variation; Microprocessor chips
Trade-offs in loop transformations,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849148709&doi=10.1145%2f1497561.1497565&partnerID=40&md5=6df3b7d39df31b6583be5634f33c5c9c,"Nowadays, multimedia systems deal with huge amounts of memory accesses and large memory footprints. To alleviate the impact of these accesses and reduce the memory footprint, high-level memory exploration and optimization techniques have been proposed. These techniques try to more efficiently utilize the memory hierarchy. An important step in these optimization techniques are loop transformations (LT). They have a crucial effect on later data memory footprint optimization steps and code generation. However, the state-of-the-art work has focused only on individual objectives. The main one in literature involves improving the locality of data accesses, and thus reducing the data memory footprint. It does not consider the trade-offs in the LT step in relation to successive optimization steps. Therefore, it is not globally efficient in mapping the application on the target platform. In this article we will discuss several trade-offs during the loop transformations. To our knowledge, we are the first ones considering these global trade-offs. Previous work always gave mostly one solution, having the best locality and thus the optimized memory footprint, even though some research in two-dimensional trade-offs in this area exists as well. We start from this state-of-the-art solution with minimal footprint. We show that by sacrificing the footprint, we can obtain gains in data reuse (crucial for energy reduction) and reduce the control-flow complexity. We demonstrate our approach on a real-life application, namely the QSDPCM video coder. At the end, we show that considering trade-offs for this application leads to 16% energy reduction in a two-layer memory subsystem and 10% cycle reduction on the ARM platform. © 2009 ACM.",Cost components; Data transfer and storage exploration; Loop transformations; Optimization; Trade-offs,Commerce; Data transfer; Multimedia systems; Optimization; Art work; Code Generation; Control-flow; Cost components; Cycle reduction; Data access; Data memory; Data reuse; Energy reduction; Global trade; Loop transformation; Loop transformations; Memory access; Memory footprint; Memory hierarchy; Memory subsystems; Optimization techniques; Real-life applications; Trade-offs; Two layers; Video coders; Data reduction
Battery voltage modeling for portable systems,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849209232&doi=10.1145%2f1497561.1497572&partnerID=40&md5=2db5e39c6cd797c461f2781e770bee04,"Limited battery life imposes stringent constraints on the operation of battery-powered portable systems. During battery discharge, the battery voltage decreases, until a certain cutoff value is reached, marking the end of battery life. The amount of discharge capacity and energy delivered by the battery during its life depends not only on the battery characteristics, but also on the load conditions. A different system design may result in a different battery current (load) profile over time, leading to a different battery voltage profile over time. This article presents an analytical model that relates the battery voltage to the battery current, thus facilitating system design optimizations with respect to the battery performance. It captures well-known nonlinear phenomena of capacity loss at high discharge rates, charge recovery, and capacity fading. The proposed model has been validated against measurements taken on Li-ion batteries. We also describe techniques for efficient calculations of model's estimates, which lets a user exploit accuracy-complexity tradeoffs. © 2009 ACM.",Accuracy-complexity tradeoff; Analytical modeling; Battery performance; Battery-powered systems; Low-power design,Communication channels (information theory); Lithium batteries; Portable equipment; Systems analysis; Accuracy-complexity tradeoff; Analytical modeling; Battery performance; Battery-powered systems; Low-power design; Design
System-scenario-based design of dynamic embedded systems,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349092191&doi=10.1145%2f1455229.1455232&partnerID=40&md5=03ca1167ded59637aae3f8f6aa6d5e01,"In the past decade, real-time embedded systems have become much more complex due to the introduction of a lot of new functionality in one application, and due to running multiple applications concurrently. This increases the dynamic nature of today's applications and systems, and tightens the requirements for their constraints in terms of deadlines and energy consumption. State-of-the-art design methodologies try to cope with these novel issues by identifying several most used cases and dealing with them separately, reducing the newly introduced complexity. This article presents a generic and systematic design-time/run-time methodology for handling the dynamic nature of modern embedded systems, which can be utilized by existing design methodologies to increase their efficiency. It is based on the concept of system scenarios, which group system behaviors that are similar from a multidimensional cost perspectivesuch as resource requirements, delay, and energy consumptionin such a way that the system can be configured to exploit this cost similarity. At design-time, these scenarios are individually optimized. Mechanisms for predicting the current scenario at run-time, and for switching between scenarios, are also derived. This design trajectory is augmented with a run-time calibration mechanism, which allows the system to learn on-the-fly during its execution, and to adapt itself to the current input stimuli, by extending the scenario set, changing the scenario definitions, and both the prediction and switching mechanisms. To show the generality of our methodology, we show how it has been applied on four very different real-life design problems. In all presented case studies, substantial energy reductions were obtained by exploiting scenarios. © 2009 ACM.",Design methodology; Dynamic nature; Embedded systems; Energy reduction; Real-time systems; System scenarios,Applications; Design; Integrated circuits; Machine design; Real time systems; Art designs; Current inputs; Design methodology; Design problems; Dynamic embedded systems; Dynamic nature; Energy consumption; Energy reduction; Modern embedded systems; Multiple applications; On the flies; Real-time embedded systems; Resource requirements; Run-time; Scenario-based designs; Substantial energies; Switching mechanisms; System behaviors; System scenarios; Systematic designs; Embedded systems
Lens aberration aware placement for timing yield,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349130059&doi=10.1145%2f1455229.1455245&partnerID=40&md5=a2b8eda7af6007ee5cd95cc6084f0b23,"Process variations due to lens aberrations are to a large extent systematic, and can be modeled for purposes of analyses and optimizations in the design phase. Traditionally, variations induced by lens aberrations have been considered random due to their small extent. However, as process margins reduce, and as improvements in reticle enhancement techniques control variations due to other sources with increased efficacy, lens aberration-induced variations gain importance. For example, our experiments indicate that delays of most cells in the Artisan TSMC 90nm library are affected by 2 - 8% due to lens aberration. Aberration-induced variations are systematic and depend on the location in the lens field. In this article, we first propose an aberration-aware timing analysis flow that accounts for aberration-induced cell delay variations. We then propose an aberration-aware timing-driven analytical placement approach that utilizes the predictable slow and fast regions created on the chip due to aberration to improve cycle time. We study the dependence of our improvement on chip size, as well as use of the technique along with field blading which allows partial reticle exposure. We evaluate our technique on two testcases, AES and JPEG implemented in 90nm technology. The proposed technique reduces cycle time by 4.322% (80ps) at the cost of 1.587% increase in trial-routed wirelength for AES. On JPEG, we observe a cycle time reduction of 5.182% (132ps) at the cost of 1.095% increase in trial-routed wirelength. © 2009 ACM.",Design for manufacturing; Layout; Lithography; Timing yield,Camera lenses; Lenses; Optical instruments; Time measurement; 90 nm technologies; Analytical placements; Cell delay variations; Cycle time; Cycle time reductions; Design for manufacturing; Design phase; Layout; Lens aberrations; Lithography; On chips; Process margins; Process variations; Reticle enhancement techniques; Test-cases; Timing Analysis; Timing yield; Timing-driven; Wire lengths; Aberrations
A 252Kgates/4.9Kbytes SRAM/71mW multistandard video decoder for high definition video applications,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349109765&doi=10.1145%2f1455229.1455246&partnerID=40&md5=98cd92b830bb69f5f3b58e5978c69f4d,"This article proposes a low-cost, low-power multistandard video decoder for high definition (HD) video applications. The proposed design supports multiple-standard (JPEG baseline, MPEG-1/2/4 Simple Profile (SP), and H.264 Baseline Profile (BP)) video decoding through interactive parsing control and common parameter bus interface. In order to reduce hardware cost, the shared adder-based structure and reusable data management are proposed to achieve hardware sharing and reduce internal memory size, respectively. In addition, the proposed design is optimized through reducing memory bandwidth by increasing both data reuse amount and burst length of memory access as well as eliminating cycle overhead in data access for supporting HD video decoding with single AHB-based SDR memory. The proposed 252Kgates/4.9kB/71mW/0.13μm multi-standard video decoder reduces 72% in gate count and 87% in power consumption as compared to the state-of-the-art design, when operating at 120MHz for real-time HD1080 video decoding with single AHB-based SDR memory. © 2009 ACM.",H.264; MPEG; Video decoder,Information retrieval; Motion Picture Experts Group standards; Art designs; Burst lengths; Bus interfaces; Data access; Data managements; Data re use; Design supports; Gate counts; H.264; Hardware costs; Hardware sharing; High definition videos; High definitions; Internal memories; Jpeg baselines; Low-power; Memory access; Memory bandwidths; MPEG; MPEG-1/2/4; Multi standards; Power consumption; Video applications; Video decoder; Video decoding; Decoding
Opposite-phase register switching for peak current minimization,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349129525&doi=10.1145%2f1455229.1455243&partnerID=40&md5=02b6d5aca4228c89079443fc6399b5b8,"In a synchronous sequential circuit, huge current peaks are often observed at the moment of clock transition (since all registers are clocked). Previous works focus on reducing the number of switching registers. However, even though the switching registers are the same, different combinations of switching directions still result in different peak currents. Based on that observation, in this article, we propose an ECO (engineering change order) approach to minimize the peak current by considering the switching directions of registers. Our approach is well suitable for reducing the peak current in IC testing. Experimental data consistently show that our approach works well in practice. © 2009 ACM.",IC testing; Logic synthesis; Peak current; Sequential circuit synthesis,Sequential circuits; Switching; Switching circuits; Clock transitions; Current peaks; Engineering Change Orders; Experimental datum; IC testing; Logic synthesis; Peak current; Phase registers; Sequential circuit synthesis; Synchronous sequential circuits; Logic circuits
"Straightforward construction of depth-size optimal, parallel prefix circuits with fan-out 2",2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349126777&doi=10.1145%2f1455229.1455244&partnerID=40&md5=d06646d3b24834fe1c940736ecebffeb,"Prefix computation is used in various areas and is considered as a primitive operation. Parallel prefix circuits are parallel prefix algorithms on the combinational circuit model. The depth of a prefix circuit is a measure of its processing time; smaller depth implies faster computation. The size of a prefix circuit is the number of operation nodes in it. Smaller size implies less power consumption, less VLSI area, and less cost. A prefix circuit with n inputs is depth-size optimal if its depth plus size equals 2n - 2. A circuit with a smaller fan-out is in general faster and occupies less VLSI area. To be of practical use, the depth and fan-out of a prefix circuit should be small. In this paper, a family of depth-size optimal, parallel prefix circuits with fan-out 2 is presented. This family of prefix circuits is easier to construct and more amenable to automatic synthesis than two other families of the same type, although the three families have the same minimum depth among all depth-size optimal prefix circuits with fan-out 2. The balanced structure of the new family is also a merit. © 2009 ACM.",Depth-size optimal; Fan-out; Parallel prefix circuits,Optimization; Parallel algorithms; Terminology; Automatic synthesis; Balanced structures; Combinational circuits; Depth-size optimal; Fan-out; Parallel prefix circuits; Power consumption; Practical use; Prefix circuits; Prefix computations; Primitive operations; Processing Time; Parallel processing systems
Energy and switch area optimizations for FPGA global routing architectures,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349129247&doi=10.1145%2f1455229.1455242&partnerID=40&md5=b3a5dc3fc02cdbdee8f7cdeb7470d0dd,"Low energy and small switch area usage are two important design objectives in FPGA global routing architecture design. This article presents an improved MCF model based CAD flow that performs aggressive optimizations, such as topology and wire style optimization, to reduce the energy and switch area of FPGA global routing architectures. The experiments show that when compared to traditional mesh architecture, the optimized FPGA routing architectures achieve up to 10% to 15% energy savings and up to 20% switch area savings in average for a set of seven benchmark circuits. © 2009 ACM.",FPGA; Global routing; Low power,Computer aided logic design; Energy conservation; Integrated circuit design; Topology; Area optimization; Benchmark circuit; Design objectives; Fpga routing architectures; Global routing; Low Power; Mesh architecture; Model-based OPC; Field programmable gate arrays (FPGA)
A design automation and power estimation flow for RFID systems,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349129246&doi=10.1145%2f1455229.1455236&partnerID=40&md5=bbc68cb91b357bf5f515acb6870fcad4,"While RFID has become a ubiquitous technology, there is still a need for RFID systems with different capabilities, protocols, and features depending on the application. This article describes a design automation flow and power estimation technique for fast implementation and design feedback of new RFID systems. Physical layer features are described using waveform features, which are used to automatically generate physical layer encoding and decoding hardware blocks. RFID primitives to be supported by the tag are enumerated with RFID macros and the behavior of each primitive is specified using ANSI-C within the template to automatically generate the tag controller. Case studies implementing widely used standards such as ISO 18000 Part 7 and ISO 18000 Part 6C using this automation technique are presented. The power macromodeling flow demonstrated here is shown to be within 5% to 10% accuracy, while providing results 100 times faster than traditional methods. When eliminating the need for certain features of ISO 18000 Part 6C, the design flow shows that the power required by the implementation is reduced by nearly 50%. © 2009 ACM.",Design automation; Low-power; Prototyping; RFID,Automation; Computer aided design; Decoding; Tropical engineering; Automation techniques; Design automation; Design flows; Encoding and decoding; Fast implementations; Hardware blocks; Low-power; Macromodeling; Physical layers; Power estimations; Prototyping; RFID; RFID systems; Ubiquitous technologies; Wave forms; Design
Model checking sequential software programs via mixed symbolic analysis,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349086094&doi=10.1145%2f1455229.1455239&partnerID=40&md5=207d994ba36b35a8ba8b2aa82eb8a46a,"We present an efficient symbolic search algorithm for software model checking. Our algorithms perform word-level reasoning by using a combination of decision procedures in Boolean and integer and real domains, and use novel symbolic search strategies optimized specifically for sequential programs to improve scalability. Experiments on real-world C programs show that the new symbolic search algorithms can achieve several orders-of-magnitude improvements over existing methods based on bit-level (Boolean) reasoning. © 2009 ACM.",Binary decision diagram; Composite symbolic formula; Image computation; Model checking; Presburger arithmetic; Reachability analysis,Binary decision diagrams; Data structures; Learning algorithms; Mathematical models; Binary decision diagram; Composite symbolic formula; Image computation; Presburger arithmetic; Reachability analysis; Model checking
A gateway node with duty-cycled radio and processing subsystems for wireless sensor networks,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349104189&doi=10.1145%2f1455229.1455234&partnerID=40&md5=7217aac5e4d267d777681edc46f4f8f0,"Wireless sensor nodes are increasingly being tasked with computation and communication intensive functions while still subject to constraints related to energy availability. On these embedded platforms, once all low power design techniques have been explored, duty-cycling the various subsystems remains the primary option to meet the energy and power constraints. This requires the ability to provide spurts of high MIPS and high bandwidth connections. However, due to the large overheads associated with duty-cycling the computation and communication subsystems, existing high performance sensor platforms are not efficient in supporting such an option. In this article, we present the design and optimizations taken in a wireless gateway node (WGN) that bridges data from wireless sensor networks to Wi-Fi networks in an on-demand basis. We discuss our strategies to reduce duty-cycling related costs by partitioning the system and by reducing the amount of time required to activate or deactivate the high-powered components. We compare the design choices and performance parameters with those made in the Intel Stargate platform to show the effectiveness of duty-cycling on our platform. We have built a working prototype, and the experimental results with two different power management schemes show significant reductions in latency and average power consumption compared to the Stargate. The WGN running our power-gating scheme performs about six times better in terms of average system power consumption than the Stargate running the suspend-system scheme for large working-periods where the active power dominates. For short working-periods where the transition (enable/disable) power becomes dominant, we perform up to seven times better. The comparative performance of our system is even greater when the sleep power dominates. © 2009 ACM.",Embedded systems; Gateway; Power savings; Sensor nodes,Computer networks; Electric network synthesis; Electric power supplies to apparatus; Electric power utilization; Embedded systems; Energy management; Gateways (computer networks); Integrated circuits; Metal drawing; Plasma waves; Sensor networks; Sensor nodes; Sensors; Telecommunication equipment; Telecommunication systems; Wireless telecommunication systems; Active power; Average power; Average systems; Communication subsystems; Design and optimizations; Duty-cycling; Embedded platforms; Energy availabilities; Gateway; Gateway nodes; High bandwidths; High-performance sensors; Low power design techniques; On demands; Performance parameters; Power constraints; Power consumption; Power management schemes; Power savings; Power-gating; System schemes; Wi-fi networks; Wireless gateways; Wireless sensor nodes; Wireless sensors; Wireless sensor networks
SystemCoDesigneran automatic ESL synthesis approach by design space exploration and behavioral synthesis for streaming applications,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349125677&doi=10.1145%2f1455229.1455230&partnerID=40&md5=64b18e0c300a320d9cf438b55157f928,"With increasing design complexity, the gap from ESL (Electronic System Level) design to RTL synthesis becomes more and more crucial to many industrial projects. Although several behavioral synthesis tools exist to automatically generate synthesizable RTL code from C/C ++based input descriptions and software generation for embedded processors is automated as well, an efficient ESL synthesis methodology combining both is still missing. This article presents SystemCoDesigner, a novel SystemC-based ESL tool to automatically optimize a hardware/software SoC (System on Chip) implementation with respect to several objectives. Starting from a SystemC behavioral model, SystemCoDesigner automatically extracts the mathematical model, performs a behavioral synthesis step, and explores the multiobjective design space using state-of-the-art multiobjective optimization algorithms. During design space exploration, a single design point is evaluated by simulating highly accurate performance models, which are automatically generated from the SystemC behavioral model and the behavioral synthesis results. Moreover, SystemCoDesigner permits the automatic generation of bit streams for FPGA targets from any previously optimized SoC implementation. Thus SystemCoDesigner is the first fully automated ESL synthesis tool providing a correct-by- construction generation of hardware/software SoC implementations. As a case study, a model of a Motion-JPEG decoder was automatically optimized and implemented using SystemCoDesigner. Several synthesized SoC variants based on this model show different tradeoffs between required hardware costs and achieved system throughput, ranging from software-only solutions to pure hardware implementations that reach real-time performance for QCIF streams on a 50MHz FPGA. © 2009 ACM.",Hardware/software codesign; System design,Computer hardware description languages; Design; Embedded software; Field programmable gate arrays (FPGA); Hardware; Microprocessor chips; Multiobjective optimization; Photolithography; Programmable logic controllers; Space research; Synthesis (chemical); System theory; Systems analysis; Accurate performance; Automatic generations; Automatically generated; Behavioral models; Behavioral synthesis; Bit streams; Correct by constructions; Design complexity; Design points; Design space explorations; Electronic system levels; Embedded processors; Hardware costs; Hardware implementations; Hardware/software; Hardware/software codesign; Industrial projects; Jpeg decoders; Multi objectives; Multi-objective designs; Optimization algorithms; Real-time performance; Rtl codes; Rtl synthesis; Soc implementations; Software generations; Still missing; Streaming applications; Synthesis tools; System design; System on chips; System throughputs; SystemC; Logic design
Design intent coverage revisited,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349101067&doi=10.1145%2f1455229.1455238&partnerID=40&md5=9ce7684e3e78ac74bb17e3cc0fe412c3,"Design intent coverage is a formal methodology for analyzing the gap between a formal architectural specification of a design and the formal functional specifications of the component RTL blocks of the design. In this article we extend the design intent coverage methodology to hybrid specifications containing both state-machines and formal properties. We demonstrate the benefits of this extension in two domains of considerable recent interest, namely (a) the use of auxiliary state-machines in formal specifications, and (b) the use of modest sized RTL blocks in the design intent coverage analysis. © 2009 ACM.",Design Intent Coverage,Design; Specifications; Architectural specifications; Coverage analysis; Design Intent Coverage; Design intents; Formal properties; Formal specifications; Functional specifications; State-machines; Two domains; Machine design
SOC test-architecture optimization for the testing of embedded cores and signal-integrity faults on core-external interconnects,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349113440&doi=10.1145%2f1455229.1455233&partnerID=40&md5=22d9b87e9e1912f7c7d5a5f63cd6e333,"The test time for core-external interconnect shorts and opens is typically much less than that for core-internal logic. Therefore, prior work on test-infrastructure design for core-based system-on-a-chip (SOC) has mainly focused on minimizing the test time for core-internal logic. However, as feature sizes shrink for newer process technologies, the test time for signal integrity (SI) faults on interconnects cannot be neglected. The test time for SI faults can be comparable to, or even larger than, the test time for the embedded cores. We investigate the impact of interconnect SI tests on SOC test-architecture design and optimization. A compaction method for SI faults and algorithms for test-architecture optimization are also presented. Experimental results for the ITC'02 benchmarks show that the proposed approach can significantly reduce the overall testing time for core-internal logic and core-external interconnects. © 2009 ACM.",Core-based system-on-chip; Interconnect testing; Test scheduling; Test-access mechanism (TAM),Application specific integrated circuits; Architectural design; Data compression; Integrated circuits; Microprocessor chips; Optimization; Programmable logic controllers; Scheduling; Signal processing; Architecture designs; Architecture optimizations; Core-based system-on-chip; Embedded cores; Feature sizes; Infrastructure designs; Interconnect testing; Process Technologies; Signal integrities; Soc tests; System on a chips; Test scheduling; Test time; Test-access mechanism (TAM); Testing time; Testing
Provably efficient algorithms for resolving temporal and spatial difference constraint violations,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349121466&doi=10.1145%2f1455229.1455237&partnerID=40&md5=58c037e805beadc2c81d7c49463526a8,"A system of difference constraints is a formal model of temporal and spatial constraints in many areas such as scheduling, constraint satisfaction, and layout compaction. During construction of such a system, constraint violations often arise, and they need to be resolved. Previous algorithms for this task fall into two groups: those algorithms that are fast but cannot resolve all violations, and those algorithms that can resolve all violations but are exponentially slow. We propose the first algorithms that are fast as well as able to resolve all violations. Moreover, unlike the previous algorithms, our algorithms support the ordering of violations using their inherent criticality or user-defined priority. We provably and experimentally justify the efficiency and efficacy of our algorithms. © 2009 ACM.",Behavioral synthesis; Constraint satisfaction; Interface timing; Layout compaction; Multimedia synchronization; Rate analysis; Real-time systems; Scheduling; Timing constraints,Algorithms; Compaction; Multimedia systems; Real time systems; Scheduling; Steelmaking; Time measurement; Timing circuits; Behavioral synthesis; Constraint satisfaction; Interface timing; Layout compaction; Multimedia synchronization; Rate analysis; Timing constraints; Scheduling algorithms
Congestion prediction in early stages of physical design,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349123085&doi=10.1145%2f1455229.1455241&partnerID=40&md5=cc36d331fd45dc99db5dd0691b5078c4,"Routability optimization has become a major concern in physical design of VLSI circuits. Due to the recent advances in VLSI technology, interconnect has become a dominant factor of the overall performance of a circuit. In order to optimize interconnect cost, we need a good congestion estimation method to predict routability in the early designing stages. Many congestion models have been proposed but there's still a lot of room for improvement. Besides, routers will perform rip-up and reroute operations to prevent overflow, but most models do not consider this case. The outcome is that the existing models will usually underestimate the routability. In this paper, we have a comprehensive study on our proposed congestion models. Results show that the estimation results of our approaches are always more accurate than the previous congestion models. © 2009 ACM.",Estimation; Floorplanning; Placement,Estimation; Comprehensive studies; Congestion estimations; Congestion models; Congestion predictions; Dominant factors; Estimation results; Floorplanning; Physical designs; Placement; Routability; VLSi technologies; VLSI circuits
An energy-efficient I/O request mechanism for multi-bank flash-memory storage systems,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349123083&doi=10.1145%2f1455229.1455235&partnerID=40&md5=21298cece7769b8b771c41481f4559cb,"Emerging critical issues for flash-memory storage systems, especially with regard to implementation within many embedded systems, are the programmed I/O nature of data transfers and their energy-efficient nature. We propose an I/O request mechanism in the Memory-Technology-Device (MTD) layer to exploit the programmed I/O-based data transfers for flash-memory storage systems. We propose to revise the waiting function in the Memory-Technology-Device (MTD) layer to relieve the microprocessor from busy-waiting, in order to make more CPU cycles available for other tasks. An energy-efficient mechanism based on the I/O request mechanism is also presented for multi-bank flash-memory storage systems, which particularly focuses on switching the power state of each flash-memory bank. We demonstrate that the energy-efficient I/O request mechanism not only saves more CPU cycles to execute other tasks, but also reduces the energy consumption of flash-memory, based on experiments incorporating realistic system workloads. © 2009 ACM.",Embedded systems; Energy-efficient; Flash Memory; Programmed I/O; Storage systems,Data transfer; Energy efficiency; Flash memory; Integrated circuits; Monolithic microwave integrated circuits; Cpu cycles; Critical issues; Energy consumption; Energy-efficient; Power state; Programmed I/O; Realistic systems; Storage systems; Embedded systems
Efficient error detection codes for multiple-bit upset correction in SRAMs with BICS,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349110286&doi=10.1145%2f1455229.1455247&partnerID=40&md5=f088a685a15772ee5d94680594234352,"Memories are one of the most widely used elements in electronic systems, and their reliability when exposed to Single Events Upsets (SEUs) has been studied extensively. As transistor sizes shrink, Multiple Bits Upsets (MBUs) are becoming an increasingly important factor in the reliability of memories exposed to radiation effects. To address this issue, Built-in Current Sensors (BICS) have recently been applied in conjunction with Single Error Correction/Double Error Detection (SEC-DED) codes to protect memories from MBUs. In this article, this approach is taken one step further, proposing specific codes optimized to be combined with BICS to provide protection against MBUs in memories. By exploiting the locality of errors within an MBU and the error detection and location capabilities of BICS, the proposed codes result in both a better protection level and a reduced cost compared with the existing SEC-DED approach. © 2009 ACM.",Error correcting codes; Fault tolerant memory; High-level protection technique; Protection against radiation,Codes (symbols); Data storage equipment; Error correction; Radiation effects; Radiation protection; Electronic systems; Error correcting codes; Error detection codes; Fault tolerant memory; High-level protection technique; Multiple-bit upsets; One steps; Protection against radiation; Protection levels; Reduced costs; Transistor sizes; Error detection
CoMPSoC: A template for composable and predictable multi-processor system on chips,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349104720&doi=10.1145%2f1455229.1455231&partnerID=40&md5=2b0018518831da3f85a2a57a46ed6aa9,"A growing number of applications, often with firm or soft real-time requirements, are integrated on the same System on Chip, in the form of either hardware or software intellectual property. The applications are started and stopped at run time, creating different use-cases. Resources, such as interconnects and memories, are shared between different applications, both within and between use-cases, to reduce silicon cost and power consumption. The functional and temporal behaviour of the applications is verified by simulation and formal methods. Traditionally, designers resort to monolithic verification of the system as whole, since the applications interfere in shared resources, and thus affect each other's behaviour. Due to interference between applications, the integration and verification complexity grows exponentially in the number of applications, and the task to verify correct behaviour of concurrent applications is on the system designer rather than the application designers. In this work, we propose a Composable and Predictable Multi-Processor System on Chip (CoMPSoC) platform template. This scalable hardware and software template removes all interference between applications through resource reservations. We demonstrate how this enables a divide-and-conquer design strategy, where all applications, potentially using different programming models and communication paradigms, are developed and verified independently of one another. Performance is analyzed per application, using state-of-the-art dataflow techniques or simulation, depending on the requirements of the application. These results still apply when the applications are integrated onto the platform, thus separating system-level design and application design. © 2009 ACM.",Composable; Model of computation; Network on chip; Predictable; System on chip,Computer software; Computer software selection and evaluation; Data compression; Design; Distributed computer systems; Formal methods; Microprocessor chips; Multiprocessing systems; Programming theory; Real time systems; Static random access storage; Composable; Model of computation; Network on chip; Predictable; System on chip; Applications
Interconnect customization for a hardware fabric,2009,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349114118&doi=10.1145%2f1455229.1455240&partnerID=40&md5=63ea04c02ccfd14a4335daa655bb331a,"This article describes several multiplexer-based interconnection strategies designed to improve energy consumption of stripe-based coarse-grain reconfigurable fabrics. Application requirements for the architecture as well as two dense subgraphs are extracted from a suite of signal and image processing benchmarks. These statistics are used to drive the strategy of the composition of multiplexer-based interconnect. The article compares interconnects that are fully connected between stripes, those with a cardinality of 8:1 to 4:1, and extensions that provide a 5:1 cardinality, limited 6:1 cardinality, and hybrids between 5:1 and 3:1 cardinalities. Additionally, dedicated vertical routes are considered replacing some computational units with dedicated pass-gates. Using a fabric interconnect model (FIM) written in XML, we demonstrate that fabric instances and mappers can be automatically generated using a Web-based design flow. Upon testing these instances, we found that using an 8:1 cardinality interconnect with 33% of the computational units replaced with dedicated pass-gates provided the best energy versus mappability tradeoff, resulting in a 50% energy improvement over fully connected rows and 20% energy improvement over an 8:1 cardinality interconnect without dedicated vertical routes. © 2009 ACM.",Architecture; Computer-aided design; Demonstrable; Hardware fabric; Low-energy; Reconfigurable,Benchmarking; Computer aided design; Computer architecture; Fabrics; Image processing; Markup languages; Multiplexing; Multiplexing equipment; Architecture; Demonstrable; Hardware fabric; Low-energy; Reconfigurable; Computer hardware
A high-level clustering algorithm targeting dual Vdd FPGAs,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849087198&doi=10.1145%2f1391962.1391965&partnerID=40&md5=5eb69d46880939740c203e6717bd417a,"Recent advanced power optimizations deployed in commercial FPGAs, laid out a roadmap towards FPGA devices that can be integrated into ultra low power systems. In this article, we present a high-level design tool to support the process of mapping an application onto a FPGA device with dual supply voltages. Our main contribution in this paper is an algorithm, which creates voltage scaling ready clusters by utilizing the timing slack available in the designs. We propose to first create clusters of CLBs within a given CLB-level netlist. This clustering algorithm intends to group chains of CLBs possessing similar amounts of timing slack along their critical path together. Once these clusters are identified, they are placed onto respective Vdd partitions on the device. We have evaluated different dual Vdd fabrics and the potential gain in power consumption is explored. When a subset of the logic blocks on the device can be driven by low Vdd levels (either with a dedicated low Vdd supply or with a programmable selection between low and high Vdd levels for these blocks) this affects placement and routing. As a result the maximum frequency of the designs may be affected. In order to evaluate the overall impact of creating voltage islands, we measured the Energy-Delay Product for our benchmark designs. We observed that the Energy-Delay product can be decreased by 26.9% when the placement of the designs into different voltage levels is guided by our clustering algorithm. © 2008 ACM.",Clustering; Dynamic power; Field programmable gate arrays; Partitioning; Placement; Voltage scaling,Boolean functions; Clustering algorithms; Electric power systems; Field programmable gate arrays (FPGA); Flow of solids; Process engineering; Time measurement; Clustering; Dynamic power; Field programmable gate arrays; Partitioning; Placement; Voltage scaling; Vanadium compounds
Timing-aware power-optimal ordering of signals,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849115781&doi=10.1145%2f1391962.1391973&partnerID=40&md5=8de270f5d49f73689541e8218ea9597e,"A computationally efficient technique for reducing interconnect active power in VLSI systems is presented. Power reduction is accomplished by simultaneous wire spacing and net ordering, such that cross-capacitances between wires are optimally shared. The existence of a unique power-optimal wire order within a bundle is proven, and a method to construct this order is derived. The optimal order of wires depends only on the activity factors of the underlying signals; hence, it can be performed prior to spacing optimization. By using this order of wires, optimality of the combined solution is guaranteed (as compared with any other ordering and spacing of the wires). Timing-aware power optimization is enabled by simultaneously considering timing criticality weights and activity factors for the signals. The proposed algorithm has been applied to various interconnect layouts, including wire bundles from high-end microprocessor circuits in 65 nm technology. Interconnect power reduction of 17% on average has been observed in such bundles. © 2008 ACM.",Interconnect optimization; Power optimization; Wire ordering; Wire spacing,Microprocessor chips; Optimization; Time measurement; Interconnect optimization; Power optimization; Wire ordering; Wire spacing; Wire
Efficiently scheduling runtime reconfigurations,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849097392&doi=10.1145%2f1391962.1391966&partnerID=40&md5=5d798f250c5f93877d0ff9b4ba0d2ed0,"Due to the emergence of portable devices that must run complex dynamic applications there is a need for flexible platforms for embedded systems. Runtime reconfigurable hardware can provide this flexibility but the reconfiguration latency can significantly decrease the performance. When dealing with task graphs, runtime support that schedules the reconfigurations in advance can drastically reduce this overhead. However, executing complex scheduling heuristics at runtime may generate an excessive penalty. Hence, we have developed a hybrid design-time/runtime reconfiguration scheduling heuristic that generates its final schedule at runtime but carries out most computations at design-time. We have tested our approach in a PowerPC 405 processor embedded on a FPGA demonstrating that it generates a very small runtime penalty while providing almost as good schedules as a full runtime approach. © 2008 ACM.",FPGAs; Hardware multitasking; Reconfigurable architectures; Runtime/design-time scheduling,Heuristic methods; Integrated circuits; Scheduling; FPGAs; Hardware multitasking; Reconfigurable architectures; Runtime/design-time scheduling; Embedded systems
Constraint-driven floorplan repair,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849137014&doi=10.1145%2f1391962.1391975&partnerID=40&md5=6b24d84563e2a4c076256c788e8eb394,"In this work, we propose a new and efficient approach to the floorplan repair problem, where violated design constraints are satisfied by applying small changes to an existing rough floorplan. Such a floorplan can be produced by a human designer, a scalable placement algorithm, or result from engineering adjustments to an existing floorplan. In such cases, overlapping modules must be separated, and others may need to be repositioned to satisfy additional requirements. Our algorithmic framework uses an expressive graph-based encoding of constraints which can reflect fixed-outline, region, proximity and alignment constraints. By tracking the implications of existing constraints, we resolve violations by imposing gradual modifications to the floorplan, in an attempt to preserve the characteristics of its initial design. Empirically, our approach is effective at removing overlaps and repairing violations that may occur when design constraints are acquired and imposed dynamically. © 2008 ACM.",Constraints; Floorplanning; Legalization,Design; Maintenance; Algorithmic framework; Constraints; Design constraints; Fixed-outline; Floorplanning; Gradual modifications; Graph-based; Initial design; Legalization; Placement algorithms; Repair
Effective decap insertion in area-array SoC floorplan design,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849091032&doi=10.1145%2f1391962.1391974&partnerID=40&md5=8a5fe89ef5cee9dd48eb410c5b9836e1,"As VLSI technology enters the nanometer era, supply voltages continue to drop due to the reduction of power dissipation, but it makes power integrity problems even worse. Employing decoupling capacitances (decaps) in floorplan stage is a common approach to alleviating supply noise problems. Previous researches overestimate the decap budget and do not fully utilize the empty space of the floorplan. A floorplan usually has a lot of available space that can be used to insert the decap without increasing the floorplan area. Therefore, the goal of this work is to develop a better model to calculate the required decap to solve the power supply noise problem in area-array based designs, and increase the usage of available space in the floorplan to reduce the area overhead caused by decap insertion. The experimental results of this work are encouraging. Compared with previous approaches, our methodology reduces 38% of the decap budget in average for MCNC benchmarks but can still meet the power supply noise requirements. The final floorplan areas with decap are also smaller than the numbers reported in previous works. © 2008 ACM.",Decap insertion; Floorplan; Power supply noise,Budget control; Electric power distribution; Electric power transmission networks; Noise pollution; Area overhead; De-coupling capacitance; Decap insertion; Empty space; Floor planning; Floorplan; Floorplan design; MCNC benchmarks; Nanometer era; Power dissipations; Power integrity; Power supply noise; Supply noise; Supply voltages; VLSI technologies; Electric power utilization
System-level throughput analysis for process variation aware multiple voltage-frequency island designs,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849083495&doi=10.1145%2f1391962.1391967&partnerID=40&md5=2a58fff69aaec22b334a1db5f66a26d3,"The increasing variability in manufacturing process parameters is expected to lead to significant performance degradation in deep submicron technologies. Multiple Voltage-Frequency Island (VFI) design styles with fine-grained, process-variation aware clocking have recently been shown to possess increased immunity to manufacturing process variations. In this article, we propose a theoretical framework that allows designers to quantify the performance improvement that is to be expected if they were to migrate from a fully synchronous design to the proposed multiple VFI design style. Specifically, we provide techniques to efficiently and accurately estimate the probability distribution of the execution rate (or throughput) of both single and multiple VFI systems under the influence of manufacturing process variations. Finally, using an MPEG-2 encoder benchmark, we demonstrate how the proposed analysis framework can be used by designers to make architectural decisions such as the granularity of VFI domain partitioning based on the throughput constraints their systems are required to satisfy. © 2008 ACM.",Globally asynchronous locally synchronous; Manufacturing process variations; Maximum cycle mean; Performance analysis; System-level design; Voltage-frequency islands,Design; Industrial engineering; Landforms; Motion Picture Experts Group standards; Probability distributions; Production engineering; Risk assessment; Throughput; Globally asynchronous locally synchronous; Manufacturing process variations; Maximum cycle mean; Performance analysis; System-level design; Voltage-frequency islands; Process engineering
ACM Transactions on Design Automation of Electronic Systems: Editorial,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849094499&doi=10.1145%2f1391962.1391963&partnerID=40&md5=38a81fce9dbd855aece0f5f5bc282ec4,[No abstract available],,
Schedulability analysis of preemptive and nonpreemptive EDF on partial runtime-reconfigurable FPGAs,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849144685&doi=10.1145%2f1391962.1391964&partnerID=40&md5=084559b32cb80ad7c156d46e7ea1cac1,"Field Programmable Gate Arrays (FPGAs) are very popular in today's embedded systems design, and Partial Runtime-Reconfigurable (PRTR) FPGAs allow HW tasks to be placed and removed dynamically at runtime. Hardware task scheduling on PRTR FPGAs brings many challenging issues to traditional real-time scheduling theory, which have not been adequately addressed by the research community compared to software task scheduling on CPUs. In this article, we consider the schedulability analysis problem of HW task scheduling on PRPR FPGAs. We derive utilization bounds for several variants of global preemptive/nonpreemptive EDF scheduling, and compare the performance of different utilization bound tests. © 2008 ACM.",FPGA; Real-time scheduling; Reconfigurable devices,Embedded software; Embedded systems; Field programmable gate arrays (FPGA); Multitasking; Program processors; Scheduling; Scheduling algorithms; Field programmable gate array (FPGAs); Hardware task scheduling; Real - time scheduling; Reconfigurable devices; Research communities; Run-time reconfigurable; Schedulability analysis; Utilization bounds; Reconfigurable hardware
Access pattern-based code compression for memory-constrained systems,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849134531&doi=10.1145%2f1391962.1391968&partnerID=40&md5=13c9d19ee8a0a99ad424323d951b5976,"As compared to a large spectrum of performance optimizations, relatively less effort has been dedicated to optimize other aspects of embedded applications such as memory space requirements, power, real-time predictability, and reliability. In particular, many modern embedded systems operate under tight memory space constraints. One way of addressing this constraint is to compress executable code and data as much as possible. While researchers on code compression have studied efficient hardware and software based code compression strategies, many of these techniques do not take application behavior into account; that is, the same compression/decompression strategy is used irrespective of the application being optimized. This article presents an application-sensitive code compression strategy based on control flow graph (CFG) representation of the embedded program. The idea is to start with a memory image wherein all basic blocks of the application are compressed, and decompress only the blocks that are predicted to be needed in the near future. When the current access to a basic block is over, our approach also decides the point at which the block could be compressed. We propose and evaluate several compression and decompression strategies that try to reduce memory requirements without excessively increasing the original instruction cycle counts. Some of our strategies make use of profile data, whereas others are fully automatic. Our experimental evaluation using seven applications from the MediaBench suite and three large embedded applications reveals that the proposed code compression strategy is very successful in practice. Our results also indicate that working at a basic block granularity, as opposed to a procedure granularity, is important for maximizing memory space savings. © 2008 ACM.",CFG; Code access pattern; Code compression; Embedded systems; Memory optimization,Applications; Codes (symbols); Concurrency control; Constrained optimization; Data compression; Integrated circuits; Optimization; Power spectrum; Real time systems; CFG; Code access pattern; Code compression; Memory optimization; Embedded systems
Layout-aware scan chain reorder for launch-off-shift transition test coverage,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849107840&doi=10.1145%2f1391962.1391972&partnerID=40&md5=d0b71a4d5191157c48a6a38946ef5d3f,"Launch-off-shift (LOS) is a popular delay test technique for scan-based designs. However, it is usually not possible to achieve good delay fault coverage in LOS test due to conflicts in test vectors. In this article, we propose a layout-based scan chain ordering method to improve fault coverage for LOS test with limited routing overhead. A fast and effective algorithm is used to eliminate conflicts in test vectors while at the same time restrict the extra scan chain routing. This approach provides many advantages. (1) The proposed method can improve delay fault coverage for LOS test. (2) With layout information taken into account, the routing penalty is limited, and thus the impact on circuit performance will not be significant. Experimental results show that the proposed LOS test method achieves about the same level of delay fault coverage as enhanced scan does, while the average scan chain wire length is about 2.2 times of the shortest scan chain. © 2008 ACM.",Scan chain ordering; Scan test; Test generation; Transition faults,Delay circuits; Flip flop circuits; Launching; Networks (circuits); Routing algorithms; Vectors; Scan chain ordering; Scan test; Test generation; Transition faults; Testing
Optimal routing algorithms for rectilinear pin clusters in high-density multichip modules,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849103001&doi=10.1145%2f1391962.1391976&partnerID=40&md5=1d457f5964ba385cb0998bb60f6cd7c7,"As the circuit densities and transistor counts are increasing, the package routing problem is becoming more and more challenging. In this article, we study an important routing problem encountered in typical high-end MCM designs: routing within dense pin clusters. Pin clusters are often formed by pins that belong to the same functional unit or the same data bus, and can become bottlenecks in terms of overall routability. Typically, these clusters have irregular shapes, which can be approximated with rectilinear convex boundaries. Since such boundaries have often irregular shapes, a traditional escape routing algorithm may give unroutable solutions. In this article, we study how the positions of escape terminals on a convex boundary affect the overall routability. For this purpose, we propose a set of necessary and sufficient conditions to model routability outside a rectilinear convex boundary. Given an escape routing solution, we propose an optimal algorithm to select the maximal subset of nets that are routable outside the boundary. After that, we focus on an integrated approach to consider routability constraints (outside the boundary) during the actual escape routing algorithm. Here, we propose an optimal algorithm to find the best escape routing solution that satisfies all routability constraints. Our experiments demonstrate that we can reduce the number of layers by 17% on the average, by using this integrated methodology. © 2008 ACM.",Escape routing; Multi-chip modules; Network flow,Algorithms; Boolean functions; Boundary conditions; Marine biology; Multichip modules; Solutions; Zeolites; Circuit densities; Data buses; Escape routing; Functional unit; High-density; Integrated approach; Integrated methodology; Necessary and sufficient conditions; Network flow; Optimal algorithms; Optimal routing; Package routing; Routability; Routing problems; Transistor counts; Routing algorithms
Simulation-based verification using Temporally Attributed Boolean Logic,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849089291&doi=10.1145%2f1391962.1391971&partnerID=40&md5=2df45b799aeaaa06d69adba7f2063f0c,"We propose a specification logic called Temporally Attributed Boolean (TAB) Logic for Assertion Based Verification, which allows us to: (i) represent assertions succinctly, (ii) incorporate data-orientation and (iii) associate timing to design intentions. TAB Logic allows us to write specifications functionally linking system variables from different temporal contexts. We present examples to show the motivation for this logic especially in the context of high level modeling of complex real time systems. We formally define TAB Logic, formulate the problem of verification on a simulation trace and present efficient algorithms to check TAB assertions, both offline and online. We present results of application of TAB Logic for Instruction Semantics and Bus Transaction Verification of a bus integrated pipelined processor core implementation. We also employ TAB Logic to validate the Interrupt mode behavior of the processor core implementation. Further, we show the utility of TAB Logic in fault detection. Finally, we demonstrate the applicability of TAB Logic in the domain of simulation based verification of analog circuits like Operational Amplifiers and DC-DC Converters. We finally discuss the limitations of TAB logic and conclude. © 2008 ACM.",Bus verification; Instruction semantics verification; Interrupt testing; Offline-online verification algorithm; Simulation based verification; Temporal logic; Timing verification,Analog circuits; Boolean functions; Fault detection; Formal logic; Information theory; Interactive computer systems; Operational amplifiers; Pipeline processing systems; Real time systems; Specifications; Bus verification; Instruction semantics verification; Interrupt testing; Offline-online verification algorithm; Simulation based verification; Timing verification; Temporal logic
Auxiliary state machines + context-triggered properties in verification,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849142936&doi=10.1145%2f1391962.1391970&partnerID=40&md5=62a6c049a584727ae931f604994aba2c,"Formal specifications of interface protocols between a design-under-test and its environment mostly consist of two types of correctness requirements, namely (a) a set of invariants that applies throughout the protocol execution and (b) a set of context-triggered properties that applies only when the protocol state belongs to a specific set of contexts. To model such requirements, an increasingly popular design choice in the assertion IP design community has been the use of abstract context state machines and state-oriented properties. In this paper, we formalize this modeling style and present algorithms for verifying such specifications. Specifically, we present a purely formal approach and a semi-formal approach for verifying such specifications. We demonstrate the use of this design style in modeling some of the industry standard protocol descriptions and present encouraging results. © 2008 ACM.",Verification,Contour followers; Design; Internet protocols; Machine design; Network protocols; Specifications; Formal approach; Formal specifications; Industry standards; Interface protocols; IP design; Protocol execution; State machines; Two types; Verification; Computer networks
A compiler approach to managing storage and memory bandwidth in configurable architectures,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849105813&doi=10.1145%2f1391962.1391969&partnerID=40&md5=f35f7cf3e12f21ace0b97f473bdba5d4,"Configurable architectures offer the unique opportunity of realizing hardware designs tailored to the specific data and computational patterns of an application code. Customizing the storage structures is becoming increasingly important in mitigating the continuing gap between memory latencies and internal computing speeds. In this article we describe and evaluate a compiler algorithm that maps the arrays of a loop-based computation to internal storage structures, either RAM blocks or discrete registers. Our objective is to minimize the overall execution time while considering the capacity and bandwidth constraints of the storage resources. The novelty of our approach lies in creating a single framework that combines high-level compiler techniques with lower-level scheduling information for mapping the data. We illustrate the benefits of our approach for a set of image/signal processing kernels using a Xilinx Virtex#8482; Field-Programmable Gate Array (FPGA). Our algorithm leads to faster designs compared to the state-of-the-art custom data layout mapping technique, in some instances using less storage. When compared to hand-coded designs, our results are comparable in terms of execution time and resources, but are derived in a minute fraction of the design time. © 2008 ACM.",Compiler analysis; Configurable architectures; High-level hardware synthesis; Storage allocation and management,Boolean functions; Conformal mapping; Design; Field programmable gate arrays (FPGA); Program compilers; Telecommunication systems; Wireless telecommunication systems; Compiler analysis; Configurable architectures; High-level hardware synthesis; Storage allocation and management; Data storage equipment
Partitioning parameterized 45-degree polygons with constraint programming,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849101694&doi=10.1145%2f1367045.1367061&partnerID=40&md5=e74558366409754d606d60e3c1a0a7a0,An algorithm for partitioning parameterized 45-degree polygons into parameterized trapezoids is proposed in this article. The algorithm is based on the plane-sweep technique and can handle polygons with complicated constraints. The input to the algorithm consists of the contour of a parameterized polygon to be partitioned and a set of constraints for parameters of the contour. The algorithm uses horizontal cuts only and generates a number of nonoverlapping trapezoids whose union is the original parameterized polygon. Processing of constraints and coordinates that contain first-order multiple-variable polynomials has been made possible by incorporating the JaCoP constraint programming library. The proposed algorithm has been implemented in Java programming language and can be used as the basis to build the trapezoidal corner stitching data structure for parameterized VLSI layout masks. © 2008 ACM.,Analog and mixed-signal design; Parameterized layouts; Parameterized polygons; Polygon decomposition; Trapezoidal corner stitching,Algorithms; BASIC (programming language); Computer programming; Computer software; Constraint theory; Data structures; File organization; Java programming language; Analog and mixed-signal design; Constraint programming; First orders; JA VA programming; Parameterized; Parameterized layouts; Parameterized polygons; Polygon decomposition; Sweep technique; Trapezoidal corner stitching; VLSI layout; Computer programming languages
Combining system scenarios and configurable memories to tolerate unpredictability,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849101854&doi=10.1145%2f1367045.1367058&partnerID=40&md5=b075949afbae61f28fe5ab29118ea454,"Process variability and the dynamism of new applications increase the uncertainty of embedded systems and force designers to use pessimistic assumptions, which have a tremendous impact on both the performance and energy consumption of their memory organizations. In this article we introduce an experimental framework which tries to mitigate the effects of both sources of unpredictability. At compile time, an extensive profiling helps us to detect system scenarios and bounds application dynamism. At the organization level, we incorporate a heterogeneous memory architecture composed by several configurable memories. A calibration process and a runtime control system adapt the platform to the current application needs. Our approach manages to reduce significantly the energy overhead associated to both variability and application dynamism (up to 60%, according to our simulations) without compromising the timing constraints existing in our target domain of dynamic periodic multimedia applications. © 2008 ACM.",Parametric yield; Process variation; Variability compensation,Embedded systems; Energy utilization; Calibration process; Heterogeneous memory; Memory organizations; Multimedia applications; Parametric yield; Process Variability; Process Variation; Variability compensation; Memory architecture
Evolution of synthetic RTL benchmark circuits with predefined testability,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849098494&doi=10.1145%2f1367045.1367063&partnerID=40&md5=6abc38d379143ce3fe5c5667decb8e25,"This article presents a new real-world application of evolutionary computing in the area of digital-circuits testing. A method is described which enables to evolve large synthetic RTL benchmark circuits with a predefined structure and testability. Using the proposed method, a new collection of synthetic benchmark circuits was developed. These benchmark circuits will be useful in a validation process of novel algorithms and tools in the area of digital-circuits testing. Evolved benchmark circuits currently represent the most complex benchmark circuits with a known level of testability. Furthermore, these circuits are the largest that have ever been designed by means of evolutionary algorithms. This work also investigates suitable parameters of the evolutionary algorithm for this problem and explores the limits in the complexity of evolved circuits. © 2008 ACM.",Benchmark circuit; Evolvable hardware; Testability analysis,Digital circuits; Evolutionary algorithms; Timing circuits; Benchmark circuit; Complex benchmark; Evolutionary computing; Evolvable hardware; Novel algorithm; Synthetic benchmark; Testability Analysis; Validation process; Electric network analysis
Specification-driven directed test generation for validation of pipelined processors,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849095038&doi=10.1145%2f1367045.1367051&partnerID=40&md5=6761e90d63854ba4cc4dae98c4a3acc4,"Functional validation is a major bottleneck in pipelined processor design due to the combined effects of increasing design complexity and lack of efficient techniques for directed test generation. Directed test vectors can reduce overall validation effort, since shorter tests can obtain the same coverage goal compared to the random tests. This article presents a specification-driven directed test generation methodology. The proposed methodology makes three important contributions. First, a general graph model is developed that can capture the structure and behavior (instruction set) of a wide variety of pipelined processors. The graph model is generated from the processor specification. Next, we propose a functional fault model that is used to define the functional coverage for pipelined architectures. Finally, we propose two complementary test generation techniques: test generation using model checking, and test generation using template-based procedures. These test generation techniques accept the graph model of the architecture as input and generate test programs to detect all the faults in the functional fault model. Our experimental results on two pipelined processor models demonstrate several orders-of-magnitude reduction in overall validation effort by drastically reducing both test-generation time and number of test programs required to achieve a coverage goal. © 2008 ACM.",Functional validation; Model checking; Test generation,Chlorine compounds; Graph theory; Microprocessor chips; Model checking; Specifications; Testing; Combined effects; Design complexities; Functional coverage; Functional fault model; Functional validation; Generation time; Graph modeling; Instruction sets; Orders-of-magnitude; Pipelined architectures; Pipelined processors; Random Testing; Test generation; Test generations; Test programs; Test vectors; Pipeline processing systems
A retargetable parallel-programming framework for MPSoC,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849107178&doi=10.1145%2f1367045.1367048&partnerID=40&md5=c25b772ea7dcdd0e637bda1a9c58c83f,"As more processing elements are integrated in a single chip, embedded software design becomes more challenging: It becomes a parallel programming for nontrivial heterogeneous multiprocessors with diverse communication architectures, and design constraints such as hardware cost, power, and timeliness. In the current practice of parallel programming with MPI or OpenMP, the programmer should manually optimize the parallel code for each target architecture and for the design constraints. Thus, the design-space exploration of MPSoC (multiprocessor systems-on-chip) costs become prohibitively large as software development overhead increases drastically. To solve this problem, we develop a parallel-programming framework based on a novel programming model called common intermediate code (CIC). In a CIC, functional parallelism and data parallelism of application tasks are specified independently of the target architecture and design constraints. Then, the CIC translator translates the CIC into the final parallel code, considering the target architecture and design constraints to make the CIC retargetable. Experiments with preliminary examples, including the H.263 decoder, show that the proposed parallel-programming framework increases the design productivity of MPSoC software significantly. © 2008 ACM.",Design-space exploration; Embedded software; Multiprocessor system on chip; Parallel-programming; Software generation,Codes (standards); Codes (symbols); Design of experiments; Fault tolerance; Microprocessor chips; Multiprocessing systems; Parallel programming; Quality assurance; Reliability; Software design; Space research; Targets; Communication architectures; Data parallelisms; Design constraints; Design productivity; Design-space exploration; Embedded software; Embedded software design; Hardware costs; Heterogeneous multiprocessors; Multiprocessor system on chip; Multiprocessor systems on chips; Parallel coding; Processing elements; Programming frameworks; Programming models; Retargetable; Single chips; Software development; Software generation; Target architectures; Architectural design
Reuse and optimization of testbenches and properties in a TLM-to-RTL design flow,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849113523&doi=10.1145%2f1367045.1367056&partnerID=40&md5=f53c6f7cc711bc5a787dc31c9e31e25c,"In transaction-level modeling (TLM), verification methodologies based on transactions allow testbenches, properties, and IP cores in mixed TL-RTL designs to be reused. However, no papers in the literature analyze the effectiveness of transaction-based verification (TBV) in comparison to the more traditional RTL approach. The first contribution of this article is the introduction of a functional-fault-model-based methodology for demonstrating the effectiveness of reuse through TBV. A second contribution is the introduction of a similar methodology for efficient property checking which identifies and removes redundant properties prior to assertion-based verification or model checking. © 2008 ACM.",Fault models; Functional verification; Model checking; TBV; TLM,Random access storage; Assertion-based verification; Fault model; Functional fault model; Functional verification; Property checking; RTL design flow; Transaction level modeling; Verification methodology; Model checking
Processor virtualization for secure mobile terminals,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849095219&doi=10.1145%2f1367045.1367057&partnerID=40&md5=e4dcabd49933738797a9231bcfea9f7d,"We propose a processor virtualization architecture, VIRTUS, to provide a dedicated domain for preinstalled applications and virtualized domains for downloaded native applications. With it, security-oriented next-generation mobile terminals can provide any number of domains for native applications. VIRTUS features three new technologies, namely, VMM asymmetrization, dynamic interdomain communication (IDC), and virtualization-assist logic, and it is first in the world to virtualize an ARM-based multiprocessor. Evaluations have shown that VMM asymmetrization results in significantly less performance degradation and LOC increase than do other VMMs. Further, dynamic IDC overhead is low enough, and virtualization-assist logic can be implemented in a sufficiently small area. © 2008 ACM.",Multiprocessor; Processor virtualization,Computer circuits; Computer terminals; Hardware security; Mobile telecommunication systems; Multiprocessing systems; Programmable logic controllers; Interdomain communication; Mobile terminal; Multiprocessor; Performance degradation; Processor virtualization; Secure Mobile Terminal; Small area; Virtualization
Designing secure systems on reconfigurable hardware,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849095859&doi=10.1145%2f1367045.1367053&partnerID=40&md5=fb8c8cb49db675b69a11bbcac2abbc28,"The extremely high cost of custom ASIC fabrication makes FPGAs an attractive alternative for deployment of custom hardware. Embedded systems based on reconfigurable hardware integrate many functions onto a single device. Since embedded designers often have no choice but to use soft IP cores obtained from third parties, the cores operate at different trust levels, resulting in mixed-trust designs. The goal of this project is to evaluate recently proposed security primitives for reconfigurable hardware by building a real embedded system with several cores on a single FPGA and implementing these primitives on the system. Overcoming the practical problems of integrating multiple cores together with security mechanisms will help us to develop realistic security-policy specifications that drive enforcement mechanisms on embedded systems. © 2008 ACM.",Advanced encryption standard (AES); Controlled sharing; Enforcement mechanisms; Execution monitors; Field programmable gate arrays (FPGAs); Hardware security; Isolation; Memory protection; Reference monitors; Security policies; Security primitives; Separation,Application specific integrated circuits; Computer programming languages; Field programmable gate arrays (FPGA); Integrated circuits; Mechanisms; Optical design; Project management; Specifications; Advanced encryption standard (AES); Controlled sharing; Custom hardwares; Enforcement mechanisms; Execution monitors; Field programmable gate arrays (FPGAs); Hardware security; Isolation; Memory protection; Practical problems; Reconfigurable hardware; Reference monitors; Secure systems; Security mechanisms; Security policies; Security primitives; Separation; Soft-IP; Third parties; Embedded systems
Power-aware SoC test planning for effective utilization of port-scalable testers,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849107875&doi=10.1145%2f1367045.1367062&partnerID=40&md5=c2f86a045c7cbf79b593e28a8b5c6a3b,"Many system-on-chip (SoC) integrated circuits contain embedded cores with different scan frequencies. To better meet the test requirements for such heterogeneous SoCs, leading tester companies have recently introduced port-scalable testers, which can simultaneously drive groups of channels at different data rates. However, the number of tester channels available for scan testing is limited; therefore, a higher shift frequency can increase the test time for a core if the resulting test access architecture reduces the bit-width used to access it. We present a scalable test planning technique that exploits port scalability of testers to reduce SoC test time. We compare the proposed heuristic optimization method to two baseline methods based on prior works that use a single scan data rate for all embedded cores. We also propose a power-aware test planning technique to effectively utilize port-scalable testers under constraints of test power consumption. Experimental results are presented for power-aware test scheduling to illustrate the impact of power constraints on overall test time. © 2008 ACM.",Integer linear programming; Port-scalable testers; SoC test; Test access architecture,Digital storage; Heuristic methods; Integer programming; Power management; Programmable logic controllers; Heuristic optimization method; Integer Linear Programming; Port-scalable testers; Power constraints; SOC tests; System on chips (SoC); Test access architecture; Test requirements; System-on-chip
Resource sharing among mutually exclusive sum-of-product blocks for area reduction,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849085484&doi=10.1145%2f1367045.1367060&partnerID=40&md5=b41406a7222a3f405efa1e3c60c5adeb,"In state-of-the-art digital designs, arithmetic blocks consume a major portion of the total area of the IC. The arithmetic sum-of-product (SOP) is the most widely used arithmetic block. Some of the examples of SOP are adder, subtractor, multiplier, multiply-accumulator (MAC), squarer, chain-of-adders, incrementor, decrementor, etc. In this article, we introduce a novel, area-efficient architecture to share different SOP blocks which are used in a mutually exclusive manner. We implement the core functions of the largest SOP only once and reuse different parts of the core subblocks for all other SOP operations with the help of multiplexers. This architecture can be used in the nontiming-critical paths of the design, to save significant amounts of area. Our experimental data shows that the proposed sharing-based architecture results in about 37% area savings compared to the results obtained from a commercially available best-in-class datapath synthesis tool. In addition, our proposed shared implementation consumes about 18% less power. These improvements were verified on placed-and-routed designs as well. © 2008 ACM.",,Adders; Architecture; Area reduction; Arithmetic blocks; Digital designs; Multiply accumulators; Resource sharing; State of the art; Sum of products; Synthesis tool; Classifiers
Implementing the scale vector-thread processor,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849117734&doi=10.1145%2f1367045.1367050&partnerID=40&md5=ffb2d1427a0196a025a7cfefc2d1c499,"The Scale vector-thread processor is a complexity-effective solution for embedded computing which flexibly supports both vector and highly multithreaded processing. The 7.1-million transistor chip has 16 decoupled execution clusters, vector load and store units, and a nonblocking 32KB cache. An automated and iterative design and verification flow enabled a performance-, power-, and area-efficient implementation with two person-years of development effort. Scale has a core area of 16.6 mm2 in 180 nm technology, and it consumes 400 mW - 1.1 W while running at 260 MHz. © 2008 ACM.",Hybrid C++/Verilog simulation; Iterative VLSI design flow; Multithreaded processors; Procedural datapath pre-placement; Vector processors; Vector-thread processors,Chlorine compounds; Java programming language; Core area; Effective solution; Efficient implementation; Embedded computing; Hybrid C++/Verilog simulation; Iterative designs; Iterative VLSI design flow; Multi-threaded; Multithreaded processors; Non-blocking; Procedural datapath pre-placement; Vector processors; Vector-thread processors; Vectors
An energy characterization platform for memory devices and energy-aware data compression for multilevel-cell flash memory,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849086567&doi=10.1145%2f1367045.1367052&partnerID=40&md5=5a0721c6edf7f84f5d5d4ca42c188040,"Memory devices often consume more energy than microprocessors in current portable embedded systems, but their energy consumption changes significantly with the type of transaction, data values, and access timing, as well as depending on the total number of transactions. These variabilities mean that an innovative tool and framework are required to characterize modern memory devices running in embedded system architectures. We introduce an energy measurement and characterization platform for memory devices, and demonstrate an application to multilevel-cell (MLC) flash memories, in which we discover significant value-dependent programming energy variations. We introduce an energy-aware data compression method that minimizes the flash programming energy, rather than the size of the compressed data, which is formulated as an entropy coding with unequal bit-pattern costs. Deploying a probabilistic approach, we derive energy-optimal bit-pattern probabilities and expected values of the bit-pattern costs which are applicable to the large amounts of compressed data typically found in multimedia applications. Then we develop an energy-optimal prefix coding that uses integer linear programming, and construct a prefix-code table. From a consideration of Pareto-optimal energy consumption, we can make tradeoffs between data size and programming energy, such as a 41% energy savings for a 52% area overhead. © 2008 ACM.",Compression; Flash memory; MLC,Data storage equipment; Embedded systems; Energy conservation; Energy policy; Flash memory; Integer programming; Integrated circuits; Linear programming; Linearization; Probability; Risk assessment; Terminology; Area overhead; Compression; Compression methods; Data size; Data values; Embedded system architectures; Energy characterization; Energy consumption; Energy measurements; Energy savings; Energy variations; Energy-aware; Entropy coding; Expected values; Flash programming energy; Integer Linear Programming; MLC; Multimedia applications; Pareto-optimal; Probabilistic approaches; Running-in; Data compression
ILP-based energy minimization techniques for banked memories,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849084545&doi=10.1145%2f1367045.1367059&partnerID=40&md5=afd913239befd02f2210e0931956537f,"Main memories can consume a significant portion of overall energy in many data-intensive embedded applications. One way of reducing this energy consumption is banking, that is, dividing available memory space into multiple banks and placing unused (idle) memory banks into low-power operating modes. Prior work investigated code-restructuring- and data-layout-reorganization-based approaches for increasing the energy benefits that could be obtained from a banked memory architecture. This article explores different techniques that can potentially coexist within the same optimization framework for maximizing benefits of low-power operating modes. These techniques include employing nonuniform bank sizes, data migration, data compression, and data replication. By using these techniques, we try to increase the chances for utilizing low-power operating modes in a more effective manner, and achieve further energy savings over what could be achieved by exploiting low-power modes alone. Specifically, nonuniform banking tries to match bank sizes with application-data access patterns. The goal of data migration is to cluster data with similar access patterns in the same set of banks. Data compression reduces the size of the data used by an application, and thus helps reduce the number of memory banks occupied by data. Finally, data replication increases bank idleness by duplicating select read-only data blocks across banks. We formulate each of these techniques as an ILP (integer linear programming) problem, and solve them using a commercial solver. Our experimental analysis using several benchmarks indicates that all the techniques presented in this framework are successful in reducing memory energy consumption. Based on our experience with these techniques, we recommend to compiler writers for banked memories to consider data compression, replication, and migration. © 2008 ACM.",Compilers; Data compression; DRAM; Low-power operating modes; Memory banking; Migration; Replication,Banking; Data compression; Dynamic random access storage; Energy conservation; Energy utilization; Integer programming; Memory architecture; Program compilers; Energy Minimization techniques; Experimental analysis; Integer Linear Programming; Low-power operating modes; Memory banking; Migration; Optimization framework; Replication; Data reduction
Multiprocessor systems synthesis for multiple use-cases of multiple applications on FPGA,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849084546&doi=10.1145%2f1367045.1367049&partnerID=40&md5=dd791c492afbeba5081747601ee62697,"Future applications for embedded systems demand chip multiprocessor designs to meet real-time deadlines. The large number of applications in these systems generates an exponential number of use-cases. The key design automation challenges are designing systems for these use-cases and fast exploration of software and hardware implementation alternatives with accurate performance evaluation of these use-cases. These challenges cannot be overcome by current design methodologies which are semiautomated, time consuming, and error prone. In this article, we present a design methodology to generate multiprocessor systems in a systematic and fully automated way for multiple use-cases. Techniques are presented to merge multiple use-cases into one hardware design to minimize cost and design time, making it well suited for fast design-space exploration (DSE) in MPSoC systems. Heuristics to partition use-cases are also presented such that each partition can fit in an FPGA, and all use-cases can be catered for. The proposed methodology is implemented into a tool for Xilinx FPGAs for evaluation. The tool is also made available online for the benefit of the research community and is used to carry out a DSE case study with multiple use-cases of real-life applications: H263 and JPEG decoders. The generation of the entire design takes about 100 ms, and the whole DSE was completed in 45 minutes, including FPGA mapping and synthesis. The heuristics used for use-case partitioning reduce the design-exploration time elevenfold in a case study with mobile-phone applications. © 2008 ACM.",Design exploration; FPGA; Multi-application; Multimedia systems; Multiple use-cases; Multiprocessor systems; Synchronous data-flow graphs,Computer aided design; Embedded systems; Field programmable gate arrays (FPGA); Hardware; Heuristic methods; Integrated circuits; Land use; Multiprocessing systems; Software design; Space research; Telecommunication; Telecommunication equipment; Accurate performance; Case studies; Chip multi processor; Design automation; Design exploration; Design methodologies; Design time; Design-space-exploration; Designing systems; Error-prone; Exponential numbers; FPGA; Future applications; Hardware designs; Hardware implementations; JPEG decoders; Multi-application; Multimedia systems; Multiple applications; Multiple use-cases; Multiple-use; Multiprocessor systems; Phone applications; Real-life applications; Research communities; Synchronous data-flow graphs; Time consuming; Time deadlines; Real time systems
ACM Transactions on Design Automation of Electronic Systems: Editorial,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849095860&doi=10.1145%2f1367045.1367046&partnerID=40&md5=e918f365105753f252eb18fd870cc00d,[No abstract available],,
Postplacement voltage assignment under performance constraints,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849103306&doi=10.1145%2f1367045.1367055&partnerID=40&md5=6b20cb9539488c4bddb79573b595a74f,"Multi-Vdd is an effective method to reduce both leakage and dynamic power. A key challenge in a multi-Vdd design is to control the complexity of the power-supply system and limit the demand for level shifters. This can be tackled by grouping cells of different supply voltages into a small number of voltage islands. Recently, an elegant algorithm was proposed for generating voltage islands that balance the power-versus-design-cost tradeoff under performance requirement, according to the placement proximity of the critical cells. One prerequisite of this algorithm is an initial voltage assignment at the standard-cell level that meets timing. In this article, we present a novel method to produce quality voltage assignment which not only meets timing but also forms good proximity of the critical cells to provide a smooth input to the aforementioned voltage island generation. Our algorithm is based on effective delay budgeting and efficient computation of physical proximity by Voronoi diagram. Our extensive experiments on real industrial designs show that our algorithm leads to 25% - 75% improvement in the voltage island generation in terms of the number of voltage islands generated, with computation time only linear to design size. © 2008 ACM.",Low power; Timing; Voltage assignment; Voronoi diagram,Budget control; Cells; Computational geometry; Cytology; Electric power systems; Graphic methods; Integrated circuit manufacture; Efficient computation; Low Power; Performance constraints; Performance requirements; Physical proximity; Timing; Voltage assignment; Voronoi diagrams; Computational efficiency
Introduction to the special section on demonstrable software systems and hardware platforms II,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849093341&doi=10.1145%2f1367045.1367047&partnerID=40&md5=9e01d1280e07b1a3ce4915ad948f8285,[No abstract available],,
Automatic verification of safety and liveness for pipelined machines using WEB refinement,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849102231&doi=10.1145%2f1367045.1367054&partnerID=40&md5=dfbabf5203b91b31a3bc0c6221d2dfcb,"We show how to automatically verify that complex pipelined machine models satisfy the same safety and liveness properties as their instruction-set architecture (ISA) models by using well-founded equivalence bisimulation (WEB) refinement. We show how to reduce WEB-refinement proof obligations to formulas expressible in the decidable logic of counter arithmetic with lambda expressions and uninterpreted functions (CLU). This allows us to automate the verification of the pipelined machine models by using the UCLID decision procedure to transform CLU formulas to Boolean satisfiability problems. To relate pipelined machine states to ISA states, we use the commitment and flushing refinement maps. We evaluate our work using 17 pipelined machine models that contain various features, including deep pipelines, precise exceptions, branch prediction, interrupts, and instruction queues. Our experimental results show that the overhead of proving liveness, obtained by comparing the cost of proving both safety and liveness with the cost of only proving safety, is about 17%, but depends on the refinement map used; for example, the liveness overhead is 23% when flushing is used and is negligible when commitment is used. © 2008 ACM.",Bisimulation; Commitment; Flushing; Liveness; Pipelined machines; Refinement; Refinement maps; SAT; Verification,Boolean algebra; Computer aided software engineering; Computer architecture; Verification; Bisimulations; Commitment; Flushing; Liveness; Pipelined machines; Refinement; Refinement maps; Pipelines
Parametric variability analysis for multistage analog circuits using analytical sensitivity modeling,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149096517&doi=10.1145%2f1344418.1344429&partnerID=40&md5=1b9e20e40cf3605deafdb325c02090b6,"Process variations play an increasingly important role on the success of analog circuits. State-of-the-art analog circuits are based on complex architectures and contain many hierarchical layers and parameters. Knowledge of the parameter variances and their contribution patterns is crucial for a successful design process. This information is valuable to find solutions for many problems in design, design automation, testing, and fault tolerance. In this article, we present a hierarchical variance analysis methodology for multistage analog circuits. Starting from the process/layout level, we derive implicit hierarchical relations and extract the sensitivity information analytically. We make use of previously computed values whenever possible so as to reduce computational time. The proposed approach is particularly geared for the domain of design and test automation, where multiple runs on slightly different circuits are necessary. Experimental results indicate that the proposed method provides both accuracy and computational efficiency when compared with prior approaches. © 2008 ACM.",Analog circuits; Hierarchical variance analysis; Parameter correlations; Performance model; Process variations,Computational efficiency; Hierarchical systems; Mathematical models; Parameter estimation; Problem solving; Computational time; Hierarchical variance analysis; Performance model; Process variations; Analog circuits
A noniterative equivalent waveform model for timing analysis in presence of crosstalk,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149106514&doi=10.1145%2f1344418.1344421&partnerID=40&md5=895e38fb4c64ca9a7089487a2762c95a,"Due to the nonuniform interconnect scaling in the Deep Sub Micron (DSM) region, the coupling capacitance between wires becomes an increasingly dominant fraction of the total wire capacitance. This couple capacitance introduces server crosstalk which causes delay variations on signal lines and raises signal integrity problems. Therefore, including crosstalk in the timing analysis methods has become imperative for current technologies. And to correctly model the crosstalk, output loading effects, waveform shape and gate driving capability have to be considered. However, most existing crosstalk models have not yet included these factors and consequently suffer from the low accuracy problem. In this article, we propose a noniterative equivalent waveform model that addresses the above mentioned issues. Our experimental results have shown that the new model achieves 3 times speed up and 95% accuracy compared to the existing models. © 2008 ACM.",Deep sub micron; Delay; Equivalent waveform; Noise; Timing analysis,Capacitance; Crosstalk; Mathematical models; Signal analysis; Waveform analysis; Deep sub microns; Equivalent waveforms; Timing analysis; Equivalent circuits
"Introduction to joint ACM JETC/TODAES special issue on new, emerging, and specialized technologies",2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149120904&doi=10.1145%2f1344418.1344432&partnerID=40&md5=c32f68dc7e694a0a060413956888e097,[No abstract available],,
A multiprocessor system-on-chip for real-time biomedical monitoring and analysis: ECG prototype architectural design space exploration,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43349107249&doi=10.1145%2f1344418.1344427&partnerID=40&md5=1cd3b78a26faad1dbf2dad81f048a116,"In this article we focus on multiprocessor system-on-chip (MPSoC) architectures for human heart electrocardiogram (ECG) real time analysis as a hardware/software (HW/SW) platform offering an advance relative to state-of-the-art solutions. This is a relevant biomedical application with good potential market, since heart diseases are responsible for the largest number of yearly deaths. Hence, it is a good target for an application-specific system-on-chip (SoC) and HW/SW codesign. We investigate a symmetric multiprocessor architecture based on STMicroelectronics VLIW DSPs that process in real time 12-lead ECG signals. This architecture improves upon state-of-the-art SoC designs for ECG analysis in its ability to analyze the full 12 leads in real time, even with high sampling frequencies, and its ability to detect heart malfunction for the whole ECG signal interval. We explore the design space by considering a number of hardware and software architectural options. Comparing our design with present-day solutions from an SoC and application point-of-view shows that our platform can be used in real time and without failures. © 2008 ACM.",Electrocardiogram algorithms; Embedded system design; Hardware space exploration; Multiprocessor system-on-chip; Real time analysis,Biological organs; Electrocardiography; Medical applications; Multiprocessing systems; Real time control; Software architecture; Electrocardiogram algorithms; Embedded system design; Hardware space exploration; Multiprocessor system-on-chip; Microprocessor chips
ACM Transactions on Design: Editorial,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149084844&doi=10.1145%2f1344418.1344419&partnerID=40&md5=f38488e49eb13b14fdef27f5761bbedd,[No abstract available],,
Radio frequency identification prototyping,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149109738&doi=10.1145%2f1344418.1344425&partnerID=40&md5=ea52667a8798007fb196f74c3bdeb093,"While RFID is starting to become a ubiquitious technology, the variation between different RFID systems still remains high. This paper presents several prototyping environments for different components of radio frequency identification (RFID) tags to demonstrate how many of these components can be standardized for many different purposes. We include two active tag prototypes, one based on a microprocessor and the second based on custom hardware. To program these devices we present a design automation flow that allows RFID transactions to be described in terms of primitives with behavior written in ANSI C code. To save power with active RFID devices we describe a passive transceiver switch called the burst switch and demonstrate how this can be used in a system with a microprocessor or custom hardware controller. Finally, we present a full RFID system prototyping environment based on real-time spectrum analysis technology currently deployed at the University of Pittsburgh RFID Center of Excellence. Using our prototyping techniques we show how transactions from multiple standards can be combined and targeted to several microprocessors include the Microchip PIC, Intel StrongARM and XScale, and AD Chips EISC as well as several hardware targets including the Altera Apex, Actel Fusion, Xilinx Coolrunner II, Spartan 3 and Virtex 2, and cell-based ASICs. © 2008 ACM.",Design automation; Low-power; Prototyping; RFID,Computer hardware; Rapid prototyping; Standardization; Systems analysis; Technology transfer; Ubiquitous computing; ANSI C code; Design automation; Radio frequency identification prototyping; Radio frequency identification (RFID)
Reconfigurable content-based router using hardware-accelerated language parser,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149101296&doi=10.1145%2f1344418.1344424&partnerID=40&md5=069e92551c18646856f84b9382320222,"This article presents a dense logic design for matching multiple regular expressions with a field programmable gate array (FPGA) at 10+ Gbps. It leverages on the design techniques that enforce the shortest critical path on most FPGA architectures while optimizing the circuit size. The architecture is capable of supporting a maximum throughput of 12.90 Gbps on a Xilinx Virtex 4 LX200 and its performance is linearly scalable with size. Additionally, this article presents techniques for parsing data streams to provide semantic information for patterns found within a data stream. We illustrate how a content-based router can be implemented with our parsing techniques using an XML parser as an example. The content-based router presented was designed, implemented, and tested in a Xilinx Virtex XCV2000E FPGA on the FPX platform. It is capable of processing 32-bits of data per clock cycle and runs at 100 MHz. This allows the system to process and route XML messages at 3.2 Gbps. © 2008 ACM.",Content-based routing; Parser hardware; Parsing; Pattern matching; Regular expressions; XML,Computer hardware; Computer programming languages; Field programmable gate arrays (FPGA); Logic design; Pattern matching; XML; Content-based routing; Parser hardware; Regular expressions; Routers
SAT-based ATPG using multilevel compatible don't-cares,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149086880&doi=10.1145%2f1344418.1344420&partnerID=40&md5=80b72289f992a81c74d7145cef6a892a,"In a typical IC design flow, circuits are optimized using multilevel don't cares. The computed don't cares are discarded before Technology Mapping or Automatic Test Pattern Generation (ATPG). In this paper, we present two combinational ATPG algorithms for combinational designs. These algorithms utilize the multilevel don't cares that are computed for the design during technology independent logic optimization. They are based on Boolean Satisfiability (SAT), and utilize the single stuck-at fault model. Both algorithms make use of the Compatible Observability Don't Cares (CODCs) associated with nodes of the circuit, to speed up the ATPG process. For large circuits, both algorithms make use of approximate CODCs (ACODCs), which we can compute efficiently. Our first technique speeds up fault propagation by modifying the active clauses in the transitive fanout (TFO) of the fault site. In our second technique, we define new j-active variables for specific nodes in the transitive fanin (TFI) of the fault site. Using these j-active variables we write additional clauses to speed up fault justification. Experimental results demonstrate that the combination of these techniques (when using CODCs) results in an average reduction of 45% in ATPG runtimes. When ACODCs are used, a speed-up of about 30% is obtained in the ATPG run-times for large designs. We compare our method against a commercial structural ATPG tool as well. Our method is slower for small designs, but for large designs, we obtain a 31% average speedup over the commercial tool. © 2008 ACM.",Automatic test pattern generation (ATPG); Boolean satisfiabilty (SAT); Don't cares; Testing,Algorithms; Boolean algebra; Integrated circuit testing; Logic design; Optimization; Automatic test pattern generation (ATPG); Boolean satisfiabilty (SAT); Compatible Observability Don't Cares (CODC); Integrated circuit layout
A fast simultaneous input vector generation and gate replacement algorithm for leakage power reduction,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149085557&doi=10.1145%2f1344418.1344430&partnerID=40&md5=06c1521e60aa12d72249a7a75a6aed57,"The Input vector control (IVC) technique is based on the observation that the leakage current in a CMOS logic gate depends on gate input state, and a good input vector is able to minimize leakage when the circuit is in sleep mode. The gate replacement technique is a very effective method to further reduce the leakage current. In this article, we propose a fast heuristic algorithm to find a low-leakage input vector with simultaneous gate replacement. Results on MCNC91 benchmark circuits show that our algorithm produces 14% better leakage current reduction with several orders of magnitude speedup in runtime for large circuits compared to the previous state-of-the-art algorithm. In particular, the average runtime for the ten largest combinational circuits has been dramatically reduced from 1879 seconds to 0.34 seconds. © 2008 ACM.",Gate replacement; Input vector control; Leakage reduction,Benchmarking; Combinatorial circuits; Heuristic algorithms; Leakage currents; Logic gates; Benchmark circuits; Gate replacement; Gate replacement algorithms; Leakage reduction; CMOS integrated circuits
An open-source binary utility generator,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149126356&doi=10.1145%2f1344418.1344423&partnerID=40&md5=215c4b79d2bf68e7d3f659de03c481e5,"Electronic system level (ESL) modeling allows early hardware-dependent software (HDS) development. Due to broad CPU diversity and shrinking time-to-market, HDS development can neither rely on hand-retargeting binary tools, nor can it rely on pre-existent tools within standard packages. As a consequence, binary utilities which can be easily adapted to new CPU targets are of increasing interest. We present in this article a framework for automatic generation of binary utilities. It relies on two innovative ideas: platform-aware modeling and more inclusive relocation handling. Generated assemblers, linkers, disassemblers and debuggers were validated for MIPS, SPARC, PowerPC, i8051 and PIC16F84. An open-source prototype generator is available for download. © 2008 ACM.",Platform debugging; Retargetable tools; TLM,Computer operating systems; Mathematical models; Program debugging; Software packages; Electronic system level (ESL) modeling; Platform debugging; Retargetable tools; Computer aided software engineering
Timing-driven octilinear Steiner tree construction based on Steiner-point reassignment and path reconstruction,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149086632&doi=10.1145%2f1344418.1344422&partnerID=40&md5=b560d87ea6edbb3ce660967de73a527f,"It is well known that the problem of constructing a timing-driven rectilinear Steiner tree for any signal net is important in performance-driven designs and has been extensively studied. Until now, many efficient approaches have been proposed for the construction of a timing-driven rectilinear Steiner tree. As technology process advances, +45°and -45°diagonal segments can be permitted in an octilinear routing model. To our knowledge, no approach is proposed to construct a timing-driven octilinear Steiner tree for any signal net. In this paper, given a rectilinear Steiner tree for any signal net, we propose an efficient transformation-based approach to construct a timing-driven octilinear Steiner tree based on the computation of the octilinear distance and the concept of Steiner-point reassignment and path reconstruction in an octilinear routing model. The experimental results show that our proposed transformation-based approach can use reasonable CPU time to construct a TOST, and a 10% - 18% improvement in timing delay and a 5% - 14% improvement in total wire length in the original RSTs are obtained in the construction of TOSTs for the tested signal nets. © 2008 ACM.",Elmore delay; Global routing; Octilinear Steiner tree; Steiner points,Logic design; Mathematical models; Network routing; Signal analysis; Elmore delay; Global routing; Octilinear Steiner tree; Steiner points; Trees (mathematics)
Physical synthesis for FPGA interconnect power reduction by dual-Vdd budgeting and retiming,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149119759&doi=10.1145%2f1344418.1344426&partnerID=40&md5=1f2cda85c11f7cf8da4f1828b169ecc5,"Field programmable dual-Vdd interconnects are effective in reducing FPGA power. We formulate the dual-Vdd-aware slack budgeting problem as a linear program (LP) and a min-cost network flow problem, respectively. Both algorithms reduce interconnect power by 50% on average compared to single-Vdd interconnects, but the network-flow-based algorithm runs 11x faster on MCNC benchmarks. Furthermore, we develop simultaneous retiming and slack budgeting (SRSB) with flip-flop layout constraints in dual-Vdd FPGAs based on mixed integer linear programming, and speed-up the algorithm by LP relaxation and local legalization. Compared to retiming followed by slack budgeting, SRSB reduces interconnect power by up to 28.8%. © 2008 ACM.",FPGA; Low power; Retiming,Algorithms; Benchmarking; Electric power system interconnection; Linear programming; Relaxation processes; Flip-flop layout constraints; Local legalization; Slack budgeting problem; Field programmable gate arrays (FPGA)
Heterogeneously tagged caches for low-power embedded systems with virtual memory support,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149105184&doi=10.1145%2f1344418.1344428&partnerID=40&md5=34310571f8ef3cbe7d7aff4d14be0b06,"An energy-efficient data cache organization for embedded processors with virtual memory is proposed. Application knowledge regarding memory references is used to eliminate most tag translations. A novel tagging scheme is introduced, where both virtual and physical tags coexist. Physical tags and special handling of superset index bits are only used for references to shared regions in order to avoid cache inconsistency. By eliminating the need for most address translations on cache access, a significant power reduction is achieved. We outline an efficient hardware architecture, where the application information is captured in a reprogrammable way and the cache is minimally modified. © 2008 ACM.",Embedded systems,Data handling; Embedded systems; Knowledge based systems; Virtual reality; Data cache organization; Power reduction; Superset index bits; Tag translations; Virtual memory; Cache memory
SoCDAL: System-on-chip design AcceLerator,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049094644&doi=10.1145%2f1297666.1297683&partnerID=40&md5=45be68b727c2e644d5f56d36fadf19cc,"Time-to-market pressure and the ever-growing design complexity of multiprocessor system-on-chips have demanded an efficient design environment that enables fast exploration of large design space. In this article, we introduce a new design environment, called SoCDAL, for accelerating multiprocessor system-on-chip design through fast design-space exploration targeting real-time multimedia systems. SoCDAL is a set of mostly automated tools covering system specification, hardware/software estimation, application-to-architecture mapping, simulation model generation, and system verification through simulation. For system specification, the process network model has been widely used for system specification because of its modeling capability. However, it is hard to use for real-time systems design, since its behavior cannot be estimated statically. We introduce a new approach which enables analyzing a process network model statically with some restrictions. For the hardware/software estimation, we analyze codes statically. Application-to-architecture mapping process implements a novel algorithm to support an arbitrary number of processors, with performance evaluation by static scheduling considering communication behavior. Mapping results are used to generate simulation models automatically at several transaction levels to be pipelined to a commercial tool. We show the effectiveness of our approaches by some experimental results with multimedia applications such as JPEG, H.263, and H.264 encoders, as well as an H.264 decoder. © 2008 ACM.",Application-to-architecture mapping; Codesign; Multiprocessor system-on-chip; Process networks; Scheduling; Simulation; Specification; Static hardware/software estimation; Synchronous dataflow; Transaction-level model; Worst-case execution time,Computer simulation; Microprocessor chips; Multimedia systems; Multiprocessing systems; Real time systems; Scheduling algorithms; Static analysis; Application-to-architecture mapping; Multiprocessor system-on-chip; Process networks; Synchronous dataflow; Transaction-level models; Systems analysis
ReChannel: Describing and simulating reconfigurable hardware in systemC,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049092104&doi=10.1145%2f1297666.1297681&partnerID=40&md5=cb8d1a22efc21f6d9f3621b4efb92bcb,"With the ongoing integration of (dynamic) reconfiguration into current system models, new methodologies and tools are needed to help the designer during the development process. This article introduces a language extension for SystemC along with a design methodology for describing and simulating dynamically reconfigurable systems at all levels of abstraction. The presented library provides maximum freedom of description of reconfiguration behavior and its control, while featuring simulation of runtime configuration, removal, and exchange of custom modules as well as third-party IP-cores during the complete architecture refinement process. When designing at RT-level, the resulting hardware description can easily be synthesized by standard synthesis tools. © 2008 ACM.",Dynamic reconfiguration; Hardware description; Reconfigurable hardware; Refinement; Simulation; SystemC,Computer simulation; Digital libraries; Mathematical models; Custom modules; Hardware description; Reconfigurable hardware; Runtime configuration; SystemC; Computer hardware
"Introduction to special section on high-level design, validation, and test",2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049089613&doi=10.1145%2f1297666.1297668&partnerID=40&md5=c7be6ccbbfcef964dc28260ac6cee391,[No abstract available],,
C-testable bit parallel multipliers over GF(2m),2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049107126&doi=10.1145%2f1297666.1297671&partnerID=40&md5=d4ed8f534c4d166f8b5f8dad468a1daf,"We present a C-testable design of polynomial basis (PB) bit-parallel (BP) multipliers over GF(2m) for 100% coverage of stuck-at faults. Our design method also includes the method for test vector generation, which is simple and efficient. C-testability is achieved with three control inputs and approximately 6% additional hardware. Only 8 constant vectors are required irrespective of the sizes of the fields and primitive polynomial. We also present a Built-In Self-Test (BIST) architecture for generating the test vectors efficiently, which eliminates the need for the extra control inputs. Since these circuits have critical applications as parts of cryptography (e.g., Elliptic Curve Crypto (ECC) systems) hardware, the BIST architecture may provide with added level of security, as the tests would be done internally and without the requirement of probing by external testing equipment. Finally we present experimental results comprising the area, delay and power of the testable multipliers of various sizes with the help of the Synopsys tools using UMC 0.18 micron CMOS technology library. © 2008 ACM.",Built-in self-test; C-testable; Cryptography; Digital signal processing; Error control code; Fault; Galois field; Multiplier; Polynomials; Stuck-at fault; Testing; TPG; VLSI design,Built-in self test; Computational methods; Cryptography; Error compensation; Polynomials; Signal processing; Vectors; Error control code; Stuck-at fault; VLSI design; Multiplying circuits
Boosting interpolation with dynamic localized abstraction and redundancy removal,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049111341&doi=10.1145%2f1297666.1297669&partnerID=40&md5=80e861cda86750ca4bad43d3bc7ec026,"SAT - based Unbounded Model Checking based on Craig Interpolants is often able to overcome BDDs and other SAT - based techniques on large verification instances. Based on refutation proofs generated by SAT solvers, interpolants provide compact circuit representations of state sets, as they abstract away several nonrelevant details of the proofs. We propose three main contributions, aimed at controlling interpolant size and traversal depth. First of all, we introduce interpolant - based dynamic abstraction to reduce the support of computed interpolants. Subsequently, we propose new advances in interpolant compaction by redundancy removal. Finally, we introduce interpolant computation exploiting circuit quantification, instead of SAT refutation proofs. These techniques heavily rely on an effective application of the incremental SAT paradigm. The experimental results proposed in this paper are specifically oriented to prove properties, rather than disproving them, i.e., they target complete verification instead of simply hunting bugs. They show how this methodology is able to stretch the applicability of interpolant - based Model Checking to larger and deeper verification instances. © 2008 ACM.",Abstraction; Interpolant; Redundancy removal,Interpolation; Model checking; Redundancy; Compact circuits; Interpolant; Redundancy removal; Abstracting
Analysis and optimization of prediction-based flow control in networks-on-chip,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049087837&doi=10.1145%2f1297666.1297677&partnerID=40&md5=1c540ccc0a299ec6396f8f6c0136f8f4,"Networks-on-Chip (NoC) communication architectures have emerged recently as a scalable solution to on-chip communication problems. While the NoC architectures may offer higher bandwidth compared to traditional bus-based communication, their performance can degrade significantly in the absence of effective flow control algorithms. Unfortunately, flow control algorithms developed for macronetworks, either rely on local information, or suffer from large communication overhead and unpredictable delays. Hence, using them in the NoC context is problematic at best. For this reason, we propose a predictive closed-loop flow control mechanism and make the following contributions: First, we develop traffic source and router models specifically targeted to NoCs. Then, we utilize these models to predict the possible congestion in the network. Based on this information, the proposed scheme controls the packet injection rate at traffic sources in order to regulate the total number of packets in the network. We also illustrate the proposed traffic source model and the applicability of the proposed flow controller to actual designs using real NoC implementations. Finally, simulations and experimental study using our FPGA prototype show that the proposed controller delivers a better performance compared to the traditional switch-to-switch flow control algorithms under various real and synthetic traffic patterns. © 2008 ACM.",Congestion control; Flow control; Multi-processor systems; Networks-on-chip,Algorithms; Computer simulation; Congestion control (communication); Control equipment; Field programmable gate arrays (FPGA); Multiprocessing systems; Networks-on-chip; Packet injection rate; Data flow analysis
Optimizing wirelength and routability by searching alternative packings in floorplanning,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049108581&doi=10.1145%2f1297666.1297687&partnerID=40&md5=6ed903107b8ce3ce291199b180c59978,"Recent advances in VLSI technology have made optimization of the interconnect delay and routability of a circuit more important. We should consider interconnect planning as early as possible. We propose a postfloorplanning step to reduce the interconnect cost of a floorplan by searching alternative packings. If a packing contains a rectangular bounding box of a group of modules, we can rearrange the blocks in the bounding box to obtain a new floorplan with the same area, but possibly with a smaller interconnect cost. Experimental results show that we can reduce the interconnect cost of a packing without any penalty in area. © 2008 ACM.",Floorplanning; Wirelength reduction,Cost effectiveness; Cost reduction; Large scale systems; Network routing; Optimization; VLSI circuits; Floorplanning; Interconnect planning; Wirelength reduction; Floors
A fuel-cell-battery hybrid for portable embedded systems,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049088776&doi=10.1145%2f1297666.1297685&partnerID=40&md5=14a5205dd58a049f94db21eff377cea9,"This article presents our work on the development of a fuel cell (FC) and battery hybrid (FC-Bh) system for use in portable microelectronic systems. We describe the design and control of the hybrid system, as well as a dynamic power management (DPM)-based energy management policy that extends its operational lifetime. The FC is of the proton exchange membrane (PEM) type, operates at room temperature, and has an energy density which is 4 - 6 times that of a Li-ion battery. The FC cannot respond to sudden changes in the load, and so a system powered solely by the FC is not economical. An FC-Bh power source, on the other hand, can provide the high energy density of the FC and the high power density of a battery. In this work we first describe the prototype FC-Bh system that we have built. Such a prototype helps to characterize the performance of a hybrid power source, and also helps explore new energy management strategies for embedded systems powered by hybrid sources. Next we describe a Matlab/Simulink-based FC-Bh system simulator which serves as an alternate experimental platform and that enables quick evaluation of system-level control policies. Finally, we present an optimization framework that explicitly considers the characteristics of the FC-Bh system and is aimed at minimizing the fuel consumption. This optimization framework is applied on top of a prediction-based DPM policy and is used to derive a new fuel-efficient DPM scheme. The proposed scheme demonstrates up to 32% system lifetime extension compared to a competing scheme when run on a real trace-based MPEG encoding example. © 2008 ACM.",Battery; DPM; Fuel cell; Hybrid systems; Simulation; Simulator,Fuel consumption; Lithium batteries; MATLAB; Microelectronics; Motion Picture Experts Group standards; Optimization; Proton exchange membrane fuel cells (PEMFC); Dynamic power management (DPM); Hybrid systems; Microelectronic systems; Room temperature; Embedded systems
ACM Transactions on Design Automation of Electronics Systems: Editorial,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049084335&doi=10.1145%2f1297666.1297667&partnerID=40&md5=f1184eec07652469d2b137f50a877083,[No abstract available],,
Probabilistic transfer matrices in symbolic reliability analysis of logic circuits,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049089079&doi=10.1145%2f1297666.1297674&partnerID=40&md5=7e5eb1372f1730202b18ae8c97c1c1f2,"We propose the probabilistic transfer matrix (PTM) framework to capture nondeterministic behavior in logic circuits. PTMs provide a concise description of both normal and faulty behavior, and are well-suited to reliability and error susceptibility calculations. A few simple composition rules based on connectivity can be used to recursively build larger PTMs (representing entire logic circuits) from smaller gate PTMs. PTMs for gates in series are combined using matrix multiplication, and PTMs for gates in parallel are combined using the tensor product operation. PTMs can accurately calculate joint output probabilities in the presence of reconvergent fanout and inseparable joint input distributions. To improve computational efficiency, we encode PTMs as algebraic decision diagrams (ADDs). We also develop equivalent ADD algorithms for newly defined matrix operations such as eliminate variables and eliminate redundant variables, which aid in the numerical computation of circuit PTMs. We use PTMs to evaluate circuit reliability and derive polynomial approximations for circuit error probabilities in terms of gate error probabilities. PTMs can also analyze the effects of logic and electrical masking on error mitigation. We show that ignoring logic masking can overestimate errors by an order of magnitude. We incorporate electrical masking by computing error attenuation probabilities, based on analytical models, into an extended PTM framework for reliability computation. We further define a susceptibility measure to identify gates whose errors are not well masked. We show that hardening a few gates can significantly improve circuit reliability. © 2008 ACM.",Fault tolerance; Symbolic analysis,Algebra; Computational efficiency; Error analysis; Fault tolerance; Probability; Reliability theory; Error susceptibility; Matrix multiplication; Probabilistic transfer matrices; Symbolic analysis; Logic circuits
Automata-based assertion-checker synthesis of PSL properties,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049103424&doi=10.1145%2f1297666.1297670&partnerID=40&md5=0cb497417f7b54ab797f8986549f5cba,"Assertion-based verification with languages such as PSL is gaining in importance. From assertions, one can generate hardware assertion checkers for use in emulation, simulation acceleration and silicon debug. We present techniques for checker generation of the complete set of PSL properties, including all variants of operators, both strong and weak. A full automata-based approach allows an entire assertion to be represented by a single automaton, hence allowing optimizations that can not be done in a modular approach where subcircuits are created only for individual operators. For this purpose, automata algorithms are developed for the base cases, and a complete set of rewrite rules is derived for other operators. Automata splitting is introduced for an efficient implementation of the eventually operator. © 2008 ACM.",Assertion checkers; Assertion-based verification; Automata; Emulation; Hardware; PSL,Automata theory; Computer simulation; Optimization; Program debugging; Silicon; Assertion checkers; Assertion-based verification; Automata splitting; Emulation; Verification
A versatile paradigm for scan chain diagnosis of complex faults using signal processing techniques,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049099554&doi=10.1145%2f1297666.1297675&partnerID=40&md5=911581df7d6f4bdb3ae332850516f829,"Scan chains are popularly used as the channels for silicon testing and debugging. However, they have also been identified as one of the culprits of silicon failure more recently. To cope with this problem, several scan chain diagnosis approaches have been proposed in the past. The existing methods, however, suffer from one common drawback - -that is, they rely on fault models and matching heuristics to locate the faults. Such a paradigm may run into difficulty when the fault under diagnosis does not match the fault model exactly, for example, when there is a bridging between a flip-flop and a logic cell, or the fault is temporal and only manifests itself intermittently. In light of this, we propose in this article a more versatile model-free paradigm for locating the faulty flip-flops in a scan chain, incorporating a number of signal processing techniques, such as filtering and edge detection. These techniques performed on the test responses of the failing chip under diagnosis directly can effectively reveal the fault location(s) in a scan chain. As compared to the previous works, our approach is better capable of handling intermittent faults and bridging faults, even under nonideal conditions, for example, when the core logic is also faulty. Experimental results on several real designs indicate that this approach can indeed catch some nasty faults that previous methods could not catch. © 2008 ACM.",Design for testability; Diagnosis; Fault; Profiling; Scan chain,Computer aided diagnosis; Design for testability; Fault tolerance; Flip flop circuits; Mathematical models; Program debugging; Silicon; Fault models; Scan chain diagnosis; Signal processing
Enabling multimedia using resource-constrained video processing techniques: A node-centric perspective,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049091606&doi=10.1145%2f1297666.1297684&partnerID=40&md5=a495b677cdcb36a01ea60f10371764aa,"Successful proliferation of multimedia-enabled devices and advances in very large-scale integration (VLSI) technology has spawned new research efforts in migrating video processing applications onto ever smaller and more inexpensive devices. This article focuses on the technical challenges associated with that migration. Due to limitations in size, battery lifetime, and, ultimately, cost, mapping complex video applications onto resource-constrained systems is a very challenging proposition. To this end, we first consider a technique, region-of-interest (ROI) processing, of defining a window within a video frame and only operating on the data inside that window, ignoring the rest of the frame. By using this lossy technique, the processing requirements can be reduced by roughly 80% while the error introduced in the quality of the results is roughly 10%. The other technique is adaptive data partitioning (ADP) combined with a content-based power management algorithm. By distributing video processing among multiple processors and shutting them down when they are not needed, the energy consumed per processor can be reduced by 60% without sacrificing the performance of the underlying video-based application. Taken together, these novel techniques enable ambient multimedia systems and maintain the needed overall efficiency in video processing. © 2008 ACM.",Data partitioning; Lossy and lossless video processing; Real-time video processing; Region-of-interest (ROI),Adaptive algorithms; Cost effectiveness; Data reduction; Multimedia systems; Video signal processing; VLSI circuits; Data partitioning; Lossless video processing; Real-time video processing; Region-of-interest (ROI); Resource allocation
Interrupt modeling for efficient high-level scheduler design space exploration,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049110241&doi=10.1145%2f1297666.1297676&partnerID=40&md5=d651810048572abbc73812bd5d0cfb28,"Single Chip Heterogeneous Multiprocessors executing a wide variety of software are increasingly common in consumer electronics. Because of the mix of real-time and best effort software across the entire chip, a key design element of these systems is the choice of scheduling strategy. Without task migration, the benefits of single chip processing cannot be fully realized. Previously, high-level modeling environments have not been capable of modeling asynchronous events such as interrupts and preemptive scheduling while preserving the performance benefits of high level simulation. This paper shows how extensions to Modeling Environment for Software and Hardware (MESH) enable precise modeling of these asynchronous events while running more than 1000 faster than cycle-accurate simulation. We discuss how we achieved this and illustrate its use in modeling preemptive scheduling. We evaluate the potential of migrating running tasks between processors to improve performance in a multimedia cell phone example. We show that by allowing schedulers to rebalance processor loads as new tasks arrive significant performance gains can be achieved over statically partitioned and dynamic scheduling approaches. In our example, we show that system response time can be improved by as much as 1.96 times when a preemptive migratory scheduler is used, despite the overhead incurred by scheduling tasks across multiple processors and transferring state during the migration of running tasks. The contribution of this work is to provide a framework for evaluating preemptive scheduling policies and task migration in a high level simulator, by combining the new ability to model interrupts with dramatically increased efficiency in the high-level modeling of scheduling and commuincation MESH already provides. © 2008 ACM.",Heterogeneous chip multiprocessors; MESH; Scenario oriented design,Computer simulation; Computer software; Consumer electronics; Scheduling; Systems analysis; Heterogeneous chip multiprocessors; Scenario oriented design; Single chip processing; Multiprocessing systems
Chip placement in a reticle for multiple-project wafer fabrication,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049090801&doi=10.1145%2f1297666.1297688&partnerID=40&md5=6825d74071d945dc0531de5204cea210,"Chip placement in a reticle is crucial to the cost of amultiprojectwafer run. In this article we develop several chip placement methods based on the volume-driven compatibility optimization (VOCO) concept, which maximizes dicing compatibility among chips with large-volume requirements while minimizing reticle dimensions. Our mixed-integer linear programming models with VOCO are too complex to render good solutions for large test cases. Our B*-tree with VOCO and HQ with VOCO use 16% 29% fewer wafers and 8% 19% less reticle area than the hierarchical quadrisection (HQ) method proposed by Kahng et al. [2005]. © 2008 ACM.",Compatibility graph; Conflict graph; Mixed-integer linear programming (MILP); Multiple-project wafers (MPW); Reticle floorplanning; Set cover; Set partition; Shuttle mask; Simulated annealing (SA); Wafer dicing,Computational methods; Graph theory; Integer programming; Linear programming; Mathematical models; Microfabrication; Microprocessor chips; Simulated annealing; Compatibility graph; Conflict graph; Multiple-project wafers (MPW); Reticle floorplanning; Set partition; Wafer dicing; Silicon wafers
A new efficient retiming algorithm derived by formal manipulation,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049087121&doi=10.1145%2f1297666.1297673&partnerID=40&md5=cfae511076fb5cdf8ec7576c131b9d4a,"A new efficient algorithm is derived for the minimal period retiming by formal manipulation. Contrary to all previous algorithms, which used fixed period feasibility checking to binary-search a candidate range, the derived algorithm checks the optimality of a feasible period directly. It is much simpler and more efficient than previous algorithms. Experimental results showed that it is even faster than ASTRA, an efficient heuristic algorithm. Since the derived algorithm is incremental by nature, it also opens the opportunity to be combined with other optimization techniques. © 2008 ACM.",Algorithm derivation; Clockperiod minimization; Retiming,Heuristic algorithms; Model checking; Optimization; Algorithm derivation; Clockperiod minimization; Retiming; Electric power systems
Synthesis of a novel timing-error detection architecture,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049106641&doi=10.1145%2f1297666.1297680&partnerID=40&md5=6e3dfaf8df52df4214d8b8351e5fc4c2,"Delay variation can cause a design to fail its timing specification. Ernst et al. [2003] observe that the worst delay of a design is least probable to occur. They propose a mechanism to detect and correct occasional errors while the design can be optimized for the common cases. Their experimental results show significant performance (or power) gain as compared with the worst-case design. However, the architecture in Ernst et al. [2003] suffers the short path problem, which is difficult to resolve. In this article, we propose a novel error-detecting architecture to solve the short path problem. Our experimental results show considerable performance gain can be achieved with reasonable area overhead. © 2008 ACM.",Fault tolerance; Logic synthesis,Fault tolerance; Optimization; Problem solving; Systems analysis; Delay variation; Error-detecting architecture; Logic synthesis; Short path problem; Error detection
Low-power gated and buffered clock network construction,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049103745&doi=10.1145%2f1297666.1297686&partnerID=40&md5=3723f77c367fd79211f0eab08ebf5aec,"We propose an efficient algorithm to construct a low-power zero-skew gated clock network, given the module locations and activity information. Unlike previous works, we consider masking logic insertion and buffer insertion simultaneously, and guarantee to yield a zero-skew clock tree. Both the logical and physical information of the modules are carefully taken into consideration when determining where masking logic should be inserted. We also account for the power overhead of the control signals so that the total average power consumption of the constructed zero-skew gated clock network can be minimized. To this end, we present a recursive approach to compute the effective switched capacitance of a general gated and buffered clock network, accounting for both the clock tree's and controller tree's switched capacitance. The power consumptions of the gated clock networks constructed by our algorithm are 20 to 36% lower than those reported in the best previous work in the literature. © 2008 ACM.",Buffer; Clock gating; Clock tree; Low power; Zero-skew,Algorithms; Capacitance; Formal logic; Optimization; Trees (mathematics); Clock gating; Clock tree; Zero-skew; Electric power systems
Tailoring circuit-switched network-on-chip to application-specific system-on-chip by two optimization schemes,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049109215&doi=10.1145%2f1297666.1297678&partnerID=40&md5=ab0193b2b8cea37738a3c1ae73d5ea48,"As the number of cores on a chip increases, power consumed by the communication structures takes a significant portion of the overall power budget. In this article, we first propose a circuit-switched interconnection architecture which uses crossroad switches to construct dedicated channels dynamically between any pairs of cores for nonhuge application-specific SoCs. The structure of the crossroad switch is simple, which can be regarded as a NoC-lite router, and we can easily construct a low-power on-chip network with these switches by a system-level design methodology. We also present the design methodology to tailor the proposed interconnection architecture to low-power structures by two proposed optimization schemes with profiled communication characteristics. The first scheme is power-aware topology construction, which can build low-power application-specific interconnection topologies. To further reduce the power consumption, we propose the second optimization scheme to predetermine the operating mode of dual-mode switches in the NoC at runtime. We evaluate several interconnection techniques, and the results show that the proposed architecture is more low-power and high-performance than others under some constraints and scale boundaries. We take multimedia applications as case studies, and experimental results show the power savings of power-aware topology approximate to 49% of the interconnection architecture. The power consumption can be further reduced approximately 25% by applying partially dedicated path mechanism. © 2008 ACM.",Application specific; Interconnection; Low power; Networks on chip; Systems on chips,Electric power utilization; Multimedia services; Optimization; Topology; Circuit-switched interconnection; Communication structures; Dedicated path mechanism; Power-aware topology; Integrated circuits
A tool for automatic detection of deadlock in wormhole networks on chip,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049104043&doi=10.1145%2f1297666.1297672&partnerID=40&md5=1d9862606e449c75fae87041bd87687f,"We present an extension of Duato's necessary and sufficient condition a routing function must satisfy in order to be deadlock-free, to support environment constraints inducing extra-dependencies between messages. We also present an original algorithm to automatically check the deadlock-freeness of a network with a given routing function. A prototype tool has been developed and automatic deadlock checking of large scale networks with various routing functions have been successfully achieved. We provide comparative results with standard approach, highlighting the benefits of our method. © 2008 ACM.",Deadlock; Interconnection networks; Networks on chip; Wormhole routing,Algorithms; Computational methods; Computer system recovery; Electronics packaging; Message passing; Network routing; Automatic detection; Networks on chip; Routing functions; Wormhole routing; Interconnection networks
Application-aware snoop filtering for low-power cache coherence in embedded multiprocessors,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049085249&doi=10.1145%2f1297666.1297682&partnerID=40&md5=95e8e4a77e2042292eac3b285a8c5f3d,"Maintaining local caches coherently in shared-memory multiprocessors results in significant power consumption. The customization methodology we propose exploits the fact that in embedded systems, important knowledge is available to the system designers regarding memory sharing between tasks. We demonstrate how the snoop-induced cache probings can be significantly reduced by identifying and exploiting in a deterministic way the shared memory regions between the processors. Snoop activity is enabled only for the accesses referring to known shared regions. The hardware support is not only cost efficient, but also software programmable, which allows for reprogrammability and customization across different tasks and applications. © 2008 ACM.",Cache coherence; Embedded multiprocessors; Low-power embedded systems; Snoop filtering,Cache memory; Computer software; Cost effectiveness; Embedded systems; Cache coherence; Embedded multiprocessors; Low-power embedded systems; Snoop filtering; Multiprocessing systems
Wavelet-based dynamic power management for nonstationary service requests,2008,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049101978&doi=10.1145%2f1297666.1297679&partnerID=40&md5=8c96342708e287721c2487c56058edcc,"In this article, a wavelet-based dynamic power management policy (WBDPM) is proposed. In this approach, the workload source (service requester) is modeled by a nonstationary time series which, in turn, represented by a nondecimated Haar wavelet as its basis. The proposed approach is robust and has the ability to minimize energy dissipation under different performance constraints. To assess the accuracy of the model, the algorithm was implemented for data extracted from the hard disks of computers. Prediction results of this approach for the case of a nonstationary service requester exhibit accuracies of more than 95%. © 2008 ACM.",Dynamic power management; Low-power system design; Nonstationary service request; Wavelet-based prediction,Algorithms; Energy dissipation; Systems analysis; Time series analysis; Wavelet analysis; Dynamic power management; Low-power system design; Nonstationary service request; Wavelet-based prediction; Energy management
A note on a mapping algorithm for computer-assisted exploration in the design of embedded systems,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148891374&doi=10.1145%2f1278349.1278365&partnerID=40&md5=f905444a08be63ca8a35e4e99d6c2431,[No abstract available],,
Methodology for operation shuffling and L0 cluster generation for low energy heterogeneous VLIW processors,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148862803&doi=10.1145%2f1278349.1278354&partnerID=40&md5=6d691c37510107dab7a050bc837bb91d,"Clustering L0 buffers is effective for energy reduction in the instruction memory hierarchy of embedded VLIW processors. However, the efficiency of the clustering depends on the schedule of the target application. Especially in heterogeneous or data clustered VLIW processors, determining energy efficient scheduling is more constraining. This article proposes a realistic technique supported by a tool flow to explore operation shuffling for improving generation of L0 clusters. The tool flow explores assignment of operations for each cycle and generates various schedules. This approach makes it possible to reduce energy consumption for various processor architectures. However, the computational complexity is large because of the huge exploration space. Therefore, some heuristics are also developed, which reduce the size of the exploration space while the solution quality remains reasonable. Furthermore, we also propose a technique to support VLIW processors with multiple data clusters, which is essential to apply the methodology to real world processors. The experimental results indicate potential gains of up to 27.6% in energy in L0 buffers, through operation shuffling for heterogeneous processor architectures as well as a homogeneous architecture. Furthermore, the proposed heuristics drastically reduce the exploration search space by about 90%, while the results are comparable to full search, with average differences of less than 1%. The experimental results indicate that energy efficiency can be improved in most of the media benchmarks by the proposed methodology, where the average gain is around 10% in comparison with generating clusters without operation shuffling. © 2007 ACM.",Compilers for low energy; Loop buffers; VLIW processors,Computational complexity; Constraint theory; Embedded systems; Energy utilization; Program processors; Data clusters; Generating clusters; Homogeneous architecture; Loop buffers; Operation shuffling; VLIW processors; Cluster analysis
Optimization of polynomial datapaths using finite ring algebra,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148853783&doi=10.1145%2f1278349.1278362&partnerID=40&md5=c247339d106490c60c0748b1e065f974,"This article presents an approach to area optimization of arithmetic datapaths at register-transfer level (RTL). The focus is on those designs that perform polynomial computations (add, mult) over finite word-length operands (bit-vectors). We model such polynomial computations over m-bit vectors as algebra over finite integer rings of residue classes Z 2 m. Subsequently, we use the number-theoretic and algebraic properties of such rings to transform a given datapath computation into another, bit-true equivalent computation. We also derive a cost model to estimate, at RTL, the area cost of the computation. Using the transformation procedure along with the cost model, we devise algorithmic procedures to search for a lower-cost implementation. We show how these theoretical concepts can be applied to RTL optimization of arithmetic datapaths within practical CAD settings. Experiments conducted over a variety of benchmarks demonstrate substantial optimizations using our approach. © 2007 ACM.",Arithmetic datapaths; Finite ring algebra; High-level synthesis; Modulo arithmetic; Polynomial datapaths,Cost effectiveness; Digital arithmetic; Mathematical models; Optimization; Polynomials; Vectors; Arithmetic datapaths; Finite ring algebra; M-bit vectors; Polynomial datapaths; Data structures
MPSoC memory optimization using program transformation,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148859002&doi=10.1145%2f1278349.1278356&partnerID=40&md5=516ab89fd0655fc55a0ad27fd900cbe3,"Multiprocessor system-on-a-chip (MPSoC) architectures have received a lot of attention in the past years, but few advances in compilation techniques target these architectures. This is particularly true for the exploitation of data locality. Most of the compilation techniques for parallel architectures discussed in the literature are based on a single loop nest. This article presents new techniques that consist in applying loop fusion and tiling to several loop nests and to parallelize the resulting code across different processors. These two techniques reduce the number of memory accesses. However, they increase dependencies and thereby reduce the exploitable parallelism in the code. This article tries to address this contradiction. To optimize the memory space used by temporary arrays, smaller buffers are used as a replacement. Different strategies are studied to optimize the processing time spent accessing these buffers. The experiments show that these techniques yield a significant reduction in the number of data cache misses (30%) and in processing time (50%). © 2007 ACM.",Compiler transformations; Data cache; Data locality; Embedded systems,Codes (symbols); Computer architecture; Embedded systems; Microprocessor chips; Optimization; Program processors; Compiler transformations; Data cache; Data locality; Temporary arrays; Multiprocessing systems
Temporal floorplanning using the three-dimensional transitive closure subGraph,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148824784&doi=10.1145%2f1278349.1278350&partnerID=40&md5=ced18f5026e38d3a91880141433c0163,"Improving logic capacity by time-sharing, dynamically reconfigurable Field Gate Programmable Arrays (FPGAs) are employed to handle designs of high complexity and functionality. In this paper, we use a novel graph-based topological floorplan representation, named 3D-subTCG (3-Dimensional Transitive Closure subGraph), to deal with the 3-dimensional (temporal) floorplanning/placement problem, arising from dynamically reconfigurable FPGAs. The 3D-subTCG uses three transitive closure graphs to model the temporal and spatial relations between modules. We derive the feasibility conditions for the precedence constraints induced by the execution of the dynamically reconfigurable FPGAs. Because the geometric relationship is transparent to the 3D-subTCG and its induced operations (i.e., we can directly detect the relationship between any two tasks from the representation), we can easily detect any violation of the temporal precedence constraints on 3D-subTCG. We also derive important properties of the 3D-subTCG to reduce the solution space and shorten the running time for 3D (temporal) foorplanning/placement. Experimental results show that our 3D-subTCG-based algorithm is very effective and efficient. © 2007 ACM.",Partially dynamical reconfiguration; Reconfigurable computing; Temporal floorplanning,Array processing; Constraint theory; Gateways (computer networks); Problem solving; Temporal logic; Time sharing systems; Partially dynamical reconfiguration; Reconfigurable computing; Temporal floorplanning; Graph theory
Incremental hierarchical memory size estimation for steering of loop transformations,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148879778&doi=10.1145%2f1278349.1278363&partnerID=40&md5=f38b4fae87cf1637cac9bdcfaafefe59,"Modern embedded multimedia and telecommunications systems need to store and access huge amounts of data. This becomes a critical factor for the overall energy consumption, area, and performance of the systems. Loop transformations are essential to improve the data access locality and regularity in order to optimally design or utilize a memory hierarchy. However, due to abstract high-level cost functions, current loop transformation steering techniques do not take the memory platform sufficiently into account. They usually also result in only one final transformation solution. On the other hand, the loop transformation search space for real-life applications is huge, especially if the memory platform is still not fully fixed. Use of existing loop transformation techniques will therefore typically lead to suboptimal end-products. It is critical to find all interesting loop transformation instances. This can only be achieved by performing an evaluation of the effect of later design stages at the early loop transformation stage. This article presents a fast incremental hierarchical memory-size requirement estimation technique. It estimates the influence of any given sequence of loop transformation instances on the mapping of application data onto a hierarchical memory platform. As the exact memory platform instantiation is often not yet defined at this high-level design stage, a platform-independent estimation is introduced with a Pareto curve output for each loop transformation instance. Comparison among the Pareto curves helps the designer, or a steering tool, to find all interesting loop transformation instances that might later lead to low-power data mapping for any of the many possible memory hierarchy instances. Initially, the source code is used as input for estimation. However, performing the estimation repeatedly from the source code is too slow for large search space exploration. An incremental approach, based on local updating of the previous result, is therefore used to handle sequences of different loop transformations. Experiments show that the initial approach takes a few seconds, which is two orders of magnitude faster than state-of-the-art solutions but still too costly to be performed interactively many times. The incremental approach typically takes just a few milliseconds, which is another two orders of magnitude faster than the initial approach. This huge speedup allows us for the first time to handle real-life industrial-size applications and get realistic feedback during loop transformation exploration. © 2007 ACM.",Code transformation; Data optimization; High-level synthesis; Memory architecture exploration; Memory size estimation,Embedded systems; Mathematical transformations; Multimedia systems; Optimization; Real time systems; Telecommunication systems; Code transformation; Data optimization; High-level synthesis; Memory architecture exploration; Memory size estimation; Data storage equipment
Techniques for the synthesis of reversible Toffoli networks,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148830918&doi=10.1145%2f1278349.1278355&partnerID=40&md5=67d07b985870239a171d2a71f15f13d5,"We present certain new techniques for the synthesis of reversible networks of Toffoli gates, as well as improvements to previous methods. Gate count and technology oriented cost metrics are used. Two new synthesis procedures employing Reed-Muller spectra are introduced and shown to complement earlier synthesis approaches. The previously proposed template simplification method is enhanced through the introduction of a faster and more efficient template application algorithm, an updated classification of the templates, and the addition of new templates of sizes 7 and 9. A resynthesis approach is introduced wherein a sequence of gates is chosen from a network, and the reversible specification it realizes is resynthesized as an independent problem in hopes of reducing the network cost. Empirical results are presented to show that the methods are efficient in terms of the realization of reversible benchmark specifications. © 2007 ACM.",Circuit optimization; Quantum computing; Reversible logic synthesis,Algorithms; Classification (of information); Cost effectiveness; Problem solving; Quantum computers; Cost metrics; Reed-Muller spectra; Reversible logic synthesis; Reversible Toffoli networks; Computer networks
ILP and heuristic techniques for system-level design on network processor architectures,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547310435&doi=10.1145%2f1278349.1278361&partnerID=40&md5=9dfa83b0533ba2cbffd45d8a7b646ba3,"Network processors incorporate several architectural features, including symmetric multiprocessing (SMP), block multithreading, and multiple memory elements, to support the high-performance requirements of current day applications. This article presents automated system-level design techniques for application development on such architectures. We propose integer linear programming formulations and heuristic techniques for process allocation and data mapping on SMP and block-multithreading-based network processors. The techniques incorporate process transformations and multithreading-aware data mapping to maximize the throughput of the application. The article presents experimental results that evaluate the techniques by implementing network processing applications on the Intel IXP 2400 architecture. © 2007 ACM.",Block multithreading; Multiprocessor,Data acquisition; Heuristic methods; Linear programming; Multiprocessing systems; Program processors; Systems analysis; Data mapping; Heuristic techniques; Network processor architectures; Process transformations; Network architecture
Idle energy minimization by mode sequence optimization,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148856844&doi=10.1145%2f1278349.1278351&partnerID=40&md5=02618a56357c36f3824b6565209dcf15,"This article presents techniques for reducing idle energy by mode-sequence optimization (MSO) under timing constraints. Our component-level CoMSO algorithm computes energy-optimal mode-transition sequences for different lengths of idle intervals. Our system-level SyMSO algorithm shifts tasks within slack intervals while satisfying all timing and resource constraints in the given schedule. Experimental results on a commercial software-defined radio show that these new techniques can reduce idle energy by 50 - 70%, or 30 - 50% of total system energy over previous offline-optimal but unsequenced techniques based on localized break-even-time analysis, thanks to rich options offered by mode sequencing. © 2007 ACM.",Communication speed selection; Communication/ computation trade-offs; Embedded multi-processor; Functional partitioning; Low-power design,Algorithms; Computer software; Constraint theory; Optimization; Resource allocation; Systems analysis; Communication speed selection; Communication/ computation trade-offs; Embedded multi-processor; Functional partitioning; Low-power design; Energy conservation
Probabilistic system-on-a-chip architectures,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548359112&doi=10.1145%2f1255456.1255466&partnerID=40&md5=6881cc9dbe810dc3afd2ac7ec89de150,"Parameter variations, noise susceptibility, and increasing energy dissipation of cmos devices have been recognized as major challenges in circuit and microarchitecture design in the nanometer regime. Among these, parameter variations and noise susceptibility are increasingly causing cmos devices to behave in an unreliable or probabilistic manner. To address these challenges, a shift in design paradigm from current-day deterministic designs to statistical or probabilistic designs is deemed inevitable. To respond to this need, in this article, we introduce and study an entirely novel family of probabilistic architectures: the probabilistic system-on-a-chip (psoc). psoc architectures are based on cmos devices rendered probabilistic due to noise, referred to as probabilistic CMOS or PCMOS devices. We demonstrate that in addition to harnessing the probabilistic behavior of pcmos devices, psoc architectures yield significant improvements, both in energy consumed as well as performance in the context of probabilistic or randomized applications with broad utility. All of our application and architectural savings are quantified using the product of the energy and performance, denoted (energy × performance): The pcmos-based gains are as high as a substantial multiplicative factor of over 560 when compared to a competing energy-efficient cmos-based realization. Our architectural design is application specific and involves navigating design space spanning the algorithm (application), its architecture (psoc), and the probabilistic technology (pcmos). © 2007 ACM.",Embedded systems; Probabilistic computing,Algorithms; Embedded systems; Energy dissipation; Spurious signal noise; Microarchitecture; Noise susceptibility; Probabilistic computing; CMOS integrated circuits
Functional verification of task partitioning for multiprocessor embedded systems,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148836841&doi=10.1145%2f1278349.1278357&partnerID=40&md5=911b8a9ad63feecc5bda828f18973649,"With the advent of multiprocessor embedded platforms, application partitioning and mapping have gained primacy as a design step. The output of this design step is a multithreaded partitioned application where each thread is mapped to a processing element (processor or ASIC) in the multiprocessor platform. This partitioned application must be verified to be consistent with the native unpartitioned application. This verification task is called application (or task) partitioning verification. This work proposes a code-block-level containment-checking-based methodology for application partitioning verification. We use a UML-based code-block-level modeling language which is rich enough to model most designs. We formulate the application partitioning verification problem as a special case of the containment checking problem, which we call the complete containment checking problem. We propose a state space reduction technique specific to the containment checking, reachability analysis, and deadlock detection problems. We propose novel data structures and token propagation methodologies which enhance the efficiency of containment checking. We present an efficient containment checking algorithm for the application partitioning verification problem. We develop a containment checking tool called TraceMatch and present experimental results. We present a comparison of the state space reduction achieved by TraceMatch with that achieved by formal analysis and verification tools like Spin, PEP, PROD, and LoLA. © 2007 ACM.",Containment checking; Multiprocessor embedded systems; State space reduction; UML activity diagrams,Computer programming languages; Computer simulation; Functional analysis; Multiprocessing systems; Problem solving; State space methods; Containment checking; Multiprocessor embedded systems; State space reduction; UML activity diagrams; Embedded systems
A practical dynamic single assignment transformation,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148876223&doi=10.1145%2f1278349.1278353&partnerID=40&md5=1baa74d9058a82c4ff0525d66a8519a4,"This paper presents a novel method to construct a dynamic single assignment (DSA) form of array intensive, pointer free C programs. A program in DSA form does not perform any destructive update of scalars and array elements; that is, each element is written at most once. As DSA makes the dependencies between variable references explicit, it facilitates complex analyses and optimizations of programs. Existing transformations into DSA perform a complex data flow analysis with exponential analysis time, and they work only for a limited class of input programs. Our method removes irregularities from the data flow by adding copy assignments to the program, so that it can use simple data flow analyses. The presented DSA transformation scales very well with growing program sizes and overcomes a number of important limitations of existing methods. We have implemented the method and it is being used in the context of memory optimization and verification of those optimizations. Experiments show that in practice, the method scales well indeed, and that added copy operations can be removed in case they are unwanted. © 2007 ACM.",Arrays; Data flow analysis; Parallelization; Reaching definitions; Single assignment,Array processing; Computational complexity; Computer program listings; Data flow analysis; Optimization; Array intensive; Dynamic single assignment transformation; Exponential analysis time; Pointer free C programs; Mathematical transformations
Low-Power and testable circuit synthesis using Shannon decomposition,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148870240&doi=10.1145%2f1278349.1278360&partnerID=40&md5=a5850395b598b818b1c3707a7a66fe0c,"Structural transformation of a design to enhance its testability while satisfying design constraints on power and performance can result in improved test cost and test confidence. In this article, we analyze the testability in a new style of logic design based on Shannon's decomposition and supply gating. We observe that the tree structure of a logic circuit due to Shannon's decomposition makes it intrinsically more testable than a conventionally synthesized circuit, while at the same time providing an improvement in active power. We have analyzed four different aspects of the testability of a circuit: a) IDDQ test sensitivity, b) test power during scan-based testing, c) test length (for both ATPG-generated deterministic and random patterns), and d) noise immunity. Simulation results on a set of MCNC benchmarks show promising results on all these aspects (an average improvement of 94% in IDDQ sensitivity, 50% in test power, 19% (21%) in test length for deterministic (random) patterns, and 50% in coupling noise immunity). We have also demonstrated that the new logic structure can improve parametric yield (6% on average) of a circuit under process variations when considering a bound on circuit leakage. © 2007 ACM.",Design-for-test; Dynamic supply gating; IDDQ; Noise immunity; Shannon expansion; Test coverage; Test power,Computer simulation; Constraint theory; Gateways (computer networks); Logic circuits; Structural analysis; Dynamic supply gating; Noise immunity; Parametric yield; Shannon expansion; Mathematical transformations
Exploring time/resource trade-offs by solving dual scheduling problems with the ant colony optimization,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148892375&doi=10.1145%2f1278349.1278359&partnerID=40&md5=d755be4398c2fd3f24e061e0f0f201c9,"Design space exploration during high-level synthesis is often conducted through ad hoc probing of the solution space using some scheduling algorithm. This is not only time consuming but also very dependent on designer's experience. We propose a novel design exploration method that exploits the duality of time- and resource-constrained scheduling problems. Our exploration automatically constructs a time/area tradeoff curve in a fast, effective manner. It is a general approach and can be combined with any high-quality scheduling algorithm. In our work, we use the max-min ant colony optimization technique to solve both time- and resource-constrained scheduling problems. Our algorithm provides significant solution-quality savings (average 17.3% reduction of resource counts) with similar runtime compared to using force-directed scheduling exhaustively at every time step. It also scales well across a comprehensive benchmark suite constructed with classic and real-life samples. © 2007 ACM.",Ant colony optimization; Design space exploration; Instruction scheduling; Max-min ant system,Ad hoc networks; Optimization; Problem solving; Real time systems; Resource allocation; Ant colony optimization; Design space exploration; Instruction scheduling; Max-min ant system; Scheduling algorithms
Compilation for compact power-gating controls,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148852358&doi=10.1145%2f1278349.1278364&partnerID=40&md5=8ade0e9205e19f8cb8584279d5f3a6fd,"Power leakage constitutes an increasing fraction of the total power consumption in modern semiconductor technologies due to the continuing size reductions and increasing speeds of transistors. Recent studies have attempted to reduce leakage power using integrated architecture and compiler power-gating mechanisms. This approach involves compilers inserting instructions into programs to shut down and wake up components, as appropriate. While early studies showed this approach to be effective, there are concerns about the large amount of power-control instructions being added to programs due to the increasing amount of components equipped with power-gating controls in SoC design platforms. In this article we present a sink-n-hoist framework for a compiler to generate balanced scheduling of power-gating instructions. Our solution attempts to merge several power-gating instructions into a single compound instruction, thereby reducing the amount of power-gating instructions issued. We performed experiments by incorporating our compiler analysis and scheduling policies into SUIF compiler tools and by simulating the energy consumption using Wattch toolkits. The experimental results demonstrate that our mechanisms are effective in reducing the amount of power-gating instructions while further reducing leakage power compared to previous methods. © 2007 ACM.",Balanced scheduling; Compilers for low power; Data-flow analysis; Leakage-power reduction; Power-gating mechanisms,Computer simulation; Data flow analysis; Electric power utilization; Gates (transistor); Semiconductor devices; Size determination; Balanced scheduling; Leakage-power reduction; Power-gating mechanisms; SUIF compiler tools; Power control
Ultra-fast and efficient algorithm for energy optimization by gradient-based stochastic voltage and task scheduling,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148813220&doi=10.1145%2f1278349.1278352&partnerID=40&md5=fe7e6960333446b89516746b4c0fca2e,"This paper presents a new technique, called Adaptive Stochastic Gradient Voltage-and-Task Scheduling (ASG-VTS), for power optimization of multicore hard realtime systems. ASG-VTS combines stochastic and energy-gradient techniques to simultaneously solve the slack distribution and task reordering problem. It produces very efficient results with few mode transitions. Our experiments show that ASG-VTS reduces number of mode transitions by 4.8 times compared to traditional energy-gradient-based approaches. Also, our heuristic algorithm can quickly find a solution that is as good as the optimal for a real-life GSM encoder/decoder benchmark. The runtime of ASG-VTS is 150 times and 1034 times faster than energy-gradient based and optimal ILP algorithms, respectively. Since the runtime of ASG-VTS is very low, it is ideal for design space exploration in system-level design tools. We have also developed a web-based interface for ASG-VTS algorithm. © 2007 ACM.",Power management; Slack distribution; Voltage and task scheduling,Heuristic algorithms; Optimization; Problem solving; Real time systems; Stochastic control systems; Voltage measurement; Power management; Slack distribution; System-level design tools; Voltage and task scheduling; Energy conservation
Clock skew scheduling with race conditions considered,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148896481&doi=10.1145%2f1278349.1278358&partnerID=40&md5=4640b21182f571ddd056fe77a15f3b60,"In this article, we provide a fresh viewpoint to the interactions between clock skew scheduling and delay insertion. A race-condition-aware (RCA) clock skew scheduling is proposed to determine the clock skew schedule by taking race conditions (i.e., hold violations) into account. Our objective is not only to optimize the clock period, but also to minimize heuristically the required inserted delay. Compared with previous work, our major contribution includes the following two aspects. First, our approach achieves exactly the same results, but has significant improvement in time complexity. Second, our viewpoint can be generalized to other sequential timing optimization techniques. © 2007 ACM.",Logic synthesis; Performance optimization; Sequential circuits; Timing optimization,Optimization; Scheduling; Sequential circuits; Logic synthesis; Performance optimization; Race-condition-aware (RCA) clock skew scheduling; Timing optimization; Hazards and race conditions
Postplacement rewiring by exhaustive search for functional symmetries,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548266122&doi=10.1145%2f1255456.1255469&partnerID=40&md5=263bc2046253fff48a7592a8dc5b8356,"We propose two new algorithms for rewiring: a postplacement optimization that reconnects pins of a given netlist without changing the logic function and gate locations. In the first algorithm, we extract small subcircuits consisting of several gates from the design and reconnect pins according to the symmetries of the subcircuits. To enhance the power of symmetry detection, we also propose a graph-based symmetry detector that can identify permutational and phase-shift symmetries on multiple input and output wires, as well as hybrid symmetries, creating abundant opportunities for rewiring. Our second algorithm, called long-range rewiring, is based on reconnecting equivalent pins and can augment the first approach for further optimization. We apply our techniques for wirelength optimization and observe that they provide wirelength reduction comparable to that achieved by detailed placement. © 2007 ACM.",Placement; Rewiring; VLSI,Electric wiring; Formal logic; Optimization; Phase shift; VLSI circuits; Equivalent pins; Phase-shift symmetries; Rewiring; Wirelength optimization; Logic gates
Efficient power modeling and software thermal sensing for runtime temperature monitoring,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548225939&doi=10.1145%2f1255456.1255462&partnerID=40&md5=e71da78073e6c115224708dc28aaa30b,"The evolution of microprocessors has been hindered by increasing power consumption and heat dissipation on die. An excessive amount of heat creates reliability problems, reduces the lifetime of a processor, and elevates the cost of cooling and packaging considerably. It is therefore imperative to be able to monitor the temperature variations across the die in a timely and accurate manner. Most current techniques rely on on-chip thermal sensors to report the temperature of the processor. Unfortunately, significant variation in chip temperature both spatially and temporally exposes the limitation of the sensors. We present a compensating approach to tracking chip temperature through an OS resident software module that generates live power and thermal profiles of the processor. We developed such a software thermal sensor (STS) in a Linux system with a Pentium 4 Northwood core. We employed highly efficient numerical methods in our model to minimize the overhead of temperature calculation. We also developed an efficient algorithm for functional unit power modeling. Our power and thermal models are calibrated and validated against on-chip sensor readings, thermal images of the Northwood heat spreader, and the thermometer measurements on the package. The resulting STS offers detailed power and temperature breakdowns of each functional unit at runtime, enabling more efficient online power and thermal monitoring and management at a higher level, such as the operating system. © 2007 ACM.",Power; Thermal,Algorithms; Microprocessor chips; Numerical methods; Reliability analysis; Temperature sensors; Online power; Software thermal sensors (STS); Thermal monitoring; Computer software
Introduction to special issue on demonstrable software systems and hardware platforms,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548216765&doi=10.1145%2f1255456.1255457&partnerID=40&md5=f7a0cc11232306a570e17d85ec0b381c,[No abstract available],,
Circuit-simulated obstacle-aware Steiner routing,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548250445&doi=10.1145%2f1255456.1255465&partnerID=40&md5=9939ac7f0a7292cdcd2e660db40b3ed8,"This article develops circuit-simulated routing algorithms. We model the routing graph by an RC network with terminals as inputs, and show that the faster an output reaches its peak, the higher the possibility for the corresponding Hanan or escape node to become a Steiner point. This enables us to select Steiner points and then apply any minimum spanning tree algorithm to obtain obstacle-free or obstacle-aware Steiner routing. Compared with existing algorithms, our algorithms have significant gain on either wirelength or runtime for obstacle-free routing, and on both wirelength and runtime for obstacle-aware routing. © 2007 ACM.",OARSMT; Routing; RSMT; Simulation,Computer simulation; Graph theory; Circuit simulated routing algorithms; Escape node; OARSMT; RSMT; Routing algorithms
A verification system for transient response of analog circuits,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548271444&doi=10.1145%2f1255456.1255468&partnerID=40&md5=8f2b95d8a350217595c1fcca3f40e8fd,"We present a method for application of formal techniques like model checking and equivalence checking for validation of the transient response of nonlinear analog circuits. We propose a temporal logic called Ana CTL (computational tree logic for analog circuit verification) which is suitable for specifying properties specific to analog circuits. The application of Ana CTL for validation of transient behavior of arbitrarily nonlinear analog circuits is presented. The transient response of a circuit under all possible input waveforms is represented as a finite state machine (FSM), by bounding and discretizing the continuous state space of an analog circuit. We have developed algorithms to run Ana CTL queries on this discretized model using search-based methods which reduce the runtime considerably by avoiding creation of the whole FSM. The application of these methods on several real-life analog circuits is presented and we show that this system is a useful aid for detecting and debugging early design errors. We also present methods for checking the equivalence of transient response of two analog circuits. The behavior of two different analog circuits can rarely be exactly similar. Hence, we introduce a notion of approximate equivalence. A query language for checking different notions of user-definable approximate equivalence is presented which extends the syntax of the Ana CTL model checking language. In its extended form, Ana CTL can be used combining model checking with equivalence checking. © 2007 ACM.",Ana CTL; Analog circuits; Equivalence checking; Model checking; Query language; Transient response,Finite automata; Model checking; Query languages; Transient analysis; Trees (mathematics); Computational tree logics; Equivalence checking; Analog circuits
Speedups in embedded systems with a high-performance coprocessor datapath,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548234227&doi=10.1145%2f1255456.1255472&partnerID=40&md5=050187e3423b4bbfc929f50fc1d553e0,"This article presents the speedups achieved in a generic single-chip microprocessor system by employing a high-performance datapath. The datapath acts as a coprocessor that accelerates computational-intensive kernel sections thereby increasing the overall performance. We have previously introduced the datapath which is composed of Flexible Computational Components (FCCs). These components can realize any two-level template of primitive operations. The automated coprocessor synthesis method from high-level software description and its integration to a design flow for executing applications on the system is presented. For evaluating the effectiveness of our coprocessor approach, analytical study in respect to the type of the custom datapath and to the microprocessor architecture is performed. The overall application speedups of several real-life applications relative to the software execution on the microprocessor are estimated using the design flow. These speedups range from 1.75 to 5.84, with an average value of 3.04, while the overhead in circuit area is small. The design flow achieved the acceleration of the applications near to theoretical speedup bounds. A comparison with another high-performance datapath showed that the proposed coprocessor achieves smaller area-time products by an average of 23% for the generated datapaths. Additionally, the FCC coprocessor achieves better performance in accelerating kernels relative to software-programmable DSP cores. © 2007 ACM.",Chaining; Coprocessor datapath; Design flow; Kernels; Performance improvements; Synthesis,Computational methods; Data communication systems; Microprocessor chips; Program processors; Supervisory and executive programs; Chaining; Coprocessor datapath; Design flow; Kernels; Performance improvements; Embedded systems
Efficient simulation of critical synchronous dataflow graphs,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548281098&doi=10.1145%2f1255456.1255458&partnerID=40&md5=41ecb0a2e9e0c69a6974ea064b68dca2,"System-level modeling, simulation, and synthesis using electronic design automation (EDA) tools are key steps in the design process for communication and signal processing systems, and the synchronous dataflow (SDF) model of computation is widely used in EDA tools for these purposes. Behavioral representations of modern wireless communication systems typically result in critical SDF graphs: These consist of hundreds of components (or more) and involve complex intercomponent connections with highly multirate relationships (i.e., with large variations in average rates of data transfer or component execution across different subsystems). Simulating such systems using conventional SDF scheduling techniques generally leads to unacceptable simulation time and memory requirements on modern workstations and high-end PCs. In this article, we present a novel simulation-oriented scheduler (SOS) that strategically integrates several techniques for graph decomposition and SDF scheduling to provide effective, joint minimization of time and memory requirements for simulating critical SDF graphs. We have implemented SOS in the advanced design system (ADS) from Agilent Technologies. Our results from this implementation demonstrate large improvements in simulating real-world, large-scale, and highly multirate wireless communication systems (e.g., 3GPP, Bluetooth, 802.16e, CDMA 2000, XM radio, EDGE, and Digital TV). © 2007 ACM.",Scheduling; Simulation; Synchronous dataflow,Computation theory; Computer simulation; Data flow analysis; Personal computers; Software design; Electronic design automation (EDA); Graph decomposition; Synchronous dataflow; Graph theory
"On-chip communication architecture exploration: A quantitative evaluation of point-to-point, bus, and network-on-chip approaches",2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548254878&doi=10.1145%2f1255456.1255460&partnerID=40&md5=3975432bdcf46d28817bb4e5a5abd818,"Traditionally, design-space exploration for systems-on-chip (SoCs) has focused on the computational aspects of the problem at hand. However, as the number of components on a single chip and their performance continue to increase, a shift from computation-based to communication-based design becomes mandatory. As a result, the communication architecture plays a major role in the area, performance, and energy consumption of the overall system. This article presents a comprehensive evaluation of three on-chip communication architectures targeting multimedia applications. Specifically, we compare and contrast the network-on-chip (NoC) with point-to-point (P2P) and bus-based communication architectures in terms of area, performance, and energy consumption. As the main contribution, we present complete P2P, bus-, and NoC-based implementations of a real multimedia application (i. e. the MPEG-2 encoder), and provide direct measurements using an FPGA prototype and actual video clips, rather than simulation and synthetic workloads. We also support the experimental findings through a theoretical analysis. Both experimental and analysis results show that the NoC architecture scales very well in terms of area, performance, energy, and design effort, while the P2P and bus-based architectures scale poorly on all accounts except for performance and area, respectively. © 2007 ACM.",FPGA prototype; MPEG-2 encoder; Networks-on-chip; Point-to-point; System-on-chip,Computation theory; Field programmable gate arrays (FPGA); Microprocessor chips; Motion Picture Experts Group standards; Multimedia systems; FPGA prototypes; MPEG-2 encoders; Networks-on-chips; System-on-chips; Computer architecture
Event propagation for accurate circuit delay calculation using SAT,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548235393&doi=10.1145%2f1255456.1255473&partnerID=40&md5=b30f553462411a7aacdc60dea345e37d,"A SAT-based modeling for event propagation in gate-level digital circuits, which is used for accurate calculation of critical delay in combinational and sequential circuits, is presented in this article. The accuracy of the critical delay estimation process depends on the accuracy with which the circuit in operation is modeled. A high level of precision in the modeling of the internal events in a circuit for the sake of greater accuracy causes a combinatorial blowup in the size of the problem, resulting in a scalability bottleneck for which most existing techniques effect a trade-off by restricting themselves to less precise models. SAT based techniques have a good track record in efficiency and scalability when the problem sizes become too large for most other methods. This article proposes a SAT-based technique for symbolic event propagation within a circuit which facilitates the estimation of the critical delay of circuits with a greater degree of accuracy, while at the same time scaling efficiently to large circuits. We report very encouraging results on the ISCAS85 and ISCAS89 benchmark circuits using the proposed technique. © 2007 ACM.",Critical delay; Event propagation; SAT,Computer simulation; Scalability; Sequential circuits; Time delay; Critical delay; Event propagation; Circuit theory
Efficient and scalable compiler-directed energy optimization for realtime applications,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548211377&doi=10.1145%2f1255456.1255464&partnerID=40&md5=fd2584de40af9966274a99c07adc376f,"With continuing shrinkage of technology feature sizes, the share of leakage in total energy consumption of digital systems continues to grow. Coordinated supply voltage and body bias throttling enables the compiler to better optimize the total energy consumption of the system in future technology nodes. We present a compilation technique that targets realtime applications running on embedded processors with combined dynamic voltage scaling (DVS) and adaptive body biasing (ABB) capabilities. Considering the delay and energy penalty of switching between operating modes of the processor, our compiler judiciously inserts mode-switch instructions in selected locations of the code and generates executable binary that is guaranteed to meet the deadline constraint. More importantly, our algorithm runs very fast and comes reasonably close to the theoretical limit of energy optimization using DVS+ABB. At 65nm technology, we improve the energy dissipation of the generated code by an average of 33.20% under deadline constraints. While our technique's improvement in energy dissipation over conventional DVS is marginal (6.91%) at 130nm, the average improvement continues to grow to 13.19%, 22.97%, and 33.21% for 90nm, 65nm, and 45nm technology nodes, respectively. Compared to a recent ILP-based competitor, we improve the runtime by more than three orders of magnitude, while producing improved results. © 2007 ACM.",Energy-aware compiler; Leakage; Technology scaling,Adaptive systems; Digital control systems; Leakage currents; Program compilers; Real time systems; Voltage control; Adaptive body biasing (ABB); Dynamic voltage scaling (DVS); Technology scaling; Energy utilization
HW-SW emulation framework for temperature-aware design in MPSoCs,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548264659&doi=10.1145%2f1255456.1255463&partnerID=40&md5=711a27f393a28f9a8a157a4acd7a30ee,"New tendencies envisage multiprocessor systems-on-chips (MPSoCs) as a promising solution for the consumer electronics market. MPSoCs are complex to design, as they must execute multiple applications (games, video) while meeting additional design constraints (energy consumption, time-to-market). Moreover, the rise of temperature in the die for MPSoCs can seriously affect their final performance and reliability. In this article, we present a new hardware-software emulation framework that allows designers a complete exploration of the thermal behavior of final MPSoC designs early in the design flow. The proposed framework uses FPGA emulation as the key element to model hardware components of the considered MPSoC platform at multimegahertz speeds. It automatically extracts detailed system statistics that are used as input to our software thermal library running in a host computer. This library calculates at runtime the temperature of on-chip components, based on the collected statistics from the emulated system and final floorplan of the MPSoC. This enables fast testing of various thermal management techniques. Our results show speedups of three orders of magnitude compared to cycle-accurate MPSoC simulators. © 2007 ACM.",Emulation; FPGA; MPSoC; Temperature; Thermal-aware design,Computer software; Field programmable gate arrays (FPGA); Microprocessor chips; Reliability analysis; Temperature control; Emulation; Multimegahertz speeds; System statistics; Thermal-aware design; Multiprocessing systems
PeaCE: A hardware-software codesign environment for multimedia embedded systems,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548264660&doi=10.1145%2f1255456.1255461&partnerID=40&md5=b6f2f6b3ecebbf91e79e90c2cf6870f9,"Existent hardware-software (HW-SW) codesign tools mainly focus on HW-SW cosimulation to build a virtual prototyping environment that enables software design and system verification without need of making a hardware prototype. Not only HW-SW cosimulation, but also HW-SW codesign methodology involves system specification, functional simulation, design-space exploration, and hardware-software cosynthesis. The PeaCE codesign environment is the first full-fledged HW-SW codesign environment that provides seamless codesign flow from functional simulation to system synthesis. Targeting for multimedia applications with real-time constraints, PeaCE specifies the system behavior with a heterogeneous composition of three models of computation and utilizes features of the formal models maximally during the whole design process. It is also a reconfigurable framework in the sense that third-party design tools can be integrated to build a customized tool chain. Experiments with industry-strength examples prove the viability of the proposed technique. © 2007 ACM.",Design-space exploration; Embedded systems; Hardware-software codesign; Hardware-software cosimulation; Model-based design,Multimedia systems; Rapid prototyping; Software design; Virtual reality; Design-space exploration; Functional simulation; Hardware-software cosimulation; Hardware-software cosynthesis; Embedded systems
EWD: A metamodeling driven customizable multi-MoC system modeling framework,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548221419&doi=10.1145%2f1255456.1255470&partnerID=40&md5=822969d822741d6cdbd66bbaecba3978,"We present the EWD design environment and methodology, a modeling and simulation framework suited for complex and heterogeneous embedded systems with varying degrees of expressibility and modeling fidelity. This environment promotes the use of multiple models of computation (MoCs) to support heterogeneity and metamodeling for conformance tests of syntactic and static semantics during the process of modeling. Therefore, EWD is a multiple MoC modeling and simulation framework that ensures conformance of the MoC formalisms during model construction using a metamodeling approach. In addition, EWD provides a suite of translation tools that generate executable models for two simulation frameworks to demonstrate its language-independent modeling framework. The EWD methodology uses the Generic Modeling Environment for customization of the MoC-specific modeling syntax into a visual representation. To embed the execution semantics of the MoCs into the models, we have built parsing and translation tools that leverage an XML-based interoperability language. This interoperability language is then translated into executable Standard ML or Haskell models that can also be analyzed by existing simulation frameworks such as SML-Sys or ForSyDe. In summary, EWD is a metamodeling driven multitarget design environment with multi-MoC modeling capability. © 2007 ACM.",Denotational semantics; ForSyDe; Functional language; Heterogeneous system design; Interoperable modeling language; Metamodel; Metamodeling; MoC; Ptolemy II; SystemC,Computer programming languages; Computer simulation; Data structures; Semantics; XML; Generic Modeling Environment; Models of computation (MoC); Simulation framework; Computer systems
Binary synthesis,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548204342&doi=10.1145%2f1255456.1255471&partnerID=40&md5=4b1a8c2b0148ec00f62afa7e3b5fd3b9,"Recent high-level synthesis approaches and C-based hardware description languages attempt to improve the hardware design process by allowing developers to capture desired hardware functionality in a well-known high-level source language. However, these approaches have yet to achieve wide commercial success due in part to the difficulty of incorporating such approaches into software tool flows. The requirement of using a specific language, compiler, or development environment may cause many software developers to resist such approaches due to the difficulty and possible instability of changing well-established robust tool flows. Thus, in the past several years, synthesis from binaries has been introduced, both in research and in commercial tools, as a means of better integrating with tool flows by supporting all high-level languages and software compilers. Binary synthesis can be more easily integrated into a software development tool-flow by only requiring an additional backend tool, and it even enables completely transparent dynamic translation of executing binaries to configurable hardware circuits. In this article, we survey the key technologies underlying the important emerging field of binary synthesis. We compare binary synthesis to several related areas of research, and we then describe the key technologies required for effective binary synthesis: decompilation techniques necessary for binary synthesis to achieve results competitive with source-level synthesis, hardware/software partitioning methods necessary to find critical binary regions suitable for synthesis, synthesis methods for converting regions to custom circuits, and binary update methods that enable replacement of critical binary regions by circuits. © 2007 ACM.",Binary synthesis; Configurable logic; FPGA; Hardware/software codesign; Hardware/software partitioning; Synthesis from software binaries; Warp processors,Computer aided software engineering; Computer hardware description languages; Field programmable gate arrays (FPGA); High level languages; Program compilers; Binary synthesis; Configurable logic; Hardware/software partitioning; Software binaries; Warp processors; Binary codes
A functionality-directed clustering technique for low-power MTCMOS design - Computation of simultaneously discharging current,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548252375&doi=10.1145%2f1255456.1255467&partnerID=40&md5=ef487edd325c94a2284a855da5fc115d,"Multithreshold CMOS (MTCMOS) is a circuit style that can effectively reduce leakage power consumption. Sleep transistor sizing is the key issue when a MTCMOS circuit is designed. If the size of sleep transistor is large enough, the circuit performance can surely be maintained but the area and dynamic power consumption of the sleep transistor may increase. On the other hand, if the sleep transistor size is too small, there will be significant performance degradation because of the increased resistance to ground. Previous approaches [Kao et al. 1998; Anis et al. 2002] to designing sleep transistor size are based mainly on mutually-exclusive discharge patterns. However, these approaches considered only the topology of a circuit (i.e., interconnections of nodes in the circuit-graph saving the functionality of node). We observed that any two possible simultaneously switching gates may not discharge at the same time in terms of functionality. Thus, we propose an algorithm to determine how to cluster cells to share sleep transistors, while taking both topology and functionality into consideration. Moreover, one placement refinement algorithm that takes clustering information into account will be presented. At the logic level, the results show that the proposed clustering method can achieve an average of 22% reduction in terms of the number of unit-size sleep transistors as compared to a method that does not consider functionality. At the physical level, two placement results are discussed. The first is produced by a traditional placement tool plus topology check (functionality check) for insertion of sleep transistors. It shows that the functionality check algorithm produces 9% less chip area as compared with the topology check algorithm. The second result is produced by a placement refinement algorithm where the initial placement is done in the first placement experiment. It shows that the placement refinement algorithm achieves 5% more reduction in area at the expense of 4% increase in wire length. Totally, around 14% reduction is achieved by utilizing the clustering information. © 2007 ACM.",DSTN; Low power; MTCMOS; Sleep transistor,Electric currents; Electric discharges; Energy utilization; Logic design; Functionality check algorithms; Refinement algorithm; Sleep transistors; CMOS integrated circuits
A framework for heterogeneous specification and design of electronic embedded systems in SystemC,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548258236&doi=10.1145%2f1255456.1255459&partnerID=40&md5=f6a3f910db49aafdf355edc6013f910e,"This work proposes a methodology which enables heterogeneous specification of complex, electronic systems in SystemC supporting the integration of components under different models of computation (MoCs). This feature is necessary in order to deal with the growing complexity, concurrency, and heterogeneity of electronic embedded systems. The specification methodology is based on the SystemC standard language. Nevertheless, the use of SystemC for heterogeneous system specification is not straightforward. The first problem to be addressed is the efficient and predictable mapping of untimed events required by abstract MoCs over the discrete-event MoC on which the SystemC simulation kernel is based. This mapping is essential in order to understand the simulation results provided by the SystemC model of those MoCs. The specification methodology proposes the set of rules and guidelines required by each specific MoC. Moreover, the methodology supports a smooth integration of several MoCs in the same system specification. A set of facilities is provided covering the deficiencies of the language. These facilities constitute the methodology-specific library called HetSC. The methodology and associated library have been demonstrated to be useful for the specification of complex, heterogeneous embedded systems supporting essential design tasks such as performance analysis and SW generation. © 2007 ACM.",Heterogeneous specification; Models of computation; SystemC,Computational complexity; Computer programming languages; Conformal mapping; Problem solving; Software design; Electronic embedded systems; Heterogeneous specification; Models of computation; SystemC standard languages; Embedded systems
Forming N-detection test sets without test generation,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248395554&doi=10.1145%2f1230800.1230810&partnerID=40&md5=5c7f7764951f2e9e936925c750fdb436,"We describe a procedure for forming n-detection test sets for n>1 without applying a test generation procedure to target faults. The proposed procedure accepts a one-detection test set. It extracts test cubes for target faults from the one-detection test set, and merges the test cubes to obtain new test vectors. By extracting and merging different test cubes in different iterations of this process, an n-detection test set is obtained. Merging of test cubes does not require test generation or fault simulation. Fault simulation is required for extracting test cubes for target faults. We demonstrate that the resulting test set is as effective in detecting untargeted faults as an n-detection test set generated by a deterministic test generation procedure. We also discuss the application of the proposed procedure starting from a random test set (instead of a one-detection test set). © 2007 ACM.",Bridging faults; n-detection test sets; Stuck-at faults; Test generation,Computer simulation; Fault detection; Random processes; Set theory; Vectors; Bridging faults; N-detection test sets; Test cubes; Test generation; Electronic equipment testing
A model-based extensible framework for efficient application design using FPGA,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248325666&doi=10.1145%2f1230800.1230805&partnerID=40&md5=29452e623a8955776eb5c4888cb15155,"For an FPGA designer, several choices are available in terms of target FPGA devices, IP-cores, algorithms, synthesis options, runtime reconfiguration, degrees of parallelism, among others, while implementing a design. Evaluation of design alternatives in the early stages of the design cycle is important because the choices made can have a critical impact on the performance of the final design. However, a large number of alternatives not only results in a large number of designs, but also makes it a hard problem to efficiently manage, simulate, and evaluate them. In this article, we present a framework for FPGA-based application design that addresses the aforementioned issues. This framework supports a hierarchical modeling approach that integrates application and device modeling techniques and allows development of a library of models for design reuse. The framework integrates a high-level performance estimator for rapid estimation of the latency, area, and energy of the designs. In addition, a design space exploration tool allows efficient evaluation of candidate designs against the given performance requirements. The framework also supports extension through integration of widely used tools for FPGA-based design while presenting a unified environment for different target FPGAs. We demonstrate our framework through the modeling and performance estimation of a signal processing kernel and the design of end-to-end applications. © 2007 ACM.",Design tool; Extensible; Modeling; Reuse,Computer aided software engineering; Computer simulation; Field programmable gate arrays (FPGA); Intellectual property; Mathematical models; Signal processing; Design tools; End-to-end applications; Signal processing kernels; Computer aided design
The exact channel density and compound design for generic universal switch blocks,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248327916&doi=10.1145%2f1230800.1230811&partnerID=40&md5=3283ea5e8f09111d5eaeb5932397074b,"A switch block of k sides W terminals on each side is said to be universal (a (k, W)-USB) if it is routable for every set of 2-pin nets of channel density at most W. The generic optimum universal switch block design problem is to design a (k, W)-USB with the minimum number of switches for every pair of (k, W). This problem was first proposed and solved for k = 4 in Chang et al. [1996], and then solved for even W or for k ≤ 6 in Shyu et al. [2000] and Fan et al. [2002b]. No optimum (k, W)-USB is known for k ≥ 7 and odd W ≥ 3. But it is already known that when W is a large odd number, a near-optimum (k, W)-USB can be obtained by a disjoint union of (W - f2(k))/2 copies of the optimum (k, 2)-USB and a noncompound (k, f2(k))-USB, where the value of f2(k) is unknown for k ≥ 8. In this article, we show that f2(k) = k+3-i/3, where 1 ≤ i ≤ 6 and i ≡ k (mod 6), and present an explicit design for the noncompound (k, f2(k))-USB. Combining these two results we obtain the exact designs of (k, W)-USBs for all k ≥ 7 and odd W ≥ 3. The new (k, W)-USB designs also yield an efficient detailed routing algorithm. © 2007 ACM.",FPGA architecture; Routing algorithm; Universal switch block,Carrier concentration; Computer aided design; Electronic equipment manufacture; Field programmable gate arrays (FPGA); Number theory; Problem solving; Routing algorithms; FPGA architecture; Universal switch block; Switching systems
Prediction of leakage power under process uncertainties,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248335446&doi=10.1145%2f1230800.1230804&partnerID=40&md5=affe2cb3b96e9485c2114a77fc555d1a,"In this article, we present a method to analyze the total leakage current of a circuit under process variations, considering interdie and intradie variations as well as the effect of the spatial correlations of intradie variations. The approach considers both the subthreshold and gate tunneling leakage power, as well as their interactions. With process variations, each leakage component is approximated by a lognormal distribution, and the total chip leakage is computed as a sum of the correlated lognormals. Since the lognormals to be summed are large in number and have complicated correlation structures due to both spatial correlations and the correlation among different leakage mechanisms, we propose an efficient method to reduce the number of correlated lognormals for summation to a manageable quantity. We do so by identifying dominant states of leakage currents and taking advantage of the spatial correlation model and input states at the gates. An improved approach utilizing the principal components computed from spatially correlated process parameters is also proposed to further improve runtime efficiency. We show that the proposed methods are effective in predicting the probability distribution of total chip leakage, and that ignoring spatial correlations can underestimate the standard deviation of full-chip leakage power. © 2007 ACM.",Circuit; Leakage; Process variation; Yield,Correlation methods; Electron tunneling; Leakage currents; Mathematical models; Probability distributions; Uncertain systems; Correlated lognormals; Full-chip leakage; Process variation; Standard deviation; Electric power measurement
DRDU: A data reuse analysis technique for efficient scratch-pad memory management,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248334635&doi=10.1145%2f1230800.1230807&partnerID=40&md5=588552d082acf032b1958a7d7969c3e1,"In multimedia and other streaming applications, a significant portion of energy is spent on data transfers. Exploiting data reuse opportunities in the application, we can reduce this energy by making copies of frequently used data in a small local memory and replacing speed- and power-inefficient transfers from main off-chip memory by more efficient local data transfers. In this article we present an automated approach for analyzing these opportunities in a program that allows modification of the program to use custom scratch-pad memory configurations comprising a hierarchical set of buffers for local storage of frequently reused data. Using our approach we are able to both reduce energy consumption of the memory subsystem when using a scratch-pad memory by about a factor of two, on average, and improve memory system performance compared to a cache of the same size. © 2007 ACM.",Compiler analysis; Data reuse analysis; Memory hierarchy; Scratch-pad memory management,Data reduction; Data transfer; Energy utilization; Multimedia systems; Power transmission; Compiler analysis; Data reuse analysis; Memory hierarchy; Power-inefficient transfers; Scratch-pad memory management; Storage allocation (computer)
Transition-overhead-aware voltage scheduling for fixed-priority real-time systems,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248328748&doi=10.1145%2f1230800.1230803&partnerID=40&md5=13c80456a143db0a1b0ae8b0843c8d43,"Time transition overhead is a critical problem for hard real-time systems that employ dynamic voltage scaling (DVS) for power and energy management. While it is a common practice of much previous work to ignore transition overhead, these algorithms cannot guarantee deadlines and/or are less effective in saving energy when transition overhead is significant and not appropriately dealt with. In this article we introduce two techniques, one offline and one online, to correctly account for transition overhead in preemptive fixed-priority real-time systems. We present several DVS scheduling algorithms that implement these methods that can guarantee task deadlines under arbitrarily large transition time overheads and reduce energy consumption by as much as 40% when compared to previous methods. © 2007 ACM.",Dynamic voltage scaling; Fixed priority; Low power; Scheduling; Transition overhead,Energy conservation; Energy management; Online systems; Scheduling algorithms; Voltage control; Dynamic voltage scaling; Fixed priority; Transition overhead; Real time systems
Disjunctive image computation for software verification,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248368514&doi=10.1145%2f1230800.1230802&partnerID=40&md5=61ced69993d035bc451063edbe73f5f8,"Existing BDD-based symbolic algorithms designed for hardware designs do not perform well on software programs. We propose novel techniques based on unique characteristics of software programs. Our algorithm divides an image computation step into a disjunctive set of easier ones that can be performed in isolation. We use hypergraph partitioning to minimize the number of live variables in each disjunctive component, and variable scopes to simplify transition relations and reachable state subsets. Our experiments on nontrivial C programs show that BDD-based symbolic algorithms can directly handle software models with a much larger number of state variables than for hardware designs. © 2007 ACM.",Binary decision diagram; Formal verification; Image computation; Model checking; Reachability analysis,Algorithms; Codes (symbols); Graph theory; Model checking; Set theory; Software engineering; Binary decision diagram; Formal verification; Reachability analysis; Software models; Image processing
A critical-path-aware partial gating approach for test power reduction,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248368083&doi=10.1145%2f1230800.1230809&partnerID=40&md5=f62710a887c37d07c4bd545e969bfca1,"Power reduction during test application is important from the viewpoint of chip reliability and for obtaining correct test results. One of the ways to reduce scan test power is to block transitions propagating from the outputs of scan cells through combinational logic. In order to accomplish this, some researchers have proposed setting primary inputs to appropriate values or adding extra gates at the outputs of scan cells. In this article, we point out the limitations of such full gating techniques in terms of area overhead and performance degradation. We propose an alternate solution where a partial set of scan cells is gated. A subset of scan cells is selected to give maximum reduction in test power within a given area constraint. An alternate formulation of the problem is to treat maximum permitted test power as a constraint and achieve a test power that is within this limit using the fewest number of gated scan cells, thereby leading to the least impact in area overhead. Our problem formulation also comprehends performance constraints and prevents the inclusion of gating points on critical paths. The area overhead is predictable and closely corresponds to the average power reduction. © 2007 ACM.",Low-power testing; Partial gating; Scan cell gating; Scan testing,Combinatorial circuits; Constrained optimization; Energy conservation; Logic gates; Power control; Set theory; Low-power testing; Partial gating; Scan cell gating; Scan testing; Electronic equipment testing
Low test application time resource binding for behavioral synthesis,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248360345&doi=10.1145%2f1230800.1230808&partnerID=40&md5=06a72466f61b97c036981b4de4f4fb95,Recent advances in process technology have led to a rapid increase in the density of integrated circuits (ICs). Increased density and the need to test for new types of defects in nanometer technologies have resulted in a tremendous increase in test application time (TAT). This article presents a test synthesis method to reduce test application time for testing the datapath of a design. The test application time is reduced by applying a test-time-aware resource sharing algorithm on a scheduled control data flow graph (CDFG) of a design. © 2007 ACM.,CDFG; High-level synthesis; Test synthesis; Testability,Algorithms; Data flow analysis; Integrated circuit layout; Nanotechnology; Resource allocation; Three dimensional computer graphics; Time sharing systems; Application time; Control data flow graph (CDFG); High-level synthesis; Test synthesis; Electronic equipment testing
A predictive decode filter cache for reducing power consumption in embedded processors,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248356359&doi=10.1145%2f1230800.1230806&partnerID=40&md5=1d1f6a324c9cb82bd0ca35a897789432,"With advances in semiconductor technology, power management has increasingly become a very important design constraint in processor design. In embedded processors, instruction fetch and decode consume more than 40% of processor power. This calls for development of power minimization techniques for the fetch and decode stages of the processor pipeline. For this, filter cache has been proposed as an architectural extension for reducing the power consumption. A filter cache is placed between the CPU and the instruction cache (I-cache) to provide the instruction stream. A filter cache has the advantages of shorter access time and lower power consumption. However, the downside of a filter cache is a possible performance loss in case of cache misses. In this article, we present a novel technique - decode filter cache (DFC) - -for minimizing power consumption with minimal performance impact. The DFC stores decoded instructions. Thus, a hit in the DFC eliminates instruction fetch and its subsequent decoding. The bypassing of both instruction fetch and decode reduces processor power. We present a runtime approach for predicting whether the next fetch source is present in the DFC. In case a miss is predicted, we reduce the miss penalty by accessing the I-cache directly. We propose to classify instructions as cacheable or noncacheable, depending on the decode width. For efficient use of the cache space, a sectored cache design is used for the DFC so that both cacheable and noncacheable instructions can coexist in the DFC sector. Experimental results show that the DFC reduces processor power by 34% on an average and our next fetch prediction mechanism reduces miss penalty by more than 91%. © 2007 ACM.",Cache; Embedded processors; Power optimization,Constrained optimization; Digital filters; Electric power utilization; Embedded systems; Predictive control systems; Program processors; Decode filter cache (DFC); Embedded processors; Power optimization; Buffer storage
ACM Transactios on Design Automation of Electronic Systems: Editorial,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248324832&doi=10.1145%2f1230800.1230801&partnerID=40&md5=514383ef4759ff2449cb148268d4f9b7,[No abstract available],,
Impact of Intercluster Communication Mechanisms on ILP in Clustered VLIW Architectures,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024287060&doi=10.1145%2f1188275.1188276&partnerID=40&md5=3642d31ce3d3a9f1bb8a5918b923d0c3,"VLIW processors have started gaining acceptance in the embedded systems domain. However, monolithic register file VLIW processors with a large number of functional units are not viable. This is because of the need for a large number of ports to support FU requirements, which makes them expensive and extremely slow. A simple solution is to break the register file into a number of smaller register files with a subset of FUs connected to it. These architectures are termed clustered VLIW processors. In this article, we first build a case for clustered VLIW processors with four or more clusters by showing that the achievable ILP in most of the media applications for a 16 ALU and 8 LD/ST VLIW processor is around 20. We then provide a classification of the intercluster interconnection design space, and show that a large part of this design space is currently unexplored. Next, using our performance evaluation methodology, we evaluate a subset of this design space and show that the most commonly used type of interconnection, RF-to-RF, fails to meet achievable performance by a large factor, while certain other types of interconnections can lower this gap considerably. We also establish that this behavior is heavily application dependent, emphasizing the importance of application-specific architecture exploration. We also present results about the statistical behavior of these different architectures by varying the number of clusters in our framework from 4 to 16. These results clearly show the advantages of one specific architecture over others. Finally, based on our results, we propose a new interconnection network, which should lower this performance gap. © 2009, ACM. All rights reserved.",RISC/CISC; VLIW architectures,
Hierarchical Partitioning of VLSI Floorplans by Staircases,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024293338&doi=10.1145%2f1188275.1188282&partnerID=40&md5=30c6390f0b48081f92c2d0855a38789e,"This article addresses the problem of recursively bipartitioning a given floorplan F using monotone staircases. At each level of the hierarchy, a monotone staircase from one corner of F to its opposite corner is identified, such that (i) the two parts of the bipartition are nearly equal in area (or in the number of blocks), and (ii) the number of nets crossing the staircase is minimal. The problem of area-balanced bipartitioning is shown to be NP-hard, and a maxflow-based heuristic is proposed. Such a hierarchy may be useful to repeater placement in deep-submicron physical design, and also to global routing. © 2009, ACM. All rights reserved.",balanced bipartitioning; Floorplanning; global routing; network flow; NP-completeness,
Instruction Set Synthesis with Efficient Instruction Encoding for Configurable Processors,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024275539&doi=10.1145%2f1188275.1188283&partnerID=40&md5=733b6b8b512eac660360e40591b24f85,"Application-specific instructions can significantly improve the performance, energy-efficiency, and code size of configurable processors. While generating new instructions from application-specific operation patterns has been a common way to improve the instruction set (IS) of a configurable processor, automating the design of ISs for given applications poses new challenges—how to create as well as utilize new instructions in a systematic manner, and how to choose the best set of application-specific instructions considering the various effects the new instructions may have on the data path and the compilation? To address these problems, we present a novel IS synthesis framework that optimizes the IS through an efficient instruction encoding for the given application as well as for the given data path architecture. We first build a library of new instructions created with various encoding alternatives taking into account the data path architecture constraints, and then select the best set of instructions while satisfying the instruction bitwidth constraint. We formulate the problem using integer linear programming and also present an effective heuristic algorithm. Experimental results using our technique generate ISs that show improvements of up to about 40% over the native IS for several application benchmarks running on typical embedded RISC processors. © 2009, ACM. All rights reserved.",,
A Hierarchical Modeling Framework for On-Chip Communication Architectures of Multiprocessing SOCS,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024287474&doi=10.1145%2f1188275.1188281&partnerID=40&md5=07e5d7be3e5c07de74c3f1e52db0d9d7,"In multiprocessor-based SoCs, optimizing the communication architecture is often as important, if not more important, than optimizing the computation architecture. While there are mature platforms and techniques for the modeling and evaluation of architectures of processing elements, the same is not true for the communication architectures. This article presents an application-driven retargetable prototyping platform that fills this gap. This environment aims to facilitate the design exploration of the communication subsystem through application-level execution-driven simulations and quantitative analysis. Based on an analysis of a wide range of on-chip communication architectures, we describe how a specific hierarchical class library can be used to develop new on-chip communication architectures, or variants of existing ones with relatively little incremental effort. We demonstrate this through three case studies including two commercial on-chip bus systems and an on-chip packet switching network. Here we show that, through careful analysis and construction, it is possible for the modeling environment to support the common features of these architectures as part of the library and permit instantiation of the individual architectures as variants of the library design. Consequently, system-level design choices regarding the communication architecture can be made with high confidence in the early stages of design. In addition to improving design quality, this methodology also results in significantly shortening design-time. © 2009, ACM. All rights reserved.",Computer-aided Design (CAD),
Area Reduction by Deadspace Utilization on Interconnect Optimized Floorplan,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024262817&doi=10.1145%2f1188275.1188278&partnerID=40&md5=87480511bcfd6fe8cec9b65ef17ffe99,"Interconnect optimization has become the major concern in floorplanning. Many approaches would use simulated annealing (SA) with a cost function composed of a weighted sum of area, wirelength, and interconnect cost. These approaches can reduce the interconnect cost efficiently but the area penalty of the interconnect optimized floorplan is usually quite large. In this article, we propose an approach called deadspace utilization (DSU) to reclaim the unused area of an interconnect optimized floorplan by linear programming. Since modules are not necessarily rectangular in shape in floorplanning, some deadspace can be redistributed to the modules to increase the area occupied by each module. If the area of each module can be expanded by the same ratio, the whole floorplan can be compacted by that ratio to give a smaller floorplan. However, we will limit the compaction ratio to prevent overcongestion. Experiments show that we can apply this deadspace utilization technique to reduce the area and total wirelength of an interconnect optimized floorplan further while the routability can be maintained at the same time. Categories and Subject Descriptors: B.7.2 [Integrated Circuit]: Design Aids General Terms: Performance Additional Key Words and Phrases: Floorplanning, area reduction. © 2009, ACM. All rights reserved.",,
Scan-BIST Based on Cluster Analysis and the Encoding of Repeating Sequences,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024278431&doi=10.1145%2f1188275.1188279&partnerID=40&md5=5b83e95c8419e26ee7c8d6e4388eabae,"We present a built-in self-test (BIST) approach for full-scan designs that extracts the most frequently occurring sequences from deterministic test patterns. The extracted sequences are stored on-chip, and are used during test application. Three sets of test patterns are applied to the circuit under test during a BIST test session; these include pseudorandom patterns, semirandom patterns, and deterministic patterns. The semirandom patterns are generated based on the stored sequences and they are more likely to detect hard-to-detect faults than pseudorandom patterns. The deterministic patterns are encoded using either the stored sequences or the LFSR reseeding technique to reduce test data volume. We use the cluster analysis technique for sequence extraction to reduce the amount of data to be stored. Experimental results for the ISCAS-89 benchmark circuits show that the proposed approach often requires less on-chip storage and test data volume than other recent BIST methods. © 2009, ACM. All rights reserved.",Reliability; Testing—Built-intests,
System-Level Performance/Power Analysis for Platform-Based Design of Multimedia Applications,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024290274&doi=10.1145%2f1188275.1188277&partnerID=40&md5=7018766da2ea5bf01064f095b2a057c5,"The objective of this article is to introduce the use of Stochastic Automata Networks (SANs) as an effective formalism for application-architecture modeling in system-level average-case analysis for platform-based design. By platform, we mean a family of heterogeneous architectures that satisfy a set of architectural constraints imposed to allow re-use of hardware and software components. More precisely, we show how SANs can be used early in the design cycle to identify the best performance/power trade-offs among several application-architecture combinations. Having this information available not only helps avoid lengthy simulations for predicting power and performance figures, but also enables efficient mapping of different applications onto a chosen platform. We illustrate the benefits of our methodology by using the “Picture-in-Picture” video decoder as a driver application. Categories and Subject Descriptors: B.3.3 [Memory Structures]: Performance Analysis and Design Aids; C.4 [Computer Systems Organization]: Performance of Systems; G.3 [Mathematics of Computing]: Probability and Statistics—Markov processes, stochastic processes General Terms: Performance, Design, Theory Additional Key Words and Phrases: Markov chains, stochastic automata networks (SANs), performance models, design space exploration, platform-based design, average-case analysis, hardware/software codesign. © 2009, ACM. All rights reserved.",,
Area reduction by deadspace utilization on interconnect optimized floorplan,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846974770&doi=10.1145%2f1217088.1217091&partnerID=40&md5=e47ef8226219dc10890e6695d4fe1bd6,"Interconnect optimization has become the major concern in floorplanning. Many approaches would use simulated annealing (SA) with a cost function composed of a weighted sum of area, wirelength, and interconnect cost. These approaches can reduce the interconnect cost efficiently but the area penalty of the interconnect optimized floorplan is usually quite large. In this article, we propose an approach called deadspace utilization (DSU) to reclaim the unused area of an interconnect optimized floorplan by linear programming. Since modules are not necessarily rectangular in shape in floorplanning, some deadspace can be redistributed to the modules to increase the area occupied by each module. If the area of each module can be expanded by the same ratio, the whole floorplan can be compacted by that ratio to give a smaller floorplan. However, we will limit the compaction ratio to prevent overcongestion. Experiments show that we can apply this deadspace utilization technique to reduce the area and total wirelength of an interconnect optimized floorplan further while the routability can be maintained at the same time. © 2007 ACM.",Area reduction; Floorplanning,
Instruction set synthesis with efficient instruction encoding for configurable processors,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846950858&doi=10.1145%2f1217088.1217096&partnerID=40&md5=f48337387e8fdacd1d52242c97acd3ce,"Application-specific instructions can significantly improve the performance, energy-efficiency, and code size of configurable processors. While generating new instructions from application-specific operation patterns has been a common way to improve the instruction set (IS) of a configurable processor, automating the design of ISs for given applications poses new challenges - -how to create as well as utilize new instructions in a systematic manner, and how to choose the best set of application-specific instructions considering the various effects the new instructions may have on the data path and the compilation To address these problems, we present a novel IS synthesis framework that optimizes the IS through an efficient instruction encoding for the given application as well as for the given data path architecture. We first build a library of new instructions created with various encoding alternatives taking into account the data path architecture constraints, and then select the best set of instructions while satisfying the instruction bitwidth constraint. We formulate the problem using integer linear programming and also present an effective heuristic algorithm. Experimental results using our technique generate ISs that show improvements of up to about 40% over the native IS for several application benchmarks running on typical embedded RISC processors. © 2007 ACM.",Application-specific instruction set processor (ASIP); Bitwidth-economical; Configurable processor; Instruction encoding; ISA customization and specialization,
Impact of intercluster communication mechanisms on ILP in clustered VLIW architectures,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847003719&doi=10.1145%2f1217088.1217089&partnerID=40&md5=f18be173754c5b5a6b6d255318c0097e,"VLIW processors have started gaining acceptance in the embedded systems domain. However, monolithic register file VLIW processors with a large number of functional units are not viable. This is because of the need for a large number of ports to support FU requirements, which makes them expensive and extremely slow. A simple solution is to break the register file into a number of smaller register files with a subset of FUs connected to it. These architectures are termed clustered VLIW processors.In this article, we first build a case for clustered VLIW processors with four or more clusters by showing that the achievable ILP in most of the media applications for a 16 ALU and 8 LD/ST VLIW processor is around 20. We then provide a classification of the intercluster interconnection design space, and show that a large part of this design space is currently unexplored. Next, using our performance evaluation methodology, we evaluate a subset of this design space and show that the most commonly used type of interconnection, RF-to-RF, fails to meet achievable performance by a large factor, while certain other types of interconnections can lower this gap considerably. We also establish that this behavior is heavily application dependent, emphasizing the importance of application-specific architecture exploration. We also present results about the statistical behavior of these different architectures by varying the number of clusters in our framework from 4 to 16. These results clearly show the advantages of one specific architecture over others. Finally, based on our results, we propose a new interconnection network, which should lower this performance gap. © 2007 ACM.",ASIP; Clustered VLIW processors; Performance evaluation; VLIW,
Hierarchical partitioning of VLSI floorplans by staircases,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846946528&doi=10.1145%2f1217088.1217095&partnerID=40&md5=afcaaa7275ab0987b374dfb0de63b6a5,"This article addresses the problem of recursively bipartitioning a given floorplan F using monotone staircases. At each level of the hierarchy, a monotone staircase from one corner of F to its opposite corner is identified, such that (i) the two parts of the bipartition are nearly equal in area (or in the number of blocks), and (ii) the number of nets crossing the staircase is minimal. The problem of area-balanced bipartitioning is shown to be NP-hard, and a maxflow-based heuristic is proposed. Such a hierarchy may be useful to repeater placement in deep-submicron physical design, and also to global routing. © 2007 ACM.",Balanced bipartitioning; Floorplanning; Global routing; Network flow; NP-completeness,
System-level performance/power analysis for platform-based design of multimedia applications,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846984298&doi=10.1145%2f1217088.1217090&partnerID=40&md5=1392fb0872bacd1288d2f743d203a492,"The objective of this article is to introduce the use of Stochastic Automata Networks (SANs) as an effective formalism for application-architecture modeling in system-level average-case analysis for platform-based design. By platform, we mean a family of heterogeneous architectures that satisfy a set of architectural constraints imposed to allow re-use of hardware and software components. More precisely, we show how SANs can be used early in the design cycle to identify the best performance/power trade-offs among several application-architecture combinations. Having this information available not only helps avoid lengthy simulations for predicting power and performance figures, but also enables efficient mapping of different applications onto a chosen platform. We illustrate the benefits of our methodology by using the Picture-in-Picture video decoder as a driver application. © 2007 ACM.",Average-case analysis; Design space exploration; Hardware/software codesign; Markov chains; Performance models; Platform-based design; Stochastic automata networks (SANs),
Scan-BIST based on cluster analysis and the encoding of repeating sequences,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846984646&doi=10.1145%2f1217088.1217092&partnerID=40&md5=ebddfea5d517f4c367b61c2f42dfdd36,"We present a built-in self-test (BIST) approach for full-scan designs that extracts the most frequently occurring sequences from deterministic test patterns. The extracted sequences are stored on-chip, and are used during test application. Three sets of test patterns are applied to the circuit under test during a BIST test session; these include pseudorandom patterns, semirandom patterns, and deterministic patterns. The semirandom patterns are generated based on the stored sequences and they are more likely to detect hard-to-detect faults than pseudorandom patterns. The deterministic patterns are encoded using either the stored sequences or the LFSR reseeding technique to reduce test data volume. We use the cluster analysis technique for sequence extraction to reduce the amount of data to be stored. Experimental results for the ISCAS-89 benchmark circuits show that the proposed approach often requires less on-chip storage and test data volume than other recent BIST methods. © 2007 ACM.",Built-in self-test (BIST); Clustering test data volume; Test compression,
Workload-Ahead-Driven Online Energy Minimization Techniques for Battery-Powered Embedded Systems with Time-Constraints,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849031892&doi=10.1145%2f1188275.1188280&partnerID=40&md5=bab3b7380dac53f3598ba5a43894c13a,"This article proposes a new online voltage scaling (VS) technique for battery-powered embedded systems with real-time constraints. The VS technique takes into account the execution times and discharge currents of tasks to further reduce the battery charge consumption when compared to the recently reported slack forwarding technique [Ahmed and Chakrabarti 2004], while maintaining low online complexity of O(1). Furthermore, we investigate the impact of online rescheduling and remapping on the battery charge consumption for tasks with data dependency which has not been explicitly addressed in the literature and propose a novel rescheduling/remapping technique. Finally, we take leakage power into consideration and extend the proposed online techniques to include adaptive body biasing (ABB) which is used to reduce the leakage power. We demonstrate and compare the efficiency of the presented techniques using seven real-life benchmarks and numerous automatically generated examples. © 2009, ACM. All rights reserved.",Algorithms; Design,
Workload-ahead-driven online energy minimization techniques for battery-powered embedded systems with time-constraints,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846951198&doi=10.1145%2f1217088.1217093&partnerID=40&md5=20652f8a7250052da4833fb1ea09b605,"This article proposes a new online voltage scaling (VS) technique for battery-powered embedded systems with real-time constraints. The VS technique takes into account the execution times and discharge currents of tasks to further reduce the battery charge consumption when compared to the recently reported slack forwarding technique [Ahmed and Chakrabarti 2004], while maintaining low online complexity of O(1). Furthermore, we investigate the impact of online rescheduling and remapping on the battery charge consumption for tasks with data dependency which has not been explicitly addressed in the literature and propose a novel rescheduling/remapping technique. Finally, we take leakage power into consideration and extend the proposed online techniques to include adaptive body biasing (ABB) which is used to reduce the leakage power.We demonstrate and compare the efficiency of the presented techniques using seven real-life benchmarks and numerous automatically generated examples. © 2007 ACM.",Adaptive body biasing; Battery; Dynamic voltage scaling; Embedded systems,
A hierarchical modeling framework for on-chip communication architectures of multiprocessing SoCs,2007,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847003068&doi=10.1145%2f1217088.1217094&partnerID=40&md5=ddb87f7a2072e0b0ac44183b45e9ee3c,"In multiprocessor-based SoCs, optimizing the communication architecture is often as important, if not more important, than optimizing the computation architecture. While there are mature platforms and techniques for the modeling and evaluation of architectures of processing elements, the same is not true for the communication architectures. This article presents an application-driven retargetable prototyping platform that fills this gap. This environment aims to facilitate the design exploration of the communication subsystem through application-level execution-driven simulations and quantitative analysis. Based on an analysis of a wide range of on-chip communication architectures, we describe how a specific hierarchical class library can be used to develop new on-chip communication architectures, or variants of existing ones with relatively little incremental effort. We demonstrate this through three case studies including two commercial on-chip bus systems and an on-chip packet switching network. Here we show that, through careful analysis and construction, it is possible for the modeling environment to support the common features of these architectures as part of the library and permit instantiation of the individual architectures as variants of the library design. Consequently, system-level design choices regarding the communication architecture can be made with high confidence in the early stages of design. In addition to improving design quality, this methodology also results in significantly shortening design-time. © 2007 ACM.",Bus; Design exploration; Multiprocessor system; Network-on-chip; Object-oriented modeling; On-chip communication architecture; Packet-switching network; Retargetable simulation,
Statistical timing analysis using levelized covariance propagation considering systematic and random variations of process parameters,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750969691&doi=10.1145%2f1179461.1179464&partnerID=40&md5=fb67e7d97a0c8515ccecb211aec9a8b5,"Variability in process parameters is making accurate timing analysis of nano-scale integrated circuits an extremely challenging task. In this article, we propose a new algorithm for statistical static timing analysis (SSTA) using levelized covariance propagation (LCP). The algorithm simultaneously considers the effect of die-to-die variations in process parameters as well as within-die variation, including systematic and random variations. In order to efficiently handle complicated process variation models while contending with the arbitrary correlation among timing signals, we employ a compact form of the levelized statistical data structure. Furthermore, we propose two enhancements to the LCP algorithms to the make it practical for the analysis of large sized circuits. Results on several ISCAS'85 benchmark circuits in predictive 70nm technology show an average of 0.19% and 0.57% errors in the mean and standard deviation, respectively, of timing analysis using the proposed technique, as compared to the Monte Carlo-based approach. © 2006 ACM.",Process variation; Spatial correlation; Statistical timing analysis,
Multiple wire reconnections based on implication flow graph,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750973386&doi=10.1145%2f1179461.1179468&partnerID=40&md5=830b059a84049b3956a62c937856814c,"Global flow optimization (GFO) can perform multiple fanout/fanin wire reconnections at a time by modeling the problem of multiple wire reconnections with a flow graph, and then solving the problem using the maxflow-mincut algorithm on the flow graph. In this article, we propose an efficient multiple wire reconnection technique that modifies the framework of GFO, and as a result, can obtain better optimization quality. First, we observe that the flow graph in GFO cannot fully characterize wire reconnections, which causes the GFO to lose optimality in several obvious cases. In addition, we find that fanin reconnection can have more optimization power than fanout reconnection, but requires more sophisticated modeling. We reformulate the problem of fanout/fanin reconnections by a new graph, called the implication flow graph (IFG). We show that the problem of wire reconnections on the implication flow graph is NP-complete and also propose an efficient heuristic on the new graph. To demonstrate the effectiveness of our proposed method, we conduct an application which utilizes the flexibility of the wire reconnections explored in the logic domain to further minimize interconnects in the physical layout. Our experimental results are very exciting. © 2006 ACM.",Automatic test pattern generation (ATPG); Global flow optimization (GFO); Implication flow graph (IFG); Mandatory assignment; Multiple wire reconnection; Redundant wire,
Performance-driven technology mapping with MSG partition and selective gate duplication,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750967661&doi=10.1145%2f1179461.1179469&partnerID=40&md5=700ef44e1e22b76ac22fcba4ae97c6ad,"Traditionally, technology mapping is done by first partitioning a circuit into a forest of trees. Each individual tree is then mapped using dynamic programming. The links among the mappings of different trees are provided via propagating the essential mapping information along multiple fanout branches. While this approach may achieve optimality within each tree, the overall result is compromised from the very first treatment of fanouts. In this article, we propose a new scheme that greatly improves technology mapping. Instead of a forest of trees, we partition the circuit into a set of maximal super-gates (MSGs). These are used to transform the original circuit into trees. We then apply the dynamic programming technique to the trees and allow duplication of gates in the mapping of each individual MSG. Experimental results on ISCAS'85 benchmarks show that our approach delivers an average of 20.6% reduction in delay with only a 9.5% increase on area. © 2006 ACM.",Covering; Directed acyclic graph; Dynamic programming; Gate duplication; Logic synthesis; Matching; Maximal super-gate; Partition; Super-gate; Technology mapping,
Crosstalk minimization in logic synthesis for PLAs,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751007284&doi=10.1145%2f1179461.1179466&partnerID=40&md5=bbe9f5906adc7c6c2662f5fa120168e9,"We propose a maximum crosstalk effect minimization algorithm that takes logic synthesis into consideration for PLA structures. To minimize the crosstalk effect, a technique for permuting wire is used which contains the following steps. First, product terms are partitioned into long and short sets, and then the product terms in the long and short sets are interleaved. After that, we take advantage of the crosstalk immunity of product terms in the long set to further reduce the maximum coupling capacitance of the PLA. Finally, synthesis techniques such as local and global transformations are taken into consideration to search for a better result. The experiments demonstrate that our algorithm can effectively minimize the maximum coupling capacitance of a circuit by 51% as compared with the original area-minimized PLA without crosstalk effect minimization. © 2006 ACM.",Crosstalk; Domino logic; PLA; Synthesis,
Decomposition of instruction decoders for low-power designs,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750975095&doi=10.1145%2f1179461.1179465&partnerID=40&md5=79ad17c830cbcf37c731c6a3adea861b,"During the execution of processor instruction, decoding the instructions is a major task in identifying instructions and generating control signals for data paths. In this article, we propose two instruction decoder decomposition techniques for low-power designs. First, by tracing program execution sequences, we propose an algorithm that explores the relations between frequently executed instructions. Second, we propose a two-stage low-power decomposition structure for decoding instructions. Experimental results demonstrate that our proposed techniques achieve an average of 34.18% in power reduction and 12.93% in critical-path delay reduction for the instruction decoder. © 2006 ACM.",Instruction decoder; Low power,
Test sequence generation for controller verification and test with high coverage,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750974797&doi=10.1145%2f1179461.1179467&partnerID=40&md5=885f59e9949c5cda152d60d406faf109,"Verification and test are critical phases in the development of any hardware or software system. This article focuses on black box testing of the control part of hardware and software systems. Black box testing involves specification, test generation, and fault coverage. Finite state machines (FSMs) are commonly used for specifying controllers. FSMs may have shortcomings in modeling complex systems. With the introduction of X-machines, complex systems can be modeled at higher levels of abstraction. An X-machine can be converted into an FSM while preserving the level of abstraction. The fault coverage of a test sequence for an FSM specification provides a confidence level. We propose a fault coverage metric for an FSM specification based on the transition fault model, and using this metric, we derive the coverage of a test sequence. The article also presents a method which generates short test sequences that meet a specific coverage level and then extends this metric to determine the coverage of a test sequence for an FSM driven by an FSM network. We applied our FSM verification technique to a real-life FSM, namely, the fibre channel arbitrated loop port state machine, used in the field of storage area networks. © 2006 ACM.",Black box testing; Fault coverage; Finite state machine; X-machine,
Synthesis of time-constrained multitasking embedded software,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751008681&doi=10.1145%2f1179461.1179463&partnerID=40&md5=b680c7ceb5c2cf5cfe744715649b35bd,"In modern embedded systems, software development plays a vital role. Many key functions are being migrated to software, aiming at a shorter time to market and easier upgrades. Multitasking is increasingly common in embedded software, and many of these tasks incorporate real-time constraints. Although multitasking simplifies coding, it demands an operating system and imposes significant overhead on the system. The use of serializing compilers, such as the Phantom compiler, allows the synthesis of a monolithic code from a multitasking C application, eliminating the need for an operating system. In this article, we introduce the synthesis of multitasking applications that execute in a timely manner. We incorporate the notion of timing constraints into the Phantom compiler, and show that our approach is effective in meeting such constraints, allowing fine-grained concurrency among the tasks. As an additional case study, we present the implementation of a software-based modem and show that real-time applications such as the modem have guaranteed performance in the serialized code generated by the Phantom compiler. © 2006 ACM.",Code serialization; Multitasking; Real-time embedded software; Software synthesis,
Postlayout optimization for synthesis of Domino circuits,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751008838&doi=10.1145%2f1179461.1179462&partnerID=40&md5=47b72d14b43b0374a44d007d62312d74,"Logic duplication, a commonly used synthesis technique to remove trapped inverters in reconvergent paths of Domino circuits, incurs high area and power penalties. In this article, we propose a synthesis scheme to reduce the duplication cost by allowing inverters in Domino logic under certain timing constraints for both simple and complex gates. Moreover, we can include the logic duplication minimization during technology mapping for synthesis of Domino circuits with complex gates. In order to guarantee the robustness of such Domino circuits, we perform the logic optimization as a postlayout step. Experimental results show significant reduction in duplication cost, which translates into significant improvements in area and power. As a byproduct, the timing performance is also improved owing to smaller layout area and/or logic depth. © 2006 ACM.",Domino logic; Optimization; Synthesis,
Introduction to special issue: Novel paradigms in system-level design,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748435942&doi=10.1145%2f1142980.1142981&partnerID=40&md5=f8dbfe3c6fd63f6a0e3400f2ecd6e3a1,[No abstract available],,
Analysis and optimization of distributed real-time embedded systems,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748432713&doi=10.1145%2f1142980.1142984&partnerID=40&md5=92fe9f218ab85d563576e40b764c3bd1,"An increasing number of real-time applications are today implemented using distributed heterogeneous architectures composed of interconnected networks of processors. The systems are heterogeneous not only in terms of hardware and software components, but also in terms of communication protocols and scheduling policies. In this context, the task of designing such systems is becoming increasingly difficult. The success of new adequate design methods depends on the availability of efficient analysis as well as optimization techniques. In this article, we present both analysis and optimization approaches for such heterogeneous distributed real-time embedded systems. More specifically, we discuss the schedulability analysis of hard real-time systems, highlighting particular aspects related to the heterogeneous and distributed nature of the applications. We also introduce several design optimization problems characteristic of this class of systems: mapping of functionality, the optimization of access to communication channel, and the assignment of scheduling policies to processes. Optimization heuristics aiming at producing a schedulable system with a given amount of resources are presented. © 2006 ACM.",Algorithms; Design; Performance; Theory,
"A game-theoretic framework for multimetric optimization of interconnect delay, power, and crosstalk noise during wire sizing",2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748436088&doi=10.1145%2f1142980.1142988&partnerID=40&md5=47e3475061f378a1ab26d9f477d38f7b,"The continuous scaling of interconnect wires in deep submicron (DSM) circuits results in increased interconnect delay, power, and crosstalk noise. In this work, we develop a game-theoretic framework and multimetric optimization algorithms for the simultaneous optimization during wire sizing of (i) interconnect delay and crosstalk noise, and (ii) interconnect delay, power, and crosstalk noise. We formulate the wire sizing optimization problem as a normal form-game model and solve it using Nash equilibrium theory. Game theory allows the optimization of multiple metrics with conflicting objectives. This property is exploited in modeling the wire sizing problem while simultaneously optimizing various design parameters like interconnect delay, power, and crosstalk noise, which are conflicting in nature. The nets connecting the driving cell and the driven cell are divided into net segments. The net segments within a channel are modeled as players and the range of possible wire sizes forms the set of strategies. The payoff function is modeled (i) as the geometric mean of interconnect delay and crosstalk noise in the case of first formulation, and (ii) as the weighted sum of interconnect delay, power, and crosstalk noise in the second formulation. The net segments are optimized from the ones closest to the driven cell towards the ones at the driving cell. Complete information about the coupling effects among the nets is extracted after the detailed routing phase. The time and space complexities of the proposed wire sizing formulations are linear in terms of the number of net segments. Experimental results on several medium and large open-core designs indicate that the proposed algorithm for simultaneous optimization of interconnect delay and crosstalk noise yields an average reduction of 21.48% in interconnect delay and a 26.25% reduction in crosstalk noise without any area overhead, over and above the optimization from the Cadence place and route tools. It is shown through experimental results that the algorithm performs significantly better than simulated annealing and genetic search. Further, new simple but accurate models are developed for three parallel interconnect net segments. It is shown that these models yield the same level of accuracy with significantly better run times compared to the models reported in Chen et al. [2004]. A mathematical proof of existence for the Nash equilibrium solution for the proposed wire sizing formulation is also provided. © 2006 ACM.",Crosstalk noise; Game theory; Interconnect delay; Interconnect models; Transmission lines; Wire sizing,
A stimulus-free graphical probabilistic switching model for sequential circuits using dynamic Bayesian networks,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748439811&doi=10.1145%2f1142980.1142990&partnerID=40&md5=3986919fb11737c8133bc208f5779af4,"We propose a novel, nonsimulative probabilistic model for switching activity in sequential circuits, capturing both spatio-temporal correlations at internal nodes and higher order temporal correlations due to feedback. This model, which we refer to as the temporal dependency model (TDM), can be constructed from the logic structure and is shown to be a dynamic Bayesian network. Dynamic Bayesian networks are extremely powerful in modeling high order temporal, as well as spatial, correlations; TDM is an exact model for the underlying conditional independencies. The attractive feature of this graphical representation of the joint probability function is not only that it makes the dependency relationships amongst nodes explicit, but it also serves as a computational mechanism for probabilistic inference. We report average errors in switching probability of 0.006, with errors tightly distributed around mean error values, on ISCAS'89 benchmark circuits involving up to 10000 signals. © 2006 ACM.",Dynamic Bayesian networks; Sequential circuits; TDM,
Architecture Description Language (ADL)-driven software toolkit generation for architectural exploration of programmable SOCs,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748438696&doi=10.1145%2f1142980.1142985&partnerID=40&md5=9c2f241658e45e3e80a4c959a1f092d0,"Advances in semiconductor technology permit increasingly complex applications to be realized using programmable systems-on-chips (SOCs). Furthermore, shrinking time-to-market demands, coupled with the need for product versioning through software modification of SOC platforms, have led to a significant increase in the software content of these SOCs. However, designer productivity is greatly hampered by the lack of automated software generation tools for the exploration and evaluation of different architectural configurations. Traditional hardware-software codesign flows do not support effective exploration and customization of the embedded processors used in programmable SOCs. The inherently application-specific nature of embedded processors and the stringent area, power, and performance constraints in embedded systems design critically require a fast and automated architecture exploration methodology. Architecture description language (ADL)-Driven design space exploration and software toolkit generation strategies present a viable solution to this problem, providing a systematic mechanism for a top-down design and validation of complex systems. The heart of this approach lies in the ability to automatically generate a software toolkit that includes an architecture-sensitive compiler, a cycle-accurate simulator, assembler, debugger, and verification/validation tools. This article illustrates a software toolkit generation methodology using the EXPRESSION ADL. Our exploration studies demonstrate the need for and usefulness of this approach, using as an example the problem of compiler-in-the-loop design space exploration of reduced instruction-set embedded processor architectures. © 2006 ACM.",Design; Languages,
Module placement for fault-tolerant microfluidics-based biochips,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748424561&doi=10.1145%2f1142980.1142987&partnerID=40&md5=7bf542d29a0cd24ad7f1632b10d23741,"Microfluidics-based biochips are soon expected to revolutionize clinical diagnosis, DNA sequencing, and other laboratory procedures involving molecular biology. Most microfluidic biochips today are based on the principle of continuous fluid flow and they rely on permanently etched microchannels, micropumps, and microvalves. We focus here on the automated design of ""digital"" droplet-based microfluidic biochips. In contrast to conventional continuous-flow systems, digital microfluidics offers dynamic reconfigurability; groups of cells in a microfluidics array can be reconfigured to change their functionality during the concurrent execution of a set of bioassays. We present a simulated annealing-based technique for module placement in such biochips. The placement procedure not only addresses chip area, but also considers fault tolerance, which allows a microfluidic module to be relocated elsewhere in the system when a single cell is detected to be faulty. Simulation results are presented for case studies involving the polymerase chain reaction and multiplexed in vitro clinical diagnostics. © 2006 ACM.",Biochips; Microfluidics; Module placement; Physical design automation,
Simultaneous placement with clustering and duplication,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748414638&doi=10.1145%2f1142980.1142989&partnerID=40&md5=5d061895ed46fe7667131917c80dd693,"Clustering, duplication, and placement are critical steps in a cluster-based FPGA design flow. Clustering has a great impact on the wirelength, timing, and routability of a circuit. Logic duplication is an effective method for improving performance while maintaining the logic equivalence of a circuit. Based on several novel algorithmic contributions, we present an efficient and effective algorithm named SPCD (simultaneous placement with clustering and duplication) which performs clustering and duplication during placement for wirelength and timing minimization. First, we incorporate a path counting-based net weighting scheme for more effective timing optimization. Secondly, we introduce a novel method of moving a fragment of a cluster (called a fragment level move) during placement to optimize the clustering structure. To reduce the critical path detour during legalization from a more global perspective, we also introduce the notions of a monotone region and a global monotone region in which improvement to the local/global path detour is guaranteed. Furthermore, we introduce a notion of a constrained gain graph to embed all complex FPGA clustering constraints, and implement an optimal incremental legalization algorithm under such constraints. Finally, in order to reduce the circuit area, we formulate a timing-constrained global redundancy removal problem and propose a heuristic solution. Our SPCD algorithm outperforms a widely used academic FPGA placement flow, T-VPack + VPR, with an average reduction of 31% in the longest path estimate delay and 18% in the routed delay. We also apply our SPCD algorithm to Altera's Stratix architecture in a commercial FPGA implementation flow (Quartus II 4.0). The routed result achieved by our SPCD algorithm outperforms VPR by 20% and outperforms Quartus II 4.0 by 4%. © 2006 ACM.",Clustering; Duplication; FPGA; Legalization; Placement; Redundancy removal,
System level design paradigms: Platform-Based design and communication synthesis,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748419494&doi=10.1145%2f1142980.1142982&partnerID=40&md5=908bbf6391afe67e24dd0cfb7f8712c1,"Embedded system level design must be based on paradigms that make formal foundations and unification a cornerstone of their construction. Platform-Based designs and communication synthesis are important components of the paradigm shift we advocate. Communication synthesis is a fundamental productivity tool in a design methodology where reuse is enforced. Communication design in a reuse methodology starts with a set of functional requirements and constraints on the interaction among components and then proceeds to build protocols, topology, and physical implementations that satisfy requirements and constraints while optimizing appropriate measures of efficiency of the implementation. Maximum efficiency can be reached when the communication specifications are entered at high levels of abstraction and the design process optimizes the implementation from this specification. Unfortunately, this process is very difficult if it is not cast in a rigorous framework. Platform-Based design helps define a successive refinement process where each step can be carried out automatically and optimized appropriately. We present two cases, an on-chip and a wireless sensor network design, where the resulting methodology gave encouraging results. © 2006 ACM.",Communication synthesis; Embedded systems; Platform-based design,
Warp processors,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748420512&doi=10.1145%2f1142980.1142986&partnerID=40&md5=0bd377139fd6da25904e2975f8385b46,"We describe a new processing architecture, known as a warp processor, that utilizes a field-programmable gate array (FPGA) to improve the speed and energy consumption of a software binary executing on a microprocessor. Unlike previous approaches that also improve software using an FPGA but do so using a special compiler, a warp processor achieves these improvements completely transparently and operates from a standard binary. A warp processor dynamically detects the binary's critical regions, reimplements those regions as a custom hardware circuit in the FPGA, and replaces the software region by a call to the new hardware implementation of that region. While not all benchmarks can be improved using warp processing, many can, and the improvements are dramatically better than those achievable by more traditional architecture improvements. The hardest part of warp processing is that of dynamically reimplementing code regions on an FPGA, requiring partitioning, decompilation, synthesis, placement, and routing tools, all having to execute with minimal computation time and data memory so as to coexist on chip with the main processor. We describe the results of developing our warp processor. We developed a custom FPGA fabric specifically designed to enable lean place and route tools, and we developed extremely fast and efficient versions of partitioning, decompilation, synthesis, technology mapping, placement, and routing. Warp processors achieve overall application speedups of 6.3X with energy savings of 66% across a set of embedded benchmark applications. We further show that our tools utilize acceptably small amounts of computation and memory which are far less than traditional tools. Our work illustrates the feasibility and potential of warp processing, and we can foresee the possibility of warp processing becoming a feature in a variety of computing domains, including desktop, server, and embedded applications. © 2006 ACM.",Configurable logic; Dynamic optimization; FPGA; Hardware/software codesign; Hardware/software partitioning; Just-in-time (JIT) compilation; Warp processors,
Computation and communication refinement for multiprocessor SoC design: A system-level perspective,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748437678&doi=10.1145%2f1142980.1142983&partnerID=40&md5=f10703416a0489d511726b9dc29fffb9,"Continuous advancements in semiconductor technology enable the design of complex systems-on-chips (SoCs) composed of tens or hundreds of IP cores. At the same time, the applications that need to run on such platforms have become increasingly complex and have tight power and performance requirements. Achieving a satisfactory design quality under these circumstances is only possible when both computation and communication refinement are performed efficiently, in an automated and synergistic manner. Consequently, formal and disciplined system-level design methodologies are in great demand for future multiprocessor design. This article provides a broad overview of some fundamental research issues and state-of-the-art solutions concerning both computation and communication aspects of system-level design. The methodology we advocate consists of developing abstract application and platform models, followed by application mapping onto the target platform, and then optimizing the overall system via performance analysis. In addition, a communication refinement step is critical for optimizing the communication infrastructure in this multiprocessor setup. Finally, simulation and prototyping can be used for accurate performance evaluation purposes. © 2006 ACM.",Communication; Embedded systems; Energy optimization; Markov chains; Networks-on-chip; Performance analysis; Prototype; Systems-on-chip; Traffic,
Optimal simultaneous module and multivoltage assignment for low power,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746091678&doi=10.1145%2f1142155.1142161&partnerID=40&md5=16accb634d416d9cec3aab4351fe5179,"Reducing power consumption through high-level synthesis has attracted a growing interest from researchers due to its large potential for power reduction. In this work we study functional unit binding (or module assignment) given a scheduled data flow graph under a multi-Vdd framework. We assume that each functional unit can be driven by different Vdd levels dynamically during run time to save dynamic power. We develop a polynomial-time optimal algorithm for assigning low Vdds to as many operations as possible under the resource and latency constraints, and in the same time minimizing total switching activity through functional unit binding. Our algorithm shows consistent improvement over a design flow that separates voltage assignment from functional unit binding. We also change the initial scheduling to examine power/energy-latency tradeoff scenarios under different voltage level combinations. Experimental results show that we can achieve 28.1% and 38.4% power reductions when the latency bound is the tightest with two and three-Vdd levels respectively compared with the single-Vdd case. When latency is relaxed, multi-Vdd offers larger power reductions (up to 46.7%). We also show comparison data of energy consumption under the same experimental settings. © 2006 ACM.",Data path generation; Functional unit binding; High-level synthesis; Level conversion; Low power design; Multiple voltage; Power optimization; Scheduling,
LVS verification across multiple power domains for a quad-core microprocessor,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746100296&doi=10.1145%2f1142155.1142166&partnerID=40&md5=60411e3c5441af4a2a1fb201357fc96d,"A unique LVS (layout-versus-schematic) methodology has been developed for the verification of a four-core microprocessor with multiple power domains using a triple-well 90-nm CMOS technology. The chip is migrated from its previous generation that is for a twin-well process. Due to the design reuse, VDD and GND are designed as global nets but they are not globally connected across the entire chip. The standard LVS flow is unable to handle the additional design complexity and there seems to be no published literature tackling the problem. This paper presents a two-phase LVS methodology: a standard LVS phase where power and ground nets are defined as global nets and a multi-power-domain LVS phase where power and ground nets are treated as local nets. The first phase involves verifying LVS at the block level as well as the full-chip level. The second phase aims at verifying the integrity of the multi-power-domain power grid that is not covered in the first phase LVS. The proposed LVS methodology was successfully verified by real silicon. © 2006 ACM.",LVS; Multi-core microprocessor; Physical verification,
Reducing energy consumption of multiprocessor SoC architectures by exploiting memory bank locality,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746067485&doi=10.1145%2f1142155.1142163&partnerID=40&md5=0bc3bbff7c8b5229d1b18fa012827a1a,"The next generation embedded architectures are expected to accommodate multiple processors on the same chip. While this makes interprocessor communication less costly as compared to traditional high-end parallel machines, it also makes off-chip requests very costly. In particular, frequent off-chip memory accesses do not only increase execution cycles but also increase overall power consumption. One way of alleviating this power problem is to divide the off-chip memory into multiple banks, each of which can be power-controlled independently using low-power operating modes. In this article, we focus on a multiprocessor-system-on-a-chip (MPSoC) architecture with a banked memory system, and show how code and data optimizations can help us reduce memory energy consumption for embedded applications with regular data access patterns, for example, those from the embedded image and video processing domain. This is achieved by ensuring bank locality, which means that each processor localizes its accesses into a small set of banks in a given time period. We present a mathematical formulation of the bank locality problem. Our formulation is based on constructing a set of matrix equations that capture the mappings between the data, computation, processor, and memory bank spaces. Based on this formulation, we propose a heuristic solution to the bank locality problem under different scenarios. Our solution involves an iterative process through which we try to satisfy as many matrix constraints as possible; the unsatisfied constraints represent the degree of degradation in bank locality. Finally, we report extensive experimental results showing the effectiveness of our strategy in practice. Our results show that the proposed solution improves bank locality significantly, and reduces the overall memory system energy consumption by up to 34% over an approach that makes use of the low-power modes but does not employ our strategy. © 2006 ACM.",Bank locality; Banked memory systems; Compiler optimization; Energy consumption; Multiprocessor SoC,
Implicit grading of multiple path delay faults,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746096522&doi=10.1145%2f1142155.1142160&partnerID=40&md5=b839d3f424ad7bb96ed1065fea3adb87,The problem of fault grading for multiple path delay faults is introduced and a method of obtain exact coverage is presented. The faults are represented and manipulated as combinational sets using zero-suppressed binary decision diagrams. The presented methodology for fault grading uses only a polynomial number of zero-suppressed binary decision diagram operations. The efficiency of the proposed method is demonstrated by the experimental results on the ISCAS'85 and ISCAS'89 benchmarks. © 2006 ACM.,Decision diagrams; Delay fault testing; Fault simulation,
Concurrent testing of digital microfluidics-based biochips,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746104081&doi=10.1145%2f1142155.1142164&partnerID=40&md5=a18db8078cca187d46adafb91d6221a1,"We present a concurrent testing methodology for detecting catastrophic faults in digital microfluidics-based biochips and investigate the related problems of test planning and resource optimization. We first show that an integer linear programming model can be used to minimize testing time for a given hardware overhead, for example, droplet dispensing sources and capacitive sensing circuitry. Due to the NP-complete nature of the problem, we also develop efficient heuristic procedures to solve this optimization problem. We apply the proposed concurrent testing methodology to a droplet-based microfluidic array that was fabricated and used to perform multiplexed glucose and lactate assays. Experimental results show that the proposed test approach interleaves test application with the biomedical assays and prevents resource conflicts. The proposed method is therefore directed at ensuring high reliability and availability of bio-MEMS and lab-on-a-chip systems, as they are increasingly deployed for safety-critical applications. © 2006 ACM.",Biochips; Catastrophic faults; Concurrent testing; Microfluidics,
On the construction of zero-deficiency parallel prefix circuits with minimum depth,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746071917&doi=10.1145%2f1142155.1142162&partnerID=40&md5=6d2cb7ade872866a68b2eb552db655da,"A parallel prefix circuit has n inputs x 1, x 2,⋯, x n, and computes the n outputs y i = x i · x i-1 · ⋯ · x 1, 1 ≤ i ≤ n, in parallel, where · is an arbitrary binary associative operator. Snir proved that the depthi and sizesof any parallel prefix circuit satisfy the inequality t + s ≥ 2n - 2. Hence, a parallel prefix circuit is said to be of zero-deficiency if equality holds. In this article, we provide a different proof for Snir's theorem by capturing the structural information of zero-deficiency prefix circuits. Following our proof, we propose a new kind of zero-deficiency prefix circuit Z(d) by constructing a prefix circuit as wide as possible for a given depth d. It is proved that the Z(d) circuit has the minimal depth among all possible zero-deficiency prefix circuits. © 2006 ACM.",Depth-size trade-off; Parallel prefix circuits; Zero-deficiency,
Prototyping time- and space-efficient computations of algebraic operations over dynamically reconfigurable systems modeled by rewriting-logic,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746094641&doi=10.1145%2f1142155.1142156&partnerID=40&md5=1f5f48017c477a34111f6f91bef82996,"Many algebraic operations can be efficiently implemented as pipe networks in arrays of functional units such as systolic arrays that provide a large amount of parallelism. However, the applicability of classical systolic arrays is restricted to problems with strictly regular data dependencies yielding only arrays with uniform linear pipes. This limitation can be circumvented by using reconfigurable systolic arrays or reconfigurable data path arrays, where the node interconnections and operations can be redefined even at run time. In this context, several alternative reconfigurable systolic architectures can be explored and powerful tools are needed to model and evaluate them. Well-known rewriting-logic environments such as ELAN and Maude can be used to specify and simulate complex application-specific integrated systems. In this article we propose a methodology based on rewriting-logic which is adequate to quickly model and evaluate reconfigurable architectures (RA) in general and, in particular, reconfigurable systolic architectures. As an interesting case study we apply this rewriting-logic modeling methodology to the space-efficient treatment of the Fast-Fourier Transform (FFT). The FFT prototype conceived in this way, has been specified and validated in VHDL using the Quartus II system. © 2006 ACM.",Algebraic manipulation; Dynamically reconfigurable systems; Fast Fourier Transform (FFT); Reconfigurable computing; Rewriting Systems (TRS); Rewriting-logic; Systolic arrays,
Efficient thermal-oriented 3D floorplanning and thermal via planning for two-stacked-die integration,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746059766&doi=10.1145%2f1142155.1142159&partnerID=40&md5=2b2be6430d353fb2d7bc3e264618c4c7,"New three-dimensional (3D) floor-planning and thermal via planning algorithms are proposed for thermal optimization in two-stacked die integration. Our contributions include (1) a two-stage design flow for 3D floorplanning, which scales down the enlarged solution space due to multidevice layer structure; (2) an efficient thermal-driven 3D floorplanning algorithm with power distribution constraints; (3) a thermal via planning algorithm considering congestion minimization. Experiments results show that our approach is nine times faster with better solution quality compared to a recent published result. In addition, the thermal via planning approach is proven to be very efficient to eliminate localized hot spots directly. © 2006 ACM.",3D IC; Floorplanning; Thermal,
Reuse analysis of indirectly indexed arrays,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746078833&doi=10.1145%2f1142155.1142157&partnerID=40&md5=271beb8b10c5b8732d8b91efcc5f8b66,"We propose techniques for identifying and exploiting spatial and temporal reuse for indirectly indexed arrays. Indirectly indexed arrays are those arrays which are, typically, accessed inside multilevel loop nests and whose index expression includes not only loop iterators and constants but arrays as well. Existing techniques for improving locality are quite sophisticated in the case of directly indexed arrays. But, unfortunately, they are inadequate for handling indirectly indexed arrays. In this article we therefore extend the existing framework and techniques of directly indexed to indirectly indexed arrays. The concepts of reuse subspace, dependence vector, self, and group reuse are extended and applied in this new context. Also, lately scratch-pad memory has become an attractive alternative to data-cache, specially in the embedded multimedia community. This is because embedded systems are very sensitive to area and energy and the scratch-pad is smaller in area and consumes less energy on a per access basis compared to the cache of the same capacity. Several techniques have been proposed in the past for the efficient exploitation of the scratch-pad for directly indexed arrays. We extend these techniques by presenting a method for scratch-pad mapping of indirectly indexed arrays. This enables the scratch-pad to be used in a larger context than was possible before. © 2006 ACM.",Data reuse; Indirectly indexed arrays; Irregular access; Reuse vector; Scratch-pad,
Handling inverted temperature dependence in static timing analysis,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746080003&doi=10.1145%2f1142155.1142158&partnerID=40&md5=cfb08759ca8c4cf8a1b67917b3a195c1,"In digital circuit design, it is typically assumed that cell delay increases with decreasing voltage and increasing temperature. This assumption is the basis of the cornering approach with cell libraries in static timing analysis (STA). However, this assumption breaks down at low supply voltages because cell delay can decrease with increasing temperature. This phenomenon is caused by a competition between mobility and threshold voltage to dominate cell delay. We refer to this phenomenon as the inverted temperature dependence (ITD). Due to ITD, it becomes very difficult to analytically determine the temperatures that maximize or minimize the delay of a cell or a path. As such, ITD has profound consequences for STA: (1) ITD essentially invalidates the approach of defining corners by independently varying voltage and temperature; (2) ITD makes it more difficult to find short paths, leading to difficulties in detecting hold time violations; and (3) the effect of ITD will worsen as supply voltages decrease and threshold voltage variations increase. This article analyzes the consequences of ITD in STA and proposes a proper handling of ITD in an industrial sign-off STA tool. To the best of our knowledge, this article is the first such work. © 2006 ACM.",Static timing analysis; Temperature dependence; Timing corners; Voltage dependence,
Systematic dynamic memory management design methodology for reduced memory footprint,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746055691&doi=10.1145%2f1142155.1142165&partnerID=40&md5=2a1dced985cf968bbc46414cfe980ace,"New portable consumer embedded devices must execute multimedia and wireless network applications that demand extensive memory footprint. Moreover, they must heavily rely on Dynamic Memory (DM) due to the unpredictability of the input data (e.g., 3D streams features) and system behavior (e.g., number of applications running concurrently defined by the user). Within this context, consistent design methodologies that can tackle efficiently the complex DM behavior of these multimedia and network applications are in great need. In this article, we present a new methodology that allows to design custom DM management mechanisms with a reduced memory footprint for such kind of dynamic applications. First, our methodology describes the large design space of DM management decisions for multimedia and wireless network applications. Then, we propose a suitable way to traverse the aforementioned design space and construct custom DM managers that minimize the DM used by these highly dynamic applications. As a result, our methodology achieves improvements of memory footprint by 60% on average in real case studies over the current state-of-the-art DM managers used for these types of dynamic applications. © 2006 ACM.",Custom dynamic memory management; Memory management; Multimedia embedded systems; Operating systems; Reduced memory footprint,
Reliable crosstalk-driven interconnect optimization,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745218691&doi=10.1016%2fj.tsf.2005.12.184&partnerID=40&md5=eba6e5cc4ec31f932a4d9e8fd6d7a705,"As technology advances apace, crosstalk becomes a design metric of comparable importance to area and delay. This article focuses mainly on the crosstalk issue, specifically on the impacts of physical design and process variation on crosstalk. While the feature size shrinks below 0.25 μm, the impact of process variation on crosstalk increases rapidly. Hence, a crosstalk insensitive design is desirable in the deep submicron regime. In this article, crosstalk sensitivity is referred to as the influence of process variation on crosstalk in a circuit. We show that the lower bound of crosstalk sensitivity grows quadratically, while that of crosstalk increases linearly. Therefore, designers should also consider crosstalk sensitivity, when optimizing other design objectives such as crosstalk, area, and delay. According to our modeling, these objectives are all in posynomial forms, and thus the multi-objective optimization problem can optimally be solved by Lagrangian relaxation. Experimental results show that our method is effective and efficient. For instance, a circuit of 2856 gates and 5272 wires is optimized using 13-minute runtime and 2.8-MB memory on a Pentium III 1.0 GHz PC with 256-MB memory. In particular, by relaxing Lagrange multipliers to the critical paths, it takes only two iterations for all solutions to converge to the global optimal, which is much more efficient than related previous work. This relaxation scheme provides a key insight into the rapid convergence in Lagrangian relaxation. © 2006 ACM.",Interconnect; Lagrangian relaxation; Post-layout optimization; VLSI,
A survey of fault tolerant methodologies for FPGAs,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746032555&doi=10.1145%2f1142155.1142167&partnerID=40&md5=148adb4d67d312e6647de439da6dfae9,"A wide range of fault tolerance methods for FPGAs have been proposed. Approaches range from simple architectural redundancy to fully on-line adaptive implementations. The applications of these methods also differ; some are used only for manufacturing yield enhancement, while others can be used in-system. This survey attempts to provide an overview of the current state of the art for fault tolerance in FPGAs. It is assumed that faults have been previously detected and diagnosed; the methods presented are targeted towards tolerating the faults. A detailed description of each method is presented. Where applicable, the methods are compared using common metrics. Results are summarized to present a succinct, comprehensive comparison of the different approaches. © 2006 ACM.",Fault tolerance; FPGA; Self test,
ACM Transactions on Design Automation of Electronic Systems: Editorial,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745191590&doi=10.1145%2f1124713.1124714&partnerID=40&md5=fc490b1e0aa43225cd64b34417d25b1e,[No abstract available],,
Accurate modeling of substrate resistive coupling for floating substrates,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745206004&doi=10.1145%2f1124713.1124717&partnerID=40&md5=1793bef9ef7baf4c24c918100426691f,"This article focuses on the formulation of the substrate resistive coupling using boundary element methods, specifically for substrates without grounded backplates (floating substrates). An accurate and numerically stable formulation is presented. Numerical results are shown to demonstrate the correctness and the numerical robustness of the formulation. © 2006 ACM.",Analog and mixed signal; Floating substrate; Substrate coupling; Substrate modeling; System-on-chip,
Word-length optimization for differentiable nonlinear systems,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745222986&doi=10.1145%2f1124713.1124716&partnerID=40&md5=3d982a1ba49106ab663213d872be99f4,"This article introduces an automatic design procedure for determining the sensitivity of outputs in a digital signal processing design to small errors introduced by rounding or truncation of internal variables. The proposed approach can be applied to both linear and nonlinear designs. By analyzing the resulting sensitivity values, the proposed procedure is able to determine an appropriate distinct word-length for each internal variable in a fixed-point hardware implementation. In addition, the power-optimizing capabilities of word-length optimization are studied. Application of the proposed procedure to adaptive filters and polynomial evaluation circuits realized in a Xilinx Virtex FPGA has resulted in area reductions of up to 80% (mean 66%) combined with power reductions of up to 98% (mean 87%) and speed-up of up to 36% (mean 20%) over common alternative design strategies. © 2006 ACM.",Bitwidth; Signal processing; Synthesis; Word-length,
Improving the energy behavior of block buffering using compiler optimizations,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745212635&doi=10.1145%2f1124713.1124727&partnerID=40&md5=1dd17921fdbf329aa0d0b8a22d385bbe,"On-chip caches consume a significant fraction of the energy in current microprocessors. As a result, architectural/circuit-level techniques such as block buffering and sub-banking have been proposed and shown to be very effective in reducing the energy consumption of on-chip caches. While there has been some work on evaluating the energy and performance impact of different block buffering schemes, we are not aware of software solutions to take advantage of on-chip cache block buffers. This article presents a compiler-based approach that modifies code and variable layout to take better advantage of block buffering. The proposed technique is aimed at a class of embedded codes that make heavy use of scalar variables. Unlike previous work that uses only storage pattern optimization or only access pattern optimization, we propose an integrated approach that uses both code restructuring (which affects the access sequence) and storage pattern optimization (which determines the storage layout of variables). We use a graph-based formulation of the problem and present a solution for determining suitable variable placements and accompanying access pattern transformations. The proposed technique has been implemented using an experimental compiler and evaluated using a set of complete programs. The experimental results demonstrate that our © 2006 ACM.",Block buffering; Compiler transformations; Data cache; Embedded systems; Energy optimizations,
ILP models for simultaneous energy and transient power minimization during behavioral synthesis,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745222035&doi=10.1145%2f1124713.1124725&partnerID=40&md5=ef20b248320c55d83bfaf842d7fe48a6,"In low-power design for battery-driven portable applications, the reduction of peak power, peak power differential, cycle difference power, average power and energy are equally important. These are different forms of dynamic power dissipation of a CMOS circuit, which is predominant compared to static power dissipation for higher switching activity. The peak power, the cycle difference power, and the peak power differential drive the transient characteristic of a CMOS circuit. In this article, we propose an ILP-based framework for the reduction of energy and transient power through datapath scheduling during behavioral synthesis. A new metric called ""modified cycle power function"" (CPF*) is defined that captures the above power characteristics and facilitates integer linear programming formulations. The ILP-based datapath scheduling schemes with CPF* as objective function are developed assuming three modes of datapath operation, such as, single supply voltage and single frequency (SVSF), multiple supply voltages and dynamic frequency clocking (MVDFC), and multiple supply voltages and multicycling (MVMC). We conducted experiments on selected high-level synthesis benchmark circuits for various resource constraints and estimated power, energy and energy delay product for each of them. Experimental results show that significant reductions in power, energy and energy delay product can be obtained. © 2006 ACM.",Average power; Cycle difference power; Datapath scheduling; Dynamic frequency clocking; Multicycling; Multiple supply voltages; Peak power; Peak power differential,
Compile-time area estimation for LUT-based FPGAs,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745189089&doi=10.1145%2f1124713.1124721&partnerID=40&md5=aac111085249bc83281eba210e69186e,"The Cameron Project has developed a system for compiling codes written in a high-level language called SA-C, to FPGA-based reconfigurable computing systems. In order to exploit the parallelism available on the FPGAs, the SA-C compiler performs a large number of optimizations such as full loop unrolling, loop fusion and strip-mining. However, since the area on an FPGA is limited, the compiler needs to know the effect of compiler optimizations on the FPGA area; this information is typically not available until after the synthesis and place and route stage, which can take hours. In this article, we present a compile-time area estimation technique to guide SA-C compiler optimizations. We demonstrate our technique for a variety of benchmarks written in SA-C. Experimental results show that our technique predicts the area required for a design to within 2.5% of actual for small image processing operators and to within 5.0% for larger benchmarks. The estimation time is in the order of milliseconds, compared with minutes for the synthesis tool. © 2006 ACM.",Compiler optimization; Reconfigurable computing; Resource estimation,
Compilation framework for code size reduction using reduced bit-width ISAs (rISAs),2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745194531&doi=10.1145%2f1124713.1124722&partnerID=40&md5=2316bf0280ce4dc4c49721d8bfb52a0c,"For many embedded applications, program code size is a critical design factor. One promising approach for reducing code size is to employ a ""dual instruction set"", where processor architectures support a normal (usually 32-bit) Instruction Set, and a narrow, space-efficient (usually 16-bit) Instruction Set with a limited set of opcodes and access to a limited set of registers. This feature however, requires compilers that can reduce code size by compiling for both Instruction Sets. Existing compiler techniques operate at the routine-level granularity and are unable to make the trade-off between increased register pressure (resulting in more spills) and decreased code size. We present a compilation framework for such dual instruction sets, which uses a profitability based compiler heuristic that operates at the instruction-level granularity and is able to effectively take advantage of both Instruction Sets. We demonstrate consistent and improved code size reduction (on average 22%), for the MIPS 32/16 bit ISA. We also show that the code compression obtained by this ""dual instruction set"" technique is heavily dependent on the application characteristics and the narrow Instruction Set itself. © 2006 ACM.",Code compression; Code generation; Codesize reduction; Compilers; Dual instruction set; Narrow bit-width instruction set; Optimization; Register pressure-based code generation; Retargetable compilers; rISA; Thumb,
Loop scheduling with timing and switching-activity minimization for VLIW DSP,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745189997&doi=10.1145%2f1124713.1124724&partnerID=40&md5=89e4c9ca03e622644768549155a05d68,"In embedded systems, high-performance DSP needs to be performed not only with high-data throughput but also with low-power consumption. This article develops an instruction-level loop-scheduling technique to reduce both execution time and bus-switching activities for applications with loops on VLIW architectures. We propose an algorithm, SAMLS (Switching-Activity Minimization Loop Scheduling), to minimize both schedule length and switching activities for applications with loops. In the algorithm, we obtain the best schedule from the ones that are generated from an initial schedule by repeatedly rescheduling the nodes with schedule length and switching activities minimization based on rotation scheduling and bipartite matching. The experimental results show that our algorithm can reduce both schedule length and bus-switching activities. Compared with the work of Lee et al. [2003], SAMLS shows an average 11.5% reduction in schedule length and an average 19.4% reduction in bus-switching activities. © 2006 ACM.",Compilers; Instruction bus optimization; Instruction scheduling; Loops; Low-power optimization; Retiming; Software pipelining; VLIW,
An algorithm for integrated pin assignment and buffer planning,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745221871&doi=10.1145%2f1080334.1080340&partnerID=40&md5=a72354fd27a421ee06e07998e31ae2b9,"The buffer block methodology has become increasingly popular as more and more buffers are needed in deep-submicron design, and it leads to many challenging problems in physical design. In this article, we present a polynomial-time exact algorithm for integrated pin assignment and buffer planning for all two-pin nets from one macro block (source block) to all other blocks of a given buffer block plan, while minimizing the total cost α · W + β · R for any positive α and β where W is the total wirelength, and R is the number of buffers. By applying this algorithm iteratively (each time, pick one block as the source block), it provides a polynomial-time algorithm for pin assignment and buffer planning for nets among multiple macro blocks. Experimental results demonstrate its efficiency and effectiveness. © 2005 ACM.",Buffer insertion; Min-cost maximum flow; Pin assignment,
Compilers for leakage power reduction,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745216419&doi=10.1145%2f1124713.1124723&partnerID=40&md5=f3fa89dfa8adb00ccf61829cf4aec440,"Power leakage constitutes an increasing fraction of the total power consumption in modern semi-conductor technologies. Recent research efforts indicate that architectures, compilers, and software can be optimized so as to reduce the switching power (also known as dynamic power) in microprocessors. This has lead to interest in using architecture and compiler optimization to reduce leakage power (also known as static power) in microprocessors. In this article, we investigate compiler-analysis techniques that are related to reducing leakage power. The architecture model in our design is a system with an instruction set to support the control of power gating at the component level. Our compiler provides an analysis framework for utilizing instructions to reduce the leakage power. We present a framework for analyzing data flow for estimating the component activities at fixed points of programs whilst considering pipeline architectures. We also provide equations that can be used by the compiler to determine whether employing power-gating instructions in given program blocks will reduce the total energy requirements. As the duration of power gating on components when executing given program routines is related to the number and complexity of program branches, we propose a set of scheduling policies and evaluate their effectiveness. We performed experiments by incorporating our compiler analysis and scheduling policies into SUIF compiler tools and by simulating the energy consumptions on Wattch toolkits. The experimental results demonstrate that our mechanisms are effective in reducing leakage power in microprocessors. © 2006 ACM.",Compilers for low power; Leakage-power reduction; Power-gating mechanisms,
Two-layer bus routing for high-speed printed circuit boards,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745204825&doi=10.1145%2f1124713.1124726&partnerID=40&md5=48793be62416661c8ea993d145131f88,"The increasing clock frequencies in high-end industrial circuits bring new routing challenges that cannot be handled by traditional algorithms. An important design automation problem for highspeed boards today is routing nets within tight minimum and maximum length bounds. In this article, we propose an algorithm for routing bus structures between components on two layers such that all length constraints are satisfied. This algorithm handles length extension simultaneously during the actual routing process so that maximum resource utilization is achieved during length extension. Our approach here is to process one track at a time, and choose the best subset of nets to be routed on each track. The algorithm we propose for single-track routing is guaranteed to find the optimal subset of nets together with the optimal solution with length extension on one track. The experimental comparison with a recently proposed technique shows the effectiveness of this algorithm both in terms of solution quality and run-time. © 2006 ACM.",Bus routing; High-speed; Min-max length constraints; PCB,
Behavioral synthesis techniques for intellectual property protection,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745218297&doi=10.1145%2f1080334.1080338&partnerID=40&md5=15ddaee430c273e17903e763ac7c7280,"We introduce dynamic watermarking techniques for protecting the value of intellectual property of CAD and compilation tools and reusable design components. The essence of the new approach is the addition of a set of design and timing constraints which encodes the author's signature. The constraints are selected in such a way that they result in a minimal hardware overhead while embedding a unique signature that is difficult to remove and forge. Techniques are applicable in conjunction with an arbitrary behavioral synthesis task such as scheduling, assignment, allocation, transformation, and template matching. On a large set of design examples, studies indicate the effectiveness of the new approach that results in signature data that is highly resilient, difficult to detect and remove, and yet is easy to verify and can be embedded in designs with very low hardware overhead. For example, the probability that the same design with the embedded signature is obtained by any other designers by themselves is less than l in 10 102, and no register overhead was incurred. The probability of tampering, the probability that part of the embedded signature can be removed by random attempts, is shown to be extremely low, and the watermark is additionally protected from such tampering with error-correcting codes. © 2005 ACM.",Behavioral synthesis; Intellectual property protection; Watermarking,
ACM Transactions on Design Automation of Electronic Systems: Introduction,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745218690&doi=10.1145%2f1109118.1109119&partnerID=40&md5=0f0f60707317bb1c6b716b35c8a86d6b,[No abstract available],,
Effective techniques for the generalized low-power binding problem,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745214610&doi=10.1145%2f1124713.1124718&partnerID=40&md5=9d645c61ae69ab6b48c745f717d5db59,"This article proposes two very fast graph theoretic heuristics for the low power binding problem given fixed number of resources and multiple architectures for the resources. First, the generalized low power binding problem is formulated as an Integer Linear Programming (ILP) problem that happens to be an NP-complete task to solve. Then two polynomial-time heuristics are proposed that provide a speedup of up to 13.7 with an extremely low penalty for power when compared to the optimal ILP solution for our selected benchmarks. © 2006 ACM.",Graph theory; High level synthesis; Low-power binding,
Zero cost indexing for improved processor cache performance,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745185077&doi=10.1145%2f1124713.1124715&partnerID=40&md5=3a95b2d627d225fcb9d91ae2bc5e6494,"The increasing use of microprocessor cores in embedded systems as well as mobile and portable devices creates an opportunity for customizing the cache subsystem for improved performance. In traditional cache design, the index portion of the memory address bus consists of the K least significant bits, where K = log2 D and D is the depth of the cache. However, in devices where the application set is known and characterized (e.g., systems that execute a fixed application set) there is an opportunity to improve cache performance by choosing a near-optimal set of bits used as index into the cache. This technique does not add any overhead in terms of area or delay. In this article, we present an efficient heuristic algorithm for selecting K index bits for improved cache performance. We show the feasibility of our algorithm by applying it to a large number of embedded system applications as well as the integer SPEC CPU 2000 benchmarks. Specifically, for data traces, we show up to 45% reduction in cache misses. Likewise, for instruction traces, we show up to 31% reduction in cache misses. When a unified data/instruction cache architecture is considered, our results show an average improvement of 14.5% for the Powerstone benchmarks and an average improvement of 15.2% for the SPEC'00 benchmarks. © 2006 ACM.",Cache optimization; Design exploration; Index hashing,
An interactive codesign environment for domain-specific coprocessors,2006,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745183293&doi=10.1145%2f1124713.1124719&partnerID=40&md5=8733803e37ba8ba2e7a3196339403145,"Energy-efficient embedded systems rely on domain-specific coprocessors for dedicated tasks such as baseband processing, video coding, or encryption. We present a language and design environment called GEZEL that can be used for the design, verification and implementation of such coprocessor-based systems. The GEZEL environment creates a platform simulator by combining a hardware simulation kernel with one or more instruction-set simulators. The hardware part of the platform is programmed in GEZEL, a deterministic, cycle-true and implementation-oriented hardware description language. GEZEL designs are scripted, allowing the hardware configuration of the platform simulator to be changed quickly without going through lengthy recompiles. For this reason, we call the environment interactive. We present the execution ladder as an optimization framework to balance interactivity against simulation speed. We demonstrate our approach using several designs including an AES encryption coprocessor and a Viterbi decoding coprocessor. We discuss the advantages of our approach as opposed to more conventional approaches using SystemC and Verilog/VHDL. © 2006 ACM.",Cosimulation; Hardware description language; Hardware-software codesign,
"An O(min(m, n)) parallel deadlock detection algorithm",2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745182873&doi=10.1145%2f1080334.1080341&partnerID=40&md5=f7822a48fa422196704937ad4e1959f7,"This article presents a novel Parallel Deadlock Detection Algorithm (PDDA) and its hardware implementation, Deadlock Detection Unit (DDU). PDDA uses simple Boolean representations of request, grant, and no activity so that the hardware implementation of PDDA becomes easier and operates faster. We prove the correctness of PDDA and that the DDU has a runtime complexity of O(min(m, n)) where m is the number of resources and n is the number of processes. The DDU reduces deadlock detection time by 99%, (i.e., 100X) or more compared to software implementations of deadlock detection algorithms. An experiment involving a practical situation with an early deadlock condition showed that the time measured from application initialization to deadlock detection was reduced by 46% by employing the DDU as compared to detecting deadlock in software. © 2005 ACM.",Deadlock detection,
Test chip experimental results on high-level structural test,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745217784&doi=10.1145%2f1109118.1109125&partnerID=40&md5=89b499f2fda42d69e61f80cb81744743,"Using complex (high-level) gates, such as multiplexers, full adders, etc., for automatic test pattern generation (ATPG) has several advantages. It makes ATPG faster and potentially reduces the size of the test set that needs to be applied. A variety of other techniques are used to reduce the size of test sets for digital chips. They typically rely on preserving the single-stuck-fault coverage of the test set. This article presents data obtained from applying a variety of test sets on the BLF35 test chip and recording the test escapes. The data presented show the test quality effect of using complex gates as fault sites. The article also shows the impact of test compaction and reduced fault coverage on the test quality. © 2005 ACM.",Complex gates; Structural test; Test experiment; VLSI test,
Algorithmic aspects of hardware/software partitioning,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544446741&doi=10.1145%2f1044111.1044119&partnerID=40&md5=1daf7874ae986a70fe407f55b491f0a2,"One of the most crucial steps in the design of embedded systems is hardware/software partitioning, that is, deciding which components of the system should be implemented in hardware and which ones in software. Most formulations of the hardware/software partitioning problem are NP-hard, so the majority of research efforts on hardware/software partitioning has focused on developing efficient heuristics. This article considers the combinatorial structure behind hardware/software partitioning. Two similar versions of the partitioning problem are defined, one of which turns out to be NP-hard, whereas the other one can be solved in polynomial time. This helps in understanding the real cause of complexity in hardware/software partitioning. Moreover, the polynomial-time algorithm serves as the basis for a highly efficient novel heuristic for the NP-hard version of the problem. Unlike general-purpose heuristics such as genetic algorithms or simulated annealing, this heuristic makes use of problem-specific knowledge, and can thus find high-quality solutions rapidly. Moreover, it has the unique characteristic that it also calculates lower bounds on the optimum solution. It is demonstrated on several benchmarks and also large random examples that the new algorithm clearly outperforms other heuristics that are generally applied to hardware/software partitioning. © 2005 ACM.",Graph algorithms; Graph bipartitioning; Hardware/software codesign; Hardware/software partitioning; Optimization,
Energy-efficient datapath scheduling using multiple voltages and dynamic clocking,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544443870&doi=10.1145%2f1059876.1059883&partnerID=40&md5=4ae35d8041540641bf917a371dac9a06,"Recently, dynamic frequency scaling has been explored at the CPU and system levels for power optimization. Low-power datapath scheduling using multiple supply voltages has been well researched. In this work, we develop new datapath scheduling algorithms that use multiple supply voltages and dynamic frequency clocking in a coordinated manner in order to reduce the energy consumption of datapath circuits. In dynamic frequency clocking, the functional units can be operated at different frequencies depending on the computations occurring within the datapath during a given clock cycle. The strategy is to schedule high-energy units, such as multipliers at lower frequencies, so that they can be operated at lower voltages to reduce energy consumption and the low-energy units, such as adders at higher frequencies, to compensate for speed. The proposed time- and resource-constrained algorithms have been applied to various high-level synthesis benchmark circuits under different time and resource constraints. The experimental results show significant reduction in energy for both the algorithms. © 2005 ACM.",Dynamic frequency clocking; High-level synthesis; Low-power datapath synthesis; Multiple voltage scheduling; Resource-constrained scheduling; Time-constrained scheduling,
ACM transactions on design automation of electronic systems: Editorial,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544446866&doi=10.1145%2f1044111.1044112&partnerID=40&md5=30c50191192e17df60194402c10afd3e,[No abstract available],,
Energy-aware variable partitioning and instruction scheduling for multibank memory architectures,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544441853&doi=10.1145%2f1059876.1059885&partnerID=40&md5=4a1ff6bc245a9829e418c98f65d839fa,"Many high-end DSP processors employ both multiple memory banks and heterogeneous register files to improve performance and power consumption. The complexity of such architectures presents a great challenge to compiler design. In this article, we present an approach for variable partitioning and instruction scheduling to maximally exploit the benefits provided by such architectures. Our approach is built on a novel graph model which strives to capture both performance and power demands. We propose an algorithm to iteratively find the variable partition such that the maximum energy saving is achieved while satisfying the given performance constraint. Experimental results demonstrate the effectiveness of our approach. © 2005 ACM.",Instruction scheduling; Multiple memory banks; Nonorthogonal architecture; Operating mode; Parallelism and serialism balance; Runtime and energy saving tradeoff,
A detailed power model for field-programmable gate arrays,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544455212&doi=10.1145%2f1059876.1059881&partnerID=40&md5=58a895d03e5128a7df136437977a16dc,"Power has become a critical issue for field-programmable gate array (FPGA) vendors. Understanding the power dissipation within FPGAs is the first step in developing power-efficient architectures and computer-aided design (CAD) tools for FPGAs. This article describes a detailed and flexible power model which has been integrated in the widely used Versatile Place and Route (VPR) CAD tool. This power model estimates the dynamic, short-circuit, and leakage power consumed by FPGAs. It is the first flexible power model developed to evaluate architectural tradeoffs and the efficiency of power-aware CAD tools for a variety of FPGA architectures, and is freely available for noncommercial use. The model is flexible, in that it can estimate the power for a wide variety of FPGA architectures, and it is fast, in that it does not require extensive simulation, meaning it can be used to explore a large architectural space. We show how the model can be used to investigate the impact of various architectural parameters on the energy consumed by the FPGA, focusing on the segment length, switch block topology, lookuptable size, and cluster size. © 2005 ACM.",Architecture; Power consumption; Power estimation model; Sensitivity analysis,
XFM: An incremental methodology for developing formal models,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745203003&doi=10.1145%2f1109118.1109120&partnerID=40&md5=9898b58be7ed906a70d6b41ecf92b23b,"We present an agile formal methodology named eXtreme Formal Modeling (XFM), based on Extreme Programming (XP) concepts to construct abstract models from natural language specifications of complex systems. In particular, we focus on Prescriptive Formal Models (PFMs) that capture the specification of the system under design in a mathematically precise manner. Such models can be used as golden reference models for formal verification, test generation, coverage monitor generation, etc. This methodology for incrementally building PFMs works by adding user stories expressed as LTL formulae gleaned from the natural language specifications, one by one, into the model. XFM builds the models, retaining correctness with respect to incrementally added properties by regressively model-checking all the LTL properties captured theretofore in the model. We illustrate XFM with a graded set of examples consisting of a traffic light controller and a DLX pipeline. To make the regressive model-checking steps feasible with current model-checking tools, we need to control the model size increments at each subsequent step in the process. We therefore analyze the effects of ordering the LTL properties in XFM on the statespace growth rate of the model. We compare three different property-ordering methodologies: ad hoc ordering, property-based ordering, and predicate-based ordering. We experiment on the models of the ISA bus monitor and the arbitration phase of the Pentium Pro bus. We experimentally show and mathematically reason that the predicate-based ordering is the best among these orderings. Finally, we present a GUI-based toolbox that we implemented to build PFMs using XFM. © 2005 ACM.",Extreme formal modeling; Extreme programming; Formal specification; Formal verification; Prescriptive formal models; Property ordering; Property refactoring; SMV; SPIN,
The open family of temporal logics: Annotating temporal operators with input constraints,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745187349&doi=10.1145%2f1080334.1080337&partnerID=40&md5=ae1a6dc1d40a749a1ac16325314f97f8,"Assume-guarantee style verification of modules relies on the appropriate modeling of the interaction of the module with its environment. Popular temporal logics such as Computation Tree Logic (CTL) and Linear Temporal Logic (LTL) that were originally defined for closed systems (Kripke structures) do not make any syntactic discrimination between input and output variables. As a result, these logics and their recent derivatives (such as System Verilog, Sugar, Forspec, etc) permit the specification of properties that have some semantic problems when interpreted over open systems or modules. These semantic problems are quite common in practice, but are computationally hard to detect within a given specification. In this article, we propose a new style for writing temporal specifications of open systems that helps the designer to avoid most of these problems. In the proposed style, the basic temporal operators (such as next and until) are annotated with assume constraints over the input variables. We formalize this style through an extension of LTL, namely Open-LTL and an extension of CTL with fairness, called Open-CTL. We show that this simple syntactic separation between the assume and the guarantee achieves the desired results. We show that the proposed style can be integrated with traditional symbolic model-checking techniques and present a complete tool for the verification of Verilog RTL modules in isolation. © 2005 ACM.",Verification,
A framework for systematic validation and debugging of pipeline simulators,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745184865&doi=10.1145%2f1080334.1080336&partnerID=40&md5=f7717c39b6269830251aa0b11d85e688,"Microprocessor pipeline simulation at the system level is an extremely important activity in the architecture exploration process. In this article, we address the problem of validating and debugging a pipeline simulator from the specific perspective of instruction scheduling. We propose a general framework for a systematic validation process and show that the assumptions made are justified for most standard pipeline models. The framework does not need any formal specification of the pipeline logic and hence can be readily integrated into the simulation and iteration-based architectural design space exploration process. We propose a concept of semantic equivalence between two simulations called D* equivalence which effectively captures the dataflow between instructions through registers. We then proceed to propose an algorithm which decides this equivalence in time polynomial in the number of instructions executed and the number of registers. We implement the algorithm and demonstrate how the framework facilitates debugging. © 2005 ACM.",Dataflow equivalence; Design space exploration; Instruction scheduling; Pipeline validation; Pipelined architectures; Simulation-based verification,
High-level modeling and simulation of single-chip programmable heterogeneous multiprocessors,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745209889&doi=10.1145%2f1080334.1080335&partnerID=40&md5=6539d385a97a33ae517feaf57fc3ddff,"Heterogeneous multiprocessing is the future of chip design with the potential for tens to hundreds of programmable elements on single chips within the next several years. These chips will have heterogeneous, programmable hardware elements that lead to different execution times for the same software executing on different resources as well as a mix of desktop-style and embedded-style software. They will also have a layer of programming across multiple programmable elements forming the basis of a new kind of programmable system which we refer to as a Programmable Heterogeneous Multiprocessor (PHM). Current modeling approaches use instruction set simulation for performance modeling, but this will become far too prohibitive in terms of simulation time for these larger designs. The fundamental question is what the next higher level of design will be. The high-level modeling, simulation and design required for these programmable systems poses unique challenges, representing a break from traditional hardware design. Programmable systems, including layered concurrent software executing via schedulers on concurrent hardware, are not characterizable with traditional component-based hierarchical composition approaches, including discrete event simulation. We describe the foundations of our layered approach to modeling and performance simulation of PHMs, showing an example design space of a network processor explored using our simulation approach. © 2005 ACM.",Computer-aided design; Heterogeneous multiprocessors; Performance modeling; Schedulers; System modeling,
An event-based monitoring service for networks on chip,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745183091&doi=10.1145%2f1109118.1109126&partnerID=40&md5=a3085e52ee6db81fdf586f2c1fc8436f,"Networks on chip (NoCs) are a scalable interconnect solution for multiprocessor systems on chip. We propose a generic reconfigurable online event-based NoC monitoring service, based on hardware probes attached to NoC components, offering run-time observability of NoC behavior and supporting system-level debugging. We present a probe architecture, its programming model, traffic management strategies, and a cost analysis. We prove feasibility via a prototype implementation for the Æthereal NoC. Two MPEG NoC examples show that the monitoring service area, without advanced optimizations, is 17-24% of the NoC area. Two realistic monitoring examples show that monitoring traffic is several orders of magnitude lower than the 2GB/s/link raw bandwidth. © 2005 ACM.",Debugging; Monitoring; Networks-on-Chip,
A scheduling algorithm for optimization and early planning in high-level synthesis,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544439570&doi=10.1145%2f1044111.1044115&partnerID=40&md5=59d922aa3d7e5cec20dba46bcaea36cd,"Complexities of applications implemented on embedded and programmable systems grow with the advances in capacities and capabilities of these systems. Mapping applications onto them manually is becoming a very tedious task. This draws attention to using high-level synthesis within design flows. Meanwhile, it is essential to provide a flexible formulation of optimization objectives as well as to perform efficient planning for various design objectives early on in the design flow. In this work, we address these issues in the context of data flow graph (DFG) scheduling, which is an essential element within the high-level synthesis flow. We present an algorithm that schedules a chain of operations with data dependencies among consecutive operations at a single step. This local problem is repeated to generate the schedule for the whole DFG. The local problem is formulated as a maximum weight noncrossing bipartite matching. We use a technique from the computational geometry domain to solve the matching problem. This technique provides a theoretical guarantee on the solution quality for scheduling a single chain of operations. Although still being local, this provides a relatively wider perspective on the global scheduling objectives. In our experiments we compared the latencies obtained using our algorithm with the optimal latencies given by the exact solution to the integer linear programming (ILP) formulation of the problem. In 9 out of 14 DFGs tested, our algorithm found the optimal solution, while generating latencies comparable to the optimal solution in the remaining five benchmarks. The formulation of the objective function in our algorithm provides flexibility to incorporate different optimization goals. We present examples of how to exploit the versatility of our algorithm with specific examples of objective functions and experimental results on the ability of our algorithm to capture these objectives efficiently in the final schedules. © 2005 ACM.",Bipartite matching; Data flow graph; High-level synthesis; Scheduling,
Using 2-domain partitioned OBDD data structure in an enhanced symbolic simulator,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745210956&doi=10.1145%2f1109118.1109122&partnerID=40&md5=eef887926d5e9bc49bf8f7c84c2e19c5,"In this article, we propose a symbolic simulation method where Boolean functions can be efficiently manipulated through a 2-domain partitioned OBDD data structure. The functional partition is applied by automatically exploring the key decision points implicitly built inside a circuit. The partition can help to significantly reduce the OBDD sizes, solving problems that could not be solved with monolithic OBDD data structure. We demonstrate the performance of the approach through the symbolic simulation of several benchmark circuits with complex control logics and datapath. The symbolic simulation based on 2-domain partitioned OBDD can be also applied in equivalence checking. It can generate the signature of functions to identify the critical partition points in the optimized gate-level netlist. © 2005 ACM.",Equivalence checking; Formal verification; Symbolic simulation,
Scheduling and optimal register placement for synchronous circuits derived using software pipelining techniques,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544440562&doi=10.1145%2f1059876.1059877&partnerID=40&md5=fa0418f7ebe1a86b5177be055247e001,"Data dependency constraints constitute a lower bound P on the minimal clock period of single-phase clocked sequential circuits. In contrast to methods based on basic retiming, clocked sequential circuits with clock period P can always be obtained using software pipelining techniques. Such circuits can be derived by any method that can be framed in the following four-step process: Step 1, determine P; Step 2, compute a valid periodic schedule of the computational elements; Step 3, place registers back to the circuit; Step 4, assign the clock signals to control registers. Methods with polynomial run-time to implement this process are proposed in the literature. They implement these steps sequentially, starting with Step 1. These methods do not know how to optimally place registers which leads to an unnecessary number of registers. In this article, we address the problem of how to simultaneously implement Steps 2 and 3 in order to minimize the total number of registers. We conjecture that the problem is NP-hard in its general form. We formulate the problem for the first time in the literature, and devise a Mixed Integer Linear Program (MILP) to solve it. From this MILP, we derive a linear program to determine approximate solutions to the problem for large general circuits. We show that the proposed approach can handle nonzero clock skew. Experimental results confirm the effectiveness of the approach and show that significant reductions of the number of registers can be obtained although register sharing is not used. When the schedule is given, the proposed approach provides solutions to the problem of how to place the minimal number of registers in Step 3. © 2005 ACM.",Clock; Multiphase; Retiming; Sequential circuit; Software pipelining,
Bipartitioning and encoding in low-power pipelined circuits,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544443434&doi=10.1145%2f1044111.1044114&partnerID=40&md5=628f961b4c3efc424c965d17663039ca,"In this article, we present a bipartition dual-encoding architecture for low-power pipelined circuits. We exploit the bipartition approach as well as encoding techniques to reduce power dissipation not only of combinational logic blocks but also of the pipeline registers. Based on Shannon expansion, we partition a given circuit into two subcircuits such that the number of different outputs of both subcircuits are reduced, and then encode the output of both subcircuits to minimize the Hamming distance for transitions with a high switching probability. We measure the benefits of four different combinational bipartitioning and encoding architectures for comparison. The transistor-level simulation results show that bipartition dual-encoding can effectively reduce power by 72.7% for the pipeline registers and 27.1% for the total power consumption on average. To the best of our knowledge, it is the first work that presents an in-depth study on bipartition and encoding techniques to optimize power for pipelined circuits. © 2005 ACM.",Low-power design,
Voltage scheduling under unpredictabilities: A risk management paradigm,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544449395&doi=10.1145%2f1059876.1059884&partnerID=40&md5=57eef97e7cefd7b20258ea3a09106570,"This article addresses the problem of voltage scheduling in unpredictable situations. The voltage scheduling problem assigns voltages to operations such that the power is minimized under a clock delay constraint. In the presence of unpredictabilities, meeting the clock latency constraint cannot be guaranteed. This article proposes a novel risk management based technique to solve this problem. Here, the risk management paradigm assigns a quantified value to the amount of risk the designer is willing to take on the clock cycle constraint. The algorithm then assigns voltages in order to meet the expected value of clock cycle constraint while keeping the maximum delay within the specified ""risk"" and minimizing the power. The proposed algorithm is based on dynamic programming and is optimal for trees. Experimental results show that the traditional voltage scheduling approach is incapable of handling unpredictabilities. Our approach is capable of generating an effective tradeoff between power and ""risk"": the more the risk, the less the power. The results show that a small increase in design risk positively affects the power dissipation. © 2005 ACM.",Design closure; Low power; Predictability; Voltage scheduling,
Synthesis of skewed logic circuits,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544448195&doi=10.1145%2f1059876.1059878&partnerID=40&md5=c69779d7fb7c6828b853ba4b47c5c765,"Skewed logic circuits belong to a noise-tolerant high-performance static circuit family. Skewed logic circuits can achieve performance comparable to that of Domino logic circuits but with much lower power consumption. Two factors contribute to the reduction in power. First, by exploiting the static nature of skewed logic circuits, we can alleviate the cost of logic duplication which is typically required to overcome the logic reconvergence problem in both Domino logic and skewed logic circuits. Second, a selective clocking scheme can be applied to a skewed logic circuit to reduce the clock load and hence, clock power. In this article, we propose a two-step synthesis scheme of skewed logic circuits. In the first step, an integer linear programming-based approach is presented to overcome the logic reconvergence problem in skewed logic circuits with minimal logic duplication cost. In the second step, a dynamic programming-based heuristic is applied to achieve an optimal selective clocking scheme. Experimental results show that the average power saving of skewed logic circuits over Domino logic circuits is 41.1%. © 2005 ACM.",Optimization; Power; Skewed logic; Synthesis,
Simplifying the design and automating the verification of pipelines with structural hazards,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745187309&doi=10.1145%2f1109118.1109123&partnerID=40&md5=641bfe4d5c54bbf7afd2c5bfb23ae05e,"This article describes a technique that simplifies the design of pipelined circuits automates the specification and verification of structural-hazard and datapath correctness properties for pipelined circuits. The technique is based upon a template for pipeline stages, a control-circuit cell library, a decomposition of structural hazard and datapath correctness into a collection of simple properties, and a prototype design tool that generates verification scripts for use by external tools. Our case studies include scalar and superscalar implementations of a 32-bit OpenRISC integer microprocessor. © 2005 ACM.",Design automation; Formal design verification; Pipelined circuits,
Equivalence checking between behavioral and RTL descriptions with virtual controllers and datapaths,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745183745&doi=10.1145%2f1109118.1109121&partnerID=40&md5=dcf53facba89d5c1dd65f144e3ab0bb8,"In this article, we present techniques for comparison between behavioral level and register transfer level (RTL) design descriptions by mapping the designs into virtual controllers and virtual datapaths. We also discuss about how the equivalence between behavioral level and RTL designs can be defined precisely using the proposed ""attribute statements"" in an interactive fashion. Implementation issues as well as considerations on real life industrial design examples are also presented. © 2005 ACM.",Behavior synthesis; Equivalence checking; Formal verification; High-level synthesis,
Instruction-level test methodology for CPU core self-testing,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745183746&doi=10.1145%2f1109118.1109124&partnerID=40&md5=4fccfe013c827978dcbfbc94f7ec846c,"TIS is an instruction-level methodology for processor core self-testing that enhances instruction set of a CPU with test instructions. Since the functionality of test instructions is the same as the NOP instruction, NOP instructions can be replaced with test instructions. Online testing can be accomplished without any performance penalty. TIS tests different parts of the processor and detects stuck-at faults. This method can be employed in offline and online testing of single-cycle, multicycle and pipelined processors. But, TIS is more appropriate for online testing of pipelined architectures in which NOP instructions are frequently executed because of data, control and structural hazards. Running test instructions instead of these NOP instructions, TIS utilizes the time that is otherwise wasted by NOPs. In this article, two different implementations of TIS are presented. One implementation employs a dedicated hardware modules for test vector generation, while the other is a software-based approach that reads test vectors from memory. These two approaches are implemented on a pipelined processor core and their area overheads are compared. To demonstrate the appropriateness of the TIS test technique, several programs are executed and fault coverage results are presented. © 2005 ACM.",BIST; CPU core testing; Instruction level testing; Pipelined processor; Software-based self testing; Test instruction set,
Technology mapping and architecture evalution for k/m-macrocell-based FPGAs,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544444390&doi=10.1145%2f1044111.1044113&partnerID=40&md5=2eec86614ca4e41dd3f1ab7619ed2608,"In this article, we study the technology mapping problem for a novel field-programmable gate array (FPGA) architecture that is based on k-input single-output programmable logic array- (PLA-) like cells, or, k / m-macrocells. Each cell in this architecture can implement a single output function of up to k inputs and up to m product terms. We develop a very efficient technology mapping algorithm, k_m_flow, for this new type of architecture. The experimental results show that our algorithm can achieve depth-optimality on almost all the testcases in a set of 16 Microelectronics Center of North Carolina (MCNC) benchmarks. Furthermore it is shown that on this set of benchmarks, with only a relatively small number of product terms (m ≤ k + 3), the k / m-macrocell-based FPGAs can achieve the same or similar mapping depth compared with the traditional k -input single-output lookup table- (k-LUT-) based FPGAs. We also investigate the total area and delay of k / m-macrocell-based FPGAs and compare them with those of the commonly used 4-LUT-based FPGAs. The experimental results show that k / m-macrocell-based FPGAs can outperform 4-LUT-based FPGAs in terms of both delay and area after placement and routing by VPR on this set of benchmarks. © 2005 ACM.",CPLD; FPGA; PLD; Technology mapping,
Routing-aware scan chain ordering,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745202821&doi=10.1145%2f1080334.1080339&partnerID=40&md5=65064381606e66afb01cfe19e120e406,"Scan chain insertion can have a large impact on routability, wirelength, and timing of the design. We present a routing-driven methodology for scan chain ordering with minimum wirelength objective. A routing-based approach to scan chain ordering, while potentially more accurate, can result in TSP (Traveling Salesman Problem) instances which are asymmetric and highly nonmetric; this may require a careful choice of solvers. We evaluate our new methodology on recent industry place-and-route blocks with 1200 to 5000 scan cells. We show substantial wirelength reductions for the routing-based flow versus the traditional placement-based flow. In a number of our test cases, over 86% of scan routing overhead is saved. Even though our experiments are, so far, timing oblivious, the routing-based flow also improves evaluated timing, and practical timing-driven extensions appear feasible. © 2005 ACM.",Layout; Scan chain; Testing,
An efficient algorithm for finding the minimal-area FPGA technology mapping,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544451910&doi=10.1145%2f1044111.1044121&partnerID=40&md5=3549a830944dc60330071ae1445866e3,"Minimum area is one of the important objectives in technology mapping for lookup table-based field-progrmmable gate arrays (FPGAs). Although there is an algorithm that can find an optimal solution in polynomial time for the minimal-area FPGA technology mapping problem without gate duplication, its time complexity can grow exponentially with the number of inputs of the lookup-tables. This article proposes an algorithm with approximate to the area-optimal solution and lower time complexity. The time complexity of this algorithm is proven theoretically to be bounded by O(n 3), where n is the total number of gates in the given circuit. It is shown that except for some cases the proposed algorithm can find an optimal solution of a given problem. We have combined the proposed algorithm with the existing postprocessing procedures which are used to find the gates that can be duplicated on a set of benchmark examples. The experimental results demonstrate the effectiveness of our algorithm. © 2005 ACM.",Greedy method; Logic synthesis; Partition; Technology mapping,
A 4-Geometry Maze Router and Its Application on Multiterminal Nets,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27644434427&doi=10.1145%2f1044111.1044118&partnerID=40&md5=fc22c8c0b3924bd0a415d9b902a35670,"The maze routing problem is to find an optimal path between a given pair of cells on a grid plane. Lee's algorithm and its variants, probably the most widely used maze routing method, fails to work in the 4-geometry of the grid plane. Our algorithm solves this problem by using a suitable data structure for uniform wave propagation in the 4-geometry, 8-geometry, etc. The algorithm guarantees finding an optimal path if it exists and has linear time and space complexities. Next, to solve the obstacle-avoiding rectilinear and 4-geometry Steiner tree problems, a heuristic algorithm is presented. The algorithm utilizes a cost accumulation scheme based on the maze router to determine the Torricelli vertices (points) for improving the quality of multiterminal nets. Our experimental results show that the algorithm works well in practice. Furthermore, using the 4-geometry router, path lengths can be significantly reduced up to 12% compared to those in the rectilinear router. © 2005 ACM.",λ-geometry; Cell map; Maze router; Steiner tree,
Optimized wafer-probe and assembled package test design for analog circuits,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544434098&doi=10.1145%2f1059876.1059882&partnerID=40&md5=8d167922e46c915c4e530bdc8be84bad,"It is well known that wafer-probe test costs of analog ICs are an order of magnitude less than the corresponding test costs of assembled packages. It is therefore natural to push as much of the testing process into wafer-probe testing as possible to reduce the scope of assembled package testing. However, the signal drive and response observation capabilities during wafer probe testing are limited in comparison to assembled packages. In this article, it is shown that by using band-limited transient test signals, which can be supported by wafer-probe test instrumentation, significant numbers of bad ICs can be detected early during the wafer-probe test. The optimal test stimuli are determined by cooptimizing the wafer-probe and assembled package test waveforms. Overall test costs, including the cost of packaging bad ICs, are minimized and are reduced up to four times. The proposed method has been validated using hardware test data, which were obtained through measurements made on a prototype. © 2005 ACM.",Analog and mixed-signal test; Assembled package; Co-optimization; Prototype; Simulation; Test; Test cost minimization; Test generation and co-optimization; Wafer-probe,
Combinatorial techniques for mixed-size placement,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-29144472522&doi=10.1145%2f1044111.1044116&partnerID=40&md5=9f9fb181d55cba2d768b91d9696a9365,"While recent literature on circuit layout addresses large-scale standard-cell placement, the authors typically assume that all macros are fixed. Floorplanning techniques are very good at handling macros, but do not scale to hundreds of thousands of placeable objects. Therefore we combine floorplanning techniques with placement techniques to solve the more general placement problem. Our work shows how to place macros consistently with large numbers of small standard cells. Proposed techniques can also be used to guide circuit designers who prefer to place macros by hand. We address the computational difficulty of layout problems involving large macros and numerous small logic cells at the same time. Proposed algorithms are evaluated in the context of wirelength minimization because a computational method that is not scalable in optimizing wirelength is unlikely to be successful for more complex objectives (congestion, delay, power, etc.) We propose several different design flows to place mixed-size placement instances. The first flow relies on an arbitrary black-box standard-cell placer to obtain an initial placement and then removes possible overlaps using a fixed-outline floorplanner. This results in valid placements for macros, which are considered fixed. Remaining standard cells are then placed by another call to the standard-cell placer. In the second flow a standard-cell placer generates an initial placement and a force-directed placer is used in the engineering change order (ECO) mode to generate an overlap-free placement. Empirical evaluation on ibm benchmarks shows that in most cases our proposed flows compare favorably with previously published mixed-size placers, Kraftwerk, and the mixed-size floor-placer proposed at the 2003 Conference on Design, Automation, and Test in Europe (DATE 2003), and are competitive with mPG-MS. © 2005 ACM.",Floorplanning; Placement; VLSI,
RL-huffman encoding for test compression and power reduction in scan applications,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23744509738&doi=10.1145%2f1044111.1044117&partnerID=40&md5=99c2a6ca45060ebc09a36527ca514fdd,"This article mixes two encoding techniques to reduce test data volume, test pattern delivery time, and power dissipation in scan test applications. This is achieved by using run-length encoding followed by Huffman encoding. This combination is especially effective when the percentage of don't cares in a test set is high, which is a common case in today's large systems-on-chips (SoCs). Our analysis and experimental results confirm that achieving up to an 89% compression ratio and a 93% scan-in power reduction is possible for scan-testable circuits such as ISCAS89 benchmarks. © 2005 ACM.",Compression ratio; Decompression; Huffman encoding; Power reduction; Run-length encoding; Scan applications; Scan-in test power; Switching activities; Test compression; Test pattern compression,
Large-scale circuit placement,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544433363&doi=10.1145%2f1059876.1059886&partnerID=40&md5=e278e558ff3f193d540ed513906a3e2c,"Placement is one of the most important steps in the RTL-to-GDSII synthesis process, as it directly defines the interconnects, which have become the bottleneck in circuit and system performance in deep submicron technologies. The placement problem has been studied extensively in the past 30 years. However, recent studies show that existing placement solutions are surprisingly far from optimal. The first part of this tutorial summarizes results from recent optimality and scalability studies of existing placement tools. These studies show that the results of leading placement tools from both industry and academia may be up to 50% to 150% away from optimal in total wirelength. If such a gap can be closed, the corresponding performance improvement will be equivalent to several technology-generation advancements. The second part of the tutorial highlights the recent progress on large-scale circuit placement, including techniques for wirelength minimization, routability optimization, and performance optimization. © 2005 ACM.",Large-scale optimization; Optimality; Placement; Scalability,
Optimizing instruction TLB energy using software and hardware techniques,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544433247&doi=10.1145%2f1059876.1059879&partnerID=40&md5=8ab2b79ec30ff798c57bf4c8c8b0e1d0,"Power consumption and power density for the Translation Look-aside Buffer (TLB) are important considerations not only in its design, but can have a consequence on cache design as well. After pointing out the importance of instruction TLB (iTLB) power optimization, this article embarks on a new philosophy for reducing the number of accesses to this structure. The overall idea is to keep a translation currently being used in a register and avoid going to the iTLB as far as possible - until there is a page change. We propose four different approaches for achieving this, and experimentally demonstrate that one of these schemes that uses a combination of compiler and hardware enhancements can reduce iTLB dynamic power by over 85% in most cases. The proposed approaches can work with different instruction-cache (iL1) lookup mechanisms and achieve significant iTLB power savings without compromising on performance. Their importance grows with higher iL1 miss rates and larger page sizes. They can work very well with large iTLB structures that can possibly consume more power and take longer to lookup, without the iTLB getting into the common case. Further, we also experimentally demonstrate that they can provide performance savings for virtually indexed, virtually tagged iL1 caches, and can even make physically indexed, physically tagged iL1 caches a possible choice for implementation. © 2005 ACM.",Cache design; Compiler optimization; Instruction locality; Power consumption; Translation look-aside buffer,
A unified method for phase shifter computation,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17444391207&doi=10.1145%2f1044111.1044120&partnerID=40&md5=7ecd587f39a48e0c01a103cd59d137af,"Phase shifters are used to shift the bit sequences produced by the successive stages of a built-in test pattern generator (TPG) based on a linear finite state machine (LPSM) by a specified amount (phase shift) relative to the characteristic sequence. An upper bound on the number of taps to be used for each phase shifter and a lower bound on the phase-shift value between successive stages of the TPG mechanism are the general parameters of the problem. Methods to design such phase shifters have been given in the past separately for Type-1 LFSRs, Type-2 LFSRs, and three-neighborhood cellular automata. In this article, we show how phase shifters can be synthesized uniformly and efficiently for any LFSM, including the aforementioned ones. We demonstrate the method by showing how to obtain phase shifters for two-dimensional cellular automata and for ring generators © 2005 ACM.",Additional key words and phrases:Test pattern generation (TPG); Built-in self-test (BIST); Cellular automata; Linear feedback shift registers; Linear finite state machines; Phase shifters,
Efficient techniques for transition testing,2005,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544431649&doi=10.1145%2f1059876.1059880&partnerID=40&md5=6458184a0954884bea8ed056998bb9bb,"Scan-based transition tests are added to improve the detection of speed failures in sequential circuits. Empirical data suggests that both data volume and application time will increase dramatically for such transition testing. Techniques to address the above problem for a class of transition tests, called enhanced transition tests, are proposed in this article. The first technique, which combines the proposed transition test chains with the ATE repeat capability, reduces test data volume by 46.5% when compared with transition tests computed by a commercial transition test ATPG tool. However, the test application time may sometimes increase. To address the test time issue, a new DFT technique, Exchange Scan, is proposed. Exchange scan reduces both data volume and application time by 46.5%. These techniques rely on the use of hold-scan cells and highlight the effectiveness of hold-scan design to address test time and test data volume issues. In addition, we address the problem of yield loss due to incidental overtesting of functionally-untestable transition faults, and we formulate an efficient adjustment to the algorithm to keep the overtest ratio low. Our experimental results show that up to 14.5% reduction in overtest ratio can be achieved, with an average overtest reduction of 4.68%. © 2005 ACM.",Test application time reduction; Test chain; Test data volume reduction; Transition faults; Yield loss,
Achieving high encoding efficiency with partial dynamic LFSR reseeding,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30744475962&doi=10.1145%2f1027084.1027089&partnerID=40&md5=3534bf11a87dcd9f937aecac795c5995,"Previous forms of LFSR reseeding have been static (i.e., test application is stopped while each seed is loaded) and have required full reseeding (i.e., the length of the seed is equal to the length of the LFSR). A new form of LFSR reseeding is described here that is dynamic (i.e., the seed is incrementally modified while test application proceeds) and allows partial reseeding (i.e. length of the seed is less than that of the LFSR). In addition to providing better encoding efficiency, partial dynamic LFSR reseeding has a simpler hardware implementation than previous schemes based on multiple-polynomial LFSRs. © 2004 ACM.",Built-in self-test; Compression; Linear finite shift register; Reseeding,
Coordinated parallelizing compiler optimizations and high-level synthesis,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30744468841&doi=10.1145%2f1027084.1027087&partnerID=40&md5=4a565dbbc491460d5ec715a7a464075f,"We present a high-level synthesis methodology that applies a coordinated set of coarse-grain and fine-grain parallelizing transformations. The transformations are applied both during a presynthesis phase and during scheduling, with the objective of optimizing the results of synthesis and reducing the impact of control flow constructs on the quality of results. We first apply a set of source level presynthesis transformations that include common sub-expression elimination (CSE), copy propagation, dead code elimination and loop-invariant code motion, along with more coarselevel code restructuring transformations such as loop unrolling. We then explore scheduling techniques that use a set of aggressive speculative code motions to maximally parallelize the design by re-ordering, speculating and sometimes even duplicating operations in the design. In particular, we present a new technique called ""Dynamic CSE"" that dynamically coordinates CSE and code motions such as speculation and conditional speculation during scheduling. We implemented our parallelizing high-level synthesis in the SPARK framework. This framework takes a behavioral description in ANSI-C as input and generates synthesizable register-transfer level VHDL. Our results from computationally expensive portions of three moderately complex design targets, namely, MPEG-1, MPEG-2 and the GIMP image processing tool, validate the utility of our approach to the behavioral synthesis of designs with complex control flows. © 2004 ACM.",,
Cache optimization for embedded processor cores: An analytical approach,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30744453549&doi=10.1145%2f1027084.1027086&partnerID=40&md5=5db7dfe87a6688252387f3fd506c48a6,"Embedded microprocessor cores are increasingly being used in embedded and mobile devices. The software running on these embedded microprocessor cores is often a priori known; thus, there is an opportunity for customizing the cache subsystem for improved performance. In this work, we propose an efficient algorithm to directly compute cache parameters satisfying desired performance criteria. Our approach avoids simulation and exhaustive exploration, and, instead, relies on an exact algorithmic approach. We demonstrate the feasibility of our algorithm by applying it to a large number of embedded system benchmarks. © 2004 ACM.",Cache optimization; Core-based design; Design space exploration; System-on-a-chip,
Experimental analysis of the fastest optimum cycle ratio and mean algorithms,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30744446734&doi=10.1145%2f1027084.1027085&partnerID=40&md5=cbf12c502fa630bf1800bcf55e30d9c1,"Optimum cycle ratio (OCR) algorithms are fundamental to the performance analysis of (digital or manufacturing) systems with cycles. Some applications in the computer-aided design field include cycle time and slack optimization for circuits, retiming, timing separation analysis, and rate analysis. There are many OCR algorithms, and since a superior time complexity in theory does not mean a superior time complexity in practice, or vice-versa, it is important to know how these algorithms perform in practice on real circuit benchmarks. A recent published study experimentally evaluated almost all the known OCR algorithms, and determined the fastest one among them. This article improves on that study in the following ways: (1) it focuses on the fastest OCR algorithms only; (2) it provides a unified theoretical framework and a few new results; (3) it runs these algorithms on the largest circuit benchmarks available; (4) it compares the algorithms in terms of many properties in addition to running times such as operation counts, convergence behavior, space requirements, generality, simplicity, and robustness; (5) it analyzes the experimental results using statistical techniques and provides asymptotic time complexity of each algorithm in practice; and (6) it provides clear guidance to the use and implementation of these algorithms together with our algorithmic improvements. © 2004 ACM.",Cycle mean; Cycle period; Cycle ratio; Cycle time; Data flow graphs; Discrete event systems; Experimental analysis; Iteration bound; System performance analysis,
Reusing an on-chip network for the test of core-based systems,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30744455761&doi=10.1145%2f1027084.1027088&partnerID=40&md5=108ff77e51b1faaf01dac05f10ff522e,"Networks-on-chip are likely to become the main communication platform of systems-on-chip. To cope with the growing complexity of the test of such systems, the authors propose the reuse of the on-chip network as a test access mechanism to the cores embedded into systems that use this communication platform. An algorithm exploiting the network characteristics to minimize test time is presented. Then, the reuse strategy is evaluated considering a number of system configurations, such as different positions of the cores in the network, power consumption constraints and number of interfaces with the tester. Experimental results for the ITC'02 SOC Test Benchmarks show that the parallelization capability of the network can be exploited to reduce the system test time, whereas area and pin overhead are strongly minimized. © 2004 ACM.",Core-based test; Network-on-chip; SoC test; TAM and wrapper design; Test reuse; Test scheduling,
Simultaneous shield insertion and net ordering for capacitive and inductive coupling minimization,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-15844363390&doi=10.1145%2f1013948.1013950&partnerID=40&md5=9a9dd1afa2d95ebebaf12f1e3c20da4a,"In this article, we first show that existing net ordering formulations to minimize noise are no longer sufficient with the presence of inductive noise, and shield insertion is needed to minimize inductive noise. Using a K eff model as the figure of merit for inductive coupling, we then formulate two simultaneous shield insertion and net ordering (SINO) problems: the optimal SINO/NF problem to find a minimal area SINO solution that is free of capacitive and inductive noise, and the optimal SINO/NB problem to find a minimal area SINO solution that is free of capacitive noise and is under the given inductive noise bound. We reveal that both optimal SINO problems are NP-hard, and propose effective approximate algorithms for the two problems. Experiments show that our SINO/NB algorithm uses from 51% to 82% fewer shields compared to uniform shield insertion and net ordering (US 4- NO), and uses from 4% to 47% fewer shields compared to separated net ordering and shield insertion (NO + SI). Furthermore, the SINO/NB solutions under practical noise bounds use from 38% to 61% fewer shields compared to SINO/NF solutions, and use up to 36% fewer shields compared to the theoretical lower bound for optimal SINO/NF solutions. Moreover, we show that the K eff model has a high fidelity versus the noise voltage computed using accurate RLC circuit models and SPICE simulations. To the best of our knowledge, it is the first work that presents an in-depth study on the automatic layout optimization of multiple nets to minimize both capacitive and inductive noise. © 2004 ACM.",Net ordering; Noise minimization; On-chip inductance; Shielding; Signal integrity; VLSI physical design automation,
A new approach for integration of min-area retiming and min-delay padding for simultaneously addressing short-path and long-path constraints,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30744456519&doi=10.1145%2f1013948.1013949&partnerID=40&md5=681dfd73bd18fcf3679b445c983f8e76,"This article describes a polynomial time algorithm for min-area retiming for edge-triggered circuits to handle both setup and hold constraints. Given a circuit G and a target clock period c, our algorithm either outputs a retimed version of G satisfying setup and hold constraints or reports that such a solution is not possible, in O(|V| 3log|V|log(|V|C)) steps, where |V| corresponds to number of gates in the circuit and C is equal to the number of registers in the circuit. This is the first polynomial-time algorithm ever reported for min-area retiming with constraints on both long and short-paths. An alternative problem formulation that takes practical issues into consideration and lowers the problem complexity is also developed. Both the problem formulations have many parallels with the original formulation of long path only retiming by Leiserson and Saxe and all the speed improvements that have been obtained on that problem statement are also demonstrated in simulation for the approach presented here. Finally, a basis is provided for deriving efficient heuristics for addressing both long-path and short-path requirements by combining the techniques of retiming and min-delay padding. © 2004 ACM.",Application of mincost network flow; Longpath cricuit constraints; Minimum area retiming; Minimum delay padding; Shortpath circuit constraints,
Frequent value encoding for low power data buses,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30744478951&doi=10.1145%2f1013948.1013953&partnerID=40&md5=e53b532c78c7c9be00315c2b75f4f45b,"Since the I/O pins of a CPU are a significant source of energy consumption, work has been done on developing encoding schemes for reducing switching activity on external buses. Modest reductions in switching can be achieved for data and address buses using a number of general purpose encoding schemes. However, by exploiting the characteristic of memory reference locality, switching activity on the address bus can be reduced by as much as 66%. Till now no characteristic has been identified that can be used to achieve similar reductions in switching activity on the data bus. We have discovered a characteristic of values transmitted over the data bus according to which a small number of distinct values, called frequent values, account for 32% of transmissions over the external data bus. Exploiting this characteristic we have developed an encoding scheme that we call the FV encoding scheme. To implement this scheme we have also developed a technique for dynamically identifying the frequent values which compares quite favorably with an optimal offline algorithm. Our experiments show that FV encoding of 32 frequent values yields an average reduction of 30% (with on-chip data cache) and 49% (without on-chip data cache) in data bus switching activity for SPEC95 and mediabench programs. Moreover the reduction in switching achieved by FV encoding is 2 to 4 times the reduction achieved by the bus-invert coding scheme and 1.5 to 3 times the reduction achieved by the adaptive method. The overall energy savings on data bus we attained considering the coder overhead is 29%. © 2004 ACM.",Algorithms; Design; Measurement,
Annealing placement by thermodynamic combinatorial optimization,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30744451698&doi=10.1145%2f1013948.1013951&partnerID=40&md5=1febf2d0385e1a07bfa91786336f7efa,"Placement is key issue of integrated circuit physical design. There exist some thechniques inspired in thermodynamics coping with this problem as Simulated Annealing. In this article, we present a combinatorial optimization method directly derived from both Thermodynamics and Information Theory. In TCO (Thermodynamic Combinatorial Optimization), two kinds of processes are considered: microstate and macrostate transformations. Applying the Shannon's definition of entropy to reversible microstate transformations, a probability of acceptance based on Fermi-Dirac statistics is derived. On the other hand, applying thermodynamic laws to macrostate transformations, an efficient annealing schedule is provided. TCO has been compared with a custom Simulated Annealing (SA) tool on a set of benchmark circuits for the FPGA (Field Programmable Gate Arrays) placement problem. TCO has provided the high-quality results of SA, while inheriting the adaptive properties of Natural Optimization (NO). © 2004 ACM.",Combinatorial optimization; Entropy; Information theory; Programmable logic; Recofigurable; Thermodynamics,
Segmented channel routability via satisfiability,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344318897&doi=10.1145%2f1027084.1027090&partnerID=40&md5=6cda995c4c2679e52d4b64cdaee7b549,"Segmented channel routing is fundamental to the routing of row-based FPGAs. In this paper, we study segmented channel routability via satisfiability. Our method encodes the horizontal and vertical constraints of the routing problem as Boolean conditions. The routability constraint is satisfiable if and only if the net connections in the segmented channel are routable. Empirical results show that the method is time-efficient and applicable to large problem instances. © 2004 ACM.",Satisfiability,
An Adaptive Cryptographic Engine for Internet Protocol Security architectures,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-26444534119&doi=10.1145%2f1013948.1013952&partnerID=40&md5=fa120b307545a20a4abfa4099e5650a6,"Architectures that implement the Internet Protocol Security (IPSec) standard have to meet the enormous computing demands of cryptographic algorithms. In addition, IPSec architectures have to be flexible enough to adapt to diverse security parameters. This article proposes an FPGA-based Adaptive Cryptographic Engine (ACE) for IPSec architectures. By taking advantage of FPGA technology, ACE can adapt to diverse security parameters on the fly while providing superior performance compared with software-based solutions. In this paper, we focus on performance issues. A diverse set of private-key cryptographic algorithms is utilized to demonstrate the applicability of the proposed cryptographic engine. The time performance metrics are throughput and key-setup latency. The latency metric is the most important measure for IPSec where a small amount of data is processed per key and key context switching occurs repeatedly. We are not aware of any published results that include extensive key-setup latency results. © 2004 ACM.",Adaptive computing; AES; Configurable; Cryptography; High performance; IPSec; Performance tradeoffs; Reconfigurable components; Reconfigurable computing; Reconfigurable systems,
Buffer merging - A powerful technique for reducing memory requirements of synchronous dataflow specifications,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942564428&doi=10.1145%2f989995.989999&partnerID=40&md5=e520c8624b35d842689a2885d298aa42,"We develop a new technique called buffer merging for reducing memory requirements of synchronous dataflow (SDF) specifications. SDF has proven to be an attractive model for specifying DSP systems, and is used in many commercial tools like System Canvas, SPW, and Cocentric. Good synthesis from an SDF specification depends crucially on scheduling, and memory is an important metric for generating efficient schedules. Previous techniques on memory minimization have either not considered buffer sharing at all, or have done so at a fairly coarse level (the meaning of this will be made more precise in the article). In this article, we develop a buffer overlaying strategy that works at the level of an input/output edge pair of an actor. It works by algebraically encapsulating the lifetimes of the tokens on the input/output edge pair, and determines the maximum amount of the input buffer space that can be reused by the output. We develop the mathematical basis for performing merging operations, and develop several algorithms and heuristics for using the merging technique for generating efficient implementations. We show improvements of up to 48% over previous techniques.",Array lifetime; Block diagram compiler; Buffer overlaying; Dataflow; Design methodology; DSP and embedded systems; Graph coloring; Lifetime analysis; Memory optimization; Path covering; Synchronous dataflow,Algorithms; Data storage equipment; Embedded systems; Heuristic methods; Image processing; Mathematical models; Array lifetime; Block diagram compiler; Buffer overlaying; Design methodology; Graph coloring; Lifetime analysis; Memory optimization; Path covering; Synchronous dataflow; Data flow analysis
Storage requirement estimation for optimized design of data intensive applications,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942564431&doi=10.1145%2f989995.989996&partnerID=40&md5=5afd3310b1f5635c540d8568f14bddd8,"A novel storage requirement estimation methodology is presented for use in the early system design phases when the data transfer ordering is only partially fixed. At that stage, none of the existing estimation tools are adequate, as they either assume a fully specified execution order or ignore it completely. A prototype CAD tool has been developed that includes major parts of the storage requirement estimation and optimization methodology. Using representative application demonstrators, we show how our techniques and tool can effectively guide the designer to achieve a transformed specification with low storage requirement.",Code transformation; Data optimization; High-level synthesis; Memory architecture exploration; Size estimation,Computer aided design; Data transfer; Design; Feedback; Integrated circuits; Motion estimation; Multimedia systems; Performance; Systems analysis; Code transformation; Data optimization; High level synthesis; Memory architecture exploration; Size estimation; Data storage equipment
Stairway compaction using corner block list and its applications with rectilinear blocks,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942559015&doi=10.1145%2f989995.989998&partnerID=40&md5=b0f25cc70a522636bdcad1ea5986fd8a,"Corner Block List (CBL) was recently proposed as an efficient representation for MOSAIC packing of rectangles. Although the original method is really innovative, there still remains room for improvement for our purpose. This article proposes a compact algorithm for placement based on corner block list. By introducing the dummy blocks in CBL, our algorithm can intellectively employ dummy blocks in the packing to represent the placement including empty rooms, which corner block list cannot represent. Our algorithm can obtain the fast convergence to an optimal solution. Based on the compact approach, we propose, a new way to handle arbitrary shaped rectilinear modules. The experimental results are demonstrated by some benchmark data and the performance shows effectiveness of the proposed method.",Corner block list; Floorplanning; Rectilinear blocks,Algorithms; Computational complexity; Design for testability; Performance; Topology; Deep submicron technology; Extended Corner Block List (ECBL); Floorplanning; Rectilinear blocks; VLSI circuits
A two-layer library-based approach to synthesis of analog systems from VHDL-AMS specifications,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942615324&doi=10.1145%2f989995.990000&partnerID=40&md5=7db4e3a8a7012f541b5cb64144c4239a,"This paper presents a synthesis methodology for analog systems described using VHDL-AMS language. Synthesis produces net-lists of analog components that are selected from a library, and sized so that specified objectives (like AC response, signal to noise ratio, dynamic range, area) are optimized. The gap between abstract specifications and implementations is bridged using a two-layered methodology. The first layer is architecture generation. The second layer is component synthesis and constraint transformation. Architecture generation employs the branch-and-bound algorithm to create architectural alternatives for a system. Component synthesis and constraint transformation use a directed interval based genetic algorithm that operates on parameter ranges. The performance estimation engine embeds technology process parameters, SPICE models for basic circuits, and symbolic composition equations for basic structural configurations. The paper discusses the VHDL-AMS subset for synthesis. The subset offers the composition semantics. As a result, specifications offer sufficient insight into the system structure to allow automated architecture generation. To justify the flexibility of the methodology, the paper presents results for three case studies, a signal conditioning system, a filter, and an analog to digital converter. Experiments show that constraint-satisfying designs can be synthesized in a short time, at a low cost, and without requesting broad knowledge on analog circuits.",Analog synthesis; Branch-and-bound; Genetic algorithms; Performance estimation; VHDL-AMS,Analog to digital conversion; Bandwidth; Capacitance; Computer aided design; Genetic algorithms; Inductance; Networks (circuits); Signal processing; Architecture generation; Branch-and-bound; Constraint transformation; Performance estimation; Signal to noise ratio
I DDX-based test methods: A survey,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942566264&doi=10.1145%2f989995.989997&partnerID=40&md5=2c0ea053ae2db0af87c15553b7254ef4,"Supply current measurement-based test is a valuable defect-based test method for semiconductor chips. Both static leakage current (I DDQ) and transient current (I DDT) based tests have the capability of detecting unique defects that improve the fault detection capacity of a test suite. Collectively these test methods are known as I DDX tests. However, due to advances in the semiconductor manufacturing process, the future of these test methods is uncertain. This paper presents a survey of the research reported in the literature to extend the use of I DDX tests to deep sub-micron (DSM) technologies.",I <sub>DDQ</sub> test; I <sub>DDT</sub> test; VLSI testing,CMOS integrated circuits; Database systems; Electric current measurement; Flip flop circuits; Leakage currents; Performance; Process control; Reliability; Signal to noise ratio; Silicon wafers; Built-in current sensors (BICS); Static leakage current (IDDQ); Transient current (IDDT); Very low voltage (VLV); Integrated circuit testing
Manhattan-Diagonal Routing in Channels and Switchboxes,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442286840&doi=10.1145%2f966137.966141&partnerID=40&md5=8a9ea9d4b75144994f293e82165cbeae,"New techniques are presented for routing straight channels, L-channels, switchboxes, and staircase channels in a two-layer Manhattan-diagonal (MD) model with tracks in horizontal, vertical, and ±45° directions. First, an O(l.d) time algorithm is presented for routing a straight channel of length l and density d with no cyclic vertical constraints. It is shown that the number of tracks h used by the algorithm for routing multiterminal nets satisfies d ≤ h ≤ (d+1). Second, an output-sensitive algorithm is reported that can route a channel with cyclic vertical constraints in O(l.h) time using h tracks, allowing overlapping of wire segments in two layers. Next, the routing problem for a multiterminal L-channel of length l and height A is solved by an O(l.h) time algorithm. If no cyclic vertical constraints exist, its time complexity reduces to O(l.h) where d is the density of the L-channel. Finally, the switchbox routing problem in the MD model is solved elegantly. These techniques, easily extendible to the routing of staircase channels, yield efficient solutions to detailed routing in general floorplans. Experimental results on benchmarks show significantly low via count and reduced wire length, thus establishing the superiority of MD routing to classical strategies. The proposed algorithms are also potentially useful for general non-Manhattan area routing and multichip modules (MCMs).",Channel density; Diagonal wires; Manhattan routing,Algorithms; Computer aided design; Constraint theory; Mathematical models; Microprocessor chips; Problem solving; Channel density; Diagonal wires; Manhattan routing; Program processors
Power Minimization Algorithms for LUT-Based FPGA Technology Mapping,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442360327&doi=10.1145%2f966137.966139&partnerID=40&md5=4a66836e367e4240fc4c7aca27382dc2,"We study the technology mapping problem for LUT-based FPGAs targeting at power minimization. The problem has been proved to be NP-hard previously. Therefore, we present an efficient heuristic algorithm to generate low-power mapping solutions. The key idea is to compute and select low-power K -feasible cuts by an efficient incremental network flow computation method. Experimental results show that our algorithm reduces power consumption as well as area over the best algorithms reported in the literature. In addition, we present an extension to compute depth-optimal low-power mappings. Compared with Cutmap, a depth-optimal mapper with simultaneous area minimization, we achieve a 14% power savings on average without any depth penalty.",Delay minimization; FPGA; Power optimization; Technology mapping,Algorithms; Boolean algebra; Computation theory; Computer programming languages; Input output programs; Mathematical models; Optimization; Table lookup; VLSI circuits; Delay minimization; Power optimization; Technology mapping; Field programmable gate arrays
Fast Memory Bank Assignment for Fixed-Point Digital Signal Processors,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442335885&doi=10.1145%2f966137.966140&partnerID=40&md5=004f2fec639f682c14d87ca08e765dca,"Most vendors of digital signal processors (DSPs) support a Harvard architecture, which has two or more memory buses, one for program and one or more for data and allow the processor to access multiple words of data from memory in a single instruction cycle. Also, many existing fixed-point DSPs are known to have an irregular architecture with heterogeneous registers, which contains multiple register files that are distributed and dedicated to different sets of instructions. Although there have been several studies conducted to efficiently assign data to multimemory banks, most of them assumed processors with relatively simple, homogeneous general-purpose registers. Thus, several vendor-provided compilers for DSPs that we examined were unable to efficiently assign data to multiple data memory banks, thereby often failing to generate highly optimized code for their machines. As a consequence, programmers for these DSPs often manually assign program variables to memories so as to fully utilize multimemory banks in their code. This paper reports on our recent attempt to address this problem by presenting an algorithm that helps the compiler to efficiently assign data to multimemory banks. Our algorithm differs from previous work in that it assigns variables to memory banks in separate, decoupled code generation phases, instead of a single, tightly coupled phase. The experimental results have revealed that our decoupled algorithm greatly simplifies our code generation process; thus our compiler runs extremely fast, yet generates target code that is comparable in quality to the code generated by a coupled approach.",Compiler; Dependence analysis; DSP; Dual memory banks; Maximum spanning tree; Nonorthogonal architecture,Algorithms; Codes (symbols); Computer architecture; Data reduction; Data storage equipment; Program compilers; Program processors; Trees (mathematics); Dependence analysis; Dual memory banks; Maximum spanning tree; Nonorthogonal architecture; Digital signal processing
Formal Hardware Specification Languages for Protocol Compliance Verification,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442335889&doi=10.1145%2f966137.966138&partnerID=40&md5=deea5a51f45906d1912dbb49b48b5882,"The advent of the system-on-chip and intellectual property hardware design paradigms makes protocol compliance verification increasingly important to the success of a project. One of the central tools in any verification project is the modeling language, and we survey the field of candidate languages for protocol compliance verification, limiting our discussion to languages originally intended for hardware and software design and verification activities. We frame our comparison by first constructing a taxonomy of these languages, and then by discussing the applicability of each approach to the compliance verification problem. Each discussion includes a summary of the development of the language, an evaluation of the language's utility for our problem domain, and, where feasible, an example of how the language might be used to specify hardware protocols. Finally, we make some general observations regarding the languages considered.",E; Esterel; Hardware monitors; Heterogeneous Hardware Logic; Hierarchical Annotated Action Diagrams; Java; Lava; Live Sequence Charts; Message Sequence Charts; Objective VHDL; OpenVera; Property Specification Language,Computer hardware; Computer software; Data transfer; Hierarchical systems; Java programming language; Mathematical models; Microprocessor chips; Network protocols; Problem solving; Hardware monitors; Heterogeneous hardware logic; Hierarchical annotated action diagrams; Live sequence charts; Message sequence charts; Objective VHDL; OpenVera; Property specification language; System verilog; Timing diagrams; Computer programming languages
A BNF-Based Automatic Test Program Generator for Compatible Microprocessor Verification,2004,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442335882&doi=10.1145%2f966137.966142&partnerID=40&md5=47c772dcabc6a5714db24eb1a86790df,"A novel Backus-Naur-form- (BNF-) based method to automatically generate test programs from simple to complex ones for advanced microprocessors is presented in this paper. We use X86 architecture to illustrate our design method. Our method is equally applicable to other processor architectures by redefining BNF production rules. Design issues for an automatic program generator (APG) are first outlined. We have resolved the design issues and implemented the APG by a top-down recursive descent parsing method which was originated from compiler design. Our APG can produce not only random test programs but also a sequence of instructions for a specific module to be tested by specifying a user menu-driven file. In addition, test programs generated by our APG have the features of no infinite loop, not entering illegal states, controllable data dependency, flexible program size, and data cache testable. Our method has been shown to be efficient and feasible for the development of an APG compared with other approaches. We have also developed a coverage tool to integrate with the APG. Experimental evaluation of the generated test programs indicates that our APG, with the guidance of the coverage tool, only needs to generate a small number of test programs to sustain high coverage.",Advanced microprocessor; Automatic program generator; BNF; Compatibility verification; Coverage; Top-down recursive descent parsing method,Automatic programming; Computer aided design; Computer architecture; Computer hardware; Computer operating systems; Decoding; Program processors; Advanced microprocessors; BNF; Compatibility verification; Top-down recursive descent parsing method; Microprocessor chips
Test Data Compression Using Dictionaries with Selective Entries and Fixed-Length Indices,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142031627&doi=10.1145%2f944027.944032&partnerID=40&md5=b8051aae276991604c4ffc721ea01a4b,"We present a dictionary-based test data compression approach for reducing test data volume in SOCs. The proposed method is based on the use of a small number of ATE channels to deliver compressed test patterns from the tester to the chip and to drive a large number of internal scan chains in the circuit under test. Therefore, it is especially suitable for a reduced pin-count and low-cost DFT test environment, where a narrow interface between the tester and the SOC is desirable. The dictionary-based approach not only reduces test data volume but it also eliminates the need for additional synchronization and handshaking between the SOC and the ATE. The dictionary entries are determined during the compression procedure by solving a variant of the well-known clique partitioning problem from graph theory. Experimental results for the ISCAS-89 benchmarks and representative test data from IBM show that the proposed method outperforms a number of recently-proposed test data compression techniques. Compared to the previously proposed test data compression approach based on selective Huffman coding with variable-length indices, the proposed approach generally provides higher compression for the same amount of hardware overhead.",Embedded core testing; Reduced pin-count testing; SoC testing; Test application time; Test data volume,Built-in self test; Data reduction; Design for testability; Graph theory; Interfaces (computer); Logic design; Semiconductor device testing; Systems analysis; Handshake conditions; Data compression
Multimode Scan: Test per Clock BIST for IP Cores,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142126894&doi=10.1145%2f944027.944033&partnerID=40&md5=0106e8acc19d3bbfb226ce30e9010b9b,"Built-in self-test (BIST) is an attractive design-for-test methodology for core-based SoC design because of the minimal need for test access when tests are generated and evaluated within the core itself. However, the scan based logic BIST approach being widely considered for this application suffers from two significant weaknesses: slow test-per-scan execution, and a limited capability for detecting realistic timing and delay faults, critical in deep submicron technologies. The new multimode scan based approach presented here supports test-per-clock BIST, which runs orders of magnitude faster, and also provides significantly better delay fault coverage.",BIST; Digital testing; Scan; SoC,Design for testability; Intellectual property; Logic design; Systems analysis; Digital testings; Built-in self test
BIST and Production Testing of ADCs Using Imprecise Stimulus,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142126893&doi=10.1145%2f944027.944035&partnerID=40&md5=970d2c0e15cde8fee54faed7e9a3e900,"A new approach for testing mixed-signal circuits based upon using imprecise stimuli is introduced. Unlike most existing Built-in Self-Test (BIST) and production test approaches that require excitation signals that are at least 3 bits or more linear than the Device-Under-Test (DUT), the proposed approach can work with stimuli that are several bits less linear than the DUT. This dramatically reduces the requirements on stimulus generation for BIST applications and offers potential for using inexpensive signal generators in production test, or for testing DUTs that have a linearity performance exceeding that of the available test equipment. As a proof of concept, a histogram-based algorithm for linearity testing for Analog-to-Digital Converters (ADCs) has been proposed. It can estimate the Integral Nonlinearity (INL) and Differential Nonlinearity (DNL) of an n-bit ADC by using a ramp signal of much less than n-bit linearity and a shifted version of the same nonlinear ramp as excitation. The performance of the algorithm is comparable to that of the traditional method which uses (n + 3)-bits or a decade more linear input signals. Complete algorithm description, extensive simulation results and experimental results obtained from using a production tester on commercially available ICs are presented to validate the potential of this algorithm.",ADC linearity; Analog and mixed-signal testing; Built-in self-test; Imprecision measurement; Imprecision stimulus; Production test,Algorithms; Built-in self test; Computer simulation; Control nonlinearities; Digital integrated circuits; Fault tolerant computer systems; Device-under-test (DUT); Analog to digital conversion
A Data Acquisition Methodology for on-Chip Repair of Embedded Memories,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142063554&doi=10.1145%2f944027.944037&partnerID=40&md5=2c7cf8204de2c6df1e4082a8c8019fa2,"Systems-on-Chips often contain a large amount of embedded memory. In order to obtain sufficiently high yield, efficient diagnosis and repair facilities are needed for the memories. A novel and efficient approach for collecting complete failure data during on-chip memory testing is proposed that can be combined with a row/column reconfiguration algorithm for complete on-chip memory repair. A sequence of diagnostic tests of linear order is utilized that detects and localizes all cells involved in single-cell faults and two-cell coupling faults, such as idempotent coupling faults, and provides this information to on-chip circuitry for memory repair. Failure data are collected at the operating speed of the memory-under-test so that tests can be applied at speed. The data acquisition circuitry evaluates the test results and classifies faults as column failures, coupling faults, or single-cell faults for near-optimal allocation of spare resources. The proposed test and data acquisition algorithm can be realized as compact Built-In Self-Test (BIST) circuitry using standard design libraries.",Built-in self-test; Column failures; Coupling faults; Diagnosis; Embedded memory; March tests; Memory test; On-chip repair,Algorithms; Built-in self test; Data storage equipment; Embedded systems; Fault tolerant computer systems; Logic design; Repair; Embedded memories; Data acquisition
On Test Data Volume Reduction for Multiple Scan Chain Designs,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142031631&doi=10.1145%2f944027.944031&partnerID=40&md5=0a74fea39d0ca8b361050827d367f91a,We consider issues related to the reduction of scan test data in designs with multiple scan chains. We propose a metric that can be used to evaluate the effectiveness of procedures for reducing the scan data volume. The metric compares the achieved compression to the compression which is intrinsic to the use of multiple scan chains. We also propose a procedure for modifying a given test set so as to achieve reductions in test data volume assuming a combinational decompressor circuit.,Decompressor; Design for testability; Don't care identification; Encoding techniques; Test data compression,Combinatorial circuits; Design for testability; Logic design; Reliability; Semiconductor device testing; Systems analysis; Decompressor circuits; Data reduction
SOC Test Architecture Design for Efficient Utilization of Test Bandwidth,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142063562&doi=10.1145%2f944027.944029&partnerID=40&md5=de5143fbada4e7b455885a58c8d6496d,"This article deals with the design of on-chip architectures for testing large system chips (SOCs) for manufacturing defects in a modular fashion. These architectures consist of wrappers and test access mechanisms (TAMs). For an SOC with specified parameters of modules and their tests, we design an architecture that minimizes the required tester vector memory depth and test application time. In this article, we formulate the test architecture design problems for both modules with fixed- and flexible-length scan chains, assuming the relevant module parameters and a maximal SOC TAM width are given. Subsequently, we derive a formulation for an architecture-independent lower bound for the SOC test time. We analyze three types of TAM under-utilization that make the theoretical lower bound unachievable in most practical architecture instances. We present a novel architecture-independent heuristic algorithm that effectively optimizes the test architecture for a given SOC. The algorithm efficiently determines the number of TAMs and their widths, the assignment of modules to TAMs, and the wrapper design per module. We show how this algorithm can be used for optimizing both test bus and TestRail architectures with either serial or parallel test schedules. Experimental results for the ITC'02 SOC Test Benchmarks show that, compared to manual best-effort engineering approaches, we can save up to 75% in test times, while compared to previously published algorithms, we obtain comparable or better test times at negligible compute time.",Bandwidth utilization; Idle bits; Lower bound; SOC test; TAM and wrapper design; Test scheduling,Algorithms; Bandwidth; Computer architecture; Fault tolerant computer systems; Microprocessor chips; Scheduling; Systems analysis; System chips; Semiconductor device testing
Testing High-Performance Pipelined Circuits with Slow-Speed Testers,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142095231&doi=10.1145%2f944027.944034&partnerID=40&md5=c5719ceb99133974fc77a208ca8b5985,This article presents a methodology for testing high-performance pipelined circuits with slow-speed testers. The technique uses a clock timing circuit to control data transfer in the pipeline in test mode. The technique adds no extra hardware in the data path of the pipeline and therefore has virtually no performance penalty. A clock timing circuit capable of achieving a timing resolution of 50 ps in 0.18 μm CMOS technology is presented. The design provides the ability to test the clock timing circuit itself. The effectiveness of the technique is demonstrated using a 16-bit pipelined multiplier as a test vehicle. Simulations show that we are able to detect delay faults as small as 50 ps at an input clock frequency of 100 MHz.,Delay-fault testing; Design for delay testability; High-performance testing,CMOS integrated circuits; Computer simulation; Data transfer; Delay circuits; Design for testability; Logic design; Timing circuits; Delay-fault testing; Pipeline processing systems
Test Vector Decomposition-Based Static Compaction Algorithms for Combinational Circuits,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142095234&doi=10.1145%2f944027.944030&partnerID=40&md5=75aa60d2600579c9bd41288af790680a,"Testing system-on-chips involves applying huge amounts of test data, which is stored in the tester memory and then transferred to the chip under test during test application. Therefore, practical techniques, such as test compression and compaction, are required to reduce the amount of test data in order to reduce both the total testing time and memory requirements for the tester. In this article, a new approach to static compaction for combinational circuits, referred to as test vector decomposition (TVD), is proposed. In addition, two new TVD based static compaction algorithms are presented. Experimental results for benchmark circuits demonstrate the effectiveness of the two new static compaction algorithms.",Class-based clustering; Combinational circuits; Independent fault clustering; Static compaction; Taxonomy; Test vector decomposition,Algorithms; Compaction; Data storage equipment; Semiconductor device testing; Systems analysis; Vectors; Clustering techniques; Combinatorial circuits
A Circuit Level Fault Model for Resistive Bridges,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142063548&doi=10.1145%2f944027.944036&partnerID=40&md5=0bd959a63445239dd6d442b1a9154b12,"Delay faults are an increasingly important test challenge. Modeling bridge faults as delay faults helps delay tests to detect more bridge faults. Traditional bridge fault models are incomplete because these models only model the logic faults or these models are not efficient to use in delay tests for large circuits. In this article, we propose a physically realistic yet economical resistive bridge fault model to model delay faults as well as logic faults. An accurate yet simple delay calculation method is proposed. We also enumerate all possible fault behaviors and present the relationship between input patterns and output behaviors, which is useful in ATPG. Our fault simulation results show the benefit of at-speed tests.",Bridge faults; Delay faults; Fault models,Computer simulation; Electric measuring bridges; Fault tolerant computer systems; Logic circuits; Semiconductor device testing; Delay faults; Delay circuits
A Multiple Bit Upset Tolerant SRAM Memory,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142095226&doi=10.1145%2f944027.944038&partnerID=40&md5=efe2ecb222a1df4b545eedecf3e497e2,"SRAMs are used nowadays in almost every electronic product. However, as technology shrinks transistor sizes, single and multiple bit upsets only observable in space applications previously are now reported at ground level. This article presents a high level technique to protect SRAM memories against multiple upsets based on correcting codes. The proposed technique combines Reed Solomon code and Hamming code to assure reliability in presence of multiple bit flips with reduced area and performance penalties. Multiple upsets were randomly injected in various combinations of memory cells to evaluate the robustness of the method. The experiment was emulated in a Virtex FPGA platform. Results show that 100% of the injected double faults and a large amount of multiple faults were corrected by the method.",Fault injection; Fault tolerant memory; Hamming and Reed-Solomon codes; High-level protection technique; Protection against radiation,Binary codes; Fault tolerant computer systems; Robustness (control systems); Security of data; Hamming codes; Static random access storage
ACM Transactions on Design Automation of Electronic Systems: Introduction,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142126903&doi=10.1145%2f944027.944028&partnerID=40&md5=be70cfae5c7ec298dbe8bf50f4a9d1d4,[No abstract available],,
Tutorial: Compiling concurrent languages for sequential processors,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037646914&doi=10.1145%2f762488.762489&partnerID=40&md5=63418959ebc17c67297bb7df2ade1d1c,"Embedded systems often include a traditional processor capable of executing sequential code, but both control and data-dominated tasks are often more naturally expressed using one of the many domain-specific concurrent specification languages. This article surveys a variety of techniques for translating these concurrent specifications into sequential code. The techniques address compiling a wide variety of languages, ranging from dataflow to Petri nets. Each uses a different method, to some degree chosen to match the semantics of concurrent language. Each technique is considered to consist of a partial evaluator operating on an interpreter. This combination provides a clearer picture of how parts of each technique could be used in a different setting.",Code generation; Communication; Compilation; Concurrency; Dataflow; Discrete-event; Esterel; Lustre; Partial evaluation; Petri nets; Sequential; Verilog,Codes (symbols); Data flow analysis; Linguistics; Petri nets; Program interpreters; Semantics; Code generation; Program compilers
Congestion reduction during placement with provably good approximation bound,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042745487&doi=10.1145%2f785411.785414&partnerID=40&md5=3b672dfb928934b82a16dd02c39a58f8,"This paper presents a novel method to reduce routing congestion during placement stage. The proposed approach is used as a post-processing step in placement. Congestion reduction is based on local improvement on the existing layout. However, the approach has a global view of the congestion over the entired design. It uses integer linear programming (ILP) to formulate the problem of conflicts between multiple congested regions, and performs local improvement according to the solution of the ILP problem. The approximation algorithm of the formulated ILP problem is studied and good approximation bounds are given and proved. Experiments show that the proposed approach can effectively alleviate the congestion of global routing results. The low computational complexity of the proposed approach indicates its scalability on large designs.",Congestion; Physical design; Placement; Routability,Algorithms; Approximation theory; Computational complexity; Design aids; Integer programming; Integrated circuit layout; Linear programming; Congestion reduction; Routability; Routing congestion; Computer aided design
Compiler optimization on VLIW instruction scheduling for low power,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038049043&doi=10.1145%2f762488.762494&partnerID=40&md5=957ccae003ce8c663067523578931ba5,"In this article, we investigate compiler transformation techniques regarding the problem of scheduling VLIW instructions aimed at reducing power consumption of VLIW architectures in the instruction bus. The problem can be categorized into two types: horizontal scheduling and vertical scheduling. For the case of horizontal scheduling, we propose a bipartite-matching scheme for instruction scheduling. We prove that our greedy bipartite-matching scheme always gives the optimal switching activities of the instruction bus for given VLIW instruction scheduling policies. For the case of vertical scheduling, we prove that the problem is NP-hard, and we further propose a heuristic algorithm to solve the problem. Our experiment is performed on Alpha-based VLIW architectures and an ATOM simulator, and the compiler incorporated in our proposed schemes is implemented based on SUIF and MachSUIF. Experimental results of horizontal scheduling optimization show an average 13.30% reduction with four-way issue architecture and an average 20.15% reduction with eight-way issue architecture for transitional activities of the instruction bus as compared with conventional list scheduling for an extensive set of benchmarks. The additional reduction for transitional activities of the instruction bus from horizontal to vertical scheduling with window size four is around 4.57 to 10.42%, and the average is 7.66%. Similarly, the additional reduction with window size eight is from 6.99 to 15.25%, and the average is 10.55%.",Compilers; Instruction bus optimizations; Low-power optimization; VLIW instruction scheduling,Algorithms; Computer aided design; Electric potential; Energy dissipation; Heuristic methods; Program compilers; Instruction bus optimization; Very long instruction word architecture
Constraints-driven scheduling and resource assignment,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042745479&doi=10.1145%2f785411.785416&partnerID=40&md5=6457daa33dd9141466df8f9abe48db50,"This paper describes a new method for modeling and solving different scheduling and resource assignment problems that are common in high-level synthesis (HLS) and system-level synthesis. It addresses assignment of resources for operations and tasks as well as their static, off-line scheduling. Different heterogeneous constraints are considered for these problems. These constraints can be grouped into two classes: problem-specific constraints and design-oriented constraints. They are uniformly modeled, in our approach, by finite domain (FD) constraints and solved using related constrained programming (CP) techniques. This provides a way to improve quality of final solutions. We have developed in Java a constraint solver engine, JaCoP (Java Constraint Programming), to evaluate this approach. This solver and a related framework make it possible to model different resource assignment and scheduling problems, and handle them uniformly. The JaCoP prototype system has been extensively evaluated on a number of HLS and system-level synthesis beachmarks. We have been able to obtain optimal results together with related proofs of optimality for all HLS scheduling benchmarks and for all explored design styles (except one functional pipeline design). Many system-level benchmarks can also be solved optimally. For large randomly generated task graphs, we have used heuristic search methods and obtained results that are 1-3% worse than lower bounds or optimal results. These experiments have proved the feasibility of the presented approach.",Constraint programming; High-level synthesis; Resource assignment; Scheduling; System-level synthesis,Constraint theory; Design aids; High level languages; Java programming language; Mathematical models; Mathematical programming; Resource allocation; Scheduling; Systems analysis; Automatic synthesis; Constraint programming; High level synthesis; Resource assignment; Computer aided design
Address code generation for DSP instruction-set architectures,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0043246555&doi=10.1145%2f785411.785417&partnerID=40&md5=2b2591f0f068f26bdd70cd8082d8b8a2,"This paper presents a new DSP-oriented code optimization method to enhance performance by exploiting the specific architectural features of digital signal processors. In the proposed method, a source code is translated into the static single assignment form while preserving the high-level information related to the address computation of array accesses. The information is used in generating auto-modification addressing operations provided by most digital signal processors. In addition to the conventional control-data flow graph, a new graph is employed to find auto-modification addressing modes efficiently. Experimental results on benchmark programs show that the proposed method is effective in improving performance and reducing code size.",Auto-modification addressing; Code size; Cycle counts,Computer programming languages; Computer systems programming; Digital signal processing; Optimization; Reduced instruction set computing; Auto-modification addressing; Code size; Cycle counts; Program processors
On the hardware-software partitioning problem: System modeling and partitioning techniques,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042244291&doi=10.1145%2f785411.785412&partnerID=40&md5=0046e64022d68a3d7969cc8e68702da7,"This papers presents an in-depth study of several system partitioning procedures. It is based on the appropriate formulation of a general system model, being therefore independent of either the particular co-design problem or the specific partitioning procedure. The techniques under study are a knowledge-based system and three classical circuit partitioning algorithms (Simulated Annealing, Kernighan&Lin and Hierarchical Clustering). The former has been entirely proposed by the authors in previous works while the later have been properly extended to deal with system level issues. We will show how the way the problem is solved biases the results obtained, regarding both quality and convergence rate. Consequently it is extremely important to chose the most suitable technique for the particular co-design problem that is being confronted.",Clustering; Cost functions; Expert systems; Fuzzy logic; General optimization procedures; Hardware-software co-design; Hardware-software partitioning; System modeling,Algorithms; Computer aided engineering; Expert systems; Fuzzy sets; Mathematical models; Simulated annealing; Systems analysis; Systems engineering; Hardware-software partitioning; System modeling; System partitioning; Computer aided design
Gravity: Fast placement for 3-D VLSI,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042244290&doi=10.1145%2f785411.785413&partnerID=40&md5=7ddd3c3f7c1d66a7df4c3bffe3c53b62,"Three dimensional integration is an increasingly feasible method of implementing complex circuitry. For large circuits, which most benefit from 3-D designs, efficient placement algorithms with low time complexity are required. We present an iterative 3-D placement algorithm that places circuit elements in three dimensions in linear time. Using an order of magnitude less time, our proposed algorithm produces placements with better than 11% less wire lengths than partitioning placement using the best and fastest partitioner. Due to the algorithms iterative nature, wire-length results can be further improved by increasing the number of iterations. Further, we provide empirical evidence that large circuits benefit most from 3-D technology and that even a small number of layers can provide significant wire-length improvements.",3-D integrated circuits; 3-D VLSI; Placement,Algorithms; Computational complexity; Computer aided engineering; Design aids; Integrated circuit layout; Iterative methods; Three dimensional; VLSI circuits; Low time complexity; Three dimensional integrated circuits; Computer aided design
Compacting sequences with invariant transition frequencies,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038661099&doi=10.1145%2f762488.762492&partnerID=40&md5=b3b0204d44d64f6f3f30cb1c9214b04d,"Simulation-based power estimation is commonly used for its high accuracy despite excessive computation times. Techniques have been proposed to speed it up by compacting an input sequence while preserving its power-consumption characteristics. We propose a novel method to compact a sequence that preserves transition frequencies. We prove the problem is NP-complete, and propose a graph model to reduce it to that of finding a heaviest-weighted trail, and a heuristic utilizing this model. We also propose using multiple sequences for better accuracy with even shorter sequences. Experiments show that power dissipation can be estimated with an error of only 2.3%, while simulation times are reduced by 10. Proposed methods generate solutions that effectively preserve transition frequencies and that are very close to optimal. Experiments also show that multiple sequences grant more accurate results with even shorter sequences.",Graph algorithms; Power estimation; Sequence compaction,Algorithms; Computer simulation; Energy dissipation; Graph theory; Heuristic methods; Sequence compaction; Power electronics
Rectilinear block placement using B*-trees,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037984749&doi=10.1145%2f762488.762490&partnerID=40&md5=0b75791274863d1e772ba85d6e136bfa,"Due to the layout complexity in modern VLSI designs, integrated circuit blocks may not be rectangular. However, literature on general rectilinear block placement is still quite limited. In this article, we present approaches for handling the placement for arbitrarily shaped rectilinear blocks using B*-trees [Chang et al. 2000]. We derive the feasibility conditions of B*-trees to guide the placement of rectilinear blocks. Experimental results show that our algorithm achieves optimal or near-optimal block placement for benchmarks with various shaped blocks.",Computer-aided design of VLSI; Floorplanning; Layout; Placement,Algorithms; Computer aided design; Hierarchical systems; Integrated circuit layout; Rectilinear blocks; VLSI circuits
Sequential optimization in the absence of global reset,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037984743&doi=10.1145%2f762488.762493&partnerID=40&md5=0f77fd4d8b85feeed20941fb20918809,"We study the problem of optimizing synchronous sequential circuits. There have been previous efforts to optimize such circuits. However, all previous attempts make implicit or explicit assumptions about the design or the environment of the design. For example, it is widespread practice to assume the existence of a hardware reset line and consequently a fixed power-up state; in the absence of the same, a common premise is that the design's environment will apply an initializing sequence. We review the concept of safe replaceability which does away with these assumptions and the delay-safe replaceability notion, which is applicable when the design's output is not used for a certain number of cycles after power-up. We then develop procedures for optimizing the combinational next-state and output logic, as well as routines for reencoding the state space and removing state bits under these replaceability criteria. Experimental results demonstrate the effectiveness of our algorithms.",No-reset latches; Safe replaceability; Sequential logic synthesis,Algorithms; Combinatorial circuits; Computer aided design; Formal logic; Logic synthesis; Sequential circuits
Minimum delay optimization for domino logic circuits - A coupling-aware approach,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037646937&doi=10.1145%2f762488.762491&partnerID=40&md5=0d077c5092262000d5603e144358734f,"Minimum delay associated with the hold time requirement is of concern to circuit designers, since race-through hazards are inherent in any multiple clock organization or clock distribution tree irrespective of clock frequency. The monotonic property of domino logic aggravates the min-delay path failure through coupling-induced speedup. To tackle the min-delay problem for domino logic, we propose a min-delay optimization algorithm considering coupling effects. Experimental results indicate that our algorithm yields a significant increase of min-delay without incurring max-delay violation.",Coupling; Delay minimization; Domino logic; Logic synthesis,Algorithms; Capacitance; CMOS integrated circuits; Flip flop circuits; Formal logic; Waveform analysis; Logic synthesis; Logic circuits
Synthesis of saturation arithmetic architectures,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0043246565&doi=10.1145%2f785411.785415&partnerID=40&md5=95c3a31d69a27e3015016ee1dbd56514,"This paper describes a synthesis technique for automating the design of linear Digital Signal Processing (DSP) systems such as digital filters. The proposed methodology makes optimized use of saturation arithmetic to produce a small design implemented directly in hardware. An analytical technique is proposed to estimate the saturation error resulting from a particular implementation, and an optimization procedure is introduced to aim for the smallest implementation satisfying user specified bounds on saturation and roundoff error. Results are presented illustrating significant speedup and area reduction compared with standard DSP design techniques: up to 22% improvement in area and 28% improvement in speed have been obtained on Field Programmable Gate Array (FPGA) implementations.",Saturation arithmetic; Signal processing; Synthesis,Algorithms; Design aids; Digital arithmetic; Digital filters; Digital signal processing; Field programmable gate arrays; Integrated circuit layout; Optimization; Automatic synthesis; Saturation arithmetic; Computer aided design
Design theory and implementation for low-power segmented bus systems,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037218782&doi=10.1145%2f606603.606606&partnerID=40&md5=3f32ed70c2507a546bc3401b4fc1fb65,"The concept of bus segmentation has been proposed to minimize power consumption by reducing the switched capacitance on each bus [Chen et al. 1999]. This paper details the design theory and implementation issues of segmented bus systems. Based on a graph model and the Gomory-Hu cut-equivalent tree algorithm, a bus can be partitioned into several bus segments separated by pass transistors. Highly communicating devices are placed to adjacent bus segments, so most data communication can be achieved by switching a small portion of the bus segments. Thus, a significant amount of power consumption can be saved, It can be proved that the proposed bus partitioning method achieves an optimal solution. The concept of tree clustering is also proposed to merge bus segments for further power reduction. The design flow, which includes bus tree construction in the register-transfer level and bus segmentation cell placement and routing in the physical level, is discussed for design implementation. The technology has been applied to a μ-controller design, and simulation results by PowerMill show significant improvement in power consumption.",ASIC design; Bus graph model; Bus segmentation; Bus segmentation cell; Low-power design; Low-power design flow; OLA tree,Algorithms; Capacitance; Communication systems; Power control; Switching; Trees (mathematics); Bus segmentation; Application specific integrated circuits
Floorplan representations: Complexity and connections,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037218783&doi=10.1145%2f606603.606607&partnerID=40&md5=78222fa06e0fbfebb48a4cfdf5b80a31,"Floorplan representation is a fundamental issue in designing a floorplanning algorithm. In this paper, we first present a twin binary trees structure for mosaic floor-plans. It is a nonredundant representation. We then derive the exact number of configurations for mosaic floorplans and slicing floorplans. Finally, the relationships between various state-of-the-art floo r-plan representations are discussed and explored.",Baxter permutation; Floorplan representation; Mosaic floorplan; Number of combinations; O-tree; Twin binary trees,Algorithms; Product design; Topology; Trees (mathematics); Floorplan representation; VLSI circuits
Analysis of FPGA/FPIC switch modules,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037222671&doi=10.1145%2f606603.606605&partnerID=40&md5=231109e4e38812434559c7d155392c2b,"Switch modules are the most important component of the routing resources in FPGAs/FPICs. Previous works have shown that switch modules with higher routability result in better area performance for practical applications. We consider in this paper an FPGA/FPIC switch-module analysis problem: the inputs consist of a switch-module description and the number of nets required to be routed through the switch module; the question is to determine if there exists a feasible routing for the routing requirements on the switch module. As a fundamental problem for the analysis of switch modules, this problem is applicable to the design and routability evaluation of FPGA/FPIC switch modules and FPGA/FPIC routing. We present a network-flow-based approximation algorithm for this problem. Based on mathematical analyses, we show that this algorithm has provably good performance with the bounds 5 and 5/4 away from the optima for two types of switch modules, respectively. Extensive experiments show that this algorithm is highly accurate and runs very efficiently.",Computer-aided design of VLSI; FPGA; FPIC; Layout; Synthesis,Algorithms; Combinatorial circuits; Computer aided design; Switches; VLSI circuits; Flow analyzers; Field programmable gate arrays
On the properties of the input pattern fault model,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037222673&doi=10.1145%2f606603.606609&partnerID=40&md5=da1632c6b3fa2bf582134181b6546b03,"A review of traditional IC failure analysis techniques strongly indicates the need for fault models that directly analyze the function of circuit primitives. The input pattern (IP) fault model is a functional fault model that allows for both complete and partial functional verification of every circuit module, independent of the design level. We describe the IP fault model and provide a method for analyzing IP faults using standard single stuck-line- (SSL-) based fault simulators and test generation tools. The method is used to generate test sets that target the IP faults of the ISCAS85 benchmark circuits and a carry-lookahead adder. Improved IP fault coverage for the benchmarks and the adder is obtained by adding a small number of test patterns to tests that target only SSL faults. We also conducted fault simulation experiments that show IP test patterns are effective in detecting nontargeted faults such as bridging and transistor stuck-on faults. Finally, we discuss the notion of IP redundancy and show how large amounts of this redundancy exist in the benchmarks and in SSL-irredundant adder circuits.",ATPG; Defects; Fault models; Fault testing; Faults; Testing digital circuits,Algorithms; Benchmarking; Digital circuits; Product design; Transistors; Input pattern fault models; Integrated circuits
Path delay fault testing using test points,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037223376&doi=10.1145%2f606603.606604&partnerID=40&md5=aef75a926f8894d56c8c338f17354d4e,"Inserting controllable/observable points in the test architecture has been shown to be a viable method for reducing the number of path delay faults that need to be tested in a circuit. In order to have a minimal impact on the operation clock and more accuracy in testing, it is proposed that test points should be inserted with the additional constraint that every path has a bounded number of test points. A polynomial time solvable integer linear programming (ILP) formulation serves as the basis for the presented test placement methodology. Due to the ILP's global optimization property we achieve results that are comparable to those by an existing greedy technique for the less constrained test point placement problem.",Automatic test pattern generation; Delay testing; Design for testability; Path delay fault simulation (coverage); Path delay fault testing; Testing digital circuits,Automation; Design for testability; Integer programming; Linear programming; Optimization; Polynomials; Path delay fault testing; Networks (circuits)
Transistor placement for noncomplementary digital VLSI cell synthesis,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037223225&doi=10.1145%2f606603.606608&partnerID=40&md5=f6c624cce71221557f25159e4c149f22,"There is an increasing need in modern VLSI designs for circuits implemented in high-performance logic families such as Cascode Voltage Switch Logic (CVSL), Pass Transistor Logic (PTL), and domino CMOS. Circuits designed in these noncomplementary ratioed logic families can be highly irregular, with complex diffusion sharing and nontrivial routing. Traditional digital cell layout synthesis tools derived from the highly stylized ""functional cell"" style break down when confronted with such circuit topologies. These cells require a full-custom, two-dimensional layout style which currently requires skilled manual design. In this work we propose a methodology for the synthesis of such complex noncomplementary digital cell layouts. We describe a new algorithm which permits the concurrent optimization of transistor chain placement and the ordering of the transistors within these diffusion-sharing chains. The primary mechanism for supporting this concurrent optimization is the placement of transistor subchains, diffusion-break-free components of the full transistor chains. When a chain is reordered, transistors may move from one subchain (and therefore one placement component) to another. We will demonstrate how this permits the chain ordering to be optimized for both intra-chain and inter-chain routing. We combine our placement algorithms with third-party routing and compaction tools, and present the results of a series of experiments which compare our technique with a commercial cell synthesis tool. These experiments make use of a new set of benchmark circuits which provide a rich sample of representative examples in several noncomplementary digital logic families.",Benchmark circuits; Cell Synthesis; Digital circuits; Euler graphs; Noncomplementary circuits; Sequence pair optimization; Transistor chaining; Transistor placement,Algorithms; CMOS integrated circuits; Switching circuits; Topology; VLSI circuits; Cascode voltage switch logic (CVSL); Transistors
Search space definition and exploration for nonuniform data reuse opportunities in data-dominant applications,2003,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037221057&doi=10.1145%2f606603.606610&partnerID=40&md5=83ccb6218c580010432a3aeac3782bab,"Efficient exploitation of temporal locality in the memory accesses on array signals can have a very large impact on the power consumption in embedded data dominated applications. The effective use of an optimized custom memory hierarchy or a customized software controlled mapping on a predefined hierarchy is crucial for this. Only recently have effective systematic techniques to deal with this specific design step begun to appear. They are still limited in their exploration scope. In this paper we construct the design space by introducing three parameters which determine how and when copies are made between different levels in a hierarchy, and determine their impact on the total memory size, storage-related power consumption, and code complexity. Strategies are then established for an efficient exploration, such that cost-effective solutions for the memory size/power trade-off can be achieved, The effectiveness of the techniques is demonstrated for several real-life image processing algorithms.",Data reuse; Memory hierarchy; Power consumption,Algorithms; Computer software; Cost effectiveness; Image processing; Power control; Memory hierarchy; Data storage equipment
Performance-driven placement for dynamically reconfigurable FPGAs,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036826779&doi=10.1145%2f605440.605447&partnerID=40&md5=f6aa5715253e6503ea87df25cf0a0a2a,"In this article, we introduce a new placement problem motivated by the Dynamically Reconfigurable FPGA (DRFPGA) architectures. Unlike traditional placement, the problem for DRFPGAs must consider the precedence constraints among logic components. For the placement, we develop an effective metric that can consider wirelength, register requirement, and power consumption simultaneously. With the considerations of the new metric and the precedence constraints, we then present a three-stage scheme of partitioning, initial placement generation, and placement refinement to solve the new placement problem. Experimental results show that our placement scheme with the new metric achieves respective improvements of 17.2, 27.0, and 35.9% in wirelength, the number of registers, and power consumption requirements, compared with the list scheduling method.",Computer-aided design of VLSI; Dynamically reconfigurable; Field-programmable gate array; Layout; Placement,Algorithms; Electric power utilization; Integrated circuits; VLSI circuits; Logic components; Field programmable gate arrays
Run-time performance optimization of an FPGA-based deduction engine for SAT solvers,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036826771&doi=10.1145%2f605440.605444&partnerID=40&md5=7b028fe74fb457c0a50468071194e365,"FPGAs are a promising technology for accelerating SAT solvers. Besides their high density, fine granularity, and massive parallelism, FPGAs provide the opportunity for run-time customization of the hardware based on the given SAT instance. In this article, a parallel deduction engine is proposed for backtrack search algorithms. The performance of the deduction engine is critical to the overall performance of the algorithm because, for any moderate SAT instance, millions of implications are derived. We propose a novel approach in which p, the amount of parallelization of the engine, is fine-tuned during problem solving in order to optimize performance. Not only the hardware is initially customized based on the input instance, but it is also dynamically modified in terms of p based on the knowledge gained during solving the SAT instance. Compared with conventional deduction engines that correspond to p = 1, we demonstrate speedups in the range of 2.87 to 5.44 for several SAT instances.",Adaptive computing; Boolean satisfiability; Configurable; High performance; Performance trade-offs; Reconfigurable components; Reconfigurable computing; Reconfigurable systems,Adaptive algorithms; Artificial intelligence; Computer hardware; Logic design; Optimization; Deduction engines; Field programmable gate arrays
Reduction design for generic universal switch blocks,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036826842&doi=10.1145%2f605440.605443&partnerID=40&md5=2759844dfcda326fbfa55742b80625c8,"A k-side switch block with W terminals per side is said to be a universal switch block ((k, W)-USB) if every set of the nets satisfying the routing constraint (i.e., the number of nets on each side is at most W) is simultaneously routable through the switch block. The (4, W)-USB was originated by designing better switch modules for 2-D FPGAs, such as Xilinx XC4000-type FPGAs, whereas the generic USBs can be applied in multidimensional or some nonconventional 2-D FPGA architectures. The problem we study in this article is to design (k, W)-USBs with the minimum number of switches for any given pair of (k, W). We provide graph models for routing requirements and switch blocks and develop a series of decomposition theorems for routing requirements with the help of a new graph model. The powerful decomposition theory leads to the automatic generation of routing requirements and a detailed routing algorithm, as well as the reduction design method of building large USBs by smaller ones. As a result, we derive a class of well-structured and highly scalable optimum (k, W)-USBs for k ≤ 6, or even Ws, and near-optimum (k, W)-USBs for k ≥ 7 and odd Ws. We also give routing experiments to justify the routing improvement upon the entire chip using the USBs. The results demonstrate the usefulness of USBs.",Decomposition; FPGA architecture design; Routing; Routing requirement; Switch module; Universal switch block,Algorithms; Electric switches; Integrated circuits; Microprocessor chips; Universal switches; Field programmable gate arrays
Efficient circuit clustering for area and power reduction in FPGAs,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036826796&doi=10.1145%2f605440.605448&partnerID=40&md5=7f14d01ae02ea11fe769e48f4c070885,"We utilize Rent's rule as an empirical measure for efficient clustering and placement of circuits in clustered Field Programmable Gate Arrays (FPGAs). We show that careful matching of resource availability and design complexity during the clustering and placement processes can contribute to spatial uniformity in the placed design, leading to overall device decongestion after routing. We present experimental results to show that appropriate logic depopulation during clustering can have a positive impact on the overall FPGA device area. Our clustering and placement techniques can improve the overall device routing area by as much as 62%, 35% on average, for the same array size, when compared to state-of-the-art FPGA clustering, placement, and routing tools. Power dissipation simulations using a typical buffered pass-transistor-based FPGA interconnect model are also presented. They show that our clustering and placement techniques can reduce the overall device power dissipation by approximately 13%.",Clustering; Congestion; FPGA; Interconnect; Placement; Power; Rent,Algorithms; Integrated circuits; Logic design; Transistors; Circuit clustering; Field programmable gate arrays
Behavioral synthesis of field programmable analog array circuits,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036826797&doi=10.1145%2f605440.605445&partnerID=40&md5=04090aba524f394849185d5163b59cd0,"This article presents methods to translate a behavioral-level analog description into a Field Programmable Analog Array (FPAA) implementation. The methods consist of several steps that are referred to as function decomposition, macrocell synthesis, placement and routing, and post placement simulation. The focus of this article is on the first three steps. The function decomposition step deals with decomposing a high-order system function into a set of lower-order functions. We present an efficient procedure for searching for an optimal solution. This procedure is based on first formally demonstrating the equivalence of two previously used optimization criteria. The objective of the macrocell synthesis step is to generate a hardware realization. A modified signal flow graph is introduced to represent FPAA circuits and graph transformations are used to identify the realizations that comply with the FPAA hardware constraints. The modified signal flow graph also allows scaling of capacitor values due to the limited set of allowable values in an FPAA. For the placement and routing step, an efficient method to estimate the circuit performance degradation due to parasitic effects is given. Using performance degradation as the cost function, an algorithm for finding an optimal FPAA placement and routing configuration is given. The efficacy of the methods developed is demonstrated by direct measurements on a set of filters.",Analog synthesis; Programmable circuits,Computer hardware; Graph theory; Integrated circuits; Optimization; Programmable circuits; Field programmable gate arrays
Instruction generation for hybrid reconfigurable systems,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036826798&doi=10.1145%2f605440.605446&partnerID=40&md5=1d10f4c9c6310b56c13a22da88db5d84,"Future computing systems need to balance flexibility, specialization, and performance in order to meet market demands and the computing power required by new applications. Instruction generation is a vital component for determining these trade-offs. In this work, we present theory and an algorithm for instruction generation. The algorithm profiles a dataflow graph and iteratively contracts edges to create the templates. We discuss how to target the algorithm toward the novel problem of instruction generation for hybrid reconfigurable systems. In particular, we target the Strategically Programmable System, which embeds complex computational units such as ALUs, IP blocks, and so on into a configurable fabric. We argue that an essential compilation step for these systems is instruction generation, as it is needed to specify the functionality of the embedded computational units. In addition, instruction generation can be used to create soft reconfigurable macros-tightly sequenced prespecified operations placed in the reconfigurable fabric.",FPGA; High-level synthesis; Reconfigurable computing,Algorithms; Data flow analysis; Field programmable gate arrays; Graph theory; Iterative methods; Reconfigurable systems; Computer systems
BDD-based bogic bynthesis for LUT-based FPGAs,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036826761&doi=10.1145%2f605440.605442&partnerID=40&md5=5a66df33ea2a14fd28cae22d3112e424,"Contemporary FPGA synthesis is a multiphase process that involves technology-independent logic, optimization followed by FPGA-specific mapping to a target FPGA technology. Conventional technology-independent transformations target standard cells and are unable to optimize circuits with constraints and goals specific to FPGA architectures. This article describes an FPGA-specific logic synthesis approach, which unites multilevel logic transformation, decomposition, and optimization techniques into a single synthesis framework. This system performs network transformation, decomposition, and optimization at an early stage to generate a network that can be directly mapped onto FPGAs. Our techniques are built upon a BDD-based logic decomposition system. With this system, both AND-OR decompositions and AND-XOR decompositions can be identified, resulting in large area savings for synthesized XOR-intensive circuits. To induce good decompositions, a maximum fanout free cone (MFFC) -based partial clustering and collapsing technique is used. This step is followed by an area-minimizing variable partitioning heuristic that decomposes collapsed nodes into LUT-feasible subfunctions. As a postprocessing step, a performance-driven resynthesis phase is performed to alleviate increased delay caused by excessive logic sharing. We compare the quality of results obtained using our techniques with those of academic (BoolMap, SIS) and industry (Altera Quartus) FPGA synthesis tools. Experimental results indicate that the circuits generated by our techniques are not only smaller, but are also significantly faster than those synthesized by conventional FPGA synthesis tools. Furthermore, the computation times required by our techniques are significantly smaller than those of previous techniques.",BDD; Decomposition; FPGA; Logic synthesis,Logic design; Mapping; Mathematical transformations; Optimization; Logic synthesis; Field programmable gate arrays
Initializability analysis of synchronous sequential circuits,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036016193&doi=10.1145%2f544536.544538&partnerID=40&md5=52007d05423c9167c1732bfc69f7f865,"This article addresses the problem of initializing synchronous sequential circuits, that is, of generating the shortest sequence able to drive the circuit to a known state, regardless of the initial state. Logic initialization is considered, being the only one compatible with current commercial tools. A hybrid Genetic Algorithm is proposed, which combines general ideas from evolutionary computation with specific techniques, well suited to the addressed problem. For the first time, experimental results provide data about the complete set of ISCAS'89 circuits, and show that, despite the inherent algorithm incompleteness, the method is capable of finding the optimum result for the considered circuits. A prototypical tool implementing the algorithm found better results than previous methods.",Circuit initialization; Evolutionary Algorithms,Evolutionary algorithms; Genetic algorithms; Problem solving; Synchronization; Trees (mathematics); Circuit initialization; Sequential circuits
Logic transformation for low-power synthesis,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036016191&doi=10.1145%2f544536.544539&partnerID=40&md5=08529e5974b12669898d3cd545bc4fbd,"In this article we present a new approach to the problem of local logic transformation for reducing power dissipation in logic circuits. The proposed approach overcomes one of the critical limitations common to the previous approaches of local logic transformations for low power, namely, a sequential greedy transformation that identifies signals with high switching activities and then resynthesizes the signals one by one. Instead, we identify a set of signal lines as a group for logic transformation, and determine an order of transformation of the signals with the maximum reduction of power dissipation in the circuit. As a practically feasible solution to this problem, we develop a power model called a finite state input transition (FIT) model, which allows the efficient measurement of the change of power dissipation of the circuit for every possible sequence of logic transformations among the signal lines. Experimental results show that the proposed approach performs an extensive local logic transformation, reducing power consumption by 33% on average without any increase of circuit delay.",Logic synthesis; Logic transformation; Low power; Power estimation model,Algorithms; Energy dissipation; Estimation; Mathematical models; Power transmission; Finite state input transition (FIT); Sequential circuits
Monotone bipartitioning problem in a planar point set with applications to VLSI,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036016188&doi=10.1145%2f544536.544537&partnerID=40&md5=efc84efd9745b0dd2392a5431a1cc827,"The monotone bipartitioning problem, in a planar point set with applications to VLSI, was discussed. The worst-case time complexity of the proposed scheme of hierarchical bipartitioning of a floorplan was assumed to be O(nlogn). An efficient linear time algorithm for the problem was also presented.",Complexity of algorithms; Floorplanning; Partitioning; Routing; Very large scale integration (VLSI),Algebra; Algorithms; Geometry; Integrated circuit layout; Problem solving; Set theory; Planar point set; VLSI circuits
Fast placement approaches for FPGAs,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036016182&doi=10.1145%2f544536.544540&partnerID=40&md5=331c4a07fe9064c701b375d99b976514,"Recent trends in FPGA development indicate a strong shift toward design reuse through the use of intellectual property (IP). This design shift has motivated the development of Frontier, a timing-driven FPGA placement system that uses design macroblocks in conjunction with a series of placement algorithms to achieve highly routable and high-performance layouts quickly. In the first stage of design placement, a macro-based floorplanner is used to quickly identify an initial layout based on intermacro connectivity. Next, FPGA routability and performance metrics are used to evaluate the quality of the initial placement. Finally, if the floorplan is determined to be insufficient from a routability or performance standpoint, a feedback-driven placement perturbation step is employed to achieve a lower cost placement. For a collection of large reconfigurable computing benchmark circuits our timing-driven placement system exhibits a 2.6 × speedup in combined place and route time versus commercial FPGA CAD software with improved design performance for most designs. It is shown that floorplanning, placement evaluation, and backend optimization are all necessary to achieve high-performance placement solutions.",Computer-aided design of VLSI; Field-programmable gate arrays; Layout; Synthesis,Computer aided design; Integrated circuit layout; Intellectual property; Optimization; Perturbation techniques; VLSI circuits; Digital systems; Field programmable gate arrays
Technology mapping algorithms for domino logic,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036016181&doi=10.1145%2f544536.544541&partnerID=40&md5=0cb7470af9d7c4d36bb1c12195d71878,"We present an efficient algorithm for technology mapping of domino logic to a parameterized library. The algorithm is optimal for mapping trees consisting of two-input AND/OR nodes, and has a computation time that is polynomial in terms of constraint size. The mapping method is then extended to DAG covering that permits the implicit duplication of logic nodes. Our synthesis procedure maps the complementary logic cones independently when AND/OR logic is to be implemented, and together using dual-monotonic gates in the case of XOR/XNOR logic. The mapping procedure solves the output phase assignment problem as a preprocessing step. Based on a key observation that the output phase assignment could reduce the implementation cost due to the possible large cost difference between two polarities, a 0-1 integer linear programming formulation was designed to minimize the implementation cost. Our experimental results show the effectiveness of the proposed techniques.",Domino logic; Dual-monotonic gates; Parameterized library; Phase assignment; Synthesis; Technology mapping; XOR/XNOR logic,Algorithms; Linear programming; Polynomials; Transistors; Trees (mathematics); Domino logic; Logic gates
Global array reference allocation,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036016179&doi=10.1145%2f544536.544542&partnerID=40&md5=36385064662b1d78f4353617002cad4d,"Embedded systems executing specialized programs have been increasingly responsible for a large share of the computer systems manufactured every year. This trend has increased the demand for processors that can guarantee high-performance under stringent cost, power, and code size constraints. Indirect addressing is by far the most used addressing mode in programs running on these systems, since it enables the design of small and faster instructions. This paper proposes a solution to the problem of allocating registers to array references using auto-increment addressing modes. It extends previous work in the area by enabling efficient allocation in the presence of control-flow statements. The solution is based on an algorithm that merges address registers' live ranges pairwise. An optimizing DSP compiler, from Mindspeed Technologies Inc., is used to validate this idea. Experimental results reveal a substantial improvement in code performance, when comparing to a combination of local auto-increment detection and priority-based register coloring.",Address registers; Auto-increment addressing modes; DSPs; Register allocation,Algorithms; Embedded systems; Optimization; Program compilers; Program processors; Global arrays; Arrays
Cluster assignment for high-performance embedded VLIW processors,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036660095&doi=10.1145%2f567270.567274&partnerID=40&md5=f8cca345d6b8b36a799c1ea4c9ec5588,"Clustering is an effective method to increase the available parallelism in VLIW datapaths without incurring severe penalties associated with a large number of register file ports. Efficient utilization of a clustered datapath requires careful binding/assignment of operations to clusters. The article proposes a binding algorithm that effectively explores trade-offs between in-cluster operation serialization and delays associated with data transfers between clusters. Extensive experimental evidence is provided showing that the algorithm generates high quality solutions for representative kernels, with up to 33% improvement over a state-of-the-art binding algorithm.",Clustered VLIW datapaths; Embedded processors; Embedded systems; Operation binding; Partitioning,Algorithms; Data transfer; Embedded systems; Optimization; Very long instruction word architecture; Cluster assignment; Serialization; Program processors
Partitioning sequential programs for CAD using a three-step approach,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036660088&doi=10.1145%2f567270.567273&partnerID=40&md5=29134ed8fb7368fc24d35fc09a294616,"Many computer-aided design problems involve solutions that require the partitioning of a large sequential program written in a language such as C or VHDL. Such partitioning can improve design metrics such as performance, power, energy, size, input/output lines, and even CAD tool runtime and memory requirements, by partitioning among hardware modules, hardware and software processors, or even among time-slices in reconfigurable computing devices. Previous partitioning approaches typically preselect the granularity at which the program is partitioned. In this article, we define three distinct partitioning steps: procedure determination, preclustering, and N-way partitioning, with the first two steps focusing on granularity selection. Using three steps instead of one can provide for a more thorough design space exploration and for faster partitioning. We emphasize the first two steps in this article since they represent the most novel aspects. We illustrate the approach on an example, provide results of several experiments, and point to the need for future research that more fully automates the three-step approach.",Behavioral partitioning; Functional partitioning; Hardware/software partitioning; Partitioning; System level partitioning,Computational methods; Computer aided software engineering; Computer hardware; Computer software; Data storage equipment; Optimization; Problem solving; Partitioning sequential programs; Three-step approaches; Computer aided design
UST/DME: A clock tree router for general skew constraints,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036660080&doi=10.1145%2f567270.567271&partnerID=40&md5=f4461fc8336d89e02b0b0d4e66267773,"In this article, we propose new approaches for solving the useful-skew tree (UST) routing problem [Xi and Dai 1997]: clock routing subject to general skew constraints. The clock layout synthesis engine of our UST algorithms is based on the deferred-merge embedding (DME) paradigm for the zero-skew tree (ZST) [Edahiro 1992; Chao et al. 1992] and bounded-skew tree (BST) [Cong and Koh 1995; Huang et al. 1995; Kahng and Tsao 1997; Cong et al. 1998] routings; hence, the names UST/DME and Greedy-UST/DME for our UST algorithms. Our novel contribution is that we simultaneously perform skew scheduling and tree routing so that each local skew range is incrementally refined to a skew value that minimizes the wirelength increase during the bottom-up merging phase of DME. As a result, not only is the skew schedule feasible, but also the wirelength increase is minimized at each merging step of clock tree construction. The experimental results show very encouraging improvement over the previous BST/DME algorithm on three ISCAS89 benchmarks under general skew constraints in terms of total routing wirelength.",Clock tree; Feasible skew range; Incremental skew scheduling; Merging and embedding; Merging region; Useful Skew,Algorithms; Clocks; Constraint theory; Mathematical models; Optimization; Routers; Clock tree routers; Skew constraints; Digital signal processing
Efficient scheduling of conditional behaviors for high-level synthesis,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036660340&doi=10.1145%2f567270.567272&partnerID=40&md5=341cfcacd3f1964324728d545d7c8de4,"As hardware designs get increasingly complex and time-to-market constraints get tighter there is strong motivation for high-level synthesis (HLS). HLS must efficiently handle both dataflow-dominated and controlflow-dominated designs as well as designs of a mixed nature. In the past efficient tools for the former type have been developed but so far HLS of conditional behaviors lags behind. To bridge this gap an efficient scheduling heuristic for conditional behaviors is presented. Our heuristic and the techniques it utilizes are based on a unifying design representation appropriate for both types of behavioral descriptions, enabling the proposed heuristic to exploit under the same framework several well-established techniques (chaining, multicycling) as well as conditional resource sharing and speculative execution which are essential in efficiently scheduling conditional behaviors. Preliminary experiments confirm the effectiveness of our approach and prompted the development of the CODESIS HLS tool for further experimentation.",Conditional behavior; Design automation; High level synthesis (HLS); Scheduling,Algorithms; Computer aided software engineering; Computer hardware; Data flow analysis; Heuristic methods; High level synthesis (HLS); Computer aided logic design
Optimal time borrowing analysis and timing budgeting optimization for latch-based designs,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036005110&doi=10.1145%2f504914.504924&partnerID=40&md5=339d6ed5a0db14ad7265613c94b95c09,"An interesting property of a latch-based design is that the combinational path delay is allowed to be longer than the clock cycle as long as it can ""borrow"" time from the shorter paths in the subsequent logic stages. This gives designers a lot of flexibility in designing circuits, especially high performance ones. However, it also increases the complexity in timing analysis. Finding the best clock period or determining how much time to borrow from the subsequent logic stages is difficult especially for designs containing multiple clocks, mixed-clock paths, user-specified multicycle paths, and false paths. In this article, we formulate the time borrowing problem as a linear programming problem. An optimal time borrowing solution can be found by solving the formulation. Based on this time borrowing solver, algorithms are proposed for timing optimization to achieve the optimal clock period. Experimental results show our algorithm is efficient and yields very good results.",Cycle stealing; Latch-based design; Static timing analysis; Time borrowing; Timing budgeting,Algorithms; Linear programming; Optimization; Printed circuit design; Problem solving; Static timing analysis; Flip flop circuits
A search-based bump-and-refit approach to incremental routing for ECO applications in FPGAs,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036826706&doi=10.1145%2f605440.605449&partnerID=40&md5=ddc675a2cfd384336c6a783c5ec3c7a4,A search based bump-and-refit approach to incremental routing for engineering change order (ECO) aplications in field programmable gate arrays (FPGA) was discussed. A new and effective incremental routing methodology for ECO applications was found. Results also showed that the routers are 27% faster and yield almost 10% shorter net lengths than other incremental routing methods.,Bump-and-refit (B&R) paradigm; Bumping cost; Detailed routing; Dynamic programming; ECO (engineering change order); Field programmable gate arrays; Global routing; Incremental routing; Switchbox,Computer aided logic design; Dynamic programming; Integrated circuits; Routers; Incremetal routing method; Field programmable gate arrays
False-noise analysis using logic implications,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036660321&doi=10.1145%2f567270.567276&partnerID=40&md5=aeeb0183f671f577e9ae8189fa92f773,"Cross-coupled noise analysis has become a critical concern in today's VLSI designs. Typically, noise analysis makes the assumption that all aggressing nets can simultaneously switch in the same direction. This creates a worst-case noise pulse on the victim net that often leads to false noise violations. In this article we present a new approach that uses logic implications to identify the maximum set of aggressor nets that can inject noise simultaneously under the logic constraints of the circuit. We propose an approach to efficiently generate logic implications from a transistorlevel description and propagate them in the circuit using ROBDD representations. We propose a new method for lateral propagation of implications and also show how tristate gates and high-impedance signal states can be handled using tristate implications. We then show that the problem of finding the worst-case logically feasible noise can be represented as a maximum weighted independent set problem and show how to efficiently solve it. Initially, we restrict our discussion to zero-delay implications, which are valid for glitch-free circuits, and then extend our approach to timed implications. The proposed approaches were implemented in an industrial noise analysis tool and results are shown for a number of industrial test cases. We demonstrate that a significant reduction in the number of noise failures can be obtained from considering the logic implications as proposed in this article, underscoring the need for false-noise analysis.",Circuit logic; Noise analysis; VLSI (very large scale integration),Algorithms; Formal logic; Performance; Reliability; VLSI circuits; Circuit logic; False-noise; Spurious signal noise
A fast algorithm for context-aware buffer insertion,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036005115&doi=10.1145%2f504914.504922&partnerID=40&md5=217e9097332f1ae3635741d569e09fb8,"We study the problem of performing buffer insertion in the context of a given layout. In a practical situation, there are restrictions on where buffers may be inserted; for instance, it may be possible to route wires over a preplaced macro cell, but may not be possible to insert buffers in that region. As a result, it is desirable to perform route planning and buffer insertion simultaneously. Furthermore it is necessary that such an algorithm be aware of the trade-off between cost (e.g., total capacitance) and delay. In this context we propose the delay reduction to cost ratio (DRCR) problem and present a fast algorithm for the same. Solutions identified by the algorithm are characterized with respect to the overall cost versus performance trade-off curve. Computational experiments demonstrate the viability of the approach.",Physical design; Timing optimization; VLSI,Cost effectiveness; Logic design; Optimization; VLSI circuits; Delay reduction to cost ratio (DRCR); Buffer circuits
Constructing and exploiting linear schedules with prescribed parallelism,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036005117&doi=10.1145%2f504914.504921&partnerID=40&md5=47fbee0e3272c8a6b5b37643da523bd4,"We present two new results of importance in code generation for and synthesis of synchronously scheduled parallel processor arrays and multicluster VLIWs. The first is a new practical method for constructing a linear schedule for the iterations of a loop nest that schedules precisely one iteration per cycle on each of a prescribed set of processors. While this problem goes back to the era in which systolic computation was in vogue, it has defied practical solution until now. We provide a closed form solution that enables the enumeration of all such schedules. The second result is a new technique that reduces the cost of code or hardware whose function is to control the flow of data and predicate operations, and to generate memory addresses. The key idea is that by using the mathematical structure of any of the conflict-free schedules we construct, a very shallow recurrence can be developed to inexpensively update these quantities.",Linear schedule; Multicluster VLIW; Systolic array,Algorithms; Data storage equipment; Iterative methods; Matrix algebra; Parallel processing systems; Linear schedules; Systolic arrays; Very long instruction word architecture
Estimation of state line statistics in sequential circuits,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036660316&doi=10.1145%2f567270.567275&partnerID=40&md5=b793707709e71ec05fee12490280aee1,"In this article, we present a simulation-based technique for estimation of signal statistics (switching activity and signal probability) at the flip-flop output nodes (state signals) of a general sequential circuit. Apart from providing an estimate of the power consumed by the flip-flops, this information is needed for calculating power in the combinational portion of the circuit. The statistics are computed by collecting samples obtained from fast RTL simulation of the circuit under input sequences that are either randomly generated or independently selected from user-specified pattern sets. An important advantage of this approach is that the desired accuracy can be specified up front by the user; with some approximation, the algorithm iterates until the specified accuracy is achieved. This approach has been implemented and tested on a number of sequential circuits and has been shown to handle very large sequential circuits that can not be handled by other existing methods, while using a reasonable amount of CPU time and memory (the circuit s38584.1, with 1426 flip-flops, can be analyzed in about 10 minutes).",Finite-state machine; Power estimation; Sequential circuit; Signal probability; Signal statistics; Switching activity; Transition density,Algorithms; Approximation theory; Computational methods; Computer simulation; Data storage equipment; Electric power utilization; Flip flop circuits; Program processors; State assignment; Statistical methods; Switching; State line statistics; Sequential circuits
An efficient register optimization algorithm for high-level synthesis from hierarchical behavioral specifications,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036005104&doi=10.1145%2f504914.504923&partnerID=40&md5=1c2fc19440db0327918559c69345d2c6,"We address the problem of register optimization that arises during high-level synthesis from modular hierarchical behavioral specifications. Register optimization is the process of grouping carriers such that each group can be safely allocated to a hardware register. Global register optimization by inline expansion involves flattening the module hierarchy and using a heuristic register optimization procedure on the flattened description. Although inline expansion yields a near-optimal number of registers, it is very time consuming due to the large number of carrier compatibility relationships that must be considered. We present an efficient register optimization algorithm that achieves nearly the same effect of inline expansion without actually inline expanding. The distinguishing feature of the proposed algorithm is that it employs a hierarchical optimization phase which effectively exploits the properties of the module call graph and information gathered during local carrier lifecycle analysis of each module. Experimental results on a number of benchmarks show that the proposed algorithm produces nearly the same number of registers as inline expansion based global optimization and is faster by a factor of 7.0.",Behavioral synthesis; Hardware description languages; Hierarchical specifications; High-level synthesis; Lifecycle analysis; Register optimization,Algorithms; Global optimization; Graph theory; Heuristic methods; VLSI circuits; Hardware registers; Shift registers
Satisfiability models and algorithms for circuit delay computation,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036354804&doi=10.1145%2f504914.504920&partnerID=40&md5=82f9dbe1b06d21043f972b73d309be38,"The existence of false paths represents a significant and computationally complex problem in the estimation of the true delay of combinational and sequential circuits. In this article we conduct a comprehensive study of modeling circuit delay computation, accounting for false paths, as a sequence of instances of Boolean satisfiability. Several path sensitization models and delay models are studied. In addition we evaluate some of the most competitive Boolean satisfiability algorithms seeking to identify which are best suited for solving circuit delay computation problems. Finally, realistic delay modeling (taking into account extracted interconnect delays and fanout data) is considered in order to experimentally evaluate the complexity of solving real-world instances.",Boolean satisfiability; Circuit delay computation; Delay modeling; False path; Timing analysis,Algorithms; Boolean functions; Combinatorial circuits; Computational complexity; Delay circuits; Sequential circuits; Delay computations; Digital circuits
ACM Transactions on Design Automation of Electronic Systems: Guest editorial,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036826793&doi=10.1145%2f605440.605441&partnerID=40&md5=56ea5b6c5efda40d2c1d28cdd659ee83,[No abstract available],,
"Microarchitectural synthesis of performance-constrained, low-power VLSI designs",2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036005140&doi=10.1145%2f504914.504919&partnerID=40&md5=a1ce48e35cc63b6108a2113ab9724ead,"New portable signal-processing applications such as mobile telephony, wireless computing, and personal digital assistants place stringent power consumption limits on their constituent components. Substantial power savings can be realized if 5 V designs are translated to use the new lower supply voltage standards. This conversion, however, is not achieved easily: a design originally targeted for implementation in a 5 V technology will typically require significant rework to meet timing and throughput requirements at the lower operating voltage. In this paper we describe a high-level synthesis system which assists the designer in performing this task, minimizing the need for manual redesign. Techniques employed in this work include pipelining and a new approach to module selection that minimizes power consumption subject to timing constraints. Using these and other high-level synthesis techniques to target designs to 3.3 V libraries, we show that it is possible to reduce power consumption by as much as 56% as compared to the original 5 V implementation, while meeting specified minimum throughput and maximum latency constraints.",DSP datapath design; High-level synthesis; Low-power design,Digital signal processing; Electric potential; Mobile computing; Power supply circuits; High-level synthesis; Low-level synthesis; VLSI circuits
Cluster-aware iterative improvement techniques for partitioning large VLSI circuits,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036005096&doi=10.1145%2f504914.504918&partnerID=40&md5=971ed4720cc39f53ce0dc37bac2343c8,"The cluster-aware iterative improvement techniques for partitioning large VLSI circuits were presented. Two new iterative improvement partitioning (IIP) methods CLIP and CDIP were proposed. The algorithms improved partition quality while preserving the advantage of time efficiency. Experimental results on 25 medium to large-size ACM/SIGDA benchmark circuits showed upto 70% improvement over FM in mincut, and average mincut improvements of about 35% over all circuits and 47% over large circuits.",Clusters; Iterative-improvement; Mincut; Physical design/layout; VLSI circuit partitioning,Algorithms; Computer aided design; Integrated circuit layout; Iterative methods; Logic design; Benchmarking circuits; VLSI circuits
Prefetching for improved bus wrapper performance in cores,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036005098&doi=10.1145%2f504914.504917&partnerID=40&md5=cd581b8ec7908e5ddad0c89441512cf2,"Reuse of cores can reduce design time for systems-on-a-chip. Such reuse is dependent on being able to easily interface a core to any bus. To enable such interfacing, many propose separating a core's interface from its internals by using a bus wrapper. However, this separation can lead to a performance penalty when reading a core's internal registers. In this paper, we introduce prefetching, which is analogous to caching, as a technique to reduce or eliminate this performance penalty, involving a tradeoff with power and size. We describe the prefetching technique, classify different types of registers, describe our initial prefetching architectures and heuristics for certain classes of registers, and highlight experiments demonstrating the performance improvements and size/power tradeoffs. We further introduce a technique for automatically designing a prefetch unit that satisfies user-imposed register-access constraints. The technique benefits from mapping the prefetching problem to the well-known real-time process scheduling problem. We then extend the technique to allow user-specified register interdependencies, using a Petri net model, resulting in even mere efficient prefetch schedules.",Bus wrapper; Cores; Design reuse; Intellectual property; Interfacing; On-chip bus; PVCI; System-on-a-chip; VSIA,Heuristic methods; Interfaces (computer); Mapping; Petri nets; Bus wrappers; Prefetching; Microprocessor chips
ATPG tools for delay faults at the functional level,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036005095&doi=10.1145%2f504914.504916&partnerID=40&md5=1d758eac28e8318bddcf8b2ab777cdef,"We present an ATPG tool for functional delay faults which applies to the single-input transition (SIT) and the multi-input transition (MIT) fault models, and is based on Reduced Ordered Binary Decision Diagrams (ROBDDs). We are able, for the first time, to identify all faults that do not have any SIT tests, and generate all SIT tests for nonredundant faults in combinational circuits. We also provide methodologies for efficient generation of MIT tests. Our experimental results on the ISCAS'85 benchmarks is by far superior to existing methods as well as a Satisfiability-based tool that we have developed for comparative purposes. The presented tool, coupled with advancements in path delay fault coverage, shows that both the SIT and MIT functional models are very useful in ATPG for robust path delay faults for synthesized circuits.",Automatic test pattern generation; Binary Decision Diagrams; Boolean Satisfiability; Delay testing; Functional-level testing; Path delay fault simulation (coverage); Path delay fault testing; Testing digital circuits,Algorithms; Combinatorial circuits; Computer aided software engineering; Data structures; Logic design; Automatic test pattern generation (ATPG); Delay faults; Electronic equipment testing
General technology mapping for field-programmable gate arrays based on lookup tables,2002,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036005090&doi=10.1145%2f504914.504915&partnerID=40&md5=bbae90e6f772adee0c7f7191c3f60604,"We present a general technology-mapping methodology (TULIP) for field-programmable gate arrays (FPGAs) that can yield optimal results, and is applicable to any FPGA with a logic block composed of lookup tables (LUTs). We introduce the concept of a virtual switch to model the internal connections of a logic block with multiple LUTs; each configuration of virtual switches is called a multiple-LUT block (MLB). A logic block can be precisely defined by a small but complete set of representative configurations called an MLB basis. The MLB bases for various commercial FPGA families are demonstrated. Given a logic block represented by its MLB basis, technology mapping is precisely formulated as a graph-covering problem, which is transformed into a mixed integer-linear programming (MILP) optimization problem in order to achieve our optimality and generality objectives. The MILP model is solved using a general-purpose MILP solver tool. The results of using TULIP for mapping some ISCAS-85 benchmark circuits to a variety of logic blocks are presented. Circuits of a few hundred gates can be mapped directly in a few minutes. To map larger circuits to complex logic blocks, some approximation techniques are proposed based on partitioning the input circuit and simplifying the MLB basis. We show that these approximations result in close-to-optimal mappings of the benchmark circuits.",Basis; Circuit partitioning; Field-programmable gate arrays; Lookup tables (LUTs); Mapping; Multiple-LUT blocks; Nonrooted trees; Rooted trees; Synthesis,Electric switches; Integer programming; Linear programming; Mapping; Optimization; Table lookup; Trees (mathematics); Mixed integer-linear programming (MILP); Field programmable gate arrays
Using complete-1-distinguishability for FSM equivalence checking,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746838930&doi=10.1145%2f502175.502183&partnerID=40&md5=b394c015d002f56c81ec09482764b062,"This article introduces the notion of a Complete-1-Distinguishability (C-1-D) property for simplifying equivalence checking of finite state machines (FSMs). When a specification machine has the C-1-D property, the traversal of the product machine can be eliminated. Instead, a much simpler check suffices. The check consists of first obtaining a 1-equivalence mapping between the individually reachable states of the specification and the implementation machines, and then checking that it is a bisimulation relation. The C-1-D property can be used directly for specification machines on which it naturally holds - a condition that has not been exploited thus far in FSM verification. We also show how this property can be enforced on an arbitrary FSM by exposing some of its latch outputs as pseudo-primary outputs during synthesis and verification. In this sense, our synthesis/verification methodology provides another point in the trade-off curve between constraints-on-synthesis versus complexity-of-verification. Practical experiences with this methodology have resulted in success with several examples for which it is not possible to complete verification using existing implicit state space traversal techniques. © 2001 ACM.",Algorithms; Bisimulation relation; Complete-1-distinguishability; Equivalence checking; Finite state machine equivalence; Sequential logic synthesis; Theory; Verification,
Constrained polygon transformations for incremental floorplanning,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044526631&doi=10.1145%2f383251.383255&partnerID=40&md5=eb137caa3fccb58f5fc4d367f9184cab,"A productivity-driven methodology for incremental floorplanning is described and the constrained polygon transformation problem, a key step of this methodology, is formulated. The input to the problem consists of a floorplan computed using area estimates and the actual area required for each subcircuit of the floorplan. Informally, the objective is to change the areas of the modules without drastically changing their shapes or locations. We show that the constrained polygon transformation problem is NP-hard and present several fast algorithms that produce results within a few percent of a theoretical lower bound on several floorplans. General Terms: Algorithms. © 2001 ACM.",Floorplanning; Incremental design; Rectilinear polygons,
Data and memory optimization techniques for embedded systems,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746967016&doi=10.1145%2f375977.375978&partnerID=40&md5=dfa502650ff04dc5321b7d648d46bf7a,"We present a survey of the state-of-the-art techniques used in performing data and memoryrelated optimizations in embedded systems. The optimizations are targeted directly or indirectly at the memory subsystem, and impact one or more out of three important cost metrics: area, performance, and power dissipation of the resulting implementation. We first examine architecture-independent optimizations in the form of code transformations. We next cover a broad spectrum of optimization techniques that address memory architectures at varying levels of granularity, ranging from register files to on-chip memory, data caches, and dynamic memory (DRAM). We end with memory addressing related issues. © 2001 ACM.",,
A fast approach to computing exact solutions to the Resource-Constrained Scheduling problem,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044530732&doi=10.1145%2f502175.502178&partnerID=40&md5=54b33ddd4ef3734bad2350e582a1c53d,"This article presents an algorithm that substantially reduces the computational effort required to obtain the exact solution to the Resource Constrained Scheduling (RCS) problem. The reduction is obtained by (a) using a branch-and-bound search technique, which computes both upper and lower bounds, and (b) using efficient techniques to accurately estimate the possible time-steps at which each operation can be scheduled and using this to prune the search space. Results on several benchmarks with varying resource constraints indicate the clear superiority of the algorithm presented here over traditional approaches using integer linear programming, with speed-ups of several orders of magnitude. © 2001 ACM.",Design; Design automation; Enumeration; Exact scheduling; Experimentation; High-level synthesis; Lower and upper bounds; Optimization; Resource-constrained scheduling,
An exact solution to the minimum size test pattern problem,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746845358&doi=10.1145%2f502175.502186&partnerID=40&md5=9a902b9dac3d1c4ad1180a2c4f917d34,"This article addresses the problem of test pattern generation for single stuck-at faults in combinational circuits, under the additional constraint that the number of specified primary input assignments is minimized. This problem has different applications in testing, including the identification of ""don't care"" conditions to be used in the synthesis of Built-in Self-Test (BIST) logic. The proposed solution is based on an integer linear programming (ILP) formulation which builds on an existing Prepositional Satisfiability (SAT) model for test pattern generation. The resulting ILP formulation is linear on the size of the original SAT model for test generation, which is linear on the size of the circuit. Nevertheless, the resulting ILP instances represent complex optimization problems, that require dedicated ILP algorithms. Preliminary results on benchmark circuits validate the practical applicability of the test pattern minimization model and associated ILP algorithm. © 2001 ACM.",Algorithms; Automatic test pattern generation (ATPG); Built-in self-test (BIST); Integer linear programming (ILP); Propositional satisfiability (SAT); Reliability; Verification and test,
Closed form solution to simultaneous buffer insertion/sizing and wire sizing,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044525393&doi=10.1145%2f383251.383256&partnerID=40&md5=f35468d262febcefd13a1b20b951a224,"In this paper, we consider the delay minimization problem of an interconnect wire by simultaneously considering buffer insertion, buffer sizing and wire sizing. We consider three cases, namely using no buffer (i.e., wire sizing alone), using a given number of buffers, and using the optimal number of buffers. We provide elegant closed form optimal solutions for all three problems. These closed form solutions are useful in early stages of the VLSI design flow such as logic synthesis and floorplanning. General Terms: Design, Performance, Theory. © 2001 ACM.",Buffer insertion; Buffer sizing; Closed form solution; Interconnect optimization; Wire sizing,
Data memory design and exploration for low-power embedded systems,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044529719&doi=10.1145%2f502175.502182&partnerID=40&md5=29ca6a3148bd760c363aa538c4390470,"In embedded system design, the designer has to choose an on-chip memory configuration that is suitable for a specific application. To aid in this design choice, we present a memory exploration procedure based on three performance metrics, namely, cache size, the memory access time and the energy consumption. We show the importance of including energy in the performance metrics, since an increase in the cache size and line size reduces the memory access time but does not necessarily reduce the energy consumption. The memory exploration procedures enable us to find the cache configuration (cache size, line size) that satisfies the area and time constraints while minimizing the energy consumption, and the cache configuration that satisfies the area and energy constraints while minimizing the memory access time. The exploration procedures for cache configuration is very efficient since it considers only a selected set of candidate points. Finally, we validate our exploration procedures by running simulation experiments on MediaBench applications. © 2001 ACM.",Data cache; Memory; Search space pruning,
Integrated test of interacting controllers and datapaths,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044526614&doi=10.1145%2f383251.383258&partnerID=40&md5=eab5d0aeacb2957513cc5e4f684f63b0,"In systems consisting of interacting datapaths and controllers and utilizing built-in self test (BIST), the datapaths and controllers are traditionally tested separately by isolating each component from the environment of the system during test. This work facilitates the testing of datapath/controller pairs in an integrated fashion. The key to the approach is the addition of logic to the system that interacts with the existing controller to push the effects of controller faults into the data flow, so that they can be observed at the datapath registers rather than directly at the controller outputs. The result is to reduce the BIST overhead over what is needed if the datapath and controller are tested independently, and to allow a more complete test of the interface between datapath and controller, including the faults that do not manifest themselves in isolation. Fault coverage and overhead results are given for four example circuits. General Terms: Design, Reliability :. © 2001 ACM.",Built-in self-test; Register transfer level; Synthesis-for-testability,
Introducing redundant computations in rtl data paths for reducing bist resources,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044526299&doi=10.1145%2f383251.383253&partnerID=40&md5=8a5fa84def60a66ee2ef36116f018cac,"The need for considering BIST requirements during the scheduling and assignment stages of behavioral synthesis has been demonstrated in previous research and techniques for reducing BIST resources of a data path during these stages of synthesis have been developed. However, the degree of freedom that can be exploited during scheduling and assignment to minimize these resources is often limited by the data and control dependencies of a behavior. In this paper, we propose transformation of a behavior before scheduling and assignment, namely introducing redundant computations, such that the resulting data path is testable using few BIST resources. The transformation makes use of spare capacity of modules to add redundancy that enables test paths to be shared among the modules. A technique for identifying potential BIST resource sharing problems in a behavior and resolving them by redundant computation is presented. Introduction of redundant computations is performed without compromising the latency and functional resource requirement of the behavior. General Terms: Design. © 2001 ACM.",Behavioral synthesis built-in self-test; Data flow graphs; Redundant operations,
Diagnostic simulation of stuck-at faults in sequential circuits using compact lists,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746809123&doi=10.1145%2f502175.502177&partnerID=40&md5=2a388ae2b3d50b20ff21b39bbbf49785,"This article describes a diagnostic fault simulator for stuck-at faults in sequential circuits that is both time and space efficient. The simulator represents indistinguishable classes of faults as memory efficient lists. The use of lists reduces the number of output response comparisons between faults and hence speeds up the simulation process. The lists also make it easy to drop faults when they are fully distinguished from other faults. Experimental results on the ISCAS89 circuits show that the simulator runs significantly faster than an earlier work based on distinguishability matrices, and for large circuits is faster and more memory efficient than a recent method based on lists of indistinguishable faults. The paper provides the first reports on pessimistic and optimistic diagnostic measures for all faults of the large ISCAS circuits with known deterministic tests. The diagnostic fault simulator has also been modified to diagnose defects, given the output responses of failing devices. Results on simulated bridging defects show that the diagnosis time is comparable to the time for fault simulation with fault dropping. © 2001 ACM.",Design; Verification,
POSE: A parallel object-oriented synthesis environment,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044526505&doi=10.1145%2f371254.371263&partnerID=40&md5=c821839f55e6d1bdf9a99b07e8925ab7,"Design automation tools and methodologies always encounter a problem of how systems may be designed efficiently, including issues such as static modeling and dynamic manipulation of system parts. With the rapid progress of design technology, the continuously increasing number of different choices per system part and the growing complexity of today's systems, the efficiency of the design environment is not only a major concern now, but will also be a demanding problem in the near future. In contrast to heuristic methods, a novel environment called POSE is proposed that increases efficiency during design without losing optimality in the final design results. System parts are modeled using the popular object-oriented modeling technique and are dynamically manipulated using the parallel design technique. A complete integration of object-oriented and parallel techniques is one of the major features of POSE. Common problems related to parallel design such as emptiness and deadlock are also elegantly solved within POSE. Experimental results and formal analysis based on POSE all show its practical and theoretical usefulness. POSE can be used at any level of synthesis as long as off-the-shelf building-blocks manipulation is required. POSE can be applied especially to system-level synthesis, whose targets can be parallel computer architectures, systems-on-chip, or embedded systems. We will show how POSE has been applied to ICOS, a recently proposed synthesis methodology. Furthermore, POSE can be easily integrated with other heuristic design methodologies to allow increased design efficiency. © 2001 ACM.",Design; Design-completion check; Hardware synthesis; Object-oriented technology; Parallel design; Synthesis rollback,
Intrinsic response for analog module testing using an analog testability bus,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747001673&doi=10.1145%2f375977.375981&partnerID=40&md5=29072662183ee6aab15e13e93ec94802,"A parasitic effect removal methodology is proposed to handle the large parasitic effects in analog testability buses. The removal is done by an on-chip test generation technique and an intrinsic response extraction algorithm. On-chip test generation creates test signals on-chip to avoid the parasitic effects of the test application bus. The intrinisic response extraction cross-checks and cancels the parasitic effects of both test application and response observation paths. The tests using both SPICE simulation and MNABST-1 P1149.4 test chip reveal that the proposed algorithm can not only remove the parasitic effects of the test buses but also tolerate test signal variations. Furthermore, it is robust enough to handle loud environmental noise and the nonlinearity of the switching devices. © 2001 ACM.",Analog testability bus; Analog testing; Boundary scan; Design for testability; Experimentation; Intrinsic response; Theory,
Verifying sequential equivalence using atpg techniques,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244375415&doi=10.1145%2f375977.376022&partnerID=40&md5=827290e08d17220b78f127d276af30fe,"In this paper we address the problem of verifying the equivalence of two sequential circuits. State-of-the-art sequential optimization techniques such as retiming and sequential redundancy removal can handle designs with up to hundreds or even thousands of flip-flops. However, the BDD-based approaches for verifying sequential equivalence can easily run into memory explosion for such designs. In an attempt to handle larger circuits, we modify test pattern-generation techniques for verification. The suggested approach utilizes the popular efficient backward-justification technique used in most sequential ATPG programs. We present several techniques to enhance the efficiency of this approach by (1) identifying equivalent flip-flop pairs using an induction-based algorithm, and (2) generalizing the idea of exploring the structural similarity between circuits to perform verification in stages. This ATPG-based framework is suitable for verifying circuits either with or without a reset state. In order to extend this approach to verify retimed circuits, we introduce a delay-compensationbased algorithm for preprocessing the circuits. The experimental results of verifying the correctness of circuits after sequential redundancy removal and retiming with up to several hundred flip-flops are presented. © 2001 ACM.",Algorithms; ATPG; Design; Equivalence checking; Machine equivalence; Verification,
Efficient list-approximation techniques for floorplan area minimization,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044526553&doi=10.1145%2f383251.383257&partnerID=40&md5=0b420efc74eb7ca62a0052f758b8f616,"As the sizes of many IC design problems become increasingly larger, approximation has become a valuable approach for arriving at satisfactory results without incurring exorbitant computational cost. In this paper, we present several approximation techniques for solving floorplan area minimization problems. These new techniques enable us to reduce both the time and space complexities of the previously best known approximation algorithms by more than a factor of n and n2 for rectangular and L-shaped subfloorplans, respectively (where n is the number of given implementations). The improvements in the time and space complexities of such approximation techniques is critical to their applicability in floorplan area minimization algorithms. The techniques are quite general, and may be applicable to other classes of approximation problems. General Terms: Algorithms, Design, Theory. © 2001 ACM.",Area minimization; Floorplanning; K-link shortest paths; List approximation,
A mapping algorithm for computer-assisted exploration in the design of embedded systems,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746807825&doi=10.1145%2f371254.371273&partnerID=40&md5=f74768f9aaaa19da5940e3d1fb7425b0,"We present a technique for automatic exploration of architectural alternatives in the design of complex electronic embedded systems and systems-on-a-chip. The technique transforms the problem into a set of simple model-to-model operations and a mapping algorithm that becomes the core of the entire design process. The mapping algorithm is formulated as an assignment-type problem (ATP), which is, in turn, solved by a straightforward optimization method. The result is a design assistance tool, which is demonstrated through a telecommunication systems example. © 2001 ACM.",Codesign; Design; Embedded system design space exploration; Experimentation; Specification mapping,
An algorithm for synthesis of large timeconstrained heterogeneous adaptive systems,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27644493660&doi=10.1145%2f375977.375979&partnerID=40&md5=47402329f83e5e3cc0796d24de5af937,"Large time-constrained applications are highly computer-intensive and are often implemented as a complex organization of pipelined data parallel tasks on a pool of embedded processors, DSP processors, and FPGAs. The large number of design alternatives available at each task level, the application as a whole, and the special needs of the reconfigurable devices (such as the FPGA) make the manual synthesis of such systems very tedious. The automatic synthesis algorithm in this paper combines exact (MILP-based) and heuristic techniques to solve this problem, which basically involves (1) propagation of timing constraints; (2) pipelining the loops to meet throughput requirements; (3) resource selection and allocation, keeping the processing requirements and the timing constraints in view; (4) scheduling the resources across the tasks to ensure maximum utilization; and (5) hiding the reconfiguration delays of the FPGAs. While the use of MILP techniques helps in getting high-quality results, combining them with heuristics ensures acceptable synthesis times, striking a good balance between quality of results and synthesis time. Our experimental evaluation of the algorithm shows an average 40% in resource cost reduction (compared to manual synthesis) with synthesis times from minutes to as low as a few seconds in some cases. © 2001 ACM.",Algorithm; Delay/cost table; Design; Experimentation; Hierarchical control data-flow graph; List scheduling; Mixed integer linear programming; Pipelining; Reconfigurable computing; Timeconstrained synthesis,
Optimizing designs containing black boxes,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646745246&doi=10.1145%2f502175.502184&partnerID=40&md5=d45d8d99025314ad8373ddd223149d46,"We are concerned with optimizing gate-level netlists containing ""black boxes,"" that is, components whose functionality is not available to the optimization tool. We establish a notion of equivalence for gate-level netlists containing black boxes, and prove that it is sound and complete. We show that conventional approaches to optimizing such netlists fail to fully exploit the don't care flexibility available for synthesis. Based on our new notion of equivalence, we introduce a procedure that computes the complete don't care set. Experiments indicate that our procedure can achieve more minimization than conventional synthesis. © 2001 ACM.",Algorithms; Design; Don't cares; Hierarchical logic synthesis; IP-based design; Verification,
Slicible rectangular graphs and their optimal floorplans,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-26844494025&doi=10.1145%2f502175.502176&partnerID=40&md5=c29fce3d55e8725e8feba2ae422fbad6,"Rectangular dualization method of floorplanning usually involves topology generation followed by sizing. Slicible topologies are often preferred for their simplicity and efficiency. While slicible topologies can be obtained efficiently, existing linear-time algorithms for topology generation from a given rectangular graph does not guarantee slicible topologies even if one exists. Moreover, the class of rectangular graphs, known as inherently nonslicible graphs, do not have any slicible topologies. In this article, new tighter sufficiency conditions for slicibility of rectangular graphs are postulated and utilized in the generation of area-optimal floorplans. These graph-theoretic conditions not only capture a larger class of slicible rectangular graphs but also help in reducing the total effort for topology generation, and in solving problems of larger size. © 2001 ACM.",Algorithms; Design; Floorplanning; Graph dualization; Heuristic search; Nonslicible floorplans; Planar graphs; Slicible floorplans; Theory,
On the fundamental limitations of transformational design,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044530909&doi=10.1145%2f502175.502181&partnerID=40&md5=df77664499a9df0561d8197e42b98517,"The completeness of a collection of design transformations is an important aspect in transformational design. Completeness guarantees that any correct design can in principle be explored using the transformation system. In the field of transformational design the problem of incompleteness is not well understood and it is often believed that complete transformation systems can be constructed. In this article, we show, using a formal framework based on the theory of computation, that this is not the case if the transformation system is based on an expressive general-purpose design language such as VHDL. Only when restrictions are imposed on the design language and correctness relation, a transformation system can be made complete in theory, but this is expected to result in serious practical problems. It is shown that the incompleteness problem in transformational design is closely related to the syntactic variance problem in high-level synthesis and that this latter problem is not solvable in general either. © 2001 ACM.",Algorithms; Completeness; Design; Formal methods; High-level synthesis; Languages; Syntactic variance problem; Theory; Transformational design; Verification,
"Performance-constrained hierarchical pipelining for behaviors, loops, and operations",2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746807385&doi=10.1145%2f371254.371256&partnerID=40&md5=f858cf9484a1b1b6440d2fd1f4ac1464,"Behavioral specifications of DSP systems generally contain a number of nested loops. In order to obtain high date rates for such systems, it is necessary to pipeline the system within the behavior, within the loop bodies, and also within the operations. In order to hierarchically pipeline a performance-constrained system, an important step consists of distributing the performance constraint among the loops in such a manner that the constraint is satisfied and design cost is minimized. This paper presents an algorithm for propagating constraints and hierarchically pipelining a given throughput-constrained system. Along with pipelining, the algorithm schedules the operations within the loop bodies and selects components for them, with the aim of minimizing cost while satisfying the constraint propagated to the loop body. Results demonstrate the necessity of pipelining across the three granularity levels in order to obtain high performance designs. They also demonstrate the feasibility and quality of our approach, and indicate that it may be efficiently used for synthesizing or estimating within system-level design. © 2001 ACM.",Algorithms; Component selection; Design; DSP (digital signal processing) systems; Hierarchical pipelining; Loop pipelining; Performance; Pipelined systems; Scheduling,
Optimal design of synchronous circuits using software pipelining techniques,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044531573&doi=10.1145%2f502175.502180&partnerID=40&md5=f39cd8f4858f6929336f176e8e488e6e,"We present a method to optimize clocked circuits by relocating and changing the time of activation of registers to maximize the throughput. Our method is based on a modulo scheduling algorithm for software pipelining, instead of retiming. It optimizes the circuit without the constraint on the clock phases that retiming has, which permits to always achieve the optimal clock period. The two methods have the same overall time complexity, but we avoid the computation of all pair-shortest paths, which is a heavy burden regarding both space and time. From the optimal schedule found, registers are placed in the circuit without looking at where the original registers were. The resulting circuit is a multi-phase clocked circuit, where all the clocks have the same period and the phases are automatically determined by the algorithm. Edge-triggered flip-flops are used where the combinational delays exactly match that period, whereas level-sensitive latches are used elsewhere, improving the area occupied by the circuit. Experiments on existing and newly developed benchmarks show a substantial performance improvement compared to previously published work. © 2001 ACM.",Algorithms; Performance; Resynthesis; Retiming; Software pipelining,
Von neumann hybrid cellular automata for generating deterministic test sequences,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746949505&doi=10.1145%2f383251.383254&partnerID=40&md5=39b9f0d9ed6cfb6798a6549198fd61c2,"We propose an on-chip test pattern generator that uses an one-dimensional cellular automaton (CA) to generate either a precomputed sequence of test patterns or pairs of test patterns for path delay faults. To our knowledge, this is the first approach that guarantees successful on-chip generation of a given test pattern sequence (or a given test set for path delay faults) using a finite number of CA cells. Given a pair of columns (Cu, Cv) of the test matrix, the proposed method uses alternative ""link procedures"" Pj that compute the number of extra CA cells to enable the generation of (Cu, Cv) by the CA. A systematic approach uses the link procedures to minimize the total number of needed CA cells. The performance of the scheme depends on an appropriate choice of link procedures Pj. General Terms: Reliability, Design. © 2001 ACM.",Built-in self-test (BIST); Cellular automata; Test pattern generation,
Co-synthesis of pipeline structures and instruction reordering constraints for instruction set processors,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013447803&doi=10.1145%2f371254.371268&partnerID=40&md5=548f2df5bd3e8c57dfeda9aa30fde3f3,"This paper presents a hardware/software co-synthesis approach to pipelined ISP (instruction set processor) design. The approach synthesizes the pipeline structure from a given instruction set architecture (behavioral) specification. In addition, it generates a set of reordering constraints that guides the compiler back-end (reorderer) to properly schedule instructions so that possible pipeline hazards are avoided and throughput is improved. Co-synthesis takes place while resolving pipeline hazards, which can be attributed to interinstruction dependencies (IIDs). An extended taxonomy of IIDs have been proposed for the systematic analysis of pipeline hazards. Hardware/software methods are developed to resolve IIDs. Algorithms based on taxonomy and resolutions are constructed and integrated into the pipeline synthesis process to explore hardware and software design space. Application benchmarks are used to evaluate possible designs and guide the design decision. The power of the co-synthesis tool PIPER is demonstrated through pipeline synthesis of one illustrative example and two ISPs, including an industrial one (TDY-43). In comparison with other related approaches, our approach achieves higher throughput and provides a systematic way to explore the hardware/software trade-off. © 2001 ACM.",Compiler instruction optimization; Design; Instruction set processor; Pipeline hazards; Pipeline taxonomy; Synthesis; Theory,
Forced simulation: A technique for automating component reuse in embedded systems,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010942180&doi=10.1145%2f502175.502185&partnerID=40&md5=b7a1e289b957c58f789a751892e728b5,"Component reuse techniques have been a recent focus of research because they are seen as the next-generation techniques to handle increasing system complexities. However, there are several unresolved issues to be addressed and prominent among them is the issue of component matching. As the number of reusable components in a component database grows, the task of manually matching a component to the user requirements becomes infeasible. Automating this matching can help in rapid system prototyping, improving quality and reducing cost. In addition, if the matching algorithm is sound, this approach can also reduce precious validation effort. In this article, we propose an algorithm for automatic matching of a design function to a device from a component database. The distinguishing feature of the algorithm is that when successful, it generates an interface that can automatically adapt the device to behave as the function. The algorithm is based on a new simulation relation called forced simulation that is shown to be a necessary and sufficient condition for component matching to be possible for a given pair of function and device. We demonstrate the application of the algorithm by reusing on some programmable components of the Intel family. © 2001 ACM.",Algorithms; Component reuse; Embedded systems; Forced simulation; Interface generation; Simulation relation; Verification,
Introspection: A register transfer level technique for concurrent error detection and diagnosis in data dominated designs,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242325984&doi=10.1145%2f502175.502179&partnerID=40&md5=ff3af15377310525663e876c4bde2beb,We report a register transfer level technique for concurrent error detection and diagnosis in data dominated designs called Introspection. Introspection uses idle computation cyles in the data path and idle data transfer cycles in the interconnection network in a synergistic fashion for concurrent error detection and diagnosis (CEDD). The resulting on-chip fault latencies are one ten-thousandth (10-4) of previously reported system level concurrent error detection and diagnosis latencies. The associated area overhead and performance penalty are negligible. We derive a cost function that considers introspection constraints such as (i) executing an operation on three disjoint function units for diagnosis and (ii) promoting function units to participate in at least one CEDD operation. We formulate integration of introspection constraints into the operation-to-operator binding phase of high-level synthesis as a weighted bipartite matching problem. The effectiveness of introspection and its implementation are illustrated on numerous industrial strength benchmarks. © 2001 ACM.,Concurrent error detection; Diagnosis; Error detection; On line testing; Register transfer level,
Optimal test access architectures for system-on-a-chip,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012157888&doi=10.1145%2f371254.371258&partnerID=40&md5=fd2e0f224382a42ff151833d05d7d9fc,"Test access is a major problem for core-based system-on-a-chip (SOC) designs. Since embedded cores in an SOC are not directly accessible via chip inputs and outputs, special access mechanisms are required to test them at the system level. An efficient test access architecture should also reduce test cost by minimizing test application time. We address several issues related to the design of optimal test access architectures that minimize testing time., including the assignment of cores to test buses, distribution of test data width between multiple test buses, and analysis of test data width required to satisfy an upper bound on the testing time. Even though the decision versions of all these problems are shown to be NP-complete, they can be solved exactly for practical instances using integer linear programming (ILP). As a case study, the ILP models for two hypothetical but nontrivial systems are solved using a public-domain ILP software package. © 2001 ACM.",Design; Reliability,
Processor modeling and code selection for retargetable compilation,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005631775&doi=10.1145%2f383251.383252&partnerID=40&md5=db197dc8a8295b81c4903afc2aabcd66,"Embedded processors in electronic systems typically are tuned to a few applications. Development of processor-specific compilers is prohibitively expensive and, as a result, such compilers, if existing, yield code of an unacceptable quality. To improve this code quality, we developed a processor model that captures the connectivity, the parallelism, and all architectural peculiarities of an embedded processor. We also implemented a retargetable and optimizing compiler working on this model. We present the graph-based processor model, and formally define the code generation task as binding the intermediate representation of an application to this model. We also present a new method for code selection, based on this processor model, that is capable of handling directed acyclic graphs instead of trees. General Terms: Algorithms, Compilation, Design. © 2001 ACM.",Code selection; Embedded systems; Graph; Instruction set graph; Processor modeling; Retargetable code generation; Retargetable compilation; System design,
Architecture-level power estimation and design experiments,2001,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013026605&doi=10.1145%2f371254.371262&partnerID=40&md5=0431493436999455e5068388bc619c86,"Architecture-level power estimation has received more attention recently because of its efficiency. This article presents a technique used to do power analysis of processors at the architecture level. It provides cycle-by-cycle power consumption data of the architecture on the basis of the instruction/data flow stream. To characterize the power dissipation of control units, a novel hierarchical method has been developed. Using this technique, a power estimator is implemented for a commercial processor. The accuracy of the estimator is validated by comparing the power values it produces against measurements made by a gate-level power simulator for the same benchmark set. Our estimation approach is shown to provide very efficient and accurate power analysis at the architecture level. The energy models built for first-pass estimation (such as ALU, MAC unit, register files) are reusable for future architecture design modification. In this article, we demonstrate the application of the technique. Furthermore, this technique can evaluate various kinds of software to achieve hardware/software codesign for low power. © 2001 ACM.",Architecture tradeoff; Architecture-level power estimation; Computer-aided design of VLSI; Control unit; Design; Energy model; Energy table; Experimentation; Functional unit; Hardware/software codesign; Instruction format transition; Performance,
Allocation of FIFO structures in RTL data paths,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746802066&doi=10.1145%2f348019.348044&partnerID=40&md5=b85004ec557958b5184849a8f49129e0,"Along with functional units, storage and interconnects contribute significantly to data path costs. This paper addresses the issue of reducing the costs of storage and interconnect. In a post-datapath synthesis phase, one or more queues can be allocated and variables bound to it, with the goal of reducing storage and interconnect costs. Further, in contrast to earlier work, we support ""irregular"" cdfgs and multicycle functional units for queue synthesis. Initial results on HLS benchmark examples have been encouraging, and show the potential of using queue synthesis to reduce datapath cost. A novel feature of our work is the formulation of the problem for a variety of FIFO structures with their own ""queueing"" criteria. © 2000 ACM.",Algorithms; Data path; Design; FIFO; ILP; RTL; Synthesis,
Synthesis of low-power selectively-clocked systems from high-level specification,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044521615&doi=10.1145%2f348019.348050&partnerID=40&md5=0ff008d214144b070a90807aae3b87f2,"We propose a technique for synthesizing low-power systems from behavioral specifications. We analyze the control flow of the specification model to detect mutually exclusive sections of the computation. A selectively-clocked interconnection of interacting FSMs is automatically generated and optimized, where each FSM controls the execution of one section of computation. Only one of the interacting FSMs is active for a high fraction of the operation time, while the others are idle and their clocks are stopped. Periodically, the active machine releases the control of the system to another FSM and stops. Our interacting FSM implementation achieves consistently lower power dissipation than the functionally equivalent monolithic implementation. On average, 37% power savings and 12% speedup are obtained, despite a 30% area overhead. © 2000 ACM.",Algorithms; Design; Gated clock; High-level synthesis; Low power,
Timing-driven routing for symmetrical array-based FPGAs,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044523757&doi=10.1145%2f348019.348101&partnerID=40&md5=324cca6b091e731b88774609695a8ed0,"In this paper we present a timing-driven router for symmetrical array-based FPGAs. The routing resources in the FPGAs consist of segments of various lengths. Researchers have shown that the number of segments, instead of wirelength, used by a net is the most critical factor in controlling routing delay in an FPGA. Thus, the traditional measure of routing delay on the basis of geometric distance of a signal is not accurate. To consider wirelength and delay simultaneously, we study a model of timing-driven routing trees, arising from the special properties of FPGA routing architectures. Based on the solutions to the routing-tree problem, we present a routing algorithm that is able to utilize various routing segments with global considerations to meet timing constraints. Experimental results show that our approach is very effective in reducing timing violations. © 2000 ACM.",Algorithms; Computer-aided design of VLSI; Design; Experimentation; Field-programmable gate array; Layout; Measurement; Performance; Synthesis,
Code size minimization and retargetable assembly for custom EPIC and VLIW instruction formats,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746728297&doi=10.1145%2f362652.362658&partnerID=40&md5=75a53c2e0afaf265bb49c35c36532ba0,"PICO is a fully automated system for designing the architecture and the microarchitecture of VLIW and EPIC processors. A serious concern with this class of processors, due to their very long instructions, is their code size. One focus of this paper is to describe a series of code size minimization techniques used within PICO, some of which are applied during the automatic design of the instruction format, while others are applied during program assembly. The design of a retargetable assembler to support these techniques also poses certain novel challenges, which constitute the second focus of this paper. Contrary to widely held perceptions, we demonstrate that it is entirely possible to design VLIW and EPIC processors that are capable of issuing large numbers of operations per cycle, but whose code size is only moderately larger than that for a sequential CISC processor. General Terms: Design, Experimentation, Measurement. © 2001 by the Association for Computing Machinery, Inc.",Code size minimization; Custom templates; Design automation; EPIC; Instruction format design; Noop compression; Retargetable assembly; VLIW,
On-chip vs. off-chip memory: the data partitioning problem in embedded processor-based systems,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044524059&doi=10.1145%2f348019.348570&partnerID=40&md5=77c386d4785884f125da6ce7d2b752ba,"Efficient utilization of on-chip memory space is extremely important in modern embedded system applications based on processor cores. In addition to a data cache that interfaces with slower off-chip memory, a fast on-chip SRAM, called Scratch-Pad memory, is often used in several applications, so that critical data can be stored there with a guaranteed fast access time. We present a technique for efficiently exploiting on-chip Scratch-Pad memory by partitioning the application's scalar and arrayed variables into off-chip DRAM and on-chip Scratch-Pad SRAM, with the goal of minimizing the total execution time of embedded applications. We also present extensions of our proposed memory assignment strategy to handle context switching between multiple programs, as well as a generalized memory hierarchy. Our experiments on code kernels from typical applications show that our technique results in significant performance improvements. © 2000 ACM.",Data cache; Data partitioning; Memory synthesis; On-chip memory; Scratch-pad memory; System design; System synthesis,
Regression-based RTL power modeling,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044523003&doi=10.1145%2f348019.348081&partnerID=40&md5=6e8e39ab3eaa2896cfc5d14d300b21f2,"Register-transfer level (RTL) power estimation is a key feature for synthesis-based design flows. The main challenge in establishing a sound RTL power estimation methodology is the construction of accurate, yet efficient, models of the power dissipation of functional macros. Such models should be automatically built, and should produce reliable average power estimates. In this paper we propose a general methodology for building and tuning RTL power models. We address both hard macros (presynthesized functional blocks) and soft macros (functional units for which only a synthesizable HDL description is provided). We exploit linear regression and its nonparametric extensions to express the dependency of power dissipation on input and output activity. Bottom-up off-line characterization of regression-based power macromodels is discussed in detail. Moreover, we introduce a low overhead on-line characterization method for enhancing the accuracy of off-line characterization. © 2000 ACM.",Adaptive characterization; Design; Functional macros; Regression models; RTL design; RTL power modeling; Simulation; Verification,
Structural gate decomposition for depthoptimal technology mapping in LUTBased FPGA designs,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044521062&doi=10.1145%2f335043.335045&partnerID=40&md5=270608d51668d5fe62b01ab80dbdd7aa,"In this paper we study structural gate decomposition in general, simple gate networks for depth-optimal technology mapping using K-input Lookup-Tables CK-LUTs). We show that (1) structural gate decomposition in any Jf-bounded network results in an optimal mapping depth smaller than or equal to that of the original network, regardless of the decomposition method used; and (2) the problem of structural gate decomposition for depth-optimal technology mapping is NP-hard for Jf-unbounded networks when K s 3 and remains NP-hard for Ä'-bounded networks when K & 5. Based on these results, we propose two new structural gate decomposition algorithms, named DOGMA and DOGMA-m, which combine the level-driven nodepacking technique (used in Chortle-d) and the network flow-based labeling technique (used in FlowMap) for depth-optimal technology mapping. Experimental results show that (1) among five structural gate decomposition algorithms, DOGMA-m results in the best mapping solutions; and (2) compared with speed_up (an algebraic algorithm) and TOS (a Boolean approach), DOGMA-m completes decomposition of all tested benchmarks in a short time while speed_up and TOS fail in several cases. However, speed_up results in the smallest depth and area in the following technology mapping steps. General Terms: Design, Experimentation, Measurement, Performance, Theory :. © 2000 ACM.","Computer-aided design of vlsi; Decomposition; Delay minimization; Fpga, logic optimization; Programmable logic; Simplification; Synthesis; System design; Technology mapping",
Environment modeling and language universality,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746444485&doi=10.1145%2f348019.348572&partnerID=40&md5=7afc23a9a151185d8f342f1a57fa3f37,"In this paper we outline a theory for the environment-modeling problem, the problem of abstracting component finite state machines (FSMs) bordering a particular FSM of interest within a network of interacting FSMs. The goal is to lay a theoretical foundation for the automatic state reduction of large FSM networks. We feel this is a prerequisite for the efficient use of many verification techniques. We focus on computing conditions for the safe removal of a component FSM in a FSM network, where removal is safe if it preserves a certain well-defined trace equivalence. We present an optimized algorithm for determining language universality of a FSM, as well as determining independence of a FSM from those of its inputs connected to outputs of neighboring FSMs. These two properties, input independence and language universality, provide the necessary and sufficient conditions for safe removal. In addition, we show how simulation relations can be utilized, both to reduce the cost of computing safe removal and to create an appropriate abstract FSM when safe removal is not possible. © 2000 ACM.",Abstraction; Algorithms; Design; Environment modeling; Language universality; Model checking; Reliability; Theory; Verification,
Modeling layout tools to derive forward estimates of area and delay at the RTL level,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746839362&doi=10.1145%2f348019.348148&partnerID=40&md5=693ab49318929d95f199a0b23fa15402,"Forward estimates of area and delay facilitate effective decision-making when searching the solution space of digital designs. Current estimation techniques focus on modeling the layout result and fail to deliver timely or accurate estimates. This paper presents a novel approach to deriving these area and delay estimates at the RTL level by modeling the layout tool, rather than the layout result. This approach uses machine learning techniques to capture the relationships between general design features (i.e., topology, connectivity, common input, and common output) and layout concepts (i.e., relative placement). Experiments illustrate the formulation of the training set for machine learning in this domain, and also show how we can derive different tool models. Finally, they show how we can use the resultant model to derive forward estimates of area and delay in real-world designs. © 2000 ACM.",Design; Estimation; Estimation techniques; Layout; Machine learning; VLSI CAD,
High-level library mapping for memories,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044522320&doi=10.1145%2f348019.348297&partnerID=40&md5=b7f49fcb21db30a6e36ca4ab2094f1fc,"We present high-level library mapping, a technique that synthesizes a source memory module from a library of target memory modules. In this paper, we define the problem of high-level library mapping for memories, identify and solve the three subproblems associated with this task, and finally combine these solutions into a suite of two memory mapping algorithms. Experimental results on a number of memory-intensive designs demonstrate that our memory mapping approach generates a wide variety of cost-effective designs, often counter-intuitive ones, based on a user-given cost function, the target library, and the mapping algorithm used. © 2000 ACM.",Algorithms; Design; Experimentation; High-level synthesis; Memory libraries; Performance; Technology-mapping,
Three-layer bubble-sorting-based nonManhattan channel routing,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044524063&doi=10.1145%2f348019.350285&partnerID=40&md5=5e6f9b85fc79dbfd83c144a1f464cceb,"It is well known that a nonManhattan channel router can use fewer routing tracks, and is never worse than a Manhattan router in a channel. To my knowledge, a three-layer bubble-sorting-based nonManhattan channel routing problem is always solved by the solution in a two-layer bubble-sorting-based nonManhattan channel routing problem. Recently, an O(kn2) heuristic algorithm [Chaudhary et al. 1991] and an O(k2n) optimal algorithm [Chen et al. 1994] have been proposed, where k is the number of two-layer routing tracks and n is the number of terminals in a bubble-sorting-based nonManhattan channel. In this paper we propose an optimal three-layer bubble-sorting-based nonManhattan routing algorithm to minimize the number of three-layer routing tracks. Furthermore, the time complexity of this optimal algorithm is proven to be in O(hn) time, where h is the number of three-layer routing tracks and n is the number of terminals in a bubble-sorting-based nonManhattan channel. © 2000 ACM.",Algorithms; Bubble-sorting algorithm; Channel routing; Three-layer nonManhattan routing model; Verification,
CLIP: Integer-programming-based optimal layout synthesis of 2D CMOS cells,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044524462&doi=10.1145%2f348019.348234&partnerID=40&md5=fd605daf824834ae888b60402c38db2e,"A novel technique, CLIP, is presented for the automatic generation of optimal layouts of CMOS cells in the two-dimensional (2D) style. CLIP is based on integer-linear programming (ILP) and solves both the width and height minimization problems for 2D cells. Width minimization is formulated in a precise form that combines all factors influencing the 2D cell width-transistor placement, diffusion sharing, and vertical interrow connections - in a common problem space; this space is then searched in a systematic manner by the branch-and-bound algorithms used by ILP solvers. For height minimization, cell height is modeled accurately in terms of the horizontal wire routing density, and a minimum-height layout is found from among all layouts of minimum width. For exact width minimization alone, CLIP's run times are in seconds for large circuits with 30 or more transistors. For both height and width optimization, CLIP is practical for circuits with up to 20 transistors. To extend CLIP to larger circuits, hierarchical methods are necessary. Since CLIP is optimum under the modeling assumptions, its layouts are significantly better than those generated by other, heuristic, layout tools. © 2000 ACM.",Algorithms; Circuit clustering; CMOS networks; Design; Diffusion sharing; Integer linear programming; Integer programming; Layout optimization; Leaf cell synthesis; Module generation; Theory; Transistor chains; Two-dimensional layout,
"On the use of flexible, rectilinear blocks to obtain minimum-area floorplans in mixed block and cell designs",2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11144243619&doi=10.1145%2f329458.329470&partnerID=40&md5=816110c61a503953c7c20753d4ca57d5,This paper presents three minimum-area floorplanning algorithms that use flexible arbitrary rectilinear shapes for the standard cell regions in MBC designs. The first algorithm (pure HCST) introduces a grid traversal technique which guarantees a minimum-area floorplan. The second algorithm (Hybrid-BF) uses a combination of HOST and Breadth First (BF) traversals to give a practical solution that approximately places flexible blocks at specified locations called seeds. The third algorithm (Hybrid-MBF) improves on the shapes of the flexible blocks generated by Hybrid-BF by using a combination of HOST and a Modified Breadth First (MBF) traversal. All three algorithms are polynomial in the number of grid squares. Optimized implementations of Hybrid-BF and Hybrid-MBF required less than two seconds on a SUN SPARCstation10. General Terms: Algorithms. © 2000 ACM.,Floorplanning; Mixed block and cell designs; Rectilinear polygons,
Multiway FPGA partitioning by fully exploiting design hierarchy,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044519880&doi=10.1145%2f329458.329463&partnerID=40&md5=a93ddc5aa52fe8bee0aeeb1a76147972,"In this paper, we present a new integrated synthesis and partitioning method for multiple-FPGA applications. Our approach bridges the gap between HDL synthesis and physical partitioning by fully exploiting the design hierarchy. We propose a novel multiple-FPGA synthesis and partitioning method which is perfomed in three phases: (1) fine-grained synthesis, (2) functional-based clustering, and (3) hierarchical set-covering partitioning. This method first synthesizes a design specification in a fine-grained way so that functional clusters can be preserved based on the structural nature of the design specification. Then, it applies a hierarchical set-covering partitioning method to form the final FPGA partitions. Experimental results on a number of benchmarks and industrial designs demonstrate that I/O limits are the bottleneck for CLB utilization when applying a traditional multiple-FPGA synthesis method on flattened netlists. In contrast, by fully exploiting the design structural hierarchy during the multiple-FPGA partitioning, our proposed method produces fewer FPGA partitions with higher CLB and lower I/O-pin utilizations. General Terms: Design, Experimentation, Performance. © 2000 ACM.",Fine-grained synthesis; Functional clustering; Multi-way partitioning; Multiple-fpga synthesis,
Hardware/software synthesis of formal specifications in codesign of embedded systems,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044523465&doi=10.1145%2f348019.348093&partnerID=40&md5=d804d78dc6e27e38f56554bb44eeb735,"CoDesign aims to integrate the design techniques of hardware and software. In this work, we present a CoDesign methodology based on a formal approach to embedded system specification. This methodology uses the Templated T-LOTOS language to specify the system during all design phases. Templated T-LOTOS is a formal language based on CCS and CSP models. Using Templated T-LOTOS, a system can be specified by observing the temporal ordering in which the events occur from the outside. In this paper we focus on the synthesis of system specified by Templated T-LOTOS. The proposed synthesis algorithm takes advantage of peculiarities of Templates T-LOTOS. Hardware modules are translated into a register transfer-level language that manages some signals in order to drive synchronization, while the software modules are translated into C according to a finite state model whose operations are controlled by a scheduler. The synthesis of the Templated T-LOTOS specification is based on the direct translation of the language operators to ensure that the implemented system is the same as the specified one. © 2000 ACM.",Codesign; Design; Embedded system; Hardware and software synthesis; Verification,
Retargetable compiled simulation of embedded processors using a machine description language,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044525480&doi=10.1145%2f362652.362662&partnerID=40&md5=8cf6e56e81ca886f04655a8704841fd5,"Fast processor simulators are needed for the software development of embedded processors, for HW/SW cosimulation systems, and for profiling and design of application-specific processors. Such fast simulators can be generated based on the machine description language LISA. Using this language to model processor architectures enables the generation of compiled simulators on various abstraction levels, assemblers, and compiler back ends. The article discusses the requirements of software development tools on processor models and presents the approach based on the LISA language. Furthermore, the implementation of a retargetable environment consisting of compiled simulator, debugger, and assembler is presented. Measurements for a verified, cycle-based LISA model of the TI TMS320C62x DSP show that this approach achieves between 37 × and 170 × higher simulation speed compared to a commercial simulator using a standard technique and the same accuracy level. General Terms: Design, Languages, Performance, Verification.",Compiled simulation; Dsp processors; Hw/sw cosimulation; Instruction set simulators; Machine description languages; Processor modeling and simulation; System-on-chip,
Stochastic sequential machine synthesis with application to constrained sequence generation,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044522811&doi=10.1145%2f348019.348566&partnerID=40&md5=833a771a4bd36f3ad2972b727b59b3ed,"In power estimation, one is faced with two problems: (1) generating input vector sequences that satisfy a given statistical behavior (in terms of signal probabilities and correlations among bits); (2) making these sequences as short as possible so as to improve the efficiency of power simulators. Stochastic sequential machines (SSMs) can be used to solve both problems. In particular, this paper presents a general procedure for SSM synthesis and describes a new framework for sequence characterization to match designers' needs for sequence generation or compaction. Experimental results demonstrate that compaction ratios of 1-3 orders of magnitude can be obtained without much loss in accuracy of total power estimates. © 2000 ACM.",Algorithms; Design; Performance; Theory,
Simultaneous reference allocation in code generation for dual data memory bank ASIPs,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044519371&doi=10.1145%2f335043.335047&partnerID=40&md5=30ff84d45b639d01640ecefb13b52088,"We address the problem of code generation for DSP systems on a chip. In such systems, the amount of silicon devoted to program ROM is limited, so application software must be sufficiently dense. Additionally, the software must be written so as to meet various highperformance constraints, which may include hard real-time constraints. Unfortunately, current compiler technology is unable to generate high-quality code for DSPs, whose architectures are highly irregular. Thus, designers often resort to programming application software in assembly-a time-consuming task. In this paper, we focus on providing support for one architectural feature of DSPs that makes code generation difficult, namely multiple data memory banks. This feature increases memory bandwidth by permitting multiple data memory accesses to occur in parallel when the referenced variables belong to different data memory banks and the registers involved conform to a strict set of conditions. We present an algorithm that attempts to maximize the benefit of this architectural feature. While previous approaches have decoupled the phases of register allocation and memory bank assignment, thereby compromising code quality, our algorithm performs these two phases simultaneously. Experimental results demonstrate that our algorithm not only generates high-quality compiled code, but also improves the quality of completely-referenced code. General Terms: Design, Languages :. © 2000 ACM.",Code generation; Code optimization; Graph labelling; Memory bank assignment; Register allocation,
Optimizing computations for effective block-processing,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044524600&doi=10.1145%2f348019.348304&partnerID=40&md5=18cfca7cdd10c9ffe0ea92537e049e8a,"Block-processing can decrease the time and power required to perform any given computation by simultaneously processing multiple samples of input data. The effectiveness of block-processing can be severely limited, however, if the delays in the dataflow graph of the computation are placed suboptimally. In this paper we investigate the application of retiming for improving the effectiveness of block-processing in computations. In particular, we consider the k-delay problem: Given a computation dataflow graph and a positive integer k, we wish to compute a retimed computation graph in which the original delays have been relocated so that k data samples can be processed simultaneously and fully regularly. We give an exact integer linear programming formulation for the k-delay problem. We also describe an algorithm that solves the k-delay problem fast in practice by relying on a set of necessary conditions to prune the search space. Experimental results with synthetic and random benchmarks demonstrate the performance improvements achievable by block-processing and the efficiency of our algorithm. © 2000 ACM.",Algorithms; Combinatorial optimization; Computation dataflow graphs; Design; Embedded systems; High-level synthesis; Integer linear programming; Performance; Retiming; Scheduling; Vectorization,
Editorial,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025406866&doi=10.1145%2f348019.348027&partnerID=40&md5=e3af7f752aa122d98c311634583649a4,[No abstract available],,
Efficient routability check algorithms for segmented channel routing,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044523991&doi=10.1145%2f348019.348574&partnerID=40&md5=13474c5964fdcf17e6d03ad201aaf056,"The segmented channel-routing problem arises in the context of row-based field programmable gate arrays (FPGAs). Since the K-segment channel-routing problem is NP-complete for K ≥ 2, an efficient algorithm using the weighted bipartite-matching approach is developed for this problem. Connections that form a maximum clique are chosen first to be routed to the segmented channel. Then, another maximum clique of the remained connections is routed until all connections have been processed. In addition, a powerful ""unroutability check"" algorithm is uniquely proposed to tell whether the horizontal switches in an interval of the segmented channel are sufficient for routing or not. Hence, we can precisely discriminate the routable and the unroutable ones from all the test cases. As shown in the experiments, average discrimination ratios of 98.8% and 99.4% are obtained for the 2-segmentation and 3-segmentation models, respectively. Moreover, when applying our routing algorithm to the analyzed nonunroutable cases, a routing failure ratio of 1.5% is reported for the 2-segmentation model, compared to Zhu and Wong's 5.9%; also, a routing failure ratio of 0.8% (less than their 4.7%) is obtained for the 3-segmentation model. In total, the routing failure ratio of our routing algorithm is less than 21% of Zhu and Wong's. © 2000 ACM.",,
Efficient optimal design space characterization methodologies,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044523111&doi=10.1145%2f348019.348058&partnerID=40&md5=34b0e8ea0d1c7ed589fe84c37712fe33,"One of the primary advantages of a high-level synthesis system is its ability to explore the design space. This paper presents several methodologies for design space exploration that compute all optimal tradeoff points for the combined problem of scheduling, clock-length determination, and module selection. We discuss how each methodology takes advantage of the structure within the design space itself as well as the structure of, and interactions among, each of the three subproblems. © 2000 ACM.",Bounding; Clock-length determination; Design; Design space exploration; Efficient searching; High-level synthesis; Module selection; Scheduling,
Graph-based code selection techniques for embedded prrsocesso,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044527555&doi=10.1145%2f362652.362661&partnerID=40&md5=29a362bc6e14afcebb4796c103d66b20,"Code selection is an important task in code generation for programmable processors, where the goal is to find an efficient mapping of machine-independent intermediate code to processor-specific machine instructions. Traditional approaches to code selection are based on tree parsing, which enables fast and optimal code selection for intermediate code given as a set of data-flow trees. While this approach is generally useful in compilers for general-purpose processors, it may lead to poor code quality in the case of embedded processors. The reason is that the special architectural features of embedded processors require performing code selection on data-flow graphs, which are a more general representation of intermediate code. In this paper, we present data-flow graph-based code selection techniques for two architectural families of embedded processors: media processors with support for SIMD instructions and fixed-point DSPs with irregular data paths. Both techniques exploit the fact that, in the area of embedded systems, high code quality is a much more important goal than high compilation speed. We demonstrate that certain architectural features can only be utilized by graph-based code selection, while in other cases this approach leads to a significant increase in code quality as compared to tree-based code selection. General Terms: Algorithms, Experimentation. © 2000 by the Association for Computing Machinery, Inc.",Code selection; Data-flow graphs; Embedded processors; Irregular data paths; Simd instructions,
System-level power optimization: techniques and tools,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746995009&doi=10.1145%2f335043.335044&partnerID=40&md5=9ac059f14d123ae7563d4cda7adfd1e8,"This tutorial surveys design methods for energy-efficient system-level design. We consider electronic systems consisting of a hardware platform and software layers. We consider the three major constituents of hardware that consume energy, namely computation, communication, and storage units, and we review methods for reducing their energy consumption. We also study models for analyzing the energy cost of software, and methods for energy-efficient software design and compilation. This survey is organized around three main phases of a system design: conceptualization and modeling, design and implementation, and runtime management. For each phase, we review recent techniques for energy-efficient design of both hardware and software. General Terms: Design. © 2000 ACM.",,
Dynamic state traversal for sequential circuit test generation,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044521657&doi=10.1145%2f348019.348288&partnerID=40&md5=a182961fe1a0b1ed0d5ead4c0baab8f1,"A new method for state justification is proposed for sequential circuit test generation. The linear list of states dynamically obtained during the derivation of test vectors is used to guide the search during state justification. State-transfer sequences that drive the circuit from the current state to the target state may already be known. Otherwise, genetic engineering of existing state-transfer sequences is required. In both cases, genetic-algorithm-based techniques are used to generate valid state justification sequences for the circuit in the presence of the target fault. This approach achieves extremely high fault coverages, and thus outperforms previous deterministic and simulation-based techniques. © 2000 ACM.",Algorithms; Automatic test pattern generation (ATPG); Finite-statemachine traversal; Genetic algorithms; Reliability; Sequential circuits; Simulation-based; Testing; Verification,
Retiming-based factorization for sequential logic optimization,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746782205&doi=10.1145%2f348019.348068&partnerID=40&md5=bc7d074dff2a45ae2c42e5c3c0739273,Current sequential optimization techniques apply a variety of logic transformations that mainly target the combinational logic component of the circuit. Retiming is typically applied as a postprocessing step to the gate-level implementation obtained after technology mapping. This paper introduces a new sequential logic transformation which integrates retiming with logic transformations at the technology-independent level. This transformation is based on implicit retiming across logic blocks and fanout stems during logic optimization. Its application to sequential network synthesis results in the optimization of logic across register boundaries. It can be used in conjunction with any measure of circuit quality for which a fast and reliable gain estimation method can be obtained. We implemented our new technique within the SIS framework and demonstrated its effectiveness in terms of cycle-time minimization on a set of sequential benchmark circuits. © 2000 ACM.,Algorithms; Design; Finite state machines; Retiming; Sequential synthesis,
Power-Delay optimizations in gate sizing,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044519982&doi=10.1145%2f329458.329473&partnerID=40&md5=21723c5f9b81098c796ba46f6ddf1c16,"The problem of power-delay tradeoffs in transistor sizing is examined using a nonlinear optimization formulation. Both the dynamic and the short-circuit power are considered, and a new modeling technique is used to calculate the short-circuit power. The notion of transition density is used, with an enhancement that considers the effect of gate delays on the transition density. When the short-circuit power is neglected, the minimum power circuit is idential to the minimum area circuit. However, under our more realistic models, our experimental results on several circuits show that the minimum power circuit is not necessarily the same as the minimum area circuit. General Terms: Algorithms, Design, Performance. © 2000 ACM.",Optimization; Power estimation; Transistor sizing; Vlsi layout,
Constraint analysis for code generation: basic techniques and applications in facts,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044527934&doi=10.1145%2f362652.362660&partnerID=40&md5=2867c3bf1d03907954cf41e6e46fde47,"Code generation methods for digital signal processors are increasingly hampered by the combination of tight timing constraints imposed by signal processing applications and resource constraints implied by the processor architecture. In particular, limited resource availability (e.g., registers) poses a problem for traditional methods that perform code generation in separate stages (e.g., scheduling followed by register binding). This separation often results in suboptimality (or even infeasibility) of the generated solutions because it ignores the problem of phase coupling (e.g., since value lifetimes are a result of scheduling, scheduling affects the solution space for register binding). As a result, traditional methods need an increasing amount of help from the programmer (or designer) to arrive at a feasible solution. Because this requires an excessive amount of design time and extensive knowledge of the processor architecture, there is a need for automated techniques that can cope with the different kinds of constraints during scheduling. By exploiting these constraints to prune the schedule search space, the scheduler is often prevented from making a decision that inevitably violates one or more constraints. FACTS is a research tool developed for this purpose. In this paper we will elucidate the philosophy and concepts of FACTS and demonstrate them on a number of examples. General Terms: Algorithms, Design. © 2000 by the Association for Computing Machinery, Inc.",Constraint analysis; DSP; Foreground memory; Phase coupling; Register binding; Scheduling,
CMAPS: A cosynthesis methodology for application-oriented parallel systems,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746952413&doi=10.1145%2f329458.329465&partnerID=40&md5=0932ed81792877c5a01d07021b4c8193,"Currently, a lot of research is devoted to system design, and little work is done on requirements analysis. Besides going from specification to design, one of our main objectives is to show how an application problem can be transformed into specifications. Working from the hardware-software codesign perspective, a system is designed starting from an application problem itself, rather than the detailed behavioral specifications. Given an application problem specified as a directed acyclic graph of elementary problems, a hardware-software solution is derived such that the synthesized software, a parallel pseudoprogram, can be scheduled and executed on the synthesized hardware, a set of system-level parallel computer specifications, with heuristically optimal performance. This is known as system-level cosynthesis of application-oriented general-purpose parallel systems for which a novel methodology called Cosynthesis Methodology for Application-Oriented Parallel Systems (CMAPS), is presented. Since parallel programs and multiprocessor architectures are largely interdependent, CMAPS explores the relationship between hardware designs and software algorithms by interleaving the modeling phases and the synthesis phases of both hardware and software. High scalability in terms of problem complexity and easy upgradability to new technologies are achieved through modularization of the input problem specification, of the software algorithms, and of the hardware subsystem models. The work presented in this paper will be beneficial to designers of general-purpose parallel computer systems which must be oriented toward solving some user-specified problem such as the global controller of an industry automation process or a multiprocessor video server. Some application examples are given to illustrate various codesign phases of CMAPS and its feasibility. General Terms: Design. © 2000 ACM.",Application-oriented general-purpose multiprocessors; Hardware-software modeling and cosynthesis; Requirements analysis,
A predictive system shutdown method for energy saving of event-driven computation,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347241054&doi=10.1145%2f335043.335046&partnerID=40&md5=f6d029ea22d59111fddc265dec9863ec,"This paper presents a system-level power management technique for energy saving of event-driven applications. We present a new predictive system-shutdown method to exploit sleep mode operations for energy saving. We use an exponential-average approach to predict the upcoming idle period. We introduce two mechanisms, prediction-miss correction and prewake-up, to improve the hit ratio and to reduce the delay overhead. Experiments on four different event-driven applications show that our proposed method achieves high hit ratios in a wide range of delay overheads, which results in a high degree of energy saving with low delay penalties. General Terms: Design, Experimentation, Performance :. © 2000 ACM.",Event-driven; Power management; Predictive; Sleep mode; System shutdown,
Power optimization of technology-dependent circuits based on symbolic computation of logic implications,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746791189&doi=10.1145%2f348019.348028&partnerID=40&md5=b4eaf72c79352514c5f157b361600085,"This paper presents a novel approach to the problem of optimizing combinational circuits for low power. The method is inspired by the fact that power analysis performed on a technology mapped network gives more realistic estimates than it would at the technology-independent level. After each node's switching activity in the circuit is determined, high-power nodes are eliminated through redundancy addition and removal. To do so, the nodes are sorted according to their switching activity, they are considered one at a time, and learning is used to identify direct and indirect logic implications inside the network. These logic implications are exploited to add gates and connections to the circuit; this may help in eliminating high-power dissipating nodes, thus reducing the total switching activity and power dissipation of the entire circuit. The process is iterative; each iteration starts with a different target node. The end result is a circuit with a decreased switching power. Besides the general optimization algorithm, we propose a new BDD-based method for computing satisfiability and observability implications in a logic network; furthermore, we present heuristic techniques to add and remove redundancy at the technology-dependent level, that is, restructure the logic in selected places without destroying the topology of the mapped circuit. Experimental results show the effectiveness of the proposed technique. On average, power is reduced by 34%, and up to a 64% reduction of power is possible, with a negligible increase in the circuit delay. © 2000 ACM.",Aids; Automation; Design; Design synthesis; Logic design,
Guest Editorial,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025421373&doi=10.1145%2f362652.362654&partnerID=40&md5=153096417ad3b88d5ecc6876ec1c3cf6,[No abstract available],,
A code-motion pruning technique for global scheduling,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044521008&doi=10.1145%2f329458.329461&partnerID=40&md5=2b13854f6d4cee73be295abcd1da9dd2,"In the high-level synthesis of ASICs or in the code generation for ASIPs, the presence of conditionals in the behavioral description represents an obstacle to exploit parallelism. Most existing methods use greedy choices in such a way that the search space is limited by the applied heuristics. For example, they might miss opportunities to optimize across basic block boundaries when treating conditional execution. We propose a constructive method which allows generalized code motions. Scheduling and code motion are encoded in the form of a unified resource-constrained optimization problem. In our approach many alternative solutions are constructed and explored by a search algorithm, while optimal solutions are kept in the search space. Our method can cope with issues like speculative execution and code duplication. Moreover, it can tackle constraints imposed by the advance choice of a controller, such as pipelined-control delay and limited branch capabilities. The underlying timing models support chaining and multicycling. As taking code motion into account may lead to a larger search space, a code-motion pruning technique is presented. This pruning is proven to keep optimal solutions in the search space for cost functions in terms of schedule lengths. General Terms: Design, Algorithms. © 2000 ACM.",Code generation; Code motion; Global scheduling; High-level synthesis; Speculative execution,
A codesign back-end approach for embedded system design,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044523273&doi=10.1145%2f348019.348156&partnerID=40&md5=70897e04acea4ac9b463331b1e0392c9,"Continuous advances in processor and ASIC technologies enable the integration of more and more complex embedded systems. Since their implementations generally require the use of heterogeneous resources (e.g., processor cores, ASICs) in one system with stringent design constraints, the importance of hardware/software codesign methodologies increases steadily. Interfacing heterogeneous hardware and software components together through a communication structure is particularly error prone and time consuming. Hence, on the basis of a generic architecture dedicated to telecommunication and multimedia applications, we propose an extended communication synthesis method that provides characterization of communications and their implementation schemes in the target architecture. This method takes place after the partitioning and scheduling phase and may constitute the basis of a back-end of a codesign framework leading to HW/SW integration. © 2000 ACM.",Codesign; Communications synthesis; Design; Experimentation; HW/SW integration; Measurement; Template architecture,
FILL and FUNI: Algorithms to identify illegal states and sequentially untestable faults,2000,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005575639&doi=10.1145%2f348019.348311&partnerID=40&md5=c5d87f4519dc822bd39dacd91120ce77,"In this paper, we first present an algorithm (FILL) to efficiently identify a large subset of illegal states in synchronous sequential circuits, without assuming a global reset mechanism. A second algorithm, FUNI, finds sequentially untestable faults whose detection requires some of the illegal states computed by FILL. Although based on binary decision diagrams (BDDs), FILL is able to process large circuits by using a new functional partitioning procedure. The incremental building of the set of illegal states guarantees that FILL will always obtain at least a partial solution. FUNI is a direct method that identifies untestable faults without using the exhaustive search involved in automatic test generation (ATG). Experimental results show that FUNI finds a large number of untestable faults up to several orders of magnitude faster than an ATG algorithm that targeted the faults identified by FUNI. Also, many untestable faults identified by FUNI were aborted by the test generator. © 2000 ACM.",,
A flexible datapath allocation method for architectural synthesis,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-7944228490&doi=10.1145%2f323480.323486&partnerID=40&md5=085238e258623c27669932df012f9449,"We present a robust datapath allocation method that is flexible enough to handle constraints imposed by a variety of target architectures. Key features of this method are its ability to handle accurate modeling of datapath units and the simultaneous optimization of direct objective functions. The proposed method consists of a new binding model construction scheme and an optimization technique based on simulated annealing. To illustrate the flexibility of this method, two datapath allocation procedures have been developed for two problem environments: (1) a procedure that incorporates interconnection area and delay estimates, where floor-planning is tightly integrated into datapath allocation; and (2) a procedure that handles registers, register files, and multiport memories for data storage, as well as random and linear topologies for interconnection architectures. Results from these two applications show our method produces competitive designs for benchmark circuits, as well as being flexible enough to be used for a variety of different domains. © 1999 ACM.",Allocation and binding; High-level synthesis,
Two-Level logic minimization for low power,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646392291&doi=10.1145%2f298865.298869&partnerID=40&md5=58e19f0bae28dcd2094c6d1cc0529eca,"In this paper we present a complete Boolean method for reducing the power consumption in two-level combinational circuits. The two-level logic optimizer performs the logic minimization for low power targeting static PLA, general logic gates, and dynamic PLA implementations. We modify the espresso algorithm by adding our heuristics, which bias logic minimization toward lowering power dissipation. In our heuristics, signal probabilities and transition densities are two important parameters. The experimental results are promising. © 1999 ACM.",Logic synthesis; Low power design; Programmable logic array; Two-level logic minimization,
Behavioral synthesis of combinational logic using spectral-based heuristics,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746991228&doi=10.1145%2f307988.308000&partnerID=40&md5=36cad0205841ff76bc9cdbfc78f2268c,"A prototype system developed to convert a behavioral representation of a Boolean function in OBDD form into an initial structural representation is described and experimental results are given. The system produces a multilevel circuit using heuristic rules based on properties of a subset of spectral coefficients. Since the behavioral description is in OBDD form, efficient methods are used to quickly compute the small subset of spectral coefficients needed for the application of the heuristics. The heuristics guide subsequent decompositions of the OBDD, resulting in an iterative construction of the structural form. At each stage of the translation, the form of the decomposition is chosen in order to achieve optimization goals. © 1999 ACM.",Automatic synthesis; Decision diagrams; Design; Design aids; Logic design; Optimization; Spectral methods,
Power optimization using divide-and-conquer techniques for minimization of the number of operations,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746744544&doi=10.1145%2f323480.323489&partnerID=40&md5=09f3c212cd545b0d22ba60e5bee03efc,"We introduce an approach for power optimization using a set of compilation and architectural techniques. The key technical innovation is a novel divide-and-conquer compilation technique to minimize the number of operations for general computations. Our technique optimizes not only a significantly wider set of computations than the previously published techniques, but also outperforms (or performs at least as well as other techniques) on all examples. Along the architectural dimension, we investigate coordinated impact of compilation techniques on the number of processors which provide optimal trade-off between cost and power. We demonstrate that proper compilation techniques can significantly reduce power with bounded hardware cost. The effectiveness of all techniques and algorithms is documented on numerous real-life designs. © 1999 ACM.",Algorithms; Code generation; Transformations,
On the crossing distribution problem,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22644449786&doi=10.1145%2f298865.298868&partnerID=40&md5=abd62e4d6c098eec9136c3cbb201169f,"VLSI layout design is typically decomposed into four steps: placement, global routing, routing region definition, and detailed routing. The crossing distribution problem occurs prior to detailed routing [Groenveld 1989; Marek-Sadowska and Sarrafzadeh 1995; Wang and Shung 1992]. A crossing is defined as the intersection of two nets. The problem of net crossing distribution is important in layout design, such as design of dense chips, multichip modules (MCM), critical net routing, and analog circuits [Groenveld 1989; Sarrafzadeh 1995; Wang and Shung 1992]. It is observed that nets crossing each other are more difficult to route than those that do not cross. The layout of crossing nets has to be realized in more than two layers and requires a larger number of vias. In this paper we study the crossing distribution problem of two-terminal nets between two regions. We present an optimal O(n2) time algorithm for two-sided nets, where n is the number of nets. Our results are superior to previous ones [Marek-Sadowska and Sarrafzadeh 1995; Wang and Shung 1992]. We give an optimal O(n2) time algorithm for the crossing distribution problem with one-sided nets. We solve optimally the complete version of the crossing distribution problem for two-terminal nets in two regions that has not been studied before. © 1999 ACM.",Crossings; VLSI layout,
Performance estimation of embedded software with instruction cache modeling,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22844455988&doi=10.1145%2f315773.315778&partnerID=40&md5=510fe8334dd2ef2aa59e0e7591e9f92b,"Embedded systems generally interact in some way with the outside world. This may involve measuring sensors and controlling actuators, communicating with other systems, or interacting with users. These functions impose real-time constraints on system design. Verification of these specifications requires computing an upper bound on the worst-case execution time (WCET) of a hardware/software system. Furthermore, it is critical to derive a tight upper bound on WCET in order to make efficient use of system resources. The problem of bounding WCET is particularly difficult on modern processors. These processors use cache-based memory systems that vary memory access time based on the dynamic memory access pattern of the program. This must be accurately modeled in order to tightly bound WCET. Several analysis methods have been proposed to bound WCET on processors with instruction caches. Existing approaches either search all possible program paths, an intractable problem, or they use highly pessimistic assumptions to limit the search space. In this paper we present a more effective method for modeling instruction cache activity and computing a tight bound on WCET. The method uses an integer linear programming formulation and does not require explicit enumeration of program paths. The method is implemented in the program Cinderella and we present some experimental results of this implementation. © 1999 ACM.",,
A Text-Compression-Based method for code size minimization in embedded systems,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22644451924&doi=10.1145%2f298865.298867&partnerID=40&md5=4e6d8d6f1f57380582f3bdeba06539c2,"We address the problem of code-size minimization in VLSI systems with embedded DSP processors. Reducing code size reduces the production cost of embedded systems. We use data-compression methods to develop code-size minimization strategies. In our framework, the compressed program consists of a skeleton and a dictionary. We show that the dictionary can be computed by solving a set-covering problem derived from the original program. To execute the compressed code, we describe two methods that have different performance characteristics and different degrees of freedom in compressing the code. We also address performance considerations, and show that they can be incorporated easily into the set-covering formulation, and present experimental results obtained with Texas Instruments' optimizing TMS320C25 compiler. © 1999 ACM.",Code size optimization; Compression,
Symbolic synthesis of clock-gating logic for power optimization of synchronous controllers,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22844453908&doi=10.1145%2f323480.323482&partnerID=40&md5=f9c7be2cabf65d8a66617d277e95de65,"Recent results have shown that dynamic power management is effective in reducing the total power consumption of sequential circuits. In this paper, we propose a bottom-up approach for the automatic extraction and synthesis of dynamic power management circuitry starting from structural logic-level specifications. Our techniques leverage the compact BDD-based representation of Boolean and pseudo-Boolean functions to detect idle conditions where the clock can be stopped without compromising functional correctness. Moreover, symbolic techniques allow accurate probabilistic computations; in particular, they enable the use of non-equiprobable primary input distributions, a key step in the construction of models that match the behavior of real hardware devices with a high degree of fidelity. The results are encouraging, since power savings of up to 34% have been obtained on standard benchmark circuits. © 1999 ACM.",Design,
BIFEST: A built-in intermediate fault effect sensing and test generation system for cmos bridging faults,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22644448915&doi=10.1145%2f307988.307992&partnerID=40&md5=c08d896ebeea75235686d9ac65d3cc73,"This paper presents BIFEST, an ATPG system that employs the built-in intermediate voltage test technique in an efficient ATPG process to deal with CMOS bridging faults. Fast and accurate calculations of the intermediate bridging voltages and the variant threshold tolerance margins on a resistive bridging fault model are presented. A PODEM-like, PPSFP-based ATPG process is developed to generate test patterns for faults that are conventionally logic-testable. The remaining faults are then dealt with by special circuits, called built-in intermediate voltage sensors (BIVSs). By this methodology, almost the same fault coverage as that employing IDOQ testing can be achieved with only logic monitoring required. © 1999 ACM.",Design; Experimentation; Reliability,
Functional multiple-output decomposition with application to technology mapping for lookup table-based FPGAs,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22844453505&doi=10.1145%2f315773.315783&partnerID=40&md5=280ca2e9815dcb7788c40ba8e36d01e9,"Functional decomposition is an important technique for technology mapping to lookup table-based FPGA architectures. We present the theory of and a novel approach to functional disjoint decomposition of multiple-output functions, in which common subfunctions are extracted during technology mapping. While a Boolean function usually has a very large number of subfunctions, we show that not all of them are useful for multiple-output decomposition. We use a partition of the set of bound set vertices as the basis to compute preferable decomposition functions, which are sufficient for an optimal multiple-output decomposition. We propose several new algorithms that deal with central issues of functional multiple-output decomposition. First, an efficient algorithm to solve the variable partitioning problem is described. Second, we show how to implicitly compute all preferable functions of a single-output function and how to identify all common preferable functions of a multiple-output function. Due to implicit computation in the crucial steps, the algorithm is very efficient. Experimental results show significant reductions in area. © 1999 ACM.",Assignable functions; Boolean functions; Computer-aided design of VLSI; Decomposition; FPGA technology; Implicit BDD-based methods; Mapping synthesis; Multiple-output decomposition; Preferable functions; Subfunction sharing gain; Subfunction sharing potential,
A methodology and algorithms for the design of hard real-time multitasking asics,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22844455249&doi=10.1145%2f323480.323491&partnerID=40&md5=e5282d4147029393de2551e17f0c8dc1,"Traditional high-level synthesis concentrates on the implementation of a single task (e.g. filter, linear controller, A/D converter). However, many applications-multifunctional embedded controllers, intelligent wireless end-points, and DSP and multimedia servers-are defined as sets of several computational tasks. This paper describes new techniques for the synthesis of ASIC implementations that realize multiple computational processes under hard real-time constraints. Our synthesis methodology establishes connections between two important computer engineering domains: operating systems and behavioral synthesis. Our hierarchical approach starts from an incompletely-specified preliminary solution and uses, interchangeably, operating system and behavioral synthesis techniques to derive increasingly more detailed and accurate design solutions. We have experimented with both optimal and heuristic algorithms to implement this methodology. The optimal algorithm uses several heuristics to speed up the average run time of an exhaustive branch-and-bound search. Force-directed optimization is the core of the heuristic synthesis method. Analysis of the proposed algorithms and the experiments shows that matching the number of bits and type of operations in tasks assigned to the same application-specific processor was the most important factor in obtaining area-efficient designs. © 1999 ACM.",Algorithms; Design; Performance,
Code generation of nested loops for DSP processors with heterogeneous registers and structural pipelining,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22844454276&doi=10.1145%2f315773.315776&partnerID=40&md5=4c5c254620c89fbf9fe1952aa8586472,"We propose a microcode-optimizing method targeting a programmable DSP processor. Efficient generation of microcodes is essential to better utilize the computation power of a DSP processor. Since most state-of-the-art DSP processors feature some sort of irregular architectures and most DSP applications have nested loop constructs, their code generation is a nontrivial task. In this paper, we consider two features frequently found in contemporary DSP processors - structural pipelining and heterogeneous registers. We propose a code generator that performs instruction scheduling and register allocation simultaneously. The proposed approach has been implemented and evaluated using a set of benchmark core algorithms. Simulation of the generated codes targeted towards the TI TMS320C40 DSP processor shows that our system is indeed more effective compared with a commercial optimizing DSP compiler. © 1999 ACM.",Code generation; DSP,
Procedure cloning: A transformation for improved system-level functional partitioning,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22644452749&doi=10.1145%2f298865.298871&partnerID=40&md5=72d28a82ce652bdd6e0493cb5849412d,"Functional partitioning assigns the functions of a system's program-like specification among system components, such as standard-software and cusotm-hardwre processors. We introduce a new transformation, called procedure cloning, that significantly improves functional partitioning results. The transformation creates a clone of a procedure for sole use by a particular procedure caller, so the clone can be assigned to the caller's processor, which in turn improves performance through reduced communication. Heuristics are used to prevent the exponential size increase that could occur if cloning were done indiscriminately. We introduce a variety of cloning heuristics, highlight experiments demonstrating the improvements obtained using cloning, and compare the various cloning heuristics. © 1999 ACM.",Behavioral synthesis; Embedded systems; Functional partitioning; Hardware/software codesign; Replication; System-level design; System-on-a-chip; Transformations,
Simulation and sensitivity of linear analog circuits under parameter variations by robust interval analysis,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346746790&doi=10.1145%2f315773.315780&partnerID=40&md5=4be557e3c0ec8c51a64dfba4864f086d,"An interval-mathematic approach is presented for frequency-domain simulation and sensitivity analysis of linear analog circuits under parameter variations. With uncertain parameters represented as intervals, bounding frequency-domain responses is formulated as the problem of solving systems of linear interval equations. The formulation is based on a variant of modified nodal analysis, and is particularly amenable to interval analysis. Some characterizations of the solution sets of systems of linear interval equations are derived. With these characterizations, an elegant and efficient algorithm is proposed to solve systems of linear interval equations. While the widely used Monte Carlo approach requires many circuit simulations to achieve even moderate accuracy, the computational cost of the proposed approach is about twice that of one circuit simulation. The computed response bounds contain provably, or are usually very close to, the actual response bounds. Further, sensitivity under parameter variations can be computed from the response bounds at minor computational cost. The algorithms are implemented in SPICE3F5, using sparse-matrix techniques and tested on several practical analog circuits. © 1999 ACM.",,
Power reduction and power-Delay trade-offs using logic tranformations,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0006261761&doi=10.1145%2f298865.298872&partnerID=40&md5=5689184167745e651671af4d41348e88,"We present an efficient technique to reduce the switching activity in a technology-mapped CMOS combinational circuit based on local logic transformations. The transformations consist of adding redundant connections or gates so as to reduce switching activity. We describe simple and efficient procedures, based on logic implication, for identifying the sources and targets of the redundant connections. Additionally, we give procedures that permit the designer to trade-off power and delay after the transformations. Results of experiments on both the MCNC benchmark circuits and the circuits of a PowerPC microprocessor chip are given. The results indicate that significant power reduction of a CMOS combinational circuit can be achieved with very low area overhead, delay penalty, and computational cost. © 1999 ACM.",CMOS logic; Logic optimization; Logic synthesis; Low power; Power estimation,
Formal verification in. hardware design: a survey,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005440654&doi=10.1145%2f307988.307989&partnerID=40&md5=30ea531b67dac9fee4144be9280f654a,"In recent years, formal methods have emerged as an alternative approach to ensuring the quality and correctness of hardware designs, overcoming some of the limitations of traditional validation techniques such as simulation and testing. There are two/ main aspects to the application of formal methods in a design process: the formal framework used to specify desired properties of a design and the verification techniques and tools used to reason about the relationship between a specification and a corresponding implementation. We survey a variety of frameworks and techniques proposed in the literature and applied to actual designs. The specification frameworks we describe include temporal logics, predicate logic, abstraction and refinement, as well as containment between u)-regular languages. The verification techniques presented include model checking, automatatheoretic techniques, automated theorem proving, and approaches that integrate the above methods. In order to provide insight into the scope and limitations of currently available techniques, we present a selection of case studies where formal methods were applied to industrial-scale designs, such as microprocessors, floating-point hardware, protocols, memory subsystems, and communications hardware. :. © 1999 ACM.","Case studies; Design; Formal methods; Formal verification; Hardware verification; Language containment; Model checking, survey; Theorem proving; Verification",
Bus-Based communication synthesis on system level,1999,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005419196&doi=10.1145%2f298865.298866&partnerID=40&md5=f324b15a84a985c40c208fd7f3ea2daa,"In this article, we present an approach to automatic generation of communication topologies for statically scheduled systems or subsystems. Given a specification containing a set of processes that communicate via abstract send and receive functions, we show how a cost-efficient communication topology consisting of one or more buses without arbitration scheme can be set up for such applications. © 1999 ACM.",Bus generation; Bus without arbitration; Communication synthesis; Statically scheduled systems; Transfer scheduling,
Estimating the storage requirements of the rectangular and l-shaped corner stitching data structures,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746776804&doi=10.1145%2f290833.290850&partnerID=40&md5=f4fbf4736bfd90dc49e4a7e467db9a8a,"This paper proposes a technique for estimating the storage requirements of the Rectangular Corner Stitching (RCS) data structure [Ousterhout 1984] and the L-Shaped Corner Stitching (LCS) data structure [Mehta and Blust 1997] on a given circuit by studying its (the circuit's) geometric properties. This provides a method for estimating the storage requirements of a circuit without having to implement the corner stitching data structure, which is a tedious and time-consuming task. This technique can also be used to estimate the amount of space saved by employing the LCS data structure over the RCS data structure on a given circuit. © 1998 ACM.",Algorithms; Corner stitching; Data structures; L-shapes; Memory requirements analysis; Rectangle; Rectilinear polygons,
Eliminating false loops caused by sharing in control path,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746667803&doi=10.1145%2f293625.293635&partnerID=40&md5=b9da4d689b3880e02d38840c519d95d7,"In high-level synthesis, resource sharing may result in a circuit containing false loops that create great difficulty in timing validation during the design sign-off phase. It is hence desirable to avoid generating any false loops in a synthesized circuit. Previous work [Stok 1992; Huang et.al. 1995] considered mainly data path sharing for false loop elimination. However, for a complete circuit with both data path and control path, false loops can be created due to control logic sharing. In this article, we present a novel approach to detect and eliminate the false loops caused by control logic sharing. An effective filter is devised to reduce the computational complexity of false loop detection, which is based on checking the level numbers that are propagated from data path operators to inputs and outputs of the control path. Only the input/output pairs of the control path identified by the filter are further investigated by traversing into the data path for false loop detection. A removal algorithm is then applied to eliminate the detected false loops, followed by logic minimization to further optimize the circuit. Experimental results show that for the nine example circuits we tested, the final designs after false loop removal and logic minimization give only slightly larger area than the original ones that contain false loops. © 1999 ACM.",Algorithms; Control path; Design; False loop,
Breakpoints and breakpoint detection in source-level emulation,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746776803&doi=10.1145%2f290833.290843&partnerID=40&md5=14741b1ffb47fffaa542a9bb0501d93a,We present an approach for accelerating the validation speed of behavioral system descriptions through hardware emulation. The method allows source-level debugging of running hardware specified in behavioral VHDL in a way similar to source-level debugging in software programming languages. We discuss breakpoints in source-level emulation and how the circuit generated by high-level synthesis has to be modified to work with breakpoints. Breakpoint encoding and detection are shown in detail. Our approach allows breakpoint detection by hardware without seriously slowing the circuit or dramatically increasing its size. © 1998 ACM.,Debugging; Design; Emulation; High-level synthesis; Performance; Verification,
A new viewpoint on code generation for directed acyclic graphs,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645604364&doi=10.1145%2f270580.270583&partnerID=40&md5=1dfa503f3a23c04f1812d75981966060,"We present a new viewpoint on code generation for directed acyclic graphs (DAGs). Our formulation is based on binate covering, the problem of satisfying, with minimum cost, a set of disjunctive clauses, and can take into account commutativity of operators and of the machine model. An important contribution of this work is a set of necessary and sufficient conditions for a valid schedule to be derived, based on the notion of worms and worm-partitions. This set of conditions can be compactly expressed with clauses that relate scheduling to code selection. For the case of one-register machines, we can derive clauses that lead to generation of optimal code for the DAG. Recent advances in exact binate covering algorithms allow us to use this strategy to generate optimal code for large basic blocks. The optimal code generated by our algorithm results in significant reductions in overall code size. © 1998 ACM.",Algorithms; Binate covering; Code generation; Directed acyclic graphs,
On measuring the effectiveness of various design validation approaches for PowerPC™ microprocessor embedded arrays,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746742973&doi=10.1145%2f296333.296335&partnerID=40&md5=698e07c69e0f864ebc84438867317ec8,"Design validation for embedded arrays remains as a challenging problem in today's microprocessor design environment. At Somerset, validation of array designs relies on both formal verification and vector simulation. Although several methods for array design validation have been proposed and had great success [Ganguly et al. 1996; Pandey et al. 1996, 1997; Wang and Abadir 1997], little evidence has been reported for the effectiveness of these methods with respect to the detection of design errors. In this paper, we measure the effectiveness of different validation approaches based on automatic design error injection and simulation. The technique provides a systematic way to evaluate various validation approaches at both logic and transistor levels. Experimental results on recent PowerPC microprocessor arrays will be discussed and reported. General Terms: Assertion test generation, ATPG, design error model, logic verification, symbolic trajectory evaluation, validation © 1999 ACM.",,
Confidence analysis for defect-level estimation of vlsi random testing,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746736201&doi=10.1145%2f293625.293629&partnerID=40&md5=a24512c2273a39c95e57b8ca4c883597,"The defect level in circuit testing is the percentage of circuits, such as chips, that are defective and shipped for use after testing. Our previously published results showed that the defect level of circuit fabrication and testing should be a probability distribution, rather than a single value, and the concept of confidence degree was proposed [Gondalia et al. 1993; Jone et al. 1995]. In this work, defect level is represented by a confidence interval which is more conventional and easier to interpret. The point estimate of defect level analysis and conditions to avoid meaningless confidence intervals are also investigated. Methods for adaptive random test length determination driven by different confidence intervals or interval length are proposed to meet both test requirements and test costs tradeoff. Finally, a complete test plan that can direct the test flow from fabrication infancy to maturity is suggested. © 1999 ACM.",Defect-level analysis; Experimentation; Measurement; Performance; Random testing; Reliability; Test confidence analysis; Test quality; Testing; VLSI testing,
Cluster-Cover: A Theoretical Framework for a Class of VLSI-CAD Optimization Problems,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017244310&doi=10.1145%2f270580.270584&partnerID=40&md5=0e5f8b26c353bf83d7d5d09395dbb0ce,"This article introduces a mathematical framework called cluster-cover. We show that this framework captures the combinatorial structure of a class of VLSI design optimization problems, including two-level logic minimization, constrained encoding, multilayer topological planar routing, application timing assignment for delay-fault testing, and minimization of monitoring logic for BIST enhancement. These apparently unrelated problems can all be cast into two metaproblems in our framework: finding a maximum cluster and finding a minimum cover. We describe paradigms for developing algorithms for these problems. First, a simple heuristic called greedy peeling is presented and characterized. We derive sufficient conditions that guarantee optimum solutions by greedy peeling. We generalize the performance analysis of a multilayer topological planar routing heuristic to greedy peeling for the general clustercover problems. We propose a performance bound of greedy set covering that can be computed efficiently for a given problem instance; this bound is much tighter than the previously known bounds. Second, prime covering-originally developed for logic minimization-is generalized to finding exact solutions for cluster-cover problems. Previously, only the connection between logic minimization and constrained encoding was known. © 1998, ACM. All rights reserved.",Algorithms; Cluster-cover; Design; logic minimization; NP-completeness; Performance; self-checking logic design; state assignment; topological routing; Verification,
Auxiliary variables for bdd-based representation and manipulation of boolean functions,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746689130&doi=10.1145%2f293625.293626&partnerID=40&md5=43ffd85742f91e4b2894f05dddb1d348,"BDDs are the state-of-the-art technique for representing and manipulating Boolean functions. Their introduction caused a major leap forward in synthesis, verification, and testing. However, they are often unmanageable because of the large amount of nodes. To attack this problem, we insert auxiliary variables that decompose monolithic BDDs in smaller ones. This method works very well for Boolean function representation. As far as combinational circuits are concerned, representing their functions is the main issue. Going into the sequential domain, we focus on traversal techniques. We show that, once we have Boolean functions in decomposed form, symbolic manipulations are viable and efficient. We investigate the relation between auxiliary variables and static and dynamic ordering strategies. Experimental evidence shows that we achieve a certain degree of independence from variable ordering. Thus, this approach can be an alternative to dynamic re-ordering. Experimental results on Boolean function representation, and exact and approximate forward symbolic traversal of FSMs, demonstrate the benefits both in terms of memory requirements and of CPU time. © 1999 ACM.",Binary decision diagrams; Design; Finite state machines; Functional decompositions; Reachability analysis; Verification,
Structural diagnosis of interconnects by coloring,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746720100&doi=10.1145%2f290833.290848&partnerID=40&md5=7765d7e843e3b76ae8936f2d88334d10,"This paper presents a new approach for diagnosing shorts in interconnects in which the adjacencies between nets are known. This structural approach exploits different graph coloring techniques to generate a test set with no aliasing and confounding, i.e., full diagnosis (detection and location) is accomplished. Initially, a simple coloring approach based on a greedy condition, of the adjacency graph is proposed for fault detection. Then, the conditions for aliasing and confounding are analyzed with respect to the sizes of the possible shorts. These results are used to generate new colors using a process called color mixing. Color mixing guarantees that additional tests, required in order to avoid aliasing/confounding, will use appropriate codes. The characteristics of unbalanced/balanced codes for encoding the colors in the vector-generation process of interconnect diagnosis are discussed and are proved to yield full diagnosis using a novel method. An algorithm for full diagnosis is then presented; this algorithm has an execution complexity of O(max{N2, N X D3}) where N is the number of nets and D is the maximum degree of the nodes in the adjacency graph. Simulation results show that the proposed approach requires a smaller number of test vectors than previous approaches. © 1998 ACM.",Balanced code; Diagnosis; Graph coloring; Interconnect; Reliability; Syndrome; Verification,
Efficient equivalence checking of multi-phase designs using phase abstraction and retiming,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746770401&doi=10.1145%2f296333.296348&partnerID=40&md5=941c112083d38395fa35eab6f3f2c4e3,"Equivalence checking of finite state machines (FSMs) traditionally assumes single phase machines where a single clock (implicit or explicit) synchronizes the state of the FSM. We extend the equivalence checking paradigm to FSMs with multi-phase clocks. Such designs are becoming increasingly popular in high performance microprocessors since they result in lower synchronization overhead. In addition, aggressive pipelining and the use of ""sparse"" encodings results in designs where the ratio of steady states to the total state space is very low. In this paper, we show that automatically transforming such designs to ones that have more ""dense"" encodings can result in significant benefits in using implicit BDD-based techniques for their verification. We explore two such techniques: phase abstraction and retiming and demonstrate their utility in the context of FSM equivalence checking. The main contributions of our work are: - We show that a multi-phase FSM can be transformed to a functionally equivalent one phase FSM and this phase abstraction leads to significant improvement in the size of FSMs that can be checked for equivalence. - We show that min-latch retiming preserves equivalence and can be performed efficiently in multi-phase designs, even when latch borrowing and discarding is allowed at the primary inputs and outputs. - We demonstrate the utility of our approach on several controller FSMs from the industry. General Terms: Algorithms, Design, Performance, Theory, Verification. © 1999 ACM.",Binary decision diagram; Encoding density; Multi-phase FSM; Product machine; Sequential hardware equivalence; Steady states,
"ATM switch design by high-level modeling, formal verification, and high-level synthesis",1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745123126&doi=10.1145%2f296333.296342&partnerID=40&md5=5521e623459b1bc3a38ebf80669707d6,"Asynchronous Transfer Mode (ATM) has emerged as a backbone for high-speed broadband telecommunication networks. In this paper, we present ATM switch design, starting from a parametric high-level model and debugging the model using a combination of formal verification and simulation. The model has been used to synthesize ATM switches according to customers' choices, by choosing concrete values for each of the generic parameters. We provide a pragmatic combination of simulation, model checking, and theorem proving to gain confidence in the ATM switch design correctness. General Terms: Design, Modeling, Synthesis, Validation, Verification. © 1999 ACM.",ATM Switch; High-Level Design; Synthesis; Verification,
Effects of resource sharing on circuit delay: an assignment algorithm for clock period optimization,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746734867&doi=10.1145%2f290833.290852&partnerID=40&md5=9b23c4fb7a279a868bc5a09efdb7991e,"This paper analyzes the effect of resource sharing and assignment on the clock period of the synthesized circuit. The assignment phase assigns or binds operations of the scheduled behavioral description to a set of allocated resources. We focus on control-flow intensive descriptions, characterized by the presence of mutually exclusive paths due to the presence of nested conditional branches and loops. We show that clustering multiple operations in the same state of the schedule, possibly leading to chaining of functional units (FUs) in the RTL circuit, is an effective way to minimize the total number of clock cycles, and hence total execution time. We present an assignment algorithm that is particularly effective for such design styles by minimizing data chaining and hence the clock period of the circuit, thereby leading to further reduction in total execution time. Existing resource sharing and assignment approaches for reducing the clock period of the resulting circuit either increase the resource allocation or use faster modules, both leading to larger area requirements. In this paper we show that even when the type of available resource units and the number of resource units of each type is fixed, different assignments may lead to circuits with significant differences in clock period. We provide a comprehensive analysis of how resource sharing and assignment introduces long paths in the circuit. Based on the analysis, we develop an assignment algorithm that uses a high-level delay estimator to assign operations to a fixed set of available resources so as to minimize the clock period of the resultant circuit, with no or minimal effect on the area of the circuit. Experimental results on several conditional-intensive designs demonstrate the effectiveness of the assignment algorithm. © 1998 ACM.",,
Modeling reactive systems in Java,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746728236&doi=10.1145%2f296333.296334&partnerID=40&md5=cc59b782735cfc34bec8ab276f488921,"We present an application of the Java™ programming language to specify and implement reactive real-time systems. We have developed and tested a collection of classes and methods to describe concurrent modules and their asynchronous communication by means of signals. The control structures are closely patterned after those of the synchronous language Esterel, succinctly describing concurrency, sequencing and preemption. We show the user-friendliness and efficiency of the proposed technique by using an example from the automotive domain. General Terms: Design, Languages. © 1999 ACM.",High level design; Java; Prototyping; Simulation,
Specification and verification of pipelining in the ARM2 RISC microprocessor,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22644448323&doi=10.1145%2f296333.296345&partnerID=40&md5=fb555b03f14bc6d4f5e3dc5f19348c3c,"Gurevich Abstract State Machines (ASMs) provide a sound mathematical basis for the specification and verification of systems. An application of the ASM methodology to the verification of a pipelined microprocessor (an ARM2 implementation) is described. Both the sequential execution model and final pipelined model are formalized using ASMs. A series of intermediate models are introduced that gradually expose the complications of pipelining. The first intermediate model is proven equivalent to the sequential model in the absence of structural, control, and data hazards. In the following steps, these simplifying assumptions are lifted one by one, and the original proof is refined to establish the equivalence of each intermediate model with the sequential model, leading ultimately to a full proof of equivalence of the sequential and pipelined models. General Terms: Design, Verification. © 1999 ACM.",Abstract state machines; ARM processor; Design verification; Formal verification; Pipelined processors; Pipelining,
Estimation of lower bounds in scheduling algorithms for high-level synthesis,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22444454816&doi=10.1145%2f290833.290839&partnerID=40&md5=d5b87bcc6d8221edb2b68292702cb0bd,"To produce efficient design, a high-level synthesis system should be able to analyze a variety of cost-performance tradeoffs. The system can use lower-bound estimates to identify and prune inferior designs without producing complete designs. We present a lower-bound performance estimate method that is not only faster than existing methods, but also produces better lower bounds. In most cases, the lower bound produced by our algorithm is tight. Scheduling algorithms such as branch-and-bound need fast and effective lower-bound estimate methods, often for a large number of partially scheduled dataflow graphs, to reduce the search space. We extend our method to efficiently estimate completion time of partial schedules. This problem is not addressed by existing methods in the literature. Our lower-bound estimate is shown to be very effective in reducing the size of the search space when used in a branchand-bound scheduling algorithm. Our methods can handle multicycle operations, pipelined functional units, and chaining of operations. We also present an extension to handle conditional branches. A salient feature of the extended method is its applicability to speculative execution as well as C-select implementation of conditional branches. © 1998 ACM.",Algorithms; Design; Dynamic programming; Experimentation; High-level synthesis; Lower-bound estimates; Measurement; Performance; Scheduling,
Code generation for fixed-point DSPs,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22444452199&doi=10.1145%2f290833.290837&partnerID=40&md5=ac7012b1c0e8bfa282d7d27622145fc3,"This paper examines the problem of code-generation for Digital Signal Processors (DSPs). We make two major contributions. First, for an important class of DSP architectures, we propose an optimal O(n) algorithm for the tasks of register allocation and instruction scheduling for expression trees. Optimality is guaranteed by sufficient conditions derived from a structural representation of the processor Instruction Set Architecture (ISA). Second, we develop heuristics for the case when basic blocks are Directed Acyclic Graphs (DAGs). © 1998 ACM.",Algorithms; Code generation; Register allocation; Scheduling,
Rate analysis for embedded systems,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14244256365&doi=10.1145%2f293625.293631&partnerID=40&md5=be3fcd60adbebe50ca481e44586826a5,"Embedded systems consist of interacting components that are required to deliver a specific functionality under constraints on execution rates and relative time separation of the components. In this article, we model an embedded system using concurrent processes interacting through synchronization. We assume that there are rate constraints on the execution rates of processes imposed by the designer or the environment of the system, where the execution rate of a process is the number of its executions per unit time. We address the problem of computing bounds on the execution rates of processes constituting an embedded system, and propose an interactive rate analysis framework. As part of the rate analysis framework we present an efficient algorithm for checking the consistency of the rate constraints. Bounds on the execution rate of each process are computed using an efficient algorithm based on the relationship between the execution rate of a process and the maximum mean delay cycles in the process graph. Finally, if the computed rates violate some of the rate constraints, some of the processes in the system are redesigned using information from the rate analysis step. This rate analysis framework is implemented in a tool called RATAN. We illustrate by an example how RATAN can be used in an embedded system design. © 1999 ACM.",Algorithms; Average execution rate; Concurrent system modeling; Design; Embedded systems; Interactive rate violation debugging; Performance; Rate analysis; Rate constraints; Theory,
Functional partitioning improvements over structural partitioning for packaging constraints and synthesis: Tool performance,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22444453296&doi=10.1145%2f290833.290841&partnerID=40&md5=471bf9630de9b3e94dcfe9dc05d90bee,"Incorporating functional partitioning into a synthesis methodology leads to several important advantages. In functional partitioning, we first partition a functional specification into smaller subspecifications and then synthesize structure for each, in contrast to the current approach of first synthesizing structure for the entire specification and then partitioning that structure. One advantage is the improvement in I/O performance and package count, when partitioning among hardware blocks with size and I/O constraints, such as FPGAs or blocks within an ASIC. A second advantage is reduction in synthesis runtimes. We describe these important advantages, concluding that further research on functional partitioning can lead to improved results from synthesis environments. © 1998 ACM.","Behavioral systhesis; Design; Functional partitioning, system-level design",
High-level design verification of microprocessors via error modeling,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22644449714&doi=10.1145%2f296333.296347&partnerID=40&md5=f5e8b3b064939d01d97b63eecdcdc66a,"A design verification methodology for microprocessor hardware based on modeling design errors and generating simulation vectors for the modeled errors via physical fault testing techniques is presented. We have systematically collected design error data from a number of microprocessor design projects. The error data is used to derive error models suitable for design verification testing. A class of basic error models is identified and shown to yield tests that provide good coverage of common error types. To improve coverage for more complex errors, a new class of conditional error models is introduced. An experiment to evaluate the effectiveness of our methodology is presented. Single actual design errors are injected into a correct design, and it is determined if the methodology will generate a test that detects the actual errors. The experiment has been conducted for two microprocessor designs and the results indicate that very high coverage of actual design errors can be obtained with test sets that are complete for a small number of synthetic error models. General Terms: Microprocessors, Verification. © 1999 ACM.",Design errors; Design verification; Error modeling,
The edge-based design rule model revisited,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22644452567&doi=10.1145%2f293625.293633&partnerID=40&md5=4c2751db7dbf6299933ea1187614bc7c,"A model for integrated circuit design rules based on rectangle edge constraints has been proposed by Jeppson, Christensson, and Hedenstierna. This model appears to be the most rigorous proposed to date for the description of such edge-based design rules. However, in certain rare circumstances their model is unable to express the correct design rule when the constrained edges are not adjacent in the layout. We introduce a new notation, called an edge path, which allows us to extend their model to allow for constraints between edges separated by an arbitrary number of intervening edges. Using this notation we enumerate all edge paths that are required to correctly model the original design rule macros of the JCH model, and prove that these macros are sufficient to model the most common rules. We also show how this notation allows us to directly specify many kinds of conditional design rules that required ad hoc specification under the JCH model. © 1999 ACM.",Design rule checking; Design rules; Layout verification; Theory; Verification,
Semantics and verification of action diagrams with linear timing constraints,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442341425&doi=10.1145%2f270580.270582&partnerID=40&md5=6624e2a5ce54adaf33808ea8614afa4a,"Specifications containing linear timing constraints, such as found in action diagrams (timing diagrams) defining interface behaviors, are often used in practice. Although efficient O(n3) shortest path algorithms exist for computing the minimum and maximum time distances between actions, subject to the timing constraints, there is so far no accurate method that can decide (a) whether a specification of this kind is realizable (i.e., can be simulated by a causal system), and (b) given the action diagrams of the interfaces of two or more communicating systems, whether the systems implementing such independent specifications will correctly interoperate (i.e., satisfy the respective protocols and timing assumptions). First we illustrate the weaknesses of existing action diagram verification techniques: the causality issue is not addressed, and the proposed methods to answer the compatibility (interoperability) question yield false negative answers in many practical situations. We then define the meaning of causality in an action diagram specification and state a set of sufficient conditions for causality to hold. This development then leads to an exact procedure for the verification of the interface compatibility of communicating action diagrams. The results are illustrated on a practical example. © 1998 ACM.",Causality; Compatibility of interfaces; Hardware interfaces; Timing diagrams; Timing verification; Verification,
A timing-driven design and validation methodology for embedded real-time systems,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22644449964&doi=10.1145%2f296333.296338&partnerID=40&md5=102cbdc89688fbee055f8b43bca863d1,"We address the problem of timing constraint derivation and validation for reactive and real-time embedded systems. We assume that such a system is structured into its tasks, and the structure is modeled using a task graph. Our solution uses the timing behavior committed by the environment to the system first to derive the timing constraints on the system's internal behavior and then use them to derive and validate the timing constraints on the system's external behavior. Our solution consists of the following contributions: a generalized task graph model, a comprehensive classification of timing constraints, algorithms for derivation and validation of timing constraints of the system modeled in the generalized task graph model, a codesign methodology that combines the model and the algorithms, and the implementation of this methodology in a tool called RADHA-RATAN. The main advantages of our solution are that it simplifies the problem of ensuring timing correctness of the system by reducing the complexity of the problem from system level to task level, and that it makes the codesign methodology timing-driven in that our solution makes it possible to maintain a handle on the system's timing correctness from very early stages in the system's design flow. © 1999 ACM.",,
EXFI: A low-cost fault injection system for embedded microprocessor-based boards,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22644450438&doi=10.1145%2f296333.296351&partnerID=40&md5=77cdf4e6fe8248ece2209651c0d2cd12,"Evaluating the faulty behavior of low-cost embedded microprocessor-based boards is an increasingly important issue, due to their adoption in many safety critical systems. The architecture of a complete Fault Injection environment is proposed, integrating a module for generating a collapsed list of faults, and another for performing their injection and gathering the results. To address this issue, the paper describes a software-implemented Fault Injection approach based on the Trace Exception Mode available in most microprocessors. The authors describe EXFI, a prototypical system implementing the approach, and provide data about some sample benchmark applications. The main advantages of EXFI are the low cost, the good portability, and the high efficiency. General Terms: Design, Experimentation, Measurements. © 1999 ACM.",Fault coverage; Fault injection; Microprocessor systems; Software-implemented fault injection; Trace exception mode,
Functional Test Generation for Delay Faults in Combinational Circuits,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22444451732&doi=10.1145%2f290833.290845&partnerID=40&md5=95033cc405deab09cbffe7827325d1bd,"We propose a functional fault model for delay faults in combinational circuits and describe a functional test generation procedure based on this model. The proposed method is most suitable when a gate-level description of the circuit-under-test, necessary for employing existing gate-level delay fault test generators, is not available or does not accurately describe the circuit. It is also suitable for generating tests in early design stages of a circuit, before a gate-level implementation is selected. In addition, it can potentially be employed to supplement conventional test generators for gate-level circuits to reduce the cost of handling large numbers of paths. A parameter called A is used to control the number of functional faults targeted and thus the number of tests generated. If A is unlimited, the functional test set detects every robustly testable path delay fault in any gate-level implementation of the given circuit. An appropriate subset of tests can be selected once the implementation is known. The test sets generated for various values of A are fault simulated on gate-level realizations to demonstrate their effectiveness. The experiments indicate that functional test sets may be able to identify functions whose realizations have low path delay fault coverage. © 1998 ACM.",Algorithms; Delay faults; Design; Function-robust tests; Functional delay fault model; Path delay faults; Reliability; Robust tests,
Optimal clock period fpga technology mapping for sequential circuits,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22644451539&doi=10.1145%2f293625.293632&partnerID=40&md5=10916a6029c5f7213bff2eaed20aa85c,"We study the technology mapping problem for sequential circuits for look-up table (LUT) based field programmable gate arrays (FPGAs). Existing approaches to the problem simply remove the flip-flops (FFs), then map the remaining combinational logic, and finally put the FFs back. These approaches ignore the sequential nature of a circuit and assume the positions of the FFs are fixed. However, FFs in a sequential circuit can be repositioned by a functionality-preserving transformation called retiming. As a result, existing approaches can only consider a very small portion of the available solution space. We propose in this paper a novel approach to the technology mapping problem. In our approach, retiming is integrated into the technology mapping process so as to consider the full solution space. We then present a polynomial technology mapping algorithm that, for a given circuit, produces a mapping solution with the minimum clock period among all possible ways of retiming. The effectiveness of the algorithm is also demonstrated experimentally. © 1999 ACM.",Algorithms; Clock period; Field-programmable gate arrays; FPGAs; Logic replication; Look-up tables; Performance; Retiming; Sequential synthesis; Technology mapping,
ICOS: An intelligent concurrent objectoriented synthesis methodology for multiprocessor systems,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22444456517&doi=10.1145%2f290833.290834&partnerID=40&md5=8483d587127d924df4e2a3370b51b7f8,"The design of multiprocessor architectures differs from uniprocessor systems in that the number of processors and their interconnection must be considered. This leads to an enormous increase in the design-space exploration time, which is exponential in the total number of system components. The methodology proposed here, called Intelligent Concurrent ObjectOriented Synthesis (ICOS) methodology, makes feasible the synthesis of complex multiprocessor systems through the application of several techniques that speed up the design process. ICOS is based on Performance Synthesis Methodology (PSM), a recently proposed objectoriented system-level design methodology. Four major techniques: object-oriented design, fuzzy design-space exploration, concurrent design, and intelligent reuse of complete subsystems are integrated in ICOS. First, object-oriented modeling and design, through the use of object-oriented relationships and operators, make the whole design process manageable and maintainable in ICOS. Second, fuzzy comparison applied to the specializations or instances of components reduces the exponential growth of design-space exploration in ICOS. Third, independent components from different design alternatives are synthesized in parallel, this design concurrency shortens the overall design time. Lastly, the resynthesis of complete subsystems can be avoided through the application of learning, thus making the methodology intelligent enough to reuse previous design configurations. Experiments show that all these applied techniques contribute to the synthesis efficiency and the degree of automation in ICOS. © 1998 ACM.",Design,
Bounded-skew clock and steiner routing,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22644449767&doi=10.1145%2f293625.293628&partnerID=40&md5=a3d8ae744722729bffeba7b865ca9d9c,"We study the minimum-cost bounded-skew routing tree problem under the pathlength (linear) and Elmore delay models. This problem captures several engineering tradeoffs in the design of routing topologies with controlled skew. Our bounded-skew routing algorithm, called the BST/DME algorithm, extends the DME algorithm for exact zero-skew trees via the concept of a merging region. For a prescribed topology, BST/DME constructs a bounded-skew tree (BST) in two phases: (i) a bottom-up phase to construct a binary tree of merging regions which represent the loci of possible embedding points of the internal nodes, and (ii) a top-down phase to determine the exact locations of the internal nodes. We present two approaches to construct the merging regions: (i) the Boundary Merging and Embedding (BME) method which utilizes merging points that are restricted to the boundaries of merging regions, and (ii) the Interior Merging and Embedding (IME) algorithm which employs a sampling strategy and a dynamic programming-based selection technique to consider merging points that are interior to, as well as on the boundary of, the merging regions. When the topology is not prescribed, we propose a new Greedy-BST/DME algorithm which combines the merging region computation with topology generation. The Greedy-BST/DME algorithm very closely matches the best known heuristics for the zero-skew case and for the unbounded-skew case (i.e., the Steiner minimal tree problem). Experimental results show that our BST algorithms can produce a set of routing solutions with smooth skew and wirelength tradeoffs. © 1999 ACM.",(inter)connection; Algorithms; Boundary merging and embedding; Bounded-Skew; Clock tree; Design; Elmore delay; Experimentation; Interior merging and embedding; Low power; Merging region; Merging segment; Pathlength delay; Performance,
Measurement and analysis of sequential design processes,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22444454875&doi=10.1145%2f270580.270581&partnerID=40&md5=271a8347116d8ef874b08158fad2b196,"As design processes continue to increase in complexity, it is important to base process-improvement decisions on quantitative analysis. We describe the development of an analytical approach for evaluating sequential design-process completion time and for determining the sensitivities of design time with respect to individual task durations and transition probabilities. Techniques are also detailed for collecting process metadata and calibrating a design process model. Example applications illustrate the use of the methodology in analyzing and improving software and hardware design processes. © 1998 ACM.",Design; Human factors; Management; Management science; Measurement; Sensitivity analysis; Workflow,
Optimal river routing with crosstalk constraints,1998,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22644451449&doi=10.1145%2f293625.293636&partnerID=40&md5=c8d44a191f98263939be011035b69608,"With the increasing density of VLSI circuits, the interconnection wires are being packed even closer. This has increased the effect of interaction among these wires on circuit performance and hence, the importance of controlling crosstalk. In this article, we consider river routing with crosstalk constraints. Given the positions of the pins in a single-layer routing channel and the maximum tolerable crosstalk between each pair of neighboring nets, we give a polynomial time algorithm to decide whether there is a feasible river routing solution and produce one with minimum crosstalk when it is feasible. © 1999 ACM.",Algorithms; Crosstalk; Design; Experimentation; Performance; River routing,
Algorithms to compute bridging fault coverage of IDDQ test sets,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-6744231865&doi=10.1145%2f264995.264999&partnerID=40&md5=ef5cba9d2cc71f06c7a3ae2b02358d6f,"We present two algorithms, called list-based scheme and tree-based scheme, to compute bridging fault (BF) coverage of IDDQ tests. These algorithms use the novel idea of ""indistinguishable pairs,"" which makes it more efficient and versatile than known fault simulation algorithms. Unlike known algorithms, the two algorithms can be used for combinational as well as sequential circuits and for arbitrary sets of BFs. Experiments show that the tree-based scheme is, in general, better than the list-based scheme. But the list-based scheme is better for some classes of faults. © 1997 ACM.",Algorithms; Design,
Analysis of RC interconnections under ramp input,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1642604352&doi=10.1145%2f253052.253137&partnerID=40&md5=8cf2a528034e2a87d40f38ff70695cac,"We give new methods for calculating the time-domain response for a finite-length distributed RC line that is stimulated by a ramp input. The following are our contributions. First, we obtain the solution of the diffusion equation for a semiinfinite distributed RC line with ramp input. We then present a general and, in the limit, exact approach to compute the time-domain response for finite-length RC lines under ramp input by summing distinct diffusions starting at either end of the line. Next, we obtain analytical expressions for the finite time-domain voltage response for an open-ended finite RC line and for a finite RC line with capacitive load. The delay estimates using this method are very close to SPICE-computed delays. Finally, we present a general recursive equation for computing the higher-order diffusion components due to reflections at the source and load ends. Future work extends our method to response computations in general interconnection trees by modeling both reflection and transmission coefficients at discontinuities. © 1997 ACM.",Diffusion equation analysis; Ramp input response; VLSI interconnects,
Functional design for testability of control-dominated architectures,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-6444235838&doi=10.1145%2f253052.253064&partnerID=40&md5=781a77ae537d839f310f724cda7c8803,"Control-dominated architectures are usually described in a hardware description language (HDL) by means of interacting FSMs. A VHDL or Verilog specification can be translated into an interacting FSM (IFSM) representation as described here. The IFSM model allows us to approach the testable synthesis problem at the level of each FSM. The functionality is modified by the addition of transparency to data flow. The complete testability of the IFSM implementation is thus achieved by connecting fully testable implementations of each modified FSM. In this way, test sequences separately generated for each FSM are directly applied to the IFSM to achieve complete fault coverage. The addition of test functionality to each FSM description, and its simultaneous synthesis with the FSM functionality, produces a lower area overhead than that necessary for the application of a partial-scan technique. Moreover, the test generation problem is highly simplified since it is reduced to the test generation for each separate FSM. © 1997 ACM.",Functional testing; Interacting FSMs,
Agents: a distributed client-server system for leaf cell generation,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746970027&doi=10.1145%2f250243.250248&partnerID=40&md5=3975c054be129458314d7e7fa169d988,"The AGENTS system is a set of programs designed to generate automatically the mask-level layout of full custom CMOS, BICMOS, and bipolar leaf cells. The system is formed from four server programs: the Placer, Router, Database, and Broker. The Placer places components in a cell, the Router wires the circuits sent to it, the Database stores all the information that is dependent upon the fabrication process, such as the design rules, and the Broker makes the services of the other servers available. These servers communicate over a computer network using the TCP/IP Internet Protocol. The Placer server receives from its client the description and netlist of the circuit to be generated using EDIF (Electronic Design Interchange Format). The output to its client is the mask layout of the circuit, again codified in EDIF. The concept of agents as software components which have the ability to communicate and cooperate with each other is at the heart of the AGENTS system. This concept is not only used at the higher level, for the four servers, but at a lower level as well, inside the Router and Placer servers, where small relatively simple agents work together to accomplish complex tasks. These small agents are responsible for all the reasoning carried out by the two servers, as they hold the basic inference routines and the knowledge needed by the servers. The system's philosophy is that competence should emerge out of the collective behavior of a large number of relatively simple agents. In addition and integrated to these small agents, the system uses a genetic algorithm to improve components' placement before routing. General Terms: Design. © 1997 ACM.",Client; Genetic algorithms; Server model; Software agents,
Board-level multiterminal net routing for FPGA-based logic emulation,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344667944&doi=10.1145%2f253052.253136&partnerID=40&md5=1acc5620a3f59403f6f33332d6783e47,We consider a board-level routing problem applicable to FPGA-based logic emulation systems such as the Realizer System [Varghese et al. 1993] and the Enterprise Emulation System [Maliniak 1992] manufactured by Quickturn Design Systems. Optimal algorithms have been proposed for the case where all nets are two-terminal nets [Chan and Schlag 1993; Mak and Wong 1995]. We show how multiterminal nets can be handled by decomposition into two-terminal nets. We show that the multiterminal net decomposition problem can be modeled as a bounded-degree hypergraph-to-graph transformation problem where hyperedges are transformed to spanning trees. A network flow-based algorithm that solves both problems is proposed. It determines if there is a feasible decomposition and gives one whenever such a decomposition exists. © 1997 ACM.,Board-level routing; Crossbars; Field-programmable gate arrays; Logic emulation; Multi-terminal net decomposition,
A codesign experiment in acoustic echo cancellation: GMDFa,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746821996&doi=10.1145%2f268424.268433&partnerID=40&md5=a79a2bc4a1991d2ecfb9b6ff817acf24,"Continuous advances in processor and ASIC technologies enable the integration of more and more complex embedded systems. Embedded systems have become commonplace in recent years. Since their implementations generally require the use of heterogeneous resources (e.g., processor. cores, ASICs) in one system with hard design constraints, the importance of hardware/software codesign methodologies increases steadily. HW/SW codesign approaches consist generally of HW/SW partitioning and scheduling, constrained code generation, and hardware and interface synthesis. This article presents the codesign of an industrial experiment in acoustic echo cancellation (GMDFα algorithm) and emphasizes the partitioning and communication synthesis steps. This experiment brings to light interesting problems such as data and program distribution between system memories and the modeling of communications in the partitioning process. © 1997 ACM.",Codesign; Communications; Design; Experimentation; HW/SW partitioning,
Model refinement for hardware-software codesign,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747024160&doi=10.1145%2f250243.250247&partnerID=40&md5=4d646d888a381c293af031c38b1d2f95,"Hardware-software codesign, which implements a given specification with a set of system components such as ASICs and processors, includes several key tasks such as system component allocation, functional partitioning, quality metrics estimation, and model refinement. In this work, we focus on the model refinement task which transforms a specification from an original functional model to a refined implementation model. First, we categorize several commonly used implementation models and describe a set of refinement procedures to transform a specification to each of these implementation models. We also present a set of experimental results to compare the implementation models and to demonstrate how the proposed approach can be used to explore different implementation styles. General Terms: Design, Experimentation, Measurement. © 1997 ACM.",Functional model; Implementation model; Model refinement; Software-hardware codesign,
An efficient ILP-Based scheduling algorithm for control-DOminated VHDL descriptions,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747000124&doi=10.1145%2f268424.268428&partnerID=40&md5=a0ca023d96b70df67f1b2d420cf0fef0,"To adopt behavioral synthesis techniques in existing design flows, the synthesis methodology must provide the designer with a mechanism to specify a component's interface timing. This will permit pre- and postsynthesis validation through cosimulation with other subsystems or even through formal verification. In control-flow dominated designs, additional timing constraints will result in a complex specification/constraint system for which the scheduling problem has been shown to be NP-complete. In this article, we present a mathematical framework for solving a special instance of the scheduling problem in control-flow dominated behavioral VHDL descriptions given that the timing of I/O signals has been completely or partially specified. It is based on a code-transformational approach that fully preserves the VHDL semantics. The scheduling problem is mapped onto an integer linear program (ILP) solvable in polynomial time assuming a restricted partial order on selected statements. It captures both control-flow and timing constraints in a single model and also exploits dataflow information to optimize the statement sequence across basic block boundaries. © 1997 ACM.",Algorithms; Integer linear programming (ILP); Scheduling; Timing constraints,
Hmap: A fast mapper for EPGAs using extended GBDD hash tables,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27944460068&doi=10.1145%2f253052.253098&partnerID=40&md5=1de47fcbe34d8a25342d06d8489ef292,"A fast and efficient algorithm for technology mapping of electrically programmable gate arrays (EPGAs) is proposed. This Hmap algorithm covers the Boolean network with programmed logic modules bottom-up. The covering operation is based on collapsing the fanins of a node to form a bigger supernode such that fewer clusters are needed to be detected. Then Boolean matching is used to detect whether the collapsed supernode can be mapped into a logic module by looking up an extended GBDD hash table. The use of this table look-up matching can shorten the matching time significantly. As shown in the experiments, the average running time of Hmap is 20 times faster than that of MIS-pga2. © 1997 ACM.",Area minimization; Decomposition; Field programmable gate arrays (FPGAs); Technology mapping,
Datapath scheduling with multiple supply voltages and level converters,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747003362&doi=10.1145%2f264995.264997&partnerID=40&md5=3943294746b1af862d0886a541dc6093,"We present an algorithm called MOVER (Multiple Operating Voltage Energy Reduction) to minimize datapath energy dissipation through use of multiple supply voltages. In a single voltage design, the critical path length, clock period, and number of control steps limit minimization of voltage and power. Multiple supply voltages permit localized voltage reductions to take up remaining schedule slack. MOVER initially finds one minimum voltage for an entire datapath. It then determines a second voltage for operations where there is still schedule slack. New voltages can be introduced and minimized until no schedule slack remains. MOVER was exercised for a variety of DSP datapath examples. Energy savings ranged from 0% to 50% when comparing dual to single voltage results. The benefit of going from two to three voltages never exceeded 15%. Power supply costs are not reflected in these savings, but a simple analysis shows that energy savings can be achieved even with relatively inefficient DC-DC converters. Datapath resource requirements were found to vary greatly with respect to number of supplies. Area penalties ranged from 0% to 170%. Implications of multiple voltage design for IC layout and power supply requirements are discussed. © 1997 ACM.",Algorithms; Datapath scheduling; Design additional; DSP; High-level synthesis; Level conversion; Low power design; Multiple voltage; Power optimization; Scheduling,
Event propagation conditions in circuit delay computation,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001341563&doi=10.1145%2f264995.264998&partnerID=40&md5=e31d38adde31043785e5f49ad00fd719,"Accurate and efficient computation of delays is a central problem in computer-aided design of complex VLSI circuits. Delays are determined by events (signal transitions) propagated from the inputs of a circuit to its outputs, so precise characterization of event propagation is required for accurate delay computation. Although many different propagation conditions (PCs) have been proposed for delay computation, their properties and relationships have been far from clear. We present a systematic analysis of delay computation based on a series of waveform models that capture signal behavior rigorously at different levels of detail. The most general model, called the exact or W0 model, specifies each event occurring in a circuit signal. A novel method is presented that generates approximate waveforms by progressively eliminating signal values from the exact model. For each waveform model, we derive the PCs that correctly capture the requirements under which an event propagates along a path. The waveform models and their PCs are shown to form a well-defined hierarchy, which provides a means to trade accuracy for computational effort. The relationships among the derived PCs and existing ones are analyzed in depth. It is proven that though many PCs, such as the popular floating mode condition, produce a correct upper bound on the circuit delay, they can fail to recognize event propagation in some instances. This analysis further enables us to derive new and useful PCs. We describe such a PC, called safe static. Experimental results demonstrate that safe static provides an excellent accuracy/efficiency tradeoff. © 1997 ACM.",Delay computation; Event propagation; False path; Path sensitization; Performance; Propagation condition; Theory; Timing analysis; Verification; Waveform modeling,
Layout-driven RTL binding techniques for high-level synthesis using accurate estimators,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000132111&doi=10.1145%2f268424.268425&partnerID=40&md5=10a68b7b764c8828bf2aa8d9d31ab36f,"The importance of effective and efficient accounting of layout effects is well established in High-Level Synthesis (HLS), since it allows more realistic exploration of the design space and the generation of solutions with predictable metrics. This feature is highly desirable in order to avoid unnecessary iterations through the design process. In this article, we address the problem of layout-driven register-transfer-level (RTL) binding as this step has a direct relevance to the final performance of the design. By producing not only an RTL design but also an approximate physical topology of the chip-level implementation, we ensure that the solution will perform at the predicted metric once implemented, thus avoiding unnecessary delays in the design process. © 1997 ACM.",Algorithms; Binding; Design; Experimentation; Floorplan; FPGAs; High-level synthesis; Measurement; Performance,
A survey of boolean matching techniques for library binding,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001277124&doi=10.1145%2f264995.264996&partnerID=40&md5=8fedd01da1e447a4e1948202af30427c,"When binding a logic network to a set of cells, a fundamental problem is recognizing whether a cell can implement a portion of the network. Boolean matching means solving this task using a formalism based on Boolean algebra. In its simplest form, Boolean matching can be posed as a tautology check. We review several approaches to Boolean matching as well as to its generalization to cases involving don't care conditions and its restriction to specific libraries such as those typical of anti-fuse based FPGAs. We then present a general formulation of Boolean matching supporting multiple-output logic cells. © 1997 ACM.",Design; Measurement; Performance,
Memory data organization for improved cache performance in embedded processor applications,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012901719&doi=10.1145%2f268424.268464&partnerID=40&md5=340128d709275ee2910fc60b6a31daec,"Code generation for embedded processors opens up the possibility for several performance optimization techniques that have been ignored by traditional compilers due to compilation time constraints. We present techniques that take into account the parameters of the data caches for organizing scalar and array variables declared in embedded code into memory, with the objective of improving data cache performance. We present techniques for clustering variables to minimize compulsory cache misses, and for solving the memory assignment problem to minimize conflict cache misses. Our experiments with benchmark code kernels from DSP and other domains on the CW4001 embedded processor from LSI Logic indicate significant improvements in data cache performance by the application of our memory organization technique. © 1997 ACM.",Cache memory; Data cache; Memory synthesis; Performance; System design; System synthesis; Verification,
Scheduling techniques for variable voltage low power designs,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001920829&doi=10.1145%2f253052.253054&partnerID=40&md5=6fc8fdb5ffd1492df606febd5ee95890,"This paper presents an integer linear programming (ILP) model and a heuristic for the variable voltage scheduling problem. We present the variable voltage scheduling techniques that consider in turn timing constraints alone, resource constraints alone, and timing and resource constraints together for design space exploration. Experimental results show that our heuristic produces results competitive with those of the ILP method in a fraction of the run-time. The results also show that a wide range of design alternatives can be generated using our design space exploration method. Using different cost/delay combinations, power consumption in a single design can differ by as much as a factor of 6 when using mixed 3.3V and 5V supply voltages. © 1997 ACM.",High-level synthesis; Lower power design; Scheduling; Variable voltage,
A performance-driven ic/mcm placement algorithm featuring explicit design space exploration,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012977739&doi=10.1145%2f250243.250250&partnerID=40&md5=0a766a7b7f26912862e3b751c0fd4d0a,"A genetic algorithm for building-block placement of ICs and MCMs is presented that simultaneously minimizes layout area and an Elmore-based estimate of the maximum path delay while trying to meet a target aspect ratio. Explicit design space exploration is performed by using a vector-valued, 3-dimensional cost function and searching for a set of distinct solutions representing the best trade-offs of the cost dimensions. From the output solutions, the designer can choose the solution with the preferred trade-off. In contrast to existing approaches, the required properties of the output solutions are specified without using weights or bounds. Consequently, the practical problems of specifying these quantities are eliminated. Promising experimental results are obtained for various placement problems, including a real-world design. Solution sets representing good, balanced cost trade-offs are found using a reasonable amount of runtime. Furthermore, the performance is shown to be comparable to that of simulated annealing in the special case of 1-dimensional optimization, in which direct comparison is possible. General Terms: Algorithms, Design. © 1997 ACM.",Design space exploration; Timing-driven building-block placement,
Recent developments in high-level synthesis,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003136457&doi=10.1145%2f250243.250245&partnerID=40&md5=bb3eb66a023e0e14ee5f308824fbdca3,"We survey recent developments in high level synthesis technology for VLSI design. The need for higher-level design automation tools are discussed first. We then describe some basic techniques for various subtasks of high-level synthesis. Techniques that have been proposed in the past few years (since 1994) for various subtasks of high-level synthesis are surveyed. We also survey some new synthesis objectives including testability, power efficiency, and reliability. General Terms: Design, Experimentation, Languages, Reliability. © 1997 ACM.",Design automation; Design methodology; High level synthesis; VLSI design,
Code placement techniques for cache miss rate reduction,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001324927&doi=10.1145%2f268424.268469&partnerID=40&md5=7006deafe10e3eaf3c8e4cd600cf5ea4,"In the design of embedded systems with cache memories, it is important to minimize the cache miss rates to reduce power consumption of the systems as well as improve the performance. In this article, we propose two code placement methods (a simplified method and a refined one) to reduce miss rates of instruction caches. We first define a simplified code placement problem without an attempt to minimize the code size. The problem is formulated as an integer linear programming (ILP) problem, by which an optimal placement can be found. Experimental results show that the simplified method reduces cache misses by an average of 30% (max. 77%). However, the code size obtained by the simplified method tends to be large, which inevitably leads to a larger memory size. In order to overcome this limitation, we further propose a refined code placement method in which the code size provided by the system designers must be satisfied. The effectiveness of the refined method is also demonstrated. © 1997 ACM.",Code placement; Design; Instruction cache; Integer linear programming; Performance,
Parallel logic simulation on a network of workstations using a parallel virtual machine,1997,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0004778896&doi=10.1145%2f253052.253082&partnerID=40&md5=a6a8b209f992559e129f2c9949d66a54,"This paper explores parallel logic simulation on a network of workstations using a parallel virtual machine (PVM). A novel parallel implementation of the centralized-time event-driven logic simulation algorithm is carried out such that no global controlling workstation is needed to synchronize the advance of simulation time. Further advantages of our new approach include a random partitioning of the circuit onto available workstations and a pipelined execution of the different phases of the simulation algorithm. To achieve a better load balance, we employ a semioptimistic scheme for gate evaluations (in conjunction with a centralized-time algorithm) such that no rollback is required. The performance of this implementation has been evaluated using the ISCAS benchmark circuits. Speedups improve with the size of the circuit and the activity level in the circuit. Analyses of the communication overhead show that the techniques developed here will yield even higher gains as newer networking technologies like ATM are employed to connect workstations. © 1997 ACM.",Distributed computing; Parallel logic simulation; PVM; Synchronous simulation,
Series-parallel functions and fpga logic module design,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4243111024&doi=10.1145%2f225871.225891&partnerID=40&md5=64a8d05bef7e87e905fb4bfbb9830f79,"The need for a two-way interaction between logic synthesis and FPGA logic module design has been stressed recently. Having a logic module that can implement many functions is a good idea only if one can also give a synthesis strategy that makes efficient use of this functionality. Traditionally, technology mapping algorithms have been developed after the logic architecture has been designed. We follow a dual approach, by focusing on a specific technology mapping algorithm, namely, the structural tree-based mapping algorithm, and designing a logic module that can be mapped efficiently by this algorithm. It is known that the tree-based mapping algorithm makes optimal use of a library of functions, each of which can be represented by a tree of AND, OR, and NOT gates (series-parallel or SP functions). We show how to design a SP function with a minimum number of inputs that can implement all possible SP functions with a specified number of inputs. For instance, we demonstrate a seven-input SP function that can implement all four-input SP functions. Mapping results show that, on an average, the number blocks of this function needed to map benchmark circuits are 12% less than those for Actel's ACT1 logic modules. Our logic modules show a 4% improvement over ACT1, if the block count is scaled to take into account the number of transistors needed to implement different logic modules.",Field programmable gate arrays; Series-parallel technology mapping; Tree-based technology mapping algorithm; Universal logic modules 7,
The unison algorithm: fast evaluation of boolean expressions,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0043158730&doi=10.1145%2f238997.239009&partnerID=40&md5=559853606520900486963e0aac6c8027,"We present a Unison algorithm to evaluate arbitrarily complex Boolean expressions. This novel algorithm, based on the total differential of a Boolean function, enables fast evaluation of Boolean expressions in software. Any combination of Boolean operations can be packed into the bits of one computer word and evaluated in parallel by bitwise logical operations. Sample runs of the Unison algorithm show that many Boolean operations can be evaluated in one clock cycle. The Unison algorithm is able to evaluate Boolean expressions at an execution speed that is comparable to compiled evaluation while retaining the flexibility of interpreted approaches. The algorithm lends itself well to many practical applications. General Terms: Algorithms, Design, Performance, Reliability, Verification. © 1996 ACM.",Boolean differential; Boolean evaluation; Boolean expressions; Unison algorithm,
Combinational logic synthesis for LUT based field programmable gate arrays,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746950420&doi=10.1145%2f233539.233540&partnerID=40&md5=ed1b3f03dcff51e26debb9f756c94a0a,"The increasing popularity of the field programmable gate-array (FPGA) technology has generated a great deal of interest in the algorithmic study and tool development for FPGAspecific design automation problems. The most widely used FPGAs are LUT based FPGAs, in which the basic logic element is a If-input one-output lookup-table (LUT) that can implement any Boolean function of up to K variables. This unique feature of the LUT has brought new challenges to logic synthesis and optimization, resulting in many new techniques reported in recent years. This article summarizes the research results on combinational logic synthesis for LUT based FPGAs under a coherent framework. These results were dispersed in various conference proceedings and journals and under various formulations and terminologies. We first present general problem formulations, various optimization objectives and measurements, then focus on a set of commonly used basic concepts and techniques, and finally summarize existing synthesis algorithms and systems. We classify and summarize the basic techniques into two categories, namely, logic optimization and technology mapping, and describe the existing algorithms and systems in terms of how they use the classified basic techniques. A comprehensive list of references is compiled in the attached bibliography. © 1996 ACM.",Algorithms; Area minimization; Computer-aided design of VLSI; Decomposition; Delay minimization; Delay modeling; Design; Experimentation; FPGA; Logic optimization; Measurement; Performance; Power minimi-; Theory,
Efficient decomposition of polygons into L-shapes with applications to VLSI layouts,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16444375683&doi=10.1145%2f234860.234865&partnerID=40&md5=aaabbc9cf59e2d4dbbd21b0c5360966e,"We present two practical algorithms for partitioning circuit components represented by rectilinear polygons so that they can be stored using the L-shaped corner stitching data structure; that is, our algorithms decompose a simple polygon into a set of nonoverlapping L-shapes and rectangles by using horizontal cuts only. The more general of our algorithms computes an optimal configuration for a wide variety of optimization functions, whereas the other computes a minimum configuration of rectangles and L-shapes. Both algorithms run in O(n + h log h) time, where n is the number of vertices in the polygon and h is the number of H-pairs. Because for VLSI data h is small, in practice these algorithms are linear in n. Experimental results on actual VLSI data compare our algorithms and demonstrate the gains in performance for corner stitching (as measured by different objective functions) obtained by using them instead of more traditional rectangular partitioning algorithms. © 1996 ACM.",Corner stitching; L-shapes; Partition; Rectangle; Rectilinear polygons,
Optimal folding of standard and custom cells,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746992138&doi=10.1145%2f225871.225897&partnerID=40&md5=3bd0063f19af7e9cd351691166b800f5,We study the problem of folding an ordered list of standard and custom cells into rows of a chip so as to minimize either the routing area or the total chip area. Nine versions of the folding problem are formulated and fast polynomial time algorithms are obtained for each. Two of our formulations correspond to problems formulated in Paik and Sahni [1993] for the folding of a stack of bit-slice components. Our algorithms for these two formulations are asymptotically superior to those of Paik and Sahni [1993]. © 1996 ACM.,Custom cell folding; Layout area; Standard cell folding,
A fast algorithm for minimizing FPGA combinational and sequential modules,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746731138&doi=10.1145%2f234860.234863&partnerID=40&md5=a8c50f6c0aab81b6cde8a91b276515f3,"We present a quadratic-time algorithm for minimizing the number of modules in an FPGA with combinational and sequential modules (like the C-modules and S-modules of the ACT2 and ACTS architectures). The constraint is that a combinational module can be combined with one flip-flop in a single sequential module, only if the combinational module drives no other combinational modules. Our algorithm uses a minimum-cost flow formulation to solve the problem with a significant time improvement over a previous approach that used a general linear program. © 1996 ACM.",Field programmable gate arrays (FPGAs); Retiming,
Rectilinear steiner trees on a checkerboard,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746971566&doi=10.1145%2f238997.239033&partnerID=40&md5=3f826fa8f762a1e49a716e32c7149377,"The rectilinear Steiner tree problem is to find a minimum-length set of horizontal and vertical line segments that interconnect a given set of points in the plane. Here we study the thumbnail rectilinear Steiner tree problem, where the input points are drawn from a small integer grid. Specifically, we devise a full-set decomposition algorithm for computing optimal thumbnail rectilinear Steiner trees. We then present experimental results comparing the performance of this algorithm with two existing algorithms for computing optimal rectilinear Steiner trees. The thumbnail rectilinear Steiner tree problem has applications in VLSI placement algorithms, based on geometric partitioning, global routing of field-programmable gate arrays, and routing estimation during floorplanning. General Terms: Algorithms, Theory. © 1996 ACM.",Exact algorithms; Full-set decomposition; Rectilinear steiner tree; Routing,
Power minimization in 1c design: principles and applications,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-5544256331&doi=10.1145%2f225871.225877&partnerID=40&md5=51c91e593dd256b6ff3979b61eaece6c,"Low power has emerged as a principal theme in today's electronics industry. The need for low power has caused a major paradigm shift in which power dissipation is as important as performance and area. This article presents an in-depth survey of CAD methodologies and techniques for designing low power digital CMOS circuits and systems and describes the many issues facing designers at architectural, logical, and physical levels of design abstraction. It reviews some of the techniques and tools that have been proposed to overcome these difficulties and outlines the future challenges that must be met to design low power, high performance systems. General Terms: Design, Experimentation, Performance :. © 1996 ACM.",Cmos circuits; Computer-aided design of vlsi,
Low power realization of finite state machines - A decomposition approach,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942586356&doi=10.1145%2f234860.234862&partnerID=40&md5=a0264cd39e3be7b8738ebb1de3b7e493,"We present in this article a new approach to the synthesis problem for finite state machines with the reduction of power dissipation as a design objective. A finite state machine is decomposed into a number of coupled submachines. Most of the time, only one of the submachines will be activated which, consequently, could lead to substantial savings in power consumption. The key steps in our approach are: (1) decomposition of a finite state machine into submachines so that there is a high probability that state transitions will be confined to the smaller of the submachines most of the time, and (2) synthesis of the coupled submachines to optimize the logic circuits. Experimental results confirmed that our approach produced very good results (in particular, for finite state machines with a large number of states). © 1996 ACM.",Decomposition of finite state machines; Lower power design; State assignment,
Short note: Register estimation in unscheduled dataflow graphs,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039852149&doi=10.1145%2f234860.234866&partnerID=40&md5=ddb1574fe8f467e43a0fb231bab13c64,"A method for register number estimation in unscheduled or partially scheduled dataflow graphs is presented. The strategy consists of studying the probability that an edge between two nodes crosses the boundary between two control steps, and it is based on a model that associates probabilities with the different scheduling alternatives of each node. These probabilities are computed by means of an analytic method that takes into account the distribution of operations in the dataflow graph and the hardware modules available in the library. The results highlight that the estimation method is very accurate because the error between the estimated value and the real value is always within a narrow margin. © 1996 ACM.",Area estimation; High-level synthesis; Probabilities; Register estimation; Scheduling,
A recursive technique for computing lower-bound performance of schedules,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000419685&doi=10.1145%2f238997.239002&partnerID=40&md5=e68180fed39567f3496eb23f70cc4eb6,"We present a fast recursive technique for estimating lower-bound performance of data path schedules. The method relies on the determination of an ASAPUC (As Soon As Possible Under Constraint) time-step value for each node of the DFG (Data-Flow Graph) that is based on the ASAPUC values of its predecessor nodes. That is, the lower-bound estimation is applied to each subgraph permitting the derivation of a tight lower bound on the performance of the complete DFG. Applying the greedy lower-bound estimator of Rim and Jain [1994] to each subgraph improves the complete lower bound in more than 50% of the experiments reported in Rim and Jain [1994], and the CPU time is only about twice as long. The recursive methodology can be extended to exploit other lower-bound techniques, for example, considering other constraints such as the number of busses or registers. General Terms: Design, Measurement, Performance. © 1996 ACM.",Data flow graph; Lower-bound on performance; Microcode optimization; Resource constraints; Scheduling,
From VHDL to efficient and first-timeright designs: A formal approach,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003360169&doi=10.1145%2f233539.233541&partnerID=40&md5=db1a33cad0083dd94139edea8ca341e9,"In this article we provide a practical transformational approach to the synthesis of correct synchronous digital hardware designs from high-level specifications. We do this while taking into account the complete life cycle of a design from early prototype to full custom implementation. Besides time-to-market, both flexibility with respect to target architecture and efficiency issues are addressed by the methodology. The utilization of user-selected behaviorpreserving transformation steps ensures first-time-right designs while exploiting the experience, flexibility, and creativity of the designer. To ensure that design transformations are indeed behavior-preserving a novel mechanized approach to the specification and verification of design transformations on control data flow graphs which is independent of a specific behavioral model or graph size has been developed. As a demonstration of an industrial application we use a video processing algorithm needed for the conversion from a line-interlaced to progressively scanned video format. Both a video signal processor-based prototype implementation as well as a very efficient full custom implementation are developed starting from a single high-level behavioral specification of the algorithm in VHDL. Results are compared with those previously obtained using different tools and methodologies. © 1996 ACM.",,
Tutorial and Survey Paper: Gate-Level test generation for sequential circuits,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007840207&doi=10.1145%2f238997.238999&partnerID=40&md5=2881295490d9b1fbbac0cda8281a2225,"This paper discusses the gate-level automatic test pattern generation (ATPG) methods and techniques for sequential circuits. The basic concepts, examples, advantages, and limitations of representative methods are reviewed in detail. The relationship between gate-level sequential circuit ATPG and the partial scan design is also discussed. General Terms: Algorithms, Reliability, Verification. © 1996 ACM.",Automatic test generation; Ic testing; Sequential circuit test generation,
Optimal wiresizing for interconnects with multiple sources,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000357522&doi=10.1145%2f238997.239018&partnerID=40&md5=57eb4196d7c54a282b3df12f3b371007,"In this paper, we study the optimal wiresizing problem for nets with multiple sources under the RC tree model and the Elmore delay model. We decompose the routing tree for a multisource net into the source subtree (SST) and a set of loading subtrees (LSTs), and show that the optimal wiresizing solution satisfies a number of interesting properties, including: the LST separability, the LST monotone property, the SST local monotone property, and the dominance property. Furthermore, we study the optimal wiresizing problem using a variable segment-division rather than an a priori fixed segment-division as in all previous works and reveal the bundled refinement property. These properties lead to efficient algorithms to compute the optimal solutions. We have tested our algorithm on nets extracted from the multilayer layout for a high-performance Intel microprocessor. Accurate SPICE simulation shows that our methods reduce the average delay by up to 23.5% and the maximum delay by up to 37.8%, respectively, for the submicron CMOS technology when compared to the minimal wire width solution. In addition, the algorithm based on the variable segment-division yields a speedup of over 100× time and does not lose any accuracy, when compared with the algorithm based on the a priori fixed segment-division. General Terms: Algorithms, Design, Experimentation, Performance. © 1996 ACM.",Bundled refinement; Decomposition of multisource routing tree; Dominance property; Elmore delay; Fidelity; High performance; Interconnect optimization; Layout optimization; Local refinement; Multisource net; Multisource routing tree,
An optimal clock period selection method based on slack minimization criteria,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001307041&doi=10.1145%2f234860.234864&partnerID=40&md5=d822a414981d18474f033a1f3b4e1202,"An important decision in synthesizing a hardware implementation from a behavioral description is selecting the clock period to schedule the datapath operations into control steps. Prior to scheduling, most existing behavioral synthesis systems either require the designer to specify the clock period explicitly or require that the delays of the operators used in the design be specified in multiples of the clock period. An unfavorable choice of clock period could result in operations being idle for a large portion of the clock period and, consequently, affect the performance of the synthesized design. In this article, we demonstrate the effect of clock slack on the performance of designs and present an algorithm to find a slack-minimal clock period. We prove the optimality of our method and apply it to several examples to demonstrate its effectiveness in maximizing design performance.",Clock period; Clock slack; Performance estimation; Scheduling © 1996 acm,
Optimal register assignment to loops for embedded code generation,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000738903&doi=10.1145%2f233539.233542&partnerID=40&md5=ee43d5f9b5d226e9117d00bfc1f22aef,"One of the challenging tasks in code generation for embedded systems is register assignment. When more live variables than registers exist, some variables will necessarily be accessed from data memory. Because loops are typically executed many times and are often timecritical, good register assignment in loops is exceedingly important as accessing data memory can degrade performance. The issue of finding an optimal register assignment to loops has been open for some time. In this article, we present a technique for optimal (i.e., spill minimizing) register assignment to loops. First we present a technique for register assignment to architecture styles that are characterized by a consolidated register file. Then we extend the technique to include architecture styles that are characterized by distributed memories and/or a combination of general- and special-purpose registers. Experimental results demonstrate that although the optimal algorithm may be computationally prohibitive, heuristic versions obtain results with performance better than that of an existing graph coloring approach. General Terms: Algorithms, Languages. © 1996 ACM.",Code generation; Embedded systems; System design,
Object-oriented cosynthesis of distributed embedded systems,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001032060&doi=10.1145%2f234860.234861&partnerID=40&md5=637f0a9f14d3102f6b8f5f6e04076df7,"This article describes a new hardware-software cosynthesis algorithm that takes advantage of the structure inherent in an object-oriented specification. The algorithm creates a distributed system implementation with arbitrary topology, using the object-oriented structure to partition functionality in addition to scheduling and allocating processes. Process partitioning is an especially important optimization for such systems because the specification will not, in general, take into account the process structure required for efficient execution on the distributed engine. The object-oriented specification naturally provides both coarse-grained and fine-grained partitions of the system. Our algorithm uses that multilevel structure to guide synthesis. Experimental results show that our algorithm takes advantage of the object-oriented specification to quickly converge on high-quality implementations. © 1996 ACM.",Distributed embedded systems; Hardware-software codesign; Object-oriented co-synthesis,
Automatic generation of functionial vectors using the extended finite state machine model,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002063138&doi=10.1145%2f225871.225880&partnerID=40&md5=f29c895fc975576306f43841ddfc0168,"We present a method of automatic generation of functional vectors for sequential circuits. These vectors can be used for design verification, manufacturing testing, or power estimation. A high-level description of the circuit in VHDL or C is assumed available. Our method automatically transforms the high-level description of a circuit in VHDL or C into an extended finite state machine (EFSM) model that is used to generate functional vectors. The EFSM model is a generalization of the traditional state machine model. It is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. The theoretical background of the EFSM model is addressed in this article. Our method guarantees that the generated vectors cover every statement in the high-level description at least once. Experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of CPU time using our prototype system. Additional Key Words and Phrases: Automatic test generation, design verification, extended finite state machines, functional testing © 1996 ACM.",Algorithms; Reliability; Verification,
Transistor reordering for power minimization under delay constraint,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005412695&doi=10.1145%2f233539.233543&partnerID=40&md5=5956bbe0c2c032ee5a2bdbd2706fa52d,"In this article we address the problem of optimization of VLSI circuits to minimize power consumption while meeting performance goals. We present a method of estimating power consumption of a basic or complex CMOS gate which takes the internal capacitances of the gate into account. This method is used to select an ordering of series-connected transistors found in CMOS gates to achieve lower power consumption. The method is very efficient when used by library-based design styles. We describe a multipass algorithm that makes use of transistor reordering to optimize performance and power consumption of circuits, has a linear time complexity per pass, and converges to a solution in a small number of passes. Transformations in addition to transistor reordering can be used by the algorithm. The algorithm has been benchmarked on several large examples and the results are presented. General Terms: Algorithms, Design, Performance. © 1996 ACM.",Circuit optimization; Critical path enumeration; Gate input reordering; Power estimation; Transistor reordering,
Universal switch modules for fpga design,1996,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002640849&doi=10.1145%2f225871.225886&partnerID=40&md5=d8602914d7813bd0b1fae25df80f0136,"A switch module M with W terminals on each side is said to be universal if every set of nets satisfying the dimensional constraint (i.e., the number of nets on each side of M is at most W) is simultaneously routable through M. In this article, we present a class of universal switch modules. Each of our switch modules has 6W switches and switch-module flexibility three (i.e., FS = 3). We prove that no switch module with less than 6W switches can be universal. We also compare our switch modules with those used in the Xilinx XC4000 family FPGAs and the antisymmetric switch modules (with Fs = 3)1 suggested by Rose and Brown [1991]. Although these two kinds of switch modules also have Fs = 3 and 6W switches, we show that they are not universal. Based on combinatorial counting techniques, we show that each of our universal switch modules can accommodate up to 25% more routing instances, compared with the XC4000-type switch module of the same size. Experimental results demonstrate that our universal switch modules improve routability at the chip level. Finally, our work also provides a theoretical insight into the important observation by Rose and Brown [1991] (based on extensive experiments) that Fs -3 is often sufficient to provide high routability. © 1996 ACM.",Design; Experimentation; Measurement; Performance; Theory; Verification,
Machine Learning Assisted Circuit Sizing Approach for Low-Voltage Analog Circuits with Efficient Variation-Aware Optimization,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152947934&doi=10.1145%2f3567422&partnerID=40&md5=1c8638696b56f88f6cb15e5cffe5f36d,"Low-power analog design is a hot topic for various power efficient applications. Sizing low-power analog circuits is not easy because the increasing uncertainties from low-voltage techniques magnify process variation effects on the design yield. Simulation-based approaches are often adopted for analog circuit sizing because of its high accuracy and adaptability in different cases. However, if process variation is also considered, the huge number of simulations becomes almost infeasible for large circuits. Although there are some recent works that adopt machine learning (ML) techniques to speed up the optimization process, the process variation effects are still hard to be considered in those approaches. Using the popular evolutionary algorithm (EA) as an example, this paper proposes an ML-assisted prediction model to speed up the variation-aware circuit sizing technique for low-voltage analog circuits. By predicting the likelihood for a design that has worse performance, the enhanced EA process is able to skip many unnecessary simulations to reduce the convergence time. Moreover, a novel force-directed model is proposed to guide the optimization toward better yield. Based on the performance of prior circuit samples in the EA optimization, the proposed force model is able to predict the likelihood of a design that has better yield without time-consuming Monte Carlo simulations. Compared with prior works, the proposed approach significantly reduces the number of simulations in the yield-aware EA optimization, which helps to generate practical low-voltage designs with high reliability and low cost.  © 2022 Association for Computing Machinery.",evolutionary algorithm; low power analog circuit sizing; machine learning; Process variation,Analog circuits; Computer aided design; Evolutionary algorithms; Forecasting; Intelligent systems; Learning algorithms; Low power electronics; Monte Carlo methods; Timing circuits; Analog circuit sizing; Circuit Sizing; Low Power; Low power analog circuit sizing; Low-voltage analog circuits; Machine-learning; Optimisations; Performance; Process Variation; Speed up; Machine learning
BoA-PTA: A Bayesian Optimization Accelerated PTA Solver for SPICE Simulation,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152617246&doi=10.1145%2f3555805&partnerID=40&md5=fbbb4a0c5a213862187aecc152ac8d5f,"One of the greatest challenges in integrated circuit design is the repeated executions of computationally expensive SPICE simulations, particularly when highly complex chip testing/verification is involved. Recently, pseudo-transient analysis (PTA) has shown to be one of the most promising continuation SPICE solvers. However, the PTA efficiency is highly influenced by the inserted pseudo-parameters. In this work, we proposed BoA-PTA, a Bayesian optimization accelerated PTA that can substantially accelerate simulations and improve convergence performance without introducing extra errors. Furthermore, our method does not require any pre-computation data or offline training. The acceleration framework can either speed up ongoing, repeated simulations (e.g., Monte-Carlo simulations) immediately or improve new simulations of completely different circuits. BoA-PTA is equipped with cutting-edge machine learning techniques, such as deep learning, Gaussian process, Bayesian optimization, non-stationary monotonic transformation, and variational inference via reparameterization. We assess BoA-PTA in 43 benchmark circuits and real industrial circuits against other SOTA methods and demonstrate an average of 1.5x (maximum 3.5x) for the benchmark circuits and up to 250x speedup for the industrial circuit designs over the original CEPTA without sacrificing any accuracy.  © 2022 Association for Computing Machinery.",Bayesian optimization; CEPTA; circuit simulation; deep learning; Gaussian process; PTA; SPICE,Deep learning; Gaussian distribution; Gaussian noise (electronic); Integrated circuit manufacture; Intelligent systems; Learning systems; Monte Carlo methods; Timing circuits; Transient analysis; 'spice'; Bayesian optimization; Benchmark circuit; CEPTA; Chip testing; Deep learning; Gaussian Processes; Industrial circuits; Pseudo-transient analyse; SPICE simulations; SPICE
Graph Neural Networks for High-Level Synthesis Design Space Exploration,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152893155&doi=10.1145%2f3570925&partnerID=40&md5=1618b9cfbe608053615e2a41d562b59d,"High-level Synthesis (HLS) Design-Space Exploration (DSE) aims at identifying Pareto-optimal synthesis configurations whose exhaustive search is unfeasible due to the design-space dimensionality and the prohibitive computational cost of the synthesis process. Within this framework, we address the design automation problem by proposing graph neural networks that jointly predict acceleration performance and hardware costs of a synthesized behavioral specification given optimization directives. Learned models can be used to rapidly approach the Pareto curve by guiding the DSE, taking into account performance and cost estimates. The proposed method outperforms traditional HLS-driven DSE approaches, by accounting for the arbitrary length of computer programs and the invariant properties of the input. We propose a novel hybrid control and dataflow graph representation that enables training the graph neural network on specifications of different hardware accelerators. Our approach achieves prediction accuracy comparable with that of state-of-the-art simulators without having access to analytical models of the HLS compiler. Finally, the learned representation can be exploited for DSE in unexplored configuration spaces by fine-tuning on a small number of samples from the new target domain. The outcome of the empirical evaluation of this transfer learning shows strong results against state-of-the-art baselines in relevant benchmarks.  © 2022 Association for Computing Machinery.",Design space exploration; graph neural networks; high-level synthesis,Computer hardware; Cost benefit analysis; Cost estimating; Data flow analysis; Pareto principle; Specifications; Computational costs; Design space exploration; Design spaces; Graph neural networks; High-level synthesis; Optimal synthesis; Pareto-optimal; State of the art; Synthesis design; Synthesis process; High level synthesis
A Symbolic Approach to Detecting Hardware Trojans Triggered by Don't Care Transitions,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152481913&doi=10.1145%2f3558392&partnerID=40&md5=9f3ae2b3e249cc4aecbcb6756322b0b6,"Due to the globalization of Integrated Circuit supply chain, hardware Trojans and the attacks that can trigger them have become an important security issue. One type of hardware Trojans leverages the ""don't care transitions""in Finite-state Machines (FSMs) of hardware designs. In this article, we present a symbolic approach to detecting don't care transitions and the hidden Trojans. Our detection approach works at both register-transfer level (RTL) and gate level, does not require a golden design, and works in three stages. In the first stage, it explores the reachable states. In the second stage, it performs an approximate analysis to find the don't care transitions and any discrepancies in the register values or output lines due to don't care transitions. The second stage can be used for both predicting don't care triggered Trojans and for guiding don't care aware reachability analysis. In the third stage, it performs a state-space exploration from reachable states that have incoming don't care transitions to explore the Trojan payload and to find behavioral discrepancies with respect to what has been observed in the first stage. We also present a pruning technique based on the reachability of FSM states. We present a methodology that leverages both RTL and gate-level for soundness and efficiency. Specifically, we show that don't care transitions and Trojans that leverage them must be detected at the gate-level, i.e., after synthesis has been performed, for soundness. However, under specific conditions, Trojan payload exploration can be performed more efficiently at RTL. Additionally, the modular design of our approach also provides a fast Trojan prediction method even at the gate level when the reachable states of the FSM is known a priori. Evaluation of our approach on a set of benchmarks from OpenCores and TrustHub and using gate-level representation generated by two synthesis tools, YOSYS and Synopsis Design Compiler (SDC), shows that our approach is both efficient (up to 10× speedup w.r.t. no pruning) and precise (0% false positives both at RTL and gate-level netlist) in detecting don't care transitions and the Trojans that leverage them. Additionally, the total analysis time can achieve up to 1.62× (using YOSYS) and 1.92× (using SDC) speedup when synthesis preserves the FSM structure, the foundry is trusted, and the Trojan detection is performed at RTL.  © 2022 Association for Computing Machinery.",don't care transitions; symbolic execution; Trojan detection,Hardware security; Malware; Space research; Design compiler; Don't care transition; Don't-cares; Finite states machine; Gate levels; Globalisation; Register-transfer level; Symbolic execution; Trojan detections; Trojans; Supply chains
Application Mapping and Control-system Design for Microfluidic Biochips with Distributed Channel Storage,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152963077&doi=10.1145%2f3564288&partnerID=40&md5=a5cfb2db2677025c521a66bd0f01b4a8,"Continuous-flow microfluidic biochips have emerged as a potential low-cost and fast-responsive lab-on-chip platform. They have attracted much attention due to their capability of performing various biochemical applications concurrently and automatically within a coin-sized chip area. To improve execution efficiency and reduce fabrication cost, a distributed channel-storage architecture can be implemented in which the same channels can be switched between the roles of transportation and storage. Accordingly, fluid transportation, caching, and fetch can be performed simultaneously through different flow paths. Such a flow-path planning needs to be considered carefully in the mapping procedure from a biochemical application to a given biochip architecture. Moreover, all the on-chip valves should be actuated correctly and promptly to temporally block the fluid transportation in unwanted directions and seal the fluids in caching channels. Such an exact control of the valves needs to be considered systematically in control-system design to support the mapping scheme for bioassay execution. In this article, we formulate the practical mapping-control co-design problem for microfluidic biochips with distributed channel storage, considering application mapping, valve synchronization, and control-system design simultaneously, and present an efficient synthesis flow to solve this problem systematically. Given the protocol of a biochemical application and the corresponding chip layout in the flow layer, our goal is to map the biochemical application onto the chip with short execution time. Meanwhile, a practical control system considering the real valve-switching requirements can be constructed efficiently with low fabrication cost. Experimental results on multiple real-life bioassays and synthetic benchmarks demonstrate the effectiveness of the proposed design flow.  © 2022 Association for Computing Machinery.",application mapping; control-system design; distributed channel storage; Microfluidic biochips; valve synchronization,Biochips; Computer aided design; Computer architecture; Costs; Microfluidics; Motion planning; Systems analysis; Application mapping; Bio-chemical applications; Channel storage; Control system designs; Distributed channel storage; Distributed channels; Fabrication cost; Flow path; Micro fluidic biochips; Valve synchronization; Mapping
GraphPlanner: Floorplanning with Graph Neural Network,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152624952&doi=10.1145%2f3555804&partnerID=40&md5=7c43ab8dba279d2984fdbb67617c2339,"Chip floorplanning has long been a critical task with high computation complexity in the physical implementation of VLSI chips. Its key objective is to determine the initial locations of large chip modules with minimized wirelength while adhering to the density constraint, which in essence is a process of constructing an optimized mapping from circuit connectivity to physical locations. Proven to be an NP-hard problem, chip floorplanning is difficult to be solved efficiently using algorithmic approaches. This article presents GraphPlanner, a variational graph-convolutional-network-based deep learning technique for chip floorplanning. GraphPlanner is able to learn an optimized and generalized mapping between circuit connectivity and physical wirelength and produce a chip floorplan using efficient model inference. GraphPlanner is further equipped with an efficient clustering method, a unification of hyperedge coarsening with graph spectral clustering, to partition a large-scale netlist into high-quality clusters with minimized inter-cluster weighted connectivity. GraphPlanner has been integrated with two state-of-the-art mixed-size placers. Experimental studies using both academic benchmarks and industrial designs demonstrate that compared to state-of-the-art mixed-size placers alone, GraphPlanner improves placement runtime by 25% with 4% wirelength reduction on average.  © 2022 Association for Computing Machinery.",deep learning; electronic design automation; Floorplanning; graph neural network; physical design,Coarsening; Computational complexity; Computer aided design; Deep learning; Graph neural networks; Learning systems; Mapping; Optimization; Computation complexity; Critical tasks; Deep learning; Electronics design automation; Floor-planning; Graph neural networks; Mixed Size; Physical design; State of the art; Wire length; Clustering algorithms
Power Converter Circuit Design Automation Using Parallel Monte Carlo Tree Search,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152478950&doi=10.1145%2f3549538&partnerID=40&md5=aeb4f6ecb0060d4bbc3e449cb9b78316,"The tidal waves of modern electronic/electrical devices have led to increasing demands for ubiquitous application-specific power converters. A conventional manual design procedure of such power converters is computation- and labor-intensive, which involves selecting and connecting component devices, tuning component-wise parameters and control schemes, and iteratively evaluating and optimizing the design. To automate and speed up this design process, we propose an automatic framework that designs custom power converters from design specifications using Monte Carlo Tree Search. Specifically, the framework embraces the upper-confidence-bound-tree (UCT), a variant of Monte Carlo Tree Search, to automate topology space exploration with circuit design specification-encoded reward signals. Moreover, our UCT-based approach can exploit small offline data via the specially designed default policy and can run in parallel to accelerate topology space exploration. Further, it utilizes a hybrid circuit evaluation strategy to substantially reduce design evaluation costs. Empirically, we demonstrated that our framework could generate energy-efficient circuit topologies for various target voltage conversion ratios. Compared to existing automatic topology optimization strategies, the proposed method is much more computationally efficient - the sequential version can generate topologies with the same quality while being up to 67% faster. The parallelization schemes can further achieve high speedups compared to the sequential version.  © 2022 Association for Computing Machinery.",circuit synthesis; circuit topology design; Design automation; Monte Carlo Tree Search (MCTS); power converter; upper-confidence-bound tree (UCT),Automation; Computer aided design; Energy efficiency; Integrated circuit manufacture; Iterative methods; Monte Carlo methods; Power converters; Space research; Specifications; Topology; Circuit designs; Circuit synthesis; Circuit topology design; Design automations; Design specification; Monte carlo tree search; Topology design; Tree-search; Upper confidence bound; Upper-confidence-bound tree; Timing circuits
Performance-driven Wire Sizing for Analog Integrated Circuits,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151552185&doi=10.1145%2f3559542&partnerID=40&md5=bcfe7f5d140b1f2335ef7e300bc2af94,"Analog IC performance has a strong dependence on interconnect RC parasitics, which are significantly affected by wire sizes in recent technologies, where minimum-width wires have high resistance. However, performance-driven wire sizing for analog ICs has received very little research attention. In order to fill this void, we develop several techniques to facilitate an end-to-end automatic wire sizing approach. They include a circuit performance model based on customized graph neural network (GNN) and two optimization techniques: one using Bayesian optimization accelerated by the GNN model, and the other based on TensorFlow training. Experimental results show that our technique can achieve 11% circuit performance improvement or 8.7× speedup compared to a conventional Bayesian optimization method.  © 2022 Association for Computing Machinery.",analog circuit design automation; Machine learning; wire sizing,Analog integrated circuits; Computer aided design; Electric network analysis; Graph neural networks; Timing circuits; Wire; Analog Circuit Design; Analog circuit design automation; Bayesian optimization; Circuit performance; Design automations; Graph neural networks; Machine-learning; Performance; Performance-driven; Wire Sizing; Machine learning
Machine Learning Based Framework for Fast Resource Estimation of RTL Designs Targeting FPGAs,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148699797&doi=10.1145%2f3555047&partnerID=40&md5=30b7a40ae1ad79b1719e2642d91397aa,"Field-programmable gate arrays (FPGAs) have grown to be an important platform for integrated circuit design and hardware emulation. However, with the dramatic increase in design scale, it has become a key challenge to partition very large scale integration into multi-FPGA systems. Fast estimation of FPGA on-chip resource usage for individual sub-circuit blocks early in the circuit design flow will provide an essential basis for reasonable circuit partition. It will also help FPGA designers to tune the circuits in hardware description language. In this article, we propose a framework for fast estimation of the on-chip resources consumed by register transfer level (RTL) designs with machine learning methods. We extensively collect RTL designs as a dataset, extract features from the result of a parser tool and analyze their roles, and train a targeted three-stage ensemble learning model. A 5,513× speedup is achieved while having 27% relative absolute error. Although the effect is sufficient to support RTL circuit partition, we discuss how the estimation quality continues to be improved.  © 2022 Association for Computing Machinery.",FPGA; machine learning; Resource estimation; RTL design,Computer hardware description languages; Field programmable gate arrays (FPGA); Integrated circuit design; Integrated circuit manufacture; Fast estimation; Field programmables; Field-programmable gate array; Level design; Machine-learning; On chips; Programmable gate array; Register transfer level design; Register-transfer level; Resource estimation; Machine learning
Training PPA Models for Embedded Memories on a Low-data Diet,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152480935&doi=10.1145%2f3556539&partnerID=40&md5=74d92b1044aef911c16448fecc9cac67,"Supervised machine learning requires large amounts of labeled data for training. In power, performance, and area (PPA) estimation of embedded memories, every new memory compiler version is considered independently of previous compiler versions. Since the data of different memory compilers originate from similar domains, transfer learning may reduce the amount of supervised data required by pre-training PPA estimation neural networks on related domains. We show that provisioning times of PPA models for new compiler versions can be reduced significantly by exploiting similarities among different compilers, versions, and technology nodes. Through transfer learning, we shorten the time to provision PPA models for new compiler versions, which speeds up time-critical periods of the design cycle. Using only 901 training samples (10%) is sufficient to achieve an almost worst-case (98th percentile) estimation error of 2.67% and allows us to shorten model provisioning times from 40 days to less than one week without sacrificing accuracy. To enable a diverse set of source domains for transfer learning, we devise a new, application-independent method for overcoming structural domain differences through domain equalization that attains competitive results when compared to domain-free transfer. A high degree of automation necessitates the efficient assessment of the best source domains. We propose using various metrics to accurately identify four of the five best among 45 datasets with low computational effort.  © 2022 Association for Computing Machinery.",artificial neural networks; deep learning; Electronic design automation; few-shot learning; machine learning; memory compilers; regression; transfer learning,Computer aided design; Deep learning; E-learning; Learning systems; Memory architecture; Program compilers; Supervised learning; Deep learning; Electronics design automation; Embedded memory; Few-shot learning; Machine-learning; Memory compilers; Power estimations; Power performance; Regression; Transfer learning; Neural networks
Learning-based Phase-aware Multi-core CPU Workload Forecasting,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152914907&doi=10.1145%2f3564929&partnerID=40&md5=6666971bd70c367750e436311d521ead,"Predicting workload behavior during workload execution is essential for dynamic resource optimization in multi-processor systems. Recent studies have proposed advanced machine learning techniques for dynamic workload prediction. Workload prediction can be cast as a time series forecasting problem. However, traditional forecasting models struggle to predict abrupt workload changes. These changes occur because workloads are known to go through phases. Prior work has investigated machine-learning-based approaches for phase detection and prediction, but such approaches have not been studied in the context of dynamic workload forecasting. In this article, we propose phase-aware CPU workload forecasting as a novel approach that applies long-term phase prediction to improve the accuracy of short-term workload forecasting. Phase-aware forecasting requires machine learning models for phase classification, phase prediction, and phase-based forecasting that have not been explored in this combination before. Furthermore, existing prediction approaches have only been studied in single-core settings. This work explores phase-aware workload forecasting with multi-threaded workloads running on multi-core systems. We propose different multi-core settings differentiated by the number of cores they access and whether they produce specialized or global outputs per core. We study various advanced machine learning models for phase classification, phase prediction, and phase-based forecasting in isolation and different combinations for each setting. We apply our approach to forecasting of multi-threaded Parsec and SPEC workloads running on an eight-core Intel Core-i9 platform. Our results show that combining GMM clustering with LSTMs for phase prediction and phase-based forecasting yields the best phase-aware forecasting results. An approach that uses specialized models per core achieves an average error of 23% with up to 22% improvement in prediction accuracy compared to a phase-unaware setup.  © 2022 Association for Computing Machinery.",hardware counters; multi-core workload forecasting; Phase classification; phase prediction,Machine learning; Hardware counters; Machine learning models; Multi-core workload forecasting; Multi-cores; Multithreaded; Phase based; Phase classification; Phase prediction; Prediction-based; Workload predictions; Forecasting
A Multilevel Spectral Framework for Scalable Vectorless Power/Thermal Integrity Verification,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147258636&doi=10.1145%2f3529534&partnerID=40&md5=1115b367ba03377e09ad464209f86907,"Vectorless integrity verification is becoming increasingly critical to the robust design of nanoscale integrated circuits. This article introduces a general vectorless integrity verification framework that allows computing the worst-case voltage drops or temperature (gradient) distributions across the entire chip under a set of local and global workload (power density) constraints. To address the computational challenges introduced by the large power grids and three-dimensional mesh-structured thermal grids, we propose a novel spectral approach for highly scalable vectorless verification of large chip designs by leveraging a hierarchy of almost linear-sized spectral sparsifiers of input grids that can well retain effective resistances between nodes. As a result, the vectorless integrity verification solution obtained on coarse-level problems can effectively help compute the solution of the original problem. Our approach is based on emerging spectral graph theory and graph signal processing techniques, which consists of a graph topology sparsification and graph coarsening phase, an edge weight scaling phase, as well as a solution refinement procedure. Extensive experimental results show that the proposed vectorless verification framework can efficiently and accurately obtain worst-case scenarios in even very large designs.  © 2022 Association for Computing Machinery.",graph signal processing; spectral graph coarsening; spectral graph theory; Vectorless integrity verification,Coarsening; Computation theory; Computing power; Graph theory; Integrated circuits; Graph coarsening; Graph signal processing; Integrity verifications; Multilevels; Signal-processing; Spectral graph coarsening; Spectral graph theory; Thermal; Vectorless integrity verification; Verification framework; Signal processing
Rescuing ReRAM-based Neural Computing Systems from Device Variation,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147248532&doi=10.1145%2f3533706&partnerID=40&md5=eb1aaabd8b3c4592312f5c2e78031a09,"Resistive random-access memory (ReRAM)-based crossbar array (RCA) is a promising platform to accelerate vector-matrix multiplication in deep neural networks (DNNs). There are, however, some practical issues, especially device variation, that hinder the versatile development of ReRAM in neural computing systems. The device variations include device-to-device variation (DDV) and cycle-to-cycle variation (CCV) that deviate the devise resistance in the RCA from their target state. Such resistance deviation seriously degrades the inference accuracy of DNNs. To address this issue, we propose a software-hardware compensation solution that includes compensation training based on scale factors (CTSF) and variation-aware compensation training based on scale factors (VACTSF) to protect the ReRAM-based DNN accelerator against device variation. The scale factors in CTSF can be flexibly set for reducing accuracy loss due to device variation when the weights programmed into RCA are determined. For effectively handling CCV, the scale factors are introduced into the training process for obtaining variation-tolerant weights by leveraging the inherent self-healing ability of DNNs. Simulation results based on our method confirm that the accuracy losses due to device variation on LeNet-5, ResNet, and VGG16 with different datasets are less than 5% under a large device variation by CTSF. More robust weights for conquering CCV are also obtained by VACTSF. The simulation results present that our method is competitive in comparison to other variation-tolerant methods.  © 2022 Association for Computing Machinery.",deep neural networks; device variation; Resistive random access memory; scale factor,Large dataset; RRAM; Accuracy loss; Computing system; Crossbar arrays; Device variations; Neural computing; Practical issues; Random access memory; Resistive random access memory; Scale Factor; Vector-matrix multiplications; Deep neural networks
ESPSim: An Efficient Scalable Power Grid Simulator Based on Parallel Algebraic Multigrid,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147250808&doi=10.1145%2f3529533&partnerID=40&md5=378e2c5de8aa70b8d1dc5320ed1f3772,"Fast verification for the extremely large-scale power grid is demanding as CMOS technology advances consistently. In this work, we propose ESPSim, an efficient scalable power grid simulator based on a parallel smoothed aggregation-based algebraic multigrid technique. ESPSim has the ability to do fast DC and transient analysis through MPI and adaptive timestep control mechanism. Thanks to the smoother applied on the prolongation operator, ESPSim copes well with the convergence rate on extremely large-scale power grid transient analysis. Extensive experiments are conducted with a variety of serial/parallel solvers. The runtime of ESPSim is linear with case size. With 16 processors, 1,000 timesteps transient analysis of 63.4M nodes can be completed in 22.1 min. Over 22× speedup compared to the well-known direct solver Cholmod is observed.  © 2022 Association for Computing Machinery.",parallel algebraic multigrid; Power grid; transient analysis,Algebra; Adaptive time-step control; Aggregation-based algebraic multigrid; Algebraic multigrids; CMOS technology; Large-scales; Multigrid technique; Parallel algebraic multigrid; Power grids; Smoothed aggregation; Technology advances; Transient analysis
AIMCU-MESO: An In-Memory Computing Unit Constructed by MESO Device,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147257526&doi=10.1145%2f3539575&partnerID=40&md5=8ccee54cc2b7ad90a5166ec1ac021ca3,"Traditional CMOS-based von-Neumann computer architecture faces the issue of memory wall that the limitation of bus-bandwidth and the speed mismatch between processor and memory restrict the efficiency of data processing along with an irreducible energy consumption conducted by data movement, especially in some data-intensive applications. Recently, some novel in-memory computing (IMC) paradigms developed by utilizing the characteristics of different non-volatile memories provide promising ways to overcome the bottleneck of memory wall. Here, we propose a new IMC unit based on a memory array with the core element of magnetoelectric spin-orbit logic (MESO) device (AIMCU-MESO), in which the characteristics of the MESO device are exploited to achieve several in-memory logic operations with the functions of NAND, NOR, and XOR in the MESO-based memory array. With the aid of some transistor-based switches, these logic operations can be achieved between any two MESOs in the array. Furthermore, the computing process of a 1-bit full adder (FA) is achieved in AIMCU-MESO by the in-memory logic manner to demonstrate the ability of logic cascading. The result of SPICE simulation for achieving the 1-bit FA using MESO devices is demonstrated, and the performances are compared with other designs of spintronics-based devices. Compared to multilevel voltage-controlled spin-orbit torque-based magnetic memory, the proposed design demonstrates 71.4% and 49.2% reductions in terms of storage delay and logic delay, respectively. © 2022 Association for Computing Machinery.",In-memory computing (IMC); logic in memory; magnetic memory; magnetoelectric spin-orbit (MESO); spintronic logic,Computation theory; Computer circuits; Data handling; Green computing; Magnetic storage; Memory architecture; SPICE; Computing units; In-memory computing; Logic in memory; Magnetic memory; Magnetoelectric spin-orbit; Magnetoelectrics; Memory array; Memory wall; Spin orbits; Spintronic logic; Energy utilization
Encoder-Decoder Networks for Analyzing Thermal and Power Delivery Networks,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147855928&doi=10.1145%2f3526115&partnerID=40&md5=26650f7442807501250a4eaea3afd9a4,"Power delivery network (PDN) analysis and thermal analysis are computationally expensive tasks that are essential for successful integrated circuit (IC) design. Algorithmically, both these analyses have similar computational structure and complexity as they involve the solution to a partial differential equation of the same form. This article converts these analyses into image-to-image and sequence-to-sequence translation tasks, which allows leveraging a class of machine learning models with an encoder-decoder-based generative (EDGe) architecture to address the time-intensive nature of these tasks. For PDN analysis, we propose two networks: (i) IREDGe: a full-chip static and dynamic IR drop predictor and (ii) EMEDGe: electromigration (EM) hotspot classifier based on input power, power grid distribution, and power pad distribution patterns. For thermal analysis, we propose ThermEDGe, a full-chip static and dynamic temperature estimator based on input power distribution patterns for thermal analysis. These networks are transferable across designs synthesized within the same technology and packing solution. The networks predict on-chip IR drop, EM hotspot locations, and temperature in milliseconds with negligibly small errors against commercial tools requiring several hours. © 2022 Association for Computing Machinery. All rights reserved.",electromigration (EM); encoder-decoder networks; IR drop; Machine learning (ML) for electronic design automation (EDA); Power delivery network (PDN) analysis; Thermal analysis; U-Nets,Decoding; Drops; E-learning; Electromigration; Electronic design automation; Integrated circuit design; Network coding; Thermoanalysis; VLSI circuits; Electromigration; Electronics design automation; Encoder-decoder; Encoder-decoder network; IR drop; Machine learning  for electronic design automation; Machine-learning; Power delivery network; Power delivery network  analyse; U-net; Machine learning
E2-VOR: An End-to-End En/Decoder Architecture for Efficient Video Object Recognition,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147255525&doi=10.1145%2f3543852&partnerID=40&md5=6b947fbfb8c738ca86a09608f359ceaa,"High-resolution video object recognition (VOR) evolves so fast but is very compute-intensive. This is because VOR leverages compute-intensive deep neural network (DNN) for better accuracy. Although many works have been proposed for speedup, they mostly focus on DNN algorithm and hardware acceleration on the edge side. We observe that most video streams need to be losslessly compressed before going online and an encoder should have all the video information. Moreover, as the cloud should have abundant computing power to handle sophisticated VOR algorithms, we propose to take a one-shot effort for a modified VOR algorithm at the encoding stage in cloud and integrate the full VOR regeneration into a slightly extended decoder on the device. The scheme can enable lightweight VOR with server-class accuracy by simply leveraging the classic and economic video decoder universal to any mobile device. Meanwhile, the scheme can save massive computing power for not repetitively processing the same video on different user devices that makes it extremely sustainable for green computing across the whole network.We propose E2-VOR, an end-to-end encoder and decoder architecture for efficient VOR. We carefully design the scheme to have minimum impact on the video bitstream transmitted. In the cloud, the VOR extended video encoder tracks on a macro-block basis and packs intelligent information into the video stream for increased VOR accuracy and fast regenerating process. On the edge device, we extend the traditional video decoder with a small piece of dedicated hardware to enable the efficient VOR regeneration. Our experiment shows that E2-VOR can achieve 5.0× performance improvement with less than 0.4% VOR accuracy loss compared to the state-of-the-art FAVOS scheme. On average, E2-VOR can run over 54 frames-per-second (FPS) for 480P videos on an edge device. © 2022 Association for Computing Machinery.",accelerator; end-to-end; neural network; Video object recognition,Computer architecture; Computer hardware; Computing power; Decoding; Deep neural networks; Green computing; Object recognition; Video streaming; Computing power; Decoder architecture; End to end; Neural-networks; Object recognition algorithm; Objects recognition; Recognition accuracy; Video decoders; Video object recognition; Video objects; Network architecture
CoVerPlan: A Comprehensive Verification Planning Framework Leveraging PSS Specifications,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147258894&doi=10.1145%2f3543175&partnerID=40&md5=adeb7dd27af26588a97080e79c029a3a,"With increasing design complexity, the portability of tests across different designs and platforms becomes a key criterion for accelerating verification closure. The Portable Test and Stimulus Standard (PSS) is an emerging industry standard prepared by Accellera for system-on-chip verification and testing. It provides language constructs to create a target-agnostic representation of stimulus and test scenarios reused by various users across many levels of integration. In this article, we present CoVerPlan, a comprehensive verification framework built to explore the power of action inferencing on test models written in PSS. The proposed verification framework leverages a Boolean satisfiability problem planner to unwind the actual verification flow from the PSS specifications and automatically synthesizes target-specific constraint-random testbenches and formal assertions. CoVerPlan also carries out assertion-based verification of the synthesized properties. We demonstrate the efficacy of our proposed framework over several case studies, like the Advanced Microcontroller Bus Architecture advanced peripheral bus protocol, a simple Reduced Instruction Set Computer processor, and a cache coherence protocol. © 2022 Association for Computing Machinery.",portable stimulus standard; SAT planner; testbench; Verification,Computer aided design; Network architecture; Specifications; Design complexity; Industry standards; On-chip verifications; Planning framework; Portable stimulus standard; SAT planner; Standard specifications; Systems-on-Chip; Test-bench; Verification framework; System-on-chip
Memory-Throughput Trade-off for CNN-Based Applications at the Edge,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147258415&doi=10.1145%2f3527457&partnerID=40&md5=a86e1b403fd966caaebfd64e9e8f961d,"Many modern applications require execution of Convolutional Neural Networks (CNNs) on edge devices, such as mobile phones or embedded platforms. This can be challenging, as the state-of-the art CNNs are memory costly, whereas the memory budget of edge devices is highly limited. To address this challenge, a variety of CNN memory reduction methodologies have been proposed. Typically, the memory of a CNN is reduced using methodologies such as pruning and quantization. These methodologies reduce the number or precision of CNN parameters, thereby reducing the CNN memory cost. When more aggressive CNN memory reduction is required, the pruning and quantization methodologies can be combined with CNN memory reuse methodologies. The latter methodologies reuse device memory allocated for storage of CNN intermediate computational results, thereby further reducing the CNN memory cost. However, the existing memory reuse methodologies are unfit for CNN-based applications that exploit pipeline parallelism available within the CNNs or use multiple CNNs to perform their functionality. In this article, we therefore propose a novel CNN memory reuse methodology. In our methodology, we significantly extend and combine two existing CNN memory reuse methodologies to offer efficient memory reuse for a wide range of CNN-based applications.  © 2022 Association for Computing Machinery.",AI at the edge; Convolutional neural networks; memory reduction; trade-off,Budget control; Convolutional neural networks; Cost reduction; Deep neural networks; Economic and social effects; AI at the edge; Convolutional neural network; Memory cost; Memory reduction; Memory reuse; Modern applications; Network based applications; Network memory; Quantisation; Trade off; Convolution
A Learning-based Methodology for Scenario-aware Mapping of Soft Real-time Applications onto Heterogeneous MPSoCs,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147249976&doi=10.1145%2f3529230&partnerID=40&md5=c7df7fac02b932f3a223747f45f083de,"Soft real-time streaming applications often process input data that evoke varying workloads for their tasks. This may lead to high energy consumption or deadline misses in case their mapping onto a heterogeneous MPSoC target architecture is not adapted, e.g., when tasks with high execution times for the current input are assigned to resources of low computational power. To handle the vast variety of different input data, we propose to cluster data with similar execution characteristics into so-called data scenarios for which we determine specialized mappings by performing a scenario-aware design space exploration (DSE). A runtime manager (RTM) uses these mappings to adapt the execution of the running applications to their upcoming input by first identifying their best-suited scenarios. Subsequently, the RTM selects mappings considering their identified scenarios, which minimize the total number of deadline misses and the consumed energy. We embed the RTM into hybrid application mapping (HAM); ergo, performing time-consuming optimizations offline. In this article, we propose a novel data-scenario-aware HAM methodology that can cope with multiple applications and comprises two novel scenario-based mapping selection algorithms: Inter-Application Resource Mediation Mapping introduces barely any runtime overhead. Adaptive multi-app mapping selection is highly adaptive to changes in the application workload but imposes a small runtime overhead. Our HAM approach is fully automated and uses machine-learning techniques to learn the selection of suitable mappings from training data sequences at design time. Experiments on three differently complex target architectures show that our proposed approach consistently outperforms existing state-of-the-art solutions regarding the number of deadline misses and consumed energy.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",design space exploration; Hybrid application mapping; machine learning; runtime manager,Energy utilization; Input output programs; Integrated circuit design; Learning systems; Mapping; Multiprocessing systems; System-on-chip; Application mapping; Consumed energy; Design space exploration; Hybrid application mapping; Hybrid applications; Input datas; Machine-learning; Runtime manager; Runtimes; Target architectures; Machine learning
Accuracy Configurable Adders with Negligible Delay Overhead in Exact Operating Mode,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147333765&doi=10.1145%2f3549936&partnerID=40&md5=f70b52438bea70e613ef6d315a4d388d,"In this paper, two accuracy configurable adders capable of operating in approximate and exact modes are proposed. In the adders, which include a block-based carry propagate and a parallel prefix structure, the carry chains are cut off in the approximate mode limiting the carry chain depth to two blocks. In the case of parallel prefix adder, we propose a special carry generate tree equipped with a power gating means. In both of the proposed structures, the critical paths of the adders are not increased in the exact operating mode. Thus, the main objective of proposing these approximate adder structures is to present an accuracy configurable adder structure whose delay in the exact mode is almost the same as an exact adder. The efficacies of the proposed accuracy configurable adders are compared with some state-of-the-art adder structures using a 15nm CMOS technology. In addition, their efficacies are evaluated in two error-resilient applications. These studies show that the proposed carry-propagate adder has 22% (51%) lower energy consumption (error rate) compared to the best prior works. Also, the proposed parallel prefix adder provides, on average, 20% lower energy consumption compared to the exact parallel prefix adders. © 2022 Association for Computing Machinery.",accuracy configurable; Approximate adder; carry propagate adder; parallel prefix adder,Computer aided design; Energy utilization; Accuracy configurable; Approximate adder; Block based; Carry chains; Carry propagate adder; Delay overheads; Low energy consumption; Operating modes; Parallel prefix; Parallel prefix adder; Adders
A Novel Architecture Design for Output Significance Aligned Flow with Adaptive Control in ReRAM-based Neural Network Accelerator,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146052191&doi=10.1145%2f3510819&partnerID=40&md5=5be3470cf56d15dcc0b90954fb9c823d,"Resistive-RAM-based (ReRAM-based) computing shows great potential on accelerating DNN inference by its highly parallel structure. Regrettably, computing accuracy in practical is much lower than expected due to the non-ideal ReRAM device. Conventional computing flow with fixed wordline activation scheme can effectively protect computing accuracy but at the cost of significant performance and energy savings reduction. For such embarrassment of accuracy, performance and energy, this article proposes a new Adaptive-Wordline-Activation control scheme (AWA-control) and combines it with a theoretical Output-Significance-Aligned computing flow (OSA-flow) to enable fine-grained control on output significance with distinct impact on final result. We demonstrate AWA-control-supported OSA-flow architecture with maximal compatibility to conventional crossbar by input retiming and weight remapping using shifting registers to enable the new flow. However, in contrast to the conventional computing architecture, the OSA-flow architecture shows the better capability to exploit data sparsity commonly seen in DNN models. So we also design a sparsity-aware OSA-flow architecture for further DNN speedup. Evaluation results show that OSA-flow architecture can provide significant performance improvement of 21.6×, and energy savings of 96.2% over conventional computing architecture with similar DNN accuracy. © 2022 Association for Computing Machinery.",AWA-control; FWA-control; ISA-flow; OSA-flow; sparsity exploitation,Adaptive control systems; Chemical activation; Computer architecture; Energy conservation; Network architecture; Activation control; Adaptive-wordline-activation control scheme; Computing flows; Control schemes; Flow architectures; FWA-control; ISA-flow; Output-significance-aligned computing flow; Sparsity exploitations; Wordlines; RRAM
Degraded Mode-benefited I/O Scheduling to Ensure I/O Responsiveness in RAID-enabled SSDs,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146050826&doi=10.1145%2f3522755&partnerID=40&md5=88aca011709b8bf5757b855ae0195b47,"RAID-enabled SSDs commonly have unbalanced I/O workloads on their components (e.g., SSD channels), as the data/parity chunks in the same stripe may have varied access frequency, which greatly impacts I/O responsiveness. This article proposes a I/O scheduling scheme by resorting to the degraded read mode and the read-modify-write mode to reduce the long-tail latency of I/O requests in RAID-enabled SSDs. The basic idea is to avoid scheduling read or update requests to the heavily congested but targeted RAID components. Such requests are satisfied by accessing other relevant RAID components by certain XOR computations (we call the degraded modes). Specially, we build a queuing overhead assessment model on the top of factors of data redundancy and the current blocked I/O traffics on SSD channels to precisely dispatch incoming I/O requests to be fulfilled with the degraded mode or not. The trace-driven experiments illustrate that the proposed scheme can reduce the long-tail latency of read requests by 23.1% on average at the 99.99th percentile, in contrast to state-of-the-art scheduling methods. © 2022 Association for Computing Machinery.",degraded mode; long-tail latency; modeling; RAID-5; reliability; SSDs,Access frequency; Degraded mode; Long tail; Long-tail latency; Modeling; Overhead assessment; RAID-5; Read-modify writes; Scheduling schemes; SSD
Introduction to the Special Section on Energy-Efficient AI Chips,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140456030&doi=10.1145%2f3538502&partnerID=40&md5=eb7050b9245bf05a7a9c343639e271b6,[No abstract available],,
E2HRL: An Energy-efficient Hardware Accelerator for Hierarchical Deep Reinforcement Learning,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141026460&doi=10.1145%2f3498327&partnerID=40&md5=7d7bf727e529db892d8db7b0be40c739,"Recently, Reinforcement Learning (RL) has shown great performance in solving sequential decision-making and control in dynamic environment problems. Despite its achievements, deploying Deep Neural Network (DNN)-based RL is expensive in terms of time and power due to the large number of episodes required to train agents with high dimensional image representations. Additionally, at the interference the large energy footprint of deep neural networks can be a major drawback. Embedded edge devices as the main platform for deploying RL applications are intrinsically resource-constrained and deploying deep neural network-based RL on them is a challenging task. As a result, reducing the number of actions taken by the RL agent to learn desired policy, along with the energy-efficient deployment of RL, is crucial. In this article, we propose Energy Efficient Hierarchical Reinforcement Learning (E2HRL), which is a scalable hardware architecture for RL applications. E2HRL utilizes a cross-layer design methodology for achieving better energy efficiency, smaller model size, higher accuracy, and system integration at the software and hardware layers. Our proposed model for RL agent is designed based on the learning hierarchical policies, which makes the network architecture more efficient for implementation on mobile devices. We evaluated our model in three different RL environments with different level of complexity. Simulation results with our analysis illustrate that hierarchical policy learning with several levels of control improves RL agents training efficiency and the agent learns the desired policy faster compared to a non-hierarchical model. This improvement is specifically more observable as the environment or the task becomes more complex with multiple objective subgoals. We tested our model with different hyperparameters to achieve the maximum reward by the RL agent while minimizing the model size, parameters, and required number of operations. E2HRL model enables efficient deployment of RL agent on resource-constraint-embedded devices with the proposed custom hardware architecture that is scalable and fully parameterized with respect to the number of input channels, filter size, and depth. The number of processing engines (PE) in the proposed hardware can vary between 1 to 8, which provides the flexibility of tradeoff of different factors such as latency, throughput, power, and energy efficiency. By performing a systematic hardware parameter analysis and design space exploration, we implemented the most energy-efficient hardware architectures of E2HRL on Xilinx Artix-7 FPGA and NVIDIA Jetson TX2. Comparing the implementation results shows Jetson TX2 boards achieve 0.1 ∼1.3 GOP/S/W energy efficiency while Artix-7 FPGA achieves 1.1 ∼11.4 GOP/S/W, which denotes 8.8× ∼11× better energy efficiency of E2HRL when model is implemented on FPGA. Additionally, compared to similar works our design shows better performance and energy efficiency. © 2022 Association for Computing Machinery.",CNN; CPU; energy efficient hardware accelerator; FPGA; Reinforcement Learning,Complex networks; Decision making; Deep neural networks; Energy efficiency; Hierarchical systems; Integrated circuit design; Learning systems; Network architecture; Reinforcement learning; CPU; Energy efficient; Energy efficient hardware accelerator; Hardware accelerators; Learn+; Model size; Network-based; Performance; Reinforcement learning agent; Reinforcement learnings; Field programmable gate arrays (FPGA)
DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140644769&doi=10.1145%2f3510835&partnerID=40&md5=e7060c468f3e53e72bf32c1371c0e5d6,"Semantic segmentation for scene understanding is nowadays widely demanded, raising significant challenges for the algorithm efficiency, especially its applications on resource-limited platforms. Current segmentation models are trained and evaluated on massive high-resolution scene images (“data-level”) and suffer from the expensive computation arising from the required multi-scale aggregation (“network level”). In both folds, the computational and energy costs in training and inference are notable due to the often desired large input resolutions and heavy computational burden of segmentation models. To this end, we propose DANCE, general automated DAta-Network Co-optimization for Efficient segmentation model training and inference. Distinct from existing efficient segmentation approaches that focus merely on light-weight network design, DANCE distinguishes itself as an automated simultaneous data-network co-optimization via both input data manipulation and network architecture slimming. Specifically, DANCE integrates automated data slimming which adaptively downsamples/drops input images and controls their corresponding contribution to the training loss guided by the images’ spatial complexity. Such a downsampling operation, in addition to slimming down the cost associated with the input size directly, also shrinks the dynamic range of input object and context scales, therefore motivating us to also adaptively slim the network to match the downsampled data. Extensive experiments and ablating studies (on four SOTA segmentation models with three popular segmentation datasets under two training settings) demonstrate that DANCE can achieve “all-win” towards efficient segmentation (reduced training cost, less expensive inference, and better mean Intersection-over-Union (mIoU)). Specifically, DANCE can reduce ↓25%–↓77% energy consumption in training, ↓31%–↓56% in inference, while boosting the mIoU by ↓0.71%–↑ 13.34%. © 2022 Association for Computing Machinery.","Efficient training and inference methods, semantic segmentation","Automation; Energy utilization; Network architecture; Semantic Web; Semantics; Automated data; Co-optimization; Data network; Efficient training and inference method, semantic segmentation; Inference methods; Model inference; Model training; Segmentation models; Semantic segmentation; Training methods; Semantic Segmentation"
Demand-Driven Multi-Target Sample Preparation on Resource-Constrained Digital Microfluidic Biochips,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132132610&doi=10.1145%2f3474392&partnerID=40&md5=4eaf5e5d03d40473d0e245f1634da1d9,"Microfluidic lab-on-chips offer promising technology for the automation of various biochemical laboratory protocols on a minuscule chip. Sample preparation (SP) is an essential part of any biochemical experiments, which aims to produce dilution of a sample or a mixture of multiple reagents in a certain ratio. One major objective in this area is to prepare dilutions of a given fluid with different concentration factors, each with certain volume, which is referred to as the demand-driven multiple-target (DDMT) generation problem. SP with microfluidic biochips requires proper sequencing of mix-split steps on fluid volumes and needs storage units to save intermediate fluids while producing the desired target ratio. The performance of SP depends on the underlying mixing algorithm and the availability of on-chip storage, and the latter is often limited by the constraints imposed during physical design. Since DDMT involves several target ratios, solving it under storage constraints becomes even harder. Furthermore, reduction of mix-split steps is desirable from the viewpoint of accuracy of SP, as every such step is a potential source of volumetric split error. In this article, we propose a storage-aware DDMT algorithm that reduces the number of mix-split operations on a digital microfluidic lab-on-chip. We also present the layout of the biochip with k-storage cells and their allocation technique for k ≥ 0. Simulation results reveal the superiority of the proposed method compared to the state-of-the-art multi-target SP algorithms. © 2021 Association for Computing Machinery.",,Automation; Biochips; Computer aided design; Biochemical experiments; Biochemical laboratories; Demand-driven; Digital microfluidic biochips; Lab-on-chips; Laboratory protocols; Multi-targets; Multiple targets; Sample preparation; Split-step; Digital microfluidics
A Switching NMOS Based Single Ended Sense Amplifier for High Density SRAM Applications,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160019197&doi=10.1145%2f3576198&partnerID=40&md5=52aacf0a1e2fa4606ac3a495b15ce04b,"The demand for single ended static random access memory is growing, driven by the decreasing technology node and increasing processing load. This mandates the need for a single ended sense amplifier to be used along with the memory. Consequently, a single ended latch based sense amplifier is proposed in this article for high-speed, low power application. The sense amplifier is designed at a 32 nm technology node and its functioning is analyzed at 1 V supply voltage, while the environment temperature is maintained at 27 °C. It is analyzed for its delay, temperature tolerance, variability tolerance, and area occupancy. The delay requirement of 0.2 ns for the proposed scheme is significantly lower in comparison to its other counter parts. While, its false read time is 0.3μs. In terms of power consumption, the proposed sensing topology is marginally higher than SPSS, but its leakage power is 1.4 times less than SPSS. The major advantage of the proposed SA is its reduced area footprint of 7.65 μm2, which is 1.78 times better than the best pre-existing topology in terms of area.  © 2023 Association for Computing Machinery.",low power circuit; low variability; sense amplifier; Single ended Sensing; temperature resilient,Low power electronics; Static random access storage; Timing circuits; High density SRAM; Low variability; Low-power circuit; Sense amplifier; Single ended sense amplifiers; Single ended sensing; Single-ended; SRAM applications; Technology nodes; Temperature resilient; Topology
GANDSE: Generative Adversarial Network-based Design Space Exploration for Neural Network Accelerator Design,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160020746&doi=10.1145%2f3570926&partnerID=40&md5=82cb0b7e8580c2f7e34da5317ba364fe,"With the popularity of deep learning, the hardware implementation platform of deep learning has received increasing interest. Unlike the general purpose devices, e.g., CPU or GPU, where the deep learning algorithms are executed at the software level, neural network hardware accelerators directly execute the algorithms to achieve higher energy efficiency and performance improvements. However, as the deep learning algorithms evolve frequently, the engineering effort and cost of designing the hardware accelerators are greatly increased. To improve the design quality while saving the cost, design automation for neural network accelerators was proposed, where design space exploration algorithms are used to automatically search the optimized accelerator design within a design space. Nevertheless, the increasing complexity of the neural network accelerators brings the increasing dimensions to the design space. As a result, the previous design space exploration algorithms are no longer effective enough to find an optimized design. In this work, we propose a neural network accelerator design automation framework named GANDSE, where we rethink the problem of design space exploration, and propose a novel approach based on the generative adversarial network (GAN) to support an optimized exploration for high-dimension large design space. The experiments show that GANDSE is able to find the more optimized designs in negligible time compared with approaches including multilayer perceptron and deep reinforcement learning.  © 2023 Association for Computing Machinery.",Design space exploration; generative adversarial networks,Computer aided design; Computer hardware; Cost engineering; Deep neural networks; Energy efficiency; Generative adversarial networks; Reinforcement learning; Accelerator design; Design automations; Design space exploration; Design spaces; Exploration algorithms; Hardware accelerators; Hardware implementations; Network based designs; Neural-networks; Optimized designs; Learning algorithms
"A Case for Precise, Fine-Grained Pointer Synthesis in High-Level Synthesis",2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131264097&doi=10.1145%2f3491430&partnerID=40&md5=ee9194224b7fdbe11160ff66c2d17d3d,"This article combines two practical approaches to improve pointer synthesis within HLS tools. Both approaches focus on inefficiencies in how HLS tools treat the points-to graph - a mapping that connects each instruction to the memory locations that it might access at runtime. HLS pointer synthesis first computes the points-to graph via pointer analysis and then implements its connections in hardware, which gives rise to two inefficiencies. First, HLS tools typically favour pointer analysis that is fast, sacrificing precision. Second, they also favour centralising memory connections in hardware for instructions that can point to more than one location.In this article, we demonstrate that a more precise pointer analysis coupled with decentralised memory connections in hardware can substantially reduce the unnecessary sharing of memory resources. We implement both flow- and context-sensitive pointer analysis and fine-grained memory connections in two modern HLS tools, LegUp and Vitis HLS. An evaluation on three benchmark suites, ranging from non-trivial pointer use to standard HLS benchmarks, indicates that when we improve both precision and granularity of pointer synthesis, on average, we can reduce area and latency by around 42% and 37%, respectively. © 2022 Association for Computing Machinery.",Context sensitivity; flow sensitivity; high-level synthesis; pointer analysis,Computer hardware; Context sensitivity; Decentralised; Fine grained; Flow sensitivity; High-level synthesis; Memory locations; Memory resources; Pointer analysis; Points-to graph; Runtimes; High level synthesis
Introduction to the Special Section on High-level Synthesis for FPGA: Next-generation Technologies and Applications,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131267132&doi=10.1145%2f3519279&partnerID=40&md5=db27e994c89a539e6c36389bdcf6142c,[No abstract available],,
Fault Localization Scheme for Missing Gate Faults in Reversible Circuits,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131236311&doi=10.1145%2f3503539&partnerID=40&md5=b58b8f950a1ed81265c576222ade239f,"This article introduces a fault localization method to extract the exact location of single and multiple missing gate faults in reversible -CNOT -based circuits. The primary target of the proposed method is to obtain the complete test set for localizing faults in -CNOT circuits. We propose a fault localization algorithm to construct a fault localization tree that can be used to find equivalent and non-equivalent faults. For the non-equivalent faults, the test sequences can be obtained from the fault localization tree that uniquely localizes the non-equivalent faults. Finally, this article presents the experimental results and comparative analysis with existing works. © 2022 Association for Computing Machinery.",Complete test set; fault analysis table; fault localization tree; multiple missing gate fault; single missing gate fault,Computer aided design; Trees (mathematics); Complete test set; Fault analyse table; Fault analysis; Fault localization; Fault localization tree; Missing gate faults; Multiple missing gate fault; Single missing gate fault; Test sets; Timing circuits
Sherlock: A Multi-Objective Design Space Exploration Framework,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131266053&doi=10.1145%2f3511472&partnerID=40&md5=74ca75ef1378751cacda43321c8e8ffd,"Design space exploration (DSE) provides intelligent methods to tune the large number of optimization parameters present in modern FPGA high-level synthesis tools. High-level synthesis parameter tuning is a time-consuming process due to lengthy hardware compilation times - synthesizing an FPGA design can take tens of hours. DSE helps find an optimal solution faster than brute-force methods without relying on designer intuition to achieve high-quality results. Sherlock is a DSE framework that can handle multiple conflicting optimization objectives and aggressively focuses on finding Pareto-optimal solutions. Sherlock integrates a model selection process to choose the regression model that helps reach the optimal solution faster. Sherlock designs a strategy based around the multi-armed bandit problem, opting to balance exploration and exploitation based on the learned and expected results. Sherlock can decrease the importance of models that do not provide correct estimates, reaching the optimal design faster. Sherlock is capable of tailoring its choice of regression models to the problem at hand, leading to a model that best reflects the application design space. We have tested the framework on a large dataset of FPGA design problems and found that Sherlock converges toward the set of optimal designs faster than similar frameworks. © 2022 Copyright held by the owner/author(s).",design automation; Design space exploration; FPGA; optimization,Field programmable gate arrays (FPGA); High level synthesis; Integrated circuit design; Large dataset; Pareto principle; Regression analysis; Design automations; Design space exploration; FPGA design; High-level synthesis; Intelligent method; Multi-objective design space explorations; Optimal design; Optimal solutions; Optimisations; Regression modelling; Optimal systems
Correlated Multi-objective Multi-fidelity Optimization for HLS Directives Design,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131234646&doi=10.1145%2f3503540&partnerID=40&md5=68018b396d268e01ceb9aa00e0be8589,"High-level synthesis (HLS) tools have gained great attention in recent years because it emancipates engineers from the complicated and heavy hardware description language writing and facilitates the implementations of modern applications (e.g., deep learning models) on Field-programmable Gate Array (FPGA), by using high-level languages and HLS directives. However, finding good HLS directives is challenging, due to the time-consuming design processes, the balances among different design objectives, and the diverse fidelities (accuracies of data) of the performance values between the consecutive FPGA design stages.To find good HLS directives, a novel automatic optimization algorithm is proposed to explore the Pareto designs of the multiple objectives while making full use of the data with different fidelities from different FPGA design stages. Firstly, a non-linear Gaussian process (GP) is proposed to model the relationships among the different FPGA design stages. Secondly, for the first time, the GP model is enhanced as correlated GP (CGP) by considering the correlations between the multiple design objectives, to find better Pareto designs. Furthermore, we extend our model to be a deep version deep CGP (DCGP) by using the deep neural network to improve the kernel functions in Gaussian process models, to improve the characterization capability of the models, and learn better feature representations. We test our design method on some public benchmarks (including general matrix multiplication and sparse matrix-vector multiplication) and deep learning-based object detection model iSmart2 on FPGA. Experimental results show that our methods outperform the baselines significantly and facilitate the deep learning designs on FPGA. © 2022 Association for Computing Machinery.",correlated multi-objective optimization; design space exploration; Gaussian process; High-level synthesis; multi-fidelity optimization,Computer hardware description languages; Deep neural networks; Gaussian distribution; Gaussian noise (electronic); High level languages; High level synthesis; Integrated circuit design; Multiobjective optimization; Object detection; Correlated multi-objective optimization; Design objectives; Design space exploration; Design stage; Field programmable gate arrays designs; Gaussian Processes; High-level synthesis; Multi-fidelity optimization; Multi-objectives optimization; Pareto designs; Field programmable gate arrays (FPGA)
A Low-Overhead and High-Security Cryptographic Circuit Design Utilizing the TIGFET-Based Three-Phase Single-Rail Pulse Register against Side-Channel Attacks,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131262417&doi=10.1145%2f3498339&partnerID=40&md5=1e146dd9dd450ef2d78423f5c8945fd4,"Side-channel attack (SCA) reveals confidential information by statistically analyzing physical manifestations, which is the serious threat to cryptographic circuits. Various SCA circuit-level countermeasures have been proposed as fundamental solutions to reduce the side-channel vulnerabilities of cryptographic implementations; however, such approaches introduce non-negligible power and area overheads. Among all of the circuit components, flip-flops are the main source of information leakage. This article proposes a three-phase single-rail pulse register (TSPR) based on the three-independent-gate field effect transistor (TIGFET) to achieve all desired properties with improved metrics of area and security. TIGFET-based TSPR consumes a constant power (MCV is 0.25%), has a low delay (12 ps), and employs only 10 TIGFET devices, which is applicable for the low-overhead and high-security cryptographic circuit design compared to the existing flip-flops. In addition, a set of TIGFET-based combinational basic gates are designed to reduce the area occupation and power consumption as much as possible. As a proof of concept, a simplified advanced encryption algorithm (AES), SM4 block cipher algorithm (SM4), and light-weight cryptographic algorithm (PRESENT) are built with the TIGFET-based library. SCA is implemented on the cryptographic implementations to prove its SCA resilience, and the SCA results show that the correct key of cryptographic circuits with TIGFET-based TSPRs is not guessed within 2,000 power traces. © 2022 Association for Computing Machinery.",secure flip-flop design; Side-channel attack; three-independent-gate field effect transistor; three-phase single-rail pulse register,Delay circuits; Field effect transistors; Flip flop circuits; Integrated circuit manufacture; Timing circuits; CryptoGraphics; Flip-flop designs; Low overhead; Low-high; Secure flip-flop design; Side-channel attacks; Three phase; Three phasis; Three-independent-gate field effect transistor; Three-phase single-rail pulse register; Side channel attack
AutoDSE: Enabling Software Programmers to Design Efficient FPGA Accelerators,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131227569&doi=10.1145%2f3494534&partnerID=40&md5=e8dd7f59cf2685a4eb3f8698c175fe6d,"Adopting FPGA as an accelerator in datacenters is becoming mainstream for customized computing, but the fact that FPGAs are hard to program creates a steep learning curve for software programmers. Even with the help of high-level synthesis (HLS), accelerator designers still have to manually perform code reconstruction and cumbersome parameter tuning to achieve optimal performance. While many learning models have been leveraged by existing work to automate the design of efficient accelerators, the unpredictability of modern HLS tools becomes a major obstacle for them to maintain high accuracy. To address this problem, we propose an automated DSE framework - AutoDSE - that leverages a bottleneck-guided coordinate optimizer to systematically find a better design point. AutoDSE detects the bottleneck of the design in each step and focuses on high-impact parameters to overcome it. The experimental results show that AutoDSE is able to identify the design point that achieves, on the geometric mean, 19.9× speedup over one CPU core for MachSuite and Rodinia benchmarks. Compared to the manually optimized HLS vision kernels in Xilinx Vitis libraries, AutoDSE can reduce their optimization pragmas by 26.38× while achieving similar performance. With less than one optimization pragma per design on average, we are making progress towards democratizing customizable computing by enabling software programmers to design efficient FPGA accelerators. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bottleneck optimizer; customized computing; HLS; Merlin Compiler,Acceleration; High level synthesis; Integrated circuit design; Optimal systems; Program compilers; Bottleneck optimizer; Customized computing; Datacenter; Design points; High-level synthesis; Merlin compiler; Optimisations; Optimizers; Parameters tuning; Steep learning curve; Field programmable gate arrays (FPGA)
Inferencing on Edge Devices: A Time- and Space-aware Co-scheduling Approach,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160021220&doi=10.1145%2f3576197&partnerID=40&md5=a1d6d1480a8cdde40621629dfaee3ed2,"Neural Network (NN)-based real-time inferencing tasks are often co-scheduled on GPGPU-style edge platforms. Existing works advocate using different NN parameters for the same detection task in different environments. However, realizing such approaches remains challenging, given accelerator devices' limited on-chip memory capacity. As a solution, we propose a multi-pass, time- and space-aware scheduling infrastructure for embedded platforms with GPU accelerators. The framework manages the residency of NN parameters in the limited on-chip memory while simultaneously dispatching relevant compute operations. The mapping decisions for memory operations and compute operations to the underlying resources of the platform are first determined in an offline manner. For this, we proposed a constraint solver-assisted scheduler that optimizes for schedule makespan. This is followed by memory optimization passes, which take the memory budget into account and accordingly adjust the start times of memory and compute operations. Our approach reports a 74%-90% savings in peak memory utilization with 0%-33% deadline misses for schedules that suffer miss percentage in ranges of 25%-100% when run using existing methods.  © 2023 Association for Computing Machinery.",Convolutional neural network; edge device; GPU; Satisfiability Modulo Theories,Budget control; Convolutional neural networks; Program processors; Space platforms; Co-scheduling; Convolutional neural network; Detection tasks; Edge device; Network-based; Neural network parameters; Neural-networks; On-chip-memory; Real- time; Satisfiability modulo Theories; Graphics processing unit
Learning from the Past: Efficient High-level Synthesis Design Space Exploration for FPGAs,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131227400&doi=10.1145%2f3495531&partnerID=40&md5=6f5f65d571bf4089686f9b0fd5b91094,"The quest to democratize the use of Field-Programmable Gate Arrays (FPGAs) has given High-Level Synthesis (HLS) the final push to be widely accepted with FPGA vendors strongly supporting this VLSI design methodology to expand the FPGA user base. HLS takes as input an untimed behavioral description and generates efficient RTL (Verilog or VHDL). One major advantage of HLS is that it allows us to generate a variety of different micro-architectures from the same behavioral description by simply specifying different combination of synthesis options. In particular, commercial HLS tools make extensive use of synthesize directives in the form pragmas. This strength is also a weakness as it forces HLS users to fully understand how these synthesis options work and how they interact to efficiently set them to get a hardware implementation with the desired characteristics. Luckily, this process can be automated. Unfortunately, the search space grows supra-linearly with the number of synthesis options. To address this, this work proposes an automatic synthesis option tuner dedicated for FPGAs. We have explored a larger number of behavioral descriptions targeting ASICs and FPGAs and found out that due to the internal structure of the FPGA a large number of synthesis options combinations never lead to a Pareto-optimal design and, hence, the search space can be drastically reduced. Moreover, we make use of large database of DSE results that we have generated since we started working in this field to further accelerate the exploration process. For this, we use a technique based on perceptual hashing that allows our proposed explorer to recognize similar program structures in the new description to be explored and match them with structures in our database. This allows us to directly retrieve the pragma settings that lead to Pareto-optimal configurations. Experimental results show that the search space can be accelerated substantially while leading to finding most of the Pareto-optimal designs. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",design space exploration; field-programmable gate arrays; High-level synthesis,Computer architecture; High level synthesis; Integrated circuit design; Logic gates; Optimal systems; Pareto principle; Signal receivers; Behavioral descriptions; Design space exploration; High-level synthesis; Learning from the pasts; Micro architectures; Pareto-optimal design; Search spaces; Synthesis design; Synthesis options; VLSI design methodology; Field programmable gate arrays (FPGA)
"A Survey on Security of Digital Microfluidic Biochips: Technology, Attack, and Defense",2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131252710&doi=10.1145%2f3494697&partnerID=40&md5=aa6824dfe503db56b98ccee84a5b1514,"As an emerging lab-on-a-chip technology platform, digital microfluidic biochips (DMFBs) have been widely used for executing various laboratory procedures in biochemistry and biomedicine such as gene sequencing and near-patient diagnosis, with the advantages of low reagent consumption, high precision, and miniaturization and integration. With the ongoing rapid deployment of DMFBs, however, these devices are now facing serious and complicated security challenges that not only damage their functional integrity but also affect their system reliability. In this article, we present a systematic review of DMFB security, focusing on both the state-of-the-art attack and defense techniques. First, the overall security situation, the working principle, and the corresponding fabrication technology of DMFBs are introduced. Afterwards, existing attack approaches are divided into several categories and discussed in detail, including denial of service, intellectual property piracy, bioassay tampering, layout modification, actuation sequence tampering, concentration altering, parameter modification, reading forgery, and information leakage. To prevent biochips from being damaged by these attack behaviors, a number of defense measures have been proposed in recent years. Accordingly, we further classify these techniques into three categories according to their respective defense purposes, including confidentiality protection, integrity protection, and availability protection. These measures, to varying degrees, can provide effective protection for DMFBs. Finally, key trends and directions for future research that are related to the security of DMFBs are discussed from several aspects, e.g., manufacturing materials, biochip structure, and usage environment, thus providing new ideas for future biochip protection. © 2022 Association for Computing Machinery.",attack behaviors; defense techniques; Digital microfluidic biochips; security,Biochips; Digital microfluidics; Industrial research; Network security; Attack behavior; Biochip technologies; Defense techniques; Digital microfluidic biochips; Gene sequencing; Lab-on-a-chip technology; Laboratory procedures; Security; Technology attack; Technology platforms; Denial-of-service attack
Synthesis of Clock Networks with a Mode-Reconfigurable Topology,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131237603&doi=10.1145%2f3503538&partnerID=40&md5=f79ccdaf65eb2572cc37b7e7ecc7e783,"Modern digital circuits are often required to operate in multiple modes to cater to variable frequency and power requirements. Consequently, the clock networks for such circuits must be synthesized, meeting different timing constraints in different operational modes. The overall power consumption and robustness to variations of a clock network are determined by the topology. However, state-of-the-art clock networks use the same topology in every mode, despite that timing constraints in low- and high-performance modes can be very different. In this article, we propose a clock network with a mode-reconfigurable topology (MRT) for circuits with positive-edge-triggered sequential elements. In high-performance modes, the MRT structure is reconfigured into a near-tree to provide the required robustness to variations. In low-performance modes, the MRT structure is reconfigured into a tree to save power. Non-tree (or near-tree) structures provide robustness to variations by appropriately constructing multiple alternative paths from the clock source to the clock sinks, which neutralizes the negative impact of variations. In MRT structures, OR-gates are used to join multiple alternative paths into a single path. Hence, the MRT structures consume no short-circuit power because there is only one gate driving each net. Moreover, it is straightforward to reconfigure an MRT structure into a tree topology using a single clock gate. In high-performance modes, the experimental results demonstrate that MRT structures have lower power consumption than state-of-the-art near-tree structures. In low-performance modes, the power consumption of the MRT structure is similar to the power consumption of a clock tree. © 2022 Association for Computing Machinery.",Clock network synthesis; near-tree topology; non-tree topology; robustness to variations,Electric clocks; Electric power utilization; Reconfigurable hardware; Clock network; Clock network synthesis; Near-tree topology; Non-tree topologies; Performance; Reconfigurable; Robustness to variation; Timing constraints; Topology structure; Tree topology; Trees (mathematics)
Design Automation Algorithms for the NP-Separate VLSI Design Methodology,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140478848&doi=10.1145%2f3508375&partnerID=40&md5=35d3e4c9073a951402709e99886fd626,"The NP-Separate design methodology for very-large-scale integration (VLSI) design fine-controls the sizes of transistors, thereby achieving significant power, performance, and area improvement compared to the conventional standard-cell-based design methodology. NP-Separate uses NP cells formed by merging and routing N and P cells having only NFETs and PFETs, respectively. The NP cell formation, however, should be automated to design large circuits using the NP-Separate design methodology. In this paper, we propose design automation algorithms to create NP cells automatically. Simulation results show that the automated NP-Separate reduces the design time significantly, decreases the coupling capacitance by 13%, the critical path delay by 6%, and the power consumption by 10% on average compared to the manual NP-Separate designs. We also propose a detailed placement algorithm to generate more compact VLSI layouts with a little wirelength overhead. The combined effect reduces the coupling capacitance by 10%, the critical path delay by 5%, and the power consumption by 10% on average compared to the manual NP-Separate designs. © 2022 Association for Computing Machinery.",cell overlapping; cell-level routing; design automation algorithms; detailed placement; N cell; NP-Separate; P cell,Automation; Capacitance; Computer aided design; Cytology; Electric power utilization; Integrated circuit design; Routing algorithms; Separation; VLSI circuits; Automation algorithms; Cell levels; Cell overlapping; Cell-level routing; Design automation algorithm; Design automations; Detailed placement; N cell; NP-separate; P cell; Routings; Cells
Increasing the Fault Coverage of a Truncated Test Set,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145655411&doi=10.1145%2f3508459&partnerID=40&md5=d6d24a3ff5b3166a136d6ad237b5984d,"Defect-aware, cell-aware, and gate-exhaustive faults are described by input patterns of subcircuits or cells that are expected to activate defects. Even with single-cycle faults, an n-input subcircuit can have up to 2n faults with unique fault detection conditions, resulting in a large test set. Such a test set may have to be truncated to fit in the tester memory or satisfy constraints on test application time. In this case, a loss of fault coverage is inevitable. This article considers the test set denoted by T1 obtained after truncating a larger test set denoted by T0. Suppose that the truncation reduces the set of detected faults from the set denoted by D0 to the set denoted by D1. The procedure described in this article modifies the tests inT1 to gain the detection of faults from D0\D1, even at the cost of losing the detection of faults from D1. The goal is to reduce the fault coverage loss by computing a test set denoted by T2 that detects a set of faults denoted by D2 such that |T2| = |T1| and |D2| > |D1|. Experimental results for benchmark circuits demonstrate the ability of the procedure to increase the coverage of gate-exhaustive faults over several iterations. © 2022 Association for Computing Machinery.",Gate-exhaustive faults; stuck-at faults; test compaction; test generation,Fault detection; Fault coverages; Faults detection; Gate-exhaustive fault; Input patterns; Single cycle; Stuck-at faults; Sub-circuits; Test Compaction; Test generations; Test sets; Defects
Automatic Mapping of the Best-Suited DNN Pruning Schemes for Real-Time Mobile Acceleration,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135451817&doi=10.1145%2f3495532&partnerID=40&md5=cfd6b39c2c50b5afc8550a2c952ced0c,"Weight pruning is an effective model compression technique to tackle the challenges of achieving real-time deep neural network (DNN) inference on mobile devices. However, prior pruning schemes have limited application scenarios due to accuracy degradation, difficulty in leveraging hardware acceleration, and/or restriction on certain types of DNN layers. In this article, we propose a general, fine-grained structured pruning scheme and corresponding compiler optimizations that are applicable to any type of DNN layer while achieving high accuracy and hardware inference performance. With the flexibility of applying different pruning schemes to different layers enabled by our compiler optimizations, we further probe into the new problem of determining the best-suited pruning scheme considering the different acceleration and accuracy performance of various pruning schemes. Two pruning scheme mapping methods-one-search based and the other is rule based-are proposed to automatically derive the best-suited pruning regularity and block size for each layer of any given DNN. Experimental results demonstrate that our pruning scheme mapping methods, together with the general fine-grained structured pruning scheme, outperform the state-of-the-art DNN optimization framework with up to 2.48 and 1.73 DNN inference acceleration on CIFAR-10 and ImageNet datasets without accuracy loss. © 2022 Association for Computing Machinery.",mobile acceleration; Network pruning; neural architecture search,Acceleration; Computer aided design; Mapping; Network architecture; Program compilers; Compiler optimizations; Fine grained; Mapping method; Mobile acceleration; Network inference; Network pruning; Neural architecture search; Neural architectures; Performance; Real- time; Deep neural networks
Dynamic Quantization Range Control for Analog-in-Memory Neural Networks Acceleration,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141028951&doi=10.1145%2f3498328&partnerID=40&md5=284aa3a3ce1e032d8ff644b22fb33a7d,"Analog in Memory Computing (AiMC) based neural network acceleration is a promising solution to increase the energy efficiency of deep neural networks deployment. However, the quantization requirements of these analog systems are not compatible with state-of-the-art neural network quantization techniques. Indeed, while the quantization of the weights and activations is considered by modern deep neural network quantization techniques, AiMC accelerators also impose the quantization of each Matrix Vector Multiplication (MVM) result. In most demonstrated AiMC implementations, the quantization range of MVM results is considered a fixed parameter of the accelerator. This work demonstrates that dynamic control over this quantization range is possible but also desirable for analog neural networks acceleration. An AiMC compatible quantization flow coupled with a hardware aware quantization range driving technique is introduced to fully exploit these dynamic ranges. Using CIFAR-10 and ImageNet as benchmarks, the proposed solution results in networks that are both more accurate and more robust to the inherent vulnerability of analog circuits than fixed quantization range based approaches. © 2022 Association for Computing Machinery.",in-memory-computing; Neural networks; quantization,Acceleration; Computer aided design; Energy efficiency; Vector quantization; Analog systems; ART neural networks; Dynamic controls; Dynamic quantization; In-memory-computing; Matrix vector multiplication; Network deployment; Neural-networks; Quantisation; State of the art; Deep neural networks
Toward a Human-Readable State Machine Extraction,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145881644&doi=10.1145%2f3513086&partnerID=40&md5=e5ac5a4453fe90b7bdac8afccb1cfbd3,"The target of sequential reverse engineering is to extract the state machine of a design. Sequential reverse engineering of a gate-level netlist consists of the identification of so-called state flip-flops (sFFs), as well as the extraction of the state machine. The second step can be solved with an exact approach if the correct sFFs and the correct reset state are provided. For the first step, several more or less heuristic approaches exist. This work investigates sequential reverse engineering with the objective of a human-readable state machine extraction. A human-readable state machine reflects the original state machine and is not overloaded by additional design information. For this purpose, the work derives a systematic categorization of sFF sets, based on properties of single sFFs and their sets. These properties are determined by analyzing the degrees of freedom in describing state machines as the well-known Moore and Mealy machines. Based on the systematic categorization, this work presents an sFF set definition for a human-readable state machine, categorizes existing sFF identification strategies, and develops four post-processing methods. The results show that post-processing predominantly improves the outcome of several existing sFF identification algorithms. © 2022 Copyright held by the owner/author(s).",control logic extraction; flip-flop classification; gate-level netlist; human-readable state machine; IC trust; sequential reverse engineering; State flip-flop; state flip-flop identification,Degrees of freedom (mechanics); Flip flop circuits; Integrated circuits; Reverse engineering; Timing circuits; Control logic; Control logic extraction; Flip-flop classification; Gate levels; Gate-level netlist; Human-readable; Human-readable state machine; IC trust; Netlist; Sequential reverse engineering; State flip-flop; State flip-flop identification; State-machine; Extraction
FUBOCO: Structure Synthesis of Basic Op-Amps by FUnctional BlOck COmposition,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129267755&doi=10.1145%2f3522738&partnerID=40&md5=f1358c010fa86b6f01c88e8b5679de3e,"This article presents a method to automatically synthesize the structure and initial sizing of an operational amplifier. It is positioned between approaches with fixed design plans and a small search space of structures and approaches with generic structural production rules and a large search space with technically impractical structures. The presented approach develops a hierarchical composition graph based on functional blocks that spans a search space of thousands of technically meaningful structure variants for single-output, fully differential, and complementary operational amplifiers. The search algorithm is a combined heuristic and enumerative process. The evaluation is based on circuit sizing with a library of behavioral equations of functional blocks. Formalizing the knowledge of functional blocks in op-amps for structural synthesis and sizing inherently reduces the search space and lessens the number of created topologies not fulfilling the specifications. Experimental results for the three op-amp classes are presented. An outlook how this method can be extended to multi-stage op-amps is given. © 2022 Association for Computing Machinery.",Analog circuit design; CMOS; operational amplifiers,Analog circuits; CMOS integrated circuits; Differential amplifiers; Graphic methods; Integrated circuit design; Integrated circuit manufacture; Timing circuits; Analog Circuit Design; Block composition; CMOS; Design plans; Functional block; Initial sizing; Op amps; Production rules; Search spaces; Structure synthesis; Operational amplifiers
NoC Application Mapping Optimization Using Reinforcement Learning,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145646413&doi=10.1145%2f3510381&partnerID=40&md5=f528c1713a75ebba841bde501fcf60de,"Application mapping is one of the early stage design processes aimed to improve the performance of Network-on-Chip. Mapping is an NP-hard problem. A massive amount of high-quality supervised data is required to solve the application mapping problem using traditional neural networks. In this article, a reinforcement learning–based neural framework is proposed to learn the heuristics of the application mapping problem. The proposed reinforcement learning–based mapping algorithm (RL-MAP) has actor and critic networks. The actor is a policy network, which provides mapping sequences. The critic network estimates the communication cost of these mapping sequences. The actor network updates the policy distribution in the direction suggested by the critic. The proposed RL-MAP is trained with unsupervised data to predict the permutations of the cores to minimize the overall communication cost. Further, the solutions are improved using the 2-opt local search algorithm. The performance of RL-MAP is compared with a few well-known heuristic algorithms, the Neural Mapping Algorithm (NMA) and message-passing neural network-pointer network-based genetic algorithm (MPN-GA). Results show that the communication cost and runtime of the RL-MAP improved considerably in comparison with the heuristic algorithms. The communication cost of the solutions generated by RL-MAP is nearly equal to MPN-GA and improved by 4.2% over NMA, while consuming less runtime. © 2022 Association for Computing Machinery.",actor-critic network; Application mapping; network-on-chip; neural networks; reinforcement learning,Computational complexity; Conformal mapping; Cost benefit analysis; Genetic algorithms; Heuristic algorithms; Learning systems; Message passing; Network-on-chip; Servers; Actor critic; Actor-critic network; Application mapping; Communication cost; Critic network; Mapping algorithms; Networks on chips; Neural-networks; Performance; Reinforcement learnings; Reinforcement learning
Breaking the Design and Security Trade-off of Look-up-table–based Obfuscation,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134800137&doi=10.1145%2f3510421&partnerID=40&md5=74065f018cf8d6dbe5f58474a90bf7e9,"Logic locking and Integrated Circuit (IC) camouflaging are the most prevalent protection schemes that can thwart most hardware security threats. However, the state-of-the-art attacks, including Boolean Satisfiability (SAT) and approximation-based attacks, question the efficacy of the existing defense schemes. Recent obfuscation schemes have employed reconfigurable logic to secure designs against various hardware security threats. However, they have focused on specific design elements such as SAT hardness. Despite meeting the focused criterion such as security, obfuscation incurs additional overheads, which are not evaluated in the present works. This work provides an extensive analysis of Look-up-table (LUT)–based obfuscation by exploring several factors such as LUT technology, size, number of LUTs, and replacement strategy as they have a substantial influence on Power-Performance-Area (PPA) and Security (PPA/S) of the design. We show that using large LUT makes LUT-based obfuscation resilient to hardware security threats. However, it also results in enormous design overheads beyond practical limits. To make the reconfigurable logic obfuscation efficient in terms of design overheads, this work proposes a novel LUT architecture where the security provided by the proposed primitive is superior to that of the traditional LUT-based obfuscation. Moreover, we leverage the security-driven design flow, which uses off-the-shelf industrial EDA tools to mitigate the design overheads further while being non-disruptive to the current industrial physical design flow. We empirically evaluate the security of the LUTs against state-of-the-art obfuscation techniques in terms of design overheads and SAT-attack resiliency. Our findings show that the proposed primitive significantly reduces both area and power by a factor of 8× and 2×, respectively, without compromising security. © 2022 Copyright held by the owner/author(s).",gate replacement algorithm; Hardware security; look up table (LUT); obfuscation; optimized reconfigurable obfuscation; SAT-hard LUT blocks; security-driven design flow,Economic and social effects; Hardware security; Integrated circuits; Reconfigurable hardware; Security systems; Design flows; Gate replacement; Gate replacement algorithm; Look up table; Lookup tables (LUTs); Obfuscation; Optimized reconfigurable obfuscation; Reconfigurable; Replacement algorithm; Satisfiability; Satisfiability-hard look up table block; Security-driven design flow; Table lookup
Quantum Circuit Transformation: A Monte Carlo Tree Search Framework,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143071663&doi=10.1145%2f3514239&partnerID=40&md5=cc4804ab3defde3250a5a6ba8e7fe48d,"In the noisy intermediate-scale quantum era, quantum processing units suffer from, among others, highly limited connectivity between physical qubits. To make a quantum circuit effectively executable, a circuit transformation process is necessary to transform it, with overhead cost the smaller the better, into a functionally equivalent one so that the connectivity constraints imposed by the quantum processing unit are satisfied. Although several algorithms have been proposed for this goal, the overhead costs are often very high, which degenerates the fidelity of the obtained circuits sharply. One major reason for this lies in that, due to the high branching factor and vast search space, almost all of these algorithms only search very shallowly, and thus, very often, only (at most) locally optimal solutions can be reached. In this article, we propose a Monte Carlo Tree Search (MCTS) framework to tackle the circuit transformation problem, which enables the search process to go much deeper. The general framework supports implementations aiming to reduce either the size or depth of the output circuit through introducing SWAP or remote CNOT gates. The algorithms, called MCTS-Size and MCTS-Depth, are polynomial in all relevant parameters. Empirical results on extensive realistic circuits and IBM Q Tokyo show that the MCTS-based algorithms can reduce the size (respectively, depth) overhead by, on average, 66% (respectively, 84%) when compared with t|ket〉, an industrial-level compiler. © 2022 Association for Computing Machinery.",Monte Carlo Tree Search; QPU; quantum circuit transformation; Quantum computing; qubit mapping,Monte Carlo methods; Quantum optics; Timing circuits; Circuit transformation; Monte carlo tree search; Overhead costs; Processing units; QPU; Quantum circuit; Quantum circuit transformation; Quantum Computing; Qubit mapping; Tree-search; Qubits
Efficient Layout Hotspot Detection via Neural Architecture Search,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146052970&doi=10.1145%2f3517130&partnerID=40&md5=ed3caf6dfce09f8dba82878191c6104e,"Layout hotspot detection is of great importance in the physical verification flow. Deep neural network models have been applied to hotspot detection and achieved great success. Despite their success, high-performance neural networks are still quite difficult to design. In this article, we propose a bayesian optimization-based neural architecture search scheme to automatically do this time-consuming and fiddly job. Experimental results on ICCAD 2012 and ICCAD 2019 Contest benchmarks show that the architectures designed by our proposed scheme achieve higher performance on hotspot detection task compared with state-of-the-art manually designed neural networks. © 2022 Association for Computing Machinery.",Hotspot detection; neural architecture search,Benchmarking; Computer aided design; Network architecture; Neural network models; Bayesian optimization; Detection tasks; Hotspot detections; Neural architecture search; Neural architectures; Neural network model; Neural-networks; Performance; Search scheme; State of the art; Deep neural networks
"MVP: An Efficient CNN Accelerator with Matrix, Vector, and Processing-Near-Memory Units",2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141078449&doi=10.1145%2f3497745&partnerID=40&md5=50ad6c852f12c1b87652fecb184914b1,"Mobile and edge devices become common platforms for inferring convolutional neural networks (CNNs) due to superior privacy and service quality. To reduce the computational costs of convolution (CONV), recent CNN models adopt depth-wise CONV (DW-CONV) and Squeeze-and-Excitation (SE). However, existing area-efficient CNN accelerators are sub-optimal for these latest CNN models because they were mainly optimized for compute-intensive standard CONV layers with abundant data reuse that can be pipelined with activation and normalization operations. In contrast, DW-CONV and SE are memory-intensive with limited data reuse. The latter also strongly depends on the nearby CONV layers, making an effective pipelining a daunting task. Therefore, DW-CONV and SE only occupy 10% of entire operations but become memory bandwidth bound, spending more than 60% of the processing time in systolic-array-based accelerators.We propose a CNN acceleration architecture called MVP, which efficiently processes both compute-and memory-intensive operations with a small area overhead on top of the baseline systolic-array-based architecture. We suggest a specialized vector unit tailored for processing DW-CONV, including multipliers, adder trees, and multi-banked buffers to meet the high memory bandwidth requirement. We augment the unified buffer with tiny processing elements to smoothly pipeline SE with the subsequent CONV, enabling concurrent processing of DW-CONV with standard CONV, thereby achieving the maximum utilization of arithmetic units. Our evaluation shows that MVP improves performance by 2.6 and reduces energy by 47% on average for EfficientNet-B0/B4/B7, MnasNet, and MobileNet-V1/V2 with only a 9% area overhead compared to the baseline. © 2022 Copyright held by the owner/author(s).",CNN; energy-efficient AI accelerator; Neural Networks,Acceleration; Bandwidth; Convolutional neural networks; Cost reduction; Energy efficiency; Memory architecture; Network architecture; Pipeline processing systems; Pipelines; Systolic arrays; Area overhead; Common platform; Convolutional neural network; Data reuse; Energy efficient; Energy-efficient AI accelerator; Matrix-vector; Memory units; Neural network model; Neural-networks; Convolution
Software/Hardware Co-design of 3D NoC-based GPU Architectures for Accelerated Graph Computations,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145879836&doi=10.1145%2f3514354&partnerID=40&md5=5acd0f3bf1141bb08a07ad648578e9fb,"Manycore GPU architectures have become the mainstay for accelerating graph computations. One of the primary bottlenecks to performance of graph computations on manycore architectures is the data movement. Since most of the accesses in graph processing are due to vertex neighborhood lookups, locality in graph data structures plays a key role in dictating the degree of data movement. Vertex reordering is a widely used technique to improve data locality within graph data structures. However, these reordering schemes alone are not sufficient as they need to be complemented with efficient task allocation on manycore GPU architectures to reduce latency due to local cache misses. Consequently, in this article, we introduce a software/hardware co-design framework for accelerating graph computations. Our approach couples an architecture-aware vertex reordering with a priority-based task allocation technique. As the task allocation aims to reduce on-chip latency and associated energy, the choice of Network-on-Chip (NoC) as the communication backbone in the manycore platform is an important parameter. By leveraging emerging three-dimensional (3D) integration technology, we propose design of a small-world NoC (SWNoC)-enabled manycore GPU architecture, where the placement of the links connecting the streaming multiprocessors (SMs) and the memory controllers (MCs) follow a power-law distribution. The proposed 3D SWNoC-enabled software/hardware co-design framework achieves 11.1% to 22.9% performance improvement and 16.4% to 32.6% less energy consumption depending on the dataset and the graph application, when compared to the default order of dataset running on a conventional planar mesh architecture. © 2022 Copyright held by the owner/author(s).",GPU manycore; graph analytics; small world NoC; Software/Hardware Co-design; vertex reordering,Application programs; Data structures; Energy utilization; Graph theory; Graphics processing unit; Integrated circuit design; Network architecture; Data movements; GPU manycore; Graph-analytic; Many-core; Networks on chips; Performance; Small world network-on-chip; Software/hardware co designs; Task allocation; Vertex reordering; Network-on-chip
Energy-Efficient LSTM Inference Accelerator for Real-Time Causal Prediction,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129911506&doi=10.1145%2f3495006&partnerID=40&md5=7c5b931bef6e253b9f5f9c5b8fbbb750,"Ever-growing edge applications often require short processing latency and high energy efficiency to meet strict timing and power budget. In this work, we propose that the compact long short-term memory (LSTM) model can approximate conventional acausal algorithms with reduced latency and improved efficiency for real-time causal prediction, especially for the neural signal processing in closed-loop feedback applications. We design an LSTM inference accelerator by taking advantage of the fine-grained parallelism and pipelined feedforward and recurrent updates. We also propose a bit-sparse quantization method that can reduce the circuit area and power consumption by replacing the multipliers with the bit-shift operators. We explore different combinations of pruning and quantization methods for energy-efficient LSTM inference on datasets collected from the electroencephalogram (EEG) and calcium image processing applications. Evaluation results show that our proposed LSTM inference accelerator can achieve 1.19 GOPS/mW energy efficiency. The LSTM accelerator with 2-sbit/16-bit sparse quantization and 60% sparsity can reduce the circuit area and power consumption by 54.1% and 56.3%, respectively, compared with a 16-bit baseline implementation. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Calcium imaging; EEG; energy efficiency; long short-term memory (LSTM); quantization,Brain; Budget control; Calcium; Electric power utilization; Electroencephalography; Image processing; Long short-term memory; Low power electronics; Calcium imaging; Energy efficient; High energy efficiency; Long short-term memory; Memory modeling; Neural signal processing; Power budgets; Quantisation; Real- time; Reduced latencies; Energy efficiency
EF-Train: Enable Efficient On-device CNN Training on FPGA through Data Reshaping for Online Adaptation or Personalization,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130421497&doi=10.1145%2f3505633&partnerID=40&md5=41d4a3f088b79089f2ce43821904f349,"Conventionally, DNN models are trained once in the cloud and deployed in edge devices such as cars, robots, or unmanned aerial vehicles (UAVs) for real-time inference. However, there are many cases that require the models to adapt to new environments, domains, or users. In order to realize such domain adaption or personalization, the models on devices need to be continuously trained on the device. In this work, we design EF-Train, an efficient DNN training accelerator with a unified channel-level parallelism-based convolution kernel that can achieve end-to-end training on resource-limited low-power edge-level FPGAs. It is challenging to implement on-device training on resource-limited FPGAs due to the low efficiency caused by different memory access patterns among forward and backward propagation and weight update. Therefore, we developed a data reshaping approach with intra-tile continuous memory allocation and weight reuse. An analytical model is established to automatically schedule computation and memory resources to achieve high energy efficiency on edge FPGAs. The experimental results show that our design achieves 46.99 GFLOPS and 6.09 GFLOPS/W in terms of throughput and energy efficiency, respectively. © 2022 Association for Computing Machinery.",data reshaping; edge FPGAs; On-device training,Antennas; E-learning; Energy efficiency; Integrated circuit design; Memory architecture; Aerial vehicle; Channel-level; Convolution kernel; Data reshaping; Domain adaptions; Edge FPGA; On-device training; On-line adaptation; Personalizations; Real-time inference; Field programmable gate arrays (FPGA)
Implication of Optimizing NPU Dataflows on Neural Architecture Search for Mobile Devices,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140483186&doi=10.1145%2f3513085&partnerID=40&md5=07d71977284d69d14450da38eed800fe,"Recent advances in deep learning have made it possible to implement artificial intelligence in mobile devices. Many studies have put a lot of effort into developing lightweight deep learning models optimized for mobile devices. To overcome the performance limitations of manually designed deep learning models, an automated search algorithm, called neural architecture search (NAS), has been proposed. However, studies on the effect of hardware architecture of the mobile device on the performance of NAS have been less explored. In this article, we show the importance of optimizing a hardware architecture, namely, NPU dataflow, when searching for a more accurate yet fast deep learning model. To do so, we first implement an optimization framework, named FlowOptimizer, for generating a best possible NPU dataflow for a given deep learning operator. Then, we utilize this framework during the latency-aware NAS to find the model with the highest accuracy satisfying the latency constraint. As a result, we show that the searched model with FlowOptimizer outperforms the performance by 87.1% and 92.3% on average compared to the searched model with NVDLA and Eyeriss, respectively, with better accuracy on a proxy dataset. We also show that the searched model can be transferred to a larger model to classify a more complex image dataset, i.e., ImageNet, achieving 0.2%/5.4% higher Top-1/Top-5 accuracy compared to MobileNetV2-1.0 with 3.6× lower latency. © 2022 Association for Computing Machinery.",Dataflow optimization; neural architecture search; neural networks; neural processing unit,Classification (of information); Data flow analysis; Learning systems; Network architecture; Neural networks; Dataflow; Dataflow optimization; Hardware architecture; Learning models; Neural architecture search; Neural architectures; Neural processing unit; Neural-networks; Neural-processing; Processing units; Deep learning
Energy Efficient Boosting of GEMM Accelerators for DNN via Reuse,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141028994&doi=10.1145%2f3503469&partnerID=40&md5=c25faf9a6c01233582e2af0015583374,"Reuse-centric convolutional neural networks (CNN) acceleration speeds up CNN inference by reusing computations for similar neuron vectors in CNN's input layer or activation maps. This new paradigm of optimizations is, however, largely limited by the overheads in neuron vector similarity detection, an important step in reuse-centric CNN. This article presents an in-depth exploration of architectural support for reuse-centric CNN. It addresses some major limitations of the state-of-the-art design and proposes a novel hardware accelerator that improves neuron vector similarity detection and reduces the energy consumption of reuse-centric CNN inference. The accelerator is implemented to support a wide variety of neural network settings with a banked memory subsystem. Design exploration is performed through RTL simulation and synthesis on an FPGA platform. When integrated into Eyeriss, the accelerator can potentially provide improvements up to 7.75 in performance. Furthermore, it can reduce the energy used for similarity detection up to 95.46%, and it can accelerate the convolutional layer up to 3.63 compared to the software-based implementation running on the CPU. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",accelerator; deep neural networks; gemm; Reuse,Acceleration; Convolution; Convolutional neural networks; Energy efficiency; Energy utilization; Multilayer neural networks; Neurons; Simulation platform; Activation maps; Convolutional neural network; Energy efficient; Gemm; Input layers; Network inference; Reuse; Similarity detection; Speed up; Vector similarity; Deep neural networks
Low-energy Pipelined Hardware Design for Approximate Medium Filter,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160007451&doi=10.1145%2f3582005&partnerID=40&md5=9f534e2ff13cc66e432cfbc04e9eae11,"Image and video processing algorithms are currently crucial for many applications. Hardware implementation of these algorithms provides higher speed for large computation applications. Removing noise is often a typical pre-processing step to enhance the results of later analysis and processing. Median filter is a typical nonlinear filter that is very commonly used for impulse noise elimination in digital image processing. This article suggests a low-energy median filter hardware design for battery-based hardware applications. An approximate solution with high accuracy is investigated to speed up the filtering operation, reduce the area, and consume less power/energy. Pipelining and parallelism are used to optimize the speed and power of this technique. Non-pipelined, two different pipelined structures, and two parallel architectures versions are designed. The design versions are implemented first with a Virtex-5 LX110T FPGA and then using the UMC 130nm standard cell ASIC technology. The selection and the even-odd sorting-based median filters are also implemented for an equitable comparison with the standard median filtering techniques. The suggested non-pipelined median filter design enhances the throughput 35% more than the highest investigated state of the art. The pipelining enhances the throughput to more than twice its value. Additionally, the parallel architecture decreases the area and the consumed power by around 40%. The simulation results reveal that one of the suggested designs significantly decreases the area, with the same speed as the fastest design in the literature, without noticeably degrading the accuracy, and a significant decrease in energy consumption by about 60%.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Application specific integrated circuits (ASIC); field programmable gate arrays (FPGA); hardware accelerator; image denoising; median filter; parallel processing system,Application specific integrated circuits; CMOS integrated circuits; Energy utilization; Field programmable gate arrays (FPGA); Impulse noise; Integrated circuit design; Low power electronics; Median filters; Parallel architectures; Pipeline processing systems; Pipelines; Timing circuits; Video signal processing; Application specific integrated circuit; Application-specific integrated circuits; Field programmable gate array; Field programmables; Hardware accelerators; Hardware design; Lower energies; Median-Filter; Power; Programmable gate array; Image denoising
A Tensor Network based Decision Diagram for Representation of Quantum Circuits,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140285906&doi=10.1145%2f3514355&partnerID=40&md5=2f753c3321e83c0227d0727bb495516f,"Tensor networks have been successfully applied in simulation of quantum physical systems for decades. Recently, they have also been employed in classical simulation of quantum computing, in particular, random quantum circuits. This article proposes a decision diagram style data structure, called Tensor Decision Diagram (TDD), for more principled and convenient applications of tensor networks. This new data structure provides a compact and canonical representation for quantum circuits. By exploiting circuit partition, the TDD of a quantum circuit can be computed efficiently. Furthermore, we show that the operations of tensor networks essential in their applications (e.g., addition and contraction) can also be implemented efficiently in TDDs. A proof-of-concept implementation of TDDs is presented and its efficiency is evaluated on a set of benchmark quantum circuits. It is expected that TDDs will play an important role in various design automation tasks related to quantum circuits, including but not limited to equivalence checking, error detection, synthesis, simulation, and verification. © 2022 Association for Computing Machinery.",decision diagram; Tensor network,Computer aided design; Data structures; Quantum computers; Quantum efficiency; Quantum optics; Timing circuits; Canonical representations; Classical simulation; Compact representation; Decision diagram; Network-based; Physical systems; Proof of concept; Quantum circuit; Quantum Computing; Tensor network; Tensors
Magnetic Core TSV-Inductor Design and Optimization for On-chip DC-DC Converter,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140429816&doi=10.1145%2f3507700&partnerID=40&md5=6b6f518005601cbb9c8c42b88c5bfb67,"The conventional on-chip spiral inductor consumes a significant top-metal routing area, thereby preventing its popularity in many on-chip applications. Recently through-silicon-via– (TSV) based inductor (also known as a TSV-inductor) with a magnetic core has been proved to be a viable option for the on-chip DC-DC converter. The operating conditions of these inductors play a major role in maximizing the performance and efficiency of the DC-DC converter. However, there is a critical need to study the design and optimization details of magnetic core TSV-inductors with the unique three-dimensional structure embedding magnetic core. This article aims to provide a clear understanding of the modeling details of a magnetic core TSV-inductor and a design and optimization methodology to assist efficient inductor design. Moreover, a machine learning–assisted model combining physical details and artificial neural network is also proposed to extract the equivalent circuit to further facilitate DC-DC converter design. Experimental results show that the optimized TSV-inductor with the magnetic core and air-gap can achieve inductance density improvement of up to 7.7× and quality factor improvements of up to 1.6× for the same footprint compared with the TSV-inductor without a magnetic core. For on-chip DC-DC converter applications, the converter efficiency can be improved by up to 15.9% and 6.8% compared with the conventional spiral and TSV-inductor without magnetic core, respectively. © 2022 Association for Computing Machinery.",air gap; DC-DC converter; equivalent circuit; Magnetic core TSV-inductor,Computer aided design; Efficiency; Electric inductors; Equivalent circuits; Integrated circuit manufacture; Magnetic circuits; Magnetic cores; Magnetism; Neural networks; Three dimensional integrated circuits; Timing circuits; Air-gaps; Design and optimization; Inductor design; Inductor optimization; Magnetic core through-silicon-via–-inductor; On chips; On-chip applications; On-chip spiral inductors; Routing area; Silicon via; DC-DC converters
RASCv2: Enabling Remote Access to Side-Channels for Mission Critical and IoT Systems,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141027513&doi=10.1145%2f3524123&partnerID=40&md5=bb9354181aaa26f724d15131e2b8892f,"The Internet of Things (IoT) and smart devices are currently being deployed in systems such as autonomous vehicles and medical monitoring devices. The introduction of IoT devices into these systems enables network connectivity for data transfer, cloud support, and more, but can also lead to malware injection. Since many IoT devices operate in remote environments, it is also difficult to protect them from physical tampering. Conventional protection approaches rely on software. However, these can be circumvented by the moving target nature of malware or through hardware attacks. Alternatively, insertion of the internal monitoring circuits into IoT chips requires a design trade-off, balancing the requirements of the monitoring circuit and the main circuit. A very promising approach to detecting anomalous behavior in the IoT and other embedded systems is side-channel analysis. To date, however, this can be performed only before deployment due to the cost and size of side-channel setups (e.g., and oscilloscopes, probes) or by internal performance counters. Here, we introduce an external monitoring printed circuit board (PCB) named RASC to provide remote access to side-channels. RASC reduces the complete side-channel analysis system into two small PCBs (2 × 2 cm), providing the ability to monitor power and electromagnetic (EM) traces of the target device. Additionally, RASC can transmit data and/or alerts of anomalous activities detected to a remote host through Bluetooth. To demonstrate RASCs capabilities, we extract keys from encryption modules such as AES implemented on Arduino and FPGA boards. To illustrate RASC’s defensive capabilities, we also use it to perform malware detection. RASC’s success in power analysis is comparable to an oscilloscope/probe setup but is lightweight and two orders of magnitude cheaper. © 2022 Copyright held by the owner/author(s).",AES; buffer overflow; code injection; electromagnetic radiation; power; return-oriented program; Side-channel analysis,Data transfer; Economic and social effects; Electric network analysis; Embedded systems; Internet of things; Printed circuit boards; Side channel attack; AES; Buffer overflows; Code injection; Malwares; Monitoring circuit; Power; Remote access; Return-oriented program; Side-channel; Side-channel analysis; Electromagnetic waves
A Low-power Programmable Machine Learning Hardware Accelerator Design for Intelligent Edge Devices,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140439047&doi=10.1145%2f3531479&partnerID=40&md5=e55ba6d66c40d467e2f3502dc72ea1d9,"With the advent of the machine learning and IoT, many low-power edge devices, such as wearable devices with various sensors, are used for machine learning–based intelligent applications, such as healthcare or motion recognition. While these applications are becoming more complex to provide high-quality services, the performance of conventional low-power edge devices with extremely limited hardware resources is insufficient to support the emerging intelligent applications. We designed a hardware accelerator, called an Intelligence Boost Engine (IBE), for low-power smart edge devices to enable the real-time processing of emerging intelligent applications with energy efficiency and limited programmability. The measurement results confirm that the proposed IBE can reduce the power consumption of the edge node device by 75% and achieve performance improvement in processing the kernel operations of applications such as motion recognition by 69.9 times. © 2022 Association for Computing Machinery.",edge computing; hardware accelerator; internet of things; Machine learning; sensor fusion,Computing power; Energy efficiency; Internet of things; Machine learning; Motion estimation; Wearable sensors; Accelerator design; Edge computing; Hardware accelerators; Intelligent applications; Low Power; Machine-learning; Motion recognition; Performance; Programmable machines; Sensor fusion; Edge computing
DDAM: Data Distribution-Aware Mapping of CNNs on Processing-In-Memory Systems,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160007519&doi=10.1145%2f3576196&partnerID=40&md5=e3f68b08d623f12754bfa7ecf64ee460,"Convolution neural networks (CNNs) are widely used algorithms in image processing, natural language processing and many other fields. The large amount of memory access of CNNs is one of the major concerns in CNN accelerator designs that influences the performance and energy-efficiency. With fast and low-cost memory access, Processing-In-Memory (PIM) system is a feasible solution to alleviate the memory concern of CNNs. However, the distributed manner of data storing in PIM systems is in conflict with the large amount of data reuse of CNN layers. Nodes of PIM systems may need to share their data with each other before processing a CNN layer, leading to extra communication overhead. In this article, we propose DDAM to map CNNs onto PIM systems with the communication overhead reduced. Firstly, A data transfer strategy is proposed to deal with the data sharing requirement among PIM nodes by formulating a Traveling-Salesman-Problem (TSP). To improve data locality, a dynamic programming algorithm is proposed to partition the CNN and allocate a number of nodes to each part. Finally, an integer linear programming (ILP)-based mapping algorithm is proposed to map the partitioned CNN onto the PIM system. Experimental results show that compared to the baselines, DDAM can get a higher throughput of 2.0× with the energy cost reduced by 37% on average.  © 2023 Association for Computing Machinery.",Convolutional neural networks; Processing-In-Memory,Convolution; Convolutional neural networks; Cost reduction; Data transfer; Deep neural networks; Dynamic programming; Energy efficiency; Image processing; Integer programming; Memory architecture; Communication overheads; Convolution neural network; Convolutional neural network; Data distribution; Images processing; Language processing; Memory access; Natural languages; Processing-in-memory; Processing-In-Memory systems; Traveling salesman problem
"Enable Deep Learning on Mobile Devices: Methods, Systems, and Applications",2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127433864&doi=10.1145%2f3486618&partnerID=40&md5=d462a4b6656c65107bf2862ec786971b,"Deep neural networks (DNNs) have achieved unprecedented success in the field of artificial intelligence (AI), including computer vision, natural language processing, and speech recognition. However, their superior performance comes at the considerable cost of computational complexity, which greatly hinders their applications in many resource-constrained devices, such as mobile phones and Internet of Things (IoT) devices. Therefore, methods and techniques that are able to lift the efficiency bottleneck while preserving the high accuracy of DNNs are in great demand to enable numerous edge AI applications. This article provides an overview of efficient deep learning methods, systems, and applications. We start from introducing popular model compression methods, including pruning, factorization, quantization, as well as compact model design. To reduce the large design cost of these manual solutions, we discuss the AutoML framework for each of them, such as neural architecture search (NAS) and automated pruning and quantization. We then cover efficient on-device training to enable user customization based on the local data on mobile devices. Apart from general acceleration techniques, we also showcase several task-specific accelerations for point cloud, video, and natural language processing by exploiting their spatial sparsity and temporal/token redundancy. Finally, to support all these algorithmic advancements, we introduce the efficient deep learning system design from both software and hardware perspectives.  © 2022 Copyright held by the owner/author(s).",AutoML; Efficient deep learning; Model compression; Neural architecture search; TinyML,Internet of things; Natural language processing systems; Network architecture; Speech recognition; Automl; Efficient deep learning; High-accuracy; Method and technique; Model compression; Neural architecture search; Neural architectures; Performance; Resourceconstrained devices; Tinyml; Deep neural networks
Uncertainty Theory Based Partitioning for Cyber-Physical Systems with Uncertain Reliability Analysis,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127433583&doi=10.1145%2f3490177&partnerID=40&md5=c8c36c576ff1dddf3113a2f1381b7ee7,"Reasonable partitioning is a critical issue for cyber-physical system (CPS) design. Traditional CPS partitioning methods run in a determined context and depend on the parameter pre-estimations, but they ignore the uncertainty of parameters and hardly consider reliability. The state-of-the-art work proposed an uncertainty theory based CPS partitioning method, which includes parameter uncertainty and reliability analysis, but it only considers linear uncertainty distributions for variables and ignores the uncertainty of reliability. In this paper, we propose an uncertainty theory based CPS partitioning method with uncertain reliability analysis. We convert the uncertain objective and constraint into determined forms; such conversion methods can be applied to all forms of uncertain variables, not just for linear. By applying uncertain reliability analysis in the uncertainty model, we for the first time include the uncertainty of reliability into the CPS partitioning, where the reliability enhancement algorithm is proposed. We study the performance of the reliability obtained through uncertain reliability analysis, and experimental results show that the system reliability with uncertainty does not change significantly with the growth of task module numbers.  © 2021 Association for Computing Machinery.",Cyber-physical system (CPS) partitioning; Uncertain reliability analysis; Uncertainty theory,Cyber Physical System; Embedded systems; Reliability theory; Uncertainty analysis; Art work; Critical issues; Cybe-physical system  partitioning; Parameter uncertainty; Partitioning methods; Pre-estimation; State of the art; Uncertain reliability analyse; Uncertainty; Uncertainty theory; Reliability analysis
CeMux: Maximizing the Accuracy of Stochastic Mux Adders and an Application to Filter Design,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127392249&doi=10.1145%2f3491213&partnerID=40&md5=4722c6b93388c5bddd5dbcb4c89a8300,"Stochastic computing (SC) is a low-cost computational paradigm that has promising applications in digital filter design, image processing, and neural networks. Fundamental to these applications is the weighted addition operation, which is most often implemented by a multiplexer (mux) tree. Mux-based adders have very low area but typically require long bitstreams to reach practical accuracy thresholds when the number of summands is large. In this work, we first identify the main contributors to mux adder error. We then demonstrate with analysis and experiment that two new techniques, precise sampling and full correlation, can target and mitigate these error sources. Implementing these techniques in hardware leads to the design of CeMux (Correlation-enhanced Multiplexer), a stochastic mux adder that is significantly more accurate and uses much less area than traditional weighted adders. We compare CeMux to other SC and hybrid designs for an electrocardiogram filtering case study that employs a large digital filter. One major result is that CeMux is shown to be accurate even for large input sizes. CeMux's higher accuracy leads to a latency reduction of 4× to 16× over other designs. Furthermore, CeMux uses about 35% less area than existing designs, and we demonstrate that a small amount of accuracy can be traded for a further 50% reduction in area. Finally, we compare CeMux to a conventional binary design and we show that CeMux can achieve a 50% to 73% area reduction for similar power and latency as the conventional design but at a slightly higher level of error.  © 2022 Association for Computing Machinery.",Approximate computing; Digital filters; Electrocardiogram; Multiplexers; Stochastic computing; Weighted addition,Adders; Digital filters; Errors; Image processing; Multiplexing equipment; Stochastic systems; Approximate computing; Computational paradigm; Digital filter design; Filter designs; Images processing; Low-costs; Processing Network; Stochastic computing; Stochastics; Weighted addition; Electrocardiography
"Implementation, Characterization and Application of Path Changing Switch based Arbiter PUF on FPGA as a lightweight Security Primitive for IoT",2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127457108&doi=10.1145%2f3491212&partnerID=40&md5=152403f19ea68f3f99cb7a83f16850e1,"Secure authentication of any Internet-of-Things (IoT) device becomes the utmost necessity due to the lack of specifically designed IoT standards and intrinsic vulnerabilities with limited resources and heterogeneous technologies. Despite the suitability of arbiter physically unclonable function (APUF) among other PUF variants for the IoT applications, implementing it on field-programmable gate arrays (FPGAs) is challenging. This work presents the complete characterization of the path changing switch (PCS)1 based APUF on two different families of FPGA, like Spartan-3E (90 nm CMOS) and Artix-7 (28 nm CMOS). A comprehensive study of the existing tuning concept for programmable delay logic (PDL) based APUF implemented on FPGA is presented, leading to establishment of its practical infeasibility. We investigate the entropy, randomness properties of the PCS based APUF suitable for practical applications, and the effect of temperature variation signifying the adequate tolerance against environmental variation. The XOR composition of PCS based APUF is introduced to boost performance and security. The robustness of the PCS based APUF against machine learning based modeling attack is evaluated, showing similar characteristics as the conventional APUF. Experimental results validate the efficacy of PCS based APUF with a little hardware footprint removing the paucity of lightweight security primitive for IoT.  © 2021 Association for Computing Machinery.",Arbiter PUF (APUF); FPGA; IoT; Path changing switch (PCS); Physically unclonable function (PUF); Programmable delay line (PDL),CMOS integrated circuits; Cryptography; Field programmable gate arrays (FPGA); Temperature; Arbiter physically unclonable function; Lightweight securities; Path changing switch; Physically unclonable function; Physically unclonable functions; Programmable delay line; Programmable delay lines; Secure authentications; Security primitives; Internet of things
Energy Efficient Error Resilient Multiplier Using Low-power Compressors,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127382431&doi=10.1145%2f3488837&partnerID=40&md5=4d2313f80e4e32800e65181ca4224c35,"The approximate hardware design can save huge energy at the cost of errors incurred in the design. This article proposes the approximate algorithm for low-power compressors, utilized to build approximate multiplier with low energy and acceptable error profiles. This article presents two design approaches (DA1 and DA2) for higher bit size approximate multipliers. The proposed multiplier of DA1 have no propagation of carry signal from LSB to MSB, resulted in a very high-speed design. The increment in delay, power, and energy are not exponential with increment of multiplier size (n) for DA1 multiplier. It can be observed that the maximum combinations lie in the threshold Error Distance of 5% of the maximum value possible for any particular multiplier of size n. The proposed 4-bit DA1 multiplier consumes only 1.3 fJ of energy, which is 87.9%, 78%, 94%, 67.5%, and 58.9% less when compared to M1, M2, LxA, MxA, accurate designs respectively. The DA2 approach is recursive method, i.e., n-bit multiplier built with n/2-bit sub-multipliers. The proposed 8-bit multiplication has 92% energy savings with Mean Relative Error Distance (MRED) of 0.3 for the DA1 approach and at least 11% to 40% of energy savings with MRED of 0.08 for the DA2 approach. The proposed multipliers are employed in the image processing algorithm of DCT, and the quality is evaluated. The standard PSNR metric is 55 dB for less approximation and 35 dB for maximum approximation.  © 2021 Association for Computing Machinery.",Approximate compressors; Approximate hardware; Approximate multipliers; Discrete cosine transform (DCT); Energy efficient hardware,Compressors; Discrete cosine transforms; Errors; Image processing; Approximate compressor; Approximate hardware; Approximate multiplier; Discrete cosine transform; Energy; Energy efficient; Energy efficient hardware; Energy-savings; Error distance; Low Power; Energy efficiency
FPGAPRO: A Defense Framework Against Crosstalk-Induced Secret Leakage in FPGA,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127381100&doi=10.1145%2f3491214&partnerID=40&md5=fec078ddede3914a81537dd38fc31ef0,"With the emerging cloud-computing development, FPGAs are being integrated with cloud servers for higher performance. Recently, it has been explored to enable multiple users to share the hardware resources of a remote FPGA, i.e., to execute their own applications simultaneously. Although being a promising technique, multi-tenant FPGA unfortunately brings its unique security concerns. It has been demonstrated that the capacitive crosstalk between FPGA long-wires can be a side-channel to extract secret information, giving adversaries the opportunity to implement crosstalk-based side-channel attacks. Moreover, recent work reveals that medium-wires and multiplexers in configurable logic block (CLB) are also vulnerable to crosstalk-based information leakage.In this work, we propose FPGAPRO: a defense framework leveraging Placement, Routing, and Obfuscation to mitigate the secret leakage on FPGA components, including long-wires, medium-wires, and logic elements in CLB. As a user-friendly defense strategy, FPGAPRO focuses on protecting the security-sensitive instances meanwhile considering critical path delay for performance maintenance. As the proof-of-concept, the experimental result demonstrates that FPGAPRO can effectively reduce the crosstalk-caused side-channel leakage by 138 times. Besides, the performance analysis shows that this strategy prevents the maximum frequency from timing violation.  © 2021 Association for Computing Machinery.",Capacitive crosstalk; Cloud-FPGA; Long-wire; Placement isolation; Side-channel attack,Cloud computing; Computation theory; Computer circuits; Crosstalk; Field programmable gate arrays (FPGA); Network security; Side channel attack; Capacitive crosstalk; Cloud servers; Cloud-computing; Cloud-FPGA; Configurable Logic Blocks; Long-wire; Multiple user; Performance; Placement isolation; Side-channel attacks; Wire
Toward Taming the Overhead Monster for Data-flow Integrity,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127409032&doi=10.1145%2f3490176&partnerID=40&md5=5b41dd9a340981d5c4ceba089f34765a,"Data-Flow Integrity (DFI) is a well-known approach to effectively detecting a wide range of software attacks. However, its real-world application has been quite limited so far because of the prohibitive performance overhead it incurs. Moreover, the overhead is enormously difficult to overcome without substantially lowering the DFI criterion. In this work, an analysis is performed to understand the main factors contributing to the overhead. Accordingly, a hardware-assisted parallel approach is proposed to tackle the overhead challenge. Simulations on SPEC CPU 2006 benchmark show that the proposed approach can completely enforce the DFI defined in the original seminal work while reducing performance overhead by 4×, on average.  © 2021 Association for Computing Machinery.",Data-flow integrity; Processing in memory,Benchmarking; Data-flow integrity; Dataflow; Hardware-assisted; Performance; Processing-in-memory; Real-world; Software attacks; Data transfer
Ax-BxP: Approximate Blocked Computation for Precision-reconfigurable Deep Neural Network Acceleration,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127430727&doi=10.1145%2f3492733&partnerID=40&md5=2ab5ddd0fdac33f0470748bf3d1fb134,"Precision scaling has emerged as a popular technique to optimize the compute and storage requirements of Deep Neural Networks (DNNs). Efforts toward creating ultra-low-precision (sub-8-bit) DNNs for efficient inference suggest that the minimum precision required to achieve a given network-level accuracy varies considerably across networks, and even across layers within a network. This translates to a need to support variable precision computation in DNN hardware. Previous proposals for precision-reconfigurable hardware, such as bit-serial architectures, incur high overheads, significantly diminishing the benefits of lower precision. We propose Ax-BxP, a method for approximate blocked computation wherein each multiply-accumulate operation is performed block-wise (a block is a group of bits), facilitating re-configurability at the granularity of blocks. Further, approximations are introduced by only performing a subset of the required block-wise computations to realize precision re-configurability with high efficiency. We design a DNN accelerator that embodies approximate blocked computation and propose a method to determine a suitable approximation configuration for any given DNN. For the AlexNet, ResNet50, and MobileNetV2 DNNs, Ax-BxP achieves improvement in system energy and performance, respectively, over an 8-bit fixed-point (FxP8) baseline, with minimal loss (<1% on average) in classification accuracy. Further, by varying the approximation configurations at a finer granularity across layers and data-structures within a DNN, we achieve improvement in system energy and performance, respectively.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Approximate computing; Precision-reconfigurable DNN acceleration,Digital storage; Network layers; Reconfigurable architectures; Reconfigurable hardware; Approximate computing; Configurability; Lower precision; Network level; Precision-reconfigurable deep neural network acceleration; Reconfigurable; Scalings; Storage requirements; System energy; Systems performance; Deep neural networks
Hierarchical Scheduling of an SDF/L Graph onto Multiple Processors,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127383297&doi=10.1145%2f3489469&partnerID=40&md5=f1c1ddc6ef3940402397dbf57724a859,"Although dataflow models are known to thrive at exploiting task-level parallelism of an application, it is difficult to exploit the parallelism of data, represented well with loop structures, since these structures are not explicitly specified in existing dataflow models. SDF/L model overcomes this shortcoming by specifying the loop structures explicitly in a hierarchical fashion. We introduce a scheduling technique of an application represented by the SDF/L model onto heterogeneous processors. In the proposed method, we explore the mapping of tasks using an evolutionary meta-heuristic and schedule hierarchically in a bottom-up fashion, creating parallel loop schedules at lower levels first and then re-using them when constructing the schedule at a higher level. The efficiency of the proposed scheduling methodology is verified with benchmark examples and randomly generated SDF/L graphs.  © 2021 Association for Computing Machinery.",Data-parallel scheduling; Hierarchical scheduling; Mapping and scheduling,Data flow analysis; Heuristic methods; Mapping; Parallel processing systems; Response time (computer systems); Data flow modeling; Data parallel; Data-parallel scheduling; Hierarchical scheduling; Loop structure; Mapping and scheduling; Multiple processors; Parallel scheduling; Scheduling techniques; Task level parallelisms; Scheduling
Accelerating Graph Computations on 3D NoC-Enabled PIM Architectures,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160011344&doi=10.1145%2f3564290&partnerID=40&md5=dce4de99c4a6a3453cb2cc50eed0e37c,"Graph application workloads are dominated by random memory accesses with the poor locality. To tackle the irregular and sparse nature of computation, ReRAM-based Processing-in-Memory (PIM) architectures have been proposed recently. Most of these ReRAM architecture designs have focused on mapping graph computations into a set of multiply-and-accumulate (MAC) operations. ReRAMs also offer a key advantage in reducing memory latency between cores and memory by allowing for PIM. However, when implemented on a ReRAM-based manycore architecture, graph applications still pose two key challenges - significant storage requirements (particularly due to wasted zero cell storage), and significant amount of on-chip traffic. To tackle these two challenges, in this article, we propose the design of a 3D NoC-enabled ReRAM-based manycore architecture. Our proposed architecture incorporates a novel crossbar-aware node reordering to reduce ReRAM storage requirements. Secondly, its 3D NoC-enabled design reduces on-chip communication latency. Our architecture outperforms the state-of-the-art in ReRAM-based graph acceleration by up to 5× in performance while consuming up to 10.3× less energy for a range of graph inputs and workloads.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesProcessing-in-Memory; graph analytics; ReRAM; small world NoC; vertex reordering,Dynamic random access storage; Graph theory; Integrated circuit design; Memory architecture; RRAM; Additional key word and phrasesprocessing-in-memory; Graph-analytic; Key words; Many-core architecture; Processing-in-memory; Random memory access; Small world NoC; Small worlds; Storage requirements; Vertex reordering; Network-on-chip
Hardware-aware Quantization/Mapping Strategies for Compute-in-Memory Accelerators,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153854194&doi=10.1145%2f3569940&partnerID=40&md5=cd2506b5e4f8cd3e0d00d747d92d6474,"The emerging non-volatile memory (eNVM) based mixed-signal Compute-in-Memory (CIM) accelerators are of great interest in today's AI accelerators design due to their high energy efficiency. Various CIM architectures and circuit-level designs have been proposed, showing superior hardware performance for deep neural network (DNN) acceleration. However, hardware-aware quantization strategies for CIM-based accelerators are not systematically explored. Since there are a variety of design options for neural network mapping on CIM systems while improper strategies may narrow the circuit-level design space and further limit the hardware performance, it is important to make a comprehensive early-stage design space exploration and find appropriate quantization/mapping strategies to achieve better hardware performance. In this paper, we provide a joint algorithm-hardware analysis and compare the system-level hardware performance for various design options, including quantization algorithms, data representation methods and analog-to-digital converter (ADC) configurations. This work aims to propose guidelines for choosing more hardware-friendly design options for chip architects. According to our evaluation results for CIFAR-10/100 and ImageNet classification, the properly chosen quantization approach and optimal mapping strategy (dynamic fixed-point quantization + 2's complement representation/shifted unsigned INT representation + optimized precision ADC) could achieve ∼2 × energy efficiency and 1.2 ∼1.6 × throughput with 5%∼25% reduced area overhead, compared to naïve strategy (fixed-point quantization + differential pair number representation + full precision ADC).  © 2023 Association for Computing Machinery.",Compute-in-memory; deep neural network; design space exploration,Computer aided design; Digital storage; Energy efficiency; Mapping; Analog to digital converters; Circuit level design; Compute-in-memory; Design option; Design space exploration; Emerging non-volatile memory; Fixed points; Hardware performance; Mapping strategy; Quantisation; Deep neural networks
Distance-aware Approximate Nanophotonic Interconnect,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125358970&doi=10.1145%2f3484309&partnerID=40&md5=b13313d31c2f94c7c5905c1a55bdeead,"The energy consumption of manycore architectures is dominated by data movement, which calls for energy-efficient and high-bandwidth interconnects. To overcome the bandwidth limitation of electrical interconnects, integrated optics appear as a promising technology. However, it suffers from high power overhead related to low laser efficiency, which calls for the use of techniques and methods to improve its energy costs. Besides, approximate computing is emerging as an efficient method to reduce energy consumption and improve execution speed of embedded computing systems. It relies on allowing accuracy reduction on data at the cost of tolerable application output error. In this context, the work presented in this article exploits both features by defining approximate communications for error-tolerant applications. We propose a method to design realistic and scalable nanophotonic interconnect supporting approximate data transmission and power adaption according to the communication distance to improve the energy efficiency. For this purpose, the data can be sent by mixing low optical power signal and truncation for the Least Significant Bits (LSB) of the floating-point numbers, while the overall power is adapted according to the communication distance. We define two ranges of communications, short and long, which require only four power levels. This reduces area and power overhead to control the laser output power. A transmission model allows estimating the laser power according to the targeted BER and the number of truncated bits, while the optical network interface allows configuring, at runtime, the number of approximated and truncated bits and the laser output powers. We explore the energy efficiency provided by each communication scheme, and we investigate the error resilience of the benchmarks over several approximation and truncation schemes. The simulation results of ApproxBench applications show that, compared to an interconnect involving only robust communications, approximations in the optical transmission led to up to 53% laser power reduction with a limited degradation at the application level with less than 9% of output error. Finally, we show that our solution is scalable and leads to 10% reduction in the total energy consumption, 35× reduction in the laser driver size, and 10× reduction in the laser controller compared to state-of-the-art solution. © 2021 Association for Computing Machinery.",approximate communications; manycore architectures; network-on-chip; Photonic on silicon,Bandwidth; Computer architecture; Cost reduction; Data reduction; Digital arithmetic; Electric power utilization; Embedded systems; Energy efficiency; Errors; Green computing; Light transmission; Nanophotonics; Network architecture; Network-on-chip; Servers; % reductions; Approximate communication; Communication distance; Energy-consumption; Laser output power; Laser power; Many-core architecture; Output errors; Photonic on silicon; Power overhead; Silicon
Leveraging Automatic High-Level Synthesis Resource Sharing to Maximize Dynamical Voltage Overscaling with Error Control,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125328996&doi=10.1145%2f3473909&partnerID=40&md5=9bc899e3201aa3898630f5dea912e9fb,"Approximate Computing has emerged as an alternative way to further reduce the power consumption of integrated circuits (ICs) by trading off errors at the output with simpler, more efficient logic. So far the main approaches in approximate computing have been to simplify the hardware circuit by pruning the circuit until the maximum error threshold is met. One of the critical issues, though, is the training data used to prune the circuit. The output error can significantly exceed the maximum error if the final workload does not match the training data. Thus, most previous work typically assumes that training data matches with the workload data distribution. In this work, we present a method that dynamically overscales the supply voltage based on different workload distribution at runtime. This allows to adaptively select the supply voltage that leads to the largest power savings while ensuring that the error will never exceed the maximum error threshold. This approach also allows restoring of the original error-free circuit if no matching workload distribution is found. The proposed method also leverages the ability of High-Level Synthesis (HLS) to automatically generate circuits with different properties by setting different synthesis constraints to maximize the available timing slack and, hence, maximize the power savings. Experimental results show that our proposed method works very well, saving on average 47.08% of power as compared to the exact output circuit and 20.25% more than a traditional approximation method. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Approximate computing; dynamic error control; high-level synthesis; low-power; resource sharing; voltage overscaling,Computation theory; Computer circuits; Energy efficiency; High level synthesis; Integrated circuits; Low power electronics; Approximate computing; Dynamic error; Dynamic error control; Error control; High-level synthesis; Low Power; Maximum error; Resources sharing; Training data; Voltage overscaling; Errors
An Adaptive Application Framework with Customizable Quality Metrics,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125329871&doi=10.1145%2f3477428&partnerID=40&md5=fb688aaac7bcea2651c0d861df638803,"Many embedded environments require applications to produce outcomes under different, potentially changing, resource constraints. Relaxing application semantics through approximations enables trading off resource usage for outcome quality. Although quality is a highly subjective notion, previous work assumes given, fixed low-level quality metrics that often lack a strong correlation to a user's higher-level quality experience. Users may also change their minds with respect to their quality expectations depending on the resource budgets they are willing to dedicate to an execution. This motivates the need for an adaptive application framework where users provide execution budgets and a customized quality notion. This article presents a novel adaptive program graph representation that enables user-level, customizable quality based on basic quality aspects defined by application developers. Developers also define application configuration spaces, with possible customization to eliminate undesirable configurations. At runtime, the graph enables the dynamic selection of the configuration with maximal customized quality within the user-provided resource budget.An adaptive application framework based on our novel graph representation has been implemented on Android and Linux platforms and evaluated on eight benchmark programs, four with fully customizable quality. Using custom quality instead of the default quality, users may improve their subjective quality experience value by up to 3.59×, with 1.76× on average under different resource constraints. Developers are able to exploit their application structure knowledge to define configuration spaces that are on average 68.7% smaller as compared to existing, structure-oblivious approaches. The overhead of dynamic reconfiguration averages less than 1.84% of the overall application execution time. © 2021 Association for Computing Machinery.",Approximate computing; configuration management; QoS,Application programs; Benchmarking; Budget control; Computer operating systems; Dynamic models; Adaptive application; Application frameworks; Approximate computing; Configuration management; Configuration space; Customizable; Graph representation; Quality metrices; Resource budget; Resource Constraint; Semantics
A Compact High-Dimensional Yield Analysis Method using Low-Rank Tensor Approximation,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125327566&doi=10.1145%2f3483941&partnerID=40&md5=45e857938046c010b243a739f48b3fc1,"""Curse of dimensionality""has become the major challenge for existing high-sigma yield analysis methods. In this article, we develop a meta-model using Low-Rank Tensor Approximation (LRTA) to substitute expensive SPICE simulation. The polynomial degree of our LRTA model grows linearly with the circuit dimension. This makes it especially promising for high-dimensional circuit problems. Our LRTA meta-model is solved efficiently with a robust greedy algorithm and calibrated iteratively with a bootstrap-assisted adaptive sampling method. We also develop a novel global sensitivity analysis approach to generate a reduced LRTA meta-model which is more compact. It further accelerates the procedure of model calibration and yield estimation. Experiments on memory and analog circuits validate that the proposed LRTA method outperforms other state-of-the-art approaches in terms of accuracy and efficiency. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",failure probability; global sensitivity analysis; low-rank tensor approximation; meta-model; Process variation,Sensitivity analysis; SPICE; Tensors; Analysis method; Failure Probability; Global sensitivity analysis; High-dimensional; Higher-dimensional; Low-rank tensor approximations; Meta model; Metamodeling; Process Variation; Yield analysis; Iterative methods
MeF-RAM: A New Non-Volatile Cache Memory Based on Magneto-Electric FET,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125320649&doi=10.1145%2f3484222&partnerID=40&md5=39c0d2d334a1efe256e4de3ba26e5a45,"Magneto-Electric FET (MEFET) is a recently developed post-CMOS FET, which offers intriguing characteristics for high-speed and low-power design in both logic and memory applications. In this article, we present MeF-RAM, a non-volatile cache memory design based on 2-Transistor-1-MEFET (2T1M) memory bit-cell with separate read and write paths. We show that with proper co-design across MEFET device, memory cell circuit, and array architecture, MeF-RAM is a promising candidate for fast non-volatile memory (NVM). To evaluate its cache performance in the memory system, we, for the first time, build a device-to-architecture cross-layer evaluation framework to quantitatively analyze and benchmark the MeF-RAM design with other memory technologies, including both volatile memory (i.e., SRAM, eDRAM) and other popular non-volatile emerging memory (i.e., ReRAM, STT-MRAM, and SOT-MRAM). The experiment results for the PARSEC benchmark suite indicate that, as an L2 cache memory, MeF-RAM reduces Energy Area Latency (EAT) product on average by ∼98% and ∼70% compared with typical 6T-SRAM and 2T1R SOT-MRAM counterparts, respectively. © 2021 Association for Computing Machinery.",cache design; Magneto-electric FETs; memory bit-cell; non-volatile memory,Benchmarking; Cache memory; Computer aided design; Dynamic random access storage; Electric power supplies to apparatus; Integrated circuit design; Memory architecture; MRAM devices; Nonvolatile storage; Static random access storage; Bit cell; Cache design; High Speed; High-low; Magneto-electric FET; Magnetoelectrics; Memory bit-cell; Memory bits; Nonvolatile; Post-CMOS; Magnetic recording
ParTBC: Faster Estimation of Top-k Betweenness Centrality Vertices on GPU,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125343189&doi=10.1145%2f3486613&partnerID=40&md5=9faa8b38a67a9db6561f826dcabf6697,"Betweenness centrality (BC) is a popular centrality measure, based on shortest paths, used to quantify the importance of vertices in networks. It is used in a wide array of applications including social network analysis, community detection, clustering, biological network analysis, and several others. The state-of-the-art Brandes' algorithm for computing BC has time complexities of and for unweighted and weighted graphs, respectively. Brandes' algorithm has been successfully parallelized on multicore and manycore platforms. However, the computation of vertex BC continues to be time-consuming for large real-world graphs. Often, in practical applications, it suffices to identify the most important vertices in a network; that is, those having the highest BC values. Such applications demand only the top vertices in the network as per their BC values but do not demand their actual BC values. In such scenarios, not only is computing the BC of all the vertices unnecessary but also exact BC values need not be computed. In this work, we attempt to marry controlled approximations with parallelization to estimate the k-highest BC vertices faster, without having to compute the exact BC scores of the vertices. We present a host of techniques to determine the top-k vertices faster, with a small inaccuracy, by computing approximate BC scores of the vertices. Aiding our techniques is a novel vertex-renumbering scheme to make the graph layout more structured, which results in faster execution of parallel Brandes' algorithm on GPU. Our experimental results, on a suite of real-world and synthetic graphs, show that our best performing technique computes the top-k vertices with an average speedup of 2.5× compared to the exact parallel Brandes' algorithm on GPU, with an error of less than 6%. Our techniques also exhibit high precision and recall, both in excess of 94%. © 2021 Association for Computing Machinery.",approximate computing; Betweenness centrality; Brandes' algorithm; GPU; graph reordering; top-k,Graph theory; Approximate computing; Betweenness centrality; Brande' algorithm; Centrality measures; Fast estimation; Graph reordering; In networks; Real-world graphs; Short-path; Top-k; Graphics processing unit
Introduction to the Special Issue on Approximate Systems,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125313190&doi=10.1145%2f3488726&partnerID=40&md5=3baa8bcde3da5a84899367cae7355094,[No abstract available],,
Towards Fine-Grained Online Adaptive Approximation Control for Dense SLAM on Embedded GPUs,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125302200&doi=10.1145%2f3486612&partnerID=40&md5=a7826bee57ce3daf381e780de26ee146,"Dense SLAM is an important application on an embedded environment. However, embedded platforms usually fail to provide enough computation resources for high-accuracy real-time dense SLAM, even with high-parallelism architecture such as GPUs. To tackle this problem, one solution is to design proper approximation techniques for dense SLAM on embedded GPUs. In this work, we propose two novel approximation techniques, critical data identification and redundant branch elimination. We also analyze the error characteristics of the other two techniques - loop skipping and thread approximation. Then, we propose SLaPP, an online adaptive approximation controller, which aims to control the error to be under an acceptable threshold. The evaluation shows SLaPP can achieve 2.0× performance speedup and 30% energy saving on average compared to the case without approximation. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Approximate computing; dense SLAM; embedded GPUs; online adaptive control,Energy conservation; Adaptive Approximation; Adaptive Control; Approximate computing; Approximation techniques; Dense SLAM; Embedded environment; Embedded GPU; Embedded platforms; Fine grained; Online adaptive control; Program processors
Double-Shift: A Low-Power DNN Weights Storage and Access Framework based on Approximate Decomposition and Quantization,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125327369&doi=10.1145%2f3477047&partnerID=40&md5=e0173d2f5ce25b71b266ba07922dfeae,"One major challenge in deploying Deep Neural Network (DNN) in resource-constrained applications, such as edge nodes, mobile embedded systems, and IoT devices, is its high energy cost. The emerging approximate computing methodology can effectively reduce the energy consumption during the computing process in DNN. However, a recent study shows that the weight storage and access operations can dominate DNN's energy consumption due to the fact that the huge size of DNN weights must be stored in the high-energy-cost DRAM. In this paper, we propose Double-Shift, a low-power DNN weight storage and access framework, to solve this problem. Enabled by approximate decomposition and quantization, Double-Shift can reduce the data size of the weights effectively. By designing a novel weight storage allocation strategy, Double-Shift can boost the energy efficiency by trading the energy consuming weight storage and access operations for low-energy-cost computations. Our experimental results show that Double-Shift can reduce DNN weights to 3.96%-6.38% of the original size and achieve an energy saving of 86.47%-93.62%, while introducing a DNN classification error within 2%. © 2021 Association for Computing Machinery.",approximate computing; Deep neural network; matrix compression,Dynamic random access storage; Embedded systems; Energy efficiency; Energy utilization; Green computing; Low power electronics; Approximate computing; Edge nodes; Embedded-system; Energy-consumption; High-energy costs; Low Power; Matrix compression; Network weights; Quantisation; Storage and access; Deep neural networks
"Plasticine: A Cross-layer Approximation Methodology for Multi-kernel Applications through Minimally Biased, High-throughput, and Energy-efficient SIMD Soft Multiplier-divider",2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125333186&doi=10.1145%2f3486616&partnerID=40&md5=d1f601a28d8d9cf46ca569e067c610cc,"The rapid evolution of error-resilient programs intertwined with their quest for high throughput has motivated the use of Single Instruction, Multiple Data (SIMD) components in Field-Programmable Gate Arrays (FPGAs). Particularly, to exploit the error-resiliency of such applications, Cross-layer approximation paradigm has recently gained traction, the ultimate goal of which is to efficiently exploit approximation potentials across layers of abstraction. From circuit- to application-level, valuable studies have proposed various approximation techniques, albeit linked to four drawbacks: First, most of approximate multipliers and dividers operate only in SISD mode. Second, imprecise units are often substituted, merely in a single kernel of a multi-kernel application, with an end-to-end analysis in Quality of Results (QoR) and not in the gained performance. Third, state-of-the-art (SoA) strategies neglect the fact that each kernel contributes differently to the end-to-end QoR and performance metrics. Therefore, they lack in adopting a generic methodology for adjusting the approximation knobs to maximize performance gains for a user-defined quality constraint. Finally, multi-level techniques lack in being efficiently supported, from application-, to architecture-, to circuit-level, in a cohesive cross-layer hierarchy.In this article, we propose Plasticine, a cross-layer methodology for multi-kernel applications, which addresses the aforementioned challenges by efficiently utilizing the synergistic effects of a chain of techniques across layers of abstraction. To this end, we propose an application sensitivity analysis and a heuristic that tailor the precision at constituent kernels of the application by finding the most tolerable degree of approximations for each of consecutive kernels, while also satisfying the ultimate user-defined QoR. The chain of approximations is also effectively enabled in a cross-layer hierarchy, from application- to architecture- to circuit-level, through the plasticity of SIMD multiplier-dividers, each supporting dynamic precision variability along with hybrid functionality. The end-to-end evaluations of Plasticine on three multi-kernel applications employed in bio-signal processing, image processing, and moving object tracking for Unmanned Air Vehicles (UAV) demonstrate 41%-64%, 39%-62%, and 70%-86% improvements in area, latency, and Area-Delay-Product (ADP), respectively, over 32-bit fixed precision, with negligible loss in QoR. To springboard future research in reconfigurable and approximate computing communities, our implementations will be available and open-sourced at https://cfaed.tu-dresden.de/pd-downloads. © 2021 Association for Computing Machinery.",bio-signal processing; energy-efficiency; high-throughput; hybrid multiplier-divider; image processing; Mitchell's algorithm; Single instruction multiple data; unmanned air vehicles,Abstracting; Aircraft detection; Approximation algorithms; Energy efficiency; Field programmable gate arrays (FPGA); Image processing; Quality control; Biosignal processing; Cross layer; High-throughput; Hybrid multiplier; Hybrid multiplier-divider; Images processing; Mitchell algorithm; Multiple data; Single instruction multiple data; Unmanned air vehicles; Unmanned aerial vehicles (UAV)
CNNFlow: Memory-driven Data Flow Optimization for Convolutional Neural Networks,2022,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160005141&doi=10.1145%2f3577017&partnerID=40&md5=dbd6891ce04a6a39e7c804df403a5c80,"Convolution Neural Networks (CNNs) are widely deployed in computer vision applications. The datasets are large, and the data reuse across different parts is heavily interleaved. Given that memory access (SRAM and especially DRAM) is more expensive in both performance and energy than computation, maximizing data reuse to reduce data movement across the memory hierarchy is critical to improving execution efficiency. This is even more important for the common use case of CNNs on mobile devices where computing/memory resources are limited. We propose CNNFlow, a memory-driven dataflow optimization framework to automatically schedule CNN computation on a given CNN architecture to maximize data reuse at each level of the memory hierarchy. We provide a mathematical calculation for data reuses in terms of parameters including loop ordering, blocking, and memory-bank allocation for tensors in CNN. We then present a series of techniques that help prune the large search space and reduce the cost of the exploration. This provides, for the first time, an exact and practical search algorithm for optimal solutions to minimize memory access cost for CNN. The efficacy is demonstrated for two widely used CNN algorithms: AlexNet and VGG16 with 5 and 13 convolution layers, respectively. CNNFlow finds the optimal solution for each layer within tens of minutes of compute time. Its solution requires about 20% fewer DRAM accesses and 40%-80% fewer SRAM accesses compared to state-of-the-art algorithms in the literature.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Accelerator; Convolutional Neural Network; data scheduling; memory utilization; software-hardware co-design,Convolution; Convolutional codes; Data reduction; Field programmable gate arrays (FPGA); Hardware-software codesign; Integrated circuit design; Large dataset; Memory architecture; Optimal systems; Static random access storage; Convolution neural network; Convolutional neural network; Data reuse; Data scheduling; Dataflow; Memory access; Memory hierarchy; Memory utilization; Optimal solutions; Software/hardware co designs; Dynamic random access storage
A Robust Modulus-Based Matrix Splitting Iteration Method for Mixed-Cell-Height Circuit Legalization,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124146101&doi=10.1145%2f3423326&partnerID=40&md5=86b440ef4ff4d66586b9cffbcd3b592a,"Modern circuits often contain standard cells of different row heights to meet various design requirements. Taller cells give larger drive strengths and higher speed at the cost of larger areas and power. Multi-row height standard cells incur challenging issues for layout designs, especially the mixed-cell-height legalization problem with heterogeneous cell structures. Honoring the good cell positions from global placement, we present in this article a robust modulus-based matrix splitting iteration method (RMMSIM) to solve the mixed-cell-height legalization problem. Fixing the cell ordering from global placement and relaxing the right-boundary constraints, our proposed method first converts the problem into an equivalent linear complementarity problem (LCP), and then properly splits the matrices in the LCP so that the RMMSIM can solve the LCP optimally. The RMMSIM effectively explores the sparse characteristic of a circuit, and takes only linear time per iteration; as a result, it can solve the QP very efficiently. Finally, an allocation scheme for illegal cells is used to align such cells to placement sites on rows and fix the placement of out-of-right-boundary cells, if any. Experimental results show the effectiveness and efficiency of our proposed algorithm. In addition, the RMMSIM convergence and optimality are theoretically proved and empirically validated. In particular, this article provides a new RMMSIM formulation for various optimization problems that require solving large-scale convex quadratic programming problems efficiently. © 2020 Association for Computing Machinery.",legalization; linear complementarity problem; modulus-based matrix splitting iteration method; multi-row height cell; Physical design; placement; quadratic programming,Authentication; Cytology; Iterative methods; Matrix algebra; Quadratic programming; Timing circuits; Cell height; Global placements; Legalization; Linear complementarity problems; Mixed cells; Modulus-based matrix splitting iteration methods; Multi-row height cell; Physical design; Placement; Standard-cell; Cells
"A Delay-Adjustable, Self-Testable Flip-Flop for Soft-Error Tolerability and Delay-Fault Testability",2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115971903&doi=10.1145%2f3462171&partnerID=40&md5=ed470e08407936b90e14d44a708955f0,"As the demand of safety-critical applications (e.g., automobile electronics) increases, various radiation-hardened flip-flops are proposed for enhancing design reliability. Among all flip-flops, Delay-Adjustable D-Flip-Flop (DAD-FF) is specialized in arbitrarily adjusting delay in the design to tolerate soft errors induced by different energy levels. However, due to a lack of testability on DAD-FF, its soft-error tolerability is not yet verified, leading to uncertain design reliability. Therefore, this work proposes Delay-Adjustable, Self-Testable Flip-Flop (DAST-FF), built on top of DAD-FF with two extra MUXs (one for scan test and the other for latching-delay verification) to achieve both soft-error tolerability and testability. Meanwhile, a built-in self-test method is also developed on DAST-FFs to verify the cumulative latching delay before operation. The experimental result shows that for a design with 8,802 DAST-FFs, the built-in self-test method only takes 946 ns to ensure the soft-error tolerability. As to the testability, the enhanced scan capability can be enabled by inserting one extra transmission gate into DAST-FF with only 4.5 area overhead.  © 2021 Association for Computing Machinery.",Built-in self test; Enhanced scan capability; Radiation hardening; Scan flip-flop; Soft errors,Built-in self test; Error correction; Flip flop circuits; Hardening; Integrated circuit testing; Build in self tests; Builtin self tests (BIST); D flip flops; Design-reliability; Enhanced scan; Enhanced scan capability; Scan flip-flops; Self-testable; Soft error; Testability; Radiation hardening
High-throughput Near-Memory Processing on CNNs with 3D HBM-like Memory,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116630258&doi=10.1145%2f3460971&partnerID=40&md5=607bf6fa1c7ac546acbef66470238bf9,"This article discusses the high-performance near-memory neural network (NN) accelerator architecture utilizing the logic die in three-dimensional (3D) High Bandwidth Memory- (HBM) like memory. As most of the previously reported 3D memory-based near-memory NN accelerator designs used the Hybrid Memory Cube (HMC) memory, we first focus on identifying the key differences between HBM and HMC in terms of near-memory NN accelerator design. One of the major differences between the two 3D memories is that HBM has the centralized through-silicon-via (TSV) channels while HMC has distributed TSV channels for separate vaults. Based on the observation, we introduce the Round-Robin Data Fetching and Groupwise Broadcast schemes to exploit the centralized TSV channels for improvement of the data feeding rate for the processing elements. Using synthesized designs in a 28-nm CMOS technology, performance and energy consumption of the proposed architectures with various dataflow models are evaluated. Experimental results show that the proposed schemes reduce the runtime by 16.4-39.3% on average and the energy consumption by 2.1-5.1% on average compared to conventional data fetching schemes.  © 2021 Association for Computing Machinery.",HBM; Neural network accelerator,Data flow analysis; Electronics packaging; Energy utilization; Integrated circuit manufacture; Memory architecture; Network architecture; 3D memory; Accelerator design; Bandwidth memory; Centralised; High bandwidth; High bandwidth memory-; Hybrid memory; Memory likes; Neural network accelerator; Neural-networks; Three dimensional integrated circuits
A Framework for Validation of Synthesized MicroElectrode Dot Array Actuations for Digital Microfluidic Biochips,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122610241&doi=10.1145%2f3460437&partnerID=40&md5=af83a90299786c09c62f85c892e5765b,"Digital Microfluidics is an emerging technology for automating laboratory procedures in biochemistry. With more and more complex biochemical protocols getting mapped to biochip devices and microfluidics receiving a wide adoption, it is becoming indispensable to develop automated tools and synthesis platforms that can enable a smooth transformation from complex cumbersome benchtop laboratory procedures to biochip execution. Given an informal/semi-formal assay description and a target microfluidic grid architecture on which the assay has to be implemented, a synthesis tool typically translates the high-level assay operations to low-level actuation sequences that can drive the assay realization on the grid. With more and more complex biochemical assay protocols being taken up for synthesis and biochips supporting a wider variety of operations (e.g., MicroElectrode Dot Arrays (MEDAs)), the task of assay synthesis is getting intricately complex. Errors in the synthesized assay descriptions may have undesirable consequences in assay operations, leading to unacceptable outcomes after execution on the biochips. In this work, we focus on the challenge of examining the correctness of synthesized protocol descriptions, before they are taken up for realization on a microfluidic biochip. In particular, we take up a protocol description synthesized for a MEDA biochip and adopt a formal analysis method to derive correctness proofs or a violation thereof, pointing to the exact operation in the erroneous translation. We present experimental results on a few bioassay protocols and show the utility of our framework for verifiable protocol synthesis.  © 2021 Association for Computing Machinery.",Microfluidics; Preconditions; Verification,Biochips; Computer aided design; Microelectrodes; Automated synthesis; Automated tools; Digital microfluidic biochips; Dot array; Emerging technologies; Laboratory procedures; Precondition; Protocol description; Smooth transformation; Synthesised; Digital microfluidics
FTT-NAS: Discovering Fault-tolerant Convolutional Neural Architecture,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122614956&doi=10.1145%2f3460288&partnerID=40&md5=c7c1a225c332b9b33f4d8f8ddc6922ef,"With the fast evolvement of embedded deep-learning computing systems, applications powered by deep learning are moving from the cloud to the edge. When deploying neural networks (NNs) onto the devices under complex environments, there are various types of possible faults: soft errors caused by cosmic radiation and radioactive impurities, voltage instability, aging, temperature variations, malicious attackers, and so on. Thus, the safety risk of deploying NNs is now drawing much attention. In this article, after the analysis of the possible faults in various types of NN accelerators, we formalize and implement various fault models from the algorithmic perspective. We propose Fault-Tolerant Neural Architecture Search (FT-NAS) to automatically discover convolutional neural network (CNN) architectures that are reliable to various faults in nowadays devices. Then, we incorporate fault-tolerant training (FTT) in the search process to achieve better results, which is referred to as FTT-NAS. Experiments on CIFAR-10 show that the discovered architectures outperform other manually designed baseline architectures significantly, with comparable or fewer floating-point operations (FLOPs) and parameters. Specifically, with the same fault settings, F-FTT-Net discovered under the feature fault model achieves an accuracy of 86.2% (VS. 68.1% achieved by MobileNet-V2), and W-FTT-Net discovered under the weight fault model achieves an accuracy of 69.6% (VS. 60.8% achieved by ResNet-18). By inspecting the discovered architectures, we find that the operation primitives, the weight quantization range, the capacity of the model, and the connection pattern have influences on the fault resilience capability of NN models.  © 2021 Association for Computing Machinery.",Fault tolerance; Neural architecture search; Neural networks,Convolution; Convolutional neural networks; Cosmology; Deep learning; Digital arithmetic; Embedded systems; Network architecture; Radiation hardening; Complex environments; Computing system; Fault model; Fault-tolerant; Neural architecture search; Neural architectures; Neural-networks; Radioactive impurities; Soft error; System applications; Fault tolerance
A Runtime Reconfigurable Design of Compute-in-Memory Based Hardware Accelerator for Deep Learning Inference,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122624236&doi=10.1145%2f3460436&partnerID=40&md5=76ed7c8741a382ed49772af4cbbd0b10,"Compute-in-memory (CIM) is an attractive solution to address the ""memory wall""challenges for the extensive computation in deep learning hardware accelerators. For custom ASIC design, a specific chip instance is restricted to a specific network during runtime. However, the development cycle of the hardware is normally far behind the emergence of new algorithms. Although some of the reported CIM-based architectures can adapt to different deep neural network (DNN) models, few details about the dataflow or control were disclosed to enable such an assumption. Instruction set architecture (ISA) could support high flexibility, but its complexity would be an obstacle to efficiency. In this article, a runtime reconfigurable design methodology of CIM-based accelerators is proposed to support a class of convolutional neural networks running on one prefabricated chip instance with ASIC-like efficiency. First, several design aspects are investigated: (1) the reconfigurable weight mapping method; (2) the input side of data transmission, mainly about the weight reloading; and (3) the output side of data processing, mainly about the reconfigurable accumulation. Then, a system-level performance benchmark is performed for the inference of different DNN models, such as VGG-8 on a CIFAR-10 dataset and AlexNet GoogLeNet, ResNet-18, and DenseNet-121 on an ImageNet dataset to measure the trade-offs between runtime reconfigurability, chip area, memory utilization, throughput, and energy efficiency.  © 2021 Association for Computing Machinery.",Compute-in-memory; Convolutional neural network; Hardware accelerator; Reconfigurable architecture,Benchmarking; Computer hardware; Convolution; Convolutional neural networks; Data flow analysis; Data handling; Deep neural networks; Economic and social effects; Energy efficiency; Integrated circuit design; Memory architecture; Network architecture; ASIC design; Attractive solutions; Compute-in-memory; Convolutional neural network; Hardware accelerators; Memory wall; Neural network model; Reconfigurable; Reconfigurable designs; Run-time reconfigurable; Reconfigurable architectures
Voltage-Based Covert Channels Using FPGAs,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122637278&doi=10.1145%2f3460229&partnerID=40&md5=bd9ebf6e053f75367e6d5257fe926dd1,"Field Programmable Gate Arrays (FPGAs) are increasingly used in cloud applications and being integrated into Systems-on-Chip. For these systems, various side-channel attacks on cryptographic implementations have been reported, motivating one to apply proper countermeasures. Beyond cryptographic implementations, maliciously introduced covert channel receivers and transmitters can allow one to exfiltrate other secret information from the FPGA. In this article, we present a fast covert channel on FPGAs, which exploits the on-chip power distribution network. This can be achieved without any logical connection between the transmitter and receiver blocks. Compared to a recently published covert channel with an estimated 4.8 Mbit/s transmission speed, we show 8 Mbit/s transmission and reduced errors from around 3% to less than 0.003%. Furthermore, we demonstrate proper transmissions of word-size messages and test the channel in the presence of noise generated from other residing tenants' modules in the FPGA. When we place and operate other co-tenant modules that require 85% of the total FPGA area, the error rate increases to 0.02%, depending on the platform and setup. This error rate is still reasonably low for a covert channel. Overall, the transmitter and receiver work with less than 3-5% FPGA LUT resources together. We also show the feasibility of other types of covert channel transmitters, in the form of synchronous circuits within the FPGA.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Accelerator; Covert-channel; FPGA; Hardware; Multi-tenant; On-chip; PDN; Power distribution network; Remote; Side-channel; SoC; Software; Trojan,Electric network analysis; Errors; Field programmable gate arrays (FPGA); Hardware security; Malware; Programmable logic controllers; Side channel attack; Transmitters; Accelerator; Covert channels; Hardware; Multi tenants; On chips; PDN; Power distribution network; Remote; Side-channel; Software; Trojans; System-on-chip
A Conditionally Chaotic Physically Unclonable Function Design Framework with High Reliability,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122625228&doi=10.1145%2f3460004&partnerID=40&md5=154a771f215f16c0d2806b4f9ca7d93b,"Physically Unclonable Function (PUF) circuits are promising low-overhead hardware security primitives, but are often gravely susceptible to machine learning-based modeling attacks. Recently, chaotic PUF circuits have been proposed that show greater robustness to modeling attacks. However, they often suffer from unacceptable overhead, and their analog components are susceptible to low reliability. In this article, we propose the concept of a conditionally chaotic PUF that enhances the reliability of the analog components of a chaotic PUF circuit to a level at par with their digital counterparts. A conditionally chaotic PUF has two modes of operation: bistable and chaotic, and switching between these two modes is conveniently achieved by setting a mode-control bit (at a secret position) in an applied input challenge. We exemplify our PUF design framework for two different PUF variants - the CMOS Arbiter PUF and a previously proposed hybrid CMOS-memristor PUF, combined with a hardware realization of the Lorenz system as the chaotic component. Through detailed circuit simulation and modeling attack experiments, we demonstrate that the proposed PUF circuits are highly robust to modeling and cryptanalytic attacks, without degrading the reliability of the original PUF that was combined with the chaotic circuit, and incurs acceptable hardware footprint.  © 2021 Association for Computing Machinery.",Chaotic PUF; hardware security; Lorenz chaotic system; machine learning-based modeling attack; memristor; physically unclonable function (PUF),Chaotic systems; CMOS integrated circuits; Cryptography; Hardware security; Integrated circuit design; Machine learning; Memristors; Reliability; Based modelling; Chaotic physically unclonable function; Chaotics; Function circuit; Lorenz chaotic system; Machine learning-based modeling attack; Memristor; Physically unclonable function; Physically unclonable functions; Circuit simulation
A Variation-aware Hold Time Fixing Methodology for Single Flux Quantum Logic Circuits,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122611384&doi=10.1145%2f3460289&partnerID=40&md5=9783759d2a1d23a3ab6dcc5f805182fa,"Single flux quantum (SFQ) logic is a promising technology to replace complementary metal-oxide-semiconductor logic for future exa-scale supercomputing but requires the development of reliable EDA tools that are tailored to the unique characteristics of SFQ circuits, including the need for active splitters to support fanout and clocked logic gates. This article is the first work to present a physical design methodology for inserting hold buffers in SFQ circuits. Our approach is variation-aware, uses common path pessimism removal and incremental placement to minimize the overhead of timing fixes, and can trade off layout area and timing yield. Compared to a previously proposed approach using fixed hold time margins, Monte Carlo simulations show that, averaging across 10 ISCAS'85 benchmark circuits, our proposed method can reduce the number of inserted hold buffers by 8.4% with a 6.2% improvement in timing yield and by 21.9% with a 1.7% improvement in timing yield.  © 2021 Association for Computing Machinery.",Clock tree synthesis; Common path pessimism removal (CPPR); Gaussian distribution; Hold time; Legalization; Monte Carlo simulation; Placement; Single-flux quantum (SFQ); Timing uncertainty,Clocks; CMOS integrated circuits; Computer aided design; Computer circuits; Economic and social effects; Intelligent systems; Logic circuits; Metals; MOS devices; Oxide semiconductors; Quantum theory; Timing circuits; Clock tree synthesis; Common path pessimism removal; Common-path; Hold time; Monte Carlo's simulation; Placement; Single flux quantum; Single-flux quantum; Timing uncertainty; Uncertainty; Monte Carlo methods
An Energy-Efficient Inference Method in Convolutional Neural Networks Based on Dynamic Adjustment of the Pruning Level,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122642096&doi=10.1145%2f3460972&partnerID=40&md5=9ab20c1b93bb08e9936e39966e68cc80,"In this article, we present a low-energy inference method for convolutional neural networks in image classification applications. The lower energy consumption is achieved by using a highly pruned (lower-energy) network if the resulting network can provide a correct output. More specifically, the proposed inference method makes use of two pruned neural networks (NNs), namely mildly and aggressively pruned networks, which are both designed offline. In the system, a third NN makes use of the input data for the online selection of the appropriate pruned network. The third network, for its feature extraction, employs the same convolutional layers as those of the aggressively pruned NN, thereby reducing the overhead of the online management. There is some accuracy loss induced by the proposed method where, for a given level of accuracy, the energy gain of the proposed method is considerably larger than the case of employing any one pruning level. The proposed method is independent of both the pruning method and the network architecture. The efficacy of the proposed inference method is assessed on Eyeriss hardware accelerator platform for some of the state-of-the-art NN architectures. Our studies show that this method may provide, on average, 70% energy reduction compared to the original NN at the cost of about 3% accuracy loss on the CIFAR-10 dataset.  © 2021 Association for Computing Machinery.",DNN; energy efficiency; image classification; online management; Pruned neural network,Convolution; Convolutional neural networks; Cost reduction; Energy efficiency; Energy utilization; Low power electronics; Network architecture; Accuracy loss; Convolutional neural network; DNN; Energy efficient; Images classification; Inference methods; Lower energies; Neural-networks; Online management; Pruned neural networks; Image classification
Synthesizing Brain-network-inspired Interconnections for Large-scale Network-on-chips,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136460597&doi=10.1145%2f3480961&partnerID=40&md5=cc4554f0472c987e70d8cfaba7925d13,"Brain network is a large-scale complex network with scale-free, small-world, and modularity properties, which largely supports this high-efficiency massive system. In this article, we propose to synthesize brain-network-inspired interconnections for large-scale network-on-chips. First, we propose a method to generate brain-network-inspired topologies with limited scale-free and power-law small-world properties, which have a low total link length and extremely low average hop count approximately proportional to the logarithm of the network size. In addition, given the large-scale applications, considering the modularity of the brain-network-inspired topologies, we present an application mapping method, including task mapping and deterministic deadlock-free routing, to minimize the power consumption and hop count. Finally, a cycle-accurate simulator is used to validate the architecture performance with different synthetic traffic patterns and large-scale test cases, including real-world communication networks for the graph processing application. Experiments show that, compared with other topologies and methods, the brain-network-inspired network-on-chips (NoCs) generated by the proposed method present significantly lower average hop count and lower average latency. Especially in graph processing applications with a power-law and tightly coupled inter-core communication, the brain-network-inspired NoC has up to 70% lower average hop count and 75% lower average latency than mesh-based NoCs. © 2021 Association for Computing Machinery.",brain network; modularity; Network-on-chip; scale-free; small-world; topology generation,Brain mapping; Complex networks; Distributed computer systems; Interconnection networks (circuit switching); Mapping; Topology; Brain networks; Hop count; Large-scale network; Lower average; Modularity; Networks on chips; Property; Scale-free; Small worlds; Topology generation; Network-on-chip
Fault Injection Attack Emulation Framework for Early Evaluation of IC Designs,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139207270&doi=10.1145%2f3480962&partnerID=40&md5=91d5dbd23d1fa28c311413955787ea5a,"Fault injection attack (FIA) has become a serious threat to the confidentiality and fault tolerance of integrated circuits (ICs). Circuit designers need an effective method to evaluate the countermeasures of the IC designs against the FIAs at the design stage. To address the need, this article, based on FPGA emulation, proposes an in-circuit early evaluation framework, in which FIAs are emulated with parameterized fault models. To mimic FIAs, an efficient scan approach is proposed to inject faults at any time at any circuit nodes, while both the time and area overhead of fault injection are reduced. After the circuit design under test (CUT) is submitted to the framework, the scan chains insertion, fault generation, and fault injection are executed automatically, and the evaluation result of the CUT is generated, making the evaluation a transparent process to the designers. Based on the framework, the confidentiality and fault-tolerance evaluations are demonstrated with an information-based evaluation approach. Experiment results on a set of ISCAS89 benchmark circuits show that on average, our approach reduces the area overhead by 41.08% compared with the full scan approach and by over 20.00% compared with existing approaches. The confidentiality evaluation experiments on AES-128 and DES-56 and the fault-tolerance evaluation experiments on two CNN circuits, a RISC-V core, a Cordic core, and the float point arithmetic units show the effectiveness of the proposed framework. © 2021 Association for Computing Machinery.",confidentiality; evaluation; fault injection attack; fault-tolerance; FPGA emulation,Cost reduction; Field programmable gate arrays (FPGA); Integrated circuit design; Integrated circuit manufacture; Software testing; Timing circuits; Area overhead; Circuit designs; Confidentiality; Design under tests; Early evaluation; Evaluation; Fault injection; Fault injection attacks; FPGA emulation; Tolerance evaluations; Fault tolerance
Component Fault Diagnosability of Hierarchical Cubic Networks,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160020438&doi=10.1145%2f3577018&partnerID=40&md5=cd2dd3d304c4c51121b5dbb7737e0bd4,"The fault diagnosability of a network indicates the self-diagnosis ability of the network, thus it is an important measure of robustness of the network. As a neoteric feature for measuring fault diagnosability, the r-component diagnosability ctr(G) of a network G imposes the restriction that the number of components is at least r in the remaining network of G by deleting faulty set X, which enhances the diagnosability of G. In this article, we establish the r-component diagnosability for n-dimensional hierarchical cubic network HCNn, and we show that, under both PMC model and MM∗model, the r-component diagnosability of HCNn is rn-½(r-1)r+1 for n≥ 2 and 1≤ r≤ n-1. Moreover, we introduce the concepts of 0-PMC subgraph and 0-MM∗subgraph of HCNn. Then, we make use of 0-PMC subgraph and 0-MM∗subgraph of HCNn to design two algorithms under PMC model and MM∗model, respectively, which are practical and efficient for component fault diagnosis of HCNn. Besides, we compare the r-component diagnosability of HCNn with the extra conditional diagnosability, diagnosability, good-neighbor diagnosability, pessimistic diagnosability, and conditional diagnosability, and we verify that the r-component diagnosability of HCNn is higher than the other types of diagnosability.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",component diagnosability; Fault diagnosis; hierarchical cubic network; reliability,Fault detection; Component diagnosability; Component faults; Conditional diagnosability; Diagnosability; Fault diagnosability; Faults diagnosis; Hierarchical cubic network; MM* model; PMC model; Subgraphs; Failure analysis
A Novel Hybrid Cache Coherence with Global Snooping for Many-core Architectures,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139215066&doi=10.1145%2f3462775&partnerID=40&md5=acacbcd09eeb8058a8bb90c274360804,"Cache coherence ensures correctness of cached data in multi-core processors. Traditional implementations of existing protocols make them unscalable for many core architectures. While snoopy coherence requires unscalable ordered networks, directory coherence is weighed down by high area and energy overheads. In this work, we propose Wireless-enabled Share-aware Hybrid (WiSH) to provide scalable coherence in many core processors. WiSH implements a novel Snoopy over Directory protocol using on-chip wireless links and hierarchical, clustered Network-on-Chip to achieve low-overhead and highly efficient coherence. A local directory protocol maintains coherence within a cluster of cores, while coherence among such clusters is achieved through global snoopy protocol. The ordered network for global snooping is provided through low-latency and low-energy broadcast wireless links. The overheads are further reduced through share-aware cache segmentation to eliminate coherence for private blocks. Evaluations show that WiSH reduces traffic by and runtime by, while requiring smaller storage and lower energy as compared to existing hierarchical and hybrid coherence protocols. Owing to its modularity, WiSH provides highly efficient and scalable coherence for many core processors.  © 2021 Association for Computing Machinery.",Cache coherence; hybrid protocol; many core processors; mm-wave wireless links,Cache memory; Computer architecture; Internet protocols; Millimeter waves; Network architecture; Network-on-chip; Cache Coherence; Directory protocol; Hybrid caches; Hybrid protocols; Lower energies; Many-core architecture; Many-core processors; Mm waves; Mm-wave wireless link; Wireless link; Low power electronics
A Design Methodology for Energy-Aware Processing in Unmanned Aerial Vehicles,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138378773&doi=10.1145%2f3470451&partnerID=40&md5=648e36a6478e80fcd9f605ab5db586bb,"Unmanned Aerial Vehicles (UAVs) have rapidly become popular for monitoring, delivery, and actuation in many application domains such as environmental management, disaster mitigation, homeland security, energy, transportation, and manufacturing. However, the UAV perception and navigation intelligence (PNI) designs are still in their infancy and demand fundamental performance and energy optimizations to be eligible for mass adoption. In this article, we present a generalizable three-stage optimization framework for PNI systems that (i) abstracts the high-level programs representing the perception, mining, processing, and decision making of UAVs into complex weighted networks tracking the interdependencies between universal low-level intermediate representations; (ii) exploits a differential geometry approach to schedule and map the discovered PNI tasks onto an underlying manycore architecture. To mine the complexity of optimal parallelization of perception and decision modules in UAVs, this proposed design methodology relies on an Ollivier-Ricci curvature-based load-balancing strategy that detects the parallel communities of the PNI applications for maximum parallel execution, while minimizing the inter-core communication; and (iii) relies on an energy-aware mapping scheme to minimize the energy dissipation when assigning the communities onto tile-based networks-on-chip. We validate this approach based on various drone PNI designs including flight controller, path planning, and visual navigation. The experimental results confirm that the proposed framework achieves 23% flight time reduction and up to 34% energy savings for the flight controller application. In addition, the optimization on a 16-core platform improves the on-time visit rate of the path planning algorithm by 14% while reducing 81% of run time for ConvNet visual navigation. © 2021 Association for Computing Machinery.",community detection; complex networks; load balancing; manycore systems; ollivier-ricci curvature; pipeline parallelism; Unmanned aerial vehicles,Air navigation; Antennas; Decision making; Energy conservation; Energy dissipation; Environmental management; Geometry; Motion planning; Network architecture; Power management; Unmanned aerial vehicles (UAV); Aerial vehicle; Community detection; Design Methodology; Energy aware; Load-Balancing; Manycore systems; Ollivier-ricci curvature; Pipeline parallelisms; Ricci curvature; Unmanned aerial vehicle; Complex networks
A Native SPICE Implementation of Memristor Models for Simulation of Neuromorphic Analog Signal Processing Circuits,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131966668&doi=10.1145%2f3474364&partnerID=40&md5=7c22d003ad182b5c300bbfba9c9025b4,"Since the memristor emerged as a programmable analog storage device, it has stimulated research on the design of analog/mixed-signal circuits with the memristor as the enabler of in-memory computation. Due to the difficulty in evaluating the circuit-level nonidealities of both memristors and CMOS devices, SPICE-accuracy simulation tools are necessary for perfecting the art of neuromorphic analog/mixed-signal circuit design. This article is dedicated to a native SPICE implementation of the memristor device models published in the open literature and develops case studies of applying such a circuit simulation with MOSFET models to study how device-level imperfections can make adversarial effects on the analog circuits that implement neuromorphic analog signal processing. Methods on memristor stamping in the framework of modified nodal analysis formulation are presented, and implementation results are reported. Furthermore, functional simulations on neuromorphic signal processing circuits including memristors and CMOS devices are carried out to validate the effectiveness of the native SPICE implementation of memristor models from the perspectives of simulation accuracy, efficiency, and convergence for large-scale simulation tasks. © 2021 Association for Computing Machinery.",artificial neural network (ANN) circuits; circuit simulation; Memristor; neuromorphic; simulation program with IC emphasis (SPICE),CMOS integrated circuits; Integrated circuit design; Integrated circuit manufacture; Memristors; MOSFET devices; Signal processing; SPICE; Static random access storage; Timing circuits; Virtual storage; Analog/mixed-signal circuits; Analogue signal processing circuits; Artificial neural network  circuit; CMOS devices; Memristor; Modeling for simulations; Neuromorphic; Programmable analogs; Simulation program with IC emphasis; Simulation projects; Neural networks
An Efficient Execution Framework of Two-Part Execution Scenario Analysis,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139190330&doi=10.1145%2f3465474&partnerID=40&md5=3d5dba83756bd8e36832c350d341a4d9,"Response Time Analysis (RTA) is an important and promising technique for analyzing the schedulability of real-time tasks under both Global Fixed-Priority (G-FP) scheduling and Global Earliest Deadline First (G-EDF) scheduling. Most existing RTA methods for tasks under global scheduling are dominated by partitioned scheduling, due to the pessimism of the -based interference calculation where is the number of processors. Two-part execution scenario is an effective technique that addresses this pessimism at the cost of efficiency. The major idea of two-part execution scenario is to calculate a more accurate upper bound of the interference by dividing the execution of the target job into two parts and calculating the interference on the target job in each part. This article proposes a novel RTA execution framework that improves two-part execution scenario by reducing some unnecessary calculation, without sacrificing accuracy of the schedulability test. The key observation is that, after the division of the execution of the target job, two-part execution scenario enumerates all possible execution time of the target job in the first part for calculating the final Worst-Case Response Time (WCRT). However, only some special execution time can cause the final result. A set of experiments is conducted to test the performance of the proposed execution framework and the result shows that the proposed execution framework can improve the efficiency of two-part execution scenario analysis by up to in terms of the execution time.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",efficiency improvement; global fixed priority scheduling; real-time scheduling; Response time analysis,Response time (computer systems); Scheduling; Scheduling algorithms; Efficiency improvement; Execution framework; Execution scenario; Fixed priority scheduling; Global fixed priority scheduling; Real time scheduling; Real-time tasks; Response-time analysis; Scenarios analysis; Schedulability; Efficiency
"A Comprehensive Survey of Attacks without Physical Access Targeting Hardware Vulnerabilities in IoT/IIoT Devices, and Their Detection Mechanisms",2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116311662&doi=10.1145%2f3471936&partnerID=40&md5=7b8a25f4ae10d856a5963f0bd96e8f0a,"With the advances in the field of the Internet of Things (IoT) and Industrial IoT (IIoT), these devices are increasingly used in daily life or industry. To reduce costs related to the time required to develop these devices, security features are usually not considered. This situation creates a major security concern. Many solutions have been proposed to protect IoT/IIoT against various attacks, most of which are based on attacks involving physical access. However, a new class of attacks has emerged targeting hardware vulnerabilities in the micro-architecture that do not require physical access. We present attacks based on micro-architectural hardware vulnerabilities and the side effects they produce in the system. In addition, we present security mechanisms that can be implemented to address some of these attacks. Most of the security mechanisms target a small set of attack vectors or a single specific attack vector. As many attack vectors exist, solutions must be found to protect against a wide variety of threats. This survey aims to inform designers about the side effects related to attacks and detection mechanisms that have been described in the literature. For this purpose, we present two tables listing and classifying the side effects and detection mechanisms based on the given criteria. © 2021 Association for Computing Machinery.",attacks; detection; detection mechanisms; hardware vulnerabilities; IIoT; IoT; security; side effects,Computer architecture; Surveys; Attack; Attack vector; Daily lives; Detection; Detection mechanism; Hardware vulnerability; Industrial IoT; Security; Security mechanism; Side effect; Internet of things
Improving LDPC Decoding Performance for 3D TLC NAND Flash by LLR Optimization Scheme for Hard and Soft Decision,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139195406&doi=10.1145%2f3473305&partnerID=40&md5=f702c8cbcb7d4248a88303360589fc63,"Low-density parity-check (LDPC) codes have been widely adopted in NAND flash in recent years to enhance data reliability. There are two types of decoding, hard-decision and soft-decision decoding. However, for the two types, their error correction capability degrades due to inaccurate log-likelihood ratio (LLR). To improve the LLR accuracy of LDPC decoding, this article proposes LLR optimization schemes, which can be utilized for both hard-decision and soft-decision decoding. First, we build a threshold voltage distribution model for 3D floating gate (FG) triple level cell (TLC) NAND flash. Then, by exploiting the model, we introduce a scheme to quantize LLR during hard-decision and soft-decision decoding. And by amplifying a portion of small LLRs, which is essential in the layer min-sum decoder, more precise LLR can be obtained. For hard-decision decoding, the proposed new modes can significantly improve the decoder's error correction capability compared with traditional solutions. Soft-decision decoding starts when hard-decision decoding fails. For this part, we study the influence of the reference voltage arrangement of LLR calculation and apply the quantization scheme. The simulation shows that the proposed approach can reduce frame error rate (FER) for several orders of magnitude. © 2021 Association for Computing Machinery.",LDPC; LLR; reference voltage; threshold voltage distribution; TLC NAND flash,Decoding; Error correction; Lunar surface analysis; Memory architecture; Voltage distribution measurement; Hard decisions; Log likelihood ratio; Low density parity check decoding; Low-density parity-check; NAND Flash; Optimization scheme; Reference voltages; Soft decision decoding; Threshold voltage distribution; Triple level cell NAND flash; NAND circuits
Pseudo-3D Physical Design Flow for Monolithic 3D ICs: Comparisons and Enhancements,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122563030&doi=10.1145%2f3453480&partnerID=40&md5=6093e514e066a63413a152ac819ebae0,"Studies have shown that monolithic 3D (M3D) ICs outperform the existing through-silicon-via (TSV) -based 3D ICs in terms of power, performance, and area (PPA) metrics, primarily due to the orders of magnitude denser vertical interconnections offered by the nano-scale monolithic inter-tier vias. In order to facilitate faster industry adoption of the M3D technologies, physical design tools and methodologies are essential. Recent academic efforts in developing an EDA algorithm for 3D ICs, mainly targeting placement using TSVs, are inadequate to provide commercial-quality GDS layouts. Lately, pseudo-3D approaches have been devised, which utilize commercial 2D IC EDA engines with tricks that help them operate as an efficient 3D IC CAD tool. In this article, we provide thorough discussions and fair comparisons (both qualitative and quantitative) of the state-of-the-art pseudo-3D design flows, with analysis of limitations in each design flow and solutions to improve their PPA metrics. Moreover, we suggest a hybrid pseudo-3D design flow that achieves both benefits. Our enhancements and the inter-mixed design flow, provide up to an additional 26% wirelength, 10% power consumption, and 23% of power-delay-product improvements.  © 2021 Association for Computing Machinery.",3D placement; computer-aided design; Monolithic 3D IC; pseudo-3D approach; timing closure,Computer aided design; Electronics packaging; Integrated circuit design; Integrated circuit manufacture; Nanotechnology; Timing circuits; 3-d designs; 3-D placements; Computer-aided design; Design flows; Monolithic 3d IC; Monolithics; Physical design; Power performance; Pseudo-3d approach; Timing closures; Three dimensional integrated circuits
QuadSeal: Quadruple Balancing to Mitigate Power Analysis Attacks with Variability Effects and Electromagnetic Fault Injection Attacks,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122594723&doi=10.1145%2f3443706&partnerID=40&md5=0abb8aa54af9ea4a2bef0667b10e545a,"Side channel analysis attacks employ the emanated side channel information to deduce the secret keys from cryptographic implementations by analyzing the power traces during execution or scrutinizing faulty outputs. To be effective, a countermeasure must remove or conceal as many as possible side channels. However, many of the countermeasures against side channel attacks are applied independently.In this article, the authors present a novel countermeasure (referred to as QuadSeal) against Power Analysis Attacks and Electromagentic Fault Injection Attacks (FIAs), which is an extension of the work proposed in Reference [27]. The proposed solution relies on algorithmically balancing both Hamming distances and Hamming weights (where the bit transitions on the registers and gates are balanced, and the total number of 1s and 0s are balanced) by the use of four identical circuits with differing inputs and modified SubByte tables. By randomly rotating the four encryptions, the system is protected against variations, path imbalances, and aging effects. After generating the ciphertext, the output of each circuit is compared against each other to detect any fault injections or to correct the faulty ciphertext to gain reliability.The proposed countermeasure allows components to be switched off to save power or to run four executions in parallel for high performance when resistance against power analysis attacks is not of high priority, which is not available with the existing countermeasures (except software based where source code can be changed). The proposed countermeasure is implemented for Advanced Encryption Standard (AES) and tested against Correlation Power Analysis and Mutual Information Attacks attacks (for up to a million traces), and none of the secret keys was found even after one million power traces (the unprotected AES circuit is vulnerable for power analysis attacks within 5,000 power traces). A detection circuit (referred to as C-FIA circuit) is operated using the algorithmic redundancy presented in four circuits of QuadSeal to mitigate Electromagnetic Fault Injection Attacks. Using Synopsys PrimeTime, we measured the power dissipation of QuadSeal registers and XOR gates to test the effectiveness of Quadruple balancing methodology. We tested the QuadSeal countermeasure with C-FIA circuit against Differential Fault Analysis Attacks up to one million traces; no bytes of the secret key were found. This is the smallest known circuit that is capable of withstanding power-based side channel attacks when electromagnetic injection attack resistance, process variations, path imbalances, and aging effects are considered.  © 2021 Association for Computing Machinery.",analysis attacks; balancing power dissipation; concurrent error detection; countermeasures; Fault Injection attacks; power analysis attacks; Side channel,Data privacy; Electric losses; Electric network analysis; Error detection; Fault detection; Hamming distance; Hardware security; Software testing; Analyse attack; Balancing power; Balancing power dissipation; Concurrent error detection; Countermeasure; Fault injection attacks; Power traces; Power-dissipation; Secret key; Side-channel; Side channel attack
Machine Learning for Electronic Design Automation: A Survey,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111947681&doi=10.1145%2f3451179&partnerID=40&md5=4a829f7a943efcdcecb524e6c62226a6,"With the down-scaling of CMOS technology, the design complexity of very large-scale integrated is increasing. Although the application of machine learning (ML) techniques in electronic design automation (EDA) can trace its history back to the 1990s, the recent breakthrough of ML and the increasing complexity of EDA tasks have aroused more interest in incorporating ML to solve EDA tasks. In this article, we present a comprehensive review of existing ML for EDA studies, organized following the EDA hierarchy.  © 2021 Association for Computing Machinery.",Electronic design automation; machine learning; neural networks,Automation; Complex networks; Electronic design automation; Machine learning; Neural networks; CMOS technology; Design complexity; Down-scaling; Electronics design automation; Machine learning techniques; Machine-learning; Neural-networks; Very large scale integrated; E-learning
A Module-Linking Graph Assisted Hybrid Optimization Framework for Custom Analog and Mixed-Signal Circuit Parameter Synthesis,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121638823&doi=10.1145%2f3456722&partnerID=40&md5=a2ade5e4d6fc4e107ff6a8912cd0c93c,"Analog and mixed-signal (AMS) computer-aided design tools are of increasing interest owing to demand for the wide range of AMS circuit specifications in the modern system on a chip and faster time to market requirement. Traditionally, to accelerate the design process, the AMS system is decomposed into smaller components (called modules) such that the complexity and evaluation of each module are more manageable. However, this decomposition poses an interface problem, where the module's input-output states deviate from when combined to construct the AMS system, and thus degrades the system expected performance. In this article, we develop a tool module-linking-graph assisted hybrid parameter search engine with neural networks (MOHSENN) to overcome these obstacles. We propose a module-linking-graph that enforces equality of the modules' interfaces during the parameter search process and apply surrogate modeling of the AMS circuit via neural networks. Further, we propose a hybrid search consisting of a global optimization with fast neural network models and a local optimization with accurate SPICE models to expedite the parameter search process while maintaining the accuracy. To validate the effectiveness of the proposed approach, we apply MOHSENN to design a successive approximation register analog-to-digital converter in 65-nm CMOS technology. This demonstrated that the search time improves by a factor of 5 and 700 compared to conventional hierarchical and flat design approaches, respectively, with improved performance.  © 2021 Association for Computing Machinery.",CAD tool; deep neural network,Analog to digital conversion; Computer aided design; Electric signal systems; Global optimization; Interface states; Mixed signal integrated circuits; Search engines; System-on-chip; Timing circuits; Analog and mixed signal circuits; Analog and mixed signals; CAD tool; Hybrid optimization; Mixed-signal systems; Module linking; Neural-networks; Optimization framework; Performance; Search process; Deep neural networks
Design Flow and Methodology for Dynamic and Static Energy-constrained Scheduling Framework in Heterogeneous Multicore Embedded Devices,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122586099&doi=10.1145%2f3450448&partnerID=40&md5=906d9db7a6106bce6514c85d0ba18589,"With Internet of things technologies, billions of embedded devices, including smart gateways, smart phones, and mobile robots, are connected and deeply integrated. Almost all these embedded devices are battery-constrained and energy-limited systems. In recent years, several works used energy pre-assignment techniques to study the dynamic energy-constrained scheduling of a parallel application in heterogeneous multicore embedded systems. However, the existing energy pre-assignment techniques cannot satisfy the actual energy constraint, because it is the joint constraint on dynamic energy and static energy. Further, the modeling and verification of these works are based on the simulations, which have not been verified in real embedded devices. This study aims to propose a dynamic and static energy-constrained scheduling framework in heterogeneous multicore embedded devices. Solving this problem can utilize existing energy pre-assignment techniques, but it requires a deeply integrated design flow and methodology. The design flow consists of four processes: (1) power and energy modeling; (2) power parameter measurement; (3) basic framework design including energy pre-assignment; and (4) framework optimization. Each design flow has corresponding design methodology. Both our theoretical analysis and practical verification using the low-power ODROID-XU4 device confirm the effectiveness of the proposed framework.  © 2021 Association for Computing Machinery.",embedded devices; Energy design automation; energy-constrained scheduling; ODROID-XU4,Embedded systems; Scheduling; Smartphones; Constrained scheduling; Design automations; Design Methodology; Dynamic energy; Embedded device; Energy design; Energy design automation; Energy-constrained; Energy-constrained scheduling; ODROID-XU4; Computer aided design
Dataflow Model based Software Synthesis Framework for Parallel and Distributed Embedded Systems,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122591490&doi=10.1145%2f3447680&partnerID=40&md5=7130975d4f583b9178be5d4f0f3fc57a,"Existing software development methodologies mostly assume that an application runs on a single device without concern about the non-functional requirements of an embedded system such as latency and resource consumption. Besides, embedded software is usually developed after the hardware platform is determined, since a non-negligible portion of the code depends on the hardware platform. In this article, we present a novel model-based software synthesis framework for parallel and distributed embedded systems. An application is specified as a set of tasks with the given rules for execution and communication. Having such rules enables us to perform static analysis to check some software errors at compile-time to reduce the verification difficulty. Platform-specific programs are synthesized automatically after the mapping of tasks onto processing elements is determined. The proposed framework is expandable to support new hardware platforms easily. The proposed communication code synthesis method is extensible and flexible to support various communication methods between devices. In addition, the fault-tolerant feature can be added by modifying the task graph automatically according to the selected fault-tolerance configurations by the user. The viability of the proposed software development methodology is evaluated with a real-life surveillance application that runs on six processing elements.  © 2021 Association for Computing Machinery.",Code generation; embedded software development; fault tolerance; synchronous dataflow,Application programs; Data flow analysis; Embedded systems; Software design; Static analysis; Verification; Codegeneration; Distributed embedded system; Embedded software development; Hardware platform; Model-based OPC; Parallel embedded systems; Processing elements; Software development methodologies; Software synthesis; Synchronous Dataflow; Fault tolerance
FastCFI: Real-time Control-Flow Integrity Using FPGA without Code Instrumentation,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122577584&doi=10.1145%2f3458471&partnerID=40&md5=57cb2053d2d6cf6fe8faa5f792ad7b96,"Control-Flow Integrity (CFI) is an effective defense technique against a variety of memory-based cyber attacks. CFI is usually enforced through software methods, which entail considerable performance overhead. Hardware-based CFI techniques can largely avoid performance overhead, but typically rely on code instrumentation, forming a non-trivial hurdle to the application of CFI. Taking advantage of the tradeoff between computing efficiency and flexibility of FPGA, we develop FastCFI, an FPGA-based CFI system that can perform fine-grained and stateful checking without code instrumentation. We also propose an automated Verilog generation technique that facilitates fast deployment of FastCFI, and a compression algorithm for reducing the hardware expense. Experiments on popular benchmarks confirm that FastCFI can detect fine-grained CFI violations over unmodified binaries. When using FastCFI on prevalent benchmarks, we demonstrate its capability to detect fine-grained CFI violations in unmodified binaries, while incurring an average of 0.36% overhead and a maximum of 2.93% overhead.  © 2021 Association for Computing Machinery.",Control-flow integrity; field-programmable gate array; security,Codes (symbols); Computer hardware description languages; Cybersecurity; Network security; Real time control; Code instrumentation; Computing efficiency; Control-flow integrities; Defense techniques; Fine-grained control; Non-trivial; Performance; Real-time control; Security; Software methods; Field programmable gate arrays (FPGA)
A Dynamic Huffman Coding Method for Reliable TLC NAND Flash Memory,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122581036&doi=10.1145%2f3446771&partnerID=40&md5=99574c3a31fbe235f2d14b64b326314a,"With the progress of the manufacturing process, NAND flash memory has evolved from the single-level cell and multi-level cell into the triple-level cell (TLC). NAND flash memory has physical problems such as the characteristic of erase-before-write and the limitation of program/erase cycles. Moreover, TLC NAND flash memory has low reliability and short lifetime. Thus, we propose a dynamic Huffman coding method that can apply to the write operations of NAND flash memory. The proposed method exploits observations from a Huffman tree and machine learning from data patterns to dynamically select a suitable Huffman coding. According to the experimental results, the proposed method can improve the reliability of TLC NAND flash memory and also consider the compression performance for those applications that require the Huffman coding.  © 2021 Association for Computing Machinery.",Huffman coding; NAND flash memory; reliability,Flash memory; Memory architecture; NAND circuits; Trees (mathematics); Cell levels; Dynamic Huffman coding; Huffman coding; Huffman coding method; Manufacturing process; Multilevels; NAND flash memory; Program/erase; Single level cells; Write operations; Reliability
Security assessment of dynamically obfuscated scan chain against oracle-guided attacks,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104942382&doi=10.1145%2f3444960&partnerID=40&md5=b803a1e05616b89aeab4461d8e903a0b,"Logic locking has emerged as a promising solution to protect integrated circuits against piracy and tampering. However, the security provided by existing logic locking techniques is often thwarted by Boolean satisfiability (SAT)-based oracle-guided attacks. Criteria for successful SAT attacks on locked circuits include: (i) the circuit under attack is fully combinational, or (ii) the attacker has scan chain access. To address the threat posed by SAT-based attacks, we adopt the dynamically obfuscated scan chain (DOSC) architecture and illustrate its resiliency against the SAT attacks when inserted into the scan chain of an obfuscated design. We demonstrate, both mathematically and experimentally, that DOSC exponentially increases the resiliency against key extraction by SAT attack and its variants. Our results show that the mathematical estimation of attack complexity correlates to the experimental results with an accuracy of 95% or better. Along with the formal proof, we model DOSC architecture to its equivalent combinational circuit and perform SAT attack to evaluate its resiliency empirically. Our experiments demonstrate that SAT attack on DOSC-inserted benchmark circuits timeout at minimal test time overhead, and while DOSC requires less than 1% area and power overhead.  © 2021 ACM.",hardware obfuscation; logic locking; mathematical model for satisfiability; SAT attack; scan obfuscation,Locks (fasteners); Attack complexity; Benchmark circuit; Boolean satisfiability; Formal proofs; Locking technique; Mathematical estimation; Power overhead; Security assessment; Computer circuits
Equivalent faults under launch-on-shift (LOS) tests with equal primary input vectors,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104930544&doi=10.1145%2f3440013&partnerID=40&md5=6185d308674a55e55584dde5d9aaa843,"A recent work showed that it is possible to transform a single-cycle test for stuck-at faults into a launch-on-shift (LOS) test that is guaranteed to detect the same stuck-at faults without any logic or fault simulation. The LOS test also detects transition faults. This was used for obtaining a compact LOS test set that detects both types of faults. In the scenario where LOS tests are used for both stuck-at and transition faults, this article observes that, under certain conditions, the detection of a stuck-at fault guarantees the detection of a corresponding transition fault. This implies that the two faults are equivalent under LOS tests. Equivalence can be used for reducing the set of target faults for test generation and test compaction. The article develops this notion of equivalence under LOS tests with equal primary input vectors and provides an efficient procedure for identifying it. It presents experimental results to demonstrate that such equivalences exist in benchmark circuits, and shows an unexpected effect on a test compaction procedure.  © 2021 ACM.",Equivalent faults; launch-on-shift tests; test compaction; test generation; transition faults,Compaction; Dynamic random access storage; Testing; Benchmark circuit; Equivalent faults; Fault simulation; Primary input vectors; Stuck-at faults; Test Compaction; Test generations; Transition faults; Fault detection
Design space optimization of shared memory architecture in accelerator-rich systems,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105007588&doi=10.1145%2f3446001&partnerID=40&md5=6f911a5c4a8588c2a5d488cd0e28e3fc,"Shared memory architectures, as opposed to private-only memories, provide a viable alternative to meet the ever-increasing memory requirements of multi-accelerator systems to achieve high performance under stringent area and energy constraints. However, an impulsive memory sharing degrades performance due to network contention and latency to access shared memory. We propose the Accelerator Shared Memory (ASM) framework to provide an optimal private/shared memory configuration and shared data allocation under a system's resource and network constraints. Evaluations show ASM provides up to 34.35% and 31.34% improvement in performance and energy, respectively, over baseline systems.  © 2021 ACM.",design space optimization; Hardware accelerator; multi-accelerator SoC,Data Sharing; Network architecture; Accelerator system; Design space optimization; Memory configuration; Memory requirements; Network constraints; Network contention; Shared data allocation; Shared memory architecture; Memory architecture
TAAL: Tampering attack on any key-based logic locked circuits,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104953802&doi=10.1145%2f3442379&partnerID=40&md5=638f249ffd0302a8f6bb746f513af8e1,"Due to the globalization of semiconductor manufacturing and test processes, the system-on-a-chip (SoC) designers no longer design the complete SoC and manufacture chips on their own. This outsourcing of the design and manufacturing of Integrated Circuits (ICs) has resulted in several threats, such as overproduction of ICs, sale of out-of-specification/rejected ICs, and piracy of Intellectual Properties (IPs). Logic locking has emerged as a promising defense strategy against these threats. However, various attacks about the extraction of secret keys have undermined the security of logic locking techniques. Over the years, researchers have proposed different techniques to prevent existing attacks. In this article, we propose a novel attack that can break any logic locking techniques that rely on the stored secret key. This proposed TAAL attack is based on implanting a hardware Trojan in the netlist, which leaks the secret key to an adversary once activated. As an untrusted foundry can extract the netlist of a design from the layout/mask information, it is feasible to implement such a hardware Trojan. All three proposed types of TAAL attacks can be used for extracting secret keys. We have introduced the models for both the combinational and sequential hardware Trojans that evade manufacturing tests. An adversary only needs to choose one hardware Trojan out of a large set of all possible Trojans to launch the TAAL attack.  © 2021 ACM.",hardware Trojans; IC overproduction; IP piracy; Logic locking; tampering,Computer circuits; Cryptography; Hardware security; Keys (for locks); Locks (fasteners); Logic circuits; Malware; Programmable logic controllers; Semiconductor device manufacture; System-on-chip; Timing circuits; Defense strategy; Integrated circuits (ICs); Locking technique; Manufacturing tests; Semiconductor manufacturing; System on a chip; Tampering attacks; Various attacks; Integrated circuit design
Design automation for tree-based nearest neighborhood-aware placement of high-speed cellular automata on FPGA with scan path insertion,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105018243&doi=10.1145%2f3446206&partnerID=40&md5=94faa19315b392b67d1096aa6840e0a5,"Cellular Automata (CA) is attractive for high-speed VLSI implementation due to modularity, cascadability, and locality of interconnections confined to neighboring logic cells. However, this outcome is not easily transferable to tree-structured CA, since the neighbors having half and double the index value of the current CA cell under question can be sufficiently distanced apart on the FPGA floor. Challenges to meet throughput requirements, seamlessly translate algorithmic modifications for changing application specifications to gate level architectures and to address reliability challenges of semiconductor chips are ever increasing. Thus, a proper design framework assisting automation of synthesizable, delay-optimized VLSI architecture descriptions facilitating testability is desirable. In this article, we have automated the generation of hardware description of tree-structured CA that includes a built-in scan path realized with zero area and delay overhead. The scan path facilitates seeding the CA, state modification, and fault localization on the FPGA fabric. Three placement algorithms were proposed to ensure maximum physical adjacency amongst neighboring CA cells, arranged in a multi-columnar fashion on the FPGA grid. Our proposed architectures outperform implementations arising out of standard placers and behavioral designs, existing tree mapping strategies, and state-of-the-art FPGA centric error detection architectures in area and speed.  © 2021 ACM.",Cellular automata; design automation; FPGA; look-up table; placement; scan path,Automation; Cellular automata; Computer aided design; Field programmable gate arrays (FPGA); Logic devices; Robots; VLSI circuits; Design automations; Hardware descriptions; Nearest neighborhood; Placement algorithm; Proposed architectures; Semiconductor chips; VLSI architectures; VLSI implementation; Integrated circuit design
Efficient one-pass synthesis for digital microfluidic biochips,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104940715&doi=10.1145%2f3446880&partnerID=40&md5=cad17c73f03fd6fcae20232332e0435e,"Digital microfluidics biochips are a promising emerging technology that provides fluidic experimental capabilities on a chip (i.e., following the lab-on-a-chip paradigm). However, the design of such biochips still constitutes a challenging task that is usually tackled by multiple individual design steps, such as binding, scheduling, placement, and routing. Performing these steps consecutively may lead to design gaps and infeasible results. To address these shortcomings, the concept of one-pass design for digital microfluidics biochips has recently been proposed - a holistic approach avoiding the design gaps by considering the whole synthesis process as large. But implementations of this concept available thus far suffer from either high computational effort or costly results. In this article, we present an efficient one-pass solution that is runtime efficient (i.e., rarely needing more than a second to successfully synthesize a design) while, at the same time, producing better results than previously published heuristic approaches. Experimental results confirm the benefits of the proposed solution and allow for realizing really large assays composed of thousands of operations in reasonable runtime.  © 2021 ACM.",design automation; Digital microfluidic biochip (DMFB); one-pass synthesis,Biochips; Heuristic methods; Computational effort; Design steps; Digital microfluidic biochips; Emerging technologies; Heuristic approach; Holistic approach; Microfluidics biochips; Synthesis process; Digital microfluidics
Security threat analyses and attack models for approximate computing systems,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105004621&doi=10.1145%2f3442380&partnerID=40&md5=73afc18836c8408042a25d567b9abc21,"Approximate computing (AC) represents a paradigm shift from conventional precise processing to inexact computation but still satisfying the system requirement on accuracy. The rapid progress on the development of diverse AC techniques allows us to apply approximate computing to many computation-intensive applications. However, the utilization of AC techniques could bring in new unique security threats to computing systems. This work does a survey on existing circuit-, architecture-, and compiler-level approximate mechanisms/algorithms, with special emphasis on potential security vulnerabilities. Qualitative and quantitative analyses are performed to assess the impact of the new security threats on AC systems. Moreover, this work proposes four unique visionary attack models, which systematically cover the attacks that build covert channels, compensate approximation errors, terminate normal error resilience mechanisms, and propagate additional errors. To thwart those attacks, this work further offers the guideline of countermeasure designs. Several case studies are provided to illustrate the implementation of the suggested countermeasures.  © 2021 ACM.",approximate arithmetic circuit; Approximate computing; artificial neural network; attack model; covert channel; edge detection; error control; fast Fourier transform (FFT); fault tolerance; finite impulse response (FIR) filter; hardware Trojan; phase change memory; security threat; side-channel attack,Errors; Approximation errors; Computation intensives; Computing system; Paradigm shifts; Qualitative and quantitative analysis; Security threats; Security vulnerabilities; System requirements; Security systems
Directed test generation for activation of security assertions in RTL models,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104974240&doi=10.1145%2f3441297&partnerID=40&md5=56e8605006aab011f20f7deb682535e4,"Assertions are widely used for functional validation as well as coverage analysis for both software and hardware designs. Assertions enable runtime error detection as well as faster localization of errors. While there is a vast literature on both software and hardware assertions for monitoring functional scenarios, there is limited effort in utilizing assertions to monitor System-on-Chip (SoC) security vulnerabilities. We have identified common SoC security vulnerabilities and defined several classes of assertions to enable runtime checking of security vulnerabilities. A major challenge in assertion-based validation is how to activate the security assertions to ensure that they are valid. While existing test generation using model checking is promising, it cannot generate directed tests for large designs due to state space explosion. We propose an automated and scalable mechanism to generate directed tests using a combination of symbolic execution and concrete simulation of RTL models. Experimental results on diverse benchmarks demonstrate that the directed tests are able to activate security assertions non-vacuously.  © 2021 ACM.",Assertion-based validation; directed test generation; SoC security validation,Computer aided software engineering; Error detection; Model checking; Programmable logic controllers; System-on-chip; Testing; Functional scenarios; Functional validation; Runtime error detections; Security assertion; Security vulnerabilities; Software and hardwares; State-space explosion; Symbolic execution; Security of data
MaxSense: Side-channel sensitivity maximization for trojan detection using statistical test patterns,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101599595&doi=10.1145%2f3436820&partnerID=40&md5=c7cf4fbe6fddc16a56f0012274c519e6,"Detection of hardware Trojans is vital to ensure the security and trustworthiness of System-on-Chip (SoC) designs. Side-channel analysis is effective for Trojan detection by analyzing various side-channel signatures such as power, current, and delay. In this article, we propose an efficient test generation technique to facilitate side-channel analysis utilizing dynamic current. While early work on current-aware test generation has proposed several promising ideas, there are two major challenges in applying it on large designs: (i) The test generation time grows exponentially with the design complexity, and (ii) it is infeasible to detect Trojans, since the side-channel sensitivity is marginal compared to the noise and process variations. Our proposed work addresses both challenges by effectively exploiting the affinity between the inputs and rare (suspicious) nodes. The basic idea is to quickly find the profitable ordered pairs of test vectors that can maximize side-channel sensitivity. This article makes two important contributions: (i) It proposed an efficient test generation algorithm that can produce the first patterns in the test vectors to maximize activation of suspicious nodes using an SMT solver, and (ii) it developed a genetic-algorithm based test generation technique to produce the second patterns in the test vectors to maximize the switching in the suspicious regions while minimizing the switching in the rest of the design. Our experimental results demonstrate that we can drastically improve both the side-channel sensitivity (62× on average) and time complexity (13× on average) compared to the state-of-the-art test generation techniques. © 2021 ACM.",dynamic current; Hardware Trojan detection; side-channel analysis; test generation,Genetic algorithms; Integrated circuit design; Malware; Network security; Programmable logic controllers; Sensitivity analysis; Side channel attack; System-on-chip; Design complexity; Process Variation; Side-channel analysis; State of the art; System on chip design; Test generation algorithm; Test generations; Trojan detections; Testing
COPE: Reducing cache pollution and network contention by inter-tile coordinated prefetching in NoC-based MPSoCs,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101622332&doi=10.1145%2f3428149&partnerID=40&md5=15b6d811af0b5167266513f08bfa6fc0,"Prefetching helps in reducing the memory access latency in multi-banked NUCA architecture, where the Last Level Cache (LLC) is shared. In such systems, an application running on core generates significant traffic on the shared resources, the underlying network and LLC. While prefetching helps to increase application performance, but an inaccurate prefetcher can cause harm by generating unwanted traffic that additionally increases network and LLC contention. Increased network contention results in untimely prefetching of cache blocks, thereby reducing the effectiveness of a prefetcher. Prefetch accuracy is extensively used to reduce unwanted prefetches that can mitigate the prefetcher caused contention. However, the conventional prefetch accuracy parameter has major limitations in NUCA architectures. The article exposes that prefetch accuracy can create two major false-positive cases of prefetching, Under-estimation and Over-estimation problems, and false feedback loop that can mislead a prefetcher in generating more unwanted traffic. We propose a novel technique, Coordinated Prefetching for Efficient (COPE), which addresses these issues by redefining prefetch accuracy for such architectures and identifies additional parameters that can avoid generating unwanted prefetch requests. Experiment conducted using PARSEC benchmark on a 64-core system shows that COPE achieve 3% reduction in L1 cache miss rate, 12.64% improvement in IPC, 23.2% reduction in average packet latency and 18.56% reduction in dynamic power consumption of the underlying network. © 2020 ACM.",NoC; prefetch; prefetch filtering; TCMP; useless prefetch,Buffer storage; Memory architecture; Network-on-chip; Accuracy parameters; Application performance; Average packet latencies; Dynamic power consumption; Lastlevel caches (LLC); Memory access latency; Network contention; Underlying networks; Network architecture
Approximate learning and fault-tolerant mapping for energy-efficient neuromorphic systems,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101653555&doi=10.1145%2f3436491&partnerID=40&md5=4958d3b2b12cd953db3fd5c985f1f84b,"Brain-inspired deep neural networks such as Convolutional Neural Network (CNN) have shown great potential in solving difficult cognitive problems such as object recognition and classification. However, such architectures have high computational energy demand and sensitivity to variation effects, making them inapplicable for energy-constrained embedded learning platforms. To address this issue, we propose a learning and mapping approach that utilizes approximate computing during early design phases for a layer-wise pruning and fault tolerant weight mapping scheme of reliable and energy-efficient CNNs. In the proposed approach, approximate CNN is prepared first by layer-wise pruning of approximable neurons, which have high error tolerance margins using a two-level approximate learning methodology. Then, the pruned network is retrained to improve its accuracy by fine-tuning the weight values. Finally, a fault-tolerant layer-wise neural weight mapping scheme is adopted to aggressively reduce memory operating voltage when loading the weights of error resilient layers for energy-efficiency. Thus, the combination of approximate learning and fault tolerance aware memory operating voltage downscaling techniques enable us to implement robust and energy-efficient approximate inference engine for CNN applications. Simulation results show that the proposed fault tolerant and approximate learning approach can improve the energy-efficiency of CNN inference engines by more than 50% with less than 5% reduction in classification accuracy. Additionally, more than 26% energy-saving is achieved by using the proposed layer-wise mapping-based cache memory operating voltage down-scaling. © 2020 ACM.",Approximate learning; CNN; fault tolerant mapping; voltage scaling,Cache memory; Convolutional neural networks; Deep learning; Deep neural networks; Engines; Fault tolerance; Learning systems; Mapping; Object recognition; Sensitivity analysis; Approximate inference; Classification accuracy; Early design phasis; Embedded learning; Energy-constrained; Learning approach; Neuromorphic systems; Sensitivity to variations; Energy efficiency
HeM3D,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101223432&doi=10.1145%2f3424239&partnerID=40&md5=f8a2d52c44fde0071f05c5557d6674f7,"Heterogeneous manycore architectures are the key to efficiently execute compute-and data-intensive applications. Through-silicon-via (TSV)-based 3D manycore system is a promising solution in this direction as it enables the integration of disparate computing cores on a single system. Recent industry trends show the viability of 3D integration in real products (e.g., Intel Lakefield SoC Architecture, the AMD Radeon R9 Fury X graphics card, and Xilinx Virtex-7 2000T/H580T, etc.). However, the achievable performance of conventional TSV-based 3D systems is ultimately bottlenecked by the horizontal wires (wires in each planar die). Moreover, current TSV 3D architectures suffer from thermal limitations. Hence, TSV-based architectures do not realize the full potential of 3D integration. Monolithic 3D (M3D) integration, a breakthrough technology to achieve ""More Moore and More Than Moore,""opens up the possibility of designing cores and associated network routers using multiple layers by utilizing monolithic inter-Tier vias (MIVs) and hence, reducing the effective wire length. Compared to TSV-based 3D integrated circuits (ICs), M3D offers the ""true""benefits of vertical dimension for system integration: The size of an MIV used in M3D is over 100 × smaller than a TSV. This dramatic reduction in via size and the resulting increase in density opens up numerous opportunities for design optimizations in 3D manycore systems: designers can use up to millions of MIVs for ultra-fine-grained 3D optimization, where individual cores and routers can be spread across multiple tiers for extreme power and performance optimization. In this work, we demonstrate how M3D-enabled vertical core and uncore elements offer significant performance and thermal improvements in manycore heterogeneous architectures compared to its TSV-based counterpart. To overcome the difficult optimization challenges due to the large design space and complex interactions among the heterogeneous components (CPU, GPU, Last Level Cache, etc.) in a M3D-based manycore chip, we leverage novel design-space exploration algorithms to trade off different objectives. The proposed M3D-enabled heterogeneous architecture, called HeM3D, outperforms its state-of-The-Art TSV-equivalent counterpart by up to 18.3% in execution time while being up to 19°C cooler. © 2021 ACM.",execution time; Heterogeneous manycore; M3D; multi-Tier; NoC; performance; temperature,Computer architecture; Economic and social effects; Electronics packaging; Integrated circuit design; Network architecture; Programmable logic controllers; Routers; System-on-chip; Wire; 3d integrated circuit (ICs); Breakthrough technology; Data-intensive application; Heterogeneous architectures; Heterogeneous component; Heterogeneous many-core architectures; Performance optimizations; Through-Silicon-Via (TSV); Three dimensional integrated circuits
Fault-based Built-in Self-test and Evaluation of Phase Locked Loops,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101645534&doi=10.1145%2f3427911&partnerID=40&md5=7de4cbdc5bc95b3fdb71c952cf3b7760,"With the increasing pressure to obtain near-zero defect rates for the automotive industry, there is a need to explore built-in self-test and other non-traditional test techniques for embedded mixed-signal components, such as PLLs, DC-DC converters, and data converters. This article presents a very low-cost built-in self-test technique for PLLs specifically designed for fault detection. The methodology relies on exciting the PLL loop in one location via a pseudo-random signal with noise characteristics and observing the response from another location in the loop via all digital circuitry, thereby inducing low area and performance overhead. The BIST circuit along with a PLL under test is designed in 65 nm technology. Fault simulations performed at the transistor and system-level show that the majority of non-catastrophic faults that result in parametric failures can be detected with the proposed approach. © 2021 ACM.",built-in self test; Mixed signal circuit testing; phase locked loops; pseudo random binary sequence,DC-DC converters; Fault detection; Phase locked loops; 65-nm technologies; Catastrophic fault; Fault simulation; Noise characteristic; Non-traditional; Parametric failure; Pseudorandom signals; Test techniques; Built-in self test
Machine learning for statistical modeling: The case of perpendicular spin-transfer-torque random access memory,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101659443&doi=10.1145%2f3440014&partnerID=40&md5=3cdf1eccb6135dcd005f44ee939e751e,"We propose a methodology to perform process variation-aware device and circuit design using fully physics-based simulations within limited computational resources, without developing a compact model. Machine learning (ML), specifically a support vector regression (SVR) model, has been used. The SVR model has been trained using a dataset of devices simulated a priori, and the accuracy of prediction by the trained SVR model has been demonstrated. To produce a switching time distribution from the trained ML model, we only had to generate the dataset to train and validate the model, which needed g1/4500 hours of computation. On the other hand, if 106 samples were to be simulated using the same computation resources to generate a switching time distribution from micromagnetic simulations, it would have taken g1/4250 days. Spin-transfer-torque random access memory (STTRAM) has been used to demonstrate the method. However, different physical systems may be considered, different ML models can be used for different physical systems and/or different device parameter sets, and similar ends could be achieved by training the ML model using measured device data. © 2021 ACM.",machine learning; process variation; Spin-transfer-torque random access memory; support vector regression,Machine learning; Support vector regression; Computation resources; Computational resources; Micromagnetic simulations; Physics-based Simulation; Random access memory; Spin transfer torque; Statistical modeling; Support vector regression (SVR); Random access storage
Multi-objective optimization of mapping dataflow applications to MPSoCs using a hybrid evaluation combining analytic models and measurements,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101640372&doi=10.1145%2f3431814&partnerID=40&md5=4a503e9e025dfd0a4867fa9c737bb6bb,"Dataflow modeling is well suited for a large variety of applications for modern multi-core architectures, e.g., from the signal processing and the control domain. Furthermore, Design Space Exploration (DSE) can be used to explore mappings of tasks to hardware resources (cores of an MPSoC) and their scheduling to obtain optimized trade-off solutions between throughput and resource costs. However, the throughput evaluation of an implementation candidate via compilation-in-the-loop or simulation-based approaches can be extremely time-consuming. Such a deficiency is very detrimental, because a typical DSE run needs to evaluate thousands of implementation candidates. As a remedy, we propose a hybrid-adaptive DSE where a max-plus algebra-based analytic throughput calculation method is used in the initial DSE phase to enable a fast progress of the search space exploration. However, as this analysis may be inaccurate as neglecting some real-world effects like cache and scheduling overhead, throughput measurements are taken later in the DSE. Moreover, we explore the trade-off between scheduling efficiency of implementation candidates - in favor of reducing concurrency - and exploiting concurrency to a large extent for parallel execution of the application. To find solutions of highest achievable throughput, it is shown that not only highly scheduling efficient implementation candidates but also highly parallel implementation candidates are essential when determining the initial population. In this realm, we contribute a method for diversity-based population initialization. For a representative set of benchmarks, it is shown that the combination of the two major contributions allows us to find much higher throughput multi-core solutions within a given exploration time compared to a state-of-the-art DSE approach. © 2020 ACM.",clustering; Dataflow; design space exploration; max-plus algebra; MPSoC,Computer architecture; Economic and social effects; Mapping; Multiobjective optimization; Multiprocessing systems; Scheduling; Signal processing; Space research; System-on-chip; Achievable throughputs; Design space exploration; Efficient implementation; Multicore architectures; Population initializations; Scheduling efficiencies; Simulation based approaches; Throughput measurements; Data flow analysis
Covering test holes of functional broadside tests,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101629113&doi=10.1145%2f3441282&partnerID=40&md5=2efdbe88574763fc1bd3a84b021725de,"Functional broadside tests were developed to avoid overtesting of delay faults. The tests achieve this goal by creating functional operation conditions during their functional capture cycles. To increase the achievable fault coverage, close-to-functional scan-based tests are allowed to deviate from functional operation conditions. This article suggests that a more comprehensive functional broadside test set can be obtained by replacing target faults that cannot be detected with faults that have similar (but not identical) detection conditions. A more comprehensive functional broadside test set has the advantage that it still maintains functional operation conditions. It covers the test holes created when target faults cannot be detected by detecting similar faults. The article considers the case where the target faults are transition faults. When a standard transition fault, with an extra delay of a single clock cycle, cannot be detected, an unspecified transition fault is used instead. An unspecified transition fault captures the behaviors of transition faults with different extra delays. When this fault cannot be detected, a stuck-at fault is used instead. A stuck-at fault has some of the detection conditions of a transition fault. Multicycle functional broadside tests are used to allow unspecified transition faults to be detected. As a by-product, test compaction also occurs. The structure of the test generation procedure accommodates the complexity of producing functional broadside tests by considering the target as well as replacement faults together. Experimental results for benchmark circuits demonstrate the fault coverage improvements achieved, and the effect on the number of tests. © 2021 ACM.",Functional broadside tests; multicycle tests; test compaction; test generation; transition faults,Dynamic random access storage; Benchmark circuit; Functional broadside tests; Functional operation; Scan-based test; Single-clock-cycle; Stuck-at faults; Test generation procedure; Transition faults; Fault detection
Logic diagnosis with hybrid fail data,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101608763&doi=10.1145%2f3433929&partnerID=40&md5=f5bf28fe08c6c431e42618300830f7d9,"Yield improvement requires information about the defects present in faulty units. This information is derived by applying a logic diagnosis procedure to the fail data collected by a tester from faulty units. It is typical in the early stages of yield learning to find faulty units that produce excessive volumes of fail data. The current practice is to terminate the fail data collection and possibly discard the fail data already collected for the unit. An earlier study shows that a faulty unit may produce excessive volumes of fail data for some tests but not for others. Based on this observation, a possible solution is to collect full fail data only for tests where this is feasible and pass/fail information for other tests. For this approach to be practical, it is necessary to be able to perform logic diagnosis with hybrid fail data that consists of full fail data for some tests and only pass/fail information for other tests. The main challenge in designing such a procedure is to balance the use of the two types of data to produce accurate logic diagnosis results. This article describes a logic diagnosis procedure, from the class of procedures used by commercial tools, that addresses this challenge. Experimental results for benchmark circuits demonstrate the importance of pass/fail information in this scenario. © 2020 ACM.",Defect diagnosis; hybrid fail data; logic diagnosis; multiple faults,Data acquisition; Testing; Benchmark circuit; Commercial tools; Current practices; Data collection; Logic diagnosis; Yield Improvement; Yield learning; Computer circuits
TransNet: Minimally supervised deep transfer learning for dynamic adaptation of wearable systems,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099080959&doi=10.1145%2f3414062&partnerID=40&md5=83029ea9657228c74532189b45205599,"Wearables are poised to transform health and wellness through automation of cost-effective, objective, and real-time health monitoring. However, machine learning models for these systems are designed based on labeled data collected, and feature representations engineered, in controlled environments. This approach has limited scalability of wearables because (i) collecting and labeling sufficiently large amounts of sensor data is a labor-intensive and expensive process; and (ii) wearables are deployed in highly dynamic environments of the end-users whose context undergoes consistent changes. We introduce TransNet, a deep learning framework that minimizes the costly process of data labeling, feature engineering, and algorithm retraining by constructing a scalable computational approach. TransNet learns general and reusable features in lower layers of the framework and quickly reconfigures the underlying models from a small number of labeled instances in a new domain, such as when the system is adopted by a new user or when a previously unseen event is to be added to event vocabulary of the system. Utilizing TransNet on four activity datasets, TransNet achieves an average accuracy of 88.1% in cross-subject learning scenarios using only one labeled instance for each activity class. This performance improves to an accuracy of 92.7% with five labeled instances.  © 2020 ACM.",adaptation; deep learning; machine learning; reconfiguration; reliability; transfer learning; Wearable computing,Cost effectiveness; Cost engineering; Learning systems; Transfer learning; Wearable technology; Computational approach; Controlled environment; Dynamic environments; Feature engineerings; Feature representation; Health and wellness; Machine learning models; Real-time health monitoring; Deep learning
Robust Multi-Target Sample Preparation on MEDA Biochips Obviating Waste Production,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099101447&doi=10.1145%2f3414061&partnerID=40&md5=7691c8f32581da3ed533ed97311e1b9f,"Digital microfluidic biochips have fueled a paradigm shift in implementing bench-top laboratory experiments on a single tiny chip, thus replacing costly and bulky equipment. However, because of imprecise fluidic functions, several volumetric split errors may occur during the execution of bioassays. Earlier approaches to error-correcting sample preparation addressed this problem by using a cyberphysical system yielding several drawbacks such as increased sample preparation cost and time, and uncertainty in assay completion time. In addition, error correction for only a single-target sample has been considered so far, although many assays require the production of multi-target samples. In this work, we present an error-free dilution technique that guarantees the correctness of the resulting concentration factor of a sample without performing any additional roll-back or roll-forward action. To the best of our knowledge, we are the first to present a solution strategy for tackling dispensing errors during sample preparation. We use micro-electrode-dot-array biochips that offer the advantages of manipulating fractional volumes of droplets (aliquots) for navigation, as well as mix-split operations. Instead of performing traditional mix-and-split steps with integral-volume droplets, we execute only an aliquoting-and-mix sequence using differential-size aliquots. Thus, all split operations, which are the main source of errors in conventional digital microfluidic biochips, are completely eliminated, and hence neither sensing nor any correcting action is needed, and further, no management of intermediate waste droplets is needed. Additionally, the procedure can be fully parallelized for accurately producing multiple dilutions of a sample. Experimental results corroborate the superiority of the proposed method in terms of error management, as well as sample preparation cost and time.  © 2020 ACM.",bioassay; cyberphysical system; error-correcting; Microfluidic biochip; robust; roll-back and roll-forward error-recovery; sample preparation,Biochips; Drops; Electrodes; Embedded systems; Error correction; Waste management; Concentration factors; Cyber physical systems (CPSs); Digital microfluidic biochips; Dilution technique; Laboratory experiments; Multiple dilutions; Sample preparation; Solution strategy; Digital microfluidics
Ising-FPGA: A spintronics-based reconfigurable ising model solver,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099096301&doi=10.1145%2f3411511&partnerID=40&md5=dc2ad8c59c58687e853340c3124e2b2e,"The Ising model has been explored as a framework for modeling NP-hard problems, with several diverse systems proposed to solve it. The Magnetic Tunnel Junction- (MTJ) based Magnetic RAM is capable of replacing CMOS in memory chips. In this article, we propose the use of MTJs for representing the units of an Ising model and leveraging its intrinsic physics for finding the ground state of the system through annealing. We design the structure of a basic MTJ-based Ising cell capable of performing the functions essential to an Ising solver. The hardware overhead of the Ising model is analyzed, and a technique to use the basic Ising cell for scaling to large problems is described. We then go on to propose Ising-FPGA, a parallel and reconfigurable architecture that can be used to map a large class of NP-hard problems, and show how a standard Place and Route tool can be utilized to program the Ising-FPGA. The effects of this hardware platform on our proposed design are characterized and methods to overcome these effects are prescribed. We discuss how three representative NP-hard problems can be mapped to the Ising model. Further, we suggest ways to simplify these problems to reduce the use of hardware and analyze the impact of these simplifications on the quality of solutions. Simulation results show the effectiveness of MTJs as Ising units by producing solutions close/comparable to the optimum and demonstrate that our design methodology holds the capability to account for the effects of the hardware.  © 2020 ACM.",Combinatorial optimization; magnetic tunnel junction; np-hard problems; reconfigurable architecture; simulated annealing,Field programmable gate arrays (FPGA); Ground state; Integrated circuit design; Ising model; Magnetic devices; NP-hard; Random access storage; Reconfigurable architectures; Tunnel junctions; Design Methodology; Hardware overheads; Hardware platform; Large problems; Magnetic tunnel junction; Place and route; Quality of solution; Reconfigurable; Reconfigurable hardware
High-level synthesis of key-obfuscated RTL iP with design lockout and camouflaging,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099101623&doi=10.1145%2f3410337&partnerID=40&md5=2d02a2ecbe32f39b0a32feda018968c1,"We propose three orthogonal techniques to secure Register-Transfer-Level (RTL) Intellectual Property (IP). In the first technique, the key-based RTL obfuscation scheme is proposed at an early design phase during High-Level Synthesis (HLS). Given a control-dataflow graph, we identify operations on non-critical paths and leverage synthesis information during and after HLS to insert obfuscation logic. In the second approach, we propose a robust design lockout mechanism for a key-obfuscated RTL IP when an incorrect key is applied more than the allowed number of attempts. We embed comparators on obfuscation logic output to check if the applied key is correct or not and a finite-state machine checker to enforce design lockout. Once locked out, only an authorized user (designer) can unlock the locked IP. In the third technique, we design four variants of the obfuscating module to camouflage the RTL design. We analyze the security properties of obfuscation, design lockout, and camouflaging. We demonstrate the feasibility on four datapath-intensive IPs and one crypto core for 32-, 64-, and 128-bit key lengths under three design corners (best, typical, and worst) with reasonable area, power, and delay overheads on both ASIC and FPGA platforms.  © 2020 ACM.",camouflaging; design lockout; hardware obfuscation; High-level synthesis,Computer circuits; Data flow analysis; Integrated circuit design; Authorized users; Critical Paths; Delay overheads; Early design phase; Fpga platforms; Register transfer level; Robust designs; Security properties; High level synthesis
FaultDroid: An algorithmic approach for fault-induced information leakage analysis,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099106824&doi=10.1145%2f3410336&partnerID=40&md5=ee390616d70724af162c500c04a3931e,"Fault attacks belong to a potent class of implementation-based attacks that can compromise a crypto-device within a few milliseconds. Out of the large numbers of faults that can occur in the device, only a very few are exploitable in terms of leaking the secret key. Ignorance of this fact has resulted in countermeasures that have either significant overhead or inadequate protection. This article presents a framework, referred to as FaultDroid, for automated vulnerability analysis of fault attacks. It explores the entire fault attack space, identifies the single/multiple fault scenarios that can be exploited by a differential fault attack, rank-orders them in terms of criticality, and provides design guidance to mitigate the vulnerabilities at low cost. The framework enables a designer to automatically evaluate the fault attack vulnerabilities of a block cipher implementation and then incorporate efficient countermeasures. FaultDroid uses a formal model of fault attacks on a high-level specification of a block cipher and hence is equally applicable to both software and hardware implementation of the cipher. As case studies, we employ FaultDroid to comprehensively evaluate the fault scenarios in several common ciphers - AES, CLEFIA, CAMELLIA, SMS4, SIMON, PRESENT, and GIFT - and assess their vulnerability.  © 2020 ACM.",block cipher; Cryptography; fault injection attacks; formal methods; information leakage,Security of data; Algorithmic approach; Design guidance; Differential fault attack; Fault scenarios; High level specification; Information leakage; Software and hardwares; Vulnerability analysis; Cryptography
Mitigating Negative Impacts of Read Disturb in SSDs,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099111168&doi=10.1145%2f3410332&partnerID=40&md5=ec5c77a0b228826c77d6c6303258e238,"Read disturb is a circuit-level noise in solid-state drives (SSDs), which may corrupt existing data in SSD blocks and then cause high read error rate and longer read latency. The approach of read refresh is commonly used to avoid read disturb errors by periodically migrating the hot read data to other free blocks, but it places considerable negative impacts on I/O (Input/Output) responsiveness. This article proposes scheduling approaches on write data and read refresh operations, to mitigate the negative effects caused by read disturb. To be specific, we first construct a model to classify SSD blocks into two categories according to the estimated read error rate by referring to the factors of block's P/E (Program/Erase) cycle and the accumulated read count to the block. Then, the data being intensively read will be redirected to the block having a small read error rate, as it is not sensitive to read disturb even though the data will be heavily requested. Moreover, we take advantage of reinforcement learning to predict the idle interval between two I/O requests for purposely conducting (partial) read refresh operations. As a result, it is able to minimize negative impacts toward subsequent incoming I/O requests and to ensure I/O responsiveness. Through a series of emulation tests on several realistic disk traces, we demonstrate that the proposed mechanisms can noticeably yield performance improvements on the metrics of read error rate and I/O latency.  © 2020 ACM.",read disturb; read errors; read refresh; scheduling; Solid-state drive (SSD),Errors; Reinforcement learning; Circuit levels; Input/output; Program/erase; Read disturb; Read error rate; Read latencies; Solid state drives; Digital storage
A Deterministic-Path Routing Algorithm for Tolerating Many Faults on Very-Large-Scale Network-on-Chip,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099088215&doi=10.1145%2f3414060&partnerID=40&md5=fb472a43ca0b7a75cb4ac69e3f00c6d7,"Very-large-scale network-on-chip (VLS-NoC) has become a promising fabric for supercomputers, but this fabric may encounter the many-fault problem. This article proposes a deterministic routing algorithm to tolerate the effects of many faults in VLS-NoCs. This approach generates routing tables offline using a breadth-first traversal algorithm and stores a routing table locally in each switch for online packet transmission. The approach applies the Tarjan algorithm to degrade the faulty NoC and maximizes the number of available nodes in the reconfigured NoC. In 2D NoCs, the approach updates routing tables of some nodes using the deprecated channel/node rules and avoids deadlocks in the NoC. In 3D NoCs, the approach uses a forbidden-turn selection algorithm and detour rules to prevent faceted rings and ensures the NoC is deadlock-free. Experimental results demonstrate that the proposed approach provides fault-free communications of 2D and 3D NoCs after injecting 40 faulty links. Meanwhile, it maximizes the number of available nodes in the reconfigured NoC. The approach also outperforms existing algorithms in terms of average latency, throughput, and energy consumption.  © 2020 ACM.",3D NoC; avoiding deadlock; fault-tolerant NoC; Routing algorithm; turn model,Energy utilization; Routing algorithms; Servers; Supercomputers; Breadth-first traversal; Deterministic routing algorithms; Free communications; Large-scale network; Packet transmissions; Routing table; Selection algorithm; Tarjan algorithms; Network-on-chip
Modular Neural Networks for Low-Power Image Classification on Embedded Devices,2021,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099081379&doi=10.1145%2f3408062&partnerID=40&md5=3cbd29ceab4979561d4299d4af09f472,"Embedded devices are generally small, battery-powered computers with limited hardware resources. It is difficult to run deep neural networks (DNNs) on these devices, because DNNs perform millions of operations and consume significant amounts of energy. Prior research has shown that a considerable number of a DNN's memory accesses and computation are redundant when performing tasks like image classification. To reduce this redundancy and thereby reduce the energy consumption of DNNs, we introduce the Modular Neural Network Tree architecture. Instead of using one large DNN for the classifier, this architecture uses multiple smaller DNNs (called modules) to progressively classify images into groups of categories based on a novel visual similarity metric. Once a group of categories is selected by a module, another module then continues to distinguish among the similar categories within the selected group. This process is repeated over multiple modules until we are left with a single category. The computation needed to distinguish dissimilar groups is avoided, thus reducing redundant operations, memory accesses, and energy. Experimental results using several image datasets reveal the effectiveness of our proposed solution to reduce memory requirements by 50% to 99%, inference time by 55% to 95%, energy consumption by 52% to 94%, and the number of operations by 15% to 99% when compared with existing DNN architectures, running on two different embedded systems: Raspberry Pi 3 and Raspberry Pi Zero.  © 2020 ACM.",image classification; Low-power,Deep neural networks; Embedded systems; Energy utilization; Image classification; Low power electronics; Memory architecture; Network architecture; Battery powered; Embedded device; Hardware resources; Image datasets; Memory access; Memory requirements; Modular neural networks; Visual similarity; Neural networks
Core Placement Optimization for Multi-chip Many-core Neural Network Systems with Reinforcement Learning,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097584117&doi=10.1145%2f3418498&partnerID=40&md5=73bfde75462743d7a500e6f7965d4b53,"Multi-chip many-core neural network systems are capable of providing high parallelism benefited from decentralized execution, and they can be scaled to very large systems with reasonable fabrication costs. As multi-chip many-core systems scale up, communication latency related effects will take a more important portion in the system performance. While previous work mainly focuses on the core placement within a single chip, there are two principal issues still unresolved: the communication-related problems caused by the non-uniform, hierarchical on/off-chip communication capability in multi-chip systems, and the scalability of these heuristic-based approaches in a factorially growing search space. To this end, we propose a reinforcement-learning-based method to automatically optimize core placement through deep deterministic policy gradient, taking into account information of the environment by performing a series of trials (i.e., placements) and using convolutional neural networks to extract spatial features of different placements. Experimental results indicate that compared with a naive sequential placement, the proposed method achieves 1.99× increase in throughput and 50.5% reduction in latency; compared with the simulated annealing, an effective technique to approximate the global optima in an extremely large search space, our method improves the throughput by 1.22× and reduces the latency by 18.6%. We further demonstrate that our proposed method is capable to find optimal placements taking advantages of different communication properties caused by different system configurations, and work in a topology-agnostic manner. © 2020 ACM.",core placement optimization; machine learning for system; Multi-chip many-core architecture; neural network accelerator,Convolutional neural networks; Hierarchical systems; Microprocessor chips; Reinforcement learning; Simulated annealing; Communication capabilities; Communication latency; Multi chip system; Neural network systems; Optimal placements; Placement optimization; System configurations; Very large systems; Learning systems
Thermal Management for FPGA Nodes in HPC Systems,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097578943&doi=10.1145%2f3423494&partnerID=40&md5=6437f2eed7d8c983ba54ccc1b778bd0b,"The integration of FPGAs into large-scale computing systems is gaining attention. In these systems, real-time data handling for networking, tasks for scientific computing, and machine learning can be executed with customized datapaths on reconfigurable fabric within heterogeneous compute nodes. At the same time, thermal management, particularly battling the cooling cost and guaranteeing the reliability, is a continuing concern. The introduction of new heterogeneous components into HPC nodes only adds further complexities to thermal modeling and management. The thermal behavior of multi-FPGA systems deployed within large compute clusters is less explored. In this article, we first show that the thermal behaviors of different FPGAs of the same generation can vary due to their physical locations in a rack and process variation, even though they are running the same tasks. We present a machine learning-based model to capture the thermal behavior of each individual FPGA in the cluster. We then propose two thermal management strategies guided by our thermal model. First, we mitigate thermal variation and hotspots across the cluster by proactive thermal-aware task placement. Under the tested system and benchmarks, we achieve up to 26.4° C and on average 13.3° C system temperature reduction with no performance penalty. Second, we utilize this thermal model to guide HLS parameter tuning at the task design stage to achieve improved thermal response after deployment. © 2020 ACM.",high performance computing; task placement; Thermal modeling; thermal-aware design,Benchmarking; Data handling; Field programmable gate arrays (FPGA); Machine learning; Real time systems; Temperature control; Thermal management (electronics); Thermography (temperature measurement); Turing machines; Heterogeneous component; Large-scale computing systems; Multi-FPGA system; Performance penalties; Physical locations; Reconfigurable fabrics; Thermal management strategy; Thermal variation; Large scale systems
Performance-Driven Post-Processing of Control Loop Execution Schedules,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097571929&doi=10.1145%2f3421505&partnerID=40&md5=a5284594ed72ef1dc590c2d9053e14e1,"The increasing demand for mapping diverse embedded features onto shared electronic control units has brought about novel ways to co-design control tasks and their schedules. These techniques replace traditional implementations of control with new methods, such as pattern-based scheduling of control tasks and adaptive sharing of bandwidth among control loops through orchestration of their execution patterns. In the current practice of control design, once the static execution schedule is prepared for control tasks, no further control-related optimization is attempted for improving the control performance. We introduce, for the first time, an algorithmic mechanism that re-engineers a recurrent control task by enforcing switching between multiple control laws, which are designed for compensating the non-uniform gaps between successive executions of the control task. We establish that such post-processing of control task schedules may potentially help in improving the combined control performance of the co-scheduled control loops that are executing on a shared platform. © 2020 ACM.",Embedded system; loop execution; performance; task scheduling,Automation; Computer applications; Combined control; Control design; Control laws; Control performance; Current practices; Electronic control units; Performance-driven; Post processing; Control systems
Efficient Parasitic-aware gm/ID-based Hybrid Sizing Methodology for Analog and RF Integrated Circuits,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097582016&doi=10.1145%2f3416946&partnerID=40&md5=ddaa37f3bd4e53e484a722c7a6af9e57,"As the primary second-order effect, parasitic issues have to be seriously addressed when synthesizing high-performance analog and RF integrated circuits (ICs). In this article, a two-phase hybrid sizing methodology for analog and RF ICs is proposed to take into account parasitic effect in the early design stage. It involves symbolic modeling and mixed-integer nonlinear programming (MINLP) in the first phase, and a many-objective evolutionary algorithm (many-OEA)-based sizing refiner in the second phase. With the aid of our proposed current density factor and piecewise curve fitting technique, the gm/ID concept, which is typically utilized to solve the analog circuit design problem, can provide theoretical support to our accurate symbolic modeling. Thus, the intrinsic and interconnect parasitics can be accurately considered in our work with moderate modeling effort. A variety of electrical, geometric, and parasitic (including parasitic mismatch) constraints can be conveniently integrated into our MINLP problem formulation. Moreover, numerical simulations are embedded into the many-OEA-based sizing phase, which is able to tackle floorplan co-optimization. With such dynamic floorplan variation, the parasitics accuracy can be sustained along the evolution. The experimental results demonstrate high efficacy of our proposed parasitic-aware hybrid sizing methodology. © 2020 ACM.",Analog and RF ICs; circuit modeling; circuit sizing; floorplan optimization; gm/ID; many-objective evolutionary algorithm; mixed-integer nonlinear programming; parasitic modeling,Beams and girders; Curve fitting; Evolutionary algorithms; Hybrid integrated circuits; Integer programming; Integrated circuit manufacture; Nonlinear programming; Timing circuits; Analog Circuit Design; Early design stages; Interconnect parasitics; Mixed-integer nonlinear programming; Piecewise curve-fitting; Problem formulation; RF integrated circuits; Second order effect; Integrated circuit design
Leakage-Aware Dynamic Thermal Management of 3D Memories,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097587059&doi=10.1145%2f3419468&partnerID=40&md5=0bc138b9e17ef53bc44e1d0d55caf2d1,"3D memory systems offer several advantages in terms of area, bandwidth, and energy efficiency. However, thermal issues arising out of higher power densities have limited their widespread use. While prior works have looked at reducing dynamic power through reduced memory accesses, in these memories, both leakage and dynamic power consumption are comparable. Furthermore, as the temperature rises, the leakage power increases, creating a thermal-leakage loop. We study the impact of leakage power on 3D memory temperature and propose turning OFF specific memory channels to meet thermal constraints. Data is migrated to a 2D memory before closing a 3D channel. We introduce an analytical model to assess the 2D memory delay and use the model to guide data migration decisions. The above strategy is referred to as FastCool and provides an improvement of 22%, 19%, and 32% on average (up to 57%, 72%, and 82%) in performance, memory energy, and energy-delay product (EDP), respectively, on different workloads consisting of SPEC CPU2006 benchmarks. We further propose a thermal management strategy named Energy-Efficient FastCool (EEFC), which improves upon FastCool by selecting the channels to be closed by considering temperature, leakage, access rate, and position of various 3D memory channels at runtime. Our experiments demonstrate that EEFC leads to an additional improvement of up to 30%, 30%, and 51% in performance, memory energy, and EDP compared to FastCool. Finally, we analyze the effects of process variations on the efficiency of the proposed FC and EEFC strategies. Variation in the manufacturing process causes changes in the leakage power and temperature profile. Since EEFC considers both while selecting channels for closure, it is more resilient to process variations and achieves a lower application execution time and memory energy compared to FastCool. © 2020 ACM.",3D memories; dynamic thermal management; energy efficiency; leakage power,Benchmarking; Energy efficiency; Application execution; Dynamic power consumption; Dynamic thermal management; Energy delay product; Manufacturing process; Temperature profiles; Thermal constraints; Thermal management strategy; Temperature control
SmartDR: Algorithms and Techniques for Fast Detailed Routing with Good Design Rule Handling,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097584619&doi=10.1145%2f3417133&partnerID=40&md5=55885a041b296839a5f01ea14bb7c49b,"Detailed routing is one of the most time-consuming steps of physical synthesis of integrated circuits. Also, it is very challenging due to the complexity of the design rules that the router must obey. In this article, we present SmartDR, a detailed routing system that focuses on good design rule handling and fast runtime. To attend these objectives, we propose a novel pin access approach and a fast design rule aware A∗-interval-based path search algorithm. The pin access method uses resource sharing ghost pin access paths with dynamic legalization check. We also propose a design rule check algorithm to detect thick metal shapes that are widely created using the proposed pin access method. The path search algorithm integrates design rule check on its core, handling many design rules that would not be possible to be solved by postprocessing. It is aware of the minimum area rule, the cut spacing of via cuts within the same path, and the via library. We also present a new technique to improve A∗-based path search in detailed routing. The technique makes the path search algorithm aware of the global routing guides, accelerating the search. Using ISPD 2018 Contest benchmarks, our experiments show that our router is superior to the state-of-the-art routers that were also tested using the same benchmarks. Our router has presented, on average, 77.6% less runtime, 73.5% less design rule violations, with respect to Dr. CU 2.0, which is the better of the compared routers. © 2020 ACM.",A; design rules; Detailed routing; ISPD 2018 Contest; path search; pin access,Routing algorithms; A; Access methods; Design rules; Detailed routing; Good designs; ISPD 2018 contest; Path search; Path search algorithms; Pin access; Runtimes; Learning algorithms
Towards Smarter Diagnosis: A Learning-based Diagnostic Outcome Previewer,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092618728&doi=10.1145%2f3398267&partnerID=40&md5=f79ef262eddaf2a00013463dd779e0c0,"Given the inherent perturbations during the fabrication process of integrated circuits that lead to yield loss, diagnosis of failing chips is a mitigating method employed during both yield ramping and high-volume manufacturing for yield learning. However, various uncertainties in the fabrication process bring a number of challenges, resulting in diagnosis with undesirable outcomes or low efficiency, including, for example, diagnosis failure, bad resolution, and extremely long runtime. It would therefore be very beneficial to have a comprehensive preview of diagnostic outcomes beforehand, which allows fail logs to be prioritized in a more reasonable way for smarter allocation of diagnosis resources. In this work, we propose a learning-based previewer, which is able to predict five aspects of diagnostic outcomes for a failing IC, including diagnosis success, defect count, failure type, resolution, and runtime magnitude. The previewer consists of three classification models and one regression model, where Random Forest classification and regression are used. Experiments on a 28 nm test chip and a high-volume 90 nm part demonstrate that the predictors can provide accurate prediction results, and in a virtual application scenario the overall previewer can bring up to 9× speed-up for the test chip and 6× for the high-volume part. © 2020 ACM.",diagnosis economics; diagnosis preview; Random forest,Decision trees; Regression analysis; Accurate prediction; Classification models; Fabrication process; High volume manufacturing; Random forest classification; Regression model; Virtual application; Yield learning; Integrated circuits
Improving FPGA-Based Logic Emulation Systems through Machine Learning,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092621108&doi=10.1145%2f3399595&partnerID=40&md5=8a177e8df6f363a6b4065bf4090f15fc,"We present a machine learning (ML) framework to improve the use of computing resources in the FPGA compilation step of a commercial FPGA-based logic emulation flow. Our ML models enable highly accurate predictability of the final place and route design qualities, runtime, and optimal mapping parameters. We identify key compilation features that may require aggressive compilation efforts using our ML models. Experiments based on our large-scale database from an industry's emulation system show that our ML models help reduce the total number of jobs required for a given netlist by 33%. Moreover, our job scheduling algorithm based on our ML model reduces the overall time to completion of concurrent compilation runs by 24%. In addition, we propose a new method to compute ""recommendations""from our ML model to perform re-partitioning of difficult partitions. Tested on a large-scale industry system on chip design, our recommendation flow provides additional 15% compile time savings for the entire system on chip. To exploit our ML model inside the time-critical multi-FPGA partitioning step, we implement it in an optimized multi-threaded representation. © 2020 ACM.",emulation flow optimization with machine learning; Field programmable gate array; SoC verification,Computer circuits; Field programmable gate arrays (FPGA); Large scale systems; Machine learning; Programmable logic controllers; System-on-chip; Turing machines; Computing resource; Emulation system; Industry systems; Job scheduling algorithms; Large-scale database; Logic emulation systems; Logic emulations; Multi-fpga partitioning; Integrated circuit design
Wire load oriented analog routing with matching constraints,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092799623&doi=10.1145%2f3403932&partnerID=40&md5=a1b33e438cc5ed0b50ba658cc44ada9f,"As design complexity is increased exponentially, electronic design automation (EDA) tools are essential to reduce design efforts. However, the analog layout design has still been done manually for decades because it is a sensitive and error-prone task. Tool-generated layouts are still not well-accepted by analog designers due to the performance loss under non-ideal effects. Most previous works focus on adding more layout constraints on the analog placement. Routing the nets is thus considered as a trivial step that can be done by typical digital routing methodology, which is to use vias to connect every horizontal and vertical lines. Those extra vias will significantly increase the wire loads and degrade the circuit performance. Therefore, in this article, a wire load oriented analog routing methodology is proposed to reduce the number of layer changing of each routing net. Wire load is considered in the optimization goal as well as the wire length to keep the circuit performance after layout, while the analog layout constraints like symmetry and length matching are still satisfied during routing. As shown in the experimental results, this approach significantly reduces the wire load and performance loss after layout with little overhead on wire length. © 2020 Association for Computing Machinery.",Analog layout; Physical design automation,Computer aided design; Electric network analysis; Circuit performance; Design complexity; Electronic design automation tools; Error prone tasks; Layout constraint; Matching constraints; Optimization goals; Performance loss; Wire
NeuPow: A CAD Methodology for High-level Power Estimation Based on Machine Learning,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092661884&doi=10.1145%2f3388141&partnerID=40&md5=878bdce2a80c5ad1ad34e6008748e01c,"In this article, we present a new, simple, accurate, and fast power estimation technique that can be used to explore the power consumption of digital system designs at an early design stage. We exploit the machine learning techniques to aid the designers in exploring the design space of possible architectural solutions, and more specifically, their dynamic power consumption, which is application-, technology-, frequency-, and data-stimuli dependent. To model the power and the behavior of digital components, we adopt the Artificial Neural Networks (ANNs), while the final target technology is Application Specific Integrated Circuit (ASIC). The main characteristic of the proposed method, called NeuPow, is that it relies on propagating the signals throughout connected ANN models to predict the power consumption of a composite system. Besides a baseline version of the NeuPow methodology that works for a given predefined operating frequency, we also derive an upgraded version that is frequency-aware, where the same operating frequency is taken as additional input by the ANN models. To prove the effectiveness of the proposed methodology, we perform different assessments at different levels. Moreover, technology and scalability studies have been conducted, proving the NeuPow robustness in terms of these design parameters. Results show a very good estimation accuracy with less than 9% of relative error independently from the technology and the size/layers of the design. NeuPow is also delivering a speed-up factor of about 84× with respect to the classical power estimation flow. © 2020 ACM.",estimation; methodology; modeling; neural networks; Power consumption,Electric power utilization; Integrated circuit design; Machine learning; Architectural solutions; Digital components; Digital system design; Dynamic power consumption; Early design stages; High-level power estimation; Machine learning techniques; Operating frequency; Computer aided design
MNFTL: An efficient flash translation layer for MLC NAND flash memory,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092765035&doi=10.1145%2f3398037&partnerID=40&md5=c828fba4745ef1d6c663d309c4590b82,"The write constraints of Multi-Level Cell (MLC) NAND flash memory make most of the existing flash translation layer (FTL) schemes inefficient or inapplicable. In this article, we solve several fundamental problems in the design of MLC flash translation layer. The objective is to reduce the garbage collection overhead to reduce the average system response time. We make the key observation that the valid pages copy is the essential garbage collection overhead. Based on this observation, we propose two approaches, namely, concentrated mapping and postponed reclamation, to effectively reduce the valid pages copy. Besides, we propose a progressive garbage collection that can well utilize the system idle time to reclaim more spaces. We conduct a series of experiments on an embedded developing board with a set of benchmarks. The experimental results show that our scheme can achieve a significant reduction in the average system response time compared with the previous work. © 2020 Association for Computing Machinery.",Address mapping; Flash translation layer; Garbage collection; MLC NAND flash memory,Memory architecture; NAND circuits; Refuse collection; Response time (computer systems); Average system; Flash translation layer; Garbage collection; Idle time; Multi level cell (MLC); NAND flash memory; Flash memory
Introduction to the Special Issue on Machine Learning for CAD,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092632918&doi=10.1145%2f3410864&partnerID=40&md5=0b116e3c676284a60436524f8205c02d,[No abstract available],,
Machine Learning Approach for Fast Electromigration Aware Aging Prediction in Incremental Design of Large Scale On-chip Power Grid Network,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092621114&doi=10.1145%2f3399677&partnerID=40&md5=1667161e0730f59b97ca85137bf54783,"With the advancement of technology nodes, Electromigration (EM) signoff has become increasingly difficult, which requires a considerable amount of time for an incremental change in the power grid (PG) network design in a chip. The traditional Black's empirical equation and Blech's criterion are still used for EM assessment, which is a time-consuming process. In this article, for the first time, we propose a machine learning (ML) approach to obtain the EM-aware aging prediction of the PG network. We use neural network - based regression as our core ML technique to instantly predict the lifetime of a perturbed PG network. The performance and accuracy of the proposed model using neural network are compared with the well-known standard regression models. We also propose a new failure criterion based on which the EM-aging prediction is done. Potential EM-affected metal segments of the PG network is detected by using a logistic-regression - based classification ML technique. Experiments on different standard PG benchmarks show a significant speedup for our ML model compared to the state-of-the-art models. The predicted value of MTTF for different PG benchmarks using our approach is also better than some of the state-of-the-art MTTF prediction models and comparable to the other accurate models. © 2020 ACM.",Electromigration; machine learning; MTTF; neural network; power grid network; regression; reliability,Electric power transmission networks; Electromigration; Forecasting; Logistic regression; Machine learning; Predictive analytics; Turing machines; Empirical equations; Incremental changes; Incremental designs; Machine learning approaches; On-chip power grids; Prediction model; State of the art; Technology nodes; Neural networks
A locality optimizer for loop-dominated applications based on reuse distance analysis,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092775025&doi=10.1145%2f3398189&partnerID=40&md5=9b2665e7c0c0ee17f8b4cf0f263bb465,"Source code optimization can heavily improve software code implementation quality while still being complementary to conventional compilers' optimizations. Source code analysis tools are very useful in supporting source code optimization. This article discusses MemAssist, a source-level optimization environment for semi-automatic locality optimization of loop-dominated code. MemAssist applies reuse distance analysis and a relevant optimization algorithm to explore the design space. It generates a set of suggestions for locality optimizing loop transformations that reduce data cache miss rate and execution time. MemAssist has been used to optimize a number of applications. Experimental results show that MemAssist leads to cache miss rate reduction at all cache layers, memory accesses reduction by up to 42%, and to a speedup of up to three times. Therefore, MemAssist can be used for efficient early-stage software optimization leading to development effort and time reduction. © 2020 Association for Computing Machinery.",Data locality optimization; MATLAB-to-C; Reuse distance analysis; Source-to-source optimization,Computer programming languages; Cache miss rates; Locality optimization; Loop transformation; Optimization algorithms; Optimization environments; Software optimization; Source code analysis; Source code optimization; Codes (symbols)
Adversarial Perturbation Attacks on ML-based CAD: A Case Study on CNN-based Lithographic Hotspot Detection,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092656345&doi=10.1145%2f3408288&partnerID=40&md5=57b649a5fbf68c1ea01f491e31c9a7cd,"There is substantial interest in the use of machine learning (ML)-based techniques throughout the electronic computer-aided design (CAD) flow, particularly those based on deep learning. However, while deep learning methods have surpassed state-of-the-art performance in several applications, they have exhibited intrinsic susceptibility to adversarial perturbations - small but deliberate alterations to the input of a neural network, precipitating incorrect predictions. In this article, we seek to investigate whether adversarial perturbations pose risks to ML-based CAD tools, and if so, how these risks can be mitigated. To this end, we use a motivating case study of lithographic hotspot detection, for which convolutional neural networks (CNN) have shown great promise. In this context, we show the first adversarial perturbation attacks on state-of-the-art CNN-based hotspot detectors; specifically, we show that small (on average 0.5% modified area), functionality preserving, and design-constraint-satisfying changes to a layout can nonetheless trick a CNN-based hotspot detector into predicting the modified layout as hotspot free (with up to 99.7% success in finding perturbations that flip a detector's output prediction, based on a given set of attack constraints). We propose an adversarial retraining strategy to improve the robustness of CNN-based hotspot detection and show that this strategy significantly improves robustness (by a factor of ∼3) against adversarial attacks without compromising classification accuracy. © 2020 ACM.",adversarial perturbations; lithographic hotspot detection; ML-based CAD; security,Computer aided instruction; Convolutional neural networks; Deep learning; Forecasting; Learning systems; CAD tool; Classification accuracy; Design constraints; Electronic computer-aided design; Hotspot detections; Learning methods; On state; State-of-the-art performance; Computer aided design
Fine-grained Adaptive Testing Based on Quality Prediction,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092675623&doi=10.1145%2f3385261&partnerID=40&md5=513c3022bb74f1a5c8da88b6de9dc9a8,"The ever-increasing complexity of integrated circuits inevitably leads to high test cost. Adaptive testing provides an effective solution for test-cost reduction; this testing framework selects the important test items for each set of chips. However, adaptive testing methods designed for digital circuits are coarse-grained, and they are targeted only at systematic defects. To incorporate fabrication variations and random defects in the testing framework, we propose a fine-grained adaptive testing method based on machine learning. We use the parametric test results from the previous stages of test to train a quality-prediction model for use in subsequent test stages. Next, we partition a given lot of chips into two groups based on their predicted quality. A test-selection method based on statistical learning is applied to the chips with high predicted quality. An ad hoc test-selection method is proposed and applied to the chips with low predicted quality. Experimental results using a large number of fabricated chips and the associated test data show that to achieve the same defect level as in prior work on adaptive testing, the fine-grained adaptive testing method reduces test cost by 90% for low-quality chips and up to 7% for all the chips in a lot. © 2020 ACM.",Adaptive testing; quality prediction; test selection,Cost reduction; Defects; Predictive analytics; Effective solution; Fabricated chips; Quality prediction; Quality prediction models; Statistical learning; Systematic defects; Test cost reduction; Testing framework; Learning systems
Reconfigurable network-on-chip security architecture,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092799124&doi=10.1145%2f3406661&partnerID=40&md5=e231d57d8c4709ea52733e6cb1815c4f,"Growth of the Internet-of-things has led to complex system-on-chips (SoCs) being used in the edge devices in IoT applications. The increased complexity is demanding designers to consider several critical factors, such as dynamic requirement changes, long application life, mass production, and tight time-to-market deadlines. These requirements lead to more complex security concerns. SoC manufacturers outsource some of the intellectual property cores integrated on the SoC to untrusted third-party vendors. The untrusted intellectual properties can contain malicious implants, which can launch attacks using the resources provided by the on-chip interconnection network, commonly known as the network-on-chip (NoC). Existing efforts on securing NoC have considered lightweight encryption, authentication, and other attack detection mechanisms such as denial-of-service and buffer overflows. Unfortunately, these approaches focus on designing statically optimized security solutions. As a result, they are not suitable for many IoT systems with long application life and dynamic requirement changes. There is a critical need to design reconfigurable security architectures that can be dynamically tuned based on changing requirements. In this article, we propose a tier-based reconfigurable security architecture that can adapt to different use-case scenarios. We explore how to design an efficient reconfigurable architecture that can support three popular NoC security mechanisms (encryption, authentication, and denial-of-service attack detection and localization) and implement suitable dynamic reconfiguration techniques. We evaluate our proposed framework by running standard benchmarks enabling different tiers of security and provide a comprehensive analysis of how different levels of security can affect application performance, energy efficiency, and area overhead. © 2020 Association for Computing Machinery.",Hardware security; Machine learning,Authentication; Benchmarking; Complex networks; Cryptography; Denial-of-service attack; Dynamic models; Dynamics; Energy efficiency; Interconnection networks (circuit switching); Internet of things; Lead compounds; Memory architecture; Network architecture; Network security; Network-on-chip; Programmable logic controllers; Reconfigurable architectures; Servers; Application performance; Comprehensive analysis; Dynamic reconfiguration techniques; Lightweight encryption; Network-on-chip(NoC); On-chip interconnection network; Reconfigurable network; Security Architecture; Integrated circuit design
Machine Learning Approaches for Efficient Design Space Exploration of Application-Specific NoCs,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092629764&doi=10.1145%2f3403584&partnerID=40&md5=3cb681220a78d340422319fc6ea71e6e,"In many Multi-Processor Systems-on-Chip (MPSoCs), traffic between cores is unbalanced. This motivates the use of an application-specific Network-on-Chip (NoC) that is customized and can provide a high performance at low cost in terms of power and area. However, finding an optimized application-specific NoC architecture is a challenging task due to the huge design space. This article proposes to apply machine learning approaches for this task. Using graph rewriting, the NoC Design Space Exploration (DSE) is modelled as a Markov Decision Process (MDP). Monte Carlo Tree Search (MCTS), a technique from reinforcement learning, is used as search heuristic. Our experimental results show that - with the same cost function and exploration budget - MCTS finds superior NoC architectures compared to Simulated Annealing (SA) and a Genetic Algorithm (GA). However, the NoC DSE process suffers from the high computation time due to expensive cycle-accurate SystemC simulations for latency estimation. This article therefore additionally proposes to replace latency simulation by fast latency estimation using a Recurrent Neural Network (RNN). The designed RNN is sufficiently general for latency estimation on arbitrary NoC architectures. Our experiments show that compared to SystemC simulation, the RNN-based latency estimation offers a similar speed-up as the widely used Queuing Theory (QT). Yet, in terms of estimation accuracy and fidelity, the RNN is superior to QT, especially for high-traffic scenarios. When replacing SystemC simulations with the RNN estimation, the obtained solution quality decreases only slightly, whereas it suffers significantly when QT is used. © 2020 ACM.",Application-specific networks-on-chip; design space exploration; Monte-Carlo-tree search; recurrent neural network,Budget control; Computation theory; Cost functions; Costs; Genetic algorithms; Heuristic algorithms; Learning systems; Markov processes; Monte Carlo methods; Network architecture; Network-on-chip; Queueing theory; Recurrent neural networks; Reinforcement learning; Simulated annealing; Application specific network on chip; Efficient design space explorations; Machine learning approaches; Markov Decision Processes; Monte Carlo tree search (MCTS); Multi processor systems; Optimized application; Recurrent neural network (RNN); Integrated circuit design
PREASC: Automatic Portion Resilience Evaluation for Approximating SystemC-based Designs Using Regression Analysis Techniques,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090417702&doi=10.1145%2f3388140&partnerID=40&md5=b410ea5b9dd2732c4db75038fdc36639,"The increasing functionality of electronic systems due to the constant evolution of the market requirements makes the non-functional aspects of such systems (e.g., energy consumption, area overhead, or performance) a major concern in the design process. Approximate computing is a promising way to optimize these criteria by trading accuracy within acceptable limits. Since the cost of applying significant structural changes to a given design increases with the stage of development, the optimization solution needs to be incorporated into the design as early as possible. For the early design entry, modeling hardware at the Electronic System Level (ESL) using the SystemC language is nowadays widely used in the industry. To apply approximation techniques to optimize a given SystemC design, designers need to know which parts of the design can be approximated. However, identifying these parts is a crucial and non-trivial starting point of approximate computing, as the incorrect detection of even one critical part as resilient may result in an unacceptable output. This usually requires a significant programming effort by designers, especially when exploring the design space manually. In this article, we present PREASC, a fully automated framework to identify the resilience portions of a given SystemC design. PREASC is based on a combination of static and dynamic analysis methods along with regression analysis techniques (a fast machine learning method providing an accurate function estimation). Once the resilient portions are identified, an approximation degree analysis is performed to determine the maximum error rate that each resilient portion can tolerate. Subsequently, the maximum number of resilient portions that can be approximated at the same time are reported to designers at different granularity levels. The effectiveness of our approach is evaluated using several standard SystemC benchmarks from various domains. © 2020 ACM.",approximate computing; Clang; dynamic analysis; machin learning technique; regression analysis; Resilience evaluation; static analysis,Commerce; Electronics industry; Energy utilization; Learning systems; Modeling languages; Regression analysis; System theory; Approximation degrees; Approximation techniques; Different granularities; Electronic system level; Machine learning methods; Non-functional aspects; Optimization solution; Static and dynamic analysis; Logic design
LDE-aware analog layout migration with OPC-inclusive routing,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092763239&doi=10.1145%2f3398190&partnerID=40&md5=e1afe99bc1b14fba47b1f27efb13d714,"Performance degradation in analog circuits due to layout dependent effects (LDEs) has become increasingly challenging in advanced technologies. To address this issue, LDEs have to be seriously considered as performance constraints in the physical design process. In this article, we have proposed an innovative LDE-aware retargeting methodology for analog layout migration from old technologies to new ones with LDEs optimized for performance preservation. The LDE constraints, which are first identified with the aid of a specialized sensitivity analysis scheme, are satisfied during the layout migration process. Moreover, optical proximity correction (OPC), as one of the most popular resolution enhancement techniques for subwavelength lithography in modern nanometer technology manufacturing, is also included in this study. We have developed an OPC-inclusive ILP-based analog router to route electrical nets for improving image fidelity of the final layout while the routability and other analog constraints are respected in the meantime. The experimental results show our proposed layout migration methodology along with the routing scheme is able to retarget analog layouts with better circuit performance and finer image quality compared to the previous works. © 2020 Association for Computing Machinery.",Integer linear programming; Layout dependent effect (LDE); Layout migration; Optical proximity correction (OPC),Photolithography; Sensitivity analysis; Advanced technology; Circuit performance; Nanometer technology; Optical proximity corrections; Performance constraints; Performance degradation; Resolution enhancement technique; Subwavelength lithography; Image enhancement
Machine Learning for Congestion Management and Routability Prediction within FPGA Placement,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092666240&doi=10.1145%2f3373269&partnerID=40&md5=ed08f6aec3aa05826a606bf0bb4350c1,"Placement for Field Programmable Gate Arrays (FPGAs) is one of the most important but time-consuming steps for achieving design closure. This article proposes the integration of three unique machine learning models into the state-of-the-art analytic placement tool GPlace3.0 with the aim of significantly reducing placement runtimes. The first model, MLCong, is based on linear regression and replaces the computationally expensive global router currently used in GPlace3.0 to estimate switch-level congestion. The second model, DLManage, is a convolutional encoder-decoder that uses heat maps based on the switch-level congestion estimates produced by MLCong to dynamically determine the amount of inflation to apply to each switch to resolve congestion. The third model, DLRoute, is a convolutional neural network that uses the previous heat maps to predict whether or not a placement solution is routable. Once a placement solution is determined to be routable, further optimization may be avoided, leading to improved runtimes. Experimental results obtained using 372 benchmarks provided by Xilinx Inc. show that when all three models are integrated into GPlace3.0, placement runtimes decrease by an average of 48%. © 2020 ACM.",congestion; field programmable gate array; heterogeneous; Placement; routing-aware; UltraScale architecture,Convolution; Convolutional neural networks; Machine learning; Predictive analytics; Analytic placement; Congestion management; Convolutional encoders; Design closure; Global routers; Machine learning models; State of the art; Three models; Field programmable gate arrays (FPGA)
Interval arithmetic and self-similarity based RTL input vector control for datapath leakage minimization,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092755041&doi=10.1145%2f3408061&partnerID=40&md5=92980a969e84e365f00b9c5cb636bcd8,"With technology scaling, subthreshold leakage has dominated the overall power consumption in a design. Input vector control is an effective technique to minimize subthreshold leakage. Low leakage input vector determination is not often possible due to large design space and simulation time. Similarly, applying an appropriate minimum leakage vector (MLV) to each Register Transfer Level (RTL) module instance in a design often results in a low leakage state with significant area overhead. In this work, we propose a top-down and bottom-up approach for propagating the input vector interval to identify low leakage input vector at primary inputs of an RTL datapath. For each module, via Monte Carlo simulation, we identify a set of MLV intervals such that maximum leakage is within (say) 10% of the lowest leakage points. As the module bit width increases, exhaustive simulation to find the low leakage vector is not feasible. Further, we need to uniformly search the entire input space to obtain as many low leakage intervals as possible. Based on empirical observations, we observe self-similarity in the subthreshold leakage distribution of adder/multiplier modules with highly regular bit-slice architectures when input space is partitioned into smaller cells. This property enables the uniform search of low leakage vectors in the entire input space where the time taken for characterization increases linearly with the module size. We further process the reduced interval set with simulated annealing to arrive at the best low-leakage vector at the primary inputs. We also propose to reduce area overhead (in some cases to 0%) by choosing Primary Input (PI) MLVs such that resultant inputs to internal nodes are also MLVs. Compared to existing work, experimental results for DSP filters simulated in 16nm technology demonstrated leakage savings of 93.6% and 89.2% for top-down and bottom-up approaches with no area overhead. © 2020 Association for Computing Machinery.",Interval arithmetic; Monte Carlo simulation; Self similarity; Subthreshold leakage,Leakage currents; Monte Carlo methods; Simulated annealing; Vector control (Electric machinery); Vectors; Bottom up approach; Exhaustive simulation; Input vector control; Interval arithmetic; Leakage minimization; Minimum leakage vector; Register transfer level; Sub-threshold leakage; Vector spaces
Predicting Memory Compiler Performance Outputs Using Feed-forward Neural Networks,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092653363&doi=10.1145%2f3385262&partnerID=40&md5=4ad4752b6544390ba1628cac494baa5a,"Typical semiconductor chips include thousands of mostly small memories. As memories contribute an estimated 25% to 40% to the overall power, performance, and area (PPA) of a product, memories must be designed carefully to meet the system's requirements. Memory arrays are highly uniform and can be described by approximately 10 parameters depending mostly on the complexity of the periphery. Thus, to improve PPA utilization, memories are typically generated by memory compilers. A key task in the design flow of a chip is to find optimal memory compiler parametrizations that, on the one hand, fulfill system requirements while, on the other hand, they optimize PPA. Although most compiler vendors also provide optimizers for this task, these are often slow or inaccurate. To enable efficient optimization in spite of long compiler runtimes, we propose training fully connected feed-forward neural networks to predict PPA outputs given a memory compiler parametrization. Using an exhaustive search-based optimizer framework that obtains neural network predictions, PPA-optimal parametrizations are found within seconds after chip designers have specified their requirements. Average model prediction errors of less than 3%, a decision reliability of over 99%, and productive usage of the optimizer for successful, large volume chip design projects illustrate the effectiveness of the approach. © 2020 ACM.",Electronic design automation; machine learning; memory compiler; neural networks; PPA optimization; supervised learning,Forecasting; Program compilers; Chip designers; Decision reliability; Memory compilers; Neural network predictions; Optimal memory; Parametrizations; Semiconductor chips; System requirements; Feedforward neural networks
Energy-efficient GPU L2 cache design using instruction-level data locality similarity,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092786879&doi=10.1145%2f3408060&partnerID=40&md5=80c9fca893ef80bce8534ec70ace97bf,"This article presents a novel energy-efficient cache design for massively parallel, throughput-oriented architectures like GPUs. Unlike L1 data cache on modern GPUs, L2 cache shared by all of the streaming multiprocessors is not the primary performance bottleneck, but it does consume a large amount of chip energy. We observe that L2 cache is significantly underutilized by spending 95.6% of the time storing useless data. If such “dead time” on L2 is identified and reduced, L2's energy efficiency can be drastically improved. Fortunately, we discover that the SIMT programming model of GPUs provides a unique feature among threads: instruction-level data locality similarity, which can be used to accurately predict the data re-reference counts at L2 cache block level. We propose a simple design that leverages this Locality Similarity to build an energy-efficient GPU L2 Cache, named LoSCache. Specifically, LoSCache uses the data locality information from a small group of cooperative thread arrays to dynamically predict the L2-level data re-reference counts of the remaining cooperative thread arrays. After that, specific L2 cache lines can be powered off if they are predicted to be “dead” after certain accesses. Experimental results on a wide range of applications demonstrate that our proposed design can significantly reduce the L2 cache energy by an average of 64% with only 0.5% performance loss. In addition, LoSCache is cost effective, independent of the scheduling policies, and compatible with the state-of-the-art L1 cache designs for additional energy savings. © 2020 Association for Computing Machinery.",Cache; Energy-efficiency; GPU; Locality similarity,Cache memory; Cost effectiveness; Graphics processing unit; Program processors; Energy-efficient caches; Instruction-level; Massively parallels; Performance bottlenecks; Performance loss; Programming models; Scheduling policies; Streaming multiprocessors; Energy efficiency
Machine Learning-based Defect Coverage Boosting of Analog Circuits under Measurement Variations,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092624550&doi=10.1145%2f3408063&partnerID=40&md5=469182293a2410fa34898ff115ee2411,"Safety-critical and mission-critical systems, such as airplanes or (semi-)autonomous cars, are relying on an ever-increasing number of embedded integrated circuits. Consequently, there is a need for complete defect coverage during the testing of these circuits to guarantee their functionality in the field. In this context, reducing the escape rate of defects during production testing is crucial, and significant progress has been made to this end. However, production testing using automatic test equipment is subject to various measurement parasitic variations, which may have a negative impact on the testing procedure and therefore limit the final defect coverage. To tackle this issue, this article proposes an improved test flow targeting increased analog defect coverage, both at the system and block levels, by analyzing and improving the coverage of typical functional and structural tests under these measurement variations. To illustrate the flow, the technique of inserting a pseudo-random signal at available circuit nodes and applying machine learning techniques to its response is presented. A DC-DC converter, derived from an industrial product, is used as a case study to validate the flow. In short, results show that system-level tests for the converter suffer strongly from the measurement variations and are limited to just under 80% coverage, even when applying the proposed test flow. Block-level testing, however, can achieve only 70% fault coverage without improvements but is able to consistently achieve 98% of fault coverage at a cost of at most 2% yield loss with the proposed machine learning-based boosting technique. © 2020 ACM.",AMS IC test; Machine learning for test; test under measurements variations,Analog circuits; Automatic testing; DC-DC converters; Defects; Embedded systems; Equipment testing; Machine learning; Safety engineering; Timing circuits; Automatic test equipment; Industrial product; Machine learning techniques; Mission critical systems; Production testing; Pseudorandom signals; System-level test; Testing procedure; Adaptive boosting
Editorial: A message from the new editor-in-chief,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092778874&doi=10.1145%2f3419376&partnerID=40&md5=52b1d8da775592fa3d085b22cee55b74,[No abstract available],,
Multi-Fidelity Surrogate-Based Optimization for Electromagnetic Simulation Acceleration,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092661675&doi=10.1145%2f3398268&partnerID=40&md5=13717032c7a7e5e94083992e90a32be6,"As circuits' speed and frequency increase, fast and accurate capture of the details of the parasitics in metal structures, such as inductors and clock trees, becomes more critical. However, conducting high-fidelity 3D electromagnetic (EM) simulations within the design loop is very time consuming and computationally expensive. To address this issue, we propose a surrogate-based optimization methodology flow, namely multi-fidelity surrogate-based optimization with candidate search (MFSBO-CS), which integrates the concept of multi-fidelity to reduce the full-wave EM simulation cost in analog/RF simulation-based optimization problems. To do so, a statistical co-kriging model is adapted as the surrogate to model the response surface, and a parallelizable perturbation-based adaptive sampling method is used to find the optima. Within the proposed method, low-fidelity fast RC parasitic extraction tools and high-fidelity full-wave EM solvers are used together to model the target design and then guide the proposed adaptive sample method to achieve the final optimal design parameters. The sampling method in this work not only delivers additional coverage of design space but also helps increase the accuracy of the surrogate model efficiently by updating multiple samples within one iteration. Moreover, a novel modeling technique is developed to further improve the multi-fidelity surrogate model at an acceptable additional computation cost. The effectiveness of the proposed technique is validated by mathematical proofs and numerical test function demonstration. In this article, MFSBO-CS has been applied to two design cases, and the result shows that the proposed methodology offers a cost-efficient solution for analog/RF design problems involving EM simulation. For the two design cases, MFSBO-CS either reaches comparably or outperforms the optimization result from various Bayesian optimization methods with only approximately one- to two-thirds of the computation cost. © 2020 ACM.",electromagnetic simulation; multi-fidelity; Statistical machine learning; surrogate-based optimization,Functions; Iterative methods; Optimization; Sampling; Adaptive sampling methods; Bayesian optimization; Mathematical proof; Modeling technique; Optimal design parameters; Parasitic extraction; Simulation-based optimizations; Surrogate-based optimization; Electromagnetic simulation
A Hierarchical HVAC Control Scheme for Energy-aware Smart Building Automation,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092701619&doi=10.1145%2f3393666&partnerID=40&md5=d85c918f71e993aee7a8fde8077949fa,"Heating ventilation and air conditioning (HVAC) systems usually account for the highest percentage of overall energy usage in large-sized smart building infrastructures. The performance of HVAC control systems for large buildings strongly depend on the outside environment, building architecture, and (thermal) zone usage pattern of the building. In large buildings, HVAC system with multiple air handling units (AHUs) is required to fulfill the cooling/heating requirements. In the present work, we propose an energy-aware building resource allocation and economic model predictive control (eMPC) framework for multi-AHU-based HVAC system. The energy consumption of a multi-AHU-based HVAC system significantly depends on how long the AHUs are running, which again is governed by the zone usage demands. Our approach comprises a two-step hierarchical technique where we first minimize the running time of AHUs by suitably allocating building resources (thermal zones) to usage demands for zones. Next, we formulate a finite receding horizon control problem for trading off energy consumption against thermal comfort during HVAC operations. Given a high-level building specification and usage demand, our computer-aided design framework generates building thermal models, allocates usage demands, formulates the control scheme, and simulates it to generate power consumption statistics for the given building with usage demands. We believe that the proposed framework will help in early analysis during the design phase of energy-aware building architecture and HVAC control. The framework can also be useful from a building operator point of view for energy-aware HVAC control as well as for satisfying smart grid demand-response events by HVAC system peak power reduction through automated control actions.  © 2020 ACM.",CAD for green building energy automation; economic model predictive control; energy-aware HVAC control; Smart buildings,Air conditioning; Architectural design; Computer aided design; Economics; Electric power transmission networks; Energy utilization; Green computing; HVAC; Intelligent buildings; Model predictive control; Power management; Smart power grids; Structural design; Air-handling unit; Building automation; Building Thermal models; Consumption statistics; Energy-aware Buildings; Heating ventilation and air conditioning; Hierarchical techniques; Receding horizon control; Electric power system control
Machine Learning Assisted PUF Calibration for Trustworthy Proof of Sensor Data in IoT,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092715985&doi=10.1145%2f3393628&partnerID=40&md5=13bfc953af189d453382410401622059,"Remote integrity verification plays a paramount role in resource-constraint devices owing to emerging applications such as Internet-of-Things (IoT), smart homes, e-health, and so on. The concept of Virtual Proof of Reality (VPoR) proposed by Rührmair et al. in 2015 has come up with a Sense-Prove-Validate framework for integrity checking of abundant data generated from billions of connected sensors. It leverages the unreliability factor of Physically Unclonable Functions (PUFs) with respect to ambient parameter variations such as temperature, supply voltages, and so on, and claims to prove the authenticity of the sensor data without using any explicit keys. The state-of-the-art authenticated sensing protocols majorly lack in limited authentications and huge storage overhead. These protocols also assume that the behaviour of the PUF instances varies unpredictably for different levels of ambient factors, which in turn makes them hard to go beyond the theoretical concept. We address these issues in this work1 and propose a Machine Learning (ML) assisted PUF calibration scheme to predict the Challenge-Response Pair (CRP) behaviour of a PUF instance in a specific environment, given the CRP behaviour in a pivot environment. Here, we present a new class of authenticated sensing protocols where we leverage the beneficence of ML techniques to validate the authenticity and integrity of sensor data over ambient factor variations. The scheme also reduces the storage complexity of the verifier from O(p ∗ K ∗ l ∗ (c + r)) to O(p ∗ l ∗(c + r)), where p is the number of PUF instances deployed in the framework, l is the number of challenge-response pairs used for authentication, c is the bit lengths of the challenge, r is the response bits of the PUF, and K is the number of levels of ambient factor variations. The scheme alleviates the issue of limited authentication as well, whereby every CRP is used only once for authentication and then deleted from the database. To validate the proposed protocol through actual experiments on FPGA, we propose 5-4 Double Arbiter PUF, which is an extension of Double Arbiter PUFs (DAPUFs) as this design is more suited for FPGA, and implement it on Xilinx Artix-7 FPGAs. We characterise the proposed PUF instance from -20°C to 80°C and use Random Forest - based ML technique to generate a soft model of the PUF instance. This model is further used by the verifier to authenticate the actual PUF circuit. According to the FPGA-based validation, the proposed protocol with DAPUF can be effectively used to authenticate sensor devices across wide variations of temperature values.  © 2020 ACM.",Authenticated sensing; double arbiter PUFs; FPGA; physically unclonable functions (PUFs); reliability; virtual proofs (VPs),Authentication; Automation; Calibration; Cryptography; Decision trees; Digital storage; Field programmable gate arrays (FPGA); Intelligent buildings; Machine learning; Turing machines; Calibration schemes; Challenge-response pair; Challenge-response pairs (CRP); Emerging applications; Integrity verifications; Internet of Things (IOT); Physically unclonable functions; Resource-constraint devices; Internet of things
Soft-HaT: Software-Based Silicon Reprogramming for Hardware Trojan Implementation,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092744992&doi=10.1145%2f3396521&partnerID=40&md5=c928b49a7125844d63016bb5a0761c7b,"A hardware Trojan is a malicious modification to an integrated circuit (IC) made by untrusted third-party vendors, fabrication facilities, or rogue designers. Although existing hardware Trojans are designed to be stealthy, they can, in theory, be detected by post-manufacturing and acceptance tests due to their physical connections to IC logic. Manufacturing tests can potentially trigger the Trojan and propagate its payload to an output. Even if the Trojan is not triggered, the physical connections to the IC can enable detection due to additional side-channel activity (e.g., power consumption). In this article, we propose a novel hardware Trojan design, called Soft-HaT, which only becomes physically connected to other IC logic after activation by a software program. Using an electrically programmable fuse (E-fuse), the hardware can be ""re-programmed""remotely. We illustrate how Soft-HaT can be used for offensive applications in system-on-chips. Examples of Soft-HaT attacks are demonstrated on an open source system-on-chip (OrpSoC) and implemented in Virtex-7 FPGA to show their efficacy in terms of stealthiness.  © 2020 ACM.",Hardware Trojan; kill switch; unauthorized memory accesses,Acceptance tests; Computer circuits; Hardware security; Open source software; Open systems; Semiconductor device manufacture; Silicon; System-on-chip; Manufacturing tests; Open source system; Physical connections; Side-channel; Software program; Third party vendors; Trojans; Malware
Generating Representative Test Sequences from Real Workload for Minimizing DRAM Verification Overhead,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092700080&doi=10.1145%2f3391891&partnerID=40&md5=ff5ea21aa08ccf04fef0b538876d3ec5,"Dynamic Random Access Memory (DRAM) standards have evolved for higher bandwidth, larger capacity, and lower power consumption, so their specifications have become complicated to satisfy the design goals. These complex implementations have significantly increased the test time overhead for design verification; thus, a tremendous amount of command sequences are used. However, since the sequences generated by real machines or memory simulators are the results of scheduling for high performance, they result in low test coverage with repetitive patterns. Eventually, various workloads should be applied to increase the coverage, but this approach incurs significant test time overhead. A few preliminary studies have been proposed to generate predefined or random sequences to cover various test cases or increase test coverage. However, they have limitations in representing various memory behaviors of real workloads. In this article, we define a performance metric for estimating the test coverage when using command sequences. Then, our experiment shows that the coverage of a real machine and a simulator is low and similar. Also, the coverage patterns are almost the same in all tested benchmarks. To alleviate the problem, we propose a test-oriented command scheduling algorithm that increases the test coverage while preserving the memory behaviors of workloads and reducing the test time overhead by extracting representative sequences based on the similarity between command sequences. For the sequence extraction and the coverage estimation, our test sequences are embedded into vectors using bag-of-Ngrams. Compared to the simulator, our algorithm achieves 2.94x higher coverage while reducing the test overhead to 7.57%.  © 2020 ACM.",command scheduling; DRAM verification; test coverage; test sequence,Cost reduction; Integrated circuit design; Simulators; Testing; Command sequences; Coverage estimations; Coverage patterns; Design verification; Dynamic random access memory; Lower-power consumption; Performance metrices; Repetitive pattern; Dynamic random access storage
Strong Logic Obfuscation with Low Overhead against IC Reverse Engineering Attacks,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092702961&doi=10.1145%2f3398012&partnerID=40&md5=8399c582b7132677dc4bccb871fbb6ca,"Untrusted foundries pose threats of integrated circuit (IC) piracy and counterfeiting, and this has motivated research into logic locking. Strong logic locking approaches potentially prevent piracy and counterfeiting by preventing unauthorized replication and use of ICs. Unfortunately, recent work has shown that most state-of-the-art logic locking techniques are vulnerable to attacks that utilize Boolean Satisfiability (SAT) solvers. In this article, we extend our prior work on using silicon nanowire (SiNW) field-effect transistors (FETs) to produce obfuscated ICs that are resistant to reverse engineering attacks, such as the sensitization attack, SAT and approximate SAT attacks, as well as tracked signal attacks. Our method is based on exchanging some logic gates in the original design with a set of polymorphic gates (PLGs), designed using SiNW FETs, and augmenting the circuit with a small block, whose output is untraceable, namely, URSAT. The URSAT may not offer very strong resilience against the combined AppSAT-removal attack. Strong URSAT is achieved using only CMOS-logic gates, namely, S-URSAT. The proposed technique, S-URSAT + PLG-based traditional encryption, designed using SiNW FETs, increases the security level of the design to robustly thwart all existing attacks, including combined AppSAT-removal attack, with small penalties. Then, we evaluate the effectiveness of our proposed methods and subject it to a thorough security analysis. We also evaluate the performance penalty of the technique and find that it results in very small overheads in comparison to other works. The average area, power, and delay overheads of implementing 64 baseline key-bits of S-URSAT for small benchmarks are 5.03%, 2.60%, and -2.26%, respectively, while for large benchmarks they are 2.37%, 1.18%, and -1.93%.  © 2020 ACM.",Hardware security; logic encryption; reverse engineering attacks and defenses; SiNW FETs,Crime; Cryptography; Field effect transistors; Integrated circuit design; Integrated circuits; Locks (fasteners); Logic circuits; Logic gates; Reverse engineering; Silicon; Silicon compounds; Timing circuits; Boolean satisfiability; CMOS logic gates; Field effect transistor (FETs); Locking technique; Performance penalties; Security analysis; Silicon nanowires; State of the art; Computer circuits
Runtime Identification of Hardware Trojans by Feature Analysis on Gate-Level Unstructured Data and Anomaly Detection,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092736767&doi=10.1145%2f3391890&partnerID=40&md5=aa1dbb97e6390f92420b7cb23f786f20,"As the globalization of chip design and manufacturing process becomes popular, malicious hardware inclusions such as hardware Trojans pose a serious threat to the security of digital systems. Advanced Trojans can mask many architectural-level Trojan signatures and adapt against several detection mechanisms. Runtime Trojan detection techniques are considered as a last line of defense against Trojan inclusion and activation. In this article, we propose an offline analysis to select a subset of flip-flops as surrogates and build an anomaly detection model based on the activity profile of flip-flops. These flip-flops are monitored online, and the anomaly detection model implemented online analyzes the flip-flop data to detect any anomalous Trojan activity. The effectiveness of our approach has been tested on several Trojan-inserted designs of the Leon3 processor. Trojan activation is detected with an accuracy score of above 0.9 (ratio of the number of true predictions to total number of predictions) with no false positives by monitoring less than 0.5% of the total number of flip-flops.  © 2020 ACM.",Hardware trojans; machine learning; processor; workload,Chemical activation; Flip flop circuits; Hardware security; Malware; Network security; Anomaly detection models; Architectural levels; Detection mechanism; Manufacturing process; Off-line analysis; Runtime identification; Trojan detections; Unstructured data; Anomaly detection
Reuse Distance-based Victim Cache for Effective Utilisation of Hybrid Main Memory System,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085247782&doi=10.1145%2f3380732&partnerID=40&md5=8af56edcfe2c07e6192131d1fd941ed6,"Hybrid main memories comprising DRAM and Non-volatile memories (NVM) are projected as potential replacements of the traditional DRAM-based memories. However, traditional cache management policies designed for improving the hit rate lack awareness of the comparative latency of read-write for NVM blocks where the write latency is more than the read latency. Therefore, developing cache management techniques that reduce costly write-backs of the NVM blocks, yet maintain a fair hit rate in the cache, is of paramount importance. We propose two techniques based on the use of a small victim cache associated with the last-level cache that helps in retaining on the chip critical DRAM and NVM blocks. Victim cache being a scarce resource, we intend to keep only performance-critical blocks in the victim cache by exploiting the idea of reuse distance. The first technique, Victim Cache Replacement Policy, works on the replacement policy of the victim cache by preferential eviction of DRAM blocks over NVM blocks. However, the second technique, Prioritized Partitioning of victim cache, logically partitions the victim cache, giving a smaller share to the DRAM blocks and a relatively larger share to the NVM blocks. Experimental evaluation on full-system simulator shows significant improvement in system performance and reduction in the number of write-backs to the NVM partition of the main memory compared to the baseline and existing technique. Additionally, NVM reads and DRAM miss rate are also improved, leading to further performance enhancement. © 2020 ACM.",Hybrid main memory; last-level cache; reuse distance; victim cache,Dynamic random access storage; Cache management policies; Cache management techniques; Experimental evaluation; Full system simulators; Hybrid main memory; Non-volatile memory; Performance enhancements; Replacement policy; Cache memory
Algorithmic Fault Detection for RRAM-based Matrix Operations,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085243448&doi=10.1145%2f3386360&partnerID=40&md5=3f8e119ef3d152c34eebc381cbfab6de,"An RRAM-based computing system (RCS) provides an energy-efficient hardware implementation of vector-matrix multiplication for machine-learning hardware. However, it is vulnerable to faults due to the immature RRAM fabrication process. We propose an efficient fault tolerance method for RCS; the proposed method, referred to as extended-ABFT (X-ABFT), is inspired by algorithm-based fault tolerance (ABFT). We utilize row checksums and test-input vectors to extract signatures for fault detection and error correction. We present a solution to alleviate the overflow problem caused by the limited number of voltage levels for the test-input signals. Simulation results show that for a Hopfield classifier with faults in 5% of its RRAM cells, X-ABFT allows us to achieve nearly the same classification accuracy as in the fault-free case. © 2020 ACM.",fault detection; neural network; RRAM,Energy efficiency; Error correction; Fault tolerance; Fault tolerant computer systems; Matrix algebra; RRAM; Algorithm based fault tolerance; Classification accuracy; Computing system; Energy efficient; Fabrication process; Hardware implementations; Matrix operations; Vector-matrix multiplications; Fault detection
SCRIPT: A CAD Framework for Power Side-channel Vulnerability Assessment Using Information Flow Tracking and Patern Generation,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085251014&doi=10.1145%2f3383445&partnerID=40&md5=b3c2c9db4cdd35f81d93b1660e4851e7,"Power side-channel attacks (SCAs) have been proven to be effective at extracting secret keys from hardware implementations of cryptographic algorithms. Ideally, the power side-channel leakage (PSCL) of hardware designs of a cryptographic algorithm should be evaluated as early as the pre-silicon stage (e.g., gate level). However, there has been little effort in developing computer-aided design (CAD) tools to accomplish this. In this article, we propose an automated CAD framework called SCRIPT to evaluate information leakage through side-channel analysis. SCRIPT starts by defining the underlying properties of the hardware implementation that can be exploited by side-channel attacks. It then utilizes information flow tracking (IFT) to identify registers that exhibit those properties and, therefore, leak information through the side-channel. Here, we develop an IFT-based side-channel vulnerability metric (SCV) that is utilized by SCRIPT for PSCL assessment. SCV is conceptually similar to the traditionally used signal-to-noise ratio (SNR) metric. However, unlike SNR, which requires thousands of traces from silicon measurements, SCRIPT utilizes formal methods to generate SCV-guided patterns/plaintexts, allowing us to derive SCV using only a few patterns (ideally as low as two) at gate level. SCV estimates PSCL vulnerability at pre-silicon stage based on the number of plaintexts required to attain a specific SCA success rate. The integration of IFT and pattern generation makes SCRIPT efficient, accurate, and generic to be applied to any hardware design. We validate the efficacy of the SCRIPT framework by demonstrating that it can effectively and accurately determine SCA success rates for different AES designs at pre-silicon stage. SCRIPT is orders of magnitude more efficient than traditional pre-silicon PSCL assessment (SNR-based), with an average evaluation time of 15 minutes; whereas, traditional PSCL assessment at pre-silicon stage would require more than a month. We also analyze the PSCL characteristic of the multiplication unit of RISC processor using SCRIPT to demonstrate SCRIPT's applicability. © 2020 ACM.",CAD framework; information flow tracking; pattern generation; security metric; Side-channel; vulnerability,Computer aided design; Computer hardware; Formal methods; Hardware security; Reduced instruction set computing; Signal to noise ratio; Computer aided design tools; Cryptographic algorithms; Hardware implementations; Information flow tracking; Information leakage; Orders of magnitude; Side-channel analysis; Vulnerability assessments; Side channel attack
An Energy-aware Online Learning Framework for Resource Management in Heterogeneous Platforms,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085257238&doi=10.1145%2f3386359&partnerID=40&md5=532395ebc03bfca72d47632bbefb2b74,"Mobile platforms must satisfy the contradictory requirements of fast response time and minimum energy consumption as a function of dynamically changing applications. To address this need, systems-on-chip (SoC) that are at the heart of these devices provide a variety of control knobs, such as the number of active cores and their voltage/frequency levels. Controlling these knobs optimally at runtime is challenging for two reasons. First, the large configuration space prohibits exhaustive solutions. Second, control policies designed offline are at best sub-optimal, since many potential new applications are unknown at design-time. We address these challenges by proposing an online imitation learning approach. Our key idea is to construct an offline policy and adapt it online to new applications to optimize a given metric (e.g., energy). The proposed methodology leverages the supervision enabled by power-performance models learned at runtime. We demonstrate its effectiveness on a commercial mobile platform with 16 diverse benchmarks. Our approach successfully adapts the control policy to an unknown application after executing less than 25% of its instructions. © 2020 ACM.",Dynamic power management; imitation learning; online learning; reinforcement learning,Drilling platforms; Energy utilization; Power management; Programmable logic controllers; System-on-chip; Configuration space; Fast response time; Heterogeneous platforms; Imitation learning; Minimum energy consumption; Power performance; Resource management; Systems on chips; E-learning
Architectural Design of Flow-Based Microfluidic Biochips for Multi-Target Dilution of Biochemical Fluids,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085241561&doi=10.1145%2f3357604&partnerID=40&md5=c80a60addb4d69e71796384bc41cd34e,"Microfluidic technologies enable replacement of time-consuming and complex steps of biochemical laboratory protocols with a tiny chip. Sample preparation (i.e., dilution or mixing of fluids) is one of the primary tasks of any bioprotocol. In real-life applications where several assays need to be executed for different diagnostic purposes, the same sample fluid is often required with different target concentration factors (CFs). Although several multi-target dilution algorithms have been developed for digital microfluidic biochips, they are not efficient for implementation with continuous-flow-based microfluidic chips, which are preferred in the laboratories. In this article, we present a multi-target dilution algorithm (MTDA) for continuous-flow-based microfluidic biochips, which to the best of our knowledge is the first of its kind. We design a flow-based rotary mixer with a suitable number of segments depending on the target-CF profile, error tolerance, and optimization criteria. To schedule several intermediate fluid-mixing tasks, we develop a multi-target scheduling algorithm (MTSA) aiming to minimize the usage of storage units while producing dilutions with multiple CFs. Furthermore, we propose a storage architecture for efficiently loading (storing) of intermediate fluids from (to) the storage units. © 2020 ACM.",Biochips; continuous-flow microfluidics; dilution; mixer; sample preparation; storage structure,Biochips; Dilution; Mixing; Scheduling algorithms; Biochemical laboratories; Digital microfluidic biochips; Micro fluidic biochips; Microfluidic technologies; Optimization criteria; Real-life applications; Storage architectures; Target concentrations; Digital microfluidics
Security of Microfluidic Biochip: Practical Atacks and Countermeasures,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085252764&doi=10.1145%2f3382127&partnerID=40&md5=4b5948a74a559bd6d00b4217e8af3cca,"With the advancement of system miniaturization and automation, Lab-on-a-Chip (LoC) technology has revolutionized traditional experimental procedures. Microfluidic Biochip (MFB) is an emerging branch of LoC with wide medical applications such as DNA sequencing, drug delivery, and point of care diagnostics. Due to the critical usage of MFBs, their security is of great importance. In this article, we exploit the vulnerabilities of two types of MFBs: Flow-based Microfluidic Biochip (FMFB) and Digital Microfluidic Biochip (DMFB). We propose a systematic framework for applying Reverse Engineering (RE) attacks and Hardware Trojan (HT) attacks on MFBs as well as for practical countermeasures against the proposed attacks. We evaluate the attacks and defense on various benchmarks where experimental results prove the effectiveness of our methods. Security metrics are defined to quantify the vulnerability of MFBs. The overhead and performance of the proposed attacks as well as countermeasures are also discussed. © 2020 ACM.",camouflaging; hardware obfuscation; hardware Trojans; Microfluidic biochip; security; Trojan detection,Biochips; Diagnosis; DNA sequences; Drug delivery; Gene encoding; Malware; Medical applications; Digital microfluidic biochips; DNA Sequencing; Experimental procedure; Micro fluidic biochips; Point of care diagnostic; Security metrics; System miniaturization; Systematic framework; Digital microfluidics
How secure is split manufacturing in preventing hardware trojan?,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082397005&doi=10.1145%2f3378163&partnerID=40&md5=d1783e14cf76600b6b1c1c477680132e,"With the trend of outsourcing fabrication, split manufacturing is regarded as a promising way to both acquire the high-end nodes in untrusted external foundries and protect the design from potential attackers. However, in this article, we show that split manufacturing is not inherently secure, that a hardware Trojan attacker can still recover necessary information with a proximity-based or a simulated-annealing-based mapping approach together with a probability-based or net-based pruning method at the placement level. We further propose a defense approach by moving the insecure gates away from their easily attacked candidate locations. Results on benchmark circuits show the effectiveness of our proposed methods. © 2020 BMJ Publishing Group. All rights reserved.",Hardware trojan; Proximity-based attak; Simulated annealing attack; Split manufacturing,Malware; Manufacture; Simulated annealing; Benchmark circuit; Candidate locations; Manufacturing IS; Proximity-based attak; Pruning methods; Hardware security
On fundamental principles for thermal-aware design on periodic real-time multi-core systems,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082392294&doi=10.1145%2f3378063&partnerID=40&md5=e5cc09181eeb5991b3b1eab392ecd8c1,"With the exponential rise of the transistor count in one chip, the thermal problem has become a pressing issue in computing system design. While there have been extensive methods and techniques published for design optimization with thermal awareness, there is a need for more rigorous and formal thermal analysis in designing real-time systems and applications that demand a strong exception guarantee. In this article, we analytically prove a series of fundamental properties and principles concerning the RC thermal model, peak temperature identification, and peak temperature reduction for periodic real-time systems, which are general enough to be applied on 2D and 3D multi-core platforms. These findings enhance the worst-case temperature predictability in runtime scenarios, as well as help to develop more effective thermal management policy, which is key to thermal-constrained periodic real-time system design. © 2020 BMJ Publishing Group. All rights reserved.",Dynamic thermal management (DTM); Dynamic voltage frequency scaling (DVFS); Peak temperature minimization; Temperature bound; Thermal-aware design; Worst-case execution time (WCET),Design; Dynamic frequency scaling; Interactive computer systems; Systems analysis; Temperature control; Thermal management (electronics); Thermoanalysis; Voltage scaling; Dynamic thermal management; Dynamic voltage frequency scaling; Peak temperatures; Thermal-aware design; Worst-case execution time; Real time systems
Single-layer obstacle-aware substrate routing via iterative pin reassignment and wire assignment,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082401402&doi=10.1145%2f3378162&partnerID=40&md5=094fae0ec95b814c29b5d87c3a8d245d,"It is known that single-layer obstacle-aware substrate routing is necessary for modern IC/Package designs. In this article, given a set of two-pin nets and a set of rectangular obstacles inside a single-layer routing plane, a two-phase routing algorithm including an iterative routing phase and a rip-up-and-reroute phase can be proposed to maximize the number of the routed nets in single-layer obstacle-aware substrate routing. In the iterative routing phase, based on the pin and path distribution of the routing nets and the locations of the obstacles inside a single-layer routing plane, the start or target pins on some routing nets inside dense obstacle regions may be firstly reassigned to complete the partial wiring paths on the nets. Based on the region extraction of two intersected nets in single-layer routing, the private regions of some routing nets inside sparse obstacle regions can be extracted and the nets inside the extracted regions can be further routed by using maze routing. In the rip-up-and-reroute phase, the routability of the routing nets can be improved by ripping up some routed nets and rerouting the unrouted nets. Compared with Liu's modified algorithm and Yan's flow-based algorithm in single-layer obstacle-aware substrate routing, the experimental results show that the proposed algorithm can use less CPU time to increase 3.4% and 1.8% of the routability on the routing nets for eight tested examples on the average. Additionally, the percentage of the tested examples with the 100% routability of the routing nets on the eight tested examples has been improved from 25% to 62.5%. © 2020 BMJ Publishing Group. All rights reserved.",IC/Package design; Non-crossing constraint; Pin reassignment; Region extraction; Single-layer obstacle-aware routing,Extraction; Integrated circuits; Flow based algorithms; Modified algorithms; Non-crossing constraint; Obstacle aware routing; Path distribution; Pin reassignment; Rectangular obstacles; Region extraction; Iterative methods
A theoretical foundation for timing synchronous systems using asynchronous structures,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082395776&doi=10.1145%2f3373355&partnerID=40&md5=2d3008a128b644d9fcdd8bec924dc3d6,"Timing of synchronous systems is an everlasting stumbling block to the booming demands for lower power consumption and higher operation speeds in the electronics industry. This hardship is aggravated by the growing levels of variability in state-of-the-art silicon dimensions and in other beyond-CMOS technologies. Although some designers continue to strongly believe in the performance advantages of being fully synchronous, others have radically shifted toward extremely robust delay-insensitive domains. Targeting a different compromise of both performance and robustness, this article provides sufficient conditions for an asynchronous system to be able to generate the periodic signals necessary for the timing of a fully synchronous system and highlights a specific hierarchical clocking structure that with a single tunable delay satisfies these conditions. Using an asynchronous clock distribution network benefits from both the natural robustness of asynchronous structures and the advantageous performance of synchronous clocking. © 2020 BMJ Publishing Group. All rights reserved.",Asynchronous circuits; Clocking; Marked graphs; Petri nets; Single flux quantum; Superconducting electronics; Synchronous systems; Timing analysis,Clocks; Petri nets; Timing circuits; Asynchronous circuits; Clocking; Marked graphs; Single flux quantum; Superconducting electronics; Synchronous system; Timing Analysis; Electronics industry
Lagrangian relaxation-based time-division multiplexing optimization for multi-fpga systems,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082391830&doi=10.1145%2f3377551&partnerID=40&md5=e31ea3821dc8c31ed3fdb8c64d399a5f,"To increase the resource utilization in multi-FPGA (field-programmable gate array) systems, time-division multiplexing (TDM) is a widely used technique to accommodate a large number of inter-FPGA signals. However, with this technique, the delay imposed by the inter-FPGA connections becomes significant. Previous research has shown that the TDM ratios of signals can greatly affect the performance of a system. In this article, to minimize the system clock period and support more practical constraints in modern multi-FPGA systems, we propose an analytical framework to optimize the TDM ratios of inter-FPGA nets. A Lagrangian relaxationbased method first gives a continuous result under relaxed constraints. A binary search-based discretization algorithm is then used to assign the TDM ratio of each net such that the resulting maximum displacement is optimal and all the constraints are satisfied. Finally, a swapping-based post refinement is performed to further optimize the TDM ratios. For comparison, we also solve the problem using linear programming (LP)- basedmethods, which have guaranteed error bounds to the optimal solutions. Experimental results show that our framework can achieve similar quality with much shorter runtime compared to the LP-based methods. Moreover, our framework scales for designs with over 45,000 inter-FPGA nets while the runtime and memory usage of the LP-based methods will increase dramatically as the design scale becomes larger. © 2020 BMJ Publishing Group. All rights reserved.",EDA; FPGA; Lagrangian relaxation; Routing; Time-division multiplexing,Error analysis; Field programmable gate arrays (FPGA); Lagrange multipliers; Linear programming; Discretization algorithms; Guaranteed error bounds; LaGrangian relaxation; Maximum displacement; Multi-FPGA system; Resource utilizations; Routing; Runtime and memory usage; Time division multiplexing
Tunable FPGA bitstream obfuscation with boolean satisfiability attack countermeasure,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082397930&doi=10.1145%2f3373638&partnerID=40&md5=9fb032c07413dc9126cbdfc2fdffadb7,"Field Programmable Gate Arrays (FPGAs) are seeing a surge in usage in many emerging application domains, where the in-field reconfigurability is an attractive characteristic for diverse applications with dynamic design requirements, such as cloud computing, automotive, IoT, and aerospace. The security of the FPGA configuration file, or bitstream, is critical, especially for devices with long in-field lifetimes, where attackers may attempt to extract valuable Intellectual Property (IP) from within. In this article, we propose a tunable obfuscation approach that protects IP from typical bitstream attacks while enabling designers to trade off security with acceptable overhead. We also consider two potential attacks on this protection mechanism: Boolean SAT Attacks on the obfuscation and removal attacks on the protection circuitry. The obfuscation and SAT countermeasure are integrated in a custom CAD framework within a commercial FPGA toolflow and together provide mathematically strong protection against common bitstream attacks. Further, we quantify the difficulty of a removal attack on the protection circuitry through pattern matching and direct bitstream manipulation. The average area, power, and delay overhead for obfuscation with 95% mismatch probability are 18%, 16%, and 8%, respectively, for small combinational circuits, and 1%, 2%, and 5% for larger arithmetic modules. © 2020 BMJ Publishing Group. All rights reserved.",Field Programmable Gate Array; Obfuscation; Overheads; Removal attack; SAT attack,Binary sequences; Computer aided logic design; Delay circuits; Economic and social effects; Logic gates; Pattern matching; Signal receivers; Bitstream manipulations; Boolean satisfiability; Emerging applications; Obfuscation; Overheads; Protection mechanisms; Removal attacks; SAT attack; Field programmable gate arrays (FPGA)
Secure assay execution on meda biochips to thwart attacks using real-time sensing,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078270429&doi=10.1145%2f3374213&partnerID=40&md5=63def97409cc1a84c234a133310bc3f1,"Digital microfluidic biochips (DMFBs) have emerged as a promising platform for DNA sequencing, clinical chemistry, and point-of-care diagnostics. Recent research has shown that DMFBs are susceptible to various types of malicious attacks. Defenses proposed thus far only offer probabilistic guarantees of security due to the limitation of on-chip sensor resources. A micro-electrode-dot-array (MEDA) biochip is a next-generation DMFB that enables the real-time sensing of on-chip droplet locations, which are captured in the form of a droplet-location map. We propose a security mechanism that validates assay execution by reconstructing the sequencing graph (i.e., the assay specification) from the droplet-location maps and comparing it against the golden sequencing graph. We prove that there is a unique (one-to-one) mapping from the set of droplet-location maps (over the duration of the assay) to the set of possible sequencing graphs. Any deviation in the droplet-location maps due to an attack is detected by this countermeasure because the resulting derived sequencing graph is not isomorphic to the original sequencing graph. We highlight the strength of the security mechanism by simulating attacks on real-life bioassays. We also address the concern that the proposed mechanism may raise false alarms when some fluidic operations are executed on MEDA biochips. To avoid such false alarms, we propose an enhanced sensing technique that provides fine-grained sensing for the security mechanism. © 2020 Association for Computing Machinery.",Biological system modeling; Computer security; Microfluidics; Phrases: MEDA biochips; Sensors,Biochemistry; Biological systems; Digital microfluidics; DNA sequences; Drops; Electrodes; Errors; Gene encoding; Location; Microfluidics; Network security; Security of data; Sensors; Biological system modeling; Clinical chemistry; Digital microfluidic biochips; Point of care diagnostic; Probabilistic guarantees; Recent researches; Security mechanism; Sensing techniques; Biochips
Search-space decomposition for system-level design space exploration of embedded systems,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077819369&doi=10.1145%2f3369388&partnerID=40&md5=7648564d9ba675009662b96059d029af,"The development of large-scale multi- and many-core platforms and the rising complexity of embedded applications have led to a significant increase in the number of implementation possibilities for a single application. Furthermore, rising demands on safe, energy-efficient, or real-time capable application execution make the problem of determining feasible implementations that are optimal with respect to such design objectives even more of a challenge. State-of-the-art Design Space Exploration (DSE) techniques for this problem demonstrably suffer from the vast and sparse search spaces posed by modern embedded systems, emphasizing the need for novel design methodologies in this field. Based on the idea of reducing problem complexity by a suitable decomposition of the system specification-in particular, by a reduction of target architecture or task mapping options-the work at hand proposes a portfolio of dynamic decomposition mechanisms that automatically decompose any system specification based on a short pre-exploration of the complete system. We present a two-phase approach consisting of (a) a set of novel data extraction and representation techniques combined with (b) a selection of filtering operations that automatically extract a decomposed system specification based on information gathered during pre-exploration. In particular, we employ heat map data structures and threshold as well as graph-partitioning filters to reduce problem complexity. The proposed decomposition procedure can seamlessly be integrated in any DSE flow, constituting a flexible extension for existing DSE approaches. Furthermore, it improves existing static decomposition techniques and other heuristics relying on information about the problem instance, since systems with irregular architectural topology or distribution of resource types can now be decomposed based on an automatic, problem-independent pre-exploration phase. We illustrate the efficiency of the proposed decomposition portfolio applied to state-of-the-art DSEs for many-core systems as well as networked embedded systems from the automotive domain. Experimental results show significant increases in optimization quality of up to 87% within constant DSE time compared to existing approaches. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Complexity reduction; Design automation; Multi-objective optimization,Computer aided design; Computer architecture; Energy efficiency; Information filtering; Multiobjective optimization; Specifications; Systems analysis; Application execution; Complexity reduction; Decomposition technique; Design automations; Modern embedded systems; Networked embedded systems; Novel design methodology; Representation techniques; Embedded systems
Target faults for test compaction based on multicycle tests,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078259454&doi=10.1145%2f3375278&partnerID=40&md5=e4652e55e77bdc1749aeddc0654a07c8,"The use of multicycle tests, with several functional capture cycles between scan operations, contributes significantly to the ability to compact a test set. Multicycle tests have the added benefit that they can contribute to the detection of defects with complex behaviors that are not detected by single-cycle or two-cycle tests. To ensure that this benefit is materialized when test compaction is applied to transition faults, this article suggests to incorporate into the test compaction procedure an additional fault model whose fault coverage increases when multicycle tests are used. To ensure that the computational complexity of test compaction is not increased by a fault model with a large number of faults, or faults with complex behaviors, the added fault model is required to have the same characteristics as the transition fault model. A type of transition fault called unspecified transition fault satisfies these requirements. The article describes a test compaction procedure for transition faults that incorporates unspecified transition faults, and presents experimental results for benchmark circuits to demonstrate the levels of test compaction and fault coverage that can be achieved. © 2020 Association for Computing Machinery.",And Phrases: Multicycle tests; Test compaction; Test generation; Transition faults,Dynamic random access storage; Benchmark circuit; Detection of defects; Multi cycle tests; Scan operations; Test Compaction; Test generations; Transition fault models; Transition faults; Compaction
LBNoc: Design of low-latency router architecture with lookahead bypass for network-on-chip using FPGA,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078244245&doi=10.1145%2f3365994&partnerID=40&md5=5027b9bb8455a76777485bfef4d1f9f6,"An FPGA-based Network-on-Chip (NoC) using a low-latency router with a look-ahead bypass (LBNoC) is discussed in this article. The proposed design targets the optimized area with improved network performance. The techniques such as single-cycle router bypass, adaptive routing module, parallel Virtual Channel (VC), and Switch allocation, combined virtual cut through and wormhole switching, have been employed in the design of the LBNoC router. The LBNoC router is parameterizable with the network topology, traffic patterns, routing algorithms, buffer depth, buffer width, number of VCs, and I/O ports being configurable. A table-based routing algorithm has been employed to support the design of custom topologies. The input buffer modules of NoC router have been mapped on the FPGA Block RAM hard blocks to utilize resources efficiently. The LBNoC architecture consumes 4.5% and 27.1% fewer hardware resources than the ProNoC and CONNECT NoC architectures. The average packet latency of the LBNoC NoC architecture is 30% and 15% lower than the CONNECT and ProNoC architectures. The LBNoC architecture is 1.15× and 1.18× faster than the ProNoC and CONNECT NoC frameworks. © 2020 Association for Computing Machinery.",FPGA-based simulation framework; Low latency; Network-on-Chip; NoC Router architecture; System-on-Chip; Xilinx Zynq 7000,Binary alloys; Cesium alloys; Computer architecture; Field programmable gate arrays (FPGA); Network architecture; Network-on-chip; Routers; Servers; System-on-chip; Topology; Average packet latencies; Fpga based simulations; Hardware resources; Low latency; Network-on-chip(NoC); Router architecture; Virtual cut-through; Xilinx Zynq 7000; Integrated circuit design
Hardware Trojan mitigation in pipelined MPSoCs,2020,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078274885&doi=10.1145%2f3365578&partnerID=40&md5=eaa5255e1933786b43c18d687868e83b,"Multiprocessor System-on-Chip (MPSoC) has become necessary due to the the billions of transistors available to the designer, the need for fast design turnaround times, and the power wall. Thus, present embedded systems are designed with MPSoCs, and one possible way MPSoCs can be realized is through Pipelined MPSoC (PMPSoC) architectures, which are used in applications from video surveillance to cryptosystems. Hardware Trojans (HTs) on PMPSoCs are a significant concern due to the damage caused by their stealth. An adversary could use HTs to extract secret information (data leakage) to modify functionality/data (functional modification) or make PMPSoCs deny service. In this article, we present PMPGuard, a mechanism that (1) detects the presence of hardware Trojans in Third Party Intellectual Property (3PIP) cores of PMPSoCs by continuous monitoring and testing and (2) recovers the system by switching the infected processor core with another one. We designed, implemented, and tested the system on a commercial cycle accurate multiprocessor simulation environment. Compared to the state-of-the-art system-level techniques that use Triple Modular Redundancy (TMR) and therefore incur at least 3× area and power overheads, our proposed system incurs about 2× area and 1.5× power overheads without any adverse impact on throughput. © 2020 Association for Computing Machinery.",Hardware Trojans; On-line monitoring; Pipelined MPSoCs,Embedded systems; Fault tolerant computer systems; Malware; Multiprocessing systems; Pipelines; Security systems; System-on-chip; Turnaround time; Continuous monitoring; Functional modification; Multiprocessor simulation; Multiprocessor system on chips; Online monitoring; Pipelined MPSoCs; State-of-the-art system; Triple modular redundancy; Hardware security
Hidden in plaintext: An obfuscation-based countermeasure against FPGA bitstream tampering attacks,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077752784&doi=10.1145%2f3361147&partnerID=40&md5=37fa69fd5373bd9735f5982602b39e95,"Field Programmable Gate Arrays (FPGAs) have become an attractive choice for diverse applications due to their reconfigurability and unique security features. However, designs mapped to FPGAs are prone to malicious modifications or tampering of critical functions. Besides, targeted modifications have demonstrably compromised FPGA implementations of various cryptographic primitives. Existing security measures based on encryption and authentication can be bypassed using their side-channel vulnerabilities to execute bitstream tampering attacks. Furthermore, numerous resource-constrained applications are now equipped with low-end FPGAs, which may not support power-hungry cryptographic solutions. In this article, we propose a novel obfuscation-based approach to achieve strong resistance against both random and targeted pre-configuration tampering of critical functions in an FPGA design. Our solution first identifies the unique structural and functional features that separate the critical function from the rest of the design using a machine learning guided framework. The selected features are eliminated by applying appropriate obfuscation techniques, many of which take advantage of “FPGA dark silicon”—unused lookup table resources—to mask the critical functions. Furthermore, following the same obfuscation principle, a redundancy-based technique is proposed to thwart targeted, rule-based, and random tampering. We have developed a complete methodology and custom software toolflow that integrates with commercial tools. By applying the masking technique on a design containing AES, we show the effectiveness of the proposed framework in hiding the critical S-Box function. We implement the redundancy integrated solution in various cryptographic designs to analyze the overhead. To protect 16.2% critical component of a design, the proposed approach incurs an average area overhead of only 2.4% over similar redundancy-based approaches, while achieving strong security. © 2019 Copyright held by the owner/author(s).",FPGA bitstream tampering; FPGA security; Trojan prevention,Authentication; Binary sequences; Integrated circuit design; Malware; Redundancy; Side channel attack; Table lookup; Bit stream; Cryptographic primitives; Diverse applications; FPGA implementations; FPGA security; Functional features; Integrated solutions; Trojans; Field programmable gate arrays (FPGA)
Lithography hotspot detection with FFT-based feature extraction and imbalanced learning rate,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077745972&doi=10.1145%2f3372044&partnerID=40&md5=f4cb3133bb306c17e8f0d32f5f8e7cc0,"With the increasing gap between transistor feature size and lithography manufacturing capability, the detection of lithography hotspots becomes a key stage of physical verification flow to enhance manufacturing yield. Although machine learning approaches are distinguished for their high detection efficiency, they still suffer from problems such as large-scale layout and class imbalance. In this article, we develop a hotspot detection model based on machine learning with high performance. In the proposed model, we first apply an Fast Fourier Transform–based feature extraction method that can compress large-scale layout to a 5 multi-dimensional representation with much smaller size while preserving the discriminative layout pattern information to improve the detection efficiency. Second, addressing the class imbalance problem, we propose a new technique called imbalanced learning rate and embed it into the convolutional neural network model to further reduce false alarms without accuracy decay. Compared with the results of current state-of-the-art approaches on ICCAD 2012 Contest benchmarks, our proposed model can achieve better solutions in many evaluation metrics, including the official metrics. © 2019 Association for Computing Machinery.",Deep learning; Design for manufacturability; Lithography hotspot,Deep learning; Design for manufacturability; Efficiency; Extraction; Fast Fourier transforms; Learning algorithms; Lithography; Machine design; Machine learning; Manufacture; Neural networks; Class imbalance problems; Convolutional neural network; Detection efficiency; Feature extraction methods; Hot spot; Machine learning approaches; Manufacturing capability; State-of-the-art approach; Feature extraction
Hierarchical ensemble reduction and learning for resource-constrained computing,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077752069&doi=10.1145%2f3365224&partnerID=40&md5=6841c8de49f138e35f22aa2a0af2e701,"Generic tree ensembles (such as Random Forest, RF) rely on a substantial amount of individual models to attain desirable performance. The cost of maintaining a large ensemble could become prohibitive in applications where computing resources are stringent. In this work, a hierarchical ensemble reduction and learning framework is proposed. Experiments show our method consistently outperforms RF in terms of both accuracy and retained ensemble size. In other words, ensemble reduction is achieved with enhancement in accuracy rather than degradation. The method can be executed efficiently, up to >590× time reduction than a recent ensemble reduction work. We also developed Boolean logic encoding techniques to directly tackle multiclass problems. Moreover, our framework bridges the gap between software-based ensemble methods and hardware computing in the IoT era. We developed a novel conversion paradigm that supports the automatic deployment of >500 trees on a chip. Our proposed method reduces power consumption and overall area utilization by >21.5% and >62%, respectively, comparing with RF. The hierarchical approach provides rich opportunities to balance between the computation (training and response time), the hardware resource (memory and energy), and accuracy. © 2019 Association for Computing Machinery.",Boolean logic; Edge computing; Ensemble reduction; Hardware and energy efficiency; Hardware implementation; Hierarchical learning; Logic minimization; Machine learning,Computation theory; Decision trees; Edge computing; Energy efficiency; Forestry; Green computing; Learning systems; Logic Synthesis; Automatic deployments; Boolean logic; Hardware implementations; Hierarchical approach; Hierarchical ensemble; Hierarchical learning; Logic minimization; Multi-class problems; Computer circuits
An implication-based test scheme for both diagnosis and concurrent error detection applications,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077756829&doi=10.1145%2f3364681&partnerID=40&md5=132a0d22859a58e719b8a2616e0aa0c9,"This article describes a diagnosis-aware hybrid concurrent error detection (DAH-CED) scheme that can facilitate both off-line and on-line test applications. By using the proposed scheme, not only the probability of detecting errors (on-line) but also the diagnosability of the target circuit (off-line) can be significantly enhanced. The proposed scheme combines the implication-based method with the parity check method. In particular, novel algorithms are developed to identify specific implications for enhancing the diagnosability for the modeled faults proactively. Furthermore, a reduction algorithm is also presented to minimize the number of the employed implications, while no loss on probability of detecting errors and diagnosability is also guaranteed. To the best of our knowledge, this issue is not addressed in the literature. To validate the proposed scheme, not only stuck-at faults but also transition faults are considered to simulate the timing-related errors. The experimental results on nine ITC’99 benchmark circuits show that the diagnosability for stuck-at (transition) faults is enhanced by 6.88% (7.78%) by applying the proposed scheme. As for the probability of detecting errors, 97.73% (97.10%) is achieved for errors caused by stuck-at (transition) faults. Moreover, only 3.11% of implications are needed. © 2019 Association for Computing Machinery.",Concurrent error detection; Fault diagnosis; Implication,Convolutional codes; Error detection; Failure analysis; Probability; Benchmark circuit; Concurrent error detection; Implication; Novel algorithm; Reduction algorithms; Stuck-at faults; Target circuits; Transition faults; Fault detection
Bio-chemical assay locking to thwart bio-IP theft,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077747443&doi=10.1145%2f3365579&partnerID=40&md5=8df30f02415bb84fe67feb4755bcb76f,"It is expected that as digital microfluidic biochips (DMFBs) mature, the hardware design flow will begin to resemble the current practice in the semiconductor industry: design teams send chip layouts to third-party foundries for fabrication. These foundries are untrusted and threaten to steal valuable intellectual property (IP). In a DMFB, the IP consists of not only hardware layouts but also of the biochemical assays (bioassays) that are intended to be executed on-chip. DMFB designers therefore must defend these protocols against theft. We propose to “lock” biochemical assays by inserting dummy mix-split operations. We experimentally evaluate the proposed locking mechanism, and show how a high level of protection can be achieved even on bioassays with low complexity. We also demonstrate a new class of attacks that exploit the side-channel information to launch sophisticated attacks on the locked bioassay. © 2019 Association for Computing Machinery.",Bioassay; Digital microfluidic biochip; IP-theft; Locking,Bioassay; Biochips; Crime; Foundries; Internet protocols; Locks (fasteners); Semiconductor device manufacture; Side channel attack; Biochemical assay; Current practices; Digital microfluidic biochips; IP-theft; Locking; Locking mechanism; Semiconductor industry; Side-channel information; Digital microfluidics
Harnessing the granularity of micro-electrode-dot-array architectures for optimizing droplet routing in biochips,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077745811&doi=10.1145%2f3365993&partnerID=40&md5=73544b7d418669d3c184cb7f333e8857,"In this article, we consider the problem of droplet routing for Microelectrode-Dot-Array (MEDA) biochips. MEDA biochips today provide a host of useful features for droplet movement by making it possible to manoeu-vre droplets at a much finer granularity and with significantly increased flexibility. More precisely, MEDA biochips support more degrees of freedom in navigation and volumetric manipulation such as diagonal movement, droplet reshaping, and fractional-level split-and-merge. This helps improve routing of droplets on microfluidic grids—in particular, when the space available on the grid is limited or blocked by obstacles. In this work, we discuss how these improved capabilities can be utilized in the realization of the desired routes on those biochips. To this end, we introduce a routing method that utilizes satisfiability solvers and guarantees the generation of optimal solutions, considering the set of MEDA operations we model. This significantly improves the state of the art, since previously proposed solutions either (1) relied on heuristics and, hence, were not able to guarantee the optimum or (2) only considered a subset of the MEDA features. The solution proposed in this work includes a formulation of all MEDA features, which, as illustrated by examples, allows for the determination of routing solutions with smaller completion times. Experimental evaluations confirm these findings. © 2019 Association for Computing Machinery.",Biochips; Droplet routing; Microfluidics; Physical design automation; SAT modelling,Biochips; Computer aided design; Degrees of freedom (mechanics); Microelectrodes; Microfluidics; Optimization; Droplet movement; Droplet routing; Experimental evaluation; Increased flexibility; Optimal solutions; Physical design; Satisfiability solvers; State of the art; Drops
Analog/RF post-silicon tuning via Bayesian optimization,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077744619&doi=10.1145%2f3365577&partnerID=40&md5=d33fd47962394d2233ceaf9dd2415e32,"Tunable analog/RF circuit has emerged as a promising technique to address the significant performance uncertainties caused by process variations. To optimize these tunable circuits after fabrication, most existing post-silicon programming methods are developed by using real-valued performance metrics. However, when measuring a performance of interest on silicon, it is often substantially more expensive to obtain a real-valued measurement than a binary testing outcome (i.e., pass or fail). In this article, we propose a Gaussian Process Classification model to capture the binary performance metrics of tunable analog/RF circuits. Based on these models, post-silicon programming is cast into an optimization problem that can be solved by a novel Bayesian optimization algorithm. Moreover, measurement noises are further incorporated into our proposed post-silicon programming to produce a robust circuit. Two circuit examples demonstrate that the proposed approach can efficiently program tunable circuits with binary performance metrics while other conventional methods are not applicable. © 2019 Association for Computing Machinery.",Bayesian optimization; Gaussian process classification; Post-silicon tuning,Gaussian distribution; Gaussian noise (electronic); Optimization; Silicon; Analog/RF circuits; Bayesian optimization; Bayesian optimization algorithms; Conventional methods; Gaussian process classifications; Optimization problems; Performance metrics; Post-silicon tuning; Tuning
Security-aware routing and scheduling for control applications on ethernet TSN networks,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077731290&doi=10.1145%2f3358604&partnerID=40&md5=923065aa68b21d6c031143f596be08c4,"Today, it is common knowledge in the cyber-physical systems domain that the tight interaction between the cyber and physical elements provides the possibility of substantially improving the performance of these systems that is otherwise impossible. On the downside, however, this tight interaction with cyber elements makes it easier for an adversary to compromise the safety of the system. This becomes particularly important, since such systems typically are composed of several critical physical components, e.g., adaptive cruise control or engine control that allow deep intervention in the driving of a vehicle. As a result, it is important to ensure not only the reliability of such systems, e.g., in terms of schedulability and stability of control plants, but also resilience to adversarial attacks. In this article, we propose a security-aware methodology for routing and scheduling for control applications in Ethernet networks. The goal is to maximize the resilience of control applications within these networked control systems to malicious interference while guaranteeing the stability of all control plants, despite the stringent resource constraints in such cyber-physical systems. Our experimental evaluations demonstrate that careful optimization of available resources can significantly improve the resilience of these networked control systems to attacks. © 2019 Association for Computing Machinery.",Ethernet TSN; Joint routing and scheduling; Real-time control systems; Resilience optimization,Adaptive control systems; Adaptive cruise control; Control system stability; Cyber Physical System; Embedded systems; Ethernet; Network routing; Real time control; Scheduling; Control applications; Experimental evaluation; Joint routing; Physical components; Resource Constraint; Routing and scheduling; Security aware routing; Stability of control; Networked control systems
Making aging useful by recycling aging-induced clock skew,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076720073&doi=10.1145%2f3363186&partnerID=40&md5=cd8bb8ce35b31fa093eed3e6830467de,"Device aging, which causes significant loss on circuit performance and lifetime, has been a primary factor in reliability degradation of nanoscale designs. In this article, we propose to take advantage of aging-induced clock skews (i.e., make them useful for aging tolerance) by manipulating and recycling these time-varying skews to compensate for the performance degradation of logic networks. The goal is to assign achievable/reasonable aging-induced clock skews in a circuit, such that its effective performance degradation due to aging can be tolerated. On average, 21.21% aging tolerance can be achieved with insignificant design overhead. Moreover, we employVth assignment on clock buffers to further tolerate the aging-induced degradation of logic networks. WhenVth assignment is applied on top of aforementioned aging manipulation, the average aging tolerance can be enhanced to 29.15%. © 2019 Copyright held by the owner/author(s).",Aging; Clock network; Degradation; Reliability,Aging of materials; Degradation; Electric clocks; Recycling; Reliability; Circuit performance; Clock network; Effective performance; Induced degradation; Nano-scale design; Performance degradation; Primary factors; Reliability degradation; Computer circuits
Memristive crossbar mapping for neuromorphic computing systems on 3D IC,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077742769&doi=10.1145%2f3365576&partnerID=40&md5=56c67884ca828ef6dd7b4b765a4401a2,"In recent years, neuromorphic computing systems based on memristive crossbar have provided a promising solution to enable acceleration of neural networks. However, most of the neural networks used in realistic applications are often sparse. If such sparse neural network is directly implemented on a single memristive crossbar, then it would result in inefficient hardware realizations. In this work, we propose E3D-FNC, an enhanced three-dimesnional (3D) floorplanning framework for neuromorphic computing systems, in which the neuron clustering and the layer assignment are considered interactively. First, in each iteration, hierarchical clustering partitions neurons into a set of clusters under the guidance of the proposed distance metric. The optimal number of clusters is determined by L-method. Then matrix re-ordering is proposed to re-arrange the columns of the weight matrix in each cluster. As a result, the reordered connection matrix can be easily mapped into a set of crossbars with high utilizations. Next, since the clustering results will in turn affect the floorplan, we perform the floorplanning of neurons and crossbars again. All the proposed methodologies are embedded in an iterative framework to improve the quality of NCS design. Finally, a 3D floorplan of neuromorphic computing systems is generated. Experimental results show that E3D-FNC can achieve highly hardware-efficient designs compared to the state of the art. © 2019 Association for Computing Machinery.",3D floorplanning; Hierarchical clustering; Memristive crossbar; Neuromorphic computing,Clustering algorithms; Integrated circuit design; Iterative methods; Neural networks; Neurons; Three dimensional integrated circuits; Timing circuits; Floor-planning; Hardware realization; Hier-archical clustering; Iterative framework; Memristive crossbar; Neuromorphic computing; Realistic applications; Sparse neural networks; Matrix algebra
Energy-aware scheduling of task graphs with imprecise computations and end-to-end deadlines,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077730734&doi=10.1145%2f3365999&partnerID=40&md5=fc14fedf86ee935244a6d616dd0aaad8,"Imprecise computations allow scheduling algorithms developed for energy-constrained computing devices to trade off output quality with utilization of system resources. The goal of such scheduling algorithms is to utilize imprecise computations to find a feasible schedule for a given task graph while maximizing the quality of service (QoS) and satisfying a hard deadline and an energy bound. This work presents a heuristic for scheduling tasks with potentially imprecise computations, represented with directed acyclic graphs, on multiprocessor platforms. Furthermore, it presents a mixed integer linear program formulation of the same problem, which provides the optimal reference scheduling solutions, enabling evaluation of the efficacy of the proposed heuristic. Both the heuristic and mathematical program take account of potentially imprecise inputs of tasks on their output quality. Furthermore, the presented heuristic is capable of finding feasible schedules even under tight energy budgets. Through extensive experiments, it is shown that in some cases, the proposed heuristic is capable of finding the same QoS as the ones found by MILP. Furthermore, for those task graphs that MILP outperforms the proposed heuristic, QoS values obtained with the proposed heuristic are, on average, within 1.24% of the optimal solutions while improving the runtime by a factor of 100 or so. This clearly demonstrates the advantage of the proposed heuristic over the exact solution, especially for large task graphs where solving the mathematical problem is hampered by its lengthy runtime. © 2019 Copyright held by the owner/author(s).",Imprecise computations; Input error; Real-time MPSoCs; Task scheduling,Budget control; Directed graphs; Economic and social effects; Graphic methods; Heuristic programming; Integer programming; Power management; Scheduling; Scheduling algorithms; System-on-chip; Directed acyclic graph (DAG); Energy-aware scheduling; Imprecise computation; Mathematical problems; Mixed-integer linear program formulations; Multi-processor platforms; Real time; Task-scheduling; Quality of service
Efficient cache reconfiguration using machine learning in NoC-based many-core CMPs,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075713282&doi=10.1145%2f3350422&partnerID=40&md5=fa89514f3b45cd4100207016ce1c8aa5,"Dynamic cache reconfiguration (DCR) is an effective technique to optimize energy consumption inmany-core architectures. While early work on DCR has shown promising energy saving opportunities, prior techniques are not suitable for many-core architectures since they do not consider the interactions and tight coupling between memory, caches, and network-on-chip (NoC) traffic. In this article, we propose an efficient cache reconfiguration framework in NoC-based many-core architectures. The proposed work makes three major contributions. First, we model a distributed directory based many-core architecture similar to Intel Xeon Phi architecture. Next, we propose an efficient cache reconfiguration framework that considers all significant components, including NoC, caches, and main memory. Finally, we propose a machine learning-based framework that can reduce the exploration time by an order of magnitude with negligible loss in accuracy. Our experimental results demonstrate 18.5% energy savings on average compared to base cache configuration. © 2019 Association for Computing Machinery.",Cache reconfiguration; Machine learning,Cache memory; Energy conservation; Energy utilization; Learning systems; Machine learning; Memory architecture; Network architecture; Cache configurations; Cache reconfigurations; Dynamic cache; Main memory; Many core; Many-core architecture; Network-on-chip(NoC); Tight coupling; Network-on-chip
IP protection and supply chain security through logic obfuscation: A systematic overview,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075749596&doi=10.1145%2f3342099&partnerID=40&md5=11ba1fc7daabac7d2dcb74e259925fc9,"The globalization of the semiconductor supply chain introduces ever-increasing security and privacy risks. Two major concerns are IP theft through reverse engineering and malicious modification of the design. The latter concern in part relies on successful reverse engineering of the design as well. IC camouflaging and logic locking are two of the techniques under research that can thwart reverse engineering by end-users or foundries.However, developing lowoverhead locking/camouflaging schemes that can resist the ever-evolving state-of-the-art attacks has been a challenge for several years. This article provides a comprehensive review of the state of the art with respect to locking/camouflaging techniques. We start by defining a systematic threat model for these techniques and discuss how various real-world scenarios relate to each threat model. We then discuss the evolution of generic algorithmic attacks under each threat model eventually leading to the strongest existing attacks. The article then systematizes defences and along the way discusses attacks that are more specific to certain kinds of locking/camouflaging. The article then concludes by discussing open problems and future directions. © 2019 Association for Computing Machinery.",Hardware security; IC camouflaging; Logic locking; Logic obfuscation,Hardware security; Integrated circuits; Locks (fasteners); Reverse engineering; Supply chains; Algorithmic attacks; Logic locking; Logic obfuscation; Real-world scenario; Security and privacy; Semiconductor supply chain; State of the art; Supply chain security; Computer circuits
Two-sided net untangling with internal detours for single-layer bus routing,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075754272&doi=10.1145%2f3363184&partnerID=40&md5=ef630865860c7a451314951b4b47a9db,"It is known that one-sided net untangling can be used to untangle the twisted nets inside a bus for single-layer bus routing. However, limited space behind one pin-row may make one-sided net untangling unsuccessful for single-layer bus routing. In this article, the concept of using internal detours on untangled nets can be introduced into two-sided net untangling. Given a set of 2-pin nets inside a bus, based on two one-sided untangling results with internal detours on untangled nets [8], an efficient algorithm first uses a minimal set of internal detours to guarantee that the crossing conditions of the given nets inside the bus can be eliminated in one initial two-sided untangling result with no capacity constraint behind two pin-rows and between two adjacent pins inside any pin-row. Furthermore, based on the maintenance of the non-crossing constraint on any pair of nets and the capacity constraint behind two pin-rows in one initial two-sided untangling result, an iterative rip-up-and-reassign algorithm can be proposed to eliminate the possible capacity violations between two adjacent pins inside two pin-rows to route a maximal set of nets in two-sided net untangling. Compared with Yan's one-sided net untangling [8] for 12 tested examples with different capacity constraints, the experimental results show that our proposed two-sided untangling algorithm can improve 3.5% of routability and use the benefit of more routing space behind two pin-rows to reduce 86.4% of the used internal detours on average in reasonable CPU time. Compared with Yan's two-sided net untangling [9] for 12 tested examples with different capacity constraints, the experimental results show that our proposed two-sided untangling algorithm can improve 2.8% of routability by introducing some internal detours and using iterative rip-upand-reassign on the average in reasonable CPU time. © 2019 Association for Computing Machinery.",Bus routing; Net untangling; PCB design; Single-layer routing,Iterative methods; Polychlorinated biphenyls; Bus Routing; Capacity constraints; Limited space; Net untangling; Non-crossing constraint; PCB design; Routing space; Single layer; Buses
Runtime stress estimation for three-dimensional IC reliability management using artificial neural network,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075712756&doi=10.1145%2f3363185&partnerID=40&md5=6b67cce7c1a22f716ee86511a705f7fa,"Heat dissipation and the related thermal-mechanical stress problems are the major obstacles in the development of the three-dimensional integrated circuit (3D IC). Reliability management techniques can be used to alleviate such problems and enhance the reliability of 3D IC. However, it is difficult to obtain the time-varying stress information at runtime, which limits the effectiveness of the reliability management. In this article, we propose a fast stress estimation method for runtime reliability management using artificial neural network (ANN). The newmethod builds ANN-based stressmodel by training offline using temperature and stress data. The ANN stress model is then used to estimate the important stress information, such as the maximum stress around each TSV, for reliability management at runtime. Since there are a variety of potential ANN structures to choose fromfor the ANN stress model, we analyze and test three ANN-based stressmodels with three major types of ANNs in this work: The normal ANN-based stress model, the ANN stress model with handcrafted feature extraction, and the convolutional neural network-(CNN) based stress model. The structures of each ANN stress model and the functions of these structures in 3D IC stress estimation are demonstrated and explained. The new runtime stress estimation method is tested using the three ANN stress models with different layer configurations. Experiments show that the new method is able to estimate important stress information at extremely fast speed with good accuracy for runtime 3D IC reliability enhancement. Although all three ANN stress models show acceptable capabilities in runtime stress estimation, the CNN-based stress model achieves the best performance considering both stress estimation accuracy and computing overhead. Comparison with traditional method reveals that the new ANN-based stress estimationmethod ismuch more accurate with a slightly larger but still very small computing overhead. © 2019 Association for Computing Machinery.",3D IC; Artificial neural network; Reliability management; Stress estimation,Neural networks; Reliability; Three dimensional integrated circuits; Timing circuits; Convolutional neural network; Different layers; Reliability management; Stress estimation; Thermal mechanical stress; Three dimensional IC; Three dimensional integrated circuits (3-D IC); Time-varying stress; Stresses
JAMS-SG: A framework for jitter-aware message scheduling for time-triggered automotive networks,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075736445&doi=10.1145%2f3355392&partnerID=40&md5=a2f7e7b075cf9cb605ad3fcc6066492b,"Time-triggered automotive networks use time-triggered protocols (FlexRay, TTEthernet, etc.) for periodic message transmissions that often originate from safety and time-critical applications. One of the major challenges with time-triggered transmissions is jitter, which is the unpredictable delay-induced deviation fromthe actual periodicity of a message. Failure to account for jitter can be catastrophic in time-sensitive systems, such as automotive platforms. In this article, we propose a novel scheduling framework (JAMS-SG) that satisfies timing constraints during message delivery for both jitter-affected time-triggered messages and high-priority event-triggered messages in automotive networks. At design time, JAMS-SG performs jitter-aware frame packing (packing of multiple signals from Electronic Control Units (ECUs) into messages) and schedules synthesis with a hybrid heuristic. At runtime, aMulti-Level Feedback Queue (MLFQ) handles jitter-affected timetriggered messages and high-priority event-triggered messages that are scheduled using a runtime scheduler. Our simulation results, based onmessages and network traffic data from a real vehicle, indicate that JAMS-SG is highly scalable and outperforms the best-known prior work in the area in the presence of jitter. © 2019 Association for Computing Machinery.",Automotive networks; Cyber-physical systems; FlexRay; Jitter; Scheduling; Time-triggered systems,Control systems; Cyber Physical System; Embedded systems; Scheduling; Automotive networks; Electronic control units; Flexray; Message scheduling; Scheduling frameworks; Time triggered protocol; Time-critical applications; Time-triggered systems; Jitter
Real-time scheduling of DAG tasks with arbitrary deadlines,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075752044&doi=10.1145%2f3358603&partnerID=40&md5=f60ab5a339a3d9e09362e8ee70ec1f37,"Real-time and embedded systems are shifting from single-core to multi-core processors, on which the software must be parallelized to fully utilize the computation capacity of the hardware. Recently, much work has been done on real-time scheduling of parallel tasks modeled as directed acyclic graphs (DAG). However, most of these studies assume tasks to have implicit or constrained deadlines. Much less work considered the general case of arbitrary deadlines (i.e., the relative deadline is allowed to be larger than the period), which is more difficult to analyze due to intra-task interference among jobs. In this article, we study the analysis of Global Earliest Deadline First (GEDF) scheduling for DAG parallel tasks with arbitrary deadlines.We develop new analysis techniques for GEDF scheduling of a single DAG task and this new analysis techniques can guarantee a better capacity augmentation bound 2.41 (the best known result is 2.5) in the case of a single task. Furthermore, the proposed analysis techniques are also extended to the case of multiple DAG tasks under GEDF and federated scheduling. Finally, through empirical evaluation, we justify the out-performance of our schedulability tests compared to the state-of-the-art in general. © 2019 Association for Computing Machinery.",Arbitrary deadline; GEDF scheduling; Parallel task,Directed graphs; Embedded systems; Scheduling; Arbitrary deadlines; Directed acyclic graph (DAG); Earliest deadline first; Empirical evaluations; Multi-core processor; Parallel task; Real - time scheduling; Real-time and embedded systems; Real time systems
Impact of electrostatic coupling on monolithic 3d-enabled network on chip,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073052128&doi=10.1145%2f3357158&partnerID=40&md5=c774978bf2ac37748ce05a14016378a3,"Monolithic-3D-integration (M3D) improves the performance and energy efficiency of 3D ICs over conventional through-silicon-vias-based counterparts. The smaller dimensions of monolithic inter-tier vias offer high-density integration, the flexibility of partitioning logic blocks across multiple tiers, and significantly reduced total wire-length enable high-performance and energy-efficiency. However, the performance of M3D ICs degrades due to the presence of electrostatic coupling when the inter-layer-dielectric thickness between two adjacent tiers is less than 50nm. In this work, we evaluate the performance of an M3D-enabled Networkon-chip (NoC) architecture in the presence of electrostatic coupling. Electrostatic coupling induces significant delay and energy overheads for the multi-tier NoC routers. This in turn results in considerable performance degradation if the NoC design methodology does not incorporate the effects of electrostatic coupling. We demonstrate that electrostatic coupling degrades the energy-delay-product of anM3D NoC by 18.1% averaged over eight different applications from SPLASH-2 and PARSEC benchmark suites. As a countermeasure, we advocate the adoption of electrostatic coupling-aware M3D NoC design methodology. Experimental results show that the coupling-aware M3D NoC reduces performance penalty by lowering the number of multi-tier routers significantly. © 2019 Association for Computing Machinery.",EDP; Electrostatic coupling; energy; Energy efficiency; Latency; Monolithic 3D; NoC; Optimization; Performance,Benchmarking; Electronics packaging; Electrostatics; Energy efficiency; Network-on-chip; Optimization; Routers; Three dimensional integrated circuits; Electrostatic coupling; energy; High-density integration; Inter-layer dielectrics; Latency; Network-on-chip architectures; Performance; Performance degradation; Integrated circuit design
Cut optimization for redundant via insertion in self-Aligned double patterning,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075547815&doi=10.1145%2f3355391&partnerID=40&md5=fe56d48c3b07cd5c2d10cb002efe4f08,"Redundant via (RV) insertion helps prevent via defects and hence leads to yield enhancement. However, RV insertion in self-Aligned double patterning (SADP) processes is challenging since cut optimization has to be considered together. In SADP, parallel one-dimensional metal lines are divided into signal wires and dummy wires by line-end cuts. If an RV is inserted, signal wires need to be extended to connect to the RV. To this end, an additional cut, which we call RV cut, is introduced to make a space for the extension. Since RV cuts and line-end cuts are manufactured with the same mask set, design rules between those cuts have to be honored, which incurs proper distribution and mask assignment to individual cuts. In this article, we address a problem of integrated RV insertion and cut optimization. We show that the problem can be formulated as an integer linear programming (ILP). We also propose a heuristic algorithm is presented for practical application, in which potential locations of RVs are first identified and used to properly insert as many RVs as possible while minimizing the conflict between RV cuts. Our experimental results demonstrate that 75% of vias receive RVs with 8% increase in total wire length, which is only slightly worse than the optimal result obtained by ILP. © 2019 Association for Computing Machinery. All rights reserved.",Cut mask optimization; Redundant via insertion; Self-Aligned double patterning,Heuristic algorithms; Signal processing; Wire; Integer Linear Programming; Mask optimization; Optimal results; Redundant via; Redundant via insertion; Self-aligned double patterning; Total wire length; Yield enhancement; Integer programming
Approximate data reuse-based accelerator design for embedded processor,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075740771&doi=10.1145%2f3342098&partnerID=40&md5=3b9c9ae6c7def1c7fcd0511991171993,"Due to increasing diversity and complexity of applications in embedded systems, accelerator designs tradingoff area/energy-efficiency and design-productivity are becoming a further crucial issue. Targeting applications in the category of Recognition, Mining, and Synthesis (RMS), this study proposes a novel accelerator design to achieve a good trade-off in efficiency and design-productivity (or reusability) by introducing a new computing paradigm called ""approximate computing"" (AC). Leveraging from the facts that frequently executed parts of applications (i.e., hotspots) are conventionally the target of acceleration and that RMS applications are error-tolerant and often take similar input data repeatedly, our proposed accelerator reuses previous computational results of similar enough data to reduce computations. The proposed accelerator is composed of a simple controller and a dedicated memory to store limited sets of previous input data with corresponding computational results in a hotspot. Therefore, this accelerator can be applied to different and/or multiple hotspots/applications only through small extension of the controller, to achieve efficient accelerator design and resolve the design-productivity issue.We conducted quantitative evaluations using a representative RMS application (image compression) to demonstrate the effectiveness of our method over conventional ones with precise computing. Moreover, we provide important findings on parameter exploration for our accelerator design, offering a wider applicability of our accelerator to other applications. © 2019 Association for Computing Machinery.",Approximate computing; Data reuse; Embedded processor,Economic and social effects; Embedded systems; Image compression; Input output programs; Integrated circuit design; Productivity; Reusability; Approximate computing; Computational results; Computing paradigm; Data reuse; Design productivity; Embedded processors; Parameter exploration; Quantitative evaluation; Acceleration
Optimization of threshold logic networks with node merging and wire replacement,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075573126&doi=10.1145%2f3358748&partnerID=40&md5=0fa6ca10d0be3cc7f432df60a42fd6e5,"In this article, we present an optimization method for threshold logic networks (TLNs) based on observability don't-care-based node merging. To reduce gate count in a TLN, it iteratively merges two gates that are functionally equivalent or whose differences are never observed at the primary outputs. Furthermore, it is able to identify redundant wires and replace wires for removing more gates. Basically, the proposed method is primarily adapted from an ATPG-based node-merging approach which works for conventional Boolean logic networks. To extend the approach for TLNs, we develop a method for computing mandatory assignments of a stuck-At fault test on a threshold gate and a method for conducting logic implication in a TLN. Additionally, to achieve a better optimization quality, we integrate the proposed method with other optimization methods. The experimental results show that the overall optimization method can save an average of approximately 4.7% threshold gates for a set of TLNs which are generated by using the latest TLN synthesis method. The experimental results also demonstrate the efficiency of the optimization method. © 2019 Association for Computing Machinery. All rights reserved.",logic optimization; node merging; Threshold logic; wire replacement,Iterative methods; Logic Synthesis; Merging; Resonant tunneling; Threshold elements; Threshold logic; Wire; Boolean logic; Logic optimization; Optimization method; Optimization quality; Primary outputs; Stuck-at fault tests; Synthesis method; Threshold gates; Computer circuits
Modeling and simulation of dynamic applications using scenario-aware dataflow,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075723707&doi=10.1145%2f3342997&partnerID=40&md5=0176beedca837a917f0bed133ed1d48e,"The tradeoff between analyzability and expressiveness is a key factor when choosing a suitable dataflow model of computation (MoC) for designing, modeling, and simulating applications considering a formal base. A large number of techniques and analysis tools exist for static dataflow models, such as synchronous dataflow. However, they cannot express the dynamic behavior required for more dynamic applications in signal streaming or to model runtime reconfigurable systems. On the other hand, dynamic dataflow models like Kahn process networks sacrifice analyzability for expressiveness. Scenario-aware dataflow (SADF) is an excellent tradeoff providing sufficient expressiveness for dynamic systems, while still giving access to powerful analysis methods. In spite of an increasing interest in SADF methods, there is a lack of formallydefined functional models for describing and simulating SADF systems. This article overcomes the current situation by introducing a functional model for the SADF MoC, as well as a set of abstract operations for simulating it. We present the first modeling and simulation tool for SADF so far, implemented as an open source library in the functional framework ForSyDe.We demonstrate the capabilities of the functional model through a comprehensive tutorial-style example of a RISC processor described as an SADF application, and a traditional streaming application where we model an MPEG-4 simple profile decoder. We also present a couple of alternative approaches for functionally modeling SADF on different languages and paradigms. One of such approaches is used in a performance comparison with our functional model using theMPEG-4 simple profile decoder as a test case. As a result, our proposed model presented a good tradeoff between execution time and implementation succinctness. Finally, we discuss the potential of our formal model as a frontend for formal system design flows regarding dynamic applications. © 2019 Association for Computing Machinery.",Modeling; Scenario-aware dataflow (SADF); Simulation,Abstracting; Decoding; Modeling languages; Models; Motion Picture Experts Group standards; Reduced instruction set computing; Structural design; Dataflow; Kahn process networks; Modeling and simulation tools; Open-source libraries; Performance comparison; Simulation; Streaming applications; Synchronous Dataflow; Data flow analysis
Smart-hop arbitration request propagation: Avoiding quadratic arbitration complexity and false negatives in smart nocs,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075547591&doi=10.1145%2f3356235&partnerID=40&md5=68599f1192fe7931f299fc9308337d0e,"SMART-based NoC designs achieve ultra-low latencies by enabling flits to traverse multiple hops within a single clock cycle. Notwithstanding the clear performance benefits, SMART-based NoCs suffer from several shortcomings: each router must arbitrate among a quadratic number of requests, which leads to high costs; each router independently makes its own arbitration decisions, which leads to a problem called false negatives that causes throughput loss. In this article, we propose a new SMART-based NoC design called SHARP that overcomes these shortcomings. Our evaluation demonstrates that SHARP increases throughput by up to 19% and average link utilization by up to 24% by avoiding false negatives. By avoiding quadratic arbitration, our evaluation further demonstrates that SHARP reduces the wiring and area overhead significantly. © 2019 Association for Computing Machinery. All rights reserved.",Networks-on-chip (NoC); single-cycle multi-hop asynchronous traversal (SMART),Routers; False negatives; Link utilization; Multihop; Networks on chips; Performance benefits; Quadratic number; Sharp increase; Single-clock-cycle; Network-on-chip
Investigating the impact of image content on the energy efficiency of hardware-accelerated digital spatial filters,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075750354&doi=10.1145%2f3341819&partnerID=40&md5=521f23571afa2c6a34d69d0b21c2ff0e,"Battery-operated low-power portable computing devices are becoming an inseparable part of human daily life. One of the major goals is to achieve the longest battery life in such a device. Additionally, the need for performance in processing multimedia content is ever increasing. Processing image and video content consume more power than other applications. A widely used approach to improving energy efficiency is to implement the computationally intensive functions as digital hardware accelerators. Spatial filtering is one of the most commonly used methods of digital image processing. As per the Fourier theory, an image can be considered as a two-dimensional signal that is composed of spatially extended two-dimensional sinusoidal patterns called gratings. Spatial frequency theory states that sinusoidal gratings can be characterised by its spatial frequency, phase, amplitude, and orientation. This article presents results from our investigation into assessing the impact of these characteristics of a digital image on the energy efficiency of hardware-accelerated spatial filters employed to process the same image. Two greyscale images each of size 128 × 128 pixels comprising two-dimensional sinusoidal gratings at maximum spatial frequency of 64 cycles per image orientated at 0° and 90°, respectively, were processed in a hardware implemented Gaussian smoothing filter. The energy efficiency of the filter was compared with the baseline energy efficiency of processing a featureless plain black image. The results show that energy efficiency of the filter drops to 12.5% when the gratings are orientated at 0° whilst rises to 72.38% at 90°. © 2019 Association for Computing Machinery.",Energy consumption; Energy efficiency; FPGA; Hardware acceleration; Image processing; Power consumption; Sinusoidal grating; Spatial frequency,Beamforming; Computation theory; Computerized tomography; Electric batteries; Electric power utilization; Energy utilization; Field programmable gate arrays (FPGA); Image processing; Gaussian smoothing filter; Hardware acceleration; Hardware-accelerated; Portable computing devices; Sinusoidal grating; Spatial frequency; Spatial-frequency theory; Two-dimensional signals; Energy efficiency
Energy efficient chip-To-chip wireless interconnection for heterogeneous architectures,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075574313&doi=10.1145%2f3340109&partnerID=40&md5=8af2757c946e26b454925a3819c86613,"Heterogeneous multichip architectures have gained significant interest in high-performance computing clusters to cater to a wide range of applications. In particular, heterogeneous systems with multiple multicore CPUs, GPUs, and memory have become common to meet application requirements. The shared resources like interconnection network in such systems pose significant challenges due to the diverse traffic requirements of CPUs and GPUs. Especially, the performance and energy consumption of inter-chip communication have remained a major bottleneck due to limitations imposed by off-chip wired links. To overcome these challenges, we propose a wireless interconnection network to provide energy-efficient, high-performance communication in heterogeneous multi-chip systems. Interference-free communication between GPUs and memory modules is achieved through directional wireless links, while omnidirectional wireless interfaces connect cores in the CPUs with other components in the system. Besides providing low-energy, highbandwidth inter-chip communication, the wireless interconnection scales efficiently with system size to provide high performance across multiple chips. The proposed inter-chip wireless interconnection is evaluated on two system sizes with multiple CPU and multiple GPU chips, along with main memory modules. On a system with 4 CPU and 4 GPU chips, application runtime is sped up by 3.94, packet energy is reduced by 94.4%, and packet latency is reduced by 58.34% as compared to baseline system with wired inter-chip interconnection. © 2019 Association for Computing Machinery. All rights reserved.",Heterogeneous architectures; Inter-chip wireless; Multi-chip system,Energy efficiency; Energy utilization; Interconnection networks (circuit switching); Memory architecture; Microprocessor chips; Network architecture; Program processors; Wireless interconnects; Application requirements; Heterogeneous architectures; High performance communication; High-performance computing clusters; Inter-chip; Interchip communications; Multi chip system; Wireless interconnection; Integrated circuit interconnects
Revealing cluster hierarchy in gate-level ics using block diagrams and cluster estimates of circuit embeddings,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075564940&doi=10.1145%2f3329081&partnerID=40&md5=6cbe8947835a5408eef9a4e9735d565e,"Contemporary integrated circuits (ICs) are increasingly being constructed using intellectual property blocks (IPs) obtained from third parties in a globalized supply chain. The increased vulnerability to adversarial changes during this untrusted supply chain raises concerns about the integrity of the end product. The difference in the levels of abstraction between the initial specification and the final available circuit design poses a challenge for analyzing the final circuit for malicious insertions. Reverse engineering presents one way to help reduce the difficulty of circuit analysis and inspection. In this work, we provide a framework that given (i) a gate-level netlist of a design and (ii) a block diagram for the design with relative sizes of the blocks, outputs a matching between the partitions of the circuit and blocks in the block diagram. We first compute a geometric embedding for each node in the circuit and then apply a clustering algorithm on the embedding features to obtain circuit partitions. Each partition is then mapped to the high-level blocks in the block diagram. These partitions can then be further analyzed for malicious insertions with much reduced complexity in comparison with the full chip. We tested our algorithm on different designs with varying sizes to evaluate the efficacy of algorithm, including the open-source processor OpenSparc T1, and showed that we can successfully match over 90% of gates to their corresponding blocks. © 2019 Association for Computing Machinery. All rights reserved.",Clustering; Partitioning; Reverse engineering,Electric network analysis; Embeddings; Integrated circuit design; Integrated circuit manufacture; Integrated circuits; Reverse engineering; Supply chains; Timing circuits; Block diagrams; Circuit designs; Clustering; Geometric embedding; Integrated circuits (ICs); Levels of abstraction; Partitioning; Reduced complexity; Clustering algorithms
Stress-induced performance shifts in 3d drams,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075568963&doi=10.1145%2f3331527&partnerID=40&md5=4081d08c3f20e2845df58d9c9eb21c4f,"3D-stacked DRAMs can significantly increase cell density and bandwidth while also lowering power consumption. However, 3D structures experience significant thermomechanical stress due to the differential rate of contraction of the constituent materials, which have different coefficients of thermal expansion. This impacts circuit performance. This article develops a procedure that performs a performance analysis of 3D DRAMs, capturing the impact of both layout-Aware stress and layout-independent stress on parameters such as latency, leakage power, refresh power, area, and bus delay. The approach first proposes a semianalytical stress analysis method for the entire 3D DRAM structure, capturing the stress induced by through-siliconvias (TSVs), micro bumps, package bumps, and warpage. Next, this stress is translated to variations in device mobility and threshold voltage, after which analytical models for latency, leakage power, and refresh power are derived. Finally, a complete analysis of performance variations is performed for various 3D DRAM layout configurations to assess the impact of layout-dependent stress. We explore the use of alternative flexible package substrate options to mitigate the performance impact of stress. Specifically, we explore the use of an alternative bendable package substrate made of polyimide to reduce warpage-induced stress, and show that it reduces stress-induced variations and improves the performance metrics for stacked 3D DRAMs. © 2019 Association for Computing Machinery. All rights reserved.",3D DRAMs; finite element analysis; package substrate; performance analysis; Stress; wide I/O,Finite element method; Integrated circuit layout; Stress analysis; Stresses; Thermal expansion; Threshold voltage; 3d drams; Coefficients of thermal expansions; Constituent materials; Package substrates; Performance analysis; Performance variations; Thermo-mechanical stress; wide I/O; Three dimensional integrated circuits
Reducing dram refresh rate using retention time aware universal hashing redundancy repair,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075569397&doi=10.1145%2f3339851&partnerID=40&md5=823af379b7a48739faf83f57229654c0,"As the device capacity of Dynamic Random Access Memory (DRAM) increases, refresh operation becomes a significant contributory factor toward total power consumption and memory throughput of the device. To reduce the problems associated with the refresh operation, a multi-rate refresh technique that changes the refresh period based on the retention time of DRAM cells has been proposed. Unfortunately, the multi-rate refresh technique has a scalability issue, because the additional storage and logic overhead on a memory controller increases as the device capacity increases. In this article, we propose a novel redundancy repair technique to increase the refresh period of DRAM by using a universal hashing technique. Our redundancy repair technique efficiently repairs both hard faults, which occur during themanufacturing process, and weak cells that have short retention time using the remaining spare elements after the process. Also, our technique solves the Variable Retention Time problem by repairing weak cells at boot time by exploiting the Builtin self-repair (BISR) technique and Error Correction Code. Our technique outperforms a conventional BISR redundancy repair with very little hardware overhead, and ensure reliability with more extended refresh period in the entire system. In particular, our experimental results show that our BISR technique achieves 100% repair rate at a 384ms refresh period in 1.0e-6 hard fault BER configuration, and reduces the refresh energy consumption by 83.9% compared to the 64ms refresh and 12% compared to the conventional multirate refresh technique for the state-of-The-Art 4Gb device. © 2019 Association for Computing Machinery. All rights reserved.",BISR algorithm; DRAM refresh; Redundancy repair; Retention time,Energy utilization; Error correction; Redundancy; Repair; Built-in self-repair; Contributory factors; DRAM refresh; Dynamic random access memory; Error correction codes; Redundancy repair; Retention time; Total power consumption; Dynamic random access storage
Time-multiplexed fpga overlay architectures: A survey,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075615914&doi=10.1145%2f3339861&partnerID=40&md5=b15a26f8baca1f1a7aa69412aa442b91,"This article presents a comprehensive survey of time-multiplexed (TM) FPGA overlays from the research literature. These overlays are categorized based on their implementation into two groups: processor-based overlays, as their implementation follows that of conventional silicon-based microprocessors, and; CGRAlike overlays, with either an array of interconnected processor-based functional units or medium-grained arithmetic functional units. Time-multiplexing the overlay allows it to change its behavior with a cycle-bycycle execution of the application kernel, thus allowing better sharing of the limited FPGA hardware resource. However, most TMoverlays suffer from large resource overheads, due to either the underlying processor-like architecture (for processor-based overlays) or due to the routing array and instruction storage requirements (for CGRA-like overlays). Reducing the area overhead for CGRA-like overlays, specifically that required for the routing network, and better utilizing the hard macros in the target FPGA are active areas of research. © 2019 Association for Computing Machinery. All rights reserved.",FPGA overlay; Reconfigurable system; Time-multiplexing,Field programmable gate arrays (FPGA); Network architecture; Surveys; Arithmetic functional units; Functional units; Overlay architecture; Reconfigurable systems; Routing networks; Storage requirements; Time multiplexed; Time multiplexing; Time division multiplexing
Exploring the role of large centralised caches in thermal efficient chip design,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075549996&doi=10.1145%2f3339850&partnerID=40&md5=b994b3418d85f39987f2a640e03ac430,"In the era of short channel length, Dynamic Thermal Management (DTM) has become a challenging task for the architects and designers engineering modern Chip Multi-Processors (CMPs). Ever-increasing demand of processing power along with the developed integration technology produces CMPs with high power density, which in turn increases effective chip temperature. This increased temperature leads to increase in the reliability issues for the chip-circuitry with significant increment in leakage power consumption. Recent DTM techniques apply DVFS or Task Migration to reduce temperature at the cores, the hottest on-chip components, but often ignore the on-chip hot caches. To commensurate the high data demand of these cores, most of the modern CMPs are equipped with large multi-level on-chip caches, out of which on-chip Last Level Caches (LLCs) occupy the largest on-chip area. These LLCs are accounted for their significantly high leakage power consumption that can also potentially generate on-chip hotspots at the LLCs similar to the cores. As power consumption constructs the backbone of heat dissipation, hence, this work dynamically shrinks cache size while maintaining performance constraint to reduce LLC leakage, primarily. These turned-off cache portions further work as on-chip thermal buffers for reducing average and peak temperature of the CMP without affecting the computation. Simulation results claim that, at a minimal penalty on the performance, proposed cache-based thermal management having 8MB centralised multi-banked shared LLC gives around 5 C reduction in peak and average chip temperature, which are comparable with a Greedy DVFS policy. © 2019 Association for Computing Machinery. All rights reserved.",Cache memory; chip multi-processors(CMPs); dynamic power; hotspot; IPC; Last Level Cache(LLC); leakage power; reconfiguration time; temperature; thermal buffer,Buffer storage; Cache memory; Electric power utilization; Multiprocessing systems; Temperature; Temperature control; Thermal Engineering; Chip multi-processors (CMPs); Dynamic Power; Hot spot; Lastlevel caches (LLC); Leakage power; Reconfiguration time; Thermal buffers; Thermal management (electronics)
Automatic stage-form circuit reduction for multistage opamp design equation generation,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074880419&doi=10.1145%2f3363499&partnerID=40&md5=b75d4c32c4d4770faa9ca18938ce6391,"An automatic stage-form circuit reductionmethod for multistage operational amplifiers (opamps) is proposed. A tool based on this method can reduce a multistage opamp into a condensed stage-form macromodel, from which design equations can be generated automatically by another existing symbolic program. The proposed model generation method is fully symbolic; namely, it does not make reference to any numerical device values with a circuit, hence it does not require circuit biasing and sizing at an early design stage. The parameters coming with the generated models are dominant-effect approximation of the stage-related characteristics of the original circuits and thus are visually readable for design reasoning. Compensations in the original circuits are extracted automatically and reserved in the macromodel circuits. The user of this tool is only required to input the circuit stage information by identifying several key devices in the original circuits. As design equations can also be automatically generated from stage-form macromodels by a purely symbolic method, the proposed model generation method completes the path from a transistor-level opamp circuit to its characteristic design equations in a completely formal way. Examples are provided to demonstrate the effectiveness of the proposed model generation method, and numerical validation is further carried out to verify that the reduced symbolic models can successfully capture the key circuit behavior in the frequency domain for multistage opamps. © 2019 Association for Computing Machinery.",Analog integrated circuit (IC); Computer-aided design (CAD); Design equation; Operational amplifier (opamp); Pole-zero; Sizing; Symbolic circuit analysis; Symbolic model reduction,Analog integrated circuits; Computer aided analysis; Electric network analysis; Equivalent circuits; Frequency domain analysis; Numerical methods; Operational amplifiers; Timing circuits; Design equation; Operational amplifier (opamp); Pole-zero; Sizing; Symbolic circuit analysis; Symbolic model; Computer aided design
Energy-efficient and quality-assured approximate computing framework using a co-training method,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071101640&doi=10.1145%2f3342239&partnerID=40&md5=e761fad5f0011f1d1b57fe2d38b45dc8,"Approximate computing is a promising design paradigm that introduces a new dimension-error-into the original design space. By allowing the inexact computation in error-tolerance applications, approximate computing can gain both performance and energy efficiency. A neural network (NN) is a universal approximator in theory and possesses a high level of parallelism. The emerging deep neural network accelerators deployed with NN-based approximator is thereby a promising candidate for approximate computing. Nevertheless, the approximation result must satisfy the users' requirement, and the approximation result varies across different applications.We normally deploy an NN-based classifier to ensure the approximation quality. Only the inputs predicted to meet the quality requirement can be executed by the approximator. The potential of these two NNs, however, is fully explored; the involving of two NNs in approximate computing imposes critical optimization questions, such as two NNs' distinct views of the input data space, how to train the two correlated NNs, and what are their topologies. In this article, we propose a novel NN-based approximate computing framework with quality insurance. We advocate a co-training approach that trains the classifier and the approximator alternately to maximize the agreement of the two NNs on the input space. In each iteration, we coordinate the training of the two NNs with a judicious selection of training data. Next, we explore different selection policies and propose to select training data from multiple iterations, which can enhance the invocation of the approximate accelerator. In addition, we optimize the classifier by integrating a dynamic threshold tuning algorithm to improve the invocation of the approximate accelerator further. The increased invocation of accelerator leads to higher energy efficiency under the same quality requirement. We propose two efficient algorithms to explore the smallest topology of the NN-based approximator and the classifier to achieve the quality requirement. The first algorithm straightforward searches the minimum topology using a greedy strategy. However, the first algorithm incurs too much training overhead. To solve this issue, the second one gradually grows the topology of NNs to match the quality requirement by transferring the learned parameters. Experimental results show significant improvement on the quality and the energy efficiency compared to the existing NN-based approximate computing frameworks. © 2019 Association for Computing Machinery.",Approximate computing; Error control,Computation theory; Deep neural networks; Errors; Green computing; Iterative methods; Topology; Approximate computing; Approximation quality; Approximation results; Computing frameworks; Error control; Neural network (nn); Quality requirements; Universal approximators; Energy efficiency
Cross-point Resistive Memory: Nonideal properties and solutions,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069196340&doi=10.1145%2f3325067&partnerID=40&md5=bc84c6df208e692c792a52eabb2692b6,"Emerging computational resistive memory is promising to overcome the challenges of scalability and energy efficiency that DRAM faces and also break through the memory wall bottleneck. However, cell-level and array-level nonideal properties of resistive memory significantly degrade the reliability, performance, accuracy, and energy efficiency during memory access and analog computation. Cell-level nonidealities include nonlinearity, asymmetry, and variability. Array-level nonidealities include interconnect resistance, parasitic capacitance, and sneak current. This review summarizes practical solutions that can mitigate the impact of nonideal device and circuit properties of resistive memory. First, we introduce several typical resistive memory devices with focus on their switching modes and characteristics. Second, we review resistive memory cells and memory array structures, including 1T1R, 1R, 1S1R, 1TnR, and CMOL. We also overview three-dimensional (3D) cross-point arrays and their structural properties. Third, we analyze the impact of nonideal device and circuit properties during memory access and analog arithmetic operations with focus on dot-product and matrix-vector multiplication. Fourth, we discuss the methods that can mitigate these nonideal properties by static parameter and dynamic runtime co-optimization from the viewpoint of device and circuit interaction. Here, dynamic runtime operation schemes include line connection, voltage bias, logical-to-physical mapping, read reference setting, and switching mode reconfiguration. Then, we highlight challenges on multilevel cell cross-point arrays and 3D cross-point arrays during these operations. Finally, we investigate design considerations of memory array peripheral circuits. We also portray an unified reconfigurable computational memory architecture. © 2019 Association for Computing Machinery.",Analog circuits; Array architecture; CMOS-memristor hybrid circuits; Crossbar; Memory integration technology; Multilevel; Selector; Transistor structure; Transposable memory,Analog circuits; Analog computers; Capacitance; Cells; Cytology; Dynamic random access storage; Electric network analysis; Energy efficiency; Reconfigurable architectures; Array architecture; Crossbar; Integration technologies; Memristor; Multilevel; Selector; Transistor structure; Memory architecture
Fault tolerance technique offlining faulty blocks by heap memory management,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069207735&doi=10.1145%2f3329079&partnerID=40&md5=aaac163254a4f2472c8face8155cb279,"As dynamic random access memory (DRAM) cells continue to be scaled down for higher density and capacity, they have more faults. Thus, DRAM reliability becomes a major concern in computer systems. Previous studies have proposed many techniques preserving the reliability in various system components, such as DRAM internal, memory controller, caches, and operating systems. By reviewing the techniques, we identified the following two considerations: First, it is possible to recover faults with reasonable overhead at high fault rate only if the recovery unit is fine-grained. Second, since hardware modification requires additional cost in the employment of a technique, a pure software-based recovery technique is preferable. However, in the existing software-based recovery technique, the recovery unit is too coarse-grained to tolerate the high fault rate. In this article, we propose a pure software-based recovery technique with fine-granularity. Our key idea is based on heap segments being managed by the system library with variable-sized chunks to handle dynamic allocation in user applications. In our technique, faulty blocks in pages are offlined by marking them as allocated chunks. Thus, not only fault-free pages but also the remaining clean blocks in faulty pages are allowed to be usable space. Our technique is implemented by modifying the operating system and the system library. Since hardware assistance is unnecessary in the implementation, we evaluated our method on a real machine. Our evaluation results show that our technique has negligible performance overhead at high bit error rate (BER) 5.12e-5, which a hardware-based recovery technique could not tolerate without unacceptable area overhead. Also, at the same BER, our method provides 5.22× usable space, compared with page-offline, which is the state-of-the-art pure software-based technique. © 2019 Association for Computing Machinery.",DRAM fault recovery,Bit error rate; Cache memory; Fault tolerance; Fault tolerant computer systems; Recovery; Dynamic allocations; Dynamic random access memory; Fault recovery; Fault tolerance techniques; Hardware modifications; High bit error rates; Recovery techniques; Software-based techniques; Dynamic random access storage
Layout resynthesis by applying design-for-manufacturability guidelines to avoid low-coverage areas of a cell-based design,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069190034&doi=10.1145%2f3325066&partnerID=40&md5=0f41d7e71578f8c5170d71743f78fe52,"Design-for-manufacturability (DFM) guidelines are recommended layout design practices intended to capture layout features that are difficult to manufacture correctly. Avoiding such features prevents the occurrence of potential systematic defects. Layout features that result in DFM guideline violations may not be avoided completely due to the design constraints of chip area, performance, and power consumption. A framework for translating DFM guideline violations into potential systematic defects, and faults, was described earlier. In a cell-based design, the translated faults may be internal or external to cells. In this article, we focus on undetectable faults that are external to cells. Using a resynthesis procedure that makes fine changes to the layout while maintaining the design constraints, we target areas of the design where large numbers of external faults related to DFM guideline violations are undetectable. By eliminating the corresponding DFM guideline violations, we ensure that the circuit does not suffer from low-coverage areas that may result in detectable systematic defects escaping detection, but failing the circuit in the field. The layout resynthesis procedure is applied to benchmark circuits and logic blocks of the OpenSPARC T1 microprocessor. Experimental results indicate that the improvement in the coverage of potential systematic defects is significant. © 2019 Association for Computing Machinery.",Design-for-manufacturability (DFM) guidelines; Layout resynthesis; Systematic defects; undetectable faults,Defects; Electric fault currents; Machine design; Benchmark circuit; Cell-based design; Design constraints; External fault; Layout designs; Resynthesis; Systematic defects; Undetectable faults; Design for manufacturability
Compiler-Assisted and Profiling-Based Analysis for Fast and Efficient STT-MRAM On-Chip Cache Design,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069458060&doi=10.1145%2f3321693&partnerID=40&md5=5bddf75bdf51b1d86ff38e9d0fd841d8,"Spin Transfer Torque Magnetic Random Access Memory (STT-MRAM) is a promising candidate for large onchip memories as a zero-leakage, high-density and non-volatile alternative to the present SRAM technology. Since memories are the dominating component of a System-on-Chip, the overall performance of the system is highly dependent on that memories. Nevertheless, the high write energy and latency of the emerging STTMRAM are the most challenging design issues in a modern computing system. By relaxing the non-volatility of these devices, it is possible to reduce the write energy and latency costs, at the expense of reducing the retention time, which in turn may lead to loss of data. In this article, we propose a hybrid STT-MRAM design for caches with different retention capabilities. Then, based on the application requirements (i.e., execution time and memory access rate), program data layout is re-Arranged at compilation time for achieving fast and energy-efficient hybrid STT-MRAM on-chip memory design with no reliability degradation. The application requirements have been defined at function granularity based on profiling and compiler-level analysis, which estimate the required retention time and memory access rate, respectively. Experimental results show that the proposed hybrid STT-MRAM cache combined with profiling-based and compiler-level analysis for the data re-Arranging, on average, reduces the write energy per access by 49.7%. At system level, overall static and dynamic energy of the cache are reduced by 8.1% and 44%, respectively, whereas, the system performance has been improved up to 8.1%. © 2019 Association for Computing Machinery.",application requirements; compiler analysis; hybrid caches; magnetic tunneling junction; profiling analysis; read disturb; retention failures; Thermal stability factor,Application programs; Energy efficiency; Integrated circuit design; Magnetic leakage; Magnetic recording; Magnetic storage; Memory architecture; Program compilers; Static random access storage; System-on-chip; Application requirements; Compiler analysis; Hybrid caches; Magnetic tunneling junctions; profiling analysis; Read disturb; MRAM devices
Adaptive test for RF/analog circuit using higher order correlations among measurements,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069161577&doi=10.1145%2f3308566&partnerID=40&md5=ad6a47aaf708556038b4c0c02ae40cae,"As process variations increase and devices get more diverse in their behavior, using the same test list for all devices is increasingly inefficient. Methodologies that adapt the test sequence with respect to lot, wafer, or even a device's own behavior help contain the test cost while maintaining test quality. In adaptive test selection approaches, the initial test list, a set of tests that are applied to all devices to learn information, plays a crucial role in the quality outcome. Most adaptive test approaches select this initial list based on fail probability of each test individually. Such a selection approach does not take into account the correlations that exist among various measurements and potentially will lead to the selection of correlated tests. In this work, we propose a new adaptive test algorithm that includes a mathematical model for initial test ordering that takes correlations among measurements into account. The proposed method can be integrated within an existing test flow running in the background to improve not only the test quality but also the test time. Experimental results using four distinct industry circuits and large amounts of measurement data show that the proposed technique outperforms prior approaches considerably. © 2019 Association for Computing Machinery.",Fail rate; Higher order correlation; Initial test list; Marginality,Automation; Computer applications; Adaptive tests; Higher order correlation; Initial test list; Marginality; Measurement data; Process Variation; Test orderings; Test sequence; Timing circuits
MEMS-IC robustness optimization considering electrical and mechanical design and process parameters,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069148913&doi=10.1145%2f3325068&partnerID=40&md5=69a81fd4fb3e1ae45d890876aa5dd75f,"MEMS-based sensor circuits are traditionally designed separately using CAD tools specific to each energy domain (electrical and mechanical). This article presents a complete approach for combined MEMS-IC robustness optimization. Advanced methods for robustness analysis and optimization considering design, operating and process parameters, developed for integrated circuits, are transferred to MEMS-IC systems. Both electrical and mechanical design and process parameters are included in the optimization. The methodology is exemplified on two demonstrator examples: a MEMS microphone and a MEMS accelerometer, each with an integrated readout circuit. A successful optimization requires the simultaneous inclusion of design parameters and process tolerances from both energy domains. To save CPU time, a reduced-order, circuit-level model is used for the MEMS part and this model is created only when necessary. To integrate the generation of the simplified model into the optimization flow, a simulation-in-a-loop flow based on commercial tools for both the electrical and the mechanical domain has been implemented. © 2019 Association for Computing Machinery.",Circuit-level model; Diagnosis; MEMS-CMOS; Parallel simulation; Robustness optimization,Diagnosis; Integrated circuits; Microphones; Timing circuits; Circuit-level models; Integrated readout; MEMS accelerometer; MEMS-based sensors; Parallel simulations; Process parameters; Robustness analysis; Robustness optimizations; Integrated circuit design
On chip reconfigurable CMOS analog circuit design and automation against aging phenomena: Sense and react,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069198972&doi=10.1145%2f3325069&partnerID=40&md5=966901e9e84a56ede6ad0f92db6d02ba,"Performance of analog circuits degrades over time due to several time-dependent degradation mechanisms. Due to the increased aging problems in ever-shrinking dimensions, reliability of complementary metal-oxide-semiconductor analog circuits has become a major concern. Overdesign is a popular aging-aware circuit design approach, where circuit operation is guardbanded by choosing the design point beyond the optimal region. For the sake of reliability, power consumption and chip area are sacrificed in this approach, which is undesirable considering strict energy limitations in modern applications. Conversely, Sense and React (S&R) approach serves the same purpose without any additional power consumption, in which degradation of circuit features is detected by online monitoring and recovered immediately. Furthermore, such systems enable remote control and healing of circuits. However, design of an S&R system is quite complicated. In particular, determination of efficient aging signatures and design of recovery strategy are highly challenging problems. This study thoroughly discusses the design process of S&R systems and proposes computer-aided-design-based design strategies that reduce the designer effort considerably. A novel design automation tool for S&R systems was developed, in which signature selection and recovery determination were integrated. To demonstrate proposed design strategies, two different S&R systems are implemented, simulated, and discussed in detail. © 2019 Association for Computing Machinery.",Aging; Analog; CAD; CMOS; Design automation; Reconfigurable; Reliability; Sense and react; Signature selection,Aging of materials; Analog circuits; Automation; CMOS integrated circuits; Degradation; Electric power utilization; Integrated circuit design; Integrated circuit manufacture; Metals; MOS devices; Oxide semiconductors; Printed circuit design; Reconfigurable hardware; Recovery; Reliability; Remote control; Timing circuits; Analog; Design automations; Reconfigurable; Sense and react; Signature selections; Computer aided design
DCW: A reactive and predictable programming framework for LET-Based distributed real-time systems,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067800484&doi=10.1145%2f3317574&partnerID=40&md5=56a6aecd80f62437d3b4c2d8b0ec3ff0,"Real-time systems continuously interact with the physical environment and often have to satisfy stringent timing constraints imposed by their interactions. Those systems involve two main properties: reactivity and predictability. Reactivity allows the system to continuously react to a non-deterministic external environment, while predictability guarantees the deterministic execution of safety-critical parts of applications.However, with the increase in software complexity, traditional approaches to develop real-time systems make temporal behaviors difficult to infer, especially when the system is required to address non-deterministic aperiodic events from the physical environment. In this article, we propose a reactive and predictable programming framework, Distributed Clockwerk (DCW), for distributed real-time systems. DCWintroduces the Servant, which is a non-preemptible execution entity, to implement periodic tasks based on the Logical Execution Time (LET) model. Furthermore, a joint schedule policy, based on the slack stealing algorithm, is proposed to efficiently address aperiodic events with no violated hard-time constraints. To further support predictable communication among distributed nodes, DCW implements the Time-Triggered Controller Area Network (TTCAN) to avoid collisions while accessing the shared communication medium. Moreover, a programming framework implements to provide a set of programming APIs for defining timing and functional behaviors of concurrent tasks. An example is further implemented to illustrate the DCW design flow. The evaluation results demonstrate that our proposal can improve both periodic and aperiodic reactivity compared with existing work, and the implemented DCW can also ensure the system predictability by achieving extremely low overheads. © 2019 Association for Computing Machinery.",Distributed real-time systems; Logical execution time; Predictability; Reactivity,Application programming interfaces (API); Control system synthesis; Distributed computer systems; Functional programming; Interactive computer systems; Reactivity (nuclear); Safety engineering; Controller area network; Deterministic execution; Distributed real time system; Logical execution time; Predictability; Programming framework; Shared communication medium; Traditional approaches; Real time systems
Enabling IC traceability via blockchain pegged to embedded PUF,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065724970&doi=10.1145%2f3315669&partnerID=40&md5=9381f8b9449d7fc08ed1561c2d0c4ccf,"Globalization of IC supply chain has increased the risk of counterfeit, tampered, and re-packaged chips in the market. Counterfeit electronics poses a security risk in safety critical applications like avionics, SCADA systems, and defense. It also affects the reputation of legitimate suppliers and causes financial losses. Hence, it becomes necessary to develop traceability solutions to ensure the integrity of supply chain, from the time of fabrication to the end of product-life, which allows a customer to verify the provenance of a device or a system. In this article, we present an IC traceability solution based on blockchain. A blockchain is a public immutable database that maintains a continuously growing list of data records secured from tampering and revision. Over the lifetime of an IC, all ownership transfer information is recorded and archived in a blockchain. This safe, verifiable method prevents any party from altering or challenging the legitimacy of the information being exchanged. However, a chain of sales record is not enough to ensure provenance of an IC. There is a need for clone-proof method for securely binding the identity of an IC to the blockchain information. In this article, we propose a method of IC supply chain traceability via blockchain pegged to embedded physically unclonable function (PUF). The blockchain provides ownership transfer record, while the PUF provides unique identification for an IC allowing it to be linked uniquely to a blockchain. Our proposed solution automates hardware and software protocols using blockchain-powered Smart Contract that allows supply chain participants to authenticate, track, trace, analyze, and provision chips throughout their entire life cycle. © 2019 Association for Computing Machinery.",Blockchain; Ownership transfer; Physically unclonable function; Smart contract; Supply chain; Traceability,Crime; Integrated circuits; Life cycle; Losses; Safety engineering; SCADA systems; Supply chains; Timing circuits; Entire life cycles; Hardware and software; IC supply chain; Ownership transfers; Physically unclonable functions; Safety critical applications; Traceability; Unique identifications; Blockchain
Electronics supply chain integrity enabled by blockchain,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065803990&doi=10.1145%2f3315571&partnerID=40&md5=a4ace31593c8f1885e0e4724c97a573b,"Electronic systems are ubiquitous today, playing an irreplaceable role in our personal lives, as well as in critical infrastructures such as power grids, satellite communications, and public transportation. In the past few decades, the security of software running on these systems has received significant attention. However, hardware has been assumed to be trustworthy and reliable “by default” without really analyzing the vulnerabilities in the electronics supply chain. With the rapid globalization of the semiconductor industry, it has become challenging to ensure the integrity and security of hardware. In this article, we discuss the integrity concerns associated with a globalized electronics supply chain. More specifically, we divide the supply chain into six distinct entities: IP owner/foundry (OCM), distributor, assembler, integrator, end user, and electronics recycler, and analyze the vulnerabilities and threats associated with each stage. To address the concerns of the supply chain integrity, we propose a blockchain-based certificate authority framework that can be used to manage critical chip information such as electronic chip identification, chip grade, and transaction time. The decentralized nature of the proposed framework can mitigate most threats of the electronics supply chain, such as recycling, remarking, cloning, and overproduction. © 2019 Association for Computing Machinery.",Blockchain; Electronics supply chain; Integrity; Trust,Blockchain; Electric power transmission networks; Satellite communication systems; Semiconductor device manufacture; Certificate authority; Electronics supply chain; Integrity; Public transportation; Satellite communications; Semiconductor industry; Supply chain integrities; Trust; Supply chains
Thermal-aware 3D symmetrical buffered clock tree synthesis,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065778138&doi=10.1145%2f3313798&partnerID=40&md5=98b612dabdb21cff5517a077a16ca497,"The semiconductor industry has accepted three-dimensional integrated circuits (3D ICs) as a possible solution to address speed and power management problems. In addition, 3D ICs have recently demonstrated a huge potential in reducing wire length and increasing the density of a chip. However, the growing density in chips such as TSV-based 3D ICs has brought the increased temperature on chip and temperature gradients depending on location. Thus, through silicon via (TSV)-based 3D clock tree synthesis (CTS) causes thermal problems leading to large clock skew. We propose a novel 3D symmetrical buffered clock tree synthesis considering thermal variation. First, 3D abstract tree topology based on nearest-neighbor selection with median cost (3D-NNM) is constructed by pairing sinks that have similar power consumption. Second, the layer assignment of internal nodes is determined for uniform TSV distribution. Third, in thermal-aware 3D deferred merging embedding (DME), the exact location of TSV is determined and wire routing/buffer insertion are performed after the thermal profile based on grid is obtained. The proposed method is verified using a 45nm process technology and utilized a predictive technology model (PTM) with HSPICE. It is also evaluated for the IBM benchmarks and ISPD'09 benchmarks with no blockages. In experimental result, we achieve on average 19% of clock skew reduction compared to existing thermal-aware 3D CTS. Therefore, thermal-aware 3D symmetrical buffered clock tree synthesis presented in this work is very efficient for circuit reliability. © 2019 Association for Computing Machinery.",Clock skew; Clock tree synthesis; Routing; Thermal variation; TSV,Clock distribution networks; Electric clocks; Electronics packaging; Semiconductor device manufacture; Thermal management (electronics); Topology; Clock skews; Clock tree synthesis; Increased temperature; Power-management problem; Routing; Semiconductor industry; Thermal variation; Through-Silicon-Via (TSV); Three dimensional integrated circuits
Analysis of dissipative losses in modular reconfigurable energy storage systems using systemC TLM and systemC-AMS,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065724601&doi=10.1145%2f3321387&partnerID=40&md5=ef0043e0b2b12db069df7753ef8cd6f6,"Battery storage systems are becoming more popular in the automotive industry as well as in stationary applications. To fulfill the requirements in terms of power and energy, the literature is increasingly discussing electrically reconfigurable interconnection topologies. However, these topologies use switching elements on the cell and module level that exhibit an electric resistance due to their design and hence generate undesirable dissipative losses. In this article, we propose a new analysis and optimization framework to examine and minimize the losses in such topologies. For this purpose, we develop a SystemC model to investigate static and dynamic load scenarios, e.g., from the automotive domain. The model uses SystemC TLM for the digital subsystem, SystemC-AMS for the mixed-signal subsystem, and host-compiled simulation for the microcontroller executing the embedded software. Here, we analyze the impact of the dissipative losses on the system efficiency that depend on the modularization level, implying the number of serial and parallel switching elements. Our analysis clearly shows that in reconfigurable topologies, the modularization level has a significant influence on the losses, which in our automotive example covers several orders of magnitude. For the topologies we have investigated, the highest efficiency can be reached when a parallel-only modularization is aspired and the number of serial switching elements is minimized. It is also shown that the losses of the state-of-the-art topology with one battery pack protection switch are almost as high as in a smart cell approach in which each energy storage cell has its own switching element. However, due to the high number of switching elements, this results in a reduction of energy density and increases the system costs, showing that this is a multi-criteria optimization problem. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Automotive industry; Battery Pack; Computer software; Dynamic loads; Energy storage; Modular construction; Multiobjective optimization; Switching; Topology; Battery storage system; Energy storage systems; Host-compiled simulations; Interconnection topologies; Multicriteria optimization; Optimization framework; Static and dynamic loads; Stationary applications; Electric losses
Augmenting operating systems with OpenCL accelerators,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065798837&doi=10.1145%2f3315569&partnerID=40&md5=40069b0cfd6f56ebf49c6e793c19cba0,"Heterogeneous computing leverages more than one kind of processors to boost the performance of user-space applications with the heterogeneous programming languages, e.g., OpenCL. While some works have been done to accelerate the computations required by Linux kernel software, they are either application-specific solutions or tightly coupled with the certain computing platforms and are not able to support the general-purpose in-kernel accelerations using different types of processors. In this article, the general-purpose software framework called Kernel acceleration with OpenCL (KOCL), is proposed to tackle the problem. KOCL exposes a set of the high-level programming interfaces for the Linux kernel module developers to offload compute-intensive tasks on different hardware accelerators without managing and coordinating the platform-specific computing and memory resources. The simplified programming efforts are achieved by the developed platform management and memory models, which provide a systematic means of managing the heterogeneous hardware resources. In addition, the one- and zero-copy data-buffering schemes are offered by KOCL, so that the offloaded tasks deliver high performance on the platforms with different memory architectures. We have developed the prototype system to accelerate the Network-Attached Storage server applications. Significant performance improvements are achieved with the three different types of accelerators, i.e., the multicore processor, the integrated GPU, and the discrete GPU, respectively. We believe that KOCL is useful for the design of embedded appliances to evaluate the performance of design alternatives. © 2019 Association for Computing Machinery.",Encrypted file system; Heterogeneous computing; Kernel programming; Kernel samepage merging; Linux kernel; OpenCL acceleration; Shared memory architecture,Acceleration; Application programs; Closed loop control systems; Digital storage; Graphics processing unit; Integrated circuit design; Memory architecture; Multicore programming; Network architecture; Software prototyping; Space applications; Compute-intensive tasks; Encrypted File System; General purpose software; Heterogeneous computing; Heterogeneous programming; Linux kernel; Network attached storage; Shared memory architecture; Linux
Comparing platform-aware control design flows for composable and predictable TDM-based execution platforms,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065778688&doi=10.1145%2f3315572&partnerID=40&md5=ad77e4f01f67f509d6387d20bbff50bf,"We compare three platform-aware feedback control design flows that are tailored for a composable and predictable Time Division Multiplexing (TDM)-based execution platform. The platform allows for independent execution of multiple applications. Using the precise timing knowledge of the platform execution, we accurately characterise the execution of the control application (i.e., sensing, computing, and actuating operations) to design efficient feedback controllers with high control performance in terms of settling time. The design flows are derived for Single-Rate (SR) and Multi-Rate (MR) sampling schemes. We show the applicability of the design flows based on two design considerations and their trade-off: control performance and resource utilisation. The design flows are validated by means of MATLAB and Hardware-in-the-Loop (HIL) experiments for a motion control application. © 2019 Association for Computing Machinery.",Embedded control systems; Linear Quadratic Regulator (LQR); Multi rate systems; Switched linear systems; TDM execution platforms,Economic and social effects; Embedded systems; Feedback control; Linear systems; MATLAB; Embedded control systems; Execution platforms; Linear quadratic regulator; Multi-rate systems; Switched linear system; Time division multiplexing
Data-driven anomaly detection with timing features for embedded systems,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065781902&doi=10.1145%2f3279949&partnerID=40&md5=339be2eb156f435027f7643dffbe8336,"Malware is a serious threat to network-connected embedded systems, as evidenced by the continued and rapid growth of such devices, commonly referred to as the Internet of Things. Their ubiquitous use in critical applications require robust protection to ensure user safety and privacy. That protection must be applied to all system aspects, extending beyond protecting the network and external interfaces. Anomaly detection is one of the last lines of defence against malware, in which data-driven approaches that require the least domain knowledge are popular. However, embedded systems, particularly edge devices, face several challenges in applying data-driven anomaly detection, including unpredictability of malware, limited tolerance to long data collection windows, and limited computing/energy resources. In this article, we utilize subcomponent timing information of software execution, including intrinsic software execution, instruction cache misses, and data cache misses as features, to detect anomalies based on ranges, multi-dimensional Euclidean distance, and classification at runtime. Detection methods based on lumped timing range are also evaluated and compared. We design several hardware detectors implementing these data-driven detection methods, which non-intrusively measuring lumped/subcomponent timing of all system/function calls of the embedded application. We evaluate the area, power, and detection latency of the presented detector designs. Experimental results demonstrate that the subcomponent timing model provides sufficient features to achieve high detection accuracy with low false-positive rates using a one-class support vector machine, considering sophisticated mimicry malware. © 2019 Association for Computing Machinery.",Anomaly detection; Embedded system security; One-class SVM; Software security; Timing-based detection,Cache memory; Classification (of information); Computer crime; Embedded systems; Feature extraction; Malware; Support vector machines; Timing circuits; Critical applications; Data-driven anomalies; Data-driven approach; Embedded application; Instruction cache miss; One class-SVM; One-class support vector machine; Software security; Anomaly detection
SSA-AC: Static significance analysis for approximate computing,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065719962&doi=10.1145%2f3314575&partnerID=40&md5=8f774addc065e989603f2c4e1051e606,"Recently, the quest to reduce energy consumption in digital systems has been the subject of a number of ongoing studies. One of the most researched focuses is approximate computing (AC). AC is a new computing paradigm in both hardware and software designs that aim to achieve energy-efficient digital systems. Although a variety of AC techniques have been studied so far, the main question, “How (in which section) can a program or a circuit be approximated?,” has not been answered yet. This work addresses the above issue by developing a software framework Static Significance Analysis for Approximate Computing (SSA-AC) to analyze the target application program and guide the designers to identify parts of the program to which approximation can or cannot be applied. SSA-AC statically analyzes the significance of variables in the precise version of the program and thus needs no trial-and-error evaluation or specific test data. Experimental results show that SSA-AC can successfully extract the significance ranking of inputs/variables to be approximated in much shorter time than existing statistical works that are inevitably data dependent. © 2019 Association for Computing Machinery.",Approximate computing; Framework; Path extraction; Significance analysis,Computer programming; Energy efficiency; Energy utilization; Software design; Software testing; Approximate computing; Computing paradigm; Framework; Hardware and software designs; Reduce energy consumption; Significance analysis; Software frameworks; Statistical works; Application programs
An optimized cost flow algorithm to spread cells in detailed placement,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065722982&doi=10.1145%2f3317575&partnerID=40&md5=c852373062d412165375c10baa0c88e3,"Placement is an important and challenging step in VLSI physical design. The placement solution can significantly impact timing and routability. In sub-nanometric technology nodes, several restrictions have been imposed on the placement solutions. These restrictions make designing an optimized and legal solution very hard. Achieving optimized placement solutions is especially challenging in regions with high-density utilization. The quality of placement solution can significantly impact the final circuit implementation. In this work, we present a cell spreading algorithm to move cells out from high-density utilization regions. Our algorithm opens up new spaces in regions with high cell concentration. These spaces can then be exploited by detailed placement algorithms to further optimize the placement solution. The objective of our technique is to reduce area density utilization while considering cell displacement and circuit delay. The outcome of the proposed algorithm is to obtain a uniform distribution of cells in the placement area while having minimal effects on the delay. To achieve this goal, our proposed algorithm uses branch and cut, and network flow techniques. Experimental results on industrial and academic circuits illustrate that our proposed algorithm can minimize circuit delay (up to 25%), cell displacement (up to 17μm), dynamic power consumption (up to 5.3%), and leakage power (up to 15%). © 2019 Association for Computing Machinery.",Branch and cut; Cell spreading; EDA; Network flow; Placement,Cytology; Delay circuits; Electric network analysis; Integer programming; Branch and cut; Cell spreading; Circuit implementation; Dynamic power consumption; Network flow techniques; Network flows; Placement; VLSI physical design; Cells
A novel resistive memory-based process-in-memory architecture for efficient logic and add operations,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065775391&doi=10.1145%2f3306495&partnerID=40&md5=8d42b13edd55033cffcbb1df96b0884b,"The coming era of big data revives the Processing-in-memory (PIM) architecture to relieve the memory wall problem that embarrasses the modern computing system. However, most existing PIM designs just put computing units closer to memory, rather than a complete integration of them due to their incompatibility in CMOS manufacturing. Fortunately, the emerging Resistive-RAM (ReRAM) offers new hope to this dilemma owing to its inherent memory and computing capability using the same device. In this article, we propose a ReRAM memory structure with efficient PIM capability of both logic and add operations. It first leverages non-linearity to suppress sneak current and thus sustains high memory density. Using a differential bit cell, it also enables efficient processing of arbitrary logic functions using the same memory cells with nondestructive operations. Then, a novel PIM adder is proposed, which customizes a sneak current path as the carry-chain for fast carry propagation and improves adder performance significantly. In the experiment, the proposed PIM demonstrates higher efficiency in both computing area and performance for logic and addition, which greatly increases the ReRAM PIM applicability for future computable architectures. © 2019 Association for Computing Machinery.",Carry propagation; PIM; ReRAM; Sneak current; Sneak path based adder,Adders; Computation theory; Computer circuits; Data handling; RRAM; Carry propagation; CMOS manufacturing; Computing capability; Higher efficiency; Path-based; Processing in memory; Resistive rams (ReRAM); Sneak currents; Memory architecture
Reconfigurable battery systems: A survey on hardware architecture and research challenges,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065761713&doi=10.1145%2f3301301&partnerID=40&md5=fde1cc35c541024901136e2a059b997f,"In a reconfigurable battery pack, the connections among cells can be changed during operation to form different configurations. This can lead a battery, a passive two-terminal device, to a smart battery that can reconfigure itself according to the requirement to enhance operational performance. Several hardware architectures with different levels of complexities have been proposed. Some researchers have used existing hardware and demonstrated improved performance on the basis of novel optimization and scheduling algorithms. The possibility of software techniques to benefit the energy storage systems is exciting, and it is the perfect time for such methods as the need for high-performance and long-lasting batteries is on the rise. This novel field requires new understanding, principles, and evaluation metrics of proposed schemes. In this article, we systematically discuss and critically review the state of the art. This is the first effort to compare the existing hardware topologies in terms of flexibility and functionality. We provide a comprehensive review that encompasses all existing research works, starting from the details of the individual battery including modeling and properties as well as fixed-topology traditional battery packs. To stimulate further research in this area, we highlight key challenges and open problems in this domain. © 2019 Copyright held by the owner/author(s).",Cell imbalance; Reconfigurable battery systems; State of charge; State of health,Battery management systems; Charging (batteries); Data storage equipment; Reconfigurable architectures; Reconfigurable hardware; Scheduling algorithms; Topology; Battery systems; Energy storage systems; Hardware architecture; Operational performance; Research challenges; State of charge; State of health; Two-terminal devices; Battery Pack
Compilation of Dataflow Applications for Multi-Cores using Adaptive Multi-Objective Optimization,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062822318&doi=10.1145%2f3310249&partnerID=40&md5=2e104f1533647944ef4a43c9d8f20105,"State-of-the-art system synthesis techniques employ meta-heuristic optimization techniques for Design Space Exploration (DSE) to tailor application execution, e.g., defined by a dataflow graph, for a given target platform. Unfortunately, the performance evaluation of each implementation candidate is computationally very expensive, in particular on recent multi-core platforms, as this involves compilation to and extensive evaluation on the target hardware. Applying heuristics for performance evaluation on the one hand allows for a reduction of the exploration time but on the other hand may deteriorate the convergence of the optimization technique toward performance-optimal solutions with respect to the target platform. To address this problem, we propose DSE strategies that are able to dynamically trade off between (i) approximating heuristics to guide the exploration and (ii) accurate performance evaluation, i.e., compilation of the application and subsequent performance measurement on the target platform. Technically, this is achieved by introducing a set of additional, but easily computable guiding objective functions, and varying the set of objective functions that are evaluated during the DSE adaptively. One major advantage of these guiding objectives is that they are generically applicable for dataflow models without having to apply any configuration techniques to tailor their parameters to the specific use case. We show this for synthetic benchmarks as well as a real-world control application. Moreover, the experimental results demonstrate that our proposed adaptive DSE strategies clearly outperform a state-of-the-art DSE approach known from literature in terms of the quality of the gained implementations as well as exploration times. Amongst others, we show a case for a two-core implementation where after about 3 hours of exploration time one of our proposed adaptive DSE strategies already obtains a 60% higher performance value than obtained by the state-of-the-art approach. Even when the state-of-the-art approach is given a total exploration time of more than 2 weeks to optimize this value, the proposed adaptive DSE strategy features a 20% higher performance value after a total exploration time of about 4 days. © 2019 Association for Computing Machinery.",Additional Key Words; Clustering; Design space exploration; Phrases: Dataflow,Data flow analysis; Economic and social effects; Function evaluation; Multiobjective optimization; Clustering; Dataflow; Design space exploration; Key words; Meta-heuristic optimization techniques; Performance measurements; State-of-the-art approach; State-of-the-art system; Benchmarking
Design automation for dilution of a fluid using programmable microfluidic device-based biochips,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062535716&doi=10.1145%2f3306492&partnerID=40&md5=0a1f40ff7969ef9320166d62e75d3198,"Microfluidic lab-on-a-chip has emerged as a new technology for implementing biochemical protocols on small-sized portable devices targeting low-cost medical diagnostics. Among various efforts of fabrication of such chips, a relatively new technology is a programmable microfluidic device (PMD) for implementation of flow-based lab-on-a-chip. A PMD chip is suitable for automation due to its symmetric nature. In order to implement a bioprotocol on such a reconfigurable device, it is crucial to automate a sample preparation on-chip as well. In this article, we propose a dilution PMD algorithm (namely DPMD) and its architectural mapping scheme (namely generalized architectural mapping algorithm (GAMA)) for addressing fluidic cells of such a device to perform dilution of a reagent fluid on-chip. We used an optimization function that first minimizes the number of mixing steps and then reduces the waste generation and further reagent requirement. Simulation results show that the proposed DPMD scheme is comparative to the existing state-of-the-art dilution algorithm. The proposed design automation using the architectural mapping scheme reduces the required chip area and, hence, minimizes the valve switching that, in turn, increases the life span of the PMD-chip. © 2019 Association for Computing Machinery.",Biochip; Continuous-flow; Microfluidics; Programmable microfluidic device,Automation; Biochips; Computer aided design; Conformal mapping; Diagnosis; Dilution; Fluidic devices; Lab-on-a-chip; Continuous flows; Design automations; Mapping algorithms; Medical diagnostics; Micro-fluidic devices; Optimization function; Reconfigurable devices; Sample preparation; Microfluidics
Integrated approach of airgap insertion for circuit timing optimization,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062374542&doi=10.1145%2f3306494&partnerID=40&md5=9443af013c6f82ef98af8f4f26d7a4ff,"Airgap technology enables air to be introduced in inter-metal dielectric (IMD). Airgap between certain wires reduces coupling capacitance due to the reduced permittivity; this can be utilized to decrease circuit delay. We propose an integrated approach of airgap insertion with the goal of circuit timing optimization. It consists of three sub-problems. We first select the layers that employ airgap, called airgap layers, that maximize total negative slack (TNS) improvement; this yields TNS improvement of 7% to 15% and worst negative slack (WNS) improvement of 2% to 8%, compared to a simple assumption of airgap layers. Second, we reassign the layers of wires such that more wires on critical paths can be placed in airgap layers. This is formulated as integer linear programming (ILP), and a more practical heuristic algorithm is also proposed. It provides an additional 17% TNS improvement and 6% WNS improvement. Finally, we perform airgap insertion through ILP formulation, where a number of design rules are modeled with linear constraints. To reduce the heavy runtime of ILP, a layout partitioning technique is also applied. It implements a feasible airgap mask in a manageable time where the amount of inserted airgap is close to the optimal solution. © 2019 Association for Computing Machinery.",Airgap; Airgap layer; Design rule; Layer reassignment; Timing optimization,Delay circuits; Dielectric materials; Heuristic algorithms; Integer programming; Integrated control; Wire; Air-gaps; Coupling capacitance; Design rules; Integer Linear Programming; Inter-metal dielectrics; Layer reassignment; Partitioning techniques; Timing optimization; Timing circuits
A cross-level verification methodology for digital IPs augmented with embedded timing monitors,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062889909&doi=10.1145%2f3308565&partnerID=40&md5=71613be2a0ff97e8116a2e8f0dc636c1,"Smart systems are characterized by the integration in a single device of multi-domain subsystems of different technological domains, namely, analog, digital, discrete and power devices, MEMS, and power sources. Such challenges, emerging from the heterogeneous nature of the whole system, combined with the traditional challenges of digital design, directly impact on performance and on propagation delay of digital components. This article proposes a design approach to enhance the RTL model of a given digital component for the integration in smart systems with the automatic insertion of delay sensors, which can detect and correct timing failures. The article then proposes a methodology to verify such added features at system level. The augmented model is abstracted to SystemC TLM, which is automatically injected with mutants (i.e., code mutations) to emulate delays and timing failures. The resulting TLM model is finally simulated to identify timing failures and to verify the correctness of the inserted delay monitors. Experimental results demonstrate the applicability of the proposed design and verification methodology, thanks to an efficient sensor-aware abstraction methodology, by applying the flow to three complex case studies. © 2019 Association for Computing Machinery.",Code abstraction; Razor sensor; Systemc TLM; Timing monitors; Verification,Abstracting; Artificial intelligence; Timing circuits; Verification; Code abstraction; Design approaches; Digital components; Digital designs; Propagation delays; SystemC; Timing failures; Verification methodology; Digital devices
Enhancing speculative execution with selective approximate computing,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062351989&doi=10.1145%2f3307651&partnerID=40&md5=c4b8f4c19b3c7437422a8b9a981d276d,"Speculative execution is an optimization technique used in modern processors by which predicted instructions are executed in advance with an objective of overlapping the latencies of slow operations. Branch prediction and load value speculation are examples of speculative execution used in modern pipelined processors to avoid execution stalls. However, speculative executions incur a performance penalty as an execution rollback when there is a misprediction. In this work, we propose to aid speculative execution with approximate computing by relaxing the execution rollback penalty associated with a misprediction. We propose a sensitivity analysis method for data and branches in a program to identify the data load and branch instructions that can be executed without any rollback in the pipeline and yet can ensure a certain user-specified quality of service of the application with a probabilistic reliability. Our analysis is based on statistical methods, particularly hypothesis testing and Bayesian analysis. We perform an architectural simulation of our proposed approximate execution and report the benefits in terms of CPU cycles and energy utilization on selected applications from the AxBench, ACCEPT, and Parsec 3.0 benchmarks suite. © 2019 Association for Computing Machinery.",Approximate computing; Bayesian analysis; Hypothesis testing; Speculative execution,Application programs; Benchmarking; Energy utilization; Pipeline processing systems; Quality of service; Reliability analysis; Sensitivity analysis; Statistical tests; Approximate computing; Architectural simulation; Bayesian Analysis; Hypothesis testing; Optimization techniques; Performance penalties; Pipelined processor; Speculative execution; Quality control
Integrated latch placement and cloning for timing optimization,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062348745&doi=10.1145%2f3301613&partnerID=40&md5=0443dda23c8a5f8cc37ee1a21eccb0c8,"This article presents an algorithm for integrated timing-driven latch placement and cloning. Given a circuit placement, the proposed algorithm relocates some latches while circuit timing is improved. Some latches are replicated to further improve the timing; the number of replicated latches along with their locations are automatically determined. After latch cloning, each of the replicated latches is set to drive a subset of the fanouts that have been driven by the original single latch. The proposed algorithm is then extended such that relocation and cloning are applied to some latches together with their neighbor logic gates. Experimental results demonstrate that the worst negative slack and the total negative slack are improved by 24% and 59%, respectively, on average of test circuits. The negative impacts on circuit area and power consumption are both marginal, at 0.7% and 1.9% respectively. © 2019 Association for Computing Machinery.",Latch cloning; Latch placement; Timing optimization; Timing-driven placement,Timing circuits; Circuit placement; Circuit timing; Latch placement; Test circuit; Timing driven placement; Timing optimization; Timing-driven; Cloning
Formal modeling and verification of a victim DRAM cache,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062331038&doi=10.1145%2f3306491&partnerID=40&md5=08ee0fe974b3d4c71a5e922ba3de456f,"The emerging Die-stacking technology enables DRAM to be used as a cache to break the “Memory Wall” problem. Recent studies have proposed to use DRAM as a victim cache in both CPU and GPU memory hierarchies to improve performance. DRAM caches are large in size and, hence, when realized as a victim cache, non-inclusive design is preferred. This non-inclusive design adds significant differences to the conventional DRAM cache design in terms of its probe, fill, and writeback policies. Design and verification of a victim DRAM cache can be much more complex than that of a conventional DRAM cache. Hence, without rigorous modeling and formal verification, ensuring the correctness of such a system can be difficult. The major focus of this work is to show how formal modeling is applied to design and verify a victim DRAM cache. In this approach, we identify the agents in the victim DRAM cache design and model them in terms of interacting state machines. We derive a set of properties from the specifications of a victim cache and encode them using Linear Temporal Logic. The properties are then proven using symbolic and bounded model checking. Finally, we discuss how these properties are related to the dataflow paths in a victim DRAM cache. © 2019 Association for Computing Machinery.",Architectural modeling; Communicating state machines; DRAM cache; GPGPU; Model checking; Victim cache,Dynamic random access storage; Formal verification; Integrated circuit design; Model checking; Program processors; Architectural modeling; Bounded model checking; Communicating state machines; Formal modeling and verification; GPGPU; Improve performance; Linear temporal logic; Victim caches; Cache memory
Incomplete tests for undetectable faults to improve test set quality,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062327577&doi=10.1145%2f3306493&partnerID=40&md5=d785a7dccd361d738225b4b92599600b,"The presence of undetectable faults in a set of target faults implies that tests, which may be important for detecting defects, are missing from the test set. This article suggests an approach for addressing missing tests that fits with the rationale for computing an n-detection test set. The artcile defines the concept of an incomplete test that is relevant when a target fault is undetectable. An incomplete test activates the fault but fails to detect it because of one or more assignments that are missing from the test. The procedure described in this article improves the quality of a test set by attempting to ensure that every undetectable fault has n incomplete tests with the smallest possible numbers of missing assignments, for a constant n ≥ 1. The incomplete tests are expected to contribute to the detection of detectable defects around the site of the undetectable fault. The computation of missing assignments for a test is performed in linear time by avoiding fault simulation and considering all the undetectable faults simultaneously. Experimental results demonstrate the extent to which a given test set can be improved without increasing the number of tests. © 2019 Association for Computing Machinery.",N-detection test set; Test generation; Test set quality; Undetectable faults,Defects; Fault detection; Detecting defects; Detection tests; Fault simulation; Linear time; Target faults; Test generations; Test sets; Undetectable faults; Testing
A novel rule mapping on TCAM for power efficient packet classification,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067178330&doi=10.1145%2f3328103&partnerID=40&md5=b83a495883074be1a3af3bc83e585adb,"Packet Classification is the enabling function performed in commodity switches for providing various services such as access control, intrusion detection, load balancing, and so on. Ternary Content Addressable Memories (TCAMs) are the de facto standard for performing packet classification at high speeds. However, TCAMs are highly costlier both in terms of cost and power consumption, forcing the switch vendors towards placing lots of effort for power management. Hence, power-efficient solutions for TCAM-based packet classification are highly relevant even today. In this article, we propose a novel rule placement algorithm based on the unique field values’ presence within the rule databases. We evaluate the total search that is needed to be inspected with respect to the traditional placement approach and the proposed placement approach based on the information content within the fields. Simulation results showed an average reduction of 30.55% in the search space by the proposed placement approach, thereby resulting in an average reduction of 18.85% per search energy over TCAM. With typical TCAM clock speeds ranging between 200–400MHz, this reduction in the per-search energy maps to a huge reduction in the total energy consumed by the TCAM-based network switches. The proposed solution is plug-and-play type requiring only minimal pre-processing within the Network Processing Unit (NPU) of the switches and edge routers. © 2019 Association for Computing Machinery.",Packet classification; Power efficient switches and routers; Rule database; Ternary content addressable memories,Access control; Associative storage; Classification (of information); Intrusion detection; Logic gates; Packet switching; Information contents; Network processing units; Packet classification; Placement algorithm; Power efficient; Power-efficient solutions; Rule database; Ternary content addressable memory; Ternary content adressable memory
Improving test and diagnosis efficiency through ensemble reduction and learning,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067199200&doi=10.1145%2f3328754&partnerID=40&md5=5a12540fbe8e5fc6e13820fd27c8d2ab,"Machine learning is a powerful lever for developing, improving, and optimizing test methodologies to cope with the demand from the advanced nodes. Ensemble methods are a particular learning paradigm that uses multiple models to boost performance. In this work, ensemble reduction and learning is explored for integrated circuit test and diagnosis. For testing, the proposed method is able to reduce the number of system-level tests without incurring substantial increase in defect escapes or yield losses. Significant cost from test execution and set-up preparation can thereby be saved. Experiments are performed on two designs of commercially fabricated chips, for an overall population of >264,000 chips. The results demonstrate that our method is able to reduce 29.27% and 21.74% of the number of tests for the two chips, respectively, at the cost of very low defect escapes. For failure diagnosis, the framework is able to predict an adequate amount of test data necessary for accurate failure diagnosis. Experiments performed on five standard benchmarks demonstrate that our method outperforms a state-of-the-art work in terms of data-volume reduction. The proposed ensemble-based methodology creates opportunities for improving test and diagnosis efficiency. © 2019 Association for Computing Machinery.",Diagnosis; Ensemble reduction; Integrated system test; Logic minimization; Machine learning; Test economics; Test-set reduction,Defects; Diagnosis; Failure analysis; Learning systems; Logic Synthesis; Machine learning; Ensemble methods; Fabricated chips; Failure Diagnosis; Integrated systems; Learning paradigms; Logic minimization; System-level test; Test sets; Efficiency
Share-N-learn: A framework for sharing activity recognition models in wearable systems with context-varying sensors,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065608675&doi=10.1145%2f3318044&partnerID=40&md5=08a5edbfd265276207830905d3735180,"Wearable sensors utilize machine learning algorithms to infer important events such as the behavioral routine and health status of their end users from time-series sensor data. A major obstacle in large-scale utilization of these systems is that the machine learning algorithms cannot be shared among users or reused in contexts different from the setting in which the training data are collected. As a result, the algorithms need to be retrained from scratch in new sensor contexts, such as when the on-body location of the wearable sensor changes or when the system is utilized by a new user. The retraining process places significant burden on end users and system designers to collect and label large amounts of training sensor data. In this article, we challenge the current algorithm training paradigm and introduce Share-n-Learn to automatically detect and learn physical sensor contexts from a repository of shared expert models without collecting any new labeled training data. Share-n-Learn enables system designers and end users to seamlessly share and reuse machine learning algorithms that are trained under different contexts and data collection settings. We develop algorithms to autonomously identify sensor contexts and propose a gating function to automatically activate the most accurate machine learning model among the set of shared expert models. We assess the performance of Share-n-Learn for activity recognition when a dynamic sensor constantly migrates from one body location to another. Our analysis based on real data collected with human subjects on three datasets demonstrates that Share-n-Learn achieves, on average, 68.4% accuracy in detecting physical activities with context-varying wearables. This accuracy performance is about 19% more than 'majority voting,' 10% more than the state-ofthe-art transfer learning, and only 8% less than the experimental upper bound. © 2019 Association for Computing Machinery.",Activity recognition; Machine learning; Transfer learning; Wearable sensors,Learning algorithms; Learning systems; Machine learning; Pattern recognition; Systems analysis; Activity recognition; Gating functions; Labeled training data; Machine learning models; Physical activity; Physical sensors; Scale utilization; Transfer learning; Wearable sensors
CAD-base: An attack vector into the electronics supply chain,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065613918&doi=10.1145%2f3315574&partnerID=40&md5=646dac2cff9c29caad0bb422f014b40f,"Fabless semiconductor companies design system-on-chips (SoC) by using third-party intellectual property (IP) cores and fabricate them in offshore, potentially untrustworthy foundries. Owing to the globally distributed electronics supply chain, security has emerged as a serious concern. In this article, we explore electronics computer-aided design (CAD) software as a threat vector that can be exploited to introduce vulnerabilities into the SoC. We show that all electronics CAD tools-high-level synthesis, logic synthesis, physical design, verification, test, and post-silicon validation-are potential threat vectors to different degrees. We have demonstrated CAD-based attacks on several benchmarks, including the commercial ARM Cortex M0 processor [1]. © 2019 Association for Computing Machinery.",Computer-aided design; Electronic design automation; Hardware security,Computer aided design; Computer aided logic design; Computer hardware; Electronic design automation; Hardware security; High level synthesis; Logic Synthesis; Offshore oil well production; Semiconductor device manufacture; Supply chains; System-on-chip; Attack vector; Design systems; Electronics supply chain; Fabless semiconductor companies; Physical design; Post-silicon validations; Potential threats; Third parties; Integrated circuit design
Remote attestation via self-measurement,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060136861&doi=10.1145%2f3279950&partnerID=40&md5=f48752a65b0cfea9afea0c0b242c6442,"Remote attestation (RA) is a popular means of detecting malware in embedded and IoT devices. RA is usually realized as an interactive protocol, whereby a trusted party (verifier) measures software integrity of a potentially compromised remote device (prover). Early work focused on purely software-based and fully hardware-based techniques, neither of which is ideal for low-end embedded devices. More recent results yielded hybrid (SW/HW) architectures with a minimal set of features to support efficient and secure RA on low-end devices. All prior techniques require on-demand operation, i.e., RA is performed in real time. We identify some drawbacks of this general approach in the context of unattended devices: First, it fails to detect mobile malware that enters and leaves prover between successive RA instances. Second, it requires prover to engage in a potentially expensive (in terms of time and energy) computation, which can be harmful for mission-critical or real-time devices. To address these drawbacks, we introduce the concept of self-measurement, whereby prover periodically and securely measures and records its own software state, based on a pre-established schedule. A (possibly untrusted) verifier occasionally collects and verifies these measurements. We present the design of a concrete technique, called Efficient Remote Attestation via Self-Measurement for Unattended Settings, (ERASMUS), justify its features and evaluate its performance. In the process, we also define a new metric, Quality of Attestation (QoA). We believe that ERASMUS is well suited for time-sensitive and/or safety-critical applications that are not served well by on-demand RA. Finally, we show that ERASMUS is a promising stepping stone toward handling attestation of multiple devices (i.e., a group or swarm) with high mobility. © 2018 Association for Computing Machinery.",Internet-of-things; Malware detection; Real-time systems; Remote attestation; Swarm attestation,Computer crime; Interactive computer systems; Internet of things; Malware; Safety engineering; Interactive protocols; Malware detection; Mission critical; On demand operation; Remote attestation; Safety critical applications; Software integrity; Swarm attestation; Real time systems
Knowledge- And simulation-based synthesis of area-efficient passive loop filter incremental zoom-ADC for built-in self-test applications,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060061381&doi=10.1145%2f3266227&partnerID=40&md5=738b19d8f81e68fbf4a74d76c5910a35,"We propose a fully differential, synthesizable zoom-ADC architecture with a passive loop filter for low-frequency Built-In Self-Test (BIST) applications, along with a synthesis tool that can target various design specifications. We present the detailed ADC architecture and a step-by-step process for designing the zoom-ADC. The design flow does not rely on the extensive knowledge of an experienced ADC designer. Two ADCs have been synthesized with different performance requirements in the 65nm CMOS process. The first ADC achieves a 90.4dB Signal-to-Noise Ratio (SNR) in 512μs measurement time and consumes 17μW power. The second design achieves a 78.2dB SNR in 31.25μs measurement time and consumes 63μW power. © 2018 Association for Computing Machinery.",Design automation; Incremental zooming ADC; Passive delta sigma modulators,Analog to digital conversion; Built-in self test; Computer aided design; Modulators; Signal to noise ratio; Delta sigma modulator; Design automations; Design specification; Fully differential; Incremental zooming ADC; Measurement time; Passive loop filters; Performance requirements; Passive filters
Performance-aware test scheduling for diagnosing coexistent channel faults in topology-agnostic networks-on-chip,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060028949&doi=10.1145%2f3291532&partnerID=40&md5=e3a11b5d44a71379cbfe72322b59881f,"High-performance multiprocessor SoCs used in practice require a complex network-on-chip (NoC) as communication architecture, and the channels therein often suffer from various manufacturing defects. Such physical defects cause a multitude of system-level failures and subsequent degradation of reliability, yield, and performance of the computing platform. Most of the existing test approaches consider mesh-based NoC channels only and do not perform well for other regular topologies such as octagons or spidergons, with regard to test time and overhead issues. This article proposes a topology-agnostic test mechanism that is capable of diagnosing on-line, coexistent channel-short, and stuck-at faults in these special NoCs as well as in traditional mesh architectures. We introduce a new test model called Damaru to decompose the network and present an efficient scheduling scheme to reduce test time without compromising resource utilization during testing. Additionally, the proposed scheduling scheme scales well with network size, channel width, and topological diversity. Simulation results show that the method achieves nearly 92% fault coverage and improves area overhead by almost 60% and test time by 98% compared to earlier approaches. As a sequel, packet latency and energy consumption are also improved by 67.05% and 54.69%, respectively, and they are further improved with increasing network size. © 2019 Association for Computing Machinery.",Channels in NoC; Short faults; Testing stuck-at,Complex networks; Defects; Energy utilization; Mesh generation; Network architecture; Scheduling; Topology; Communication architectures; Computing platform; Efficient scheduling; Manufacturing defects; Networks on chips; Resource utilizations; Scheduling schemes; Short fault; Network-on-chip
Optimization of fault-tolerant mixed-criticality multi-core systems with enhanced WCRT analysis,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060178473&doi=10.1145%2f3275154&partnerID=40&md5=48dad827d0abfaf9a6cb129a2f0cbb2e,"This article proposes a novel optimization technique of fault-tolerant mixed-criticality multi-core systems with worst-case response time (WCRT) guarantees. Typically, in fault-tolerant multi-core systems, tasks can be replicated or re-executed in order to enhance the reliability. In addition, based on the policy of mixed-criticality scheduling, low-criticality tasks can be dropped at runtime. Such uncertainties caused by hardening and mixed-criticality scheduling make WCRT analysis very difficult. We show that previous analysis techniques are pessimistic as they consider avoidably extreme cases that can be safely ignored within the given reliability constraint. We improve the analysis in order to tighten the pessimism of WCRT estimates by considering the maximum number of faults to be tolerated. Further, we improve the mixed-criticality scheduling by allowing partial dropping of low-criticality tasks. On top of those, we explore the design space of hardening, task-to-core mapping, and quality-of-service of the multi-core mixed-criticality systems. The effectiveness of the proposed technique is verified by extensive experiments with synthetic and real-life benchmarks. © 2018 Association for Computing Machinery.",Fault-tolerance; Mixed-criticality; Real-time systems; Scheduling; Worst-case analysis,Criticality (nuclear fission); Fault tolerance; Hardening; Interactive computer systems; Quality of service; Reliability analysis; Scheduling; Uncertainty analysis; Analysis techniques; Mixed criticalities; Mixed-criticality systems; Multi-core systems; Optimization techniques; Reliability constraints; Worst case response time; Worst-case analysis; Real time systems
Harvesting row-buffer hits via orchestrated last-level cache and DRAM scheduling for heterogeneous multicore systems,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060171676&doi=10.1145%2f3269982&partnerID=40&md5=d2f393ba661b644826c9b1cb2b09065c,"In heterogeneous multicore systems, the memory subsystem, including the last-level cache and DRAM, is widely shared among the CPU, the GPU, and the real-time cores. Due to their distinct memory traffic patterns, heterogeneous cores result in more frequent cache misses at the last-level cache. As cache misses travel through the memory subsystem, two schedulers are involved for the last-level cache and DRAM, respectively. Prior studies treated the scheduling of the last-level cache and DRAM as independent stages. However, with no orchestration and limited visibility of memory traffic, neither scheduling stage is able to ensure optimal scheduling decisions for memory efficiency. Unnecessary precharges and row activations happen in DRAM when the memory scheduler is ignorant of incoming cache misses, and DRAM row-buffer states are invisible to the last-level cache. In this article, we propose a unified memory controller for the the last-level cache and DRAM with orchestrated schedulers. The memory scheduler harvests row-buffer hit opportunities in cache request buffers during spare time without inducing significant implementation cost. We further introduce a dynamic orchestrated scheduling policy to improve memory efficiency while achieving target CPU IPC. Extensive evaluations show that the proposed controller improves the total memory bandwidth of DRAM by 16.8% on average and saves DRAM energy by up to 29.7% while achieving comparable CPU IPCs. With the dynamic scheduling policy, the unified controller achieves the same IPC as the conventional design and increases DRAM bandwidth by 9.2%. In addition, we explore the potential of the proposed memory controller to attain improvements on both memory bandwidth and CPU IPC. © 2018 Association for Computing Machinery.",DRAM efficiency; Memory scheduling; Row-buffer hit,Bandwidth; Controllers; Dynamic random access storage; Memory architecture; Real time systems; Scheduling; Conventional design; Heterogeneous cores; Heterogeneous multi-core systems; Implementation cost; Memory scheduling; Optimal scheduling; Row-buffer hit; Scheduling policies; Cache memory
A hardware-efficient block matching algorithm and its hardware design for variable block size motion estimation in ultra-high-definition video encoding,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060031491&doi=10.1145%2f3290408&partnerID=40&md5=58f1011d1076d7e2bfa5603948718ba4,"Variable block size motion estimation has contributed greatly to achieving an optimal interframe encoding, but involves high computational complexity and huge memory access, which is the most critical bottleneck in ultra-high-definition video encoding. This article presents a hardware-efficient block matching algorithm with an efficient hardware design that is able to reduce the computational complexity of motion estimation while providing a sustained and steady coding performance for high-quality video encoding. A three-level memory organization is proposed to reduce memory bandwidth requirement while supporting a predictive common search window. By applying multiple search strategies and early termination, the proposed design provides 1.8 to 3.7 times higher hardware efficiency than other works. Furthermore, on-chip memory has been reduced by 96.5% and off-chip bandwidth requirement has been reduced by 39.4% thanks to the proposed three-level memory organization. The corresponding power consumption is only 198mW at the highest working frequency of 500MHz. The proposed design is attractive for high-quality video encoding in real-time applications with low power consumption. © 2019 Association for Computing Machinery.",Hardware architecture; Hardware efficiency; Memory organization; Motion estimation; Variable block size; Video encoding,Bandwidth; Computational complexity; Computational efficiency; Efficiency; Electric power utilization; Encoding (symbols); Image coding; Image quality; Memory architecture; Motion compensation; Motion estimation; Signal encoding; Hardware architecture; Hardware efficiency; Memory organizations; Variable block size; Video encodings; Video signal processing
Writeback-aware LLC management for PCM-based main memory systems,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060016758&doi=10.1145%2f3292009&partnerID=40&md5=b429174b61459057e8ace23e4f501357,"With the increase in the number of data-intensive applications on today's workloads, DRAM-based main memories are struggling to satisfy the growing data demand capacity. Phase Change Memory (PCM) is a type of non-volatile memory technology that has been explored as a promising alternative for DRAM-based main memories due to its better scalability and lower leakage energy. Despite its many advantages, PCM also has shortcomings such as long write latency, high write energy consumption, and limited write endurance, which are all related to the write operations. In this article, we propose a novel writeback-aware Last Level Cache (LLC) management scheme named WALL to reduce the number of LLC writebacks and consequently improve performance, energy efficiency, and lifetime of a PCM-based main memory system. First, we investigate the writeback behavior of LLC sets and show that writebacks are not uniformly distributed among sets; some sets observe much higher writeback rates than others. We then propose a writeback-aware set-balancing mechanism, which employs the underutilized LLC sets with few writebacks as an auxiliary storage for the evicted dirty lines from sets with frequent writebacks. We also propose a simple and effective writeback-aware replacement policy to avoid the eviction of the dirty blocks that are highly reused after being evicted from the cache. Our experimental results show that WALL achieves an average of 30.9% reduction in the total number of LLC writebacks, compared to the baseline scheme, which uses the LRU replacement policy. As a result, WALL can reduce the memory energy consumption by 23.1% and enhance PCM lifetime by 1.29×, on average, on an 8-core system with a 4GB PCM main memory, running memory-intensive applications. © 2019 Association for Computing Machinery.",Energy consumption; Last level cache; Performance; Phase change memory; Write endurance,Dynamic random access storage; Energy efficiency; Energy utilization; Phase change memory; Balancing mechanisms; Improve performance; Last-level caches; Lastlevel caches (LLC); Non-volatile memory technology; Performance; Phase change memory (pcm); Write endurances; Cache memory
Reducing writebacks through in-cache displacement,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060024034&doi=10.1145%2f3289187&partnerID=40&md5=b31b2425c521513d3ff65b7e0f97a418,"Non-Volatile Memory (NVM) technology is a promising solution to fulfill the ever-growing need for higher capacity in the main memory of modern systems. Despite having many great features, however, NVM's poor write performance remains a severe obstacle, preventing it from being used as a DRAM alternative in the main memory. Most of the prior work targeted optimizing writes at the main memory side and neglected the decisive role of upper-level cache management policies on reducing the number of writes. In this article, we propose a novel cache management policy that attempts to maximize write-coalescing in the on-chip SRAM last-level cache (LLC) for the sake of reducing the number of costly writes to the off-chip NVM. We decouple a few physical ways of the LLC to have a dedicated and exclusive storage for the dirty blocks after being evicted from the cache and before being sent to the off-chip memory. By displacing dirty blocks in exclusive storage, they are kept in the cache based on their rewrite distance and are evicted when they are unlikely to be reused shortly. To maximize the effectiveness of exclusive storage, we manage it as a Cuckoo Cache to offer associativity based on the various applications' demands. Through detailed evaluations targeting various single- and multi-threaded applications, we show that our proposal reduces the number of writebacks by 21%, on average, over the state-of-the-art method and enhances both performance and energy efficiency. © 2019 Association for Computing Machinery.",Cache management; Non-volatile memory; Phase change memory; Read-write disparity; Writeback,Dynamic random access storage; Energy efficiency; Flocculation; Nonvolatile storage; Phase change memory; Static random access storage; Cache management; Cache management policies; Multi- threaded applications; Non-volatile memory; Non-volatile memory technology; Read-write disparity; State-of-the-art methods; Write-back; Cache memory
Instruction-level abstraction (ILA): A uniform specification for system-on-chip (SOC) verification,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060144673&doi=10.1145%2f3282444&partnerID=40&md5=f27fc860026d9c7db72280945b08bba9,"Modern Systems-on-Chip (SoC) designs are increasingly heterogeneous and contain specialized semi-programmable accelerators in addition to programmable processors. In contrast to the pre-accelerator era, when the ISA played an important role in verification by enabling a clean separation of concerns between software and hardware, verification of these “accelerator-rich” SoCs presents new challenges. From the perspective of hardware designers, there is a lack of a common framework for formal functional specification of accelerator behavior. From the perspective of software developers, there exists no unified framework for reasoning about software/hardware interactions of programs that interact with accelerators. This article addresses these challenges by providing a formal specification and high-level abstraction for accelerator functional behavior. It formalizes the concept of an Instruction Level Abstraction (ILA), developed informally in our previous work, and shows its application in modeling and verification of accelerators. This formal ILA extends the familiar notion of instructions to accelerators and provides a uniform, modular, and hierarchical abstraction for modeling software-visible behavior of both accelerators and programmable processors. We demonstrate the applicability of the ILA through several case studies of accelerators (for image processing, machine learning, and cryptography), and a general-purpose processor (RISC-V). We show how the ILA model facilitates equivalence checking between two ILAs, and between an ILA and its hardware finite-state machine (FSM) implementation. Further, this equivalence checking supports accelerator upgrades using the notion of ILA compatibility, similar to processor upgrades using ISA compatibility. © 2018 Association for Computing Machinery.",Application-specific accelerator; Architecture; Equivalence checking; Formal verification; Hardware specification; Instruction-level abstraction; System on chip,Abstracting; Acceleration; Application specific integrated circuits; Architecture; Formal specification; Formal verification; General purpose computers; Image processing; Learning systems; Programmable logic controllers; Application specific; Equivalence checking; Functional specification; General purpose processors; Hardware specifications; Instruction-level; Modeling and verifications; Programmable processors; System-on-chip
Boundary-functional broadside and skewed-load tests,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060182014&doi=10.1145%2f3276976&partnerID=40&md5=c937b2c579291620501bb6eaf941573a,"Close-to-functional broadside tests are used for avoiding overtesting of delay faults that can result from nonfunctional operation conditions, while avoiding test escapes because of faults that cannot be detected under functional operation conditions. When a close-to-functional broadside test deviates from functional operation conditions, the deviation can affect the entire circuit. This article defines the concept of a boundary-functional broadside test where non-functional operation conditions are prevented from crossing a preselected boundary. Using the procedure described in this article, the boundary maintains the same values under a boundary-functional broadside test as under a functional broadside test from which it is derived. Indirectly, this ensures that the deviations from functional operation conditions throughout the entire circuit are limited. The concept of a boundary-functional broadside test is extended to skewed-load tests, and to partial-boundary-functional tests. Experimental results are presented for benchmark circuits to demonstrate the fault coverage improvements that can be achieved using boundary-functional broadside and skewed-load tests as well as partial-boundary-functional tests of both types. © 2018 Association for Computing Machinery.",Functional broadside tests; Functional test sequences; Skewed-load tests; Switching activity; Test generation; Transition faults,Automation; Computer applications; Functional broadside tests; Functional test sequences; Skewed-load tests; Switching activities; Test generations; Transition faults; Load testing
SynergyFlow: An elastic accelerator architecture supporting batch processing of large-scale deep neural networks,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060139038&doi=10.1145%2f3275243&partnerID=40&md5=3bd8e177f108f426a2243b481e1256a3,"Neural networks (NNs) have achieved great success in a broad range of applications. As NN-based methods are often both computation and memory intensive, accelerator solutions have been proved to be highly promising in terms of both performance and energy efficiency. Although prior solutions can deliver high computational throughput for convolutional layers, they could incur severe performance degradation when accommodating the entire network model, because there exist very diverse computing and memory bandwidth requirements between convolutional layers and fully connected layers and, furthermore, among different NN models. To overcome this problem, we proposed an elastic accelerator architecture, called SynergyFlow, which intrinsically supports layer-level and model-level parallelism for large-scale deep neural networks. SynergyFlow boosts the resource utilization by exploiting the complementary effect of resource demanding in different layers and different NN models. SynergyFlow can dynamically reconfigure itself according to the workload characteristics, maintaining a high performance and high resource utilization among various models. As a case study, we implement SynergyFlow on a P395-AB FPGA board. Under 100MHz working frequency, our implementation improves the performance by 33.8% on average (up to 67.2% on AlexNet) compared to comparable provisioned previous architectures. © 2018 Association for Computing Machinery.",Accelerator; Architecture; Complementary effect; Convolutional neural networks; Deep neural networks; Resource utilization,Architecture; Batch data processing; Convolution; Energy efficiency; Memory architecture; Network architecture; Neural networks; Particle accelerators; Accelerator architectures; Complementary effect; Convolutional neural network; Fully-connected layers; Neural networks (NNS); Performance degradation; Resource utilizations; Workload characteristics; Deep neural networks
SystemC-AMS thermal modeling for the co-simulation of functional and extra-functional properties,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060126404&doi=10.1145%2f3267125&partnerID=40&md5=348835b99b92f0941bdaf45cec90c1a8,"Temperature is a critical property of smart systems, due to its impact on reliability and to its inter-dependence with power consumption. Unfortunately, the current design flows evaluate thermal evolution ex-post on offline power traces. This does not allow to consider temperature as a dimension in the design loop, and it misses all the complex inter-dependencies with design choices and power evolution. In this article, by adopting the functional language SystemC-AMS (Analog Mixed Signal), we propose a method to enable thermal/power/functional co-simulation. The system thermal model is built by using state-of-the-art circuit equivalent models, by exploiting the support for electrical linear networks intrinsic of SystemC-AMS. The experimental results will show that the choice of SystemC-AMS is a winning strategy for building a simultaneous simulation of multiple functional and extra-functional properties of a system. The generated code exposes an accuracy comparable to that of the reference thermal simulator HotSpot. Additionally, the initial overhead due to the general purpose nature of SystemC-AMS is compensated by the surprisingly high performance of transient simulation, with speedups as high as two orders of magnitude. © 2018 Association for Computing Machinery.",Simulation; SystemC-AMS; Thermal analysis; Thermal estimation,Thermoanalysis; Critical properties; Extra-functional properties; Functional languages; Orders of magnitude; Simulation; SystemC-AMS; Thermal simulators; Transient simulation; Linear networks
Three-dimensional floorplan representations by using corner links and partial order,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060181839&doi=10.1145%2f3289179&partnerID=40&md5=fabfd3ec8e6cade1fcaa6715064e171c,"Three-dimensional integrated circuit (3D IC) technology offers a potential breakthrough to enable a paradigm-shift strategy, called “more than Moore,” with novel features and advantages over the conventional 2D process technology. By having three-dimensional interconnections, 3D IC provides substantial wirelength reduction and a massive amount of bandwidth, which gives significant performance improvement to overcome many of the nontrivial challenges in semiconductor industry. Moreover, 3D integration technology enables to stack disparate technologies with various functionalities into a single system-in-package (SiP), introducing “true 3D IC” design. As the first physical design (PD) step, IC floorplanning takes a crucial role to determine IC's overall design qualities such as footprint area, timing closure, power distribution, thermal management, and so on. However, lack of efficient 3D floorplanning algorithms that practically implement advantages of 3D integration technology is a critical bottleneck for PD automation of 3D IC design and implementation. 3D floorplanning (or packing, block partitioning) is a well-known NP-hard problem, and most of 3D floorplanning algorithms rely on heuristics and iterative improvements. Thus, developing complete and efficient 3D floorplan representations is important, since floorplan representation provides the foundation of data structure to search the solution space for 3D IC floorplanning. A well-defined floorplan representation provides a well-organized and cost-effective methodology to design high-performance 3D IC. We propose a new 3D IC floorplan representation methodology using corner links and partial order. Given a fixed number of cuboidal blocks and their volume, algorithmic 3D floorplan representations describe topological structure and physical positions/orientations of each block relative to the origin in the 3D floorplan space. In this article, (1) we introduce our novel 3D floorplan representation, called corner links representation, (2) we analyze the equivalence relation between the corner links representation and its corresponding partial order representation, and (3) we discuss several key properties of the corner links representation and partial order representation. The corner links representation provides a complete and efficient structure to assemble the original 3D mosaic floorplan. Also, the corner links representation for the non-degenerate 3D mosaic floorplan can be equivalently expressed by the four trees representation. The partial order representation defines the topological structure of the 3D floorplan with three transitive closure graphs (TCG) for each direction and captures all stitching planes in the 3D floorplan in the order of their respective directions. We demonstrate that the corner links representation can be reduced to its corresponding partial order representation, indicating that the corner links representation shares well-defined and -studied features/properties of 3D TCG-based floorplan representation. If the partial order representation describes relations between any pairs of blocks in the 3D floorplan, then the floorplan is a valid floorplan. We show that the partial order representation can restore the absolute coordinates of all blocks in the 3D mosaic floorplan by using the given physical dimensions of blocks. © 2018 Association for Computing Machinery.",3D floorplan; 3D IC design; 3D integrated circuit; Floorplan representation,Computational complexity; Cost effectiveness; Iterative methods; Semiconductor device manufacture; System-in-package; Three dimensional integrated circuits; Topology; 3D IC design; Equivalence relations; Floorplans; Iterative improvements; Partial order representation; Performance improvements; Semiconductor industry; Three dimensional integrated circuits (3-D IC); Integrated circuit design
Efficiently managing the impact of hardware variability on GPUs' streaming processors,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060170319&doi=10.1145%2f3287308&partnerID=40&md5=c1c1a9cc3661ec1c44b5feaf6eb0900d,"Graphics Processing Units (GPUs) are widely used in general-purpose high-performance computing fields due to their highly parallel architecture. In recent years, a new era with the nanometer scale integrated circuit manufacture process has come. As a consequence, GPUs' computation capability gets even stronger. However, as process technology scales down, hardware variability, e.g., process variations (PVs) and negative bias temperature instability (NBTI), has a higher impact on the chip quality. The parallelism of GPU desires high consistency of hardware units on chip; otherwise, the worst unit will inevitably become the bottleneck. So the hardware variability becomes a pressing concern to further improve GPUs' performance and lifetime, not only in integrated circuit fabrication, but more in GPU architecture design. Streaming Processors (SPs) are the key units in GPUs, which perform most of parallel computing operations. Therefore, in this work, we focus on mitigating the impact of hardware variability in GPU SPs. We first model and analyze SPs' performance variations under hardware variability. Then, we observe that both PV and NBTI have a large impact on SPs' performance. We further observe unbalanced SP utilization, e.g., some SPs are idle when others are active, during program execution. Leveraging this observation, we propose a Hardware Variability-aware SPs' Management policy (HVSM), which dynamically dispatches computation in appropriate SPs to balance the utilizations. In addition, we find that a large portion of compute operations are duplicate. We also propose an Operation Compression (OC) technique to minimize the unnecessary computations to further mitigate the hardware variability effects. Our experimental results show the combined HVSM and OC technique effectively reduces the impact of hardware variability, which can translate to 37% performance improvement or 18.3% lifetime extension for a GPU chip. © 2018 Association for Computing Machinery.",GPU; Hardware variability; NBTI; Process variation; Streaming processor,Computer graphics; Graphics processing unit; Integrated circuit manufacture; Integrated circuits; Negative bias temperature instability; Parallel architectures; Program processors; Architecture designs; High performance computing; Integrated circuit fabrication; Negative bias temperature instability (NBTI); Performance improvements; Performance variations; Process Technologies; Process Variation; Computer hardware
Automatic optimization of the VLAN partitioning in automotive communication networks,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060132635&doi=10.1145%2f3278120&partnerID=40&md5=a70ea920cd7cf0280274e7712828b4e3,"Dividing the communication network into so-called Virtual Local Area Networks (VLANs), i.e., subnetworks that are isolated at the data link layer (OSI layer 2), is a promising approach to address the increasing security challenges in automotive networks. The automation of the VLAN partitioning is a well-researched problem in the domain of local or metropolitan area networks. However, the approaches used there are hardly applicable for the design of automotive networks as they mainly focus on reducing the amount of broadcast traffic and cannot capture the many design objectives of automotive networks like the message timing or the link load, which are affected by the VLAN partitioning. As a remedy, this article proposes an approach based on a set of Pseudo-Boolean constraints to generate a message routing which is feasible with respect to the VLAN-related routing restrictions in automotive networks. This approach can be used for a design space exploration to optimize not only the VLAN partitioning but also other routing-related objectives. We demonstrate both the efficiency of our message routing approach and the now accessible optimization potential for the complete Electric/Electronic architecture with a mixed-criticality system from the automotive domain. There we thoroughly investigate the impact of the VLAN partitioning on the message timing and the link loads by optimizing these design objectives concurrently. During the exploration of the huge design space, where each resource can be assigned to one of four VLANs, our approach requires less than 40ms for the creation of a valid solution and ensures that all messages satisfy their deadlines and link load bounds. © 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Automotive ethernet; Design automation; Packet-switching networks; Virtual local area networks,Computer aided design; Metropolitan area networks; Network layers; Packet switching; Virtual addresses; Automatic optimization; Automotive networks; Design automations; Design space exploration; Mixed-criticality systems; Optimization potential; Pseudo-boolean constraints; Virtual local area networks; Local area networks
Optimal allocation of computation and communication in an IoT network,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061244612&doi=10.1145%2f3236623&partnerID=40&md5=6e09a2229f967b30e4df68e667da71d6,"Internet of things (IoT) is being developed for a wide range of applications from home automation and personal fitness to smart cities. With the extensive growth in adaptation of IoT devices comes the uncoordinated and substandard designs aimed at promptly making products available to the end consumer. This substandard approach restricts the growth of IoT in the near future and necessitates that studies understand requirements for an efficient design. A particular area where IoT applications have grown significantly is surveillance and monitoring. Applications of IoT in this domain are relying on distributed sensors, each equipped with a battery, capable of collecting images, processing images, and communicating the raw or processed data to the nearest node until it reaches the base station for decision making. In such an IoT network where processing can be distributed over the network, the important research question is how much of data each node should process and how much it should communicate for a given objective. This work answers this question and provides a deeper understanding of energy and delay tradeoffs in an IoT network with three different target metrics. © 2018 Association for Computing Machinery.",Media access control; Multi-channel; Radio interference; Time synchronization; Wireless sensor networks,Decision making; Medium access control; Network security; Radio access networks; Radio interference; Sensor nodes; Wireless sensor networks; Distributed sensor; Efficient designs; Internet of Things (IOT); Media access control; Multi channel; Optimal allocation; Research questions; Time synchronization; Internet of things
Instinctive Assistive Indoor Navigation using Distributed Intelligence,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061241900&doi=10.1145%2f3212720&partnerID=40&md5=ba6a609f65a9a34d427eec3851d2d963,"Cyber-physical systems (CPS) and the Internet of Things (IoT) offer a significant potential to improve the effectiveness of assistive technologies for those with physical disabilities. Practical assistive technologies should minimize the number of inputs from users to reduce their cognitive and physical effort. This article presents an energy-efficient framework and algorithm for assistive indoor navigation with multi-modal user input. The goal of the proposed framework is to simplify the navigation tasks and make them more instinctive for the user. Our framework automates indoor navigation using only a few user commands captured through a wearable device. The proposed methodology is evaluated using both a virtual smart building and a prototype. The evaluations for three different floorplans show one order of magnitude reduction in user effort and communication energy required for navigation, when compared to conventional navigation methodologies that require continuous user inputs. © 2018 Association for Computing Machinery.",Assistive technologies; Human-machine interface; IoT devices; Wearable computers,Embedded systems; Energy efficiency; Internet of things; Navigation; Wearable computers; Wearable technology; Assistive technology; Communication energy; Cyber-physical systems (CPS); Distributed intelligence; Framework and algorithms; Human Machine Interface; Internet of thing (IOT); IoT devices; Indoor positioning systems
Remote detection of unauthorized activity via spectral analysis,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061256972&doi=10.1145%2f3276770&partnerID=40&md5=a95877a98c9794662dd2c9de18e24d03,"Unauthorized hardware or firmware modifications, known as trojans, can steal information, drain the battery, or damage IoT devices. Since trojans may be triggered in the field at an unknown instance, it is important to detect their presence at runtime. However, it is difficult to run sophisticated detection algorithms on these devices due to limited computational power and energy and, in some cases, lack of accessibility. This article presents a stand-off self-referencing technique for detecting unauthorized activity. The proposed technique processes involuntary electromagnetic emissions on a separate hardware, which is physically decoupled from the device under test. When the device enters the test mode, a predefined test application is run on the device repetitively for a known period. The periodicity ensures that the spectral electromagnetic power of the test application concentrates at known frequencies, leaving the remaining frequencies within the operating bandwidth at the noise level. Any deviations from the noise level for these unoccupied frequency locations indicate the presence of unknown (unauthorized) activity. Hence, we are able to differentiate trojan activity without using a golden reference, or any knowledge of the attributes of the trojan activity. Experiments based on hardware measurements show that the proposed technique achieves close to 100% detection accuracy at up to 120cm distance. © 2018 Association for Computing Machinery.",EM emission; Hardware/firmware trojan detection; IoT security,Firmware; Internet of things; Spectrum analysis; Computational power; Detection algorithm; Electromagnetic emissions; Electromagnetic power; IoT security; Operating bandwidth; Self-referencing technique; Trojan detections; Malware
Probabilistic evaluation of hardware security vulnerabilities,2019,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060004112&doi=10.1145%2f3290405&partnerID=40&md5=3dd5e8589977fb4b1e3dfb3d851b9d1c,"Various design techniques can be applied to implement the finite state machine (FSM) functions in order to optimize timing, performance, power, and to reduce overhead. Recently, malicious attacks to hardware systems have emerged as a critical problem. Fault injection attacks, in particular, alter the function or reveal the critical information of a hardware system through precisely controlled fault injection processes. Attackers can utilize the loopholes and vulnerabilities of FSM functions to access the states that are under protection. A probabilistic model is developed in this article to evaluate the potential vulnerabilities of FSM circuits at the design stage. Analysis based on the statistical behaviors of FSM also shows that the induced circuit errors can be exploited to access the protected states. An effective solution based on state re-encoding is proposed to minimize the risk of unauthorized transitions. Simulation results demonstrate that vulnerable transition paths can be protected with small hardware overheads. © 2019 Association for Computing Machinery.",Fault injection; Fault tolerant system; Re-encoding; Transition probability,Computer aided software engineering; Encoding (symbols); Function evaluation; Network security; Signal encoding; Software testing; Fault injection; Fault injection attacks; Fault tolerant systems; Probabilistic evaluation; Probabilistic modeling; Re-encoding; Security vulnerabilities; Transition probabilities; Hardware security
Programmable gates using hybrid CMOS-STT design to prevent IC reverse engineering,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061291238&doi=10.1145%2f3236622&partnerID=40&md5=b5a76874c97991f173597f4b5ec75942,"This article presents a rigorous step towards design-for-assurance by introducing a new class of logically reconfigurable design resilient to design reverse engineering. Based on the non-volatile spin transfer torque (STT) magnetic technology, we introduce a basic set of non-volatile reconfigurable Look-Up-Table (LUT) logic components (NV-STT-based LUTs). An STT-based LUT with a significantly different set of characteristics compared to CMOS provides new opportunities to enhance design security yet makes it challenging to remain highly competitive with custom CMOS or even SRAM-based LUT in terms of power, performance, and area. To address these challenges, we propose several algorithms to select and replace custom CMOS gates with reconfigurable STT-based LUTs during design implementation such that the functionality of STT-based components and therefore the entire design cannot be determined in any manageable time, rendering any design reverse engineering attack ineffective. Our study, conducted on a large number of standard circuit benchmarks, concludes significant resiliency of hybrid STT-CMOS circuits against various types of attacks. Furthermore, the selection algorithms on average have a small impact on the performance of the circuit. We also tested these techniques against satisfiability attacks developed recently and show that these techniques also render more advanced reverse-engineering techniques computationally infeasible. © 2018 Association for Computing Machinery.",Hardware Security; Hybrid CMOS-STT; Reconfigurable Gates; Reverse Engineering; Standard IC Design Flow,CMOS integrated circuits; Electric network analysis; Hardware security; Reconfigurable hardware; Reverse engineering; Table lookup; Timing circuits; Design flows; Design implementation; Hybrid CMOS; Magnetic technologies; Reconfigurable; Reconfigurable designs; Reverse engineering techniques; Spin transfer torque; Integrated circuit design
UCR: An unclonable environmentally sensitive chipless RFID tag for protecting supply chain,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061229630&doi=10.1145%2f3264658&partnerID=40&md5=4eea9ee2a34017dce21718f3744b3fc8,"Chipless Radio Frequency Identification (RFID) tags that do not include an integrated circuit (IC) in the transponder are more appropriate for supply-chain management of low-cost commodities and have been gaining extensive attention due to their relatively lower price. However, existing chipless RFID tags consume considerable tag area and manufacturing time/cost because of complex fabrication process (e.g., requiring removing or shorting some resonators on the tag substrate to encode data). Worse still, their identifiers (IDs) are deterministic, clonable, and small in terms of bitwidth. To address these shortcomings and help preserve the cold chain for commodities (e.g., vaccines, pharmaceuticals, etc.) sensitive to temperature, we develop a novel unclonable environmentally sensitive chipless RFID (UCR) tag that intrinsically generates a unique ID from both manufacturing variations and ambient temperature variation. A UCR tag consists of two parts: (i) a certain number of concentric ring slot resonators integrated on a certain laminate (e.g., TACONIC TLX-0), whose resonance frequencies rely on geometric parameters of slot resonators and dielectric constant of substrate material that are sensitive to manufacturing variations, and (ii) a stand-alone circular ring slot resonator integrated on a particular substrate (e.g., grease) that will be melted at a high temperature, whose resonance frequency relies on geometric parameters of slot resonator, dielectric constant of substrate material, and ambient temperature. UCR tags have the capability to track commodities and their temperatures in the supply chain. The area of UCR tag is comparable to regular quick response (QR) code. Experimental results based on UCR tag prototypes have verified their uniqueness and reliability. © 2018 Association for Computing Machinery.",Chipless RFID tag; Cold chain; Irreversible temperature tracking; Unclonability; Uniqueness,Costs; Integrated circuits; Natural frequencies; Resonators; Substrates; Supply chain management; Temperature; Chipless RFID; Cold chain; Temperature tracking; Unclonability; Uniqueness; Radio frequency identification (RFID)
ShaiP: Secure hamming distance for authentication of intrinsic PUFs,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061273813&doi=10.1145%2f3274669&partnerID=40&md5=ce1bdb1acedd29255ef45276a9ffbafa,"In this article, we present SHAIP, a secure Hamming distance-based mutual authentication protocol. It allows an unlimited number of authentications by employing an intrinsic Physical Unclonable Function (PUF). PUFs are being increasingly employed for remote authentication of devices. Most of these devices have limited resources. Therefore, the intrinsic PUFs are most suitable for this task as they can be built with little or no modification to the underlying hardware platform. One major drawback of the current authentication schemes is that they expose the PUF response. This makes the intrinsic PUFs, which have a limited number of challenge-response pairs, unusable after a certain number of authentication sessions. Moreover, these schemes are one way in the sense that they only allow one party, the prover, to authenticate herself to the verifier. We propose a symmetric mutual authentication scheme based on secure (privacy-preserving) computation of the Hamming distance between the PUF response from the remote device and reference response stored at the verifier end. This allows both parties to authenticate each other without revealing their respective sets of inputs. We show that our scheme is effective with all state-of-the-art intrinsic PUFs. The proposed scheme is lightweight and does not require any modification to the underlying hardware. © 2018 Association for Computing Machinery.",Biometric authentication; Intrinsic PUF; Physical unclonable function; Remote authentication; Secure hamming distance,Authentication; Data privacy; Hamming distance; Hardware security; Authentication scheme; Biometric authentication; Challenge-response pair; Intrinsic PUF; Mutual authentication; Mutual authentication protocols; Physical unclonable functions (PUF); Remote authentication; Cryptography
"Editorial for Todaes special issue on internet of things system performance, reliability, and security",2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061243214&doi=10.1145%2f3276908&partnerID=40&md5=162ff8485207205b6f24d9fb12a32b9f,[No abstract available],,
Fault-tolerant unicast-based multicast for reliable network-on-chip testing,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061273493&doi=10.1145%2f3243214&partnerID=40&md5=c0bb99f236af227ed90e1d50215fc365,"We present a unified test technique that targets faults in links, routers, and cores of a network-on-chip design based on test sessions. We call an entire procedure, that delivers test packets to the subset of routers/cores, a test session. Test delivery for router/core testing is formulated as two fault-tolerant multicast algorithms. Test packet delivery for routers is implemented as a fault-tolerant unicast-based multicast scheme via the fault-free links and routers that were identified in the previous test sessions to avoid packet corruption. A new fault-tolerant routing algorithm is also proposed for the unicast-based multicast core test delivery in the whole network. Identical cores share the same test set, and they are tested within the same test session. Simulation results highlight the effectiveness of the proposed method in reducing test time. © 2018 Association for Computing Machinery.",Core testing; Fault-tolerant routing; On-chip networks; Router testing; Unicast-based multicast,Fault tolerance; Multicasting; Network architecture; Network routing; Network-on-chip; Servers; Testing; Core testing; Fault tolerant routing; Fault-tolerant routing algorithm; Multicast algorithms; Network-on-chip design; On-chip networks; Reliable Networks; Unicast; Routers
P3: Privacy preserving positioning for smart automotive systems,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061225451&doi=10.1145%2f3236625&partnerID=40&md5=90c94acb5bc7a1a25a23981feb140613,"This article presents the first privacy-preserving localization method based on provably secure primitives for smart automotive systems. Using this method, a car that is lost due to unavailability of GPS can compute its location with assistance from three nearby cars, while the locations of all the participating cars including the lost car remain private. Technological enhancement of modern vehicles, especially in navigation and communication, necessitates parallel enhancement in security and privacy. Previous approaches to maintaining user location privacy suffered from one or more of the following drawbacks: trade-off between accuracy and privacy, one-sided privacy, and the need of a trusted third party that presents a single point to attack. The localization method presented here is one of the very first location-based services that eliminates all these drawbacks. Two protocols for computing the location is presented here based on two Secure Function Evaluation (SFE) techniques that allow multiple parties to jointly evaluate a function on inputs that are encrypted to maintain privacy. The first one is based on the two-party protocol named Yao's Garbled Circuit (GC). The second one is based on the Beaver-Micali-Rogaway (BMR) protocol that allows inputs from more than two parties. The two secure localization protocols exhibit trade-offs between performance and resilience against collusion. Along with devising the protocols, we design and optimize netlists for the functions required for location computation by leveraging conventional logic synthesis tools with custom libraries optimized for SFE. Proof-of-concept implementation of the protocol shows that the complete operation can be performed within only 355ms. The fast computing time enables localization of even moving cars. © 2018 Association for Computing Machinery.",Connected cars; Garbled circuit; Location privacy; Location-based services; Secure automotive system; Secure function evaluation,Computation theory; Data privacy; Economic and social effects; Function evaluation; Location; Logic design; Logic Synthesis; Telecommunication services; Automotive Systems; Garbled circuits; Location privacy; Logic synthesis tools; Secure function evaluation; Security and privacy; Technological enhancement; Trusted third parties; Location based services
Rapid triggering capability using an adaptive overlay during FPGA debug,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061262195&doi=10.1145%2f3241045&partnerID=40&md5=ed209452fc81959569fa6990864e3ba6,"Field Programmable Gate Array (FPGA) technology is rapidly gaining traction in a wide range of applications. Nonetheless, FPGAs still require long design and debug cycles. To debug hardware circuits, trace-based instrumentation is inserted into the design that enables capturing data during the circuit execution into on-chip memories for later offline analysis. Since on-chip memories are limited, a trigger circuitry is used to only record data related to specific events during the execution. However, during debugging, a circuit recompilation is required on modifying these instruments. This can be very slow, reducing debug productivity. In this article, we propose a non-intrusive and rapid triggering solution with a tailored overlay fabric and mapping algorithm that seeks to enable fast debug iterations without performing a recompilation. This overlay is specialized for small combinational and sequential circuits with a single output; such circuits are typical of common trigger functions. We present an adaptive strategy to construct the overlay fabric using spare FPGA resources at compile time. At debug time, our proposed trigger mapping algorithms adapt to this specialized overlay to rapidly implement combinational and sequential trigger circuits. Our results show that the overlay fabric can be reconfigured to map different triggering scenarios in less than 40s instead of recompiling the circuit during debug iterations, increasing debug productivity. © 2018 Association for Computing Machinery.",Debug instruments; FPGA debug; Overlay fabric; Trace buffers; Trigger,Conformal mapping; Electric network analysis; Field programmable gate arrays (FPGA); Integrated circuit design; Productivity; Trigger circuits; Adaptive strategy; Capturing data; Hardware circuits; Mapping algorithms; Off-line analysis; On chip memory; Trigger; Trigger functions; Program debugging
PV-aware analog sizing for robust analog layout retargeting with optical proximity correction,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056399467&doi=10.1145%2f3236624&partnerID=40&md5=69856c2ee34f86b5f7eb96219ea39e5a,"For analog integrated circuits (ICs) in nanometer technology nodes, process variation (PV) induced by lithography may not only cause serious wafer pattern distortion, but also result in device mismatch, which can readily ruin circuit performance. Although the conventional optical proximity correction (OPC) operations can effectively improve the wafer image fidelity, an analog circuit without robust device sizes is still highly vulnerable to such a mismatch effect. In this article, a PV-aware sizing-inclusive analog layout retargeting framework, which encloses an efficient hybrid OPC scheme for yield enhancement, is proposed. The device sizes are tuned during the layout retargeting process by using a deterministic circuit-sizing algorithm considering PV conditions. Our hybrid OPC method combines global rule-based OPC with local model-based OPC functions to boost the wafer image quality improvement but without degrading the computational efficiency. The experimental results show that our proposed framework can achieve the best wafer image quality and circuit performance preservation compared to any other alternative approaches. © 2018 Association for Computing Machinery.",Analog layout retargeting; Deterministic circuit sizing; Hybrid OPC; Optical proximity correction; Process variation,Computational efficiency; Electric network analysis; Image enhancement; Image quality; Integrated circuit layout; Photolithography; Process design; Analog layout; Circuit Sizing; Hybrid OPC; Optical proximity corrections; Process Variation; Analog integrated circuits
Detection mechanisms for unauthorized wireless transmissions,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056423661&doi=10.1145%2f3241046&partnerID=40&md5=d2e1daea291fcb7ae805bcc40a8b4099,"With increasing diversity of supply chains from design to delivery, there is an increasing risk that unauthorized changes can be made within an IC. One of the motivations for this type of change is to learn important information (such as encryption keys, spreading codes) from the hardware, and transmit this information to a malicious party. To evade detection, such unauthorized communication can be hidden within legitimate bursts of transmit signal. In this article, we present several signal processing techniques to detect unauthorized transmissions which can be hidden within the legitimate signal. We employ a scheme where the legitimate transmission is configured to emit a single sinusoidal waveform. We use time and spectral domain analysis techniques to explore the transmit spectrum. Since every transmission, no matter how low the signal power is, must have a spectral signature, we identify unauthorized transmission by eliminating the desired signal from the spectrum after capture. Experiment results show that when spread spectrum techniques are used, the presence of an unauthorized signal can be determined without the need for decoding the malicious signal. The proposed detection techniques need to be used as enhancements to the regular testing and verification procedures if hardware security is a concern. © 2018 Association for Computing Machinery.",Detection mechanism; Hardware security; Spectral analysis; Spread spectrum,Cryptography; Hardware; Hardware security; Spectroscopy; Spectrum analysis; Supply chains; Detection mechanism; Proposed detection techniques; Signal processing technique; Sinusoidal waveforms; Spectral domain analysis; Spread spectra; Spread spectrum techniques; Wireless transmissions; Signal detection
An approximation algorithm for threshold voltage optimization,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056382617&doi=10.1145%2f3232538&partnerID=40&md5=33352539960100a13f0090622bfe415a,"We present a primal-dual approximation algorithm for minimizing the leakage power of an integrated circuit by assigning gate threshold voltages. While most existing techniques do not provide a performance guarantee, we prove an upper bound on the power consumption. The algorithm is practical and works with an industrial sign-off timer. It can be used for post-routing power reduction or for optimizing leakage power throughout the design flow. We demonstrate the practical performance on recent microprocessor units. Our implementation obtains significant leakage power reductions of up to 8% on top of one of the most successful algorithms for gate sizing and threshold voltage optimization. After timing-aware global routing, we achieve leakage power reductions of up to 34%. © 2018 Association for Computing Machinery.",Leakage power; Set cover; Time-cost tradeoff; V<sub>t</sub> optimization,Computer circuits; Gates (transistor); Threshold voltage; Leakage power; Leakage power reduction; Microprocessor units; Performance guarantees; Primal-dual approximation algorithm; Set cover; Time-cost tradeoff; Voltage optimization; Approximation algorithms
Quality-enhanced OLED power savings on mobile devices,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057162105&doi=10.1145%2f3243215&partnerID=40&md5=9c6ae4cd3bbc0052b2988a3e9a7ce6c1,"In the future, mobile systems will increasingly feature more advanced organic light-emitting diode (OLED) displays. The power consumption of these displays is highly dependent on the image content. However, existing OLED power-saving techniques either change the visual experience of users or degrade the visual quality of images in exchange for a reduction in the power consumption. Some techniques attempt to enhance the image quality by employing a compound objective function. In this article, we present a win-win scheme that always enhances the image quality while simultaneously reducing the power consumption. We define metrics to assess the benefits and cost for potential image enhancement and power reduction. We then introduce algorithms that ensure the transformation of images into their quality-enhanced power-saving versions. Next, the win-win scheme is extended to process videos at a justifiable computational cost. All the proposed algorithms are shown to possess the win-win property without assuming accurate OLED power models. Finally, the proposed scheme is realized through a practical camera application and a video camcorder on mobile devices. The results of experiments conducted on a commercial tablet with a popular image database and on a smartphone with real-world videos are very encouraging and provide valuable insights for future research and practices. © 2018 Association for Computing Machinery.",Image enhancement; Mobile devices; OLED displays; Power savings,Display devices; Electric power utilization; Image quality; Mobile devices; Organic light emitting diodes (OLED); Benefits and costs; Computational costs; Objective functions; OLED displays; Organic light emitting diode display; Power savings; Real world videos; Visual experiences; Image enhancement
Switching predictive control using reconfigurable state-based model,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057171755&doi=10.1145%2f3267126&partnerID=40&md5=5baf9fa94dc78307dfb86ac85b903340,"Advanced control methodologies have helped the development of modern vehicles that are capable of path planning and path following. For instance, Model Predictive Control (MPC) employs a predictive model to predict the behavior of the physical system for a specific time horizon in the future. An optimization problem is solved to compute optimal control actions while handling model uncertainties and nonlinearities. However, these prediction routines are computationally intensive and the computational overhead grows with the complexity of the model. Switching MPC addresses this issue by combining multiple predictive models, each with a different precision granularity. In this artcle, we proposed a novel switching predictive control method based on a model reduction scheme to achieve various model granularities for path following in autonomous vehicles. A state-based model with tunable parameters is proposed to operate as a reconfigurable predictive model of the vehicle. A runtime switching algorithm is presented that selects the best model using machine learning. We employed a metric that formulates the tradeoff between the error and computational savings due to model reduction. Our simulation results show that the use of the predictive model in the switching scheme as opposed to single granularity scheme, yields a 45% decrease in execution time in tradeoff for a small 12% loss in accuracy in prediction of future outputs and no loss of accuracy in tracking the reference trajectory. © 2018 Association for Computing Machinery.",Linear regression; Modeling; Neural networks; Reconfigurable; Switching predictive control,Control nonlinearities; Forecasting; Learning systems; Linear regression; Models; Motion planning; Neural networks; Predictive control systems; Switching; Uncertainty analysis; Vehicles; Computational overheads; Computational savings; Optimization problems; Predictive control; Predictive control methods; Reconfigurable; Reference trajectories; Switching algorithms; Model predictive control
CASCA: A design automation approach for designing hardware countermeasures against side-channel attacks,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056373026&doi=10.1145%2f3241047&partnerID=40&md5=64c205a7c5b1c984d37eb2716d90a545,"Implementing a cryptographic circuit poses challenges not always acknowledged in the backing mathematical theory. One of them is the vulnerability against side-channel attacks. A side-channel attack is a procedure that uses information leaked by the circuit through, for example, its own power consumption or electromagnetic emissions, to derive sensitive data (e.g, the secret key used for encryption). Nowadays, we design circuitry to keep this sensitive information from leaking (i.e., a countermeasure), but the path from specification down to implementation is far from being fully automatic. As we know, manual refinement steps can be error prone and the sheer potential of these errors can be devastating in a scenario such as the one we are dealing with. In this article, we investigate whether a single embedded domain specific language (EDSL) can, at the same time, help us in specifying and enforcing the functionality of the circuit as well as its protection against side-channel attacks. The EDSL is a fundamental block of an original design flow (named Countermeasure Against Side-Channel Attacks, i.e., CASCA) whose aim is to complement an existing industrial scenario and to provide the necessary guarantee that a secure primitive is not vulnerable up to a first-order attack. As a practical case study, we will show how we applied the proposed tools to ensure both functional and extra-functional correctness of a composite-field Advanced Encryption Standard (AES) S-Box. To ensure the reproducibility of this research, this article is accompanied by an open source release of the EDSL1 that contains the presented S-Box implementation and an additional 3-Shares threshold implementation of the Keccak χ function [7]. © 2018 Association for Computing Machinery.",Domain specific languages; Extra-functional validation,Computer aided design; Data privacy; Graphical user interfaces; Problem oriented languages; Advanced Encryption Standard; Domain specific languages; Electromagnetic emissions; Embedded domain specific languages; Functional correctness; Functional validation; Hardware Countermeasures; Sensitive informations; Side channel attack
"Learning from sleeping experts: Rewarding informative, available, and accurate experts",2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056384435&doi=10.1145%2f3236617&partnerID=40&md5=10ea3ac7e63131ed25a3abefb92bc68d,"We consider a generalized model of learning from expert advice in which experts could abstain from participating at some rounds. Our proposed online algorithm falls into the class of weighted average predictors and uses a time-varying multiplicative weight update rule. This update rule changes the weight of an expert based on his or her relative performance compared to the average performance of available experts at the current round. This makes the algorithm suitable for recommendation systems in the presence of an adversary with many potential applications in the new emerging area of the Internet of Things. We prove the convergence of our algorithm to the best expert, defined in terms of both availability and accuracy, in the stochastic setting. In particular, we show the applicability of our definition of best expert through convergence analysis of another well-known algorithm in this setting. Finally, through simulation results on synthetic and real datasets, we justify the out-performance of our proposed algorithms compared to the existing ones in the literature. © 2018 Association for Computing Machinery.",Accuracy; Availability; Convergence analysis; Internet of Things; Learning; Performance based; Sleeping expert; Stochastic approximation; Weighted average predictor,Availability; Internet of things; Stochastic systems; Accuracy; Convergence analysis; Learning; Performance based; Sleeping expert; Stochastic approximations; Weighted averages; Statistical methods
Trading off power consumption and prediction performance in wearable motion sensors: An optimal and real-time approach,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061278473&doi=10.1145%2f3198457&partnerID=40&md5=a0332a22e9e6aafea36c89fa530e30dd,"Power consumption is identified as one of the main complications in designing practical wearable systems, mainly due to their stringent resource limitations. When designing wearable technologies, several system-level design choices, which directly contribute to the energy consumption of these systems, must be considered. In this article, we propose a computationally lightweight system optimization framework that trades off power consumption and performance in connected wearable motion sensors. While existing approaches exclusively focus on one or a few hand-picked design variables, our framework holistically finds the optimal power-performance solution with respect to the specified application need. Our design tackles a multi-variant non-convex optimization problem that is theoretically hard to solve. To decrease the complexity, we propose a smoothing function that reduces this optimization to a convex problem. The reduced optimization is then solved in linear time using a devised derivative-free optimization approach, namely cyclic coordinate search. We evaluate our framework against several holistic optimization baselines using a real-world wearable activity recognition dataset. We minimize the energy consumption for various activity-recognition performance thresholds ranging from 40% to 80% and demonstrate up to 64% energy savings. © 2018 Association for Computing Machinery.",Activity recognition; Body sensor networks; Embedded systems; Energy optimization; Wearable monitoring systems,Body sensor networks; Convex optimization; Electric power utilization; Embedded systems; Energy conservation; Energy utilization; Functions; Pattern recognition; Activity recognition; Derivative-free optimization; Energy optimization; Holistic optimizations; Monitoring system; Nonconvex optimization; Prediction performance; Resource limitations; Wearable sensors
Distributed machine learning on smart-gateway network toward real-time smart-grid energy management with behavior cognition,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061286478&doi=10.1145%2f3209888&partnerID=40&md5=f9bf0eed6b094d914dcb4e7fb6958f0c,"Real-time data analytics for smart-grid energy management is challenging with consideration of both occupant behavior profiles and energy profiles. This article proposes a distributed and networked machine-learning platform on smart-gateway-based smart-grid in residential buildings. It can analyze occupant behaviors, provide short-term load forecasting, and allocate renewable energy resources. First, occupant behavior profile is captured by real-time indoor positioning system with WiFi data analytics; and the energy profile is extracted by real-time meter system with electricity load data analytics. Then, the 24-hour occupant behavior profile and energy profile are fused with prediction using an online distributed machine-learning algorithm with real-time data update. Based on the forecasted occupant behavior profile and energy profile, solar energy source is allocated to reduce peak demand on the main electricity power-grid. The whole management flow can be operated on the distributed smart-gateway network with limited computational resources but with a supported general machine-learning engine. Experimental results on occupant behavior extraction show that the proposed algorithm can achieve 91.2% positioning accuracy within 3.64m. Moreover, 50× and 38× speed-up is obtained during data testing and training, respectively, when compared to traditional support vector machine (SVM) method. For short-term load forecasting, it is 14.83% more accurate when compared to SVM-based data analytics. Based on the predicted occupant behavior profile and energy profile, our proposed energy management system can achieve 19.66% more peak load reduction and 26.41% more cost saving as compared to the SVM-based method. © 2018 ACM",Behavior cognition; Distributed machine learning; Energy management system; Smart-gateway network,Data Analytics; Distributed computer systems; E-learning; Electric power plant loads; Electric power transmission networks; Energy management; Energy management systems; Forecasting; Gateways (computer networks); Learning algorithms; Machine learning; Solar energy; Support vector machines; Behavior cognition; Computational resources; Distributed machine learning; Peak load reductions; Positioning accuracy; Residential building; Short term load forecasting; SVM-based methods; Smart power grids
GPlace3.0: Routability-driven analytic placer for ultrascale FPGA architectures,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060310495&doi=10.1145%2f3233244&partnerID=40&md5=ef400bea137123551c431a3fe5aab5e9,"Optimizing for routability during FPGA placement is becoming increasingly important, as failure to spread and resolve congestion hotspots throughout the chip, especially in the case of large designs, may result in placements that either cannot be routed or that require the router to work excessively hard to obtain success. In this article, we introduce a new, analytic routability-aware placement algorithm for Xilinx UltraScale FPGA architectures. The proposed algorithm, called GPlace3.0, seeks to optimize both wirelength and routability. Our work contains several unique features including a novel window-based procedure for satisfying legality constraints in lieu of packing, an accurate congestion estimation method based on modifications to the pathfinder global router, and a novel detailed placement algorithm that optimizes both wirelength and external pin count. Experimental results show that compared to the top three winners at the recent ISPD'16 FPGA placement contest, GPlace3.0 is able to achieve (on average) a 7.53%, 15.15%, and 33.50% reduction in routed wirelength, respectively, while requiring less overall runtime. As well, an additional 360 benchmarks were provided directly from Xilinx Inc. These benchmarks were used to compare GPlace3.0 to the most recently improved versions of the first- and second-place contest winners. Subsequent experimental results show that GPlace3.0 is able to outperform the improved placers in a variety of areas including number of best solutions found, fewest number of benchmarks that cannot be routed, runtime required to perform placement, and runtime required to perform routing. © 2018 Association for Computing Machinery.",Congestion; Field programmable gate array; Heterogeneous; Placement; Routing-aware; Ultrascale architecture,Automation; Computer applications; Analytic placer; Congestion; Congestion estimation; FPGA architectures; Heterogeneous; Placement; Placement algorithm; Routing-aware; Field programmable gate arrays (FPGA)
An algorithmic approach to formally verify an ECC library,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053441263&doi=10.1145%2f3224205&partnerID=40&md5=d7e65e2565702923a3a0c1452f9d0b86,"The weakest link in cryptosystems is quite often due to the implementation rather than the mathematical underpinnings. A vast majority of attacks in the recent past have targeted programming flaws and bugs to break security systems. Due to the complexity, empirically verifying such systems is practically impossible, while manual verification as well as testing do not provide adequate guarantees. In this article, we leverage model checking techniques to prove the functional correctness of an elliptic curve cryptography (ECC) library with respect to its formal specification. We demonstrate how the huge state space of the C library can be aptly verified using a hierarchical assume-guarantee verification strategy. To test the scalability of this approach, we verify the correctness of five NIST-specified elliptic curve implementations. We also verify the newer curve25519 elliptic curve, which is finding multiple applications, due to its higher security and simpler implementation. The 192-bit NIST elliptic curve took 1 day to verify. This was the smallest curve we verified. The largest curve with a 521-bit prime field took 26 days to verify. Curve25519 took 1.5 days to verify. © 2018 ACM.",Bounded model checking (BMC); Cryptosystems; Elliptic curve cryptography (ECC); Formal verification; Security,Cryptography; Formal verification; Geometry; Program debugging; Public key cryptography; Algorithmic approach; Bounded model checking; Elliptic Curve Cryptography (ECC); Functional correctness; Model-checking techniques; Multiple applications; Security; Verification Strategy; Model checking
Guiding formal verification orchestration using machine learning methods,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052604961&doi=10.1145%2f3224206&partnerID=40&md5=4a304d409c66a455cccd5717d3075e82,"Typical modern HW designs include many blocks associated with thousands of design properties. Having today's commercial formal verifiers utilize a complementary set of state-of-art formal algorithms is a key in enabling the formal verification tools to successfully cope with verification problems of different sizes, types, and complexities. Formal engines orchestration is the methodology used to pick the most appropriate formal engine for a specific verification problem. It assures proper scheduling of the formal engines to minimize the time consumed to solve individual design verification problems, hence highly impacts the time required to verify the overall design properties. This work proposes the utilization of supervised machine learning classification techniques to guide the orchestration step by predicting the formal engines that should be assigned to a design property. Up to 16,500 formal verification runs on RTL designs and their properties are used to train the classifier to create a prediction model. The classifier assigns any new verification problem to an appropriate list of formal engines associated with a probability distribution over the set of engines classes. Our results indicate how the proposed model is able to improve the formal suite total run-time by up to 59% of its maximum allowable time improvement using multi-classification-based orchestration and to nominate with 88% accuracy the appropriate formal engines for new-to-verify HW designs. © 2018 ACM.",Algorithm selection; Classification ML; Formal verification,Artificial intelligence; Engines; Probability distributions; Supervised learning; Algorithm selection; Complementary sets; Design verification; Formal verification tools; Machine learning methods; Multi-classification; Supervised machine learning; Verification problems; Formal verification
Enhancing flash memory reliability by jointly considering write-back pattern and block endurance,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052591392&doi=10.1145%2f3229192&partnerID=40&md5=77b33fc5f940e69bc3f18d1ddc8b7d20,"Owing to high cell density caused by the advanced manufacturing process, the reliability of flash drives turns out to be rather challenging in flash system designs. To enhance the reliability of flash drives, error-correcting code (ECC) has been widely utilized in flash drives to correct error bits during programming/reading data to/from flash drives. Although ECC can effectively enhance the reliability of flash drives by correcting error bits, the capability of ECC would degrade while the program/erase (P/E) cycles of flash blocks is increased. Finally, ECC could not correct a flash page, because a flash page contains too many error bits. As a result, reducing error bits is an effective solution to further improve the reliability of flash drives when a specific ECC is adopted in the flash drive. This work focuses on how to reduce the probability of producing error bits in a flash page. Thus, we propose a pattern-aware write strategy for flash reliability enhancement. The proposed write strategy considers both the P/E cycle of blocks and the pattern of written data while a flash block is allocated to store the written data. Since the proposed write strategy allocates young blocks (respectively, old blocks) for hot data (respectively, cold data) and flips the bit pattern of the written data to the appropriate bit pattern, the proposed strategy can effectively improve the reliability of flash drives. The experimental results show that the proposed strategy can reduce the number of error pages by up to 50%, compared with the well-known DFTL solution. Moreover, the proposed strategy is orthogonal with all ECC mechanisms so that the reliability of the flash drives with ECC mechanisms can be further improved by the proposed strategy. © 2018 ACM.",Bit error; Bit-flip; Flash reliability; Multi-level cell; NAND flash,Drives; Errors; Reliability; Advanced manufacturing; Bit-errors; Bit-flips; Effective solution; Error correcting code; Multilevel cell; NAND Flash; Reliability enhancement; Flash memory
Toward effective reliability requirement assurance for automotive functional safety,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052603977&doi=10.1145%2f3230620&partnerID=40&md5=9e8e7900500e1360ddd04e8456e5960e,"Automotive functional safety requirement includes response time and reliability requirements learning from the functional safety standard ISO 26262. These two requirements must be simultaneously satisfied to assure automotive functional safety requirement. However, increasing reliability increases the response time intuitively. This study proposes a method to find the solution with the minimum response time while assuring reliability requirement. Pre-assigning reliability values to unassigned tasks by transferring the reliability requirement of the function to each task is a useful reliability requirement assurance approach proposed in recent years. However, the pre-assigned reliability values in state-of-the-art studies have unbalanced distribution of the reliability of all tasks, thereby resulting in a limited reduction in response time. This study presents the geometric mean-based non-fault-tolerant reliability pre-assignment (GMNRP) and geometric mean-based fault-tolerant reliability pre-assignment (GMFRP) approaches, in which geometric mean-based reliability values are pre-assigned to unassigned tasks. Geometric mean can make the pre-assigned reliability values of unassigned tasks to the central tendency, such that it can distribute the reliability requirements in a more balanced way. Experimental results show that GMNRP and GMFRP can effectively reduce the response time compared with their individual state-of-the-art counterparts. © 2018 ACM.",Fault-tolerance; Functional safety; Geometric mean; Reliability requirement assurance,Geometry; Automotive functional safeties; Central tendencies; Functional Safety; Geometric mean; Reliability requirements; Reliability values; State of the art; Unbalanced distribution; Fault tolerance
Performance and thermal tradeoffs for energy-efficient monolithic 3d network-on-chip,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052559182&doi=10.1145%2f3223046&partnerID=40&md5=7fa768a70934dfaa5010325b6307f564,"Three-dimensional (3D) integration enables the design of high-performance and energy-efficient network on chip (NoC) architectures as communication backbones for manycore chips. To exploit the benefits of the vertical dimension of 3D integration, through-silicon-via (TSV) has been predominantly used in state-of-the-art manycore chip design. However, for TSV-based systems, high power density and the resultant thermal hotspot remain major concerns from the perspectives of chip functionality and overall reliability. The power consumption and thermal profiles of 3D NoCs can be improved by incorporating a Voltage-Frequency-Island (VFI)-based power management strategy. However, due to inherent thermal constraints of a TSV-based 3D system, we are unable to fully exploit the benefits offered by the power management methodology. In this context, emergence of monolithic 3D (M3D) integration has opened up new possibility of designing ultra-low-power and high-performance circuits and systems. The smaller dimensions of the inter-layer dielectric (ILD) and monolithic inter-tier vias (MIVs) offer high-density integration, flexibility of partitioning logic blocks across multiple tiers, and significant reduction of total wire-length. In this work, we present the first-ever study of the performance-thermal tradeoffs for energy efficient monolithic 3D manycore chips. In particular, we present a comparative performance evaluation of M3D NoCs with respect to their conventional TSV-based counterparts. We demonstrate that the proposed M3D-based NoC architecture incorporating VFI-based power management achieves a maximum of 29.4% lower energy-delay-product (EDP) compared to the TSV-based designs for a large set of benchmarks. We also demonstrate that the M3D-based NoC shows up to 29.1% lower maximum temperature than the TSV-based counterpart for these benchmarks. © 2018 ACM.",3D NoC; Monolithic 3D; Thermal hotspots; VFI power management,Commerce; Electronics packaging; Energy efficiency; Integrated circuit design; Integrated circuit manufacture; Integration; Low power electronics; Network architecture; Network-on-chip; Power management; Servers; Energy efficient network on chips; High-density integration; High-performance circuits; Power management strategies; Thermal Hotspots; Three dimensional (3D) integration; Through-Silicon-Via (TSV); Voltage-frequency islands; Three dimensional integrated circuits
Non-intrusive in-situ requirements monitoring of embedded system,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052604740&doi=10.1145%2f3206213&partnerID=40&md5=d1d2a613edf1874898bb9d365c32fb7a,"Accounting for all operating conditions of a system at the design stage is typically infeasible for complex systems. Monitoring and verifying system requirements at runtime enable a system to continuously and introspectively ensure the system is operating correctly in the presence of dynamic execution scenarios. In this article, we present a requirements-driven methodology enabling efficient runtime monitoring of embedded systems. The proposed approach extracts a runtime monitoring graph from system requirements specified using UML sequence diagrams. Non-intrusive, on-chip hardware dynamically monitors the system execution, verifies the execution adheres to the requirements model, and in the event of a failure provides detailed information that can be analyzed to determine the root cause. Using case studies of an autonomous vehicle and pacemaker prototypes, we analyze the relationship between event coverage, detection rate, and hardware requirements. © 2018 ACM.",Embedded systems; Non-intrusive system monitoring; Runtime requirements monitoring,Embedded systems; Hardware; Requirements engineering; Autonomous Vehicles; Operating condition; Requirements Models; Requirements monitoring; Runtime Monitoring; System monitoring; System requirements; UML sequence diagrams; Monitoring
Folded circuit synthesis: Min-area logic synthesis using dual-edge-triggered flip-flops,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052604109&doi=10.1145%2f3229082&partnerID=40&md5=6ad679d741c2c208e3ec0d0d346219be,"The area required by combinational logic of a sequential circuit based on standard flip-flops can be reduced by identifying subcircuits that are identical. Pairs of matching subcircuits can then be replaced by circuits in which dual-edge-triggered flip-flops operate on multiplexed data at the rising and falling edges of the clock signal. We show how to modify the Boolean network describing a combinational logic to increase the opportunities for folding, without affecting its function. Experiments with benchmark circuits achieved an average reduction in circuit area of 18%. © 2018 ACM.",Area reduction; Dual-edge-triggered flip-flop; Logic synthesis,Boolean functions; Codes (symbols); Flip flop circuits; Logic circuits; Logic Synthesis; Timing circuits; Trigger circuits; Area reduction; Benchmark circuit; Boolean Networks; Circuit synthesis; Clock signal; Combinational logic; Falling edge; Sub-circuits; Computer circuits
Dynamically determined preferred values and a design-for-testability approach for multiplexer select inputs under functional test sequences,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052564564&doi=10.1145%2f3219778&partnerID=40&md5=e9ca679fb734d5c8847e5663857f2177,"Earlier works observed that certain primary inputs have preferred values, which help increase the gate-level fault coverage when they appear in a functional test sequence. This article observes that multiplexers present additional opportunities for increasing the fault coverage of a functional test sequence, which are not captured by preferred primary input values. Because multiplexers are prevalent, their effect on the fault coverage can be significant. A static analysis that is independent of any functional test sequence is performed in this article to identify preferred values for the outputs of multiplexers. This is followed by a dynamic analysis that adjusts the select inputs of the multiplexers for a given functional test sequence to ensure that the preferred values appear on the outputs of the multiplexers more often. The analysis yields design-for-testability logic for the select inputs of the multiplexers that have preferred values. The logic is independent of the functional test sequence, and it allows the fault coverage to be increased when the select inputs are not primary inputs, or when the same select inputs are used for different multiplexers. Experimental results are presented to demonstrate that this approach has a significant effect on the fault coverage of functional test sequences. © 2018 ACM.",Design-for-testability; Functional test sequence; Multiplexer; Preferred value; Sequential test generation,Computer circuits; Design for testability; Multiplexing equipment; Preferred numbers; Fault coverages; Functional test sequences; Gate levels; Multiplexer; Primary inputs; Sequential tests; Testing
A comprehensive side-channel information leakage analysis of an in-order RISC CPU microarchitecture,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052591370&doi=10.1145%2f3212719&partnerID=40&md5=84d1bc11b7705271a37e6aa7b257a665,"Side-channel attacks are a prominent threat to the security of embedded systems. To perform them, an adversary evaluates the goodness of fit of a set of key-dependent power consumption models to a collection of side-channel measurements taken from an actual device, identifying the secret key value as the one yielding the best-fitting model. In this work, we analyze for the first time the microarchitectural components of a 32-bit in-order RISC CPU, showing which one of them is accountable for unexpected side-channel information leakage. We classify the leakage sources, identifying the data serialization points in the microarchitecture and providing a set of hints that can be fruitfully exploited to generate implementations resistant against side-channel attacks, either writing or generating proper assembly code. © 2018 ACM.",Applied cryptanalysis; Security by design; Simulation-based side-channel analysis,Codes (symbols); Computer architecture; Embedded systems; Applied cryptanalysis; Fitting model; Goodness of fit; Micro architectures; Power consumption model; Security by designs; Side-channel analysis; Side-channel information; Side channel attack
A maze routing-based methodology with bounded exploration and path-assessed retracing for constrained multilayer obstacle-avoiding rectilinear steiner tree construction,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053432392&doi=10.1145%2f3177878&partnerID=40&md5=1dc3264d291b968c45223c5934815ec3,"Owing to existing intellectual properties, prerouted nets, and power/ground wires, the routing of a system on chip design demands to detour around multilayer obstacles. Traditional approaches for the multilayer obstacle-avoiding rectilinear Steiner tree (ML-OARST) problem are thus nonmaze routing-based approaches for runtime issues, yet they cannot be directly applied to deal with additional constraints such as variant edge weights on a routing layer. In this article, we propose the maze routing-based methodology with bounded exploration and path-assessed retracing to reduce runtime and routing cost for the constrained ML-OARST construction problem. The exploration of maze routing is bounded to reduce the runtime; the costs of connecting pins are computed to select Steiner points in the retracing phase. To further reduce the routing cost, we develop a Steiner point-based ripping-up and rebuilding scheme for altering tree topology. Experimental results on industrial and randomly generated benchmarks demonstrate that the proposed methodology can provide a solution with good quality in terms of routing cost and has a significant speedup compared to traditional maze routing. A commercial tool is also used to show the effectiveness of the proposed methodology. © 2018 ACM.",Layout; Physical design; Routing; Steiner tree,Benchmarking; Multilayers; System-on-chip; Construction problem; Layout; Physical design; Rectilinear steiner trees; Routing; Steiner trees; System on chip design; Traditional approaches; Cost reduction
An integration flow for mixed-critical embedded systems on a flexible time-triggered platform,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053413293&doi=10.1145%2f3190837&partnerID=40&md5=d3e80240fb9d1b101ad3f634ac23edbd,"The rise of mixed-critical embedded systems imposes novel challenges on the specification, development, and functional validation in a design flow. In the emerging dynamic scheduling context of mixed-criticality platforms, the system behaviour needs to be estimated in an early step in the design flow to assess the integration impact, especially for quality of service-driven, low-critical subsystems.We provide a modelling and integration flow for specifying, estimating, and evaluating software functions, ranging from an initial executable specification to an implementation candidate on anMPSoC. Based on a data-driven model to evaluate dynamic resource consumption effects of high-critical subsystems and the scheduling overhead, we propose a systematic method for constructing workload models of high-critical software components on the target. Our proxies provide an integration environment for low-critical functions by mimicking the high-critical temporal behaviour on the target. By integrating a low-critical video encoding subsystem with a benchmark suite as the high-critical subsystem we show that the performance model allows for evaluating end-to-end execution times in the low-critical function with an average error of 0.37% and the application proxy only introduces a maximum error of 1.14% in a performance evaluation. © 2018 ACM.",Design flow; Integration; Mixed criticality; Performance modelling; Real-time systems,Benchmarking; Criticality (nuclear fission); Embedded systems; Function evaluation; Integral equations; Integration; Interactive computer systems; Quality of service; Scheduling; Specifications; Video signal processing; Design flows; Evaluating software; Executable specifications; Functional validation; Integration environments; Mixed criticalities; Performance evaluations; Performance modelling; Real time systems
Thermal-sensor-based occupancy detection for smart buildings using machine-learning methods,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053375961&doi=10.1145%2f3200904&partnerID=40&md5=e261b14cfac2c140ee5449ca2bc9e773,"In this article, we propose a novel approach to detect the occupancy behavior of a building through the temperature and/or possible heat source information. The new method can be used for energy reduction and security monitoring for emerging smart buildings. Our work is based on a building simulation program, EnergyPlus, from the Department of Energy. EnergyPlus can model various time-series inputs to a building such as ambient temperature; heating, ventilation, and air-conditioning (HVAC) inputs; power consumption of electronic equipment; lighting; and number of occupants in a room, sampled each hour, and produce resulting temperature traces of zones (rooms). Two machine-learning-based approaches for detecting human occupancy of a smart building are applied herein, namely support vector regression (SVR) and recurrent neural network (RNN). Experimental results with SVR show that the four-feature model provides accurate detection rates, giving a 0.638 average error and 5.32% error rate, and the five-feature model delivers a 0.317 average error and 2.64% error rate. This indicates that SVR is a viable option for occupancy detection. In the RNN method, Elman's RNN can estimate occupancy information of each room of a building with high accuracy. It has local feedback in each layer and, for a five-zone building, it is very accurate for occupancy behavior estimation. The error level, in terms of number of people, can be as low as 0.0056 on average and 0.288 at maximum, considering ambient, room temperatures, and HVAC powers as detectable information. Without knowing HVAC powers, the estimation error can still be 0.044 on average, and only 0.71% estimated points have errors greater than 0.5. Our article further shows that both methods deliver similar accuracy in the occupancy detection. But the SVR model is more stable for adding or removing features of the system, while the RNN method can deliver more accuracy when the features used in the model do not change a lot. © 2018 ACM.",Indoor temperature; Neural network; Occupancy detection; Smart building; Support vector regression,Air conditioning; Errors; Intelligent buildings; Learning systems; Neural networks; Oscillators (electronic); Recurrent neural networks; Building simulation; Department of Energy; Indoor temperature; Machine learning methods; Occupancy detections; Recurrent neural network (RNN); Security monitoring; Support vector regression (SVR); Buildings
Optimal allocation of LDOs and decoupling capacitors within a distributed on-chip power grid,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053392179&doi=10.1145%2f3177877&partnerID=40&md5=bb3fa921f69807e7fc254849c5431516,"Parallel on-chip voltage regulation, where multiple regulators are connected to the same power grid, has recently attracted significant attention with the proliferation of small on-chip voltage regulators. In this article, the number, size, and location of parallel low-dropout (LDO) regulators and intentional decoupling capacitors are optimized using mixed integer non-linear programming formulation. The proposed optimization function concurrently considers multiple objectives such as area, power noise, and overall power consumption. Certain objectives are optimized by putting constraints on the other objectives with the proposed technique. Additional constraints have been added to avoid the overlap of LDOs and decoupling capacitors in the optimization process. The results of an optimized LDO allocation in the POWER8 chip is compared with the recent LDO allocation in the same IBM chip in a case study where a 20% reduction in the noise is achieved. The results of the proposed multi-criteria objective function under a different area, power, and noise constraints are also evaluated with a sample ISPD'11 benchmark circuits in another case study. © 2018 ACM.",Current sharing; Decoupling capacitors; Distributed on-chip voltage regulator; Physical design; Power delivery network (PDN),Electric current regulators; Electric power transmission; Integer programming; Nonlinear programming; Voltage regulators; Current-sharing; Decoupling capacitor; On-chip voltage regulator; Physical design; Power delivery network (PDN); Electric power transmission networks
Ordered escape routing with consideration of differential pair and blockage,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053402272&doi=10.1145%2f3185783&partnerID=40&md5=4d0d13f9c4c954d312a5138e85c518e2,"Ordered escape routing is a critical issue in high-speed PCB routing. Differential pair and thermal-blockageavoided are useful in PCB design to obtain high noise immunity and low electromagnetic interference. In this article, a Min-cost Multi-commodity Flow (MMCF) approach is proposed to solve the ordered escape routing. First, the characteristic of grid pin array and staggered pin array is analyzed and then a basic networkmodel is used to convert ordered escape routing to MMCF model. To satisfy the constraints of ordered escape routing, three novel transformations, such as non-crossing transformation, ordering transformation, and capacity transformation, are used to convert the basic network model to the final correct MMCF model. After that, the differential pair in ordered escape routing is discussed. Finally, a method to deal with the blockage issue is proposed. Experimental results show that our method achieves 100% routability for all the test cases. The method can get both a feasible solution and an optimal solution for ordered escape routing. Compared to published approaches, our method improves in both wire length and CPU time remarkably. At the same time, the proposed method can effectively avoid the blockage and deal with the differential pair. © 2018 ACM.",Blockage-avoided routing; Differential-pair; Grid pin array; Min-cost multi-commodity flow; Ordered escape routing; PCB routing; Staggered pin array,Electromagnetic pulse; Organic pollutants; Blockage-avoided routing; Differential pairs; Escape routing; Grid pin array; Multi-commodity flow; PCB routing; Staggered pin array; Polychlorinated biphenyls
Partially invariant patterns for LFSR-based generation of close-to-functional broadside tests,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053376668&doi=10.1145%2f3201405&partnerID=40&md5=e30e8f350ccae4e9100a2a230f65be90,"Close-to-functional scan-based tests are expected to create close-to-functional operation conditions in order to avoid overtesting of delay faults. Existing metrics for the proximity to functional operation conditions are based on the scan-in state. For example, they consider the distance between the scan-in state and a reachable state (a state that the circuit can visit during functional operation). However, the deviation from functional operation conditions can increase during a test beyond the deviation that is measured by the scan-in state. To ensure that the deviation does not increase, this article introduces the concept of a partially invariant pattern. The article describes a procedure for extracting partially invariant patterns from functional broadside tests whose scan-in states are reachable states. Being partially specified, partially invariant patterns are suitable for test data compression. The article studies the use of partially invariant patterns for linear-feedback shift-register (LFSR) based test data compression. Noting that a seed may not exist for a given partially invariant pattern with a given LFSR, the procedure described in this article uses an iterative process that not only matches a seed to a partially invariant pattern, but also adjusts the partially invariant pattern based on the test that the seed produces. The article also addresses the selection of LFSRs for the generation of close-to-functional broadside tests based on partially invariant patterns. Experimental results are presented to demonstrate the feasibility of the procedure. © 2018 ACM.",Functional broadside tests; Linear-feedback shift-register (LFSR); Test data compression; Test generation; Transition faults,Data compression; Feedback; Functional broadside tests; Linear feedback shift registers; Test Data Compression; Test generations; Transition faults; Shift registers
ITimerM: A compact and accurate timing macro model for efficient hierarchical timing analysis,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053389454&doi=10.1145%2f3149818&partnerID=40&md5=bbe13c51c65560481e118aa307d03910,"As designs continue to grow in size and complexity, EDA paradigm shifts from flat to hierarchical timing analysis. In this article, we present compact and accurate timing macro modeling, which is the key to efficient and accurate hierarchical timing analysis. Our goal is to contain only a minimal amount of interface logic in our timing macro model. The main idea is to separate the interface logic into variant and constant timing regions. Then, the variant timing region is reserved for accuracy, while the constant timing region is reduced for compactness. For reducing the constant timing region, we propose anchor pin insertion and deletion by generalizing existing timing graph reduction techniques. Furthermore, we devise a lookup table index selection technique to achieve high model accuracy over the possible operating condition range. Compared with two common models used in industry, extracted timing model and interface logic model, our model has high model accuracy and small model size. Based on the TAU 2016 and 2017 timing macro modeling contest benchmark suites, our results show that our algorithm delivers superior efficiency and accuracy: Hierarchical timing analysis using our model can significantly reduce runtime and memory compared with flat timing analysis on the original design. Moreover, our algorithm outperforms TAU 2016 and 2017 contest winners in model accuracy, model size, model generation performance, and model usage performance. © 2018 ACM.",Extracted timing model; Hierarchical timing analysis; Interface logic model; Timing macro modeling,Table lookup; Timing circuits; Accurate timing; Benchmark suites; Hierarchical timing analysis; Interface logic; Macro model; Model generation; Operating condition; Timing modeling; Computer circuits
Demand-driven single- and multitarget mixture preparation using digital microfluidic biochips,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053374923&doi=10.1145%2f3200903&partnerID=40&md5=3776f4fa78bc80dbd2a45b4f6b95689c,"Recent studies in algorithmic microfluidics have led to the development of several techniques for automated solution preparation using droplet-based digital microfluidic (DMF) biochips. A major challenge in this direction is to produce a mixture of several reactants with a desired ratio while optimizing reactant cost and preparation time. The sequence of mix-split operations that are to be performed on the droplets is usually represented as a mixing tree (or graph). In this article, we present an efficient mixing algorithm, namely, Mixing Tree with Common Subtrees (MTCS), for preparing single-target mixtures. MTCS attempts to best utilize intermediate droplets, which were otherwise wasted, and uses morphing based on permutation of leaf nodes to further reduce the graph size. The technique can be generalized to produce multitarget ratios, and we present another algorithm, namely, Multiple Target Ratios (MTR). Additionally, in order to enhance the output load, we also propose an algorithm for droplet streaming called Multitarget Multidemand (MTMD). Simulation results on a large set of target ratios show that MTCS can reduce the mean values of the total number of mix-split steps (Tms) and waste droplets (W) by 16% and 29% over Min-Mix (Thies et al. 2008) and by 22% and 34% over RMA (Roy et al. 2015), respectively. Experimental results also suggest that MTR can reduce the average values of Tms andW by 23% and 44% over the repeated version of Min-Mix, by 30% and 49% over the repeated version of RMA, and by 9% and 22% over the repeated-version of MTCS, respectively. It is observed that MTMD can reduce the mean values ofTms andW by 64% and 85%, respectively, over MTR. Thus, the proposed multitarget techniques MTR and MTMD provide efficient solutions to multidemand, multitarget mixture preparation on a DMF platform. © 2018 ACM.",Biochips; Digital microfluidics; Mixing; Sample preparation,Biochips; Drop formation; Forestry; Mixing; Mixtures; Trees (mathematics); Automated solutions; Average values; Digital microfluidic biochips; Mixing algorithms; Mixture preparations; Multiple targets; Sample preparation; Split operations; Digital microfluidics
Enhancements to SAT attack: Speedup and breaking cyclic logic encryption,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053419808&doi=10.1145%2f3190853&partnerID=40&md5=ac7f2865ee2334b7a61c715080faef52,"Logic encryption is an IC protection technique for preventing an IC design from overproduction and unauthorized use. It hides a design's functionality by inserting key gates and key inputs, such that a secret key is required to activate the design and make it functioncorrectly. The security of a logic encryption algorithm is evaluated according to the difficulty of cracking the secret key. The state-of-the-art attack method identifies a secret key with a series of SAT-solving calls to prune all the incorrect keys. Although it can break most of the existing logic encryption algorithms within a few hours, we observe that there exist two enhancements for increasing its efficiency. First, we introduce a preprocess to identify and eliminate redundant key inputs and simplify SAT problems. Second, we present a key checking process for increasing the pruned incorrect keys in each SAT-solving iteration. We conducted the experiments on a set of benchmark circuits encrypted by six different logic encryption algorithms. The simulation results show that the enhanced method can successfully unlock 10 benchmark circuits which originally could not be cracked within 1 hour. For all the benchmark circuits, the average speedup is approximately 2.2× in terms of simulation time. Furthermore, a recent logic encryption method locks a design by creating cyclic paths, which can invalidate the SAT-based attack method. We analyze the impact of cyclic paths and propose an enhancement to break the cyclic logic encryption method. © 2018 ACM.",IC camouflaging; IC protection; Logic decryption; Logic encryption; Satisfiability attack,Computer circuits; Integrated circuit design; Integrated circuits; Iterative methods; Benchmark circuit; Encryption algorithms; Encryption methods; Its efficiencies; Logic decryption; Protection techniques; Satisfiability; State of the art; Cryptography
Eh?Legalizer: A high performance standard-cell legalizer observing technology constraints,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053436041&doi=10.1145%2f3158215&partnerID=40&md5=61befd94e2c7a5b5804b63e5b0d133f7,"The legalization step is performed after global placement where wire length and routability are optimized or during timing optimization where buffer insertion or gate sizing are applied to meet timing requirements. Therefore, an ideal legalization approach must preserve the quality of the input placement in terms of routability, wire length, and timing constraints. These requirements indirectly impose maximum and average cell movement constraints during legalization. In addition, the legalization step should effectively manage white space availability with a highly efficient runtime in order to be used in an iterative process such as timing optimization. In this article, a robust and fast legalization method called Eh?Legalizer for standard-cell placement is presented. Eh?Legalizer legalizes input placements while minimizing the maximum and average cell movements using a highly efficient novel network flow-based approach. In contrast to the traditional network flow-based legalizers, areas with high cell utilizations are effectively legalized by finding several candidate paths and there is no need for a post-process step. The experimental results conducted on several benchmarks show that Eh?Legalizer results in 2.5 times and 3.3 times less the maximum and average cell movement, respectively, while its runtime is significantly (18×) lower compared to traditional legalizers. In addition, the experimental results illustrate the scalability and robustness of Eh?Legalizer with respect to the floorplan complexity. Finally, the detailed-routing results show detailed-routing violations are reduced on average by 23% when Eh?Legalizer is used to generate legal solutions. © 2018 ACM.",Fence regions; Legalization; Network flows; Placement; Target density,Authentication; Contracts; Cytology; Data flow analysis; Embedded systems; Gates (transistor); Iterative methods; Legalization; Network flows; Performance standards; Placement; Standard-cell placement; Target density; Technology constraints; Timing requirements; Cells
Reverse engineering digital ICs through geometric embedding of circuit graphs,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053407383&doi=10.1145%2f3193121&partnerID=40&md5=0523bb220fcb9de7a3c3e1398043beb3,"Outsourcing of design and manufacturing processes makes integrated circuits (ICs) vulnerable to adversarial changes and raises concerns about their integrity. Reverse engineering the manufactured netlist helps identify malicious insertions. In this article, we present an automated approach that, given a reference design description with high-level blocks, infers these blocks in an untrusted gate-level (test) implementation. Using the graph connectivity of the netlists, we compute a geometric embedding for each wire in the circuits, which, then, is used to compute a bipartite matching between the nodes of the two designs and identify high-level blocks in the test circuit. Experiments to evaluate the efficacy of the proposed technique on various-sized designs, including the multi-core processor OpenSparc T1, show that it can correctly match over 90% of gates in the test circuit to their corresponding block in the reference model. © 2018 ACM.",Clustering; Partitioning; Reverse engineering,Integrated circuit design; Reverse engineering; Timing circuits; Automated approach; Bipartite matchings; Clustering; Geometric embedding; Integrated circuits (ICs); Manufacturing process; Multi-core processor; Partitioning; Digital integrated circuits
UTPlaceF 2.0: A high-performance clock-aware FPGA placement engine,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053420025&doi=10.1145%2f3174849&partnerID=40&md5=405a87b17c83223c1834f5122cee834b,"Modern field-programmable gate array (FPGA) devices contain complex clock architectures on top of configurable logics. Unlike application specific integrated circuits (ASICs), the physical structure of clock networks in an FPGA is pre-manufactured and cannot be adjusted to different applications. Furthermore, clock routing resources are typically limited for high-utilization designs. Consequently, clock architectures impose extra clock constraints and further complicate physical implementation tasks such as placement. Traditional ASIC placement techniques only optimize conventional design metrics such as wirelength, routability, power, and timing without clock legality consideration. It is imperative to have newtechniques to honor clock constraints during placement for FPGAs. In this article, we propose a high-performance FPGA placement engine, UTPlaceF 2.0, that optimizes wirelength and routability while honoring complex clock constraints. Our proposed approaches consist of an iterative minimum-cost-flow-based cell assignment as well as a clock-aware packing for producing clock-legal yet high-quality placement solutions. UTPlaceF 2.0 won first place in the ISPD'17 clock-aware FPGA placement contest organized by Xilinx, outperforming the second- and the third-place winners by 4.0% and 10.0%, respectively, in routed wirelength with competitive runtime, on a set of industry benchmarks. © 2018 ACM.",Clock legalization; FPGA; Packing; Placement,Application specific integrated circuits; Complex networks; Cost accounting; Engines; Field programmable gate arrays (FPGA); Iterative methods; Network architecture; Packing; Clock architecture; Configurable logic; Conventional design; High performance clocks; Industry benchmarks; Minimum cost flows; Physical structures; Placement; Clocks
Variation-aware global placement for improving timing-yield of carbon-nanotube field effect transistor circuit,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053395135&doi=10.1145%2f3175500&partnerID=40&md5=c90de609fff6d3f2db9429761996f0c3,"As the conventional silicon-based CMOS technology marches toward the sub-10nm region, the problem of high power density becomes increasingly serious. Under this circumstance, the carbon-nanotube field effect transistors (CNFETs) emerge as a promising alternative to the conventional silicon-based CMOS devices. However, they experience a much larger variation than the silicon-based CMOS devices, which results in a large circuit delay variation and hence, a significant timing yield loss. One of the main variation sources is the carbon-nanotube (CNT) density variation. However, it shows a special property not existing for silicon-based CMOS devices, namely the asymmetric spatial correlation. In this work, we propose novel global placement algorithms to reduce the timing yield loss caused by the CNT density variation. To effectively reduce the statistical circuit delay, we first develop a statistical delay measure for a segment of gates. Based on this measure, we further develop a segment-based strategy and a path-based placement strategy to reduce the delays of the statistically critical paths. Experimental results demonstrated that both of our approaches effectively improve the timing yield. © 2018 ACM.",Carbon-nanotube field effect transistor (CNFET) circuit; Statistical static timing analysis (SSTA); Statistical timing optimization; Timing-yield improvement; Variation-aware placement,Carbon nanotubes; CMOS integrated circuits; Delay circuits; Electric network analysis; Nanosensors; Timing circuits; Timing devices; Yarn; Carbon nanotube field effect transistor (CNFET); Statistical static timing analyses (SSTA); Statistical timing; Timing yield; Variation-aware placement; Carbon nanotube field effect transistors
Introduction to the special section on advances in physical design automation,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053393901&doi=10.1145%2f3199220&partnerID=40&md5=4990b5c509c046542d7d66d9409cbff7,[No abstract available],,
Routable and matched layout styles for analog module generation,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053406554&doi=10.1145%2f3182169&partnerID=40&md5=fec30155c24b02243d4f914f03719f17,"Two1 novel automatic generation methods for analog layout-a symmetrical twin-row method for MOS transistors and a twisted common-centroid method for capacitor arrays-are introduced. Based on the proposed layout styles and the corresponding algorithms, the symmetry and common-centroid placement patterns for analog devices are realized to guarantee matching properties. On this basis, as the most prominent contribution of this article, channel routing-based algorithms for the proposed layout styles are presented and could achieve 100% routability due to well-arranged devices and corresponding low routing complexity. The algorithms benefits include a small layout area that maximizes the diffusion-sharing of MOS transistors and less routing layer usage for common-centroid device arrays. Moreover, we successfully applied our algorithms to the layout designs of two typical analog modules including a two-stage operating amplifier and a Successive Approximation Register Analog-to-Digital Converter (SAR-ADC). The generated layouts and the circuit simulation results demonstrate the effectiveness of our algorithms in terms of their routability and matching properties. Our algorithms can also be extended to apply to a variety of essential MOS analog circuits. © 2018 ACM.",Analog layout automation; Capacitor array; Common-centroid placement; Diffusion-sharing,Analog to digital conversion; Approximation algorithms; Circuit simulation; Field effect transistors; Analog layout automation; Automatic Generation; Capacitor arrays; Channel routing; Common centroid; Matching property; MOS analog circuits; Successive approximation register analog-to-digital converter; Computational complexity
Avoiding Data Inconsistency in Energy Harvesting Powered Embedded Systems,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062721760&doi=10.1145%2f3182170&partnerID=40&md5=b46854d61c63b14d006bb4e49af02bfa,"Energy harvesting is becoming a favorable alternative to power future generation embedded systems, as it is more environmentally and user friendly. However, energy harvesting powered embedded systems suffer from frequent execution interruption due to unstable energy supply. To tackle this problem, nonvolatile memory has been deployed to save the whole volatile state for computation. When power resumes, the processor can restore the state back to volatile memories and continue execution. However, without careful consideration, the process of checkpointing and resuming could cause inconsistency between volatile and nonvolatile memories, which leads to irreversible errors. In this article, we propose a consistency-aware adaptive checkpointing scheme that ensures correctness for all checkpoints. The proposed technique efficiently identifies all possible inconsistency positions in programs and inserts auxiliary code to ensure correctness by offline analysis. In addition, adaptive checkpointing assisted register file profiling and online tracking techniques further reduce the overhead of each checkpoint. Evaluation results show that the proposed checkpointing strategy can successfully eliminate inconsistency errors and greatly reduce the checkpointing overhead. © 2018 ACM.",adaptive checkpointing; Energy harvesting; inconsistency,Cost reduction; Embedded systems; Adaptive checkpointing; Check pointing; Data inconsistencies; Embedded-system; Future generations; Inconsistency; Non-volatile memory; Nonvolatile memory; Power futures; Volatile memory; Energy harvesting
An Efficient Non-Gaussian Sampling Method for High Sigma SRAM Yield Analysis,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137493540&doi=10.1145%2f3174866&partnerID=40&md5=7f7ce13f05b1cdd4a12d59983618e0cf,"Yield1 analysis of SRAM is a challenging issue, because the failure rates of SRAM cells are extremely small. In this article, an efficient non-Gaussian sampling method of cross entropy optimization is proposed for estimating the high sigma SRAM yield. Instead of sampling with the Gaussian distribution in existing methods, a non-Gaussian distribution, i.e., a joint one-dimensional generalized Pareto distribution and (n-1)-dimensional Gaussian distribution, is taken as the function family of practical distribution, which is proved to be more suitable to fit the ideal distribution in the view of extreme failure event. To minimize the cross entropy between practical and ideal distributions, a sequential quadratic programing solver with multiple starting points strategy is applied for calculating the optimal parameters of practical distributions. Experimental results show that the proposed non-Gaussian sampling is a 2.2-4.1× speedup over the Gaussian sampling, on the whole, it is about a 1.6-2.3× speedup over state-of-the-art methods with low- and high-dimensional cases without loss of accuracy. © 2018 ACM.",cross entropy minimization; Failure rate; generalized pareto distribution; SRAM,Gaussian distribution; Gaussian noise (electronic); Pareto principle; Quadratic programming; Static random access storage; Cell/B.E; Cross entropy; Cross entropy minimization; Entropy minimization; Failure rate; Gaussian sampling method; Generalized Pareto Distributions; Non-Gaussian; SRAM Cell; Yield analysis; Failure rate
"Learning-based, fine-grain power modeling of system-level hardware IPs",2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042922045&doi=10.1145%2f3177865&partnerID=40&md5=4005801a46cd4096de2ab69b797391a0,"Accurate power and performance models are needed to enable rapid, early system-level analysis and optimization. There is, however, a lack of fast yet fine-grain power models of hardware components at such high levels of abstraction. In this article, we present novel learning-based approaches for extending fast functional simulation models of accelerators and other hardware intellectual property components (IPs) with accurate cycle-, block-, and invocation-level power estimates. Our proposed power modeling approach is based on annotating functional hardware descriptions with capabilities that, depending on observability, allow capturing data-dependent resource, block, or input and output (I/O) activity without a significant loss in simulation speed. We further leverage advanced machine learning techniques to synthesize abstract power models using novel decomposition techniques that reduce model complexities and increase estimation accuracy. Results of applying our approach to various industrial-strength design examples show that our power models can predict cycle-, basic block-, and invocation-level power consumption to within 10%, 9%, and 3% of a commercial gate-level power estimation tool, respectively, all while running at several order of magnitude faster speeds of 1-10Mcycles/sec. Model training and synthesis takes less than 34 minutes in all cases, including up to 30 minutes for training data and trace generation using gate-level simulations. © 2018 ACM.",Hardware accelerator; High-level synthesis; Machine learning; Power estimation; Power modeling; System-level design; Virtual platform,Abstracting; Artificial intelligence; Embedded systems; High level synthesis; Learning systems; Hardware accelerators; Power estimations; Power model; System level design; Virtual platform; Hardware
Direction-constrained rectangle escape routing,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042885158&doi=10.1145%2f3178047&partnerID=40&md5=26bf519b4d8ecd61734de07fb456383d,"Given a set of buses with available escape directions inside a chip, a two-phase algorithm is proposed to assign one feasible escape direction onto any bus such that the number of used layers is minimized and to allocate the pin rectangle and the projection rectangle of any escape bus onto the minimized layers in directionconstrained rectangle escape routing. In our proposed algorithm, based on the concept of two-dimensional maximum density inside a chip, the escape directions of the buses can be first assigned to minimize the number of the used layers by iteratively eliminating unnecessary escape directions for any bus inside a chip. Furthermore, based on the construction of the represented intervals and the assignment constraints for the escape buses, a modified left-edge algorithm can be used to allocate all the escape buses onto the minimized layers. Compared with Ma's integer linear program (ILP)-based algorithm [10] using lp-solve and Gurobi in rectangle escape routing, the experimental results show that our proposed algorithm obtains the same results but reduces CPU time by 94.2% and 35.7% when using lp-solve and Gurobi for 16 tested examples with no direction constraint on average, respectively. Compared with the modified algorithm from Ma's ILP-based algorithm [10] using lp-solve and Gurobi in direction-constrained rectangle escape routing, the experimental results show that our proposed algorithm obtains the same results but reduces CPU time by 94.3% and 37.7% when using lp-solve and Gurobi for 16 tested examples with direction constraints on average, respectively. Besides that, compared with Yan's iterative algorithm, the experimental results show that our proposed algorithm increases CPU time by 1.0% to reduce the number of used layers 11.1% for 16 tested examples on average.",,Buses; Geometry; Integer programming; Software testing; CPU time; Escape routing; Integer linear programs; Iterative algorithm; Maximum density; Modified algorithms; Two phase algorithm; Iterative methods
Multicast testing of interposer-based 2.5D ICs: Test-architecture design and test scheduling,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045385859&doi=10.1145%2f3177879&partnerID=40&md5=a95a6edf03fafe75ecc76b42a96bb051,"Interposer-based 2.5D integrated circuits (ICs) are seen today as a precursor to 3D ICs based on throughsilicon vias (TSVs). All the dies in a 2.5D IC must be adequately tested for product qualification. However, due to the limited number of package pins, it is a major challenge to test 2.5D ICs using conventional methods. Moreover, due to higher integration levels, test-application time and test power consumption for 2.5D ICs are also increased compared to their 2D counterparts. Therefore, it is imperative to take these issues into account during 2.5D IC testing. In this article, we present an efficient multicast test architecture for targeting defects in dies, in which multiple dies can be tested simultaneously to reduce the test-application time under constraints on test power and fault coverage. We also propose a test scheduling and optimization technique that can be utilized with the multicast test architecture. By considering the trade-off between test-application time, test-power budget, and test quality, the proposed technique provides test schedules with minimum test-application time under constraints on power consumption and fault coverage. Compared to previous work, the proposed technique can reduce test-application time by up to 53.4% for benchmark designs while achieving higher fault coverage. Since the loss in fault coverage due to multicast testing is extremely small, we can use top-off patterns to achieve full fault coverage for the dies at negligible additional cost. © 2018 ACM.",2.5D IC; Multicast; Silicon interposer; Test scheduling,Benchmarking; Budget control; Dies; Economic and social effects; Electric power utilization; Integrated circuit design; Integrated circuits; Multicasting; Scheduling; Testing; Timing circuits; 2.5D-IC; Conventional methods; Integrated circuits (ICs); Optimization techniques; Product qualification; Silicon interposers; Test application time; Test scheduling; Three dimensional integrated circuits
Selection of critical paths for reliable frequency scaling under BTI-aging considering workload uncertainty and process variations effects,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042909799&doi=10.1145%2f3177864&partnerID=40&md5=a97eb6ca6ea1a20a80dc7d6d96b67d1f,"Conventional clock guardbanding to assure a circuit’s reliable operation under device aging due to NBTI/PBTI and process variations introduce significant performance loss in modern nanometer circuits. Dynamic Frequency Scaling (DFS) is a more efficient technique that allows us to adjust the system clock frequency according to the process condition and aging deterioration of the circuit. At the design phase, the DFS technique requires the identification of the logic paths to be monitored to introduce the required circuitry to monitor their delay. However, critical path identification is a complex problem due to three major challenges: (1) The critical paths of the circuit depend on the stress duty cycle of the devices, which are unknown in advance at design phase; (2) the critical paths of the circuit depend on the process parameters variations, whose impact on delay depend on the spatial correlation due to proximity at the circuit layout; and (3) the critical paths reordering probability may change over time due to aging. This article presents a methodology for efficient selection of the critical paths to be monitored under a DFS framework, addressing the aforementioned challenges. Experimental results on ISCAS 85/89 benchmark circuits show the feasibility of the proposed approach to select a restricted path set while providing reliable aging monitoring. © 2018 ACM.",Aging of circuits; Dynamic Frequency Scaling; Failure prediction; Statistical timing analysis; Systems,Clocks; Computer systems; Dynamic frequency scaling; Electric network analysis; Integrated circuit testing; Failure prediction; Nanometer circuits; Process condition; Process parameters; Process Variation; Reliable operation; Spatial correlations; Statistical timing analysis; Delay circuits
Express read in MLC phase change memories,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042906961&doi=10.1145%2f3177876&partnerID=40&md5=be1fe0efc74e2271b98cef4d72c92a14,"In the era of big data, the capability of computer systems must be enhanced to support 2.5 quintillion byte/day data delivery. Among the components of a computer system, main memory has a great impact on overall system performance. DRAM technology has been used over the past four decades to build main memories. However, the scalability of DRAM technology has faced serious challenges. To keep pace with the ever-increasing demand for larger main memory, some new alternative technologies have been introduced. Phase change memory (PCM) is considered as one of such technologies for substituting DRAM. PCM offers some noteworthy properties such as low static power consumption, nonvolatility, and capability of storing more than one bit per cell (multilevel cell, or MLC). However, the short lifetime and long access latency of PCM (specifically MLC PCM) require feasible and efficient solutions. In this article, based on the observation that applications access a significant number of read-friendly data blocks, we propose Express Read to prevent the MLC PCM read circuit to spend unnecessary time sensing the cells of a memory block. A read-friendly data block (RFDB) is composed of only “11” and “00” bit pairs, and thus upon sensing the most significant bit of a cell, the read operation can be early terminated to reduce the MLC read time and power consumption. Moreover, we increase the number of RFDBs using two simple techniques to better exploit the benefits of Express Read. Results obtained from full-system simulation near 6% performance improvement and 21% energy gain, on average, over the baseline system. © 2018 ACM.",Lifetime; Nonvolatile cache; NVM; STT-RAM; Uniformity; Wear leveling,Big data; Cells; Cytology; Dynamic random access storage; Electric power utilization; Flash memory; Random access storage; Lifetime; Non-volatile; Stt rams; Uniformity; Wear leveling; Phase change memory
An efficient false path-Aware heuristic critical path selection method with high coverage of the process variation space,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042856888&doi=10.1145%2f3177866&partnerID=40&md5=6fce38f3d8d3d5f70c41295ae2148374,"In this article, we present a critical path selection method that efficiently finds true (sensitizable) critical paths of a circuit in the presence of process variations. The method, which is based on the viability analysis, tries to select the least number of true critical paths that cover all of circuit critical gates. Critical gates are those that make a path critical with a probability higher than a predefined threshold value. Selecting fewer critical paths leads to less computation time for the algorithm and shorter test time of fabricated chips. For this purpose, an efficient Statistical Static Timing Analysis- (SSTA) based technique is suggested. This technique tries to find circuit-critical gates whose process parameter variations cover a major part of the process space. Improving the process space coverage using fewer paths is achieved by considering both spatial (proximity of gates) and structural (having common gates) correlations in the analysis of choosing the critical paths. In the selection process, paths with low similarities in their characteristics are preferred. In addition, only true paths whose delays affect the maximum delay of the circuit are included. The selected paths can be used in the test process of the fabricated chips to determine if the chip meets its timing requirements. Also, amodified viability analysis that incorporates statistical computations is used in the SSTA. The efficacy of the proposed method is evaluated by comparing its results for combinational and sequential ISCAS benchmarks with those obtained by exhaustive search. Results indicate although, on average, only 4.38% of all the critical paths found by the exhaustive search are selected by the proposed method, the maximum probability of criticality for the paths that are not considered in our method is, on average, less than 4%. © 2018 ACM.",Critical path selection; Path sensitization; Process space coverage; Process variation; Viability,Delay circuits; Testing; Timing devices; Critical Paths; Path sensitization; Process Variation; Space coverage; Viability; Heuristic methods
Domino cache: An energy-efficient data cache for modern applications,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041725644&doi=10.1145%2f3174848&partnerID=40&md5=cf9688a01ab8716459c2d5aef7ba0f1e,"The energy consumption for processing modern workloads is challenging in data centers. Due to the large datasets of cloudworkloads, the miss rate of the L1 data cache is high, andwith respect to the energy efficiency concerns, suchmisses are costly formemory instructions because lower levels ofmemory hierarchy consume more energy per access than the L1. Moreover, large last-level caches are not performance effective, in contrast to traditional scientific workloads. The aim of this article is to propose a large L1 data cache, called Domino, to reduce the number of accesses to lower levels in order to improve the energy efficiency. In designing Domino, we focus on two components that use the on-chip area and are not energy efficient, which makes them good candidates to use their area for enlarging the L1 data cache. Domino is a highly associative cache that extends the conventional cache by borrowing the prefetcher and last-level-cache storage budget and using it as additional ways for data cache. In Domino, the additional ways are separated from the conventional cache ways; hence, the critical path of the first access is not altered. On a miss in the conventional part, it searches the added ways in a mix of parallel-sequential fashion to compromise the latency and energy consumption. Results on the Cloudsuite benchmark suite show that read and write misses are reduced by 30%, along with a 28% reduction in snoop messages. The overall energy consumption per access is then reduced by 20% on average (maximum 38%) as a result of filtering accesses to the lower levels. © 2018 ACM.",Cache; Cloud workloads; Computer architecture; Energy; Prefetching,Associative storage; Budget control; Buffer storage; Cache memory; Computer architecture; Digital storage; Energy utilization; Green computing; Associative cache; Cache; Energy; Energy efficient; Last-level caches; Modern applications; Prefetching; Scientific workloads; Energy efficiency
ReSC: An RFID-enabled solution for defending IoT supply chain,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041721547&doi=10.1145%2f3174850&partnerID=40&md5=7f5c9c528c70f097e5b14e33d65ea964,"The Internet of Things (IoT), an emerging global network of uniquely identifiable embedded computing devices within the existing Internet infrastructure, is transforming how we live and work by increasing the connectedness of people and things on a scale that was once unimaginable. In addition to facilitated information and service exchange between connected objects, enhanced computing power and analytic capabilities of individual objects, and increased interaction between objects and their environments, the IoT also raises new security and privacy challenges. Hardware trust across the IoT supply chain is the foundation of IoT security and privacy. Two major supply chain issues-disappearance/theft of authentic IoT devices and appearance of inauthentic ones-have to be addressed to secure the IoT supply chain and lay the foundation for further security and privacy-defensivemeasures. Comprehensive solutions that enable IoT device authentication and traceability across the entire supply chain (i.e., during distribution and after being provisioned) need to be established. Existing hardware, software, and network protection methods, however, do not address IoT supply chain issues. To mitigate this shortcoming, we propose an RFID-enabled solution called ReSC that aims at defending the IoT supply chain. By incorporating three techniques-one-to-one mapping between RFID tag identity and control chip identity; unique tag trace, which records tag provenance and history information; and neighborhood attestation of IoT devices-ReSC is resistant to split attacks (i.e., separating tag from product, swapping tags), counterfeit injection, product theft throughout the entire supply chain, device recycling, and illegal network service access (e.g., Internet, cable TV, online games, remote firmware updates). Simulations, theoretical analysis, and experimental results based on a printed circuit board (PCB) prototype demonstrate the effectiveness of ReSC. Finally, we evaluate the security of our proposed scheme against various attacks. © 2018 ACM.",Authentication; Internet of things (IoT); Radio frequency identification (RFID); Supply chain security; Traceability,Authentication; Computer privacy; Crime; Electronic document exchange; Electronic Waste; Firmware; Hardware; Printed circuit boards; Radio frequency identification (RFID); Rhenium compounds; Supply chains; Device authentications; Embedded computing devices; Internet infrastructure; Internet of thing (IOT); Internet of Things (IOT); Printed circuit boards (PCB); Supply chain security; Traceability; Internet of things
Providing SLO compliance on NVMe SSDs through parallelism reservation,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041751602&doi=10.1145%2f3174867&partnerID=40&md5=0143ef910c759c1d767275097209c844,"Non-Volatile Memory Express (NVMe) is a specification for next-generation solid-state disks (SSDs). Benefited from the massive internal parallelism and the high-speed PCIe bus, NVMe SSDs achieve extremely high data transfer rates, and they are an ideal solution of shared storage in virtualization environments. Providing virtual machines with Service Level Objective (SLO) compliance on NVMe SSDs is a challenging task, because garbage collection activities inside of NVMe SSDs globally affect the I/O performance of all virtual machines. In this study, we introduce a novel approach, called parallelism reservation, which is inspired by the rich internal parallelism of NVMe SSDs. The degree of parallelism stands for how many flash chips are concurrently active. Our basic idea is to reserve sufficient degrees of parallelism for read, write, and garbage collection operations, making sure that an NVMe SSD delivers stable read and write throughput and reclaims free space at a constant rate. The stable read and write throughput are proportionally distributed among virtual machines for SLO compliance. Our experimental results show that our parallelism reservation approach delivered satisfiable throughput and highly predictable response to virtual machines. © 2018 ACM.",Non-Volatile Memory Express; Service Level Objective; Solid State Disks,Data storage equipment; Data transfer; Data transfer rates; Network security; Nonvolatile storage; Refuse collection; Throughput; Virtual machine; Degree of parallelism; Garbage collection; High data transfer rates; Ideal solutions; Non-volatile memory; Service level objective; Solid state disks; Write throughputs; Digital storage
Repair of FPGA-based real-time systemswith variable slacks,2018,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041450618&doi=10.1145%2f3144533&partnerID=40&md5=1825c24fa8f8a55129076cccbb762ee9,"Field-programmable gate arrays (FPGAs) based on SRAM cells are an attractive alternative for real-time system designers, as they offer high density, low cost, and high performance. The use of SRAM cells in the FPGA's configuration memory, while enabling these desirable characteristics, also creates a reliability hazard as RAM cells are susceptible to single-event upsets (SEUs). The usual approach is the use of double or triple redundancy allied with a correction mechanism, such as periodic scrubbing. Although scrubbing is an effective technique to remove SEU-induced errors, the repair of real-time systems presents specific challenges, such as avoiding failures by missing real-time deadlines. In this article, a novel approach is proposed to use a deadline-aware scrubbing scheme with negligible area costs that dynamically chooses the scrubbing starting position. Such a scheme allows us to avoid missing real-time deadlines while maximizing the repair probability given a bounded repair time. Our approach reduces the failure rate, considering the probability of missing deadlines due to faults, by 33.39% on average, with an average area cost of 1.23%. © 2018 ACM.",Fault diagnosis; Fault tolerance; Field-programmable gate arrays (FPGAs); Real time; Scrubbing,Computer control systems; Costs; Failure analysis; Fault detection; Fault tolerance; Field programmable gate arrays (FPGA); Interactive computer systems; Logic gates; Radiation hardening; Random access storage; Repair; Static random access storage; Configuration memory; Correction mechanism; Deadline-aware; Failure rate; Real time; Repair time; Scrubbing; Single event upsets; Real time systems
Runtime slack creation for processor performance variability using system scenarios,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041466273&doi=10.1145%2f3152158&partnerID=40&md5=208b0339a353d01ecc59ae4620596426,"Modern microprocessors contain a variety of mechanisms used to mitigate errors in the logic and memory, referred to as Reliability, Availability, and Serviceability (RAS) techniques. Many of these techniques, such as component disabling, come at a performance cost. With the aggressive downscaling of device dimensions, it is reasonable to expect that chip-wide error rates will intensify in the future and perhaps vary throughout system lifetime. As a result, it is important to reclaim the temporal RAS overheads in a systematic way and enable dependable performance. The current article presents a closed-loop control scheme that actuates processor's frequency based on detected timing interference to ensure performance dependability. The concepts of slack and deadline vulnerability factor are introduced to support the formulation of a discrete time control problem. Default application timing is derived using the system scenario methodology, the applicability of which is demonstrated through simulations. Additionally, the proposed concept is demonstrated on a real platform and application: a Proportional-Integral-Differential controller, implemented within the application, actuates the Dynamic Voltage and Frequency Scaling (DVFS) framework of the Linux kernel to effectively reclaim temporal overheads injected at runtime. The current article discusses the responsiveness and energy efficiency of the proposed performance dependability scheme. Finally, additional formulation is introduced to predict the upper bound of timing interference that can be absorbed by actuating the DVFS of any processor and is also validated on a representative reduction to practice. © 2017 ACM.",Availability and serviceability (RAS); Dependable performance; Dynamic voltage and frequency scaling (DVFS); Proportional integral differential (PID) control; Reliability,Computer operating systems; Energy efficiency; Proportional control systems; Reliability; Two term control systems; Voltage scaling; Availability and serviceability; Dependable performance; Discrete-time control; Dynamic voltage and frequency scaling; Modern microprocessor; Proportional integral differential control; Proportional integral differential controllers; Vulnerability factors; Dynamic frequency scaling
Static mapping of applications on heterogeneous multi-core platforms combining logic-based benders decomposition with integer linear programming,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041833148&doi=10.1145%2f3133219&partnerID=40&md5=fdaae9f327c1041a98d43821ed6d88b4,"The proper mapping of an application on a multi-core platform and the scheduling of its tasks are key elements to achieve the maximum performance. In this article, a novel hybrid approach based on integrating the Logic-Based Benders Decomposition (LBBD) principle with a pure Integer Linear Programming (ILP) model is introduced for mapping applications described by Directed Acyclic Graphs (DAGs) on platforms consisting of heterogeneous cores. The LBBD approach combines two optimization techniques with complementary strengths, namely ILP and Constraint Programming (CP), and is employed as a cut generation scheme. The generated constraints are utilized by the ILP model to cut possible assignment combinations aiming at improving the solution or proving the optimality of the best-found one. The introduced approach was applied both on synthetic DAGs and on DAGs derived from real applications. Through the proposed approach, many problems were optimally solved that could not be solved by any of the above methods (ILP, LBBD) alone within a time limit of 2 hours, while the overall solution time was also significantly decreased. Specifically, the hybrid method exhibited speedups equal to 4.2 × for the synthetic instances and 10 × for the real-application DAGs over the LBBD approach and two orders of magnitude over the ILP model.",Benders decomposition; Constraint programming; Integer linear programming; Multi-core embedded platforms; Task scheduling,Computer circuits; Computer programming; Constraint theory; Directed graphs; Inductive logic programming (ILP); Mapping; Multicore programming; Stochastic programming; Benders decomposition; Constraint programming; Embedded platforms; Integer Linear Programming; Task-scheduling; Integer programming
Hardware-enabled pharmaceutical supply chain security,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041860453&doi=10.1145%2f3144532&partnerID=40&md5=c7fd27902d1934863de7cfa607d87db4,"The pharmaceutical supply chain is the pathway through which prescription and over-the-counter (OTC) drugs are delivered from manufacturing sites to patients. Technological innovations, price fluctuations of raw materials, as well as tax, regulatory, and market demands are driving change and making the pharmaceutical supply chain more complex. Traditional supply chain management methods struggle to protect the pharmaceutical supply chain, maintain its integrity, enhance customer confidence, and aid regulators in tracking medicines. To develop effective measures that secure the pharmaceutical supply chain, it is important that the community is aware of the state-of-the-art capabilities available to the supply chain owners and participants. In this article, we will be presenting a survey of existing hardware-enabled pharmaceutical supply chain security schemes and their limitations. We also highlight the current challenges and point out future research directions. This survey should be of interest to government agencies, pharmaceutical companies, hospitals and pharmacies, and all others involved in the provenance and authenticity of medicines and the integrity of the pharmaceutical supply chain.",Authentication; Pharmaceutical supply chain; Privacy; Security; Traceability,Authentication; Data privacy; Hardware; Medicine; Supply chain management; Surveys; Future research directions; Manufacturing sites; Over-the-counter drugs; Pharmaceutical company; Pharmaceutical supply chains; Security; Technological innovation; Traceability; Hardware security
A disturbance-free built-in self-test and diagnosis technique for DC-DC converters,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041432020&doi=10.1145%2f3152157&partnerID=40&md5=83ac82fb146069f69550fab29af9bcc1,"Complex electronic systems include multiple power domains and drastically varying dynamic power consumption patterns, requiring the use of multiple power conversion and regulation units. High-frequency switching converters have been gaining prominence in the DC-DC converter market due to their high efficiency and smaller form factor. Unfortunately, they are also subject to higher process variations, and faster in-field degradation, jeopardizing stable operation of the power supply. This article presents a technique to track changes in the dynamic loop characteristics of DC-DC converters without disturbing the normal mode of operation using a white noise-based excitation and correlation. Using multiple points for injection and analysis, we show that the degraded part can be diagnosed to take remedial action. White noise excitation is generated via a pseudo-random disturbance at reference, load current, and pulse-width modulation (PWM) nodes of the converter with the test signal energy being spread over a wide bandwidth, without significantly affecting the converter noise and ripple floor. The impulse response is extracted by correlating the random input sequence with the disturbed output generated. Test signal analysis is achieved by correlating the pseudo-random input sequence with the output response and thereby accumulating the desired behavior over time and pulling it above the noise floor of the measurement set-up. An off-the-shelf power converter, LM27402, is used as the device-under-test (DUT) for experimental verification. Experimental results show that the proposed technique can estimate converter natural frequency and quality factor (Q-factor) within ± 2.5% and ± 0.7% error margin respectively, over changes in load inductance and capacitance. For the diagnosis purpose, a measure of inductor's DC resistance (DCR) value, which is the inductor's series resistance and indicative of the degradation in inductor's Q-factor, is estimated within less than ± 1.6% error margin. © 2017 ACM.",Built-In Self-Test; DC DC buck converter; Diagnosis method; PRBS test method; Stability,Built-in self test; Convergence of numerical methods; Design for testability; Electric inductors; Electric inverters; Electric resistance; Floors; Impulse response; Pulse width modulation; Q factor measurement; Voltage control; White noise; Complex electronic systems; DC-DC buck converter; Diagnosis methods; Dynamic power consumption; Experimental verification; High-frequency switching; Test method; White noise excitation; DC-DC converters
Emerging NVM: A survey on architectural integration and research challenges,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041847389&doi=10.1145%2f3131848&partnerID=40&md5=63e8375263285d547cfa2452cbe2f923,"There has been a surge of interest in Non-Volatile Memory (NVM) in recent years. With many advantages, such as density and power consumption, NVM is carving out a place in the memory hierarchy and may eventually change our view of computer architecture. Many NVMs have emerged, such as Magnetoresistive random access memory (MRAM), Phase Change random access memory (PCM), Resistive random access memory (ReRAM), and Ferroelectric random access memory (FeRAM), each with its own peculiar properties and specific challenges. The scientific community has carried out a substantial amount of work on integrating those technologies in the memory hierarchy. As many companies are announcing the imminent mass production of NVMs, we think that it is time to have a step back and discuss the body of literature related to NVM integration. This article surveys state-of-the-art work on integrating NVM into the memory hierarchy. Specially, we introduce the four types of NVM, namely, MRAM, PCM, ReRAM, and FeRAM, and investigate different ways of integrating them into the memory hierarchy from the horizontal or vertical perspectives. Here, horizontal integration means that the new memory is placed at the same level as an existing one, while vertical integration means that the new memory is interleaved between two existing levels. In addition, we describe challenges and opportunities with each NVM technique.",FeRAM; Main memory; MRAM; Non-volatile memory; PCM; ReRAM; Storage,Computer architecture; Data storage equipment; Digital storage; Energy storage; Green computing; Integration; Magnetic recording; Magnetic storage; Memory architecture; MRAM devices; Nonvolatile storage; Phase change memory; Pulse code modulation; RRAM; Surveys; Architectural integrations; FeRAM; Ferroelectric random access memory; Main memory; MRAM; Non-volatile memory; Phase change random access memory; Resistive Random Access Memory (ReRAM); Random access storage
Flexible and tradeoff-aware constraint-based design space exploration for streaming applications on heterogeneous platforms,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041844360&doi=10.1145%2f3133210&partnerID=40&md5=5b8382a0fc2cf184e149e0c2ce54709e,"Due to its complexity, the problem of mapping and scheduling streaming applications on heterogeneous MPSoCs under real-time and performance constraints has traditionally been tackled by incomplete heuristic algorithms. In recent years, approaches based on Constraint Programming (CP) have shown promising results as complete methods for finding optimal mappings, in particular concerning throughput. However, so far none of the available CP approaches consider the tradeoff between throughput and buffer requirements or throughput and power consumption. This article integrates tradeoff awareness into the CP model and introduces a two-step solving approach that utilizes the advantages of heuristics, while still keeping the completeness property of CP.With a number of experiments considering several streaming applications and different platform models, the article illustrates not only the efficiency of the presented model but also its suitability for solving different problems with various combinations of performance constraints.",Constraint programming; Correct-by-construction; Design space exploration; Performance analysis,Computer programming; Constraint theory; Heuristic algorithms; Integrated circuit design; Mapping; System-on-chip; Throughput; Constraint programming; Constraint-based design; Correct-by-construction; Design space exploration; Heterogeneous platforms; Performance analysis; Performance constraints; Streaming applications; Space platforms
C-mine: Data mining of logic common cases for improved timing error resilience with energy efficiency,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041441064&doi=10.1145%2f3144534&partnerID=40&md5=fdb00b02252aac4c5f29f5bc63641a13,"The better-than-worst-case (BTW) design methodology can achieve higher circuit energy efficiency, performance, or reliability by allowing timing errors for rare cases and rectifying them with error correction mechanisms. Therefore, the performance of BTW design heavily depends on the correctness of common cases, which are frequent input patterns in a workload. However, most existing methods do not provide sufficiently scalable solutions and also overlook the whole picture of the design. Thus, we propose a new technique, common-case mining method (C-Mine), which combines two scalable techniques, data mining and Boolean satisfiability (SAT) solving, to overcome these limitations. Data mining can efficiently extract patterns from an enormous dataset, and SAT solving is famous for its scalable verification. In this article, we present two versions of C-Mine, C-Mine-DCT and C-Mine-APR, which aim at faster runtime and better energy saving, respectively. The experimental results show that, compared to a recent publication, C-Mine-DCT can achieve compatible performance with an additional 8% energy savings and 54x speedup for bigger benchmarks on average. Furthermore, C-Mine-APR can achieve up to 13% more energy saving than C-Mine-DCT while confronting designs with more common cases. © 2017 ACM.",Common cases; Data mining; Energy efficiency; Resynthesis; SAT solving; Scalability; Timing error resilience,Benchmarking; Energy conservation; Energy efficiency; Error correction; Errors; Scalability; Timing circuits; Boolean satisfiability; Correction mechanism; Design Methodology; Input patterns; Resynthesis; SAT-solving; Scalable solution; Timing errors; Data mining
Multi-objective 3D floorplanning with integrated voltage assignment,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041433202&doi=10.1145%2f3149817&partnerID=40&md5=08c6570f2586286d5521467fc2bdf598,"Voltage assignment is a well-known technique for circuit design, which has been applied successfully to reduce power consumption in classical 2D integrated circuits (ICs). Its usage in the context of 3D ICs has not been fully explored yet although reducing power in 3D designs is of crucial importance, for example, to tackle the ever-present challenge of thermal management. In this article, we investigate the effective and efficient partitioning of 3D designs into multiple voltage domains during the floorplanning step of physical design. In particular, we introduce, implement, and evaluate novel algorithms for effective integration of voltage assignment into the inner floorplanning loops. Our algorithms are compatible not only with the traditional objectives of 2D floorplanning but also with the additional objectives and constraints of 3D designs, including the planning of through-silicon vias (TSVs) and the thermal management of stacked dies. We test our 3D floorplanner extensively on the GSRC benchmarks as well as on an augmented version of the IBM-HB+ benchmarks. The 3D floorplans are shown to achieve effective trade-offs for power and delays throughout different configurations-our results surpass naïve low-power and high-performance voltage assignment by 17% and 10%, on average. Finally, we release our 3D floorplanning framework as open-source code. © 2017 ACM.",Power-performance optimization; Voltage assignment,Economic and social effects; Electronics packaging; Integrated circuit design; Integrated circuit manufacture; Open source software; Open systems; Optimization; Temperature control; Thermal variables control; Integrated circuits (ICs); Integrated voltage; Multiple voltage; Open-source code; Physical design; Power performance; Through silicon vias; Voltage assignment; Three dimensional integrated circuits
Architecture and compiler support for GPUs using energy-efficient affine register files,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041445365&doi=10.1145%2f3133218&partnerID=40&md5=215d1439d2fdce55e5ae57aa41b75158,"A modern GPU can simultaneously process thousands of hardware threads. These threads are grouped into fixed-size SIMD batches executing the same instruction on vectors of data in a lockstep to achieve high throughput and performance. The register files are huge due to each SIMD group accessing a dedicated set of vector registers for fast context switching, and consequently the power consumption of register files has become an important issue. One proposed solution is to replace some of the vector registers by scalar registers, as different threads in a same SIMD group operate on scalar values and so the redundant computations and accesses of these scalar values can be eliminated. However, it has been observed that a significant number of registers containing affine vectors v such that v[i] = b + i × s can be represented by base b and stride s. Therefore, this article proposes an affine register file design for GPUs that is energy efficient due to it reducing the redundant executions of both the uniform and affine vectors. This design uses a pair of registers to store the base and stride of each affine vector and provides specific affine ALUs to execute affine instructions. A method of compiler analysis has been developed to detect scalars and affine vectors and annotate instructions for facilitating their corresponding scalar and affine computations. Furthermore, a priority-based register allocation scheme has been implemented to assign scalars and affine vectors to appropriate scalar and affine register files. Experimental results show that this design was able to dispatch 43.56% of the computations to scalar and affine ALUs when using eight scalar and four affine registers per warp. This resulted in the current design also reducing the energy consumption of the register files and ALUs to 21.86% and 26.54%, respectively, and it reduced the overall energy consumption of the GPU by an average of 5.18%. © 2017 ACM.",Energy efficient; GPU; Register allocation; Register file organization,Computer hardware; Energy utilization; Graphics processing unit; Logic circuits; Program compilers; Program processors; Vectors; Compiler analysis; Context switching; Energy efficient; Hardware threads; High throughput; Redundant computation; Register allocation; Register file organizations; Energy efficiency
Recovering from biased distribution of faulty cells in memory by reorganizing replacement regions through universal hashing,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031760433&doi=10.1145%2f3131241&partnerID=40&md5=b9ed4ba9a0c24864ccb48389605520d4,"Recently, scaling down dynamic random access memory (DRAM) has become more of a challenge, with more faults than before and a significant degradation in yield. To improve the yield in DRAM, a redundancy repair technique with intra-subarray replacement has been extensively employed to replace faulty elements (i.e., rows or columns with defective cells) with spare elements in each subarray. Unfortunately, such technique cannot efficiently handle a biased distribution of faulty cells because each subarray has a fixed number of spare elements. In this article, we propose a novel redundancy repair technique that uses a hashing method to solve this problem. Our hashing technique reorganizes replacement regions by changing the way in which their replacement information is referred, thus making faulty cells become evenly distributed to the regions. We also propose a fast repair algorithm to find the best hash function among all possible candidates. Even if our approach requires little hardware overhead, it significantly improves the yield when compared with conventional redundancy techniques. In particular, the results of our experiment show that our technique saves spare elements by about 57% and 55% for a yield of 99% at BER 1e-6 and 5e-7, respectively. © 2017 ACM.",DRAM fault recovery; DRAM yield; Fault recovery algorithm; Universal hashing,Cells; Cytology; Hash functions; Random access storage; Redundancy; Repair; Dynamic random access memory; Fault recovery; Hardware overheads; Hashing techniques; Redundancy repair; Redundancy techniques; Repair algorithms; Universal hashing; Dynamic random access storage
An effective layout decomposition method for dsa with multiple patterning in contact-hole generation,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033232192&doi=10.1145%2f3131847&partnerID=40&md5=6d44689d93fa0fe88971d6d460a115e0,"Directed self-assembly (DSA) complemented with multiple patterning (MP) is an attractive next generation lithography (NGL) technique for contact-hole generation. Nevertheless, a high-quality DSA-aware layout decomposer is required to enable the technology. In this article, we introduce an efficient method which incorporates a set packing for generating DSA template candidates and a local search method. Besides, a multi-start strategy is integrated into the framework to prevent the local minima. Our framework encourages the reuse of existing coloring solvers. Hence, the development cost can significantly be reduced. In addition, for DSA multiple patterning where the number of masks is larger than two, we present an efficient iterative partition based method. Experimental results showthat comparedwith the state-of-the-artwork, our methods can achieve roughly 100× speedup for double patterning, and 78.8% conflict reduction with 5× speedup for triple patterning on the dense graphs.",Design for manufacturability; Directed self-assembly; Iterative partition; Multiple patterning; Multiple start points; Set packing,Design for manufacturability; Hole mobility; Machine design; Self assembly; Directed self-assembly; Double patterning; Layout decomposition; Local search method; Multiple patterning; Next generation lithography; Set packing; Start point; Iterative methods
Exploiting chip idleness for minimizing garbage collection-induced chip access conflict on SSDs,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031712480&doi=10.1145%2f3131850&partnerID=40&md5=2579b41c4d15d279a4c9f90f42db3826,"Solid state drives (SSDs) are normally constructed with a number of parallel-accessible flash chips, where host I/O requests are processed in parallel. In addition, there are many internal activities in SSDs, such as garbage collection and wear leveling induced read, write, and erase operations, to solve the issues of inability of in-place updates and limited lifetime. When internal activities are triggered on a chip, the chip will be blocked. Our preliminary studies on several workloads show that when internal activities are frequently triggered, the host I/O performance will be significantly impacted because of the access conflict between them. In this work, in order to improve the access conflict induced performance degradation, a novel access conflict minimization scheme is proposed. The basic idea of the scheme is motivated by an interesting observation in SSDs: several chips are idle when other chips are busy with internal activities and host I/O requests. Based on this observation, we propose to schedule internal activities induced operations for minimized access conflict by exploiting the idleness of the multiple chips of SSDs. This approach is realized by two steps: First, read internal activities accessed data to the controller; second, by exploiting the idle chips during internal activities, write internal activities accessed data back to these idle chips. With this scheme, the internal activities can be processed with minimized access conflict to the host requests. Simulation results show that the proposed approach significantly reduces the access conflict, and in turn leads to a significant performance improvement of SSDs. © 2017 ACM.",Access conflicts; Chip idleness; Garbage collection; Parallelism; SSD,Digital storage; Access conflicts; Chip idleness; Erase operation; Garbage collection; Internal activity; Parallelism; Performance degradation; Solid state drives; Refuse collection
Revisiting routability-driven placement for analog and mixed-signal circuits,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031710870&doi=10.1145%2f3131849&partnerID=40&md5=a3dd8620b9b7a54825e8c8c6d0dd3b38,"The exponential increase in scale and complexity of very large-scale integrated circuits (VLSIs) poses a great challenge to current electronic design automation (EDA) techniques. As an essential step in the whole EDA layout synthesis, placement is attracting more and more attention, especially for analog and mixed-signal integrated circuits. Recently, experts in this field have observed a variety of analog-specific layout constraints to obtain high-performance placement solutions. These constraints include symmetry, alignment, boundary, preplace, abutment, range and maximum separation, and routability of the placement solutions. In this article, the effectiveness of slicing and nonslicing representation is investigated. Additionally, the technique of congestion-based virtual sizing is proposed. Experimental results show that the routability can be improved significantly by applying congestion-based virtual sizing. Results also showthat the slicing representation can improve the regularity of the placement solutions and hence improve the routability with higher efficiency compared to the nonslicing representation. © 2017 ACM.",Analog and mixed signal circuits; Congestion; Routing,Computer aided design; Electric signal systems; Electronic design automation; Integrated circuit layout; Timing circuits; VLSI circuits; Analog and mixed signal circuits; Analog and mixed-signal integrated circuit; Congestion; Exponential increase; Layout constraint; Routing; Slicing representation; Very large scale integrated circuit; Mixed signal integrated circuits
DYNASCORE: DYNAmic software COntroller to increase REsource utilization in mixed-critical systems,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031711561&doi=10.1145%2f3110222&partnerID=40&md5=55fe2bc7fd101691d45f4725dbcf22d0,"In real-time mixed-critical systems, Worst-Case Execution Time (WCET) analysis is required to guarantee that timing constraints are respected-at least for high-criticality tasks. However, the WCET is pessimistic compared to the real execution time, especially for multicore platforms. AsWCET computation considers the worst-case scenario, it means that whenever a high-criticality task accesses a shared resource in multicore platforms, it is considered that all cores use the same resource concurrently. This pessimism inWCET computation leads to a dramatic underutilization of the platform resources, or even failing to meet the timing constraints. In order to increase resource utilization while guaranteeing real-time guarantees for high-criticality tasks, previous works proposed a runtime control system to monitor and decide when the interferences from low-criticality tasks cannot be further tolerated. However, in the initial approaches, the points where the controller is executed were statically predefined. In this work, we propose a dynamic runtime control which adapts its observations to online temporal properties, further increasing the dynamism of the approach, and mitigating the unnecessary overhead implied by existing static approaches. Our dynamic adaptive approach allows one to control the ongoing execution of tasks based on runtime information, and further increases the gains in terms of resource utilization compared with static approaches. © 2017 ACM.",Dynamic management; Multicore systems; Real-time systems; Resources utilization; Runtime monitoring; Safety,Accident prevention; Controllers; Criticality (nuclear fission); Interactive computer systems; Multicore programming; Dynamic management; Multi-core platforms; Multi-core systems; Real time guarantees; Resource utilizations; Resources utilizations; Runtime Monitoring; Worst-case execution time analysis; Real time systems
VFI-Based Power Management to Enhance the Lifetime of High-Performance 3D NoCs,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029794885&doi=10.1145%2f3092843&partnerID=40&md5=d5498efa3e3e51673cf7d7077219948b,"The emergence of 3D network-on-chip (NoC) has revolutionized the design of high-performance and energyefficientmanycore chips. However, the anticipated performance gain can be compromised due to the degradation and failure of vertical links (VLs). The Through-Silicon-Via (TSV)-enabled VLs may fail due to workloadinduced stress; the failure of a VL can affect the neighboring VLs, thereby causing a cascade of failures and reducing the lifetime of the chip. To enhance the reliability of 3D NoC-enabled manycore chips, we propose to incorporate a voltage-frequency island (VFI)-based power management strategy that helps to reduce the energy consumption and hence, the workload-induced stress of the highly utilized VLs. The adopted power-management strategy relies on control decisions about the voltage/frequency (V/F) levels on VLs.We demonstrate that compared to the well-known spare TSV allocation and adaptive routing strategies, power management is more effective in enhancing the reliability of a 3D NoC. VFI-based power management improves the reliability of the 3D NoC by one order of magnitude compared to both adaptive routing and spare allocation while running popular SPLASH-2 and PARSEC benchmarks. The principal benefit of power management is that it is capable of reducing the operating temperature of the system, which in turn enhances the Mean-Time-To-Failure (MTTF) of the VLs and reliability of the overall 3D NoC. © 2017 ACM.",3D NoC; Adaptive routing; Manycore chips; Small-world networks; TSV reliability; VFI power management,Distributed computer systems; Electronics packaging; Energy management; Energy utilization; Integrated circuit manufacture; Outages; Power management; Power management (telecommunication); Reliability; Small-world networks; Three dimensional integrated circuits; Adaptive routing; Control decisions; Manycore chips; Mean time to failure; Operating temperature; Power management strategies; Through-Silicon-Via (TSV); Voltage-frequency islands; Network-on-chip
An adaptive Markov model for the timing analysis of probabilistic caches,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029794258&doi=10.1145%2f3123877&partnerID=40&md5=b127c3f804d839c451d3209360cdf4d6,"Accurate timing prediction for real-time embedded software execution is becoming a problem due to the increasing complexity of computer architecture, and the presence of mixed-criticality workloads. Probabilistic caches were proposed to set bounds to Worst Case Execution Time (WCET) estimates and help designers improve real-time embedded system resource use. Static Probabilistic Timing Analysis (SPTA) for probabilistic caches is nevertheless difficult to perform, because cache accesses depend on execution history, and the computational complexity of SPTA makes it intractable for calculation as the number of accesses increases. In this paper, we explore and improve SPTA for caches with evict-on-miss random replacement policy using a state space modeling technique. A nonhomogeneous Markov model is employed for single-path programs in discrete-time fnite state space representation. To make this Markov model tractable, we limit the number of states and use an adaptive method for state modification. Experiments show that compared to the state-of-the-art methodology, the proposed adaptive Markov chain approach provides better results at the occurrence probability of 10-15: in terms of accuracy, the state-of-the-art SPTA results are more conservative, by 11% more on average. In terms of computation time, our approach is not significantly diferent from the state-of-the-art SPTA.",Cache; Probabilistic; Real-time systems,Computer architecture; Embedded systems; Interactive computer systems; Markov processes; State space methods; Timing circuits; Cache; Markov chain approaches; Occurrence probability; Probabilistic; Real-time embedded software; Real-time embedded systems; State space representation; Worst-case execution time; Real time systems
Architectural supports to protect os kernels from code-injection attacks and their applications,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029789468&doi=10.1145%2f3110223&partnerID=40&md5=1279b81b97bd4baac62035c4449a4c1f,"The kernel code injection is a common behavior of kernel-compromising attacks where the attackers aim to gain their goals by manipulating an OS kernel. Several security mechanisms have been proposed to mitigate such threats, but they all suffer from non-negligible performance overhead. This article introduces a hardware reference monitor, called Kargos, which can detect the kernel code injection attacks with nearly zero performance cost. Kargos monitors the behaviors of an OS kernel from outside the CPU through the standard bus interconnect and debug interface available with most major microprocessors. By watching the execution traces and memory access events in the monitored target system, Kargos uncovers attempts to execute malicious code with the kernel privilege. On top of this, we also applied the architectural supports for Kargos to the detection of ROP attacks. KS-Stack is the hardware component that builds and maintains the shadow stacks using the existing supports to detect this ROP attacks. According to our experiments, Kargos detected all the kernel code injection attacks that we tested, yet just increasing the computational loads on the target CPU by less than 1% on average. The performance overhead of the KS-Stack was also less than 1%. © 2017 ACM.",Architectural support for security; Codeinjection attacks; Operating system security; Return-oriented programming; Shadow stack,Codes (symbols); Hardware; Architectural support; Code injection attacks; Operating system security; Return-oriented programming; Shadow stack; Network security
Optimal don’t care filling for minimizing peak toggles during at-speed stuck-at testing,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030148625&doi=10.1145%2f3084684&partnerID=40&md5=c050d20d27928a7fbf1362bc7b05b13f,"Due to the increase in manufacturing/environmental uncertainties in the nanometer regime, testing digital chips under different operating conditions becomes mandatory. Traditionally, stuck-at tests were applied at slow speed to detect structural defects and transition fault tests were applied at-speed to detect delay defects. Recently, it was shown that certain cell-internal defects can only be detected using at-speed stuck-at testing. Stuck-at test patterns are power hungry, thereby causing excessive voltage droop on the power grid, delaying the test response, and finally leading to false delay failures on the tester. This motivates the need for peak power minimization during at-speed stuck-at testing. In this article, we use input toggle minimization as a means to minimize a circuit’s power dissipation during at-speed stuck-at testing under the Combinational State Preservation scan (CSP-scan) Design-For-Testability (DFT) scheme. For circuits whose test sets are dominated by don’t cares, this article maps the problem of optimal X-filling for peak input toggle minimization to a variant of the interval coloring problem and proposes a Dynamic Programming (DP) algorithm (DP-fill) for the same along with a theoretical proof for its optimality. For circuits whose test sets are not dominated by don’t cares, we propose a max scatter Hamiltonian path algorithm, which ensures that the ordering is done such that the don’t cares are evenly distributed in the final ordering of test cubes, thereby leading to better input toggle savings than DP-fill. The proposed algorithms, when experimented on ITC99 benchmarks, produced peak power savings of up to 48% over the best-known algorithms in literature. We have also pruned the solutions thus obtained using Greedy and Simulated Annealing strategies with iterative 1-bit neighborhood to validate our idea of optimal input toggle minimization as an effective technique for minimizing peak power dissipation during at-speed stuck-at testing. © 2017 ACM.",At-speed stuck-at testing; Don’t care filling; Dynamic programming; Greedy pruning; Max scatter Hamiltonian path algorithm; Peak test power; Simulated annealing; Test cube ordering,Codes (symbols); Defects; Design for testability; Dynamic programming; Electric losses; Electric power transmission networks; Filling; Hamiltonians; Integrated circuit testing; Iterative methods; Simulated annealing; Speed; Testing; At-speed; Greedy pruning; Hamiltonian path; Test cube; Test power; Electron device testing
A hierarchical technique for statistical path selection and criticality computation,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029818009&doi=10.1145%2f3107030&partnerID=40&md5=43993ae5b5bb7f7d87d7f6681592f5d9,"Due to process variations, every path in the circuit is associated with a probability of being critical and a measure of this probability is the criticality of the path. Identification of critical paths usually proceeds in two steps, namely, generation of a candidate path set followed by computation of path criticality. As criticality computation is expensive, the candidate path set is chosen using simpler metrics. However, these metrics are not directly related to path criticality and, often, the set also contains low criticality paths that do not need to be tested. In this article, we propose a hierarchical technique that directly gives all paths above a global criticality threshold. The circuit is divided into disjoint groups at various levels. We show that the criticality of a group at each level of hierarchy can be computed using criticality of the parent group and the local complementary delay within the group. Low criticality groups are pruned at every level, making the computation efficient. This recursive partitioning and group criticality computation is continued until the group criticality falls below a threshold. Beyond this, the path selection within the group is done using branch-and-bound algorithm with global criticality as the metric. This is possible, since our method for criticality computation is very efficient. Unlike other techniques, path selection and criticality computation are integrated together so that when the path selection is complete, path criticality is also obtained. The proposed algorithm is tested with ISCAS'85, ISCAS'89, and ITC'99 benchmark circuits and the results are verified using Monte Carlo simulation. The experimental results suggest that the proposed method gives better accuracy on average with around 90% reduction in run-time. © 2017 ACM.",At-speed test; Hierarchical partitioning; Path criticality; Path selection; Statistical timing,Branch and bound method; Computational efficiency; Intelligent systems; Monte Carlo methods; Regression analysis; At-speed test; Hierarchical partitioning; Path criticality; Path selection; Statistical timing; Criticality (nuclear fission)
Exploring energy-efficient cache design in emerging mobile platforms,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026546798&doi=10.1145%2f2843940&partnerID=40&md5=3c697bc243282047bf2a5c2c95fc4b68,"Mobile devices are quickly becoming the most widely used processors in consumer devices. Since their major power supply is battery, energy-efficient computing is highly desired. In this article, we focus on energy-efficient cache design in emerging mobile platforms. We observe that more than 40% of L2 cache accesses are OS kernel accesses in interactive smartphone applications. Such frequent kernel accesses cause serious interferences between the user and kernel blocks in the L2 cache, leading to unnecessary block replacements and high L2 cache miss rate. We first propose to statically partition the L2 cache into two separate segments, which can be accessed only by the user code and kernel code, respectively. Meanwhile, the overall size of the two segments is shrunk, which reduces the energy consumption while still maintaining the similar cache miss rate. We then find completely different access behaviors between the two separated kernel and user segments and explore the multi-retention STT-RAM-based user and kernel segments to obtain higher energy savings in this static partition-based cache design. Finally, we propose to dynamically partition the L2 cache into the user and kernel segments to minimize overall cache size. We also integrate the short-retention STT-RAM into this dynamic partition-based cache design for maximal energy savings. The experimental results show that our static technique reduces cache energy consumption by 75% with 2% performance loss, and our dynamic technique further shows strong capability to reduce cache energy consumption by 85% with only 3% performance loss. © 2017 ACM",Cache; Mobile processors; Spin-transfer torque RAM (STT-RAM),Buffer storage; Cache memory; Energy conservation; Energy utilization; Mobile phones; Cache; Cache energy consumption; Dynamic techniques; Energy efficient computing; Energy-efficient caches; Mobile processors; Smart-phone applications; Stt rams; Energy efficiency
Two-stage layout decomposition for hybrid e-beam and triple patterning lithography,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027076418&doi=10.1145%2f3084683&partnerID=40&md5=52cb9badfb223fab2eca764bb64f0883,"Hybrid e-beam lithography (EBL) and triple patterning lithography (TPL) are advanced technologies for the manufacture of integrated circuits. We propose a technology that combines the advantages of EBL and TPL, which is more promising for the pattern product industry. Layout decomposition is a crucial step in this technology. In this article, we propose a two-stage decomposition flow for the hybrid e-beam and triple patterning lithography of the general layout decomposition (HETLD) problem. At the first stage, we formulate two optimization problems: the e-beam and stitch-aware TPL mask assignment (ESTMA) problem and the extended minimum weight dominating set for R4 mask assignment (MDSR4MA) problem. Binary linear program formulations of the two problems are solved by the cutting plane approach. At the second stage, solutions of the first stage problems are legalized to feasible solutions of the HETLD problem by stitch insertion and e-beam shot. To speed up decomposition,we reduce the problem size by removing some vertices and some minor conflict edges before decomposition. Experimental results show the effectiveness of our decomposition methods based on ESTMA and MDSR4MA. © 2017 ACM.",E-beam lithography; Stitch; Triple patterning lithography; Variable shaped beam,Electron beam lithography; Linear programming; Optimization; Binary linear program; Cutting-plane approach; Decomposition methods; e-Beam lithography; Optimization problems; Stitch; Triple patterning; Variable shaped beams; Lithography
Proof-carrying hardware via inductive invariants,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026531900&doi=10.1145%2f3054743&partnerID=40&md5=2c6369ec9337c7122bdb09ba1dbf2f74,"Proof-carrying hardware (PCH) is a principle for achieving safety for dynamically reconfigurable hardware systems. The producer of a hardware module spends huge effort when creating a proof for a safety policy. The proof is then transferred as a certificate together with the configuration bitstream to the consumer of the hardware module, who can quickly verify the given proof. Previous work utilized SAT solvers and resolution traces to set up a PCH technology and corresponding tool flows. In this article, we present a novel technology for PCH based on inductive invariants. For sequential circuits, our approach is fundamentally stronger than the previous SAT-based one since we avoid the limitations of bounded unrolling. We contrast our technology to existing ones and show that it fits into previously proposed tool flows. We conduct experiments with four categories of benchmark circuits and report consumer and producer runtime and peak memory consumption, as well as the size of the certificates and the distribution of the workload between producer and consumer. Experiments clearly show that our new induction-based technology is superior for sequential circuits, whereas the previous SAT-based technology is the better choice for combinational circuits. © 2017 ACM",Hardware certificates; IC3; Inductive invariants; Proof-carrying hardware,Computer hardware; Hardware; Benchmark circuit; Configuration bitstream; Hardware modules; Inductive invariants; Peak memory consumption; Proof carrying hardwares (PCH); Safety policies; SAT solvers; Reconfigurable hardware
Novel range matching architecture for packet classification without rule expansion,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027003516&doi=10.1145%2f3105958&partnerID=40&md5=c1c2842d61e2d55108e46ee21151be26,"The speed requirement for the routing table lookup and the packet classification is rapidly increasing due to the increase in the number of packets needed to be processed per second. The hardware-based packet classification relies on ternary content addressable memory (TCAM) to meet this speed requirement. However, TCAM consumes huge power and also supports only for longest prefix match and exact match, where the classification rule also has a range match (RM) field. Hence, it is mandatory to encode the RM into prefix match to accommodate the rule in TCAM. In the worst case, one rule is encoded into (2W - 2)2 rules (where W is a number of bits to represent range). This work proposes a novel RM architecture, and a detailed analysis about the range field on the standard dataset and the real-life classifier rules are presented. In the literature, the existing RM architecture is used to avoid the range to prefix conversion, but due to the serial operation, it lacks in performance. For constant time lookup, TCAM is the best option, but it does not support RM. The proposed architecture takes one clock cycle for RM and does not require any encoding/conversion. Hence, there will be a single entry for every rule. It is observed that just 4% of the two-dimensional range rules are present in this dataset, and it will increase the rule set size by 4 times in the best case and nearly 30 times in the worst case. The proposed RM circuit is operated in parallel with TCAM without compromising the speed, and this circuit saves huge power around 70% and area around 61%, where the range to prefix conversion/encoding is completely avoided. The proposed architecture is well suited for current IPv4-and IPv6-based networks, as well as in software-defined networks in the near future. © 2017 ACM.",Packet classification; Range encoding; Range match; TCAM,Classification (of information); Encoding (symbols); Logic gates; Memory architecture; Network architecture; Signal encoding; Table lookup; Ternary content adressable memory; Longest prefix matches; Packet classification; Proposed architectures; Range encoding; Range match; Routing table lookup; TCAM; Ternary content addressable memory; Internet protocols
Optimization and quality estimation of circuit design via random region covering method,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027068905&doi=10.1145%2f3084685&partnerID=40&md5=ddbaf58e71b70baa475c670538d32187,"Random region covering is a global optimization technique that explores the landscape by introducing multiple random starting points to initiate the local optimization solvers. This study applies the random region covering technique to circuit design automation and proposes a theory to explain why this technique is efficient at searching for the global optimum. In addition to analyzing the efficiency of the random region covering algorithm, the theory gives a probability-based estimation of the goodness of the optimization result. To enhance the efficiency of the random region covering technique, this work evaluates the boundary of top performance regions and proposes a modified random region covering method that only performs the global optimization on the top design region. The results from a large number of mathematical experiments verify the proposed methodology. The optimized designs of a class-E power amplifier and a wide load range operational amplifier outperform both manual designs and other state-of-the-art optimization techniques. © 2017 ACM.",Algorithm; Circuit optimization; Gate sizing; Synthesis,Algorithms; Computer aided design; Efficiency; Estimation; Gates (transistor); Global optimization; Integrated circuit manufacture; Operational amplifiers; Optimization; Power amplifiers; Synthesis (chemical); Timing circuits; Circuit optimization; Class E power amplifiers; Gate sizing; Global optimization techniques; Local optimizations; Mathematical experiments; Optimization techniques; Performance regions; Design
Word-and partition-level write variation reduction for improving non-volatile cache lifetime,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027040497&doi=10.1145%2f3084690&partnerID=40&md5=77d56859b36234f87fba38eb503c4f91,"Non-volatile memory technologies are among the most promising technologies for implementing the main memories and caches in future microprocessors and replacing the traditional DRAM and SRAM technologies. However, one of the most challenging design issues of the non-volatile memory technologies is the limited write. In this article,we first propose to exploit the narrow-width values to improve the lifetime of non-volatile last-level caches with word-level write variation reduction. Leading zeros masking scheme is proposed to reduce thewrite stress to the upper half of the narrow-width data. To balance the write variations between the upper half and the lower half of the narrow-width data, two swapping schemes, the swap on write (SW) and swap on replacement (SRepl), are proposed. Two existing optimization schemes, the multiple dirty bit (MDB) and read before write (RBW), are adopted with our word-level swapping design. To further reduce the write variation on the partition level, we propose to exploit the cache partitioning design to improve the lifetime. Based on the observation that different applications demonstrate different cache access (write) behaviors, we propose to partition the last-level cache for different applications and balance thewrite variations by partition swapping. Both software-based and hardware-based partitioning and swapping schemes are proposed and evaluated for different situations. Our experimental results show that both our word-and partition-level designs can improve the lifetime of the non-volatile caches effectively with low performance and energy overheads. © 2017 ACM.",Cache partitioning; Last-level cache; Narrow-width value; Non-volatile memory; Wear-leveling,Data storage equipment; Digital storage; Dynamic random access storage; Integrated circuit design; Nonvolatile storage; Static random access storage; Cache partitioning; Last-level caches; Narrow width; Non-volatile memory; Wear leveling; Cache memory
Electric vehicle optimized charge and drive management,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027017023&doi=10.1145%2f3084686&partnerID=40&md5=1465149171fc2631d6688cc8704355c1,"Electric vehicles (EVs) have been considered as a solution to the environmental issues caused by transportation, such as air pollution and greenhouse gas emission. However, limited energy capacity, scarce EV supercharging stations, and long recharging time have brought anxiety to drivers who use EVs as their main mean of transportation. Furthermore, EV owners need to deal with a huge battery replacement cost when the battery capacity degrades. Yet in-house EV chargers affect the pattern of the power grid load, which is not favorable to the utilities. The driving route, departure/arrival time of daily trips, and electricity price influence the EV energy consumption, battery lifetime, electricity cost, and EV charger load on the power grid. The EV driving range and battery lifetime issues have been addressed by battery management systems and route optimization methodologies. However, in this article, we are proposing an optimized charge and drive management (OCDM) methodology that selects the optimal driving route, schedules daily trips, and optimizes the EV charging process while considering the driver's timing preference. Our methodology will improve the EV driving range, extend the battery lifetime, reduce the recharging cost, and diminish the influence of EV chargers on the power grid. The performance of our methodology compared to the state of the art have been analyzed by experimenting on three benchmark EVs and three drivers. Our methodology has decreased EV energy consumption by 27%, improved the battery lifetime by 24.8%, reduced the electricity cost by 35%, and diminished the power grid peak load by 17% while increasing less than 20 minutes of daily driving time. Moreover, the scalability of our OCDM methodology for different parameters (e.g., time resolution and multiday cycles) in terms of execution time and memory usage has been analyzed. © 2017 ACM.",Battery; Electric vehicle; MILP; Power estimation; Smart grid,Battery management systems; Benchmarking; Charging (batteries); Cost reduction; Costs; Electric batteries; Electric power transmission networks; Electric vehicles; Energy utilization; Greenhouse gases; Integer programming; Secondary batteries; Vehicles; Battery; Battery replacements; Electric Vehicles (EVs); Environmental issues; MILP; Power estimations; Route optimization; Smart grid; Smart power grids
A comprehensive bist solution for polar transceivers using on-chip resources,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027042341&doi=10.1145%2f3084689&partnerID=40&md5=2c05559b5f3f13b2624e395432103514,"This article presents a Built-in self-test (BIST) solution for polar transceivers with low cost and high accuracy. Radio frequency (RF) Polar transceivers are desirable for portable devices due to higher power efficiency compared to traditional RF Cartesian transceivers. Unfortunately, their design is quite challenging due to substantially different signal paths that need to work coherently to ensure signal quality. In the receiver, phase and gain mismatches degrade sensitivity and error vector magnitude. In the transmitter, delay skew between the envelope and phase signals and the finite envelope bandwidth can create intermodulation distortion, which leads to violation of spectral mask requirements. Typically, these parameters are not directly measured but calibrated through spectral analysis using expensive RF equipment, leading to lengthy and costly measurement/calibration cycles. However, characterization and calibration of these parameters with analytical model would reduce the test time and cost considerably. In this article, we propose a technique to measure with the intent to calibrate impairments of the polar transceiver in the loop-back mode. Simulation and hardware measurement results show that the proposed technique can characterize the targeted impairments accurately. © 2017 ACM.",Delay skew; Envelope bandwidth; IQ mismatch,Bandwidth; Built-in self test; Calibration; Parameter estimation; Spectrum analysis; Transceivers; Delay skew; Error vector magnitude; IQ mismatch; Portable device; Power efficiency; Radio frequencies; Signal quality; Spectral masks; Radio transceivers
Generating current constraints to guarantee RLC power grid safety,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027074923&doi=10.1145%2f3054746&partnerID=40&md5=d55dbd829bc7c97ea40ca781893d4986,"A critical task during early chip design is the efficient verification of the chip power distribution network. Vectorless verification, developed since the mid-2000s as an alternative to traditional simulation-based methods, requires the user to specify current constraints (budgets) for the underlying circuitry and checks if the corresponding voltage variations on all grid nodes are within a user-specified margin. This framework is extremely powerful, as it allows for efficient and early verification, but specifying/obtaining current constraints remains a burdensome task for users and a hurdle to adoption of this framework by the industry. Recently, the inverse problem has been introduced: Generate circuit current constraints that, if satisfied by the underlying logic circuitry, would guarantee grid safety from excessive voltage variations. This approach has many potential applications, including various grid quality metrics, as well as voltage drop-aware placement and floorplanning. So far, this framework has been developed assuming only resistive and capacitive (RC) elements in the power grid model. Inductive effects are becoming a significant component of the power supply noise and can no longer be ignored. In this article, we extend the constraints generation approach to allow for inductance. We give a rigorous problem definition and develop some key theoretical results related to maximality of the current space defined by the constraints. Based on this, we then develop three constraints generation algorithms that target the peak total chip power that is allowed by the grid, the uniformity of current distribution across the die area, and a combination of both metrics. © 2017 ACM.",Current budgets; Current constraints generation; Power grid; Voltage integrity,Budget control; Computer circuits; Inverse problems; Constraints generation; Current distribution; Power grid safeties; Power grids; Power-supply noise; Problem definition; Simulation-based method; Voltage integrity; Electric power transmission networks
Multiharmonic small-signal modeling of low-power PWM DC-DC converters,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027079079&doi=10.1145%2f3057274&partnerID=40&md5=8f0fba6ffb1e0a91cede17e3bae102c9,"Small-signal models of pulse-width modulation (PWM) converters are widely used for analyzing stability and play an important role in converter design and control. However, existing small-signal models either are based on averaged DC behaviors, and hence are unable to capture frequency responses that are faster than the switching frequency, or greatly approximate these high-frequency responses. We address the severe limitations of the existing models by proposing a multiharmonic model that provides a complete small-signal characterization of both DC averages and high-order harmonic responses. The proposed model captures important high-frequency overshoots and undershoots of the converter response, which are otherwise unaccounted for by the existing techniques. In two converter examples, the proposed model corrects the misleading results of the existing models by providing truthful characterization of the overall converter AC response and offers important guidance for converter design and closed-loop control. © 2017 ACM.",DC-DC converters; Integrated circuit modeling; Multiharmonic averaging; Switched mode power supplies,Electric inverters; Frequency response; Pulse width modulation; Switched mode power supplies; Voltage control; Closed-loop control; High frequency response; High order harmonics; Integrated circuit modeling; Multiharmonic; Pulse width modulation converters; Signal characterization; Small signal model; DC-DC converters
Measurement-based worst-case execution time estimation using the Coefficient of Variation,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027055676&doi=10.1145%2f3065924&partnerID=40&md5=cecb5876b15657b49a5b5b3bcbd0f76b,"Extreme Value Theory (EVT) has been historically used in domains such as finance and hydrology to model worst-case events (e.g., major stock market incidences). EVT takes as input a sample of the distribution of the variable to model and fits the tail of that sample to either the Generalised Extreme Value (GEV) or the Generalised Pareto Distribution (GPD). Recently, EVT has become popular in real-time systems to derive worst-case execution time (WCET) estimates of programs. However, the application of EVT is not straightforward and requires a detailed analysis of, and customisation for, the particular problem at hand. In this article, we tailor the application of EVT to timing analysis. To that end, (1) we analyse the response time of different hardware resources (e.g., cache memories) and identify those that may lead to radically different types of execution time distributions. (2)We show that one of these distributions, known as mixture distribution, causes problems in the use of EVT. In particular, mixture distributions challenge not only properly selecting GEV/GPD parameters (i.e., location, scale and shape) but also determining the size of the sample to ensure that enough tail values are passed to EVT and that only tail values are used by EVT to fit GEV/GPD. Failing to select these parameters has a negative impact on the quality of the derived WCET estimates. We tackle these problems, by (3) proposing Measurement-Based Probabilistic Timing Analysis using the Coefficient of Variation (MBPTA-CV), a new mixture-distribution aware, WCET-suited MBPTA method that builds on recent EVT developments in other fields (e.g., finance) to automatically select the distribution parameters that best fit the maxima of the observed execution times. Our results on a simulation environment and a real board show that MBPTA-CV produces high-quality WCET estimates. © 2017 ACM.",Extreme value theory; Probabilistic analysis; Randomisation; Worst-case execution time,Cache memory; Electric power transmission; Interactive computer systems; Mixtures; Parameter estimation; Pareto principle; Probability distributions; Coefficient of variation; Distribution parameters; Extreme value theory; Generalised Pareto distributions; Probabilistic analysis; Randomisation; Simulation environment; Worst-case execution time; Real time systems
Optimal scheduling and allocation for IC design management and cost reduction,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027024557&doi=10.1145%2f3035483&partnerID=40&md5=15b35d8b98236431aba01a7bea3f5234,"A large semiconductor product company spends hundreds of millions of dollars each year on design infrastructure to meet tapeout schedules for multiple concurrent projects. Resources (servers, electronic design automation tool licenses, engineers, and so on) are limited and must be shared - and the cost per day of schedule slip can be enormous. Co-constraints between resource types (e.g., one license per every two cores (threads)) and dedicated versus shareable resource pools make scheduling and allocation hard. In this article, we formulate two mixed integer-linear programs for optimal multi-project, multi-resource allocation with task precedence and resource co-constraints. Application to a real-world three-project scheduling problem extracted from a leading-edge design center of anonymized Company X shows substantial compute and license costs savings. Compared to the product company, our solution shows that the makespan of schedule of all projects can be reduced by seven days, which not only saves ∼2.7% of annual labor and infrastructure costs but also enhances market competitiveness. We also demonstrate the capability of scheduling over two dozen chip development projects at the design center level, subject to resource and datacenter capacity limits as well as per-project penalty functions for schedule slips. The design center ended up purchasing 600 additional servers, whereas our solution demonstrates that the schedule can be met without having to purchase any additional servers. Application to a four-project scheduling problem extracted from a leading-edge design center in a non-US location shows availability of up to ∼37% headcount reduction during a half-year schedule for just one type of chip design activity. © 2017 ACM.",Design cost optimization; Project scheduling; Resource scheduling,Availability; Computer aided design; Cost reduction; Costs; Integer programming; Integrated circuit design; Integrated circuits; Product design; Timing circuits; Design costs; Design infrastructures; Electronic design automation tools; Mixed integer linear program; Project scheduling; Project scheduling problem; Resource-scheduling; Semiconductor products; Scheduling
Time-triggered scheduling of mixed-criticality systems,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027057690&doi=10.1145%2f3073415&partnerID=40&md5=c879269a9fd2d6441009befbcef28aae,"Real-time and embedded systems are moving from the traditional design paradigm to integration of multiple functionalities onto a single computing platform. Some of the functionalities are safety critical and subject to certification. The rest of the functionalities are nonsafety critical and do not need to be certified. Designing efficient scheduling algorithms which can be used to meet the certification requirement is challenging. Our research considers the time-triggered approach to scheduling of mixed-criticality jobs with two criticality levels. The first proposed algorithm for the time-triggered approach is based on the OCBP scheduling algorithm which finds a fixed-priority order of jobs. Based on this priority order, the existing algorithm constructs two scheduling tables SocLO and SocHI. The scheduler uses these tables to find a scheduling strategy. Another time-triggered algorithm called MCEDF was proposed as an improvement over the OCBP-based algorithm. Here we propose an algorithm which directly constructs two scheduling tables without using a priority order. Furthermore, we show that our algorithm schedules a strict superset of instances which can be scheduled by the OCBP-based algorithm as well as by MCEDF. We show that our algorithm outperforms both the OCBP-based algorithm and MCEDF in terms of the number of instances scheduled in a randomly generated set of instances. We generalize our algorithm for jobs with mcriticality levels. Subsequently, we extend our algorithm to find scheduling tables for periodic and dependent jobs. Finally, we show that our algorithm is also applicable to mixed-criticality synchronous programs upon uniprocessor platforms and schedules a bigger set of instances than the existing algorithm. © 2017 ACM.",EDF; MCEDF; Mixed-criticality; OCBP; Real-time scheduling; Scheduling tables; Time-triggered,Criticality (nuclear fission); Embedded systems; Real time systems; Safety engineering; Scheduling; MCEDF; Mixed criticalities; OCBP; Real - time scheduling; Time triggered; Scheduling algorithms
Incremental layer assignment for timing optimization,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027043671&doi=10.1145%2f3083727&partnerID=40&md5=06832d1307e24e07997dc7bcbdccecc2,"With VLSI technology nodes scaling into the nanometer regime, interconnect delay plays an increasingly critical role in timing. For layer assignment, most works deal with via counts or total net delays, ignoring critical paths of each net and resulting in potential timing issues. In this article, we propose an incremental layer assignment framework targeting delay optimization in timing the critical path of each net. A set of novel techniques are presented: self-adaptive quadruple partition based on K×K division benefits the runtime; semidefinite programming is utilized for each partition; and the sequential mapping algorithm guarantees integer solutions while satisfying edge capacities; additionally, concurrent mapping offers a global view of assignment and post delay optimization reduces the path timing violations. The effectiveness of our work is verified by ISPD'08 benchmarks. © 2017 ACM.",Critical path timing; Layer assignment; Semidefinite programming,Conformal mapping; Integrated circuit testing; Mapping; Timing circuits; Concurrent mapping; Critical Paths; Delay optimization; Interconnect delay; Layer assignment; Nano-meter regimes; Semi-definite programming; Timing optimization; Integer programming
Noc-HMP: A heterogeneous multicore processor for embedded systems designed in SystemJ,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027023793&doi=10.1145%2f3073416&partnerID=40&md5=d2334df329754f08da5bfb869df8acfd,"Scalability and performance in multicore processors for embedded and real-time systems usually don't go well each with the other. Networks on Chip (NoCs) provide scalable execution platforms suitable for such kind of embedded systems. This article presents a NoC-based Heterogeneous Multi-Processor system, called NoC-HMP, which is a scalable platform for embedded systems developed in the GALS language SystemJ. NoC-HMP uses a time-predictable TDMA-MIN NoC to guarantee latencies and communication time between the two types of time-predictable cores and can be customized for a specific performance goal through the execution strategy and scheduling of SystemJ program deployed across multiple cores. Examples of different execution strategies are introduced, explored and analyzed via measurements. The number of used cores can be minimized to achieve the target performance of the application. TDMA-MIN allows easy extensions of NoC-HMP with other cores or IP blocks. Experiments show a significant improvement of performance over a single core system and demonstrate how the addition of cores affects the performance of the designed system. © 2017 ACM.",GALS; MCSoC; Multicore; Network-on-chip; Systemj; TDMA-MIN,Binary alloys; Cesium alloys; Embedded systems; Interactive computer systems; Multicore programming; Real time systems; Time division multiple access; Embedded and real-time systems; GALS; Heterogeneous multicore processors; MCSoC; Multi core; Multi processor systems; Scalability and performance; Systemj; Network-on-chip
Test modification for reduced volumes of fail data,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023171258&doi=10.1145%2f3065925&partnerID=40&md5=77a8016fcb64dbcdb74202ce5a65b9c5,"As part of a yield improvement process, fail data is collected from faulty units. Several approaches exist for reducing the tester time and the volume of fail data that needs to be collected based on the observation that a subset of the fail data is sufficient for accurate defect diagnosis. This article addresses the volume of fail data by considering the test set that is used for collecting fail data. It observes that certain faults from a set of target faults produce significantly larger numbers of faulty output values (and therefore significantly larger volumes of fail data) than other faults under a given test set. Based on this observation, it describes a procedure for modifying the test set to reduce the maximum number of faulty output values that a target fault produces. When defects are considered in a simulation experiment, and a defect diagnosis procedure is applied to the fail data that they produce, two effects are observed: the maximum and average numbers of faulty output values per defect are reduced significantly with the modified test set, and the quality of diagnosis is similar or even improved with the modified test set. © 2017 ACM.",Defect diagnosis; Fail data volume; Full-scan circuits; Test generation,Defects; Average numbers; Data volume; Defect diagnosis; Full scan circuits; Reduced volumes; Target faults; Test generations; Yield Improvement; Testing
Efficient mapping of applications for future chip-multiprocessors in dark silicon era,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022215473&doi=10.1145%2f3055202&partnerID=40&md5=5aeb4374efe37b8346f3433d06ce3e19,"The failure of Dennard scaling has led to the utilization wall that is the source of dark silicon and limits the percentage of a chip that can actively switch within a given power budget. To address this issue, a structure is needed to guarantee the limited power budget along with providing sufficient flexibility and performance for different applications with various communication requirements. In this article, we present a generalpurpose platform for future many-core Chip-Multiprocessors (CMPs) that benefits from the advantages of clustering, Network-on-Chip (NoC) resource sharing among cores, and power gating the unused components of clusters. We also propose two task mapping methods for the proposed platform in which active and dark cores are dispersed appropriately, so that an excess of power budget can be obtained. Our evaluations reveal that the first and second proposed mapping mechanisms respectively reduce the execution time by up to 28.6% and 39.2% and the NoC power consumption by up to 11.1% and 10%, and gain an excess power budget of up to 7.6% and 13.4% over the baseline architecture. © 2017 ACM.",Chip-multiprocessors; Dark silicon; Mapping; Network-on-chip; Phrases; Resource sharing,Budget control; Mapping; Multiprocessing systems; Servers; Silicon; Base-line architecture; Chip Multiprocessor; Dark silicons; Mapping mechanism; Network-on-chip(NoC); Phrases; Resource sharing; Utilization walls; Network-on-chip
Training fixed-point classifiers for on-chip low-power implementation,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023177367&doi=10.1145%2f3057275&partnerID=40&md5=9e568cebb785fe9be9c955de3c3307a0,"In this article, we develop several novel algorithms to train classifiers that can be implemented on chip with low-power fixed-point arithmetic with extremely small word length. These algorithms are based on Linear Discriminant Analysis (LDA), Support Vector Machine (SVM), and Logistic Regression (LR), and are referred to as LDA-FP, SVM-FP, and LR-FP, respectively. They incorporate the nonidealities (i.e., rounding and overflow) associated with fixed-point arithmetic into the offline training process so that the resulting classifiers are robust to these nonidealities. Mathematically, LDA-FP, SVM-FP, and LR-FP are formulated as mixed integer programming problems that can be robustly solved by the branch-and-bound methods described in this article. Our numerical experiments demonstrate that LDA-FP, SVM-FP, and LR-FP substantially outperform the conventional approaches for the emerging biomedical applications of brain decoding. © 2017 ACM.",Fixed-point arithmetic; Low power; Machine learning,Branch and bound method; Discriminant analysis; Image retrieval; Integer programming; Learning systems; Medical applications; Support vector machines; Biomedical applications; Conventional approach; Linear discriminant analysis; Logistic regressions; Low Power; Low power implementation; Mixed integer programming; Numerical experiments; Fixed point arithmetic
Spatio-temporal scheduling of preemptive real-time tasks on partially reconfigurable systems,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022327289&doi=10.1145%2f3056561&partnerID=40&md5=c3458ae74cd0eb4f96967e1155f193e8,"Reconfigurable devices that promise to offer the twin benefits of flexibility as in general-purpose processors along with the efficiency of dedicated hardwares often provide a lucrative solution for many of today's highly complex real-time embedded systems. However, online scheduling of dynamic hard real-time tasks on such systems with efficient resource utilization in terms of both space and time poses an enormously challenging problem. We attempt to solve this problem using a combined offline-online approach. The offline component generates and stores various optional feasible placement solutions for different sub-sets of tasks that may possibly be co-mapped together. Given a set of periodic preemptive real-time tasks that requires to be executed at runtime, the online scheduler first carries out an admission control procedure and then produces a schedule, which is guaranteed to meet all timing constraints provided it is spatially feasible to place designated subsets of these tasks at specified scheduling points within a future time interval. These feasibility checks are done and actual placement solutions are obtained through a low overhead search of the statically precomputed placement solutions. Based on this approach, we have proposed a periodic preemptive real-time scheduling methodology for runtime partially reconfigurable devices. Effectiveness of the proposed strategy has been verified through simulation based experiments and we observed that the strategy achieves high resource utilization with low task rejection rates over various simulation scenarios. © 2017 ACM.",FPGA; Hard real-time systems; Partial reconfiguration; Proportional-fair scheduling; Task placement,Embedded systems; Field programmable gate arrays (FPGA); General purpose computers; Online systems; Scheduling; General purpose processors; Hard real-time systems; Partial reconfiguration; Partially reconfigurable devices; Proportional fair scheduling; Real-time embedded systems; Reconfigurable systems; Task placement; Real time systems
A fast hierarchical adaptive analog routing algorithm based on integer linear programming,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027014137&doi=10.1145%2f3035464&partnerID=40&md5=c0675c29d158dc87b7424467623298f8,"The shrinking design window and high parasitic sensitivity in advanced technologies have imposed special challenges on analog and radio frequency (RF) integrated circuit design. The state-of-the-art analog routing research tends to favor linear programming to achieve various analog constraints, which, although effective, fail to offer high routing efficiency on its own. In this article, we propose a new methodology to address such a deficiency based on integer linear programming (ILP) but without compromising the capability of handling any special constraints for the analog routing problems. Our proposed method supports hierarchical routing, which can divide the entire routing area into multiple small heterogeneous regions where the ILP can efficiently derive routing solutions. Distinct from the conventional methods, our algorithm utilizes adaptive resolutions for various routing regions. For a more congested region, a routing grid with higher resolution is employed, whereas a lower-resolution grid is adopted to a less-crowded routing region. For a large empty space, routing efficiency can be even boosted by creating more routing hierarchy levels. This scheme is especially beneficial to the analog and RF layouts, which are far sparser than their digital counterparts. The experimental results show that our proposed adaptive ILP-based router is much faster than the conventional ones, since it spends much less time in the areas that need no accurate routing anyway. The higher efficiency is demonstrated for large circuits and especially sparse layouts along with promising routing quality in terms of analog constraints. © 2017 ACM.",Efficiency; Integer linear programming; Special constraints,Efficiency; Integer programming; Integrated circuit manufacture; Routers; Advanced technology; Conventional methods; Digital counterparts; Heterogeneous region; Hierarchical routings; Integer Linear Programming; Radio frequency integrated circuits; Special constraints; Routing algorithms
Content-aware bit shuffling for maximizing PCM endurance,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027059174&doi=10.1145%2f3017445&partnerID=40&md5=70ef0c4f7fd1415f2af2430390c6a64b,"Recently, phase change memory (PCM) has been emerging as a strong replacement for DRAM owing to its many advantages such as nonvolatility, high capacity, low leakage power, and so on. However, PCM is still restricted for use as main memory because of its limited write endurance. There have been many methods introduced to resolve the problem by either reducing or spreading out bit flips. Although many previous studies have significantly contributed to reducing bit flips, they still have the drawback that lower bits are flipped more often than higher bits because the lower bits frequently change their bit values. Also, interblock wear-leveling schemes are commonly employed for spreading out bit flips by shifting input data, but they increase the number of bit flips per write. In this article, we propose a noble content-aware bit shuffling (CABS) technique that minimizes bit flips and evenly distributes them to maximize the lifetime of PCM at the bit level. We also introduce two additional optimizations, namely, addition of an inversion bit and use of an XOR key, to further reduce bit flips. Moreover, CABS is capable of recovering from stuck-at faults by restricting the change in values of stuck-at cells. Experimental results showed that CABS outperformed the existing state-of-the-art methods in the aspect of PCM lifetime extension with minimal overhead. CABS achieved up to 48.5% enhanced lifetime compared to the data comparison write (DCW) method only with a few metadata bits. Moreover, CABS obtained approximately 9.7% of improved write throughput than DCW because it significantly reduced bit flips and evenly distributed them. Also, CABS reduced about 5.4% of write dynamic energy compared to DCW. Finally, we have also confirmed that CABS is fully applicable to BCH codes as it was able to reduce the maximum number of bit flips in metadata cells by 32.1%. © 2017 ACM.",Bit flips; Bit shuffling; Endurance; Lifetime; PCM; Stuck-at fault,Durability; Dynamic random access storage; Metadata; Pulse code modulation; Bit-flips; Lifetime; Lifetime extension; Low leakage power; Phase change memory (pcm); State-of-the-art methods; Stuck-at faults; Write throughputs; Phase change memory
Generation of transparent-scan sequences for diagnosis of scan chain faults,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023201808&doi=10.1145%2f3007207&partnerID=40&md5=baef1059b7484f808c0b0cc8367d7f58,"Diagnosis of scan chain faults is important for yield learning and improvement. Procedures that generate tests for diagnosis of scan chain faults produce scan-based tests with one or more functional capture cycles between a scan-in and a scan-out operation. The approach to test generation referred to as transparentscan has several advantages in this context. (1) It allows functional capture cycles and scan shift cycles to be interleaved arbitrarily. This increases the flexibility to assign to the scan cells values that are needed for diagnosis. (2) Test generation under transparent-scan considers a circuit model where the scan logic is included explicitly. Consequently, the test generation procedure takes into consideration the full effect of a scan chain fault. It thus produces accurate tests. (3) For the same reason, it can also target faults inside the scan logic. (4) Transparent-scan results in compact test sequences. Compaction is important because of the large volumes of fail data that scan chain faults create. The cost of transparent-scan is that it requires simulation procedures for sequential circuits, and that arbitrary sequences would be applicable to the scan select input. Motivated by the advantages of transparent-scan, and the importance of diagnosing scan chain faults, this article describes a procedure for generating transparent-scan sequences for diagnosis of scan chain faults. The procedure is also applied to produce transparent-scan sequences for diagnosis of faults inside the scan logic. © 2017 ACM.",Diagnostic test generation; Full-scan circuits; Scan chain faults; Transparent-scan,Chains; Computer circuits; Testing; Compact test sequences; Diagnostic test generation; Full scan circuits; Scan chain; Simulation procedures; Test generation procedure; Test generations; Transparent scan; Fault detection
Accelerated soft-error-rate (SER) estimation for combinational and sequential circuits,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027020443&doi=10.1145%2f3035496&partnerID=40&md5=3f4a1b3c7cf3dec830372753cea02f03,"Radiation-induced soft errors have posed an increasing reliability challenge to combinational and sequential circuits in advanced CMOS technologies. Therefore, it is imperative to devise fast, accurate and scalable soft error rate (SER) estimation methods as part of cost-effective robust circuit design. This paper presents an efficient SER estimation framework for combinational and sequential circuits, which considers single-event transients (SETs) in combinational logic and multiple cell upsets (MCUs) in sequential elements. A novel topdown memoization algorithm is proposed to accelerate the propagation of SETs, and a general schematic and layout co-simulation approach is proposed to model the MCUs for redundant sequential storage structures. The feedback in sequential logic is analyzed with an efficient time frame expansion method. Experimental results on various ISCAS85 combinational benchmark circuits demonstrate that the proposed approach achieves up to 560.2X times speedup with less than 3% difference in terms of SER results compared with the baseline algorithm. The average runtime of the proposed framework on a variety of ISCAS89 benchmark circuits is 7.20s, and the runtime is 119.23s for the largest benchmark circuit with more than 3,000 flip-flops and 17,000 gates. © 2017 ACM.",Algorithm; Hardened flip-flop; Multiple-cell upset; Single-event upset; Soft error,Algorithms; Cost effectiveness; Digital storage; Error correction; Errors; Flip flop circuits; High electron mobility transistors; Integrated circuit manufacture; Microcontrollers; Radiation hardening; Sequential circuits; Timing circuits; Transients; Accelerated soft error rates; Combinational benchmarks; Combinational logic; Multiple cell upset; Single event transients; Single event upsets; Soft error; Soft error rate estimations; Computer circuits
Parallel high-level synthesis design space exploration for behavioral ips of exact latencies,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019752162&doi=10.1145%2f3041219&partnerID=40&md5=6b02a3b7cd7bc51697e938d517c11b6f,"This works presents a Design Space Exploration (DSE) method for Behavioral IPs (BIPs) given in ANSI-C or SystemC to find the smallest micro-Architecture for a specific target latency. Previous work on High-Level Synthesis (HLS) DSE mainly focused on finding a tradeoff curve with Pareto-optimal designs. HLS is, however, a single process (component) synthesis method. Very often, the latency of the components requires a specific fixed latency when inserted within a larger system. This work presents a fast multi-Threaded method to find the smallest micro-Architecture for a given BIP and target latency by discriminating between all different exploration knobs and exploring these concurrently. Experimental results show that our proposed method is very effective and comprehensive results compare the quality of results vs. The speedup of your proposed explorer. © 2017 ACM.",Behavioral IP (BIP); Design space exploration (DSE); Exact latency; High-level synthesis; Multi-Threaded; Parallel,Computer architecture; Logic design; Pareto principle; Behavioral IP (BIP); Design space exploration; Exact latency; Multithreaded; Parallel; High level synthesis
Design methodology of fault-Tolerant custom 3D network-on-chip,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019767931&doi=10.1145%2f3054745&partnerID=40&md5=283169d60e5e866404b1d35e395bfbeb,"A systematic design methodology is presented for custom Network-on-Chip (NoC) in three-dimensional integrated circuits (3D-ICs). In addition, fault tolerance is supported in the NoC if extra links are included in the NoC topology. In the proposed method, processors and the communication architecture are synthesized simultaneously in the 3D floorplanning process. 3D-IC technology enables ICs to be implemented in smaller size with higher performance; on the flip side, 3D-ICs suffer yield loss due to multiple dies in a 3D stack and lower manufacturing yield of through-silicon vias (TSVs). To alleviate this problem, a known-good-dies (KGD) test can be applied to ensure every die to be packaged into a 3D-IC is fault-free. However, faulty TSVs cannot be tested in the KGD test. In this article, the proposed method deals with the problem by providing fault tolerance in the NoC topology. The efficiency of the proposed method is evaluated using several benchmark circuits, and the experimental results show that the proposed method produces 3D NoCs with comparable performance than previous methods when fault-Tolerant features are not realized. With fault tolerance in NoCs, higher yield can be achieved at the cost of performance penalty and elevated power level. © 2017 ACM.",Fault tolerance; KGD test; Network-on-Chip (NoC); Three-dimensional integrated circuits (3D-IC),Benchmarking; Design; Electronics packaging; Fault tolerance; Network-on-chip; Servers; Three dimensional integrated circuits; Topology; Communication architectures; Design Methodology; Manufacturing yield; Network-on-chip(NoC); Performance penalties; Systematic design methodologies; Three dimensional integrated circuits (3-D IC); Through silicon vias; Integrated circuit design
Scalable bandwidth shaping scheme via adaptively managed parallel heaps in manycore-based network processors,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019663587&doi=10.1145%2f3065926&partnerID=40&md5=4a1542bcf982ace143db5522120db75e,"Scalability of network processor-based routers heavily depends on limitations imposed by memory accesses and associated power consumption. Bandwidth shaping of a flow is a key function, which requires a token bucket per output queue and abuses memory bandwidth. As the number of output queues increases, managing token buckets becomes prohibitively expensive and limits scalability. In this work, we propose a scalable software-based token bucket management scheme that can reduce memory accesses and power consumption significantly. To satisfy real-Time and low-cost constraints, we propose novel parallel heap data structures running on a manycore-based network processor. By using cache locking, the performance of heap processing is enhanced significantly and is more predictable. In addition, we quantitatively analyze the performance and memory footprint of the proposed software scheme using stochastic modeling and the Lyapunov central limit theorem. Finally, the proposed scheme provides an adaptive method to limit the size of heaps in the case of oversubscribed queues, which can successfully isolate the queues showing unideal behavior. The proposed scheme reduces memory accesses by up to three orders of magnitude for one million queues sharing a 100Gbps interface of the router while maintaining stability under stressful scenarios. © 2017 ACM.",Adaptive Control; Heap Tree; Manycore; Network Processor; Stochastic Modeling; Token Bucket,Adaptive control systems; Bandwidth; Electric power utilization; Locks (fasteners); Queueing theory; Routers; Scalability; Stochastic control systems; Stochastic models; Stochastic systems; Storage allocation (computer); Adaptive Control; Heap Tree; Many-core; Network processor; Token bucket; Parallel processing systems
Automated integration of dual-edge clocking for low-power operation in nanometer nodes,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019663414&doi=10.1145%2f3054744&partnerID=40&md5=5088d412f85a73d0cb9b6c27c24dedee,"Clocking power, including both clock distribution and registers, has long been one of the primary factors in the total power consumption of many digital systems. One straightforward approach to reduce this power consumption is to apply dual-edge-Triggered (DET) clocking, as sequential elements operate at half the clock frequency while maintaining the same throughput as with conventional single-edge-Triggered (SET) clocking. However, the DET approach is rarely taken in modern integrated circuits, primarily due to the perceived complexity of integrating such a clocking scheme. In this article, we first identify the most promising conditions for achieving low-power operation with DET clocking and then introduce a fully automated design flow for applying DET to a conventional SET design. The proposed design flow is demonstrated on three benchmark circuits in a 40nm CMOS technology, providing as much as a 50% reduction in clock distribution and register power consumption. © 2017 ACM.",Clock Distribution; Digital VLSI Circuits; Dual-Edge-Triggered Clocking; Low-Power Design; Nanometer Nodes,Electric power supplies to apparatus; Electric power utilization; Integrated circuit design; Low power electronics; Transients; Benchmark circuit; Clock distribution; Dual-Edge-Triggered Clocking; Low-power design; Low-power operation; Nanometer Nodes; Sequential elements; Total power consumption; Clocks
Approximate energy-efficient encoding for serial interfaces,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019752096&doi=10.1145%2f3041220&partnerID=40&md5=fb5e856d11ec9005e2cf0d2941bf5345,"Serial buses are ubiquitous interconnections in embedded computing systems that are used to interface processing elements with peripherals, such as sensors, actuators, and I/O controllers. Despite their limited wiring, as off-chip connections they can account for a significant amount of the total power consumption of a system-on-chip device. Encoding the information sent on these buses is the most intuitive and affordable way to reduce their power contribution; moreover, the encoding can be made even more effective by exploiting the fact that many embedded applications can tolerate intermediate approximations without a significant impact on the final quality of results, thus trading off accuracy for power consumption. We propose a simple yet very effective approximate encoding for reducing dynamic energy in serial buses. Our approach uses differential encoding as a baseline scheme and extends it with bounded approximations to overcome the intrinsic limitations of differential encoding for data with low temporal correlation.We show that the proposed scheme, in addition to yielding extremely compact codecs, is superior to all state-of-The-Art approximate serial encodings over a wide set of traces representing data received or sent from/to sensor or actuators. © 2017 ACM.",Approximate Computing; Bus Encoding; Interfaces; Low Power; Serial Communication,Actuators; Buses; Electric power utilization; Embedded systems; Encoding (symbols); Energy efficiency; Interfaces (materials); System buses; System-on-chip; Ubiquitous computing; Approximate Computing; Bus encoding; Differential encoding; Embedded computing system; Low Power; Serial communications; System-on-chip devices; Total power consumption; Signal encoding
SSAGA: SMs synthesized for asymmetric gpgpu applications,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018818600&doi=10.1145%2f3014163&partnerID=40&md5=0a3486ed7dcd0c5074a22fda98a7f074,"The emergence of GPGPU applications, bolstered by flexible GPU programming platforms, has created a tremendous challenge inmaintaining high energy efficiency inmodern GPUS. In this article, we demonstrate that customizing a StreamingMultiprocessor (SM) of a GPU at a lower frequency is significantlymore energy efficient compared to employing DVFS on an SM designed for a high-frequency operation. Using a systemlevel CAD technique, we propose SSAGA-Streaming Multiprocessors Synthesized for Asymmetric GPGPU Applications-an energy-efficient GPU design paradigm. SSAGA creates architecturally identical SM cores, customized for different voltage-frequency domains. Our rigorous cross-layer methodology demonstrates an average of 20% improvement in energy efficiency over a spatially multitasking GPU across a range of GPGPU applications. © 2017 ACM 1084-4309/2017/04-ART49 15.00.",,Computer aided design; Program processors; Design paradigm; Different voltages; Energy efficient; GPU programming; High energy efficiency; High frequency operation; Lower frequencies; Streaming multiprocessors; Energy efficiency
A single-tier virtual queuing memory controller architecture for heterogeneous MPSoCs,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018970242&doi=10.1145%2f3035481&partnerID=40&md5=7f11f4916b04737d5d73e02147564e5c,"Heterogeneous MPSoCs typically integrate diverse cores, including application CPUs, GPUs, and HD coders. These cores commonly share an off-chip memory to save cost and energy, but their memory accesses often interfere with each other, leading to undesirable consequences like a slowdown of application performance or a failure to sustain real-time performance. The memory controller plays a central role in meeting the QoS needs of real-time cores while maximizing CPU performance. Previous QoS-aware memory controllers are based on a classic two-tier queuing architecture that buffers memory transactions at the first tier, followed by a second tier that buffers translated DRAM commands. In these designs, QoS-aware policies are used to schedule competing transactions at the first stage, but the translated DRAM commands are served in FIFO order at the second stage. Unfortunately, once the scheduled transactions have been forwarded to the command stage, newly arriving transactions that may be more critical cannot be served ahead of those translated commands that are already queued at the second stage. To address this, we propose a scalable memory controller architecture based on single-tier virtual queuing (STVQ) that maintains a single tier of request queues and employs an efficacious scheduler that considers both QoS requirements and DRAM bank states. In comparison with previous QoS-aware memory controllers, the proposed STVQ memory controller reduces CPU slowdown by up to 13.9% while satisfying all frame rate requirements. We propose further optimizations that can significantly increase row-buffer hits by up to 66.2% and reduce memory latency by up to 19.8%. © 2017 ACM.",Memory controller; Memory scheduling; Quality of service,Controllers; Dynamic random access storage; Image coding; Multiprocessing systems; Program processors; Quality of service; Queueing theory; Scheduling; Storage allocation (computer); System-on-chip; Virtual addresses; Application performance; Memory controller; Memory latencies; Memory scheduling; Memory transactions; Off-chip memory; QoS requirements; Real time performance; Memory architecture
Fundamental challenges toward making the iot a reachable reality: A model-centric investigation,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018844367&doi=10.1145%2f3001934&partnerID=40&md5=342c636c09708dedeb658e5e8b880875,"Constantly advancing integration capability is paving the way for the construction of the extremely large scale continuum of the Internet where entities or things from vastly varied domains are uniquely addressable and interacting seamlessly to form a giant networked system of systems known as the Internet-of-Things (IoT). In contrast to this visionary networked system paradigm, prior research efforts on the IoT are still very fragmented and confined to disjoint explorations of different applications, architecture, security, services, protocol, and economical domains, thus preventing design exploration and optimization from a unified and global perspective. In this context, this survey article first proposes a mathematical modeling framework that is rich in expressivity to capture IoT characteristics from a global perspective. It also sets forward a set of fundamental challenges in sensing, decentralized computation, robustness, energy efficiency, and hardware security based on the proposed modeling framework. Possible solutions are discussed to shed light on future development of the IoT system paradigm. © 2017 ACM.",Challenges; Internet of Things; Mathematical modeling; Optimization,Energy efficiency; Mathematical models; Network architecture; Network protocols; Network security; Optimization; Challenges; Design Exploration; Global perspective; Integration capability; Internet of thing (IOT); Model framework; Networked systems; Research efforts; Internet of things
Obfuscation-based protection framework against printed circuit boards unauthorized operation and reverse engineering,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018259652&doi=10.1145%2f3035482&partnerID=40&md5=3c70dec711dcf61a1911676efbf43066,"Printed circuit boards (PCBs) are a basic necessity for all modern electronic systems but are becoming increasingly vulnerable to cloning, overproduction, tampering, and unauthorized operation. Most efforts to prevent such attacks have only focused on the chip level, leaving a void for PCBs and higher levels of abstraction. In this article, we propose the first ever obfuscation-based framework for the protection of PCBs. Central to our approach is a permutation block that hides the inter-chip connections between chips on the PCB and is controlled by a key. If the correct key is applied, then the correct connections between chips are made. Otherwise, the connections are incorrectly permuted, and the PCB/system fails to operate. We propose a permutation network added to the PCB based on a Benes network that can easily be implemented in a complex programmable logic device or field-programmable gate arrays. Based on this implementation, we analyze the security of our approach with respect to (i) brute-force attempts to reverse engineer the PCB, (ii) brute-force attempts at guessing the correct key, and (iii) physical and logistic attacks by a range of adversaries. Performance evaluation results on 12 reference designs show that brute force generally requires prohibitive time to break the obfuscation. We also provide detailed requirements for countermeasures that prevent reverse engineering, unauthorized operation, and so on, for different classes of attackers. © 2017 ACM.",Benes network; Board-level obfuscation; IP protection; Unauthorized operation prevention,Field programmable gate arrays (FPGA); Logic devices; Reverse engineering; Timing circuits; Benes network; Board-level; Complex programmable logic device; IP protection; Levels of abstraction; Permutation network; Printed circuit board (PCBs); Unauthorized operation prevention; Printed circuit boards
Using CORESIGHT PTM to itegrate CRA monitoring IPs in an ARM-Based SoC,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018841077&doi=10.1145%2f3035965&partnerID=40&md5=7cc670040b8350b45e14900416fcfb39,"The ARM CoreSight Program Trace Macrocell (PTM) has been widely deployed in recent ARM processors for real-Time debugging and tracing of software. Using PTM, the external debugger can extract execution behaviors of applications running on an ARM processor. Recently, some researchers have been using this feature for other purposes, such as fault-Tolerant computation and security monitoring. This motivated us to develop an external security monitor that can detect control hijacking attacks, of which the goal is to maliciously manipulate the control flow of victim applications at an attacker's disposal. This article focuses on detecting a special type of attack called code reuse attacks (CRA), which use a recently introduced technique that allows attackers to perform arbitrary computation without injecting their code by reusing only existing code fragments. Our external monitor is attached to the outside of the host system via the system bus and ARM CoreSight PTM, and is fed with execution traces of a victim application running on the host. As a majority of CRAs violates the normal execution behaviors of a program, our monitor constantly watches and analyzes the execution traces of the victim application and detects a symptom of attacks when the execution behaviors violate certain rules that normal applications are known to adhere. We present two different implementations for this purpose: A hardware-based solution in which all CRA detection components are implemented in hardware, and a hardware/software mixed solution that can be employed in a more resource-constrained environment where the deployment of full hardware-level CRA detection is burdensome. © 2017 ACM.",ARM; Code reuse attack (CRA); CoreSight; Defense; Processor Trace Macrocell (PTM),Application programs; Codes (symbols); Hardware; Program debugging; Program processors; System-on-chip; Code reuse; Control hijackings; CoreSight; Defense; Fault-tolerant computation; Macro cells; Real time debugging; Security monitoring; ARM processors
TEI-power: Temperature effect inversion-aware dynamic thermal management,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018857254&doi=10.1145%2f3019941&partnerID=40&md5=9f3541a7641d890dc95549b1938472ec,"FinFETs have emerged as a promising replacement for planar CMOS devices in sub-20nm technology nodes. However, based on the temperature effect inversion (TEI) phenomenon observed in FinFET devices, the delay characteristics of FinFET circuits in sub-, near-, and superthreshold voltage regimes may be fundamentally different from those of CMOS circuits with nominal voltage operation. For example, FinFET circuits may run faster in higher temperatures. Therefore, the existing CMOS-based and TEI-unaware dynamic power and thermal management techniques would not be applicable. In this article, we present TEI-power, a dynamic voltage and frequency scaling-based dynamic thermal management technique that considers the TEI phenomenon and also the superlinear dependencies of power consumption components on the temperature and outlines a real-Time trade-off between delay and power consumption as a function of the chip temperature to provide significant energy savings, with no performance penalty-namely, up to 42% energy savings for small circuits where the logic cell delay is dominant and up to 36% energy savings for larger circuits where the interconnect delay is considerable. © 2017 ACM 1084-4309/2017/04-ART51 15.00.",,CMOS integrated circuits; Dynamic frequency scaling; Economic and social effects; Electric power utilization; Energy conservation; FinFET; Industrial management; Logic devices; MOSFET devices; Temperature; Temperature control; Thermal variables control; Voltage scaling; Chip temperature; Delay characteristics; Dynamic thermal management; Dynamic voltage and frequency scaling; Interconnect delay; Management techniques; Performance penalties; Technology nodes; Delay circuits
Application-specific residential microgrid design methodology,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017114343&doi=10.1145%2f3007206&partnerID=40&md5=e2b23435a23cf4a353110d9234a01e07,"In power systems, the traditional, non-interactive, and manually controlled power grid has been transformed to a cyber-dominated smart grid. This cyber-physical integration has provided the smart grid with communication, monitoring, computation, and controlling capabilities to improve its reliability, energy efficiency, and flexibility. A microgrid is a localized and semi-autonomous group of smart energy systems that utilizes the above-mentioned capabilities to drive modern technologies such as electric vehicle charging, home energy management, and smart appliances. Design, upgrading, test, and verification of these microgrids can get too complicated to handle manually. The complexity is due to the wide range of solutions and components that are intended to address the microgrid problems. This article presents a novel Model-Based Design (MBD) methodology to model, co-simulate, design, and optimize microgrid and its multi-level controllers. This methodology helps in the design, optimization, and validation of a microgrid for a specific application. The application rules, requirements, and design-time constraints are met in the designed/optimized microgrid while the implementation cost is minimized. Based on our novel methodology, a design automation, co-simulation, and analysis tool, called GridMAT, is implemented. Our experiments have illustrated that implementing a hierarchical controller reduces the average power consumption by 8% and shifts the peak load for cost saving. Moreover, optimizing the microgrid design using our MBD methodology considering smart controllers has decreased the total implementation cost. Compared to the conventional methodology, the cost decreases by 14% and compared to the MBD methodology where smart controllers are not considered, it decreases by 5%. © 2017 ACM.",Co-simulation; Cyber-physical system modeling; Design automation; GridLAB-D; GridMAT; Microgrid; Model-Based Design; Residential microgrid; Smart grid,Belt drives; Computer aided design; Controllers; Costs; Cyber Physical System; Design; Electric power transmission networks; Embedded systems; Energy efficiency; Housing; Power control; Remotely operated vehicles; Smart power grids; Cosimulation; Cyber physical systems (CPSs); Design automations; GridMAT; Micro grid; Model- based designs; Residential microgrid; Smart grid; Electric power system control
"CDTA: A comprehensive solution for counterfeit detection, traceability, and authentication in the IoT supply chain",2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017172591&doi=10.1145%2f3005346&partnerID=40&md5=9626cb84a74a67645350fe59ed7f8124,"The Internet of Things (IoT) is transforming the way we live and work by increasing the connectedness of people and things on a scale that was once unimaginable. However, the vulnerabilities in the IoT supply chain have raised serious concerns about the security and trustworthiness of IoT devices and components within them. Testing for device provenance, detection of counterfeit integrated circuits (ICs) and systems, and traceability of IoT devices are challenging issues to address. In this article, we develop a novel radio-frequency identification (RFID)-based system suitable for counterfeit detection, traceability, and authentication in the IoT supply chain called CDTA. CDTA is composed of different types of on-chip sensors and in-system structures that collect necessary information to detect multiple counterfeit IC types (recycled, cloned, etc.), track and trace IoT devices, and verify the overall system authenticity. Central to CDTA is an RFID tag employed as storage and a channel to read the information from different types of chips on the printed circuit board (PCB) in both power-on and power-off scenarios. CDTA sensor data can also be sent to the remote server for authentication via an encrypted Ethernet channel when the IoT device is deployed in the field. A novel board ID generator is implemented by combining outputs of physical unclonable functions (PUFs) embedded in the RFID tag and different chips on the PCB. A light-weight RFID protocol is proposed to enable mutual authentication between RFID readers and tags. We also implement a secure interchip communication on the PCB. Simulations and experimental results using Spartan 3E FPGAs demonstrate the effectiveness of this system. The efficiency of the radio-frequency (RF) communication has also been verified via a PCB prototype with a printed slot antenna. © 2017 ACM.",Authentication; Counterfeit detection; Internet of Things (IoT); Radio-frequency identification (RFID); Supply chain security; Traceability,Authentication; Cloning; Crime; Cryptography; Digital storage; Integrated circuits; Microstrip antennas; Network security; Printed circuit boards; Radio frequency identification (RFID); Radio waves; Slot antennas; Supply chains; Integrated circuits (ICs); Interchip communications; Internet of thing (IOT); Internet of Things (IOT); Printed circuit boards (PCB); Radio frequency communication; Supply chain security; Traceability; Internet of things
Low-power clock tree synthesis for 3D-ICs,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017113337&doi=10.1145%2f3019610&partnerID=40&md5=f476bb6e57759f191c500880c4350f06,"We propose efficient algorithms to construct a low-power clock tree for through-silicon-via (TSV)-based 3DICs. We use shutdown gates to save clock trees' dynamic power, which selectively turn off certain clock tree branches to avoid unnecessary clock activities when the modules in these tree branches are inactive. While this clock gating technique has been extensively studied in 2D circuits, its application in 3D-ICs is unclear. In 3D-ICs, a shutdown gate is connected to a control signal unit through control TSVs, which may cause placement conflicts with existing clock TSVs in the layout due to TSV's large physical dimension.We develop a two-phase clock tree synthesis design flow for 3D-ICs: (1) 3D abstract clock tree generation based on K-means clustering and (2) clock tree embedding with simultaneous shutdown gates' insertion based on simulated annealing (SA) and a force-directed TSV placer. Experimental results indicate that (1) the K-means clustering heuristic significantly reduces the clock power by clustering modules with similar switching behavior and close proximity, and (2) the SA algorithm effectively inserts the shutdown gates to a 3D clock tree, while considering control TSV's placement. Compared with previous 3D clock tree synthesis techniques, our Kmeans clustering-based approach achieves larger reduction in clock tree power consumption while ensuring zero clock skew.",3D-ICs; Clock gating; Clock tree synthesis; Optimization; TSV,Adaptive systems; Clock distribution networks; Clocks; Electric clocks; Electronics packaging; Integrated circuit manufacture; Optimization; Simulated annealing; Timing circuits; Trees (mathematics); 3-D ICs; Clock gating; Clock gating techniques; Clock tree synthesis; ITS applications; K-means clustering; Switching behaviors; Through-Silicon-Via (TSV); Three dimensional integrated circuits
HoPE: Hot-cacheline prediction for dynamic early decompression in compressed LLCs,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017144416&doi=10.1145%2f2999538&partnerID=40&md5=ddebd38eba1a4e3ae6ffe54800391aec,"Data compression plays a pivotal role in improving system performance and reducing energy consumption, because it increases the logical effective capacity of a compressed memory system without physically increasing the memory size. However, data compression techniques incur some cost, such as non-negligible compression and decompression overhead. This overhead becomes more severe if compression is used in the cache. In this article, we aim to minimize the read-hit decompression penalty in compressed Last-Level Caches (LLCs) by speculatively decompressing frequently used cachelines. To this end, we propose a Hot-cacheline Prediction and Early decompression (HoPE) mechanism that consists of three synergistic techniques: Hotcacheline Prediction (HP), Early Decompression (ED), and Hit-history-based Insertion (HBI). HP and HBI efficiently identify the hot compressed cachelines, while ED selectively decompresses hot cachelines, based on their size information. Unlike previous approaches, the HoPE framework considers the performance balance/tradeoff between the increased effective cache capacity and the decompression penalty. To evaluate the effectiveness of the proposed HoPE mechanism, we run extensive simulations on memory traces obtained from multi-threaded benchmarks running on a full-system simulation framework. We observe significant performance improvements over compressed cache schemes employing the conventional Least-Recently Used (LRU) replacement policy, the Dynamic Re-Reference Interval Prediction (DRRIP) scheme, and the Effective Capacity Maximizer (ECM) compressed cache management mechanism. Specifically, HoPE exhibits system performance improvements of approximately 11%, on average, over LRU, 8% over DRRIP, and 7% over ECM by reducing the read-hit decompression penalty by around 65%, over a wide range of applications. © 2017 ACM 1084-4309/2017/04-ART40 $15.00.",Cache; Cache management policy; Compression,Compaction; Computer aided software engineering; Dynamics; Energy utilization; Forecasting; Cache; Cache management policies; Data compression techniques; Extensive simulations; Full-system simulation; Least recently used; Performance balance; Reducing energy consumption; Data compression
PeaPaw: Performance and energy-aware partitioning of workload on heterogeneous platforms,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015672238&doi=10.1145%2f2999540&partnerID=40&md5=dc740f0557bcb016507b0841dc86c6d9,"Performance and energy are two major concerns for application development on heterogeneous platforms. It is challenging for application developers to fully exploit the performance/energy potential of heterogeneous platforms. One reason is the lack of reliable prediction of the system's performance/energy before application implementation. Another reason is that a heterogeneous platform presents a large design space for workload partitioning between different processors. To reduce such development cost, this article proposes a framework, PeaPaw, to assist application developers to identify a workload partition (WP) that has high potential leading to high performance or energy efficiency before actual implementation. The PeaPaw framework includes both analytical performance/energy models and two sets of workload partitioning guidelines. Based on the design goal, application developers can obtain a workload partitioning guideline from PeaPaw for a given platform and follow it to design one or multiple WPs for a given workload. Then PeaPaw can be used to estimate the performance/energy of the designed WPs, and the WP with the best estimated performance/ energy can be selected for actual implementation. To demonstrate the effectiveness of PeaPaw, we have conducted three case studies. Results from these case studies show that PeaPaw can faithfully estimate the performance/energy relationships of WPs and provide effective workload partitioning guidelines. © 2017 ACM.",Energy modeling; Heterogeneous platforms; Performancemodeling; Workload partitioning,Energy efficiency; Analytical performance; Application developers; Application development; Energy model; Heterogeneous platforms; Performancemodeling; System's performance; Workload partitioning; Power management
Layer assignment of escape buses with consecutive constraints in PCB designs,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015617370&doi=10.1145%2f3012010&partnerID=40&md5=edb92417ec415e6cb94e3fad0f51232d,"It is important for cost and reliability consideration to minimize the number of the used layers in a PCB design. In this article, given a set of n circular escape buses with their escape directions between two adjacent components and a set of m consecutive constraints on the escape buses, the problem of assigning the given escape buses between two adjacent components onto the minimized layers is first formulated for bus-oriented escape routing. Furthermore, an efficient approach is proposed to minimize the number of the used layers for the given escape buses with the consecutive constraints and assign the escape buses onto the available layers. Compared with Yan's approach [Yan and Chen 2012] for the layer assignment of the linear escape buses with no consecutive constraint and Ma's approach [Ma et al. 2011a] for the layer assignment of the circular escape buses with consecutive constraints, the experimental results show that the proposed approach obtains the same optimal results on the number of the used layers and reduces 43.6% and 90.5% of CPU time for the tested examples on the average, respectively. © 2017 ACM.",Bus routing; Escape routing; Internal and external conflict; Layer assignment; PCB design,Organic pollutants; Polychlorinated biphenyls; Printed circuit boards; Bus Routing; Escape routing; Internal and external conflict; Layer assignment; PCB design; Buses
Security in automotive networks: Lightweight authentication and authorization,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017123300&doi=10.1145%2f2960407&partnerID=40&md5=22c50dcc2d39b9f933908641fa536e29,"With the increasing amount of interconnections between vehicles, the attack surface of internal vehicle networks is rising steeply. Although these networks are shielded against external attacks, they often do not have any internal security to protect against malicious components or adversaries who can breach the network perimeter. To secure the in-vehicle network, all communicating components must be authenticated, and only authorized components should be allowed to send and receivemessages. This is achieved through the use of an authentication framework. Cryptography is widely used to authenticate communicating parties and provide secure communication channels (e.g., Internet communication). However, the real-time performance requirements of in-vehicle networks restrict the types of cryptographic algorithms and protocols that may be used. In particular, asymmetric cryptography is computationally infeasible during vehicle operation. In this work, we address the challenges of designing authentication protocols for automotive systems. We present Lightweight Authentication for Secure Automotive Networks (LASAN), a full lifecycle authentication approach.We describe the core LASAN protocols and show how they protect the internal vehicle network while complying with the real-time constraints and low computational resources of this domain. By leveraging the fixed structure of automotive networks, we minimize bandwidth and computation requirements. Unlike previous work, we also explain how this framework can be integrated into all aspects of the automotive product lifecycle, including manufacturing, vehicle maintenance, and software updates. We evaluate LASAN in two different ways: First, we analyze the security properties of the protocols using established protocol verification techniques based on formal methods. Second, we evaluate the timing requirements of LASAN and compare these to other frameworks using a new highly modular discrete event simulator for in-vehicle networks, which we have developed for this evaluation. © 2017 ACM.",Authentication; Authorization; Automotive; Lightweight; Security,Authentication; Cryptography; Formal methods; Internet protocols; Life cycle; Vehicles; Authentication and authorization; Authorization; Automotive; Communicating components; Cryptographic algorithms; Discrete-event simulators; Lightweight; Security; Network security
Leak stopper: An actively revitalized snoop filter architecture with effective generation control,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015640383&doi=10.1145%2f3015770&partnerID=40&md5=743d5f794d605c2fa19984b8299ef2c2,"To alleviate high energy dissipation of unnecessary snooping accesses, snoop filters have been designed to reduce snoop lookups. These filters have the problem of decreasing filtering efficiency, and thus usually rely on partial or whole filter reset by detecting block evictions. Unfortunately, the reset conditions occur infrequently or unevenly (called passive filter deletion). This work proposes the concept of revitalized snoop filter (RSF) design, which can actively renew the destination filter by employing a generation wrappingaround scheme for various reference behaviors. We further utilize a sampling mechanism for RSF to timely trigger precise filter revitalizations, so that unnecessary RSF flushing can be minimized. The proposed RSF can be integrated to various existent inclusive snoop filters with only a minor change to their designs. We evaluate our proposed design and demonstrate that RSF eliminates 58.6% of snoop energy compared to JETTY on average while inducing only 6.5% of revitalization energy overhead. In addition, RSF eliminates 45.5% of snoop energy compared to stream registers on average and only induces 2.5% of revitalization energy overhead. Overall, these RSFs reduce the total L2 cache energy consumption by 52.1% (58.6% - 6.5%) as compared to JETTY and by 43% (45.5% - 2.5%) as compared to stream registers. Furthermore, RSF improves the overall performance by 1% to 1.4% on average compared to JETTY and stream registers for various benchmark suites. © 2017 ACM.",Cache storage; Computer architecture; Filters; Memory architecture,Bandpass filters; Benchmarking; Cache memory; Computer architecture; Energy dissipation; Energy utilization; Filters (for fluids); Jetties; Memory architecture; Benchmark suites; Energy overheads; Filter architecture; Filtering efficiency; Generation controls; L2 Cache; Reference behaviors; Sampling mechanisms; Passive filters
Topological approach to automatic symbolic macromodel generation for analog integrated circuits,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015622482&doi=10.1145%2f3015782&partnerID=40&md5=51c76e0559a2380c3370c4b1565141eb,"In the field of analog integrated circuit (IC) design, small-signal macromodels play indispensable roles for developing design insight and sizing reference. However, the subject of automatically generating symbolic low-order macromodels in human readable circuit form has not been well studied. Traditionally, work has been published on reducing full-scale symbolic transfer functions to simpler forms but without the guarantee of interpretability. On the other hand, methodologies developed for interconnect circuits (mainly resistorcapacitor- inductor (RCL) networks) are not suitable for analog ICs. In this work, a topological reduction method is introduced that is able to automatically generate interpretable macromodel circuits in symbolic form; that is, the circuit elements in the compact model maintain analytical relations of the parameters of the original full circuit. This type of symbolic macromodel has several benefits that other traditional modeling methods do not offer: First, reusability, namely that designer need not repeatedly generate macromodels for the same circuit even it is re-sized or re-biased; second, interpretability, namely a designer may directly identify circuit parameters (in the original circuit) that are closely related to the dominant frequency characteristics, such as dc gain, gain/phase margins, and dominant poles/zeros. The effectiveness and computational efficiency of the proposed method have been validated by several operational amplifier (opamp) circuit examples. © 2017 ACM.",Analog integrated circuit (IC); Automatic model generation; Graph-pair decision diagram (GPDD); Operational amplifier (opamp); Symbolic analysis,Computational efficiency; Electric network analysis; Equivalent circuits; Integrated circuit design; Operational amplifiers; Reusability; Timing circuits; Topology; Analytical relations; Automatic model generation; Decision diagram; Interconnect circuits; Operational amplifier (opamp); Symbolic analysis; Topological approach; Topological reduction; Analog integrated circuits
On the restore time variations of future DRAM memory,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017189568&doi=10.1145%2f2967609&partnerID=40&md5=6eb98d31149579710ab05470d4a55a90,"As the de facto main memory standard, DRAM (Dynamic Random Access Memory) has achieved dramatic density improvement in the past four decades, along with the advancements in process technology. Recent studies reveal that one of the major challenges in scaling DRAM into the deep sub-micron regime is its significant variations on cell restore time, which affect timing constraints such as write recovery time. Adopting traditional approaches results in either low yield rate or large performance degradation. In this article, we propose schemes to expose the variations to the architectural level. By constructing memory chunks with different access speeds and, in particular, exploiting the performance benefits of fast chunks, a variation-aware memory controller can effectively mitigate the performance loss due to relaxed timing constraints. We then proposed restore-time-aware rank construction and page allocation schemes to make better use of fast chunks. Our experimental results show that, compared to traditional designs such as row sparing and Error Correcting Codes, the proposed schemes help to improve system performance by about 16% and 20%, respectively, for 20nm and 14nm technology nodes on a four-core multiprocessor system. © 2017 ACM.",DRAM; Further scaling; Memory access; Process variation; Remapping; Restore,Molecular biology; Random access storage; Restoration; Further scaling; Memory access; Process Variation; Remapping; Restore; Dynamic random access storage
"Special section: Integrating dataflow, embedded computing and architecture",2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017147177&doi=10.1145%2f3023455&partnerID=40&md5=4e8efa904b03c0319bb39b056a3fce3d,[No abstract available],,
Scrubbing mechanism for heterogeneous applications in reconfigurable devices,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017265285&doi=10.1145%2f3038295&partnerID=40&md5=a2a4475db7400a8e1fea5f0d9d1cf865,"Commercial off-The-shelf (COTS) reconfigurable devices have been recognized as one of the most suitable processing devices to be applied in nano-satellites, since they can satisfy and combine their most important requirements, namely processing performance, reconfigurability, and low cost. However, COTS reconfigurable devices, in particular Static-RAM Field Programmable Gate Arrays, can be affected by cosmic radiation, compromising the overall nano-satellite reliability. Scrubbing has been proposed as a mechanism to repair faults in configuration memory. However, the current scrubbing mechanisms are predominantly static, unable to adapt to heterogeneous applications and their runtime variations. In this article, a dynamically adaptive scrubbing mechanism is proposed. Through a window-based scrubbing scheduling, this mechanism adapts the scrubbing process to heterogeneous applications (composed of periodic/sporadic and streaming/DSP (Digital Signal Processing) tasks), as well as their reconfigurations and modifications at runtime. Conducted simulation experiments show the feasibility and the efficiency of the proposed solution in terms of system reliability metric and memory overhead. © 2017 ACM.",Fault-Tolerance; heterogeneous applications; reconfigurable devices; scrubbing mechanisms,Cosmology; Digital signal processing; Fault tolerance; Field programmable gate arrays (FPGA); Nanosatellites; Random access storage; Reliability; Commercial off-the shelves; Configuration memory; Processing device; Processing performance; Reconfigurability; Reconfigurable devices; Run-time variations; System reliability; Signal processing
Reducing the complexity of dataflow graphs using slack-based merging,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011411549&doi=10.1145%2f2956232&partnerID=40&md5=9bad30821f067b49a25fc8c540827b10,"There exist many dataflow applications with timing constraints that require real-time guarantees on safe execution without violating their deadlines. Extraction of timing parameters (offsets, deadlines, periods) from these applications enables the use of real-time scheduling and analysis techniques, and provides guarantees on satisfying timing constraints. However, existing extraction techniques require the transformation of the dataflow application from highly expressive dataflow computational models, for example, Synchronous Dataflow (SDF) and Cyclo-Static Dataflow (CSDF) to Homogeneous Synchronous Dataflow (HSDF). This transformation can lead to an exponential increase in the size of the application graph that significantly increases the runtime of the analysis. In this article, we address this problem by proposing an offline heuristic algorithm called slack-based merging. The algorithm is a novel graph reduction technique that helps in speeding up the process of timing parameter extraction and finding a feasible real-time schedule, thereby reducing the overall design time of the real-time system. It uses two main concepts: (a) the difference between the worst-case execution time of the SDF graph's firings and its timing constraints (slack) to merge firings together and generate a reducedsize HSDF graph, and (b) the novel concept of merging called safe merge, which is a merge operation that we formally prove cannot cause a live HSDF graph to deadlock. The results show that the reduced graph (1) respects the throughput and latency constraints of the original application graph and (2) typically speeds up the process of extracting timing parameters and finding a feasible real-time schedule for real-time dataflow applications. They also show that when the throughput constraint is relaxed with respect to the maximal throughput of the graph, the merging algorithm is able to achieve a larger reduction in graph size, which in turn results in a larger speedup of the real-time scheduling algorithms. © 2017 ACM.",Merging algorithms; Synchronous dataflow model,Extraction; Heuristic algorithms; Interactive computer systems; Mergers and acquisitions; Merging; Parameter extraction; Real time systems; Response time (computer systems); Scheduling algorithms; Throughput; Cyclo-static dataflow; Extraction techniques; Merging algorithms; Real - time scheduling; Real-time scheduling algorithms; Synchronous dataflow model; Throughput constraints; Worst-case execution time; Data flow analysis
A MATLAB vectorizing compiler targeting application-specific instruction set processors,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011360784&doi=10.1145%2f2996182&partnerID=40&md5=f8ab04c2d5e12d24a60022183284fe05,"This article discusses a MATLAB-to-C vectorizing compiler that exploits custom instructions, for example, for Single Instruction Multiple Data (SIMD) processing and instructions for complex arithmetic present in Application-Specific Instruction Set Processors (ASIPs). Custom instructions are represented via specialized intrinsic functions in the generated code, and the generated code can be used as input to any C/C++ compiler supporting the target processor. Furthermore, the specialized instruction set of the target processor is described in a parameterized way using a target processor-independent architecture description approach, thus allowing the support of any processor. The compiler has been used for the generation of application code for two different ASIPs for several benchmarks. The code generated by the compiler achieves a speedup between 2×-74× and 2×-97× compared to the code generated by the MathWorks MATLAB-to-C compiler. Experimental results also prove that the compiler efficiently exploits SIMD custom instructions achieving a 3.3 factor speedup compared to cases where no SIMD processing is used. Thus the compiler can be employed to reduce the development time/effort/cost and time to market through raising the abstraction of application design in an embedded systems/system-on-chip development context. © 2017 ACM.",Application specific instruction set processor (ASIP); Compilation; Embedded systems; MATLAB; System-on-chip (SoC),Benchmarking; C (programming language); Codes (symbols); Digital signal processors; Embedded systems; MATLAB; Programmable logic controllers; System-on-chip; Systems analysis; Application design; Application specific instruction set processor; Architecture description; Compilation; Complex arithmetic; Intrinsic functions; Single instruction multiple data; System on chips (SoC); Program compilers
Scale & cap: Scaling-aware resource management for consolidated multi-threaded applications,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011385011&doi=10.1145%2f2994145&partnerID=40&md5=b56bf6a2787441420b406a245a6f2cea,"As the number of cores per server node increases, designing multi-threaded applications has become essential to efficiently utilize the available hardware parallelism. Many application domains have started to adopt multi-threaded programming; thus, efficient management of multi-threaded applications has become a significant research problem. Efficient execution of multi-threaded workloads on cloud environments, where applications are often consolidated by means of virtualization, relies on understanding the multi-threaded specific characteristics of the applications. Furthermore, energy cost and power delivery limitations require data center server nodes to work under power caps, which bring additional challenges to runtime management of consolidated multi-threaded applications. This article proposes a dynamic resource allocation technique for consolidated multi-threaded applications for power-constrained environments. Our technique takes into account application characteristics specific to multi-threaded applications, such as power and performance scaling, to make resource distribution decisions at runtime to improve the overall performance, while accurately tracking dynamic power caps.We implement and evaluate our technique on state-of-the-art servers and show that the proposed technique improves the application performance by up to 21% under power caps compared to a default resource manager. © 2017 ACM.",Energy efficiency; Multi-core; Multi-threaded; Power; Virtual machines,Electric power transmission; Energy efficiency; Java programming language; Virtual machine; Application performance; Dynamic resource allocations; Hardware parallelisms; Multi core; Multi- threaded applications; Multithreaded; Power; Resource distribution decisions; Resource allocation
A model-driven engineering methodology to design parallel and distributed embedded systems,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011371581&doi=10.1145%2f2999537&partnerID=40&md5=86d5f6cb9dd88ed4905c0d73b2d8444a,"In Model-Driven Engineering system-level approaches, the design of communication protocols and patterns is subject to the design of processing operations (computations) and to their mapping onto execution resources. However, this strategy allows us to capture simple communication schemes (e.g., processor-bus-memory) and prevents us from evaluating the performance of both computations and communications (e.g., impact of application traffic patterns onto the communication interconnect) in a single step. To solve these issues, we introduce a novel design approach-the ψ-chart-where we design communication patterns and protocols independently of a system's functionality and resources, via dedicated models. At the mapping step, both application and communication models are bound to the platform resources and transformed to explore design alternatives for both computations and communications. We present the ψ-chart and its implementation (i.e., communication models and Design Space Exploration) in TTool/DIPLODOCUS, a Unified Modeling Language (UML)/SysML framework for the modeling, simulation, formal verification and automatic code generation of data-flow embedded systems. The effectiveness of our solution in terms of better design quality (e.g., portability, time) is demonstrated with the design of the physical layer of a ZigBee (IEEE 802.15.4) transmitter onto a multi-processor architecture. © 2017 ACM.",Design space exploration; Hardware/software co-design; Model driven engineering; SysML; UML,Automatic programming; Data flow analysis; Design; Hardware-software codesign; Information theory; Integrated circuit design; Mapping; Modeling languages; Network layers; Standards; Systems analysis; Unified Modeling Language; Automatic code generations; Design space exploration; Distributed embedded system; Model-driven Engineering; Model-driven engineering methodology; Multi processor architecture; SysML; ZigBee (IEEE 802.15.4); Embedded systems
Worst-case response time analysis of a synchronous dataflow graph in a multiprocessor system with real-time tasks,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011372955&doi=10.1145%2f2997644&partnerID=40&md5=670dc03c2da45db17be887c51792acdb,"In this article, we propose a novel technique that estimates a tight upper bound of the worst-case response time (WCRT) of a synchronous dataflow (SDF) graph when the SDF graph shares processors with other realtime tasks. When an SDF graph is executed at runtime under a self-timed or static assignment scheduling policy on a multi-processor system, static scheduling of the SDF graph does not guarantee the satisfaction of latency constraints since changes to the schedule may result in timing anomalies. To estimate the WCRT of an SDF graph with a given mapping and scheduling result, we first construct a task instance dependency graph that depicts the dependency between node executions in a static schedule. The proposed technique combines two techniques in a novel way: schedule time bound analysis and response time analysis. The former is used to consider the interference between task instances in the same SDF graph, and the latter is used to consider the interference from other real-time tasks. Through extensive experiments with synthetic examples and benchmarks, we verify the superior performance of the proposed technique compared to other existent techniques. © 2017 ACM.",Multiprocessor; Partitioned scheduling; Performance analysis; Real-time system; Response time analysis; Synchronous dataflow graph; Worst-case response time,Benchmarking; Data flow analysis; Interactive computer systems; Multiprocessing systems; Program processors; Real time systems; Scheduling; Multiprocessor; Performance analysis; Response-time analysis; Synchronous dataflow graphs; Worst case response time; Response time (computer systems)
Symbolic analyses of dataflow graphs,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011423507&doi=10.1145%2f3007898&partnerID=40&md5=eeac72287cb29b19efd34d5bc6029f35,"The synchronous dataflow model of computation is widely used to design embedded stream-processing applications under strict quality-of-service requirements (e.g., buffering size, throughput, input-output latency). The required analyses can either be performed at compile time (for design space exploration) or at runtime (for resource management and reconfigurable systems). However, these analyses have an exponential time complexity, which may cause a huge runtime overhead or make design space exploration unacceptably slow. In this article, we argue that symbolic analyses are more appropriate since they express the system performance as a function of parameters (i.e., input and output rates, execution times). Such functions can be quickly evaluated for each different configuration or checked with respect to different quality-of-service requirements. We provide symbolic analyses for computing the maximal throughput of acyclic synchronous dataflow graphs, theminimum required buffers for which as soon as possible (ASAP) scheduling achieves this throughput, and finally, the corresponding input-output latency of the graph. The article first investigates these problems for a single parametric edge. The results are extended to general acyclic graphs using linear approximation techniques. We assess the proposed analyses experimentally on both synthetic and real benchmarks. © 2017 ACM.",Buffer minimization; Latency; Static analysis; Synchronous dataflow graphs; Throughput,Graphic methods; Quality control; Quality of service; Static analysis; Structural design; Throughput; Buffer minimization; Design space exploration; Exponential time complexity; Latency; Linear approximations; Reconfigurable systems; Synchronous dataflow graphs; Synchronous dataflow model; Data flow analysis
Computation of seeds for LFSR-based n-detection test generation,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011332583&doi=10.1145%2f2994144&partnerID=40&md5=98c06552949af06652d73bad8fe44884,"This article describes a new procedure that generates seeds for LFSR-based test generation when the goal is to produce an n-detection test set. The procedure does not use test cubes in order to avoid the situation where a seed does not exist for a given test cube with a given LFSR. Instead, the procedure starts from a set of seeds that produces a one-detection test set. It modifies seeds to obtain new seeds such that the tests they produce increase the numbers of detections of target faults. The modification procedure also increases the number of faults that each additional seed detects. Experimental results are presented to demonstrate the effectiveness of the procedure. © 2017 ACM.",LFSR-based test generation; Scan circuits; Test data compression; Transition faults,Automation; Computer applications; Detection tests; nocv1; Scan circuits; Target faults; Test cube; Test Data Compression; Test generations; Transition faults; Information dissemination
Multiprocessor scheduling of a multi-mode dataflow graph considering mode transition delay,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011339269&doi=10.1145%2f2997645&partnerID=40&md5=588a8944721fde017f715b2c395a6c28,"The Synchronous Data Flow (SDF) model is widely used for specifying signal processing or streaming applications. Since modern embedded applications become more complex with dynamic behavior changes at runtime, several extensions of the SDF model have been proposed to specify the dynamic behavior changes while preserving static analyzability of the SDF model. They assume that an application has a finite number of behaviors (or modes), and each behavior (mode) is represented by an SDF graph. They are classified as multi-mode dataflow models in this article. While there exist several scheduling techniques for multi-mode dataflow models, no one allows task migration between modes. By observing that the resource requirement can be additionally reduced if task migration is allowed, we propose a multiprocessor scheduling technique of a multi-mode dataflow graph considering task migration between modes. Based on a genetic algorithm, the proposed technique schedules all SDF graphs in all modes simultaneously to minimize the resource requirement. To satisfy the throughput constraint, the proposed technique calculates the actual throughput requirement of eachmode and the output buffer size for tolerating throughput jitter.We compare the proposed technique with a method that analyzes SDF graphs in each execution mode separately, a method that does not allow taskmigration, and amethod that does not allowmode-overlapped schedule for synthetic examples and five real applications: H.264 decoder, lane detection, vocoder, MP3 decoder, and printer pipeline. © 2017 ACM.",Mode transition delay; Mode-overlapped schedule; Multi-mode dataflow; Synchronous dataflow; Task migration; Throughput requirement,Decoding; Genetic algorithms; Multiprocessing systems; Response time (computer systems); Scheduling; Signal processing; Throughput; Dataflow; Mode transitions; Multi processor scheduling; Scheduling techniques; Streaming applications; Synchronous Dataflow; Task migration; Throughput constraints; Data flow analysis
A survey of parametric dataflow models of computation,2017,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011355693&doi=10.1145%2f2999539&partnerID=40&md5=acab67d0f5b4b13de6ea351446ec5ad1,"Dataflow models of computation (MoCs) are widely used to design embedded signal processing and streaming systems. Dozens of dataflow MoCs have been proposed in the past few decades. More recently, several parametric dataflow MoCs have been presented as an interesting tradeoff between analyzability and expressiveness. They offer a controlled form of dynamism under the form of parameters (e.g., parametric rates), along with runtime parameter configuration. This survey provides a comprehensive description of the existing parametric dataflow MoCs (constructs, constraints, properties, static analyses) and compares them using a common example. The main objectives are to help designers of streaming applications choose the most suitable model for their needs and pave the way for the design of new parametric MoCs. © 2017 ACM.",Dataflow graphs; Parameterization; Reconfiguration; Static analysis,Computation theory; Data flow graphs; Embedded systems; Parameterization; Signal processing; Static analysis; Surveys; Dataflow; Dataflow model; Embedded signal processing; Reconfiguration; Run time parameters; Streaming applications; Streaming systems; Data flow analysis
A hybrid DRAM/PCM buffer cache architecture for smartphones with QoS consideration,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008936689&doi=10.1145%2f2979143&partnerID=40&md5=f7e07bafa1c776c982b2db4bfc8f103c,"Flash memory is widely used in mobile phones to store contact information, application files, and other types of data. In an operating system, the buffer cache keeps the I/O blocks in dynamic random access memory (DRAM) to reduce the slow flash accesses. However, in smartphones, we observed two issues which reduce the benefits of the buffer cache. First, a large number of synchronous writes force writing the data from the buffer cache to flash frequently. Second, the large amount of I/O accesses from background applications diminishes the buffer cache efficiency of the foreground application, which degrades the quality-of-service (QoS). In this article, we propose a buffer cache architecture with hybrid DRAM and phase change memory (PCM) memory, which improves the I/O performance and QoS for smartphones. We use a DRAM first-level buffer cache to provide high buffer cache performance and a PCM last-level buffer cache to reduce the impact of frequent synchronous writes. Based on the proposed hierarchical buffer cache architecture, we propose a sub-block management and background flush to reduce the impact of the PCM write limitation and the dirty block write-back overhead, respectively. To improve the QoS, we propose a least-recently-activated first replacement policy (LRA) to keep the data from the applications that are most likely to become the foreground one. The experimental results show that with the proposed mechanisms, our hierarchical buffer cache can improve the I/O response time by 20% compared to the conventional buffer cache. The proposed LRA can improve the foreground application performance by 1.74x compared to the conventional CLOCK policy. © 2016 ACM.",Buffer cache; Phase change memory (PCM); Smartphone,Dynamic random access storage; Flash memory; Memory architecture; Phase change memory; Quality of service; Random access storage; Smartphones; Application files; Application performance; Buffer caches; Dynamic random access memory; Large amounts; Most likely; Phase change memory (pcm); Replacement policy; Cache memory
Security analysis of arbiter PUF and its lightweight compositions under predictability test,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008956931&doi=10.1145%2f2940326&partnerID=40&md5=20bced88731fa12b2850d3540246ed0c,"Unpredictability is an important security property of Physically Unclonable Function (PUF) in the context of statistical attacks, where the correlation between challenge-response pairs is explicitly exploited. In the existing literature on PUFs, the Hamming Distance Test, denoted by HDT(t), was proposed to evaluate the unpredictability of PUFs, which is a simplified case of the Propagation Criterion test PC(t). The objective of these test schemes is to estimate the output transition probability when there are t or fewer than t bits flips, and ideally this probability value should be 0.5. In this work, we show that aforementioned two test schemes are not enough to ensure the unpredictability of a PUF design. We propose a new test, which is denoted as HDT(e, t). This test scheme is a fine-tuned version of the previous schemes, as it considers the flipping bit pattern vector e along with parameter t. As a contribution, we provide a comprehensive discussion and analytic interpretation of HDT(t), PC(t), and HDT(e, t) test schemes for Arbiter PUF (APUF), Exclusive-OR (XOR) PUF, and Lightweight Secure PUF (LSPUF). Our analysis establishes that HDT(e, t) test is more general in comparison with HDT(t) and PC(t) tests. In addition, we demonstrate a few scenarios where the adversary can exploit the information obtained from the analysis of HDT(e, t) properties of APUF, XOR PUF, and LSPUF to develop statistical attacks on them, if the ideal value of HDT(e, t) = 0.5 is not achieved for a given PUF. We validate our theoretical observations using the simulated and Field Programmable Gate Array (FPGA) implemented APUF, XOR PUF, and LSPUF designs. © 2016 ACM.",Adaptive chosenchallenge attack; Arbiter physically unclonable function (APUF); Hamming distance test; Propagation criteria; Statistical attack; Unpredictability property,Field programmable gate arrays (FPGA); Hamming distance; Testing; Adaptive chosenchallenge attack; Physically unclonable functions; Propagation criterion; Statistical attacks; Unpredictability property; Hardware security
An elastic mixed-criticality task model and early-release EDF scheduling algorithms,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008945768&doi=10.1145%2f2984633&partnerID=40&md5=8094b5f54974e675e1231ba3c3c448e9,"Many algorithms have recently been studied for scheduling mixed-criticality (MC) tasks. However, most existing MC scheduling algorithms guarantee the timely executions of high-criticality (HC) tasks at the expense of discarding low-criticality (LC) tasks, which can cause serious service interruption for such tasks. In this work, aiming at providing guaranteed services for LC tasks, we study an elastic mixed-criticality (E-MC) task model for dual-criticality systems. Specifically, the model allows each LC task to specify its maximum period (i.e., minimum service level) and a set of early-release points. We propose an early-release (ER) mechanism that enables LC tasks to be released more frequently and thus improve their service levels at runtime, with both conservative and aggressive approaches to exploiting system slack being considered, which is applied to both earliest deadline first (EDF) and preference-oriented earliest-deadline schedulers. We formally prove the correctness of the proposed early-release-earliest deadline first scheduler on guaranteeing the timeliness of all tasks through judicious management of the early releases of LC tasks. The proposed model and schedulers are evaluated through extensive simulations. The results show that by moderately relaxing the service requirements of LC tasks in MC task sets (i.e., by having LC tasks' maximum periods in the E-MC model be two to three times their desired MC periods), most transformed E-MC task sets can be successfully scheduled without sacrificing the timeliness of HC tasks. Moreover, with the proposed ER mechanism, the runtime performance of tasks (e.g., execution frequencies of LC tasks, response times, and jitters of HC tasks) can be significantly improved under the ER schedulers when compared to that of the state-of-the-art earliest deadline first-virtual deadline scheduler. © 2016 ACM.",Earliest deadline first (EDF) scheduling; Early release; Elastic task models; Mixed-criticality systems; Real-time tasks; Scheduling algorithms,Criticality (nuclear fission); Real time systems; Response time (computer systems); Scheduling; Earliest deadline first scheduling; Early release; Elastic tasks; Mixed-criticality systems; Real-time tasks; Scheduling algorithms
Secure and flexible trace-based debugging of systems-on-chip,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008498549&doi=10.1145%2f2994601&partnerID=40&md5=50a9fd878533d68c5f2767a236b3a270,"This work tackles the conflict between enforcing security of a system-on-chip (SoC) and providing observability during trace-based debugging. On one hand, security objectives require that assets remain confidential at different stages of the SoC life cycle. On the other hand, the trace-based debug infrastructure exposes values of internal signals that can leak the assets to untrusted third parties. We propose a secure trace-based debug infrastructure to resolve this conflict. The secure infrastructure tags each asset to identify its owner (to whom it can be exposed during debug) and nonintrusively enforces the confidentiality of the assets during runtime debug. We implement a prototype of the enhanced infrastructure on an FPGA to validate its functional correctness. ASIC estimations show that our approach incurs practical area and power costs. © 2016 ACM.",Debug traces; Secure debug; Security and privacy → embedded systems security; System-on-chip,Application specific integrated circuits; Embedded systems; Life cycle; Program debugging; Programmable logic controllers; Debug traces; Different stages; Embedded systems securities; Functional correctness; Internal signals; Secure debug; Security objectives; System on chips (SoC); System-on-chip
CALM: Contention-aware latency-minimal application mapping for flattened butterfly on-chip networks,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008952945&doi=10.1145%2f2950045&partnerID=40&md5=0bb27dde3490c9fe30f5b687c9a7382a,"With the emergence of many-core multiprocessor system-on-chips (MPSoCs), on-chip networks are facing serious challenges in providing fast communication among various tasks and cores. One promising on-chip network design approach shown in recent studies is to add express channels to traditional mesh network as shortcuts to bypass intermediate routers, thereby reducing packet latency. This approach not only changes the packet latency models, but also greatly affects network traffic behaviors, both of which have not been fully exploited in existing mapping algorithms. In this article, we explore the opportunities in optimizing application mapping for flattened butterfly, a popular express channel-based on-chip network. Specifically, we identify the unique characteristics of flattened butterfly, analyze the opportunities and new challenges, and propose an efficient heuristic mapping algorithm. The proposed algorithm Contention-Aware Latency Minimal (CALM) is able to reduce unnecessary turns that would otherwise impose additional router pipeline latency to packets, as well as adjust forwarding traffic to reduce network contention latency. Simulation results show that the proposed algorithm can achieve, on average, 3.4X reduction in the number of turns, 24.8% reduction in contention latency, and 14.12% reduction in the overall packet latency. © 2016 ACM.",Application mapping; Contention awareness; Network on chip,Conformal mapping; Distributed computer systems; Mapping; Network-on-chip; Routers; System-on-chip; Application mapping; Contention awareness; Contention-aware; Heuristic mapping; Intermediate routers; Mapping algorithms; Multiprocessor system on chips; Network contention; Embedded systems
Scalable SMT-based equivalence checking of nested loop pipelining in behavioral synthesis,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008870040&doi=10.1145%2f2953879&partnerID=40&md5=16e7236ca9e8ab62a17f1757a2c601b3,"In this article, we present a novel methodology based on SMT-solvers to verify equality of a high-level described specification and a pipelined RTL implementation produced by a high-level synthesis tool. The complex transformations existing in the high-level synthesis process, such as nested loop pipelining, cause the conventional methods of equivalence checking to be inefficient. The proposed equivalence checking method simultaneously attacks the two problems in this context: (1) state space explosion and (2) complex high-level synthesis transformations. To show the scalability and efficiency of the proposed method, the verification results of large designs are compared with those of the SAT-based method, including three different stateof- the-art SAT-solvers: the SMT-based procedure, the modular Horner expansion diagram (M-HED)-based method, and the M-HED partitioning approach. The results show 2470×, 2540×, and 142× average memory usage reduction and 252×, 28×, and 914× speedup in comparison with M-HED, M-HED partitioning, and SMT-solver without using the proposed method, respectively. © 2016 ACM.",Behavioral synthesis; Equivalence checking; Formal verification; Nested loops; Pipeline,Formal verification; Pipelines; Behavioral synthesis; Complex transformations; Conventional methods; Equivalence checking; Nested Loops; Novel methodology; State-space explosion; Verification results; High level synthesis
Optimized implementation of multirate mixed-criticality synchronous reactive models,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008500383&doi=10.1145%2f2968445&partnerID=40&md5=a71b0b85715f380702516c52246a757c,"Model-based design using Synchronous Reactive (SR) models enables early design and verification of application functionality in a platform-independent manner, and the implementation on the target platform should guarantee the preservation of application semantic properties. Mixed-Criticality Scheduling (MCS) is an effective approach to addressing diverse certification requirements of safety-critical systems that integrate multiple subsystems with different levels of criticality. This article considers fixed-priority scheduling of mixed-criticality SR models, and considers two scheduling approaches: Adaptive MCS and Elastic MCS. We formulate the optimization problem of minimizing the total system cost of added functional delays in the implementation while guaranteeing schedulability, and present an optimal algorithm based on branch-and-bound search, and an efficient heuristic algorithm. © 2016 ACM.",Computer systems organization → embedded software; Mixed-criticality systems; Real-time operating systems; Real-time scheduling; Synchronous reactive model,Criticality (nuclear fission); Embedded systems; Heuristic algorithms; Optimization; Real time systems; Safety engineering; Scheduling; Semantics; Computer systems organization; Mixed-criticality systems; Reactive model; Real - time scheduling; Real time operating system; Computer operating systems
An adaptive demand-based caching mechanism for NAND flash memory storage systems,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007001304&doi=10.1145%2f2947658&partnerID=40&md5=ec0c033ca579f0cfeb459190924f44b4,"During past decades, the capacity of NAND flash memory has been increasing dramatically, leading to the use of nonvolatile flash in the system's memory hierarchy. The increasing capacity of NAND flash memory introduces a large RAM footprint to store the logical to physical address mapping. The demand-based approach can effectively reduce and well control the RAM footprint. However, extra address translation overhead is also introduced which may degrade the system performance. In this article, we present CDFTL, an adaptive Caching mechanism for Demand-based Flash Translation Layer, for NAND flash memory storage systems. CDFTL adopts both the fine-grained entry-based caching mechanism to exploit temporal locality and the coarse-grained translation-page-based caching mechanism to exploit spatial locality of workloads. By selectively caching the on-demand address mappings and adaptively changing the space configurations of two granularities, CDFTL can effectively utilize the RAM space and improve the cache hit ratio. We evaluate CDFTL under a real hardware embedded platform using a variety of I/O traces. Experimental results show that our technique can achieve an 11.13% reduction in average system response time and a 35.21% reduction in translation block erase counts compared with the previous work. © 2016 ACM.",,Embedded systems; Mapping; NAND circuits; Physical addresses; Random access storage; Address translation; Caching mechanism; Embedded platforms; Flash translation layer; Increasing capacities; NAND flash memory; Space configuration; Temporal locality; Flash memory
ERfair scheduler with processor suspension for real-time multiprocessor embedded systems,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007016812&doi=10.1145%2f2948979&partnerID=40&md5=e853f9d9385816cc864e6aeec4f307e9,"Proportional fair schedulers with their ability to provide optimal schedulability along with hard timeliness and quality-of-service guarantees on multiprocessors form an attractive alternative in real-time embedded systems that concurrently run a mix of independent applications with varying timeliness constraints. This article presents ERfair Scheduler with Suspension on Multiprocessors (ESSM), an efficient, optimal proportional fair scheduler that attempts to reduce system wide energy consumption by locally maximizing the processor suspension intervals while not sacrificing the ERfairness timing constraints of the system. The proposed technique takes advantage of higher execution rates of tasks in underloaded ERfair systems and uses a procrastination scheme to search for time points within the schedule where suspension intervals are locally maximal. Evaluation results reveal that ESSM achieves good sleep efficiency and provides up to 50% higher effective total sleep durations as compared to the Basic-ERfair scheduler on systems consisting of 2 to 20 processors. © 2016 ACM.",,Energy utilization; Multiprocessing systems; Quality of service; Real time systems; Scheduling; Suspensions (components); Evaluation results; Multiprocessor embedded system; Proportional fair scheduler; Quality of service guarantees; Real-time embedded systems; Sleep efficiencies; System-wide energy consumption; Timing constraints; Embedded systems
Non-enumerative generation of path delay distributions and its application to critical path selection,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006993654&doi=10.1145%2f2940327&partnerID=40&md5=85ed7aff6ee876bcf0dcdca763315778,"A Monte Carlo-based approach is proposed capable of identifying in a non-enumerative and scalable manner the distributions that describe the delay of every path in a combinational circuit. Furthermore, a scalable approach to select critical paths from a potentially exponential number of path candidates is presented. Paths and their delay distributions are stored in Zero Suppressed Binary Decision Diagrams. Experimental results on some of the largest ISCAS-89 and ITC-99 benchmarks shows that the proposed method is highly scalable and effective. © 2016 ACM.",,Binary decision diagrams; Critical Paths; Delay distributions; Exponential numbers; ITS applications; Path delay; Scalable approach; Zero-suppressed binary decision diagrams; Delay circuits
Hybrid power management for office equipment,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007011048&doi=10.1145%2f2910582&partnerID=40&md5=fe236ddde4bb3d61acaa23b6ec85073e,"Office machines (such as printers, scanners, facsimile machines, and copiers) can consume significant amounts of power. Most office machines have sleep modes to save power. Power management of these machines is usually timeout-based: a machine sleeps after being idle long enough. Setting the time-out duration can be difficult: if it is too long, the machine wastes power during idleness. If it is too short, the machine sleeps too soon and too often - the wake-up delay can significantly degrade productivity. Thus, power management is a tradeoff between saving energy and keeping response time short. Many power management policies have been published and one policy may outperform another in some scenarios. There is no definite conclusion regarding which policy is always better. This article describes two methods for office equipment power management. The first method adaptively reduces power based on a constraint of the wake-up delay. The second is a hybrid method with multiple candidate policies and it selects the most appropriate power management policy. Using 6 months of request traces from 18 different printers, we demonstrate that the hybrid policy outperforms individual policies. We also discover that power management based on business hours does not produce consistent energy savings. © 2016 ACM.",,Energy conservation; Energy management; Office equipment; Printing machinery; Printing presses; Wakes; Hybrid method; Hybrid policies; Management policy; Saving energy; SLEEP mode; Time-outs; Wake up; Power management
DReAM: An approach to estimate per-task DRAM energy in multicore systems,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006977869&doi=10.1145%2f2939370&partnerID=40&md5=65de1042e98641f31453991ba7bb3946,"Accurate per-task energy estimation in multicore systems would allow performing per-task energy-aware task scheduling and energy-aware billing in data centers, among other applications. Per-task energy estimation is challenged by the interaction between tasks in shared resources, which impacts tasks' energy consumption in uncontrolled ways. Some accurate mechanisms have been devised recently to estimate pertask energy consumed on-chip in multicores, but there is a lack of such mechanisms for DRAM memories. This article makes the case for accurate per-task DRAM energy metering in multicores, which opens new paths to energy/performance optimizations. In particular, the contributions of this article are (i) an ideal per-task energy metering model for DRAM memories; (ii) DReAM, an accurate yet low cost implementation of the ideal model (less than 5% accuracy error when 16 tasks share memory); and (iii) a comparison with standard methods (even distribution and access-count based) proving that DReAM is much more accurate than these other methods. © 2016 ACM.",,Energy utilization; Power management; Accuracy errors; Data centers; Energy aware; Energy estimation; Energy-aware task scheduling; Low cost implementations; Multi-core systems; Shared resources; Dynamic random access storage
A compact implementation of Salsa20 and its power analysis vulnerabilities,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997108098&doi=10.1145%2f2934677&partnerID=40&md5=2f0481ef0fcc344b0d8516ed0089a513,"In this article, we present a compact implementation of the Salsa20 stream cipher that is targeted towards lightweight cryptographic devices such as radio-frequency identification (RFID) tags. The Salsa20 stream cipher, ann addition-rotation-XOR (ARX) cipher, is used for high-security cryptography in NEON instruction sets embedded in ARM Cortex A8 CPU core-based tablets and smartphones. The existing literature shows that although classical cryptanalysis has been effective on reduced rounds of Salsa20, the stream cipher is immune to software side-channel attacks such as branch timing and cache timing attacks. To the best of our knowledge, this work is the first to perform hardware power analysis attacks, where we evaluate the resistance of all eight keywords in the proposed compact implementation of Salsa20. Our technique targets the three subrounds of the first round of the implemented Salsa20. The correlation power analysis (CPA) attack has an attack complexity of 219. Based on extensive experiments on a compact implementation of Salsa20, we demonstrate that all these keywords can be recovered within 20,000 queries on Salsa20. The attacks show a varying resilience of the key words against CPA that has not yet been observed in any stream or block cipher in the present literature. This makes the architecture of this stream cipher interesting from the side-channel analysis perspective. Also, we propose a lightweight countermeasure that mitigates the leakage in the power traces as shown in the results of Welch's t-test statistics. The hardware area overhead of the proposed countermeasure is only 14% and is designed with compact implementation in mind. © 2016 ACM.",ARX; Correlation analysis DPA; Differential power analysis; Hamming weight; Salsa20; Success rate,Cryptography; Hardware; Radio frequency identification (RFID); Statistical tests; Correlation analysis; Correlation power analysis (CPA); Cryptographic devices; Differential power Analysis; Hamming weights; Radio-frequency-identification tags (RFID); Salsa20; Side-channel analysis; Side channel attack
Probabilistic model checking for uncertain scenario-aware data flow,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988311453&doi=10.1145%2f2914788&partnerID=40&md5=c4a0ce86f620bdb0951ca52456d2a4dc,"The Scenario-Aware Dataflow (SADF) model is based on concurrent actors that interact via channels. It combines streaming data and control to capture scenarios while incorporating hard and soft real-time aspects. To model data-flow computations that are subject to uncertainty, SADF models are equipped with random primitives.We propose to use probabilistic model checking to analyze uncertain SADF models.We show how measures such as expected time, long-run objectives like throughput, as well as timed reachability-can a given system configuration be reached within a deadline with high probability?-can be automatically determined. The crux of our method is a compositional semantics of SADF with exponential agent execution times combined with automated abstraction techniques akin to partial-order reduction. We present the semantics in detail and show how it accommodates the incorporation of execution platforms, enabling the analysis of energy consumption. The feasibility of our approach is illustrated by analyzing several quantitative measures of an MPEG-4 decoder and an industrial face recognition application. © 2016 ACM.",Data-flow computation; Digital signal processing; Embedded systems; Energy consumption; Markov (reward) automata; Model checking,Computer programming languages; Data flow analysis; Data transfer; Digital signal processing; Embedded systems; Energy utilization; Face recognition; Motion Picture Experts Group standards; Semantics; Signal processing; State space methods; Uncertainty analysis; Abstraction techniques; Compositional semantics; Data-flow computation; Markov (reward) automata; Partial order reductions; Probabilistic model checking; Quantitative measures; System configurations; Model checking
Genetic-algorithm-based FPGA architectural exploration using analytical models,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988411898&doi=10.1145%2f2939372&partnerID=40&md5=cad4a3941b5c1e9990b02724a658c26d,"FPGA architectural optimization has emerged as one of the most important digital design challenges. In recent years, experimental methods have been replaced by analytical ones to find the optimized architecture. Time is the main reason for this replacement. Conventional Geometric Programming (GP) is a routine framework to solve analytical models, including area, delay, and power models. In this article, we discuss the application of the Genetic Algorithm (GA) to the design of FPGA architectures. The performance model has been integrated into the Genetic Algorithm framework in order to investigate the impact of various architectural parameters on the performance efficiency of FPGAs. This way, we are able to rapidly analyze FPGA architectures and select the best one. The main advantages of using GA versus GP are concurrency and speed. The results show that concurrent optimization of high-level architecture parameters, including lookup table size (K) and cluster size (N), and low-level parameters, like scaling of transistors, is possible for GA, whereas GP does not capture K and N under its concurrency and it needs to exhaustively search all possible combinations of K and N. The results also show that more than two orders of magnitude in runtime improvement in comparison with GP-based analysis is achieved. © 2016 ACM.",Analytical models; Application-specific FPGA architectures; General-purpose FPGA architectures; Genetic algorithm; Power,Cluster computing; Field programmable gate arrays (FPGA); Genetic algorithms; Integrated circuit design; Mathematical programming; Optimization; Reconfigurable hardware; Table lookup; Architectural parameters; Experimental methods; FPGA architectures; Geometric programming; High level architecture; Optimized architectures; Performance efficiency; Power; Analytical models
Area-aware decomposition for single-electron transistor arrays,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989315960&doi=10.1145%2f2898998&partnerID=40&md5=6e7681693472c0b35322d60b4d4a7cdd,"Single-electron transistor (SET) at room temperature has been demonstrated as a promising device for extending Moore's law due to its ultra-low power consumption. Existing SET synthesis methods synthesize a Boolean network into a large reconfigurable SET array where the height of SET array equals the number of primary inputs. However, recent experiments on device level have shown that this height is restricted to a small number, say, 10, rather than arbitrary value due to the ultra-low driving strength of SET devices. On the other hand, the width of an SET array is also suggested to be a small value. Consequently, it is necessary to decompose a large SET array into a set of small SET arrays where each of them realizes a sub-function of the original circuit with no more than 10 inputs. Thus, this article presents two techniques for achieving area-efficient SET array decomposition: One is a width minimization algorithm for reducing the area of a single SET array; the other is a depth-bounded mapping algorithm, which decomposes a Boolean network into many sub-functions such that the widths of the corresponding SET arrays are balanced. The width minimization algorithm leads to a 25%-41% improvement compared to the state of the art, and the mapping algorithm achieves a 60% reduction in total area compared to a naïve approach. © 2016 ACM.",Circuit synthesis; Low-power electronics; Minimization methods; Single-electron transistors; Singleelectron devices,Capacitance measurement; Conformal mapping; Field effect transistors; Low power electronics; Array decompositions; Circuit synthesis; Mapping algorithms; Minimization algorithms; Minimization methods; Single-electron devices; Synthesis method; Ultra-low power consumption; Single electron transistors
Partitioning and data mapping in reconfigurable cache and scratchpad memory-based architectures,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988385336&doi=10.1145%2f2934680&partnerID=40&md5=6be54f2db2836bbf85ff6a418eea27c8,"Scratchpad memory (SPM) is considered a useful component in the memory hierarchy, solely or along with caches, for meeting the power and energy constraints as performance ceases to be the sole criteria for processor design. Although the efficiency of SPM is well known, its use has been restricted owing to difficulties in programmability. Real applications usually have regions that are amenable to exploitation by either SPM or cache and hence can benefit if the two are used in conjunction. Dynamically adjusting the local memory resources to suit application demand can significantly improve the efficiency of the overall system. In this article, we propose a compiler technique to map application data objects to the SPM-cache and also partition the local memory between the SPM and cache depending on the dynamic requirement of the application. First, we introduce a novel graph-based structure to tackle data allocation in an application. Second, we use this to present a data allocation heuristic to map program objects for a fixed-size SPM-cache hybrid system that targets whole program optimization. We finally extend this formulation to adapt the SPM and cache sizes, as well as the data allocation as per the requirement of different application regions. We study the applicability of the technique on various workloads targeted at both SPM-only and hardware reconfigurable memory systems, observing an average of 18% energy-delay improvement over state-of-the-art techniques. © 2016 ACM.",Memory management strategies; Reconfigurable cache; Scratchpad memory optimization,Buffer storage; Graphic methods; Heuristic programming; Hybrid systems; Memory architecture; Optimization; Reconfigurable architectures; Reconfigurable hardware; Compiler techniques; Energy constraint; Memory management; Program optimization; Reconfigurable; Reconfigurable memory; Scratch pad memory; State-of-the-art techniques; Cache memory
Ripple 2.0: Improved movement of cells in routability-driven placement,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988375003&doi=10.1145%2f2925989&partnerID=40&md5=f1ff4273e52dec64f37137cfe3bb56a2,"Routability is one of the most important problems in high-performance circuit designs. From the viewpoint of placement design, two major factors cause routing congestion: (i) interconnections between cells and (ii) connections on macro blockages. In this article, we present a routability-driven placer, Ripple 2.0, which emphasizes both kinds of routing congestion. Several techniques will be presented, including (i) cell inflation with routing path consideration, (ii) congested cluster optimization, (iii) routability-driven cell spreading, and (iv) simultaneous routing and placement for routability refinement. With the official evaluation protocol, Ripple 2.0 outperforms other published academic routability-driven placers. Compared with top results in the ICCAD 2012 contest, Ripple 2.0 achieves a better detailed routing solution obtained by a commercial router. © 2016 ACM.",EDA; Placement; Routability; Routing; VLSI,Cells; Commercial routers; Evaluation protocol; High-performance circuits; Placement; Routability; Routing; Routing congestion; VLSI; Cytology
Hierarchical statistical leakage analysis and its application,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988005920&doi=10.1145%2f2896820&partnerID=40&md5=d0d298c8ced8101703de84a57a5fa508,"In this article, we investigate a hierarchical statistical leakage analysis (HSLA) design flow where modulelevel statistical leakage models supplied by IP vendors are used to improve the efficiency and capacity of SoC statistical leakage power analysis. To solve the challenges of incorporating spatial correlations between IP modules at system level, we first propose a method to extract correlation-inclusive leakage models. Then a method to handle the spatial correlations at system level is proposed. Using this method, the runtime of system statistical leakage analysis (SLA) can be significantly improved without disclosing the netlists of the IP modules. Experimental results demonstrate that the proposed HSLA method is about 100 times faster than gate-level full-chip SLA methods while maintaining the accuracy. In addition, we also investigate one application of this HSLA method, a leakage-yield-driven floorplanning framework, to demonstrate the benefits of such an HSLA method in practice. Moreover, an optimized hierarchical leakage analysis method dedicated to the floorplanning framework is proposed. The effectiveness of the floorplanning framework and the optimized method are confirmed by extensive experimental results. © 2016 ACM.",Hierarchical analysis; Leakage model extraction; Process variations; Statistical leakage analysis; Variation-aware floorplanning,Integrated circuit design; Leakage currents; Programmable logic controllers; Statistics; System-on-chip; Threshold voltage; Floor-planning; Hierarchical analysis; Leakage model; Process Variation; Statistical leakage analysis; Statistical methods
Preface to special section on new physical design techniques for the next generation of integration technology,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049120895&doi=10.1145%2f2902365&partnerID=40&md5=eea251da46eb20a3539a6f5ae8a6d762,[No abstract available],,
Error-correcting sample preparation with cyberphysical digital microfluidic lab-on-chip,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027244315&doi=10.1145%2f2898999&partnerID=40&md5=1cf37ddbec49883082d51b1c42038af2,"Digital (droplet-based) microfluidic technology offers an attractive platform for implementing a wide variety of biochemical laboratory protocols, such as point-of-care diagnosis, DNA analysis, target detection, and drug discovery. A digital microfluidic biochip consists of a patterned array of electrodes on which tiny fluid droplets are manipulated by electrical actuation sequences to perform various fluidic operations, for example, dispense, transport, mix, or split. However, because of the inherent uncertainty of fluidic operations, the outcome of biochemical experiments performed on-chip can be erroneous even if the chip is tested a priori and deemed to be defect-free. In this article, we address an important error recoverability problem in the context of sample preparation. We assume a cyberphysical environment, in which the physical errors, when detected online at selected checkpoints with integrated sensors, can be corrected through recovery techniques. However, almost all prior work on error recoverability used checkpointing-based rollback approach, that is, re-execution of certain portions of the protocol starting from the previous checkpoint. Unfortunately, such techniques are expensive both in terms of assay completion time and reagent cost, and can never ensure full error-recovery in deterministic sense. We consider imprecise droplet mix-split operations and present a novel roll-forward approach where the erroneous droplets, thus produced, are used in the error-recovery process, instead of being discarded or remixed. All erroneous droplets participate in the dilution process and they mutually cancel or reduce the concentration-error when the target droplet is reached. We also present a rigorous analysis that reveals the role of volumetric-error on the concentration of a sample to be prepared, and we describe the layout of a lab-on-chip that can execute the proposed cyberphysical dilution algorithm. Our analysis reveals that fluidic errors caused by unbalanced droplet splitting can be classified as being either critical or non-critical, and only those of the former type require correction to achieve error-free sample dilution. Simulation experiments on various sample preparation test cases demonstrate the effectiveness of the proposed method. © 2016 ACM",And Phrases: Algorithmic microfluidics; Biochips; Cyberphysical systems; Error-correction; Roll-forward; Sample preparation,Biochips; Drops; Embedded systems; Error correction; Recovery; Biochemical experiments; Biochemical laboratories; Cyber physical systems (CPSs); Digital microfluidic biochips; Electrical actuation; Microfluidic technologies; Roll-forward; Sample preparation; Digital microfluidics
Hierarchical dynamic thermal management method for high-performance many-core microprocessors,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042681588&doi=10.1145%2f2891409&partnerID=40&md5=7478f8a63ae2e7674e70afedd9a8a913,"It is challenging to manage the thermal behavior of many-core microprocessors while still keeping them running at high performance since the control complexity increases as the core number increases. In this article, a novel hierarchical dynamic thermal management method is proposed to overcome this challenge. The new method employs model predictive control (MPC) with task migration and a DVFS scheme to ensure smooth control behavior and negligible computing performance sacrifice. In order to be scalable to many-core systems, the hierarchical control scheme is designed with two levels. At the lower level, the cores are spatially clustered into blocks, and local task migration is used to match current power distribution with the optimal distribution calculated by MPC. At the upper level, global task migration is used with the unmatched powers from the lower level. A modified iterative minimum cut algorithm is used to assist the task migration decision making if the power number is large at the upper level. Finally, DVFS is applied to regulate the remaining unmatched powers. Experiments show that the new method outperforms existing methods and is very scalable to manage many-core microprocessors with small performance degradation. © 2016 ACM",And Phrases: Dynamic thermal management; DVFS; Many-core microprocessor; Model predictive control; Task migration,Decision making; Hierarchical systems; Iterative methods; Model predictive control; Predictive control systems; Temperature control; Computing performance; DVFS; Dynamic thermal management; Hierarchical control scheme; Many core; Optimal distributions; Performance degradation; Task migration; Thermal management (electronics)
A hardware-Assisted energy-efficient processing model for activity recognition using Wearables,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977100938&doi=10.1145%2f2886096&partnerID=40&md5=fb103e8fad6b5d5a91fcd1b6317886fd,"Wearables are being widely utilized in health and wellness applications, primarily due to the recent advances in sensor and wireless communication, which enhance the promise of wearable systems in providing continuous and real-Time monitoring and interventions. Wearables are generally composed of hardware/software components for collection, processing, and communication of physiological data. Practical implementation of wearable monitoring in real-life applications is currently limited due to notable obstacles. The wearability and form factor are dominated by the amount of energy needed for sensing, processing, and communication. In this article, we propose an ultra-low-power granular decision-making architecture, also called screening classifier, which can be viewed as a tiered wake-up circuitry, consuming three orders of magnitude-less power than the state-of-The-Art low-power microcontrollers. This processing model operates based on computationally simple template matching modules, based on coarse-to fine-grained analysis of the signals with on-demand and gradually increasing the processing power consumption. Initial template matching rejects signals that are clearly not of interest from the signal processing chain, keeping the rest of processing blocks idle. If the signal is likely of interest, the sensitivity and the power of the template matching modules are gradually increased, and ultimately, the main processing unit is activated. We pose optimization techniques to efficiently split a full template into smaller bins, called mini-Templates, and activate only a subset of bins during each classification decision. Our experimental results on real data show that this signal screening model reduces power consumption of the processing architecture by a factor of 70% while the sensitivity of detection remains at least 80%. © 2016 ACM.",body sensor networks; Medical embedded systems; power optimization; signal processing; wearable monitoring systems,Bins; Body sensor networks; Computer architecture; Continuous time systems; Decision making; Diagnosis; Electric power utilization; Embedded systems; Energy efficiency; Hardware; Low power electronics; Network architecture; Real time systems; Reconfigurable hardware; Signal processing; Template matching; Wearable computers; Wearable technology; Wireless telecommunication systems; Classification decision; Hardware/software components; Low-power microcontrollers; Monitoring system; Power Optimization; Processing architectures; Three orders of magnitude; Wireless communications; Wearable sensors
Cyber-physical co-simulation framework for smart cells in scalable battery packs,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977097692&doi=10.1145%2f2891407&partnerID=40&md5=bbf5d1c6110a52ac7c8ba25c682bef63,"This article introduces a Cyber-physical Co-Simulation Framework (CPCSF) for design and analysis of smart cells that enable scalable battery pack and Battery Management System (BMS) architectures. In contrast to conventional cells in battery packs, where all cells are monitored and controlled centrally, each smart cell is equipped with its own electronics in the form of a Cell Management Unit (CMU). The CMU maintains the cell in a safe and healthy operating state, while system-level battery management functions are performed by cooperation of the smart cells via communication. Here, the smart cells collaborate in a self-organizing fashion without a central controller instance. This enables maximum scalability and modularity, significantly simplifying integration of battery packs. However, for this emerging architecture, system-level design methodologies and tools have not been investigated yet. By contrast, components are developed individually and then manually tested in a hardware development platform. Consequently, the systematic design of the hardware/software architecture of smart cells requires a cyber-physical multi-level co-simulation of the network of smart cells that has to include all the components from the software, electronic, electric, and electrochemical domains. This comprises distributed BMS algorithms running on the CMUs, the communication network, control circuitry, cell balancing hardware, and battery cell behavior. For this purpose, we introduce a CPCSF that enables rapid design and analysis of smart cell hardware/software architectures. Our framework is then applied to investigate request-driven active cell balancing strategies that make use of the decentralized system architecture. In an exhaustive analysis on a realistic 21.6kW h Electric Vehicle (EV) battery pack containing 96 smart cells in series, the CPCSF is able to simulate hundreds of balancing runs together with all system characteristics, using the proposed request-driven balancing strategies at highest accuracy within an overall time frame of several hours. Consequently, the presented CPCSF for the first time allows us to quantitatively and qualitatively analyze the behavior of smart cell architectures for real-world applications. © 2016 ACM.",active cell balancingcell balancing strategy; battery management; co-simulation; Smart battery cells,Computer architecture; Computer software; Design; Electric batteries; Hardware; Network architecture; Reconfigurable hardware; Secondary batteries; Time series analysis; Balancing strategy; Battery cells; Battery Management; Cosimulation; Decentralized system; Emerging architectures; Hardware development; System characteristics; Battery management systems
Timing path-driven cycle cutting for sequential controllers,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977085477&doi=10.1145%2f2893473&partnerID=40&md5=f4dfd063250a1af837a58e4c8c9210a5,"Power and performance optimization of integrated circuits is performed by timing-driven algorithms that operate on directed acyclic graphs. Sequential circuits and circuits with topological feedback contain cycles. Cyclic circuits must be represented as directed acyclic graphs to be optimized and evaluated using static timing analysis. Algorithms in commercial electronic design automation tools generate the required acyclic graphs by cutting cycles without considering timing paths. This work reports on a method for generating directed acyclic circuit graphs that do not cut the specified timing paths. The algorithm is applied to over 125 benchmark designs and asynchronous handshake controllers. The runtime is less than 1 second, even for even the largest published controllers. Circuit timing graphs generated using this method retain the necessary timing paths, which enables circuit validation and optimization employing the commercial tools. Additional benefits show these designs are on an average a third in size, operate 33.3% faster, and consume one-fourth the energy. © 2016 ACM.",Asynchronous; design automation,Algorithms; Benchmarking; Computer aided design; Controllers; Cutting tools; Design; Directed graphs; Electric network analysis; Graph theory; Graphic methods; Optimization; Reconfigurable hardware; Asynchronous; Benchmark designs; Circuit validation; Commercial electronics; Design automations; Directed acyclic graph (DAG); Performance optimizations; Static timing analysis; Timing circuits
Improving PCM endurance with a constant-cost wear leveling design,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978151485&doi=10.1145%2f2905364&partnerID=40&md5=f44349f9b24a3b9fcb07ef27475843ee,"Improving PCM endurance is a fundamental issue when it is considered as an alternative to replace DRAM as main memory. Memory-based wear leveling (WL) is an effective way to improve PCM endurance, but its major challenge is how to efficiently determine the appropriate memory pages for allocation or swapping. In this article, we present a constant-cost WL design that is compatible with existing memory management. Two implementations, namely bucket-based and array-based WL, with constant-time (or nearly zero) search cost are proposed to be integrated into the OS layer and the hardware layer, respectively, as well as to trade between time and space complexity. The results of experiments conducted based on an implementation in Android, as well as simulations with popular benchmarks, to evaluate the effectiveness of the proposed design are very encouraging. © 2016 ACM.",Algorithms; Design; Management; Performance,Algorithms; Costs; Design; Management; Constant time; Main memory; Memory management; Memory pages; Performance; Search costs; Time and space complexity; Wear leveling; Dynamic random access storage
"Power, area, and performance optimization of standard cell memory arrays through controlled placement",2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974602988&doi=10.1145%2f2890498&partnerID=40&md5=8b670b57b8543caf7e424cd6c5cdb9c0,"Embedded memory remains a major bottleneck in current integrated circuit design in terms of silicon area, power dissipation, and performance; however, static random access memories (SRAMs) are almost exclusively supplied by a small number of vendors through memory generators, targeted at rather generic design specifications. As an alternative, standard cell memories (SCMs) can be defined, synthesized, and placed and routed as an integral part of a given digital system, providing complete design flexibility, good energy efficiency, low-voltage operation, and even area efficiency for small memory blocks. Yet implementing an SCM block with a standard digital flow often fails to exploit the distinct and regular structure of such an array, leaving room for optimization. In this article, we present a design methodology for optimizing the physical implementation of SCM macros as part of the standard design flow. This methodology introduces controlled placement, leading to a structured, noncongested layout with close to 100% placement utilization, resulting in a smaller silicon footprint, reduced wire length, and lower power consumption compared to SCMs without controlled placement. This methodology is demonstrated on SCM macros of various sizes and aspect ratios in a state-of-the-art 28nm fully depleted silicon-on-insulator technology, and compared with equivalent macros designed with the noncontrolled, standard flow, as well as with foundry-supplied SRAM macros. The controlled SCMs provide an average 25% reduction in area as compared to noncontrolled implementations while achieving a smaller size than SRAM macros of up to 1Kbyte. Power and performance comparisons of controlled SCM blocks of a commonly found 256 × 32 (1 Kbyte) memory with foundry-provided SRAMs show greater than 65% and 10% reduction in read and write power, respectively, while providing faster access than their SRAM counterparts, despite being of an aspect ratio that is typically unfavorable for SCMs. In addition, the SCM blocks function correctly with a supply voltage as low as 0.3V, well below the lower limit of even the SRAM macros optimized for low-voltage operation. The controlled placement methodology is applied within a full-chip physical implementation flow of an OpenRISC-based test chip, providing more than 50% power reduction compared to equivalently sized compiled SRAMs under a benchmark application. © 2016 ACM.",Controlled placement; Low power; Power-area-performance trade-off; Standard cell memories; Subthreshold operation,Aspect ratio; Benchmarking; Design; Economic and social effects; Energy efficiency; Foundries; Integrated circuit design; Memory architecture; Random access storage; Silicon; Silicon on insulator technology; Controlled placement; Low Power; Performance trade-off; Standard cell; Subthreshold operation; Static random access storage
Streaming sorting networks,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974530263&doi=10.1145%2f2854150&partnerID=40&md5=223a49e3be5079c774ac159bf1fd42c5,"Sorting is a fundamental problem in computer science and has been studied extensively. Thus, a large variety of sorting methods exist for both software and hardware implementations. For the latter, there is a trade-off between the throughput achieved and the cost (i.e., the logic and storage invested to sort n elements). Two popular solutions are bitonic sorting networks with O(nlog2 n) logic and storage, which sort n elements per cycle, and linear sorters with O(n) logic and storage, which sort n elements per n cycles. In this article, we present new hardware structures that we call streaming sorting networks, which we derive through a mathematical formalism that we introduce, and an accompanying domain-specific hardware generator that translates our formal mathematical description into synthesizable RTL Verilog. With the new networks, we achieve novel and improved cost-performance trade-offs. For example, assuming that n is a two-power and w is any divisor of n, one class of these networks can sort in n/w cycles with O(w log2 n) logic and O(nlog2 n) storage; the other class that we present sorts in nlog2 n/w cycles with O(w) logic and O(n) storage.We carefully analyze the performance of these networks and their cost at three levels of abstraction: (1) asymptotically, (2) exactly in terms of the number of basic elements needed, and (3) in terms of the resources required by the actual circuit when mapped to a field-programmable gate array. The accompanying hardware generator allows us to explore the entire design space, identify the Pareto-optimal solutions, and show superior cost-performance trade-offs compared to prior work.",Design space exploration; Hardware sorting; HDL generation,Computer circuits; Costs; Economic and social effects; Field programmable gate arrays (FPGA); Hardware; Pareto principle; Systems analysis; Design space exploration; Hardware structures; HDL generation; Levels of abstraction; Mathematical descriptions; Mathematical formalism; Pareto optimal solutions; Software and hardwares; Computer hardware description languages
Statistical rare-event analysis and parameter guidance by elite learning sample selection,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974623574&doi=10.1145%2f2875422&partnerID=40&md5=ddc3d8ea65065d8dadf2564caf6f80e4,"Accurately estimating the failure region of rare events for memory-cell and analog circuit blocks under process variations is a challenging task. In this article, we propose a new statistical method, called EliteScope, to estimate the circuit failure rates in rare-event regions and to provide conditions of parameters to achieve targeted performance. The new method is based on the iterative blockade framework to reduce the number of samples, but consists of two new techniques to improve existingmethods. First, the new approach employs an elite-learning sample-selection scheme, which can consider the effectiveness of samples and well coverage for the parameter space. As a result, it can reduce additional simulation costs by pruning less effective samples while keeping the accuracy of failure estimation. Second, the EliteScope identifies the failure regions in terms of parameter spaces to provide a good design guidance to accomplish the performance target. It applies variance-based feature selection to find the dominant parameters and then determine the in-spec boundaries of those parameters. We demonstrate the advantage of our proposed method using several memory and analog circuits with different numbers of process parameters. Experiments on four circuit examples show that EliteScope achieves a significant improvement on failure-region estimation in terms of accuracy and simulation cost over traditional approaches. The 16b 6T-SRAM column example also demonstrates that the new method is scalable for handling large problems with large numbers of process variables. © 2016 ACM.",Monte Carlo method; Process variations; Rare event analysis; Statistical analysis,Analog circuits; Cost estimating; Failure analysis; Iterative methods; Monte Carlo methods; Static random access storage; Failure estimation; Number of samples; Performance targets; Process parameters; Process Variables; Process Variation; Rare event analysis; Traditional approaches; Statistical methods
DC characteristics and variability on 90nm CMOS transistor array-style analog layout,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974627133&doi=10.1145%2f2888395&partnerID=40&md5=35834645189667c1fe477869befd772a,"In the MOS analog layout, variability suppression is becoming a major issue, as is layout efficiency. Introducing a transistor array (TA) style to analog layout, this article addresses the layout-dependent variability based on the measurement results of test chips on 90nm CMOS process. In TA style, a large transistor is decomposed into a set of unified subtransistors, which are connected in series or parallel. Focusing on one row layout of diffusion sharing for the multiple gates, we analyze the current direction-dependent variability and the leakage current via off-gates for the electrical isolation. Furthermore, we present several analog design cases on TA including analysis of the impact on the DC characteristics caused by the transistor channel decomposition. © 2016 ACM.",Diffusion sharing; Direct current characteristics; Isolation gates; Transistor array-style analog layout; Variability of threshold voltage,CMOS integrated circuits; Reconfigurable hardware; Threshold voltage; Transistors; Analog layout; DC characteristics; Direct current characteristics; Electrical isolation; Isolation gates; Layout dependents; Transistor arrays; Transistor channels; Integrated circuit layout
"Synthesis of dual-mode circuits through library design, gate sizing, and clock-tree optimization",2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974588055&doi=10.1145%2f2856032&partnerID=40&md5=42c0c6946277e4f6b7aa919a920e07c9,"A dual-mode circuit is a circuit that has two operating modes: a default high-performance mode at nominal voltage and a secondary low-performance near-threshold voltage (NTV) mode. A key problem that we address is to maximize NTV mode clock frequency. Some cells that are particularly slow in NTV mode are optimized through transistor sizing and stack removal; static noise margin of each gate is extracted and appended in a library so that function failures can be checked and removed during synthesis. A new gate-sizing algorithm is proposed that takes account of timing slacks at both modes. A new sensitivity measure is introduced for this purpose; binary search is then applied to find the maximum NTV mode frequency. Clock-tree synthesis is reformulated to minimize clock skew at both modes. This is motivated by the fact that the proportion of load-dependent delay along clock paths, as well as clock-path delays themselves, should be made equal. Experiments on some test circuits indicate that NTV mode clock period is reduced by 24%, on average; clock skew at NTV decreases by 13%, on average; and NTV mode energy-delay product is reduced by 20%, on average. © 2016 ACM.",Clock-tree optimization; Dual-mode circuit; Gate sizing; Near-threshold voltage; Timing optimization,Clock distribution networks; Clocks; Delay circuits; Electric clocks; Forestry; Gates (transistor); Reconfigurable hardware; Threshold voltage; Timing circuits; Clock tree optimization; Clock tree synthesis; Dual modes; Gate sizing; Sensitivity measures; Static noise margin; Timing optimization; Transistor sizing; Computer circuits
FORTIS: A comprehensive solution for establishing forward trust for protecting IPs and ICs,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974528123&doi=10.1145%2f2893183&partnerID=40&md5=e7e3da702e3522dd5fc59a863ad33935,"With the advent of globalization in the semiconductor industry, it is necessary to prevent unauthorized usage of third-party IPs (3PIPs), cloning and unwanted modification of 3PIPs, and unauthorized production of ICs. Due to the increasing complexity of ICs, system-on-chip (SoC) designers use various 3PIPs in their design to reduce time-to-market and development costs, which creates a trust issue between the SoC designer and the IP owners. In addition, as the ICs are fabricated around the globe, the SoC designers give fabrication contracts to offshore foundries to manufacture ICs and have little control over the fabrication process, including the total number of chips fabricated. Similarly, the 3PIP owners lack control over the number of fabricated chips and/or the usage of their IPs in an SoC. Existing research only partially addresses the problems of IP piracy and IC overproduction, and to the best of our knowledge, there is no work that considers IP overuse. In this article, we present a comprehensive solution for preventing IP piracy and IC overproduction by assuring forward trust between all entities involved in the SoC design and fabrication process. We propose a novel design flow to prevent IC overproduction and IP overuse. We use an existing logic encryption technique to obfuscate the netlist of an SoC or a 3PIP and propose a modification to enable manufacturing tests before the activation of chips which is absolutely necessary to prevent overproduction. We have used asymmetric and symmetric key encryption, in a fashion similar to Pretty Good Privacy (PGP), to transfer keys from the SoC designer or 3PIP owners to the chips. In addition, we also propose to attach an IP digest (a cryptographic hash of the entire IP) to the header of an IP to prevent modification of the IP by the SoC designers. We have shown that our approach is resistant to various attacks with the cost of minimal area overhead. © 2016 ACM.",3PIP; Encryption; IC overproduction; IP overuse; Supply chain; System-on-chip,Crime; Cryptography; Fabrication; Integrated circuits; Process control; Programmable logic controllers; Semiconductor device manufacture; Supply chains; System-on-chip; Timing circuits; 3PIP; Cryptographic hashes; Encryption technique; IP overuse; Manufacturing tests; Semiconductor industry; Symmetric key encryption; System on chips (SoC); Integrated circuit design
PARR: Pin-access planning and regular routing for self-aligned double patterning,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974575195&doi=10.1145%2f2842612&partnerID=40&md5=2eb65f7d53a95adc4c879c2826fc6498,"Pin access has become one of the most difficult challenges for detailed routing in advanced technology nodes, for example, in 14nm and below, for which double-patterning lithography has to be used for manufacturing lower metal routing layers with tight pitches, such as M2 and M3. Self-aligned double patterning (SADP) provides better control on line edge roughness and overlay, but it has very restrictive design constraints and prefers regular layout patterns. This article presents a comprehensive pin-access planning and regular routing framework (PARR) for SADP friendliness. Our key techniques include precomputation of both intracell and intercell pin accessibility, as well as local and global pin-access planning to enable handshaking between standard cell-level pin access and detailed routing under SADP constraints. A pin access-driven rip-up and reroute scheme is proposed to improve the ultimate routability. Our experimental results demonstrate that PARR can achieve much better routability and overlay control compared with previous approaches. © 2016 ACM.",Net deferring; Pin access; Regular layout; SADP,Automation; Advanced technology; Design constraints; Line Edge Roughness; Net deferring; Pin access; Regular layout; SADP; Self-aligned double patterning; Computer applications
"Resource sharing centric dynamic voltage and frequency scaling for CMP cores, uncore, and memory",2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974559935&doi=10.1145%2f2897394&partnerID=40&md5=371494ad9859a1198dafb8b3f18079c8,"With the breakdown of Dennard's scaling over the past decade, performance growth of modern microprocessor design has largely relied on scaling core count in chip multiprocessors (CMPs). The challenge of chip power density, however, remains and demands new power management solutions. This work investigates a coordinated CMP systemwide Dynamic Voltage and Frequency Scaling (DVFS) policy centered around shared resource utilization. This approach represents a new angle on the problem, differing from the conventional core-workload-driven approaches. The key component of our work is per-core DVFS leveraging a technique similar to TCP Vegas congestion control from networking. This TCP Vegas-based DVFS can potentially identify the synergy between power reduction and performance improvement. Further, this work includes uncore (on-chip interconnect and shared last level cache) and main memory DVFS policies coordinated with the per-core DVFS policy. Full system simulations on PARSEC benchmarks show that our technique reduces total energy dissipation by over 47% across all benchmarks with less than 2.3% performance degradation. Our work also leads to 12% more energy savings compared to a prior work CMP DVFS policy. © 2016 ACM.",Chip multi-processor; Coordination; Cores; Dynamic voltage frequency scaling; Memory; Power management; Resource sharing,Benchmarking; Cache memory; Data storage equipment; Embedded systems; Energy conservation; Energy dissipation; Energy management; Multiprocessing systems; Power management; Systems analysis; Transmission control protocol; Voltage scaling; Chip multi-processors; Coordination; Cores; Dynamic voltage frequency scaling; Resource sharing; Dynamic frequency scaling
Accurate modeling of nonideal low-power PWM DC-DC converters operating in CCM and DCM using enhanced circuit-averaging techniques,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974527497&doi=10.1145%2f2890500&partnerID=40&md5=30ba559f1ddcdcc388efc535f2dbcf0c,"The development of enhanced modeling techniques for the simulation of switched-mode Pulse Width Modulated (PWM) DC-DC power converters using circuit averaging is the main focus of this article. The circuitaveraging technique has traditionally been used to model the behavior of PWM DC-DC converters without considering important nonideal characteristics of the switching devices. As a result, most of these existing approaches present simplified models that are ideal or linearized, and do not accurately account for the performance characteristics of the converter. This is especially problematic for low-power applications. In this article, we present an enhanced nonideal behavioral circuit-averaged model that makes the simulation of DC-DC converters both computationally efficient and accurate, thereby presenting an important tool for circuit designers. Experimentally, we show that our Verilog-A-based new model allows for accurate simulation of both Buck- and Boost-type PWM converters operating in either CCM or DCM modes while providing more than one order of magnitude speedup over the transistor-level simulation. © 2016 ACM.",Circuit averaging; DC-DC converters; Integrated circuit modeling; Switched-mode power supplies,Counting circuits; Electric inverters; Low power electronics; Reconfigurable hardware; Switched mode power supplies; Circuit averaging; Circuit-averaging techniques; Computationally efficient; Integrated circuit modeling; Low power application; Non-ideal characteristics; Performance characteristics; Transistor-level simulation; DC-DC converters
Ensemble reduction via logic minimization,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974577919&doi=10.1145%2f2897515&partnerID=40&md5=27cbd3561f820262bb9e0327b415b223,"An ensemble of machine learning classifiers usually improves generalization performance and is useful for many applications. However, the extra memory storage and computational cost incurred from the combined models often limits their potential applications. In this article, we propose a new ensemble reduction method called CANOPY that significantly reduces memory storage and computations. CANOPY uses a technique from logic minimization for digital circuits to select and combine particular classification models from an initial pool in the form of a Boolean function, through which the reduced ensemble performs classification. Experiments on 20 UCI datasets demonstrate that CANOPY either outperforms or is very competitive with the initial ensemble and one state-of-the-art ensemble reduction method in terms of generalization error, and is superior to all existing reduction methods surveyed for identifying the smallest numbers of models in the reduced ensembles. © 2016 ACM.",Ensemble reduction; Logic minimization; Machine learning,Artificial intelligence; Boolean functions; Computation theory; Learning systems; Logic circuits; Logic Synthesis; Reconfigurable hardware; Classification models; Combined model; Computational costs; Generalization Error; Generalization performance; Logic minimization; Reduction method; State of the art; Computer circuits
Analytical clustering score with application to postplacement register clustering,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974539567&doi=10.1145%2f2894753&partnerID=40&md5=6a635eba2fc27d40429f42c7a7252069,"Circuit clustering is usually done through discrete optimizations to enable circuit size reduction or design-specific cluster formation. In this article, we are interested in the register-clustering technique for clock-power reduction by leveraging new opportunities introduced by multibit flip-flop (MBFF). Currently, INTEGRA is the only existing postplacement MBFF clustering optimizer with a subquadratic time complexity. However, it severely degrades the wirelength, especially for realistic designs, which may nullify the benefits of MBFF clustering. In contrast, we formulate an analytical clustering score with a nonlinear programming framework, in which the wirelength objective can be seamlessly integrated and the solver has empirical subquadratic time complexity. With the MBFF library, the application of our analytical clustering method achieves comparable clock power to the state-of-the-art techniques, but further reduces the wirelength by about 25%. Even without the MBFF library, we can still achieve 30% clock wirelength reduction. In addition, the proposed method can potentially be integrated into an in-placement MBFF clustering solver and be applied to other problems that require formulating clustering scores in their objective functions. © 2016 ACM.",Clock power; Multibit flip-flops; Placement; Register clustering; Timing,Clocks; Cluster analysis; Flip flop circuits; Nonlinear programming; Clock power reductions; Clustering techniques; Discrete optimization; Multibit flip-flops; Placement; Register clustering; State-of-the-art techniques; Timing; Reduction
State assignment and optimization of ultra-high-speed FSMs utilizing tristate buffers,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973482308&doi=10.1145%2f2905366&partnerID=40&md5=5be2de9a9a0732a47380b8c2c04b6770,"The logic synthesis of ultra-high-speed FSMs is presented. The state assignment is based on a well-known method that uses output vectors. This technique is adjusted to include elements of two-level minimization and takes into account the limited number of terms contained in the programmable-AND/fixed-OR logic cell. The state assignment is based on a special form of the binary decision tree. The second phase of the FSM design is logic optimization. The optimization method is based on tristate buffers, thus making possible a one-logic-level FSM structure. The key point is to search partition variables that control the tristate buffers. This technique can also be applied to combinational circuits or the output block of FSMs only. Algorithms for state assignment and optimization are presented and richly illustrated by examples. The method is dedicated to using specific features of complex programmable logic devices. Experimental results prove its effectiveness (e.g., the implementation of the the 16-bit counter requires 136 logic cells and onelogic-cell level instead of 213 cells and four levels). The optimization method using tristate buffers and a state assignment binary decision tree can be directly applied to FPGA-dedicated logic synthesis. © 2016 ACM.",Binary decision tree; Finite state machines; Logic optimization; Logic synthesis; State assignment; Technology mapping; Tristate buffers,Binary trees; Bins; Cells; Codes (symbols); Cytology; Decision trees; Finite automata; Logic circuits; Logic devices; Logic Synthesis; Reconfigurable hardware; State assignment; Binary decision trees; Complex programmable logic device; Logic optimization; Optimization method; Output vectors; Technology mapping; Tristate buffers; Ultra high speed; Computer circuits
Periodic scan-in states to reduce the input test data volume for partially functional broadside tests,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973462891&doi=10.1145%2f2911983&partnerID=40&md5=fffdd4a0a12f5a84d3504be36fbf1e44,"This article describes a procedure for test data compression targeting functional and partially functional broadside tests. The scan-in state of such a test is either a reachable state or has a known Hamming distance from a reachable state. Reachable states are fully specified, while the popular LFSR-based test data compression methods require the use of incompletely specified test cubes. The test data compression approach considered in this article is based on the use of periodic scan-in states. Such states require the storage of a period that can be significantly shorter than a scan-in state, thus providing test data compression. The procedure computes a set of periods that is sufficient for detecting all the detectable target faults. Considering the scan-in states that the periods produce, the procedure ranks the periods based on the distances of the scan-in states from reachable states, and the lengths of the periods. Functional and partially functional broadside tests are generated preferring shorter periods with smaller Hamming distances. The results are compared with those of an LFSR-based approach. © 2016 ACM.",Functional broadside tests; Scan circuits; Test data compression; Transition faults,Digital storage; Hamming distance; Functional broadside tests; Input tests; Scan circuits; Target faults; Test cube; Test Data Compression; Transition faults; Data compression
Index-resilient zero-suppressed BDDs: Definition and operations,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973308962&doi=10.1145%2f2905363&partnerID=40&md5=34ea85fdde1ef08a1738846900829658,"Zero-Suppressed Binary Decision Diagrams (ZDDs) are widely used data structures for representing and handling combination sets and Boolean functions. In particular, ZDDs are commonly used in CAD for the synthesis and verification of integrated circuits. The purpose of this article is to design an error-resilient version of this data structure: a self-repairing ZDD. More precisely, we design a new ZDD canonical form, called index-resilient reduced ZDD, such that a faulty index can be reconstructed in time O(k), where k is the number of nodes with a corrupted index. Moreover, we propose new versions of the standard algorithms for ZDD manipulation and construction that are error resilient during their execution and produce an index-resilient ZDD as output. The experimental results validate the proposed approach. © 2016 ACM.",Binary decision diagrams; Data structures for logic synthesis; Error resilient data structures; Zero-suppressed binary decision diagrams,Bins; Boolean functions; Computer aided design; Data structures; Errors; Logic Synthesis; Reconfigurable hardware; Canonical form; Error-resilient; Self repairing; Standard algorithms; Zero-suppressed binary decision diagrams; Binary decision diagrams
Library-based placement and routing in FPGAs with support of partial reconfiguration,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973300168&doi=10.1145%2f2901295&partnerID=40&md5=8cd94dc032ccf2a7a039078d8a4a718c,"While traditional Field-Programmable Gate Array design flow usually employs fine-grained tile-based placement, modular placement is increasingly required to speed up the large-scale placement and save the synthesis time. Moreover, the commonly used modules can be pre-synthesized and stored in the library for design reuse to significantly save the design, verification time, and development cost. Previous work mainly focuses on modular floorplanning without module placement information. In this article, we propose a library-based placement and routing flow that best utilizes the pre-placed and routed modules from the library to significantly save the execution time while achieving the minimal area-delay product. The flow supports the static and reconfigurable modules at the same time. The modular information is represented in the B∗-Tree structure, and the B∗-Tree operations are amended together with Simulated Annealing to enable a fast search of the placement space. Different width-height ratios of the modules are exploited to achieve areadelay product optimization. Partial reconfiguration-aware routing using pin-to-wire abutment is proposed to connect the modules after placement. Our placer can reduce the compilation time by 65% on average with 17% area and 8.2% delay overhead compared with the fine-grained results of Versatile Place and Route through the reuse of module information in the library for the base architecture. For other architectures, the area increase ranges from 8.32% to 25.79%, the delay varies from -13.66% to 19.79%, and the runtime improves by 43.31% to 77.2%. © 2016 ACM.",-tree; B; FPGA; Partial reconfiguration; Placement; Routing,Boron; Field programmable gate arrays (FPGA); Forestry; Integrated circuit design; Logic Synthesis; Simulated annealing; Trees (mathematics); -tree; Development costs; Partial reconfiguration; Placement; Placement and routing; Product optimization; Reconfigurable module; Routing; Reconfigurable hardware
Performance evaluation of NoC-based multicore systems: From traffic analysis to NoC latency modeling,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971384407&doi=10.1145%2f2870633&partnerID=40&md5=e80e96d2a28c0c05e6e9e097b10c1f06,"In this survey, we review several approaches for predicting performance of Network-on-Chip (NoC)-based multicore systems, starting from the traffic models to the complex NoC models for latency evaluation. We first review typical traffic models to represent the application workloads in NoC. Specifically, we review Markovian and non-Markovian (e.g., self-similar or long-range memory-dependent) traffic models and discuss their applications on multicore platform design. Then, we review the analytical techniques to predict NoC performance under given input traffic. We investigate analytical models for average as well as maximum delay evaluation. We also review the developments and design challenges of NoC simulators. One interesting research direction in NoC performance evaluation consists of combining simulation and analytical models in order to exploit their advantages together. Toward this end, we discuss several newly proposed approaches that use hardware-based or learning-based techniques. Finally, we summarize several open problems and our perspective to address these challenges. © 2016 ACM.",Analytical model; Average and maximum delay; General and reference → Surveys and overviews; Network-on-chips (NoCs); Networks → Network on chip; Performance evaluation; Simulation; Traffic models,Analytical models; Distributed computer systems; Fault tolerant computer systems; Network-on-chip; Routers; Servers; Surveys; Design challenges; Maximum delay; Multi-core platforms; Multi-core systems; Network on chip (NoC); Performance evaluations; Simulation; Traffic model; Integrated circuit design
Efficient security monitoring with the core debug interface in an embedded processor,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973473702&doi=10.1145%2f2907611&partnerID=40&md5=758a0a7e81960e194ae69c1a60cd480e,"For decades, various concepts in security monitoring have been proposed. In principle, they all in common in regard to the monitoring of the execution behavior of a program (e.g., control-flow or dataflow) running on the machine to find symptoms of attacks. Among the proposed monitoring schemes, software-based ones are known for their adaptability on the commercial products, but there have been concerns that they may suffer from nonnegligible runtime overhead. On the other hand, hardware-based solutions are recognized for their high performance. However, most of them have an inherent problem in that they usually mandate drastic changes to the internal processor architecture. More recent ones have strived to minimize such modifications by employing external hardware security monitors in the system. However, these approaches intrinsically suffer from the overhead caused by communication between the host and the external monitor. Our solution also relies on external hardware for security monitoring, but unlike the others, ours tackles the communication overhead by using the core debug interface (CDI), which is readily available in most commercial processors for debugging. We build our system simply by plugging our monitoring hardware into the processor via CDI, precluding the need for altering the processor internals. To validate the effectiveness of our approach, we implement two well-known monitoring techniques on our proposed framework: dynamic information flow tracking and branch regulation. The experimental results on our FPGA prototype show that our external hardware monitors efficiently perform monitoring tasks with negligible performance overhead, mainly with thanks to the support of CDI, which helps us reduce communication costs substantially. © 2016 ACM.",Code reuse attack detection; Core debug interface (CDI); Dynamic information flow tracking (DIFT); Security monitoring,Computer debugging; Hardware; Monitoring; Program processors; Attack detection; Commercial products; Communication overheads; Debug interfaces; Dynamic information flow tracking; Monitoring techniques; Processor architectures; Security monitoring; Hardware security
Efficient algorithms for discrete gate sizing and threshold voltage assignment based on an accurate analytical statistical yield gradient,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973351616&doi=10.1145%2f2896819&partnerID=40&md5=adaec62968ab7789af208d2c1d4e287e,"In this article, we derive a simple and accurate expression for the change in timing yield due to a change in the gate delay distribution. It is based on analytical bounds that we have derived for the moments of the circuit and path delay. Based on this, we propose computationally efficient algorithms for (1) discrete gate sizing and (2) simultaneous gate sizing and threshold voltage (VT) assignment so that the circuit meets a timing yield specification under parameter variations. The use of this analytical yield gradient within a gradient-based timing yield optimization algorithm results in a significant improvement in the runtime as compared to the numerical method, while achieving the same final yield. It also allows us to explore a larger search space in each iteration more efficiently, which is required in the case of simultaneous resizing and VT assignment. We also propose heuristics for resizing/changing the VT of multiple gates in each iteration. This makes it possible to optimize the timing yield for large circuits. Results on ITC '99 benchmarks show that the proposed multinode resizing algorithm results in a significant improvement in the runtime with a marginal average area penalty and no cost to the final yield achieved. © 2016 ACM.",Discrete gate sizing; Timing yield; V<sub>T</sub> assignment; Yield optimization,Algorithms; Computer circuits; Delay circuits; Gates (transistor); Iterative methods; Numerical methods; Optimization; Reconfigurable hardware; Threshold voltage; Analytical bounds; Computationally efficient; Gate sizing; Gradient based; Large circuits; Timing yield; Voltage assignment; Yield optimization; Timing circuits
Obstacle-avoiding wind turbine placement for power loss and wake effect optimization,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973481513&doi=10.1145%2f2905365&partnerID=40&md5=6348cd84ba4448076ae12810ff2b0faa,"As finite energy resources are being consumed at faster rate than they can be replaced, renewable energy resources have drawn extensive attention. Wind power development is one such example growing significantly throughout the world. The main difficulty in wind power development is that wind turbines interfere with each other. The produced turbulence - wake effect - directly reduces the power generation. In addition, wirelength of the collection network among wind turbines is not merely an economic factor; it also decides power loss on the wind farm. Moreover, in reality, obstacles (buildings, lakes, etc.) exist on the wind farm, which are unavoidable. Nevertheless, to the best of our knowledge, none of the existing works consider wake effect, wirelength, and avoidance of obstacles all together in the wind turbine placement problem. In this article, we propose an analytical method to obtain the obstacle-avoiding placement of wind turbines, thus minimizing both power loss and wake effect. We also propose a postprocessing method to fine-tune the solution obtained from the analytical method to find a better solution. Simulation results show that our tool is 12x faster than the state-of-the-art industrial tool AWS OpenWind and 203x faster than the state-of-the-art academic tool TDA with almost the same produced power. © 2016 ACM.",Circuit; Placement; Wind turbine,Electric utilities; Energy resources; Networks (circuits); Renewable energy resources; Wakes; Wind power; Analytical method; Economic factors; Industrial tools; Obstacle-avoiding; Placement; Placement problems; Postprocessing methods; Wind power development; Wind turbines
Construction of reconfigurable clock trees for MCMM designs using mode separation and scenario compression,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973341546&doi=10.1145%2f2883609&partnerID=40&md5=b57f95ab60d19e135279849aeb3e4a47,"The clock networks of many modern circuits have to operate in multiple corners and multiple modes (MCMM). We propose to construct mode-reconfigurable clock trees (MRCTs) based on mode separation and scenario compression. The technique of scenario compression is proposed to consider the timing constraints in multiple scenarios at the same time, compressing the MCMM problem into an equivalent single-corner multiple-mode (SCMM), orsingle-corner single-mode (SCSM) problem. The compression is performed by combining the skew constraints of the different scenarios in skew constraint graphs based on delay linearization and dominating skew constraints. An MRCT consists of several clock trees and mode separation involves, depending on the active mode, selecting one of the clock trees to deliver the clock signal. To limit the overhead, the bottom part (closer to the clock sinks) of all the different clock trees are shared and only the top part (closer to the clock source) of the clock network is mode reconfigurable. The reconfiguration is realized using OR-gates and a one-input-multiple-output demultiplexer. The experimental results show that for a set of synthesized MCMM circuits, with 715 to 13,216 sequential elements, the proposed approach can achieve high yield. © 2016 ACM.",Clock networks; Clock skew; Reconfigurable,Clock distribution networks; Electric clocks; Forestry; Networks (circuits); Reconfigurable hardware; Separation; Timing circuits; Clock network; Clock skews; Constraint graph; Mode separation; Multiple outputs; Reconfigurable; Sequential elements; Timing constraints; Clocks
N-detection test sets for circuits with multiple independent scan chains,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973373587&doi=10.1145%2f2897514&partnerID=40&md5=5dd2ec37b973b05c25fb845089c0556e,"In a circuit with multiple independent scan chains, it is possible to operate groups of scan chains independently in functional or shift mode. This design-for-testability approach can be used to increase the quality of a test set. This article describes an N-detection test generation procedure for increasing the quality of a transition fault test set in such a circuit. The procedure uses the possibility of applying the same test, with the scan chains operating in different modes, to increase the numbers of detections without increasing the number of tests that need to be generated or stored on a tester. This results in reduced input storage requirements compared with a conventional N-detection test set and an increased number of applied tests. The increased quality of the test set is verified by its bridging fault coverage. © 2016 ACM.",Broadside tests; Design-for-testability; Scan circuits; Skewed-load tests; Transition faults,Chains; Design for testability; Dynamic random access storage; Information dissemination; Reconfigurable hardware; Broadside tests; Detection tests; N-detection test sets; Scan circuits; Skewed-load tests; Storage requirements; Transition fault tests; Transition faults; Testing
On battery recovery effect in wireless sensor nodes,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973300700&doi=10.1145%2f2890501&partnerID=40&md5=de53d426b58ca9c57d04a9f8da3029f2,"With the perennial demand for longer runtime of battery-powered Wireless Sensor Nodes (WSNs), several techniques have been proposed to increase the battery runtime. One such class of techniques exploiting the battery recovery effect phenomenon claims that performing an intermittent discharge instead of a continuous discharge will increase the usable battery capacity. Several works in the areas of embedded systems and wireless sensor networks have assumed the existence of this recovery effect and proposed different power management techniques in the form of power supply architectures (multiple battery setup) and communication protocols (burst mode transmission) in order to exploit it. However, until now, a systematic experimental evaluation of the recovery effect has not been performed with real battery cells, using high-accuracy battery testers to confirm the existence of this recovery phenomenon. In this article, a systematic evaluation procedure is developed to verify the existence of this battery recovery effect. Using our evaluation procedure, we investigated Alkaline, Nickel-Metal Hydride (NiMH), and Lithium-Ion (Li-Ion) battery chemistries, which are commonly used as power supplies for Wireless Sensor Node (WSN) applications. Our experimental results do not show any evidence of the aforementioned recovery effect in these battery chemistries. In particular, our results show a significant deviation from the stochastic battery models, which were used by many power management techniques. Therefore, the existing power management approaches that rely on this recovery effect do not hold in practice. Instead of a battery recovery effect, our experimental results show the existence of the rate capacity effect, which is the reduction of usable battery capacity with higher discharge power, to be the dominant electrochemical phenomenon that should be considered for maximizing the runtime of WSN applications. We outline power management techniques that minimize the rate capacity effect in order to obtain a higher energy output from the battery. © 2016 ACM.",Batteries; Battery modeling; Battery-operated electronics; Power management; Recovery effect; Wireless sensor nodes,Battery management systems; Electric batteries; Embedded systems; Energy management; Hydrides; Industrial management; Lithium alloys; Lithium-ion batteries; Molecular biology; Network architecture; Nickel metal hydride batteries; Power management; Recovery; Sensor nodes; Solar cells; Speed control; Stochastic models; Stochastic systems; Battery modeling; Burst mode transmission; Electrochemical phenomena; Experimental evaluation; Intermittent discharge; Power management techniques; Recovery effects; Wireless sensor node; Wireless sensor networks
"A framework for block placement, migration, and fast searching in Tiled-DNUCA architecture",2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973480979&doi=10.1145%2f2907946&partnerID=40&md5=c4f6307c9011532f77dae9259abff796,"Multicore processors have proliferated several domains ranging from small-scale embedded systems to large data centers, making tiled CMPs (TCMPs) the essential next-generation scalable architecture. NUCA architectures help in managing the capacity and access time for such larger cache designs. It divides the last-level cache (LLC) into multiple banks connected through an on-chip network. Static NUCA (SNUCA) has a fixed address mapping policy, whereas dynamic NUCA (DNUCA) allows blocks to relocate nearer to the processing cores at runtime. To allow this, DNUCA divides the banks into multiple banksets and a block can be placed in any bank within a particular bankset. The entire bankset may need to be searched to access a block. Optimal bankset searching mechanisms are essential for getting the benefits from DNUCA. This article proposes a DNUCA-based TCMP architecture called TLD-NUCA. It reduces the LLC access time of TCMP and also allows a heavily loaded bank to distribute its load among the underused banks. Instead of other DNUCA designs, TLD-NUCA considers larger banksets. Such relaxations result in more uniform load distribution than existing DNUCA-based TCMP (T-DNUCA). Considering larger banksets improves the utilization factor, but T-DNUCA cannot implement it because of its expensive searching mechanism. TLD-NUCA uses a centralized directory, called TLD, to search a block from all the banks. Also, the proposed block placement policy reduces the instances when the central TLD needs to be contacted. It does not require the expensive simultaneous search as needed by T-DNUCA. Better cache utilization and a reduction in LLC access time improve the miss rate as well as the average memory access time (AMAT). Improving the miss rate and AMAT results in improvements in cycles per instructions (CPI). Experimental analysis found that TLD-NUCA improves performance by 6.5% as compared to T-DNUCA. The improvement is 13% as compared to the SNUCA-based TCMP design. © 2016 ACM.",Cache utilization; CMP; Last-level cache; NUCA; Tiled-CMP,Embedded systems; Network architecture; Online searching; Average memory access time; Cache utilization; Cycles per instructions; Last-level caches; Lastlevel caches (LLC); NUCA; Scalable architectures; Tiled CMP; Memory architecture
Hardware trojans: Lessons learned after one decade of research,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973470936&doi=10.1145%2f2906147&partnerID=40&md5=e8c675a7a2f4190c3e470fe3bb1d78ff,"Given the increasing complexity of modern electronics and the cost of fabrication, entities from around the globe have become more heavily involved in all phases of the electronics supply chain. In this environment, hardware Trojans (i.e., malicious modifications or inclusions made by untrusted third parties) pose major security concerns, especially for those integrated circuits (ICs) and systems used in critical applications and cyber infrastructure. While hardware Trojans have been explored significantly in academia over the last decade, there remains room for improvement. In this article, we examine the research on hardware Trojans from the last decade and attempt to capture the lessons learned. A comprehensive adversarial model taxonomy is introduced and used to examine the current state of the art. Then the past countermeasures and publication trends are categorized based on the adversarial model and topic. Through this analysis, we identify what has been covered and the important problems that are underinvestigated. We also identify the most critical lessons for those new to the field and suggest a roadmap for future hardware Trojan research. © 2016 ACM.",Attack model; Countermeasures; Hardware security and trust; Hardware Trojan attacks,Hardware; Integrated circuits; Malware; Radar countermeasures; Supply chains; Attack model; Critical applications; Cyber infrastructures; Electronics supply chain; Integrated circuits (ICs); Model taxonomy; Security and trusts; State of the art; Hardware security
An effective chemical mechanical polishing fill insertion approach,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973867737&doi=10.1145%2f2886097&partnerID=40&md5=3e5d347c64b65ecba12c888abdcb060b,"To reduce chip-scale topography variation, dummy fill is commonly used to improve the layout density uniformity. Previous works either sought the most uniform density distribution or sought to minimize the inserted dummy fills while satisfying certain density uniformity constraint. However, due to more stringent manufacturing challenges, more criteria, like line deviation and outlier, emerge at newer technology nodes. This article presents a joint optimization scheme to consider variation, total fill, line deviation, outlier, overlap, and running time simultaneously. More specifically, first we decompose the rectilinear polygons and partition fillable regions into rectangles for easier processing. After decomposition, we insert dummy fills into the fillable rectangular regions optimizing the fill metrics simultaneously. We propose three approaches, Fast Median approach, LP approach, and Iterative approach, which are much faster with better quality, compared with the results of the top three contestants in the ICCAD Contest 2014. © 2016 ACM.",Fill insertion; Filling optimization; Overlap optimization; Polygon decomposition,Geometry; Statistics; Density distributions; Density uniformity; Iterative approach; Joint optimization; Manufacturing challenges; Polygon decomposition; Rectangular regions; Technology nodes; Chemical mechanical polishing
FH-OAOS: A fast four-step heuristic for obstacle-avoiding octilinear Steiner tree construction,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968830280&doi=10.1145%2f2856033&partnerID=40&md5=92d03a6ecf05d1b8795c0e4cf0a7791c,"With the sharp increase of very large-scale integrated (VLSI) circuit density, we are faced with many knotty issues. Particularly in the routing phase of VLSI physical design, the interconnection effects directly relate to the final performance of circuits. However, the optimization capability of traditional rectilinear architecture is limited; thus, both academia and industry have been devoted to nonrectilinear architecture in recent years, especially octilinear architecture, which is the most promising because it can greatly improve the performance of modern chips. In this article, we design FH-OAOS, an obstacle-avoiding algorithm in octilinear architecture, by constructing an obstacle-avoiding the octilinear Steiner minimal tree (OAOSMT). Our approach first constructs an obstacle-free Euclidean minimal spanning tree (OFEMST) on the given pins based on Delaunay triangulation (DT). Then, two lookup tables about OFEMST's edge are generated, which can be seen as the information center of FH-OAOS and can provide information support for algorithm operation. Next, an efficient obstacle-avoiding strategy is proposed to convert the OFEMST into an obstacleavoiding octilinear Steiner tree (OAOST). Finally, the generated OAOST is refined to construct the final OAOSMT by applying three effective strategies. Experimental results on various benchmarks show that FH-OAOS achieves 66.39 times speedup on average, while the average wirelength of the final OAOSMT is only 0.36% larger than the best existing solution. © 2016 ACM.",Obstacle; Octilinear architecture; Router; Steiner tree; VLSI design,Integrated circuit design; Optimization; Reconfigurable hardware; Routers; Table lookup; Trees (mathematics); Delaunay triangulation; Obstacle; Obstacle avoiding algorithms; Optimization capabilities; Steiner tree construction; Steiner trees; Very large scale integrated; VLSI design; VLSI circuits
Eh?Placer: A high-performance modern technology-driven Placer,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968735885&doi=10.1145%2f2899381&partnerID=40&md5=f8da1f1566338ecb363d7530b64c279e,"The placement problem has become more complex and challenging due to a wide variety of complicated constraints imposed bymodern process technologies. Some of the most challenging constraints and objectives were highlighted during the most recent ACM/IEEE International Symposium on Physical Design (ISPD) contests. In this article, the framework of Eh?Placer and its developed algorithms are elaborated, with the main focus on modern technology constraints and runtime. The technology constraints considered as part of Eh?Placer are fence region, target density, and detailed routability constraints. We present a complete description on how these constraints are considered in different stages of Eh?Placer. The results obtained from the contests indicate that Eh?Placer is able to efficiently handle modern technology constraints and ranks highly among top academic placement tools. © 2016 ACM.",Placement; Region constraints; Target density,Automation; Computer applications; Different stages; Modern technologies; Placement; Placement problems; Process Technologies; Region constraints; Target density; Technology constraints; Placers
A survey of techniques for cache locking,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965128359&doi=10.1145%2f2858792&partnerID=40&md5=c231feefd14b492a8382d43a9f88b685,"Cache memory, although important for boosting application performance, is also a source of execution time variability, and this makes its use difficult in systems requiring worst-case execution time (WCET) guarantees. Cache locking is a promising approach for simplifying WCET estimation and providing predictability, and hence, several commercial processors provide ability for locking cache. However, cache locking also has several disadvantages (e.g., extra misses for unlocked blocks, complex algorithms required for selection of locking contents) and hence, a careful management is required to realize the full potential of cache locking. In this article, we present a survey of techniques proposed for cache locking. We categorize the techniques into several groups to underscore their similarities and differences. We also discuss the opportunities and obstacles in using cache locking. We hope that this article will help researchers gain insight into cache locking schemes and will also stimulate further work in this area. © 2016 ACM.",Cache locking; Cache partitioning; Classification; CPU; GPU; Hard real-time system; Multitasking; Review; Worst-case execution time (WCET),Cache memory; Classification (of information); Graphics processing unit; Interactive computer systems; Multitasking; Program processors; Real time systems; Reviews; Surveys; Application performance; Cache locking; Cache partitioning; Careful management; Complex algorithms; Execution time; Hard real-time systems; Worst-case execution time; Locks (fasteners)
Differential write-conscious software design on phase-change memory: An SQLite case study,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968779534&doi=10.1145%2f2842613&partnerID=40&md5=2364407714d51910244e602847eb5290,"Phase-change memory (PCM) has several benefits including low cost, non-volatility, byte-addressability, etc., and limitations such as write endurance. There have been several hardware approaches to exploit the benefits while minimizing the negative impact of limitations. Software approaches could give further improvements,when used together with hardware approaches, by taking advantage of write behavior present in the program, e.g., write behavior on dynamically allocated data, which is hardly captured by hardware approaches. This work proposes a software design methodology to reduce costly PCM writes. First, on top of existing hardware approach such as Flip-N-Write, we advocate exploiting the capability of PCM bit-level differential write in the software by judiciously reusing previously allocated memory resource. In order to avoid wear-out incurred by the reuse, we present software-based wear-leveling methods that distribute writes across PCM cells. In order to further reduce PCM writes, we propose identifying data, the loss of which does not affect the functionality of the underlying software, and then diverting write traffic for those data items to volatile memory. To evaluate the effectiveness of these methods, as a case study, we applied the proposed methods to the design of journaling in SQLite, which is an important database application commonly used in smartphones. For the experiments, we used an in-house PCM-based prototype board. Our experiments with four representative mobile applications show that the proposed design methods, which is applied on top of the hardware approach, Flip-N-Write, result in 75.2% further reduction in total bit updates in PCM, on average, without aggravating wear-out compared with the baseline of PCM-based journaling, which is based only on the hardware approach. Also, the proposed design methods result in 49.4% reduction in energy consumption and 52.3% reduction in runtime compared to a typical FIFO management of free resources. © 2016 ACM.",Differential write; Phase change memory; SQLite; Wear leveling,Computer software reusability; Design; Digital storage; Energy utilization; Hardware; Reconfigurable hardware; Software design; Database applications; Differential write; Mobile applications; Phase change memory (pcm); Software approach; Software design methodologies; SQLite; Wear leveling; Phase change memory
Novel adaptive power-gating strategy and tapered TSV structure in multilayer 3D IC,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968761565&doi=10.1145%2f2894752&partnerID=40&md5=05a5922fd6cd4fe7eb23dda867e192cd,"Among power dissipation components, leakage power has become more dominant with each successive technology node. Power-gating techniques have been widely used to reduce the standby leakage energy. In this work, we investigate a power-gating strategy for through-silicon via (TSV)-based 3D IC stacking structures. Power-gating control is becoming more complicated as more dies are stacked. We combine the on-chip PDN and TSV in a multilayered 3D IC to perform power-gating analysis of the static and dynamic voltage drops and in-rush current. Then, we propose a novel power-gating strategy that optimizes the in-rush current profile, subject to the voltage-drop constraints. Our power-gating strategy provides a minimal wakeup latency such that the voltage noise safety margins are not violated. In addition, the layer dependency of the 3D IC on the power gating is analyzed in terms of the wake-up time reduction. We achieve an average wake-up time reduction of 43% for all cases with our adaptive power-gating method that exploits location (or layer) information regarding the aggressors in a 3D IC. A tapered TSV architecture based on the layer dependency has been analyzed; it exhibits up to 18% wake-up time reduction compared to that of circuits with uniform TSVs. © 2016 ACM.",3D IC; Power delivery network (PDN); Power gating; Through-silicon vias (TSVs); Wake-up time,Electric power transmission; Electronics packaging; Integrated circuit interconnects; Integrated circuit manufacture; Leakage currents; Voltage control; Wakes; Architecture-based; Power delivery network (PDN); Power gatings; Stacking structures; Stand-by leakage; Through silicon vias; Through-Silicon-Via (TSV); Wake-up time; Three dimensional integrated circuits
Parasitic-aware common-centroid FinFET placement and routing for current-ratio matching,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968883475&doi=10.1145%2f2856031&partnerID=40&md5=b04f75ebfd4a4be1c8bd932d73fa65c1,"The FinFET technology is regarded as a better alternative for modern high-performance and low-power integrated-circuit design due to more effective channel control and lower power consumption. However, the gate-misalignment problem resulting from process variation and the parasitic resistance resulting from interconnecting wires based on the FinFET technology becomes evenmore severe compared with the conventional planar CMOS technology. Such gate misalignment and unwanted parasitic resistance may increase the threshold voltage and decrease the drain current of transistors. When applying the FinFET technology to analog circuit design, the variation of drain currents can destroy current-ratio matching among transistors and degrade circuit performance. In this article, we present the first FinFET placement and routing algorithms for layout generation of a common-centroid FinFET array to precisely match the current ratios among transistors. Experimental results show that the proposed matching-driven FinFET placement and routing algorithms can obtain the best current-ratio matching compared with the state-of-the-art common-centroid placer. © 2016 ACM.",Analog placement; Common centroid; Current-ratio matching; FinFET; Gate misalignment; Parasitic resistance; Routing,Alignment; CMOS integrated circuits; Integrated circuit design; Integrated circuit manufacture; Low power electronics; Reconfigurable hardware; Routing algorithms; Threshold voltage; Analog placement; Common centroid; Current ratios; FinFET; Gate misalignment; Parasitic resistances; Routing; Drain current
Minimizing stack memory for hard real-time applications on multicore platforms with partitioned fixed-priority or EDF scheduling,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973917399&doi=10.1145%2f2846096&partnerID=40&md5=06896de544ac25d77b84f7e217139e56,"Multicore processors are increasingly adopted in resource-constrained real-time embedded applications. In the development of such applications, efficient use of RAM memory is as important as the effective scheduling of software tasks. Preemption Threshold Scheduling (PTS) is a well-known technique for controlling the degree of preemption, possibly improving system schedulability, and to reduce system stack usage. In this paper, we consider partitioned multi-processor scheduling on a multicore processor with either Fixed-Priority or Earliest Deadline First scheduling algorithms with PTS and address the design optimization problem of mapping tasks to processor cores and assignment of task priorities and preemption thresholds with the optimization objective of minimizing system stack usage. We present both optimal solution techniques based on Mixed Integer Linear Programming and efficient heuristic algorithms that can achieve high-quality results. We perform extensive performance evaluations using both synthetic tasksets and industrial case studies. © 2016 ACM.",Multicore; Partitioned scheduling; Real-time scheduling,Application programs; Fixed platforms; Heuristic algorithms; Integer programming; Multicore programming; Multiprocessing systems; Optimization; Random access storage; Real time systems; Response time (computer systems); Scheduling; Scheduling algorithms; Design optimization problem; Earliest-deadline-first scheduling algorithms; Hard real time applications; Mixed integer linear programming; Multi core; Multi processor scheduling; Preemption thresholds; Real - time scheduling; Multitasking
Process independent design methodology for the active RC and single-inverter-based rail clamp,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973392459&doi=10.1145%2f2851490&partnerID=40&md5=e90d2ba7158895020a2e690de1d02079,"RC and single-inverter-based rail clamps are widely used in semiconductor products for electrostatic discharge (ESD) protection. We propose a technology-node-independent design methodology for these rail clamp circuits that takes process, voltage, and temperature variations into consideration. The methodology can be used as a cookbook by the designer or be used to automate the entire design process. Tradeoffs between various design metrics such as ESD performance (Human Body Model), leakage, and area are considered. Simplified circuit models for the rail clamp are presented to gain insights into its working and to size the circuit components. A rail clamp for core power domain is designed using the proposed approach in 40nm low-power process and performance results of the design are also presented. The effectiveness of the design methodology is proven in three different technology nodes by comparing the obtained design with the best design from among 250,000 designs obtained by randomly sampling from the design space. © 2016 ACM.",Electrostatic discharge; On-chip ESD protection; Rail clamps,Electric inverters; Electrostatic devices; Electrostatic discharge; Electrostatics; Reconfigurable hardware; Circuit components; Design Methodology; Electrostatic discharge protection; Human body modeling; On-chip ESD protection; Rail clamps; Semiconductor products; Temperature variation; Design
Clock-tree-aware incremental timing-driven placement,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968830401&doi=10.1145%2f2858793&partnerID=40&md5=6f3ca0d1f18bf8b7458de0874003c54d,"The increasing impact of interconnections on overall circuit performance makes timing-driven placement (TDP) a crucial step toward timing closure. Current TDP techniques improve critical paths but overlook the impact of register placement on clock tree quality. On the other hand, register placement techniques found in the literature mainly focus on power consumption, disregarding timing and routabilty. Indeed, postponing register placement may undermine the optimization achieved by TDP, since the wiring between sequential and combinational elements would be touched. This work proposes a new approach for an effective coupling between register placement and TDP that relies on two key aspects to handle sequential and combinational elements separately: only the registers in the critical paths are touched by TDP (in practice they represent a small percentage of the total number of registers), and the shortening of clock tree wirelength can be obtained with limited variation in signal wirelength and placement density. The approach consists of two steps: (1) incremental register placement guided by a virtual clock tree to reduce clock wiring capacitance while preserving signal wirelength and density, and (2) incremental TDP to minimize the total negative slack. For the first step, we propose a novel technique that combines clock-net contraction and register clustering forces to reduce the clock wirelength. For the second step, we propose a novel Lagrangian Relaxation formulation that minimizes total negative slack for both setup and hold timing violations. To solve the formulation, we propose a TDP technique using a novel discrete search that employs a Euclidean distance to define a proper neighborhood. For the experimental evaluation of the proposed approach, we relied on the ICCAD 2014 TDP contest infrastructure and compared our results with the best results obtained from that contest in terms of timing closure, clock tree compactness, signal wirelength, and density. Assuming a long displacement constraint, our technique achieves worst and total negative slack reductions of around 24% and 26%, respectively. In addition, our approach leads to 44% shorter clock tree wirelength with negligible impact on signal wirelength and placement density. In the face of such results, the proposed coupling seems a useful approach to handle the challenges faced by contemporary physical synthesis. © 2016 ACM.",Density; Lagrangian relaxation; Register placement; Timing closure; Timing-driven placement,Clock distribution networks; Density (specific gravity); Lagrange multipliers; Timing circuits; Circuit performance; Displacement constraint; Euclidean distance; Experimental evaluation; LaGrangian relaxation; Register placement; Timing closures; Timing driven placement; Clocks
EBL overlapping aware stencil planning for MCC system,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973866021&doi=10.1145%2f2888394&partnerID=40&md5=851f40c288ada8ab5cee6ceb16dbd52b,"Electron beam lithography (EBL) is a promising, maskless solution for the technology beyond 14nm logic nodes. To overcome its throughput limitation, industry has proposed character projection (CP) technique, where some complex shapes (characters) can be printed in one shot. Recently, the traditional EBL system was extended into a multi-column cell (MCC) system to further improve the throughput. In an MCC system, several independent CPs are used to further speed-up the writing process. Because of the area constraint of stencil, the MCC system needs to be packed/planned carefully to take advantage of the characters. In this article, we prove that the overlapping aware stencil planning (OSP) problem is NP-hard. Then we propose E-BLOW, a tool to solve the MCC system OSP problem. E-BLOW involves several novel speedup techniques, such as successive relaxation and dynamic programming. Experimental results show that, compared with previous works, E-BLOW demonstrates better performance for both the conventional EBL system and the MCC system. © 2016 ACM.",Electron beam lithography; Multi-column cell system; Overlapping aware stencil planning,Dynamic programming; Electron beams; Electrooptical effects; Character projections; Complex shapes; EBL systems; Logic nodes; Mask less; Multi-column cells; Speed-up techniques; Writing process; Electron beam lithography
Performance-driven assignment of buffered I/O signals in area-I/O flip-chip designs,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957085431&doi=10.1145%2f2818642&partnerID=40&md5=0cac9fb78d537a869274ef3edac18830,"Due to the inappropriate assignment of bump pads or the improper assignment of I/O buffers, the constructed buffered I/O signals in an area-I/O flip-chip design may yield longer maximum delay. In this article, the problem of assigning performance-driven buffered I/O signals in an area-I/O flip-chip design is first formulated. Furthermore, the assignment of the buffered I/O signals can be divided into two sequential phases: Construction of performance-driven I/O signals and Assignment of timing-constrained I/O buffers. Finally, an efficient matching-based approach is proposed to construct the performance-driven I/O signals for the given I/O pins and assign the timing-constrained I/O buffers into the constructed I/O signals in the assignment of the buffered I/O signals in an area-I/O flip-chip design. Compared with the experimental results of seven tested circuits in the Elmore delay model, the experimental results show that the matching-based assignment in our proposed approach can reduce 3.56% of the total path delay, 9.72% of the maximum input delay, 5.90% of the input skew, 5.64% of the maximum output delay, and 6.25% of the output skew on average by reassigning the I/O buffers. Our proposed approach can further reduce 38.89% of the total path delay, 44.00% of the maximum input delay, 49.13% of the input skew, 44.93% of the maximum output delay, and 50.82% of output skew on average by reconstructing the I/O signals and reassigning the I/O buffers into the I/O signals. Compared with the experimental results of seven tested circuits in Peng's [Peng et al. 2006] publication, the experimental results show that our proposed matching-based approach can further reduce 71.06% of the total path delay, 67.83% of the maximum input delay, 59.84% of the input skew, 68.87% of the maximum output delay, and 61.46% of the output skew on average. On the other hand, compared with the experimental results of five tested circuits in Lai's [Lai and Chen 2008] publication, the experimental results show that our proposed approach can further reduce 75.36% of the total path delay, 48.94% of the input skew, and 52.80% of the output skew on the average. © 2016 ACM.",Buffered I/O signal; Bump pad; Flip-chip design; I/O buffer; Minimum weight perfect matching; Voronoi diagram,Design; Flip chip devices; Reconfigurable hardware; Signal processing; Bump pad; Elmore delay model; Flip chip; Maximum delay; Maximum output; Perfect matchings; Performance-driven; Voronoi diagrams; Delay circuits
A new unicast-based multicast scheme for network-on-chip router and interconnect testing,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957073338&doi=10.1145%2f2821506&partnerID=40&md5=3cb8a4dd84b0f7798b0c99b0ba553253,"3D technology for networks-on-chip (NOCs) becomes attractive. It is important to present an effective scheme for 3D stacked NOC router and interconnect testing. A new approach to testing of NOC routers is proposed by classifying the routers. Routers with the same number of input/output ports fall into the same class. Routers of the same class are identical if their tests are the same. A test packet is delivered to all the identical routers by a simple unicast-based multicast scheme. It is found that the depth of the consumption buffer at each router has great impact on the test delivery time because test application and test delivery for router testing cannot be handled concurrently. Test delivery must set a router to operational mode. A mathematical model is presented to evaluate the impact of consumption buffer depth on the test delivery time. A new and simple test application scheme is proposed for interconnect testing. Some interesting extensions are presented for further test time reduction and thermal considerations. Sufficient experimental results are presented by comparison with one previous method. The proposed method works for single stuck-at, transition, even small delay faults at routers, and single bridging faults at physical, consumption and injection channels. © 2016 ACM.",Interconnect testing; On-chip networks; Router testing; Unicast-based multicast,Integrated circuit interconnects; Multicasting; Network-on-chip; Three dimensional integrated circuits; VLSI circuits; Injection channels; Interconnect testing; Networks on chips; On-chip networks; Operational modes; Test applications; Test time reduction; Unicast; Routers
Array size computation under uniform overlapping and irregular accesses,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957082959&doi=10.1145%2f2818643&partnerID=40&md5=f37561476d29b8135cc9532c2a932eb5,"The size required to store an array is crucial for an embedded system, as it affects the memory size, the energy permemory access, and the overall system cost. Existing techniques for finding the minimum number of resources required to store an array are less efficient for codes with large loops and not regularly occurring memory accesses. They have to approximate the accessed parts of the array leading to overestimation of the required resources. Otherwise, their exploration time is increased with an increase over the number of the different accessed parts of the array.We propose a methodology to compute the minimum resources required for storing an array which keeps the exploration time low and provides a near-optimal result for regularly and non-regularly occurring memory accesses and overlapping writes and reads. © 2016 ACM.",Iteration space; Liveness; Near-optimality; Resources optimization; Scalability,Automation; Computer applications; Scalability; Array sizes; Iteration spaces; Liveness; Memory access; Near optimality; Near-optimal; Resources optimization; System costs; Iterative methods
"ECDSA passive attacks, leakage sources, and common design mistakes",2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957074440&doi=10.1145%2f2820611&partnerID=40&md5=04337a0dc1fe35867d4e259f34771295,"Elliptic Curves Cryptography (ECC) tends to replace RSA for public key cryptographic services. ECC is involved in many secure schemes such as Elliptic Curve Diffie-Hellman (ECDH) key agreement, Elliptic Curve Integrated Encryption Scheme (ECIES), and Elliptic Curve Digital Signature Algorithm (ECDSA). As for every cryptosystem, implementation of such schemesmay jeopardize the inherent security provided by the mathematical properties of the ECC. Unfortunate implementation or algorithm choices may create serious vulnerabilities. The elliptic curve scalar operation is particularly sensitive among these schemes. This article surveys passive attacks against well-spread elliptic curve scalar multiplication algorithms highlighting leakage sources and common mistakes that can be used to attack the ECDSA scheme. Experimental results are provided to illustrate and demonstrate the effectiveness of each vulnerability. Finally, the article describes the link between partial leakage and lattice attack in order to understand and demonstrate the impact of small leakages on the security of ECDSA. An example of side channel and lattice attack combination on NIST P-256 is provided in the case where the elliptic curve scalar multiplication is not protected against DPA/CPA and a controllable device is not accessible. © 2016 ACM.",Elliptic Curve Cryptography (ECC); Elliptic Curve Digital Signature Algorithm (ECDSA); Lattices; Scalar multiplication algorithms; Side-channel analysis; Simple power analysis; Timing attack,Authentication; Crystal lattices; Electronic document identification systems; Geometry; Public key cryptography; Elliptic Curve Cryptography (ECC); Elliptic curve digital signature algorithm; Scalar multiplication; Side-channel analysis; Simple power analysis; Side channel attack
TSocket: Thermal sustainable power budgeting,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957105168&doi=10.1145%2f2837023&partnerID=40&md5=34cb1bdc51380481f29d37b80ad6903f,"As technology scales, thermal management for multicore architectures becomes a critical challenge due to increasing power density. Existing power budgeting techniques focus on maximizing performance under a given power budget by optimizing the core configurations. In multicore era, a chip-wide power budget, however, is not sufficient to ensure thermal constraints because the thermal sustainable power capacity varies with different threading strategies and core configurations. In this article, we propose two models to dynamically estimate the thermal sustainable power capacity in homogeneous multicore systems: uniform power model and nonuniform power model. These two models convert the thermal effect of threading strategies and core configurations into power capacity, which provide a context-based core power capacity for power budgeting. Based on these models, we introduce a power budgeting framework aiming to improve the performance within thermal constraints, named as TSocket. Compared to the chip-wide power budgeting solution, TSocket shows 19% average performance improvement for the PARSEC benchmarks in single program scenario and up to 11% performance improvement in multiprogram scenario. The performance improvement is achieved by reducing thermal violations and exploring thermal headrooms. © 2016 ACM.",Multicore system; Performance optimization; Power budgeting; Thermal modeling,Benchmarking; Software architecture; Critical challenges; Multi-core systems; Multicore architectures; Performance improvements; Performance optimizations; Power budgeting; Thermal constraints; Thermal model; Budget control
Auxiliary variables in temporal specifications: Semantic and practical analysis for system-level requirements,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957092736&doi=10.1145%2f2811260&partnerID=40&md5=3977bc97504d859ffc6a93ba2a745af6,"Assertion-based verification (ABV) for IP blocks given as synchronous RTL (register transfer level) descriptions has now widely gained acceptance. The challenge addressed here is ABV for systems on chip (SoC) modeled at the system level in SystemC TLM (Transactional Level Modeling). Requirements to be verified at this level of abstraction usually express temporal constraints on the interactions and communications in the SoC. We use the IEEE standard language PSL to formalize these temporal assertions which represent properties on communication actions and their parameters. Auxiliary variables are often indispensable for this formalization, but their use may induce semantic issues. This article discusses this matter, analyzes various existing approaches and proposes a summary of their advantages and shortcomings. They are also compared to our syntactic and semantic framework, implemented in a verification tool. The proposed operational semantics has the advantages of being simple and intuitive while supporting both global and local auxiliary variables. Experimental results on industrial case studies illustrate its applicability. © 2016 ACM.",Assertion-based verification; Electronic system level modeling,Programmable logic controllers; Semantics; System theory; Assertion-based verification; Electronic system level; Industrial case study; Operational semantics; RTL (register transfer level); System-level requirements; Temporal specification; Transactional level modeling; System-on-chip
Design-for-testability for functional broadside tests under primary input constraints,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957091006&doi=10.1145%2f2831231&partnerID=40&md5=89de5648d54c88cf49464079f8a41ace,"Functional broadside tests avoid overtesting of delay faults by creating functional operation conditions during the clock cycles where delay faults are detected. When a circuit is embedded in a larger design, a functional broadside test needs to take into consideration the functional constraints that the design creates for its primary input vectors. At the same time, application of primary input vectors as part of a scan-based test requires hardware support. An earlier work considered the case where a primary input vector is held constant during a test. The approach described in this article matches the hardware for applying primary input vectors to the functional constraints that the design creates. This increases the transition fault coverage that can be achieved by functional broadside tests. This article also considers the effect on the transition fault coverage achievable using close-to-functional broadside tests. © 2016 ACM.",Design-for-testability; Functional broadside tests; Scan circuits; Transition faults,Design; Design for testability; Dynamic random access storage; Hardware; Reconfigurable hardware; Vectors; Functional broadside tests; Functional constraints; Functional operation; Hardware supports; Primary input vectors; Scan circuits; Transition fault coverage; Transition faults; Testing
A cost-effective energy optimization framework of multicore SoCs based on dynamically reconfigurable voltage-frequency islands,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957069058&doi=10.1145%2f2817207&partnerID=40&md5=999d5ec7ee6d3af02ea7b30a87bbf968,"Voltage-frequency island (VFI)-based design has been widely exploited for optimizing system energy of embedded multicore chip in recent years. The existing work either constructed a single static VFI partition for all kinds of applications or required per-core voltage domain configuration. However, the former solution is hard to find one optimal VFI partition for diverse applications while the latter one suffers from high hardware cost. In this article, we propose a cost effective energy optimization framework based on dynamically reconfigurable VFI (D-VFI). Our framework treats a small number of cores as dynamic cores (D-cores) and configures each of them with an independent voltage domain. At runtime, the D-cores can be pieced together with neighboring static VFIs by scaling their operating voltages. This can dynamically construct the optimal VFI partitions for different kinds of applications, thus achieving more aggressive energy optimization under low cost. To identify the D-cores, we propose a rules constrained task scheduling and VFI partitioning algorithm. Moreover, we analyze the task schedules to determine the optimal scaling intervals which can accommodate voltage scaling induced latency. Experimental results demonstrate that the effectiveness of the proposed scheme. © 2016 ACM.",Dynamic voltage-frequency island; Energy; Multicore system-on-chip; System energy; Task scheduling; VFI partitioning,Application specific integrated circuits; Costs; Multitasking; Reconfigurable hardware; Scheduling algorithms; System-on-chip; Voltage scaling; Dynamic voltage; Energy; Multi-core systems; System energy; Task-scheduling; VFI partitioning; Cost effectiveness
Reliability-aware resource allocation and binding in high-level synthesis,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957092279&doi=10.1145%2f2839300&partnerID=40&md5=c4ba144b7f9f83618d0004ae5d87ef0f,"Soft error is nowadays a major reliability issue for nanoscale VLSI, and addressing it during high-level synthesis is essential to improve the efficiency of error mitigation. Motivated by the observation that for behavioral designs, especially control-flow intensive ones, variables and operations have non-uniform soft error vulnerabilities, we propose a novel reliability-aware allocation and binding technique to explore more effective soft error mitigation during high level synthesis. We first perform a comprehensive vulnerability analysis at the behavioral level by considering error propagation and masking in both control and data flows. Then the optimizations based on integer linear programming, as well as heuristic algorithm, are employed to incorporate the behavioral vulnerabilities into the register and functional unit binding phases to achieve cost-efficient error mitigation. The experimental results reveal that compared with the previous techniques which ignored behavioral vulnerabilities, the proposed approach can achieve up to 85% reliability improvement with the same amount of area budget in the RTL design. © 2016 ACM.",Automatic synthesis; Binding; High-level synthesis; Optimization; Redundant design; Soft error,Bins; Budget control; Data flow analysis; Errors; Functional programming; Heuristic algorithms; Integer programming; Optimization; Radiation hardening; Reliability; Automatic synthesis; Binding; Integer Linear Programming; Redundant design; Reliability improvement; Soft error; Soft error mitigations; Vulnerability analysis; High level synthesis
Parallel power grid analysis based on enlarged partitions,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957077234&doi=10.1145%2f2806885&partnerID=40&md5=5a2106425292e410821a211a444b5b9b,"As the size and complexity of current VLSI circuits grows, faster power grid simulation is becoming more and more desirable. In this article, we present a parallel iterative method for static VLSI power grid simulation. In the proposed enlarged-partition-based preconditioned conjugate gradient (EPPCG) power grid solver, the power grid is divided into disjoint partitions that are subsequently enlarged to obtain accurate solution within each partition. The global solution obtained by solving enlarged partition problems concurrently acts as a highly effective parallel preconditioner. The combination of effective preconditioning and efficient parallelization helps achieve very high performance. The experiments show that our parallel implementation can achieve significant speed improvement [61X-142X] over a state-of-the-art direct solver. © 2016 ACM.",Enlarged partition; Iterative solver; Parallel computing; Power grid simulation,Electric power systems; Integrated circuit interconnects; Iterative methods; Parallel processing systems; VLSI circuits; Disjoint partition; Iterative solvers; Parallel implementations; Parallel iterative methods; Partition problem; Power grid simulation; Preconditioned conjugate gradient; Speed improvement; Electric power transmission networks
Adapting to varying distribution of unknown response bits,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957081688&doi=10.1145%2f2835489&partnerID=40&md5=302c9a41e5576c5228eb164eb7cccf38,"Traditionally, test patterns that are generated for a given circuit are applied in an identical manner to all manufactured devices until each device under test either fails or passes each test. With increasing process variations, the statistical diversity of manufactured devices is increasing, making such one-sizefits-all approaches increasingly inefficient. Adaptive test techniques address this problem by tailoring the test decisions for the statistical characteristics of the device under test. In this article, we present several adaptive strategies to enable adaptive unknown bit masking for faster-than-at-speed testing so as to ensure no yield loss while attaining the maximum test quality based on tester memory constraints. We also develop a tester-enabled compression scheme that helps alleviate memory constraints further, shifting the tradeoff space favorably to improve test quality. © 2016 ACM.",Adaptive test; Hardware testing; Process variations; Unknown x's,Automation; Computer applications; Adaptive tests; Compression scheme; Faster than at-speed testing; Memory constraints; Process Variation; Statistical characteristics; Statistical diversity; Unknown x's; Reconfigurable hardware
Security-aware obfuscated priority assignment for automotive CAN platforms,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957099911&doi=10.1145%2f2831232&partnerID=40&md5=66280fd6c8422a203f91c68735ae4f30,"Security in automotive in-vehicle networks is an increasing problem with the growing connectedness of road vehicles. This article proposes a security-aware priority assignment for automotive controller area network (CAN) platforms with the aim of mitigating scaling effects of attacks on vehicle fleets. CAN is the dominating field bus in the automotive domain due to its simplicity, low cost, and robustness. While messages might be encrypted to enhance the security of CAN systems, their priorities are usually identical for automotive platforms, comprising generally a large number of vehicle models. As a result, the identifier uniquely defines which message is sent, allowing attacks to scale across a fleet of vehicles with the same platform. As a remedy, we propose a methodology that is capable of determining obfuscated message identifiers for each individual vehicle. Since identifiers directly represent message priorities, the approach has to take the resulting response time variations into account while satisfying application deadlines for each vehicle schedule separately. Our approach relies on Quadratically Constrained Quadratic Program (QCQP) solving in two stages, specifying first a set of feasible fixed priorities and subsequently bounded priorities for each message. With the obtained bounds, obfuscated identifiers are determined, using a very fast randomized sampling. The experimental results, consisting of a large set of synthetic test cases and a realistic case study, give evidence of the efficiency of the proposed approach in terms of scalability. The results also show that the diversity of obtained identifiers is effectively optimized with our approach, resulting in a very good obfuscation of CAN messages in in-vehicle communication. © 2016 ACM.",Automotive; CAN; Priority assignment; Security,Control system synthesis; Fleet operations; Quadratic programming; Vehicles; Automotive; Controller area network; In-vehicle communication; In-vehicle networks; Priority assignment; Quadratically-constrained quadratic programs; Randomized sampling; Security; Network security
Optimization of 3D digital microfluidic biochips for the multiplexed polymerase chain reaction,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957064733&doi=10.1145%2f2811259&partnerID=40&md5=2f5dc1714a6760efc23bc3050aa11edd,"A digital microfluidic biochip (DMFB) is an attractive technology platform for revolutionizing immunoassays, clinical diagnostics, drug discovery, DNA sequencing, and other laboratory procedures in biochemistry. In most of these applications, real-time polymerase chain reaction (PCR) is an indispensable step for amplifying specific DNA segments. To reduce the reaction time to meet the requirement of ""real-time"" applications, multiplexed PCR is widely utilized. In recent years, three-dimensional (3D) DMFBs that integrate photodetectors (i.e., cyberphysical DMFBs) have been developed, which offer the benefits of smaller size, higher sensitivity, and faster result generations. However, current DMFB design methods target optimization in only two dimensions, thus ignoring the 3D two-layer structure of a DMFB. Furthermore, these techniques ignore practical constraints related to the interference between on-chip device pairs, the performance-critical PCR thermal loop, and the physical size of devices. Moreover, some practical issues in real scenarios are not stressed (e.g., the avoidance of the cross-contamination for multiplexed PCR). In this article, we describe an optimization solution for a 3D DMFB and present a three-stage algorithm to realize a compact 3D PCR chip layout, which includes: (i) PCR thermal-loop optimization, (ii) 3D global placement based on Strong-Push-Weak-Pull (SPWP) model, and (iii) constraint-aware legalization. To avoid cross-contamination between different DNA samples, we also propose a Minimum-Cost-Maximum-Flow-based (MCMF-based) method for reservoir assignment. Simulation results for four laboratory protocols demonstrate that the proposed approach is effective for the design and optimization of a 3D chip for multiplexed real-time PCR. © 2016 ACM.",Cross-contamination; Design automation; Digital microfluidic biochip,Bioassay; Biochips; Chains; Computer aided design; Contamination; Digital microfluidics; DNA; DNA sequences; Gene encoding; Heat pump systems; Microarrays; Microfluidics; Multiplexing; Network security; Three dimensional computer graphics; Cross contamination; Design and optimization; Design automations; Digital microfluidic biochips; Laboratory procedures; Optimization solution; Real time polymerase chain reactions; Threedimensional (3-d); Polymerase chain reaction
Improving write performance by controlling target resistance distributions in MLC PRAM,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957047629&doi=10.1145%2f2820610&partnerID=40&md5=58be0e0b3c96ef93a72437505f21e25a,"Multi-level cell (MLC) phase change RAM (PRAM) is expected to offer lower cost main memory than DRAM. However, poor write performance is one of the most critical problems for practical applications of MLC PRAM. In this article, we present two schemes to improve write performance by controlling the target resistance distribution of MLC PRAM cells. First, we propose multiple RESET/SET operations that relax the target resistance bands of intermediate logic levels with additional RESET/SET operations, which reduces the program time of intermediate logic levels, thereby improving write performance. Second, we propose a twostep write scheme consisting of lightweight write and idle-time completion write that exploits the fact that hot dirty data tend to be overwritten in a short time period and the MLC PRAM often has long idle times. Experimental results show that the multiple RESET/SET and two-step write schemes result in an average IPC improvement of 15.7% and 10.4%, respectively, on a hybrid DRAM/PRAM main memory subsystem. Furthermore, their integrated solution results in an average IPC improvement of 23.2% (up to 46.4%). © 2016 ACM.",Multi-level cell; Phase change RAM; Resistance distribution; Write performance,Dynamic random access storage; Random access storage; Critical problems; Integrated solutions; Intermediate logic; Multi level cell (MLC); Multilevel cell; Phase change rams; Resistance distribution; Write performance; Phase change memory
"A C2RTL framework supporting partition, parallelization, and FIFO sizing for streaming applications",2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957058866&doi=10.1145%2f2797135&partnerID=40&md5=e854efdd1f692e8c8a9db001200d0a50,"Developing circuits for streaming applications written in C (or its variants) can benefit greatly from C-to-RTL (C2RTL) synthesis. Yet, most existing C2RTL tools lack system-level options to trade off various design constraints, such as delay and area. This article introduces a systematic way to accomplish C2RTL synthesis for streaming applications containing thousands of lines of C (or its variants) codes. Synthesizing circuits for such large applications presents serious challenges for existing C2RTL tools. Specifically, the proposed approach determines simultaneously the number of pipeline stages and the number of times that each functional block is duplicated in each pipeline stage. A mixed integer linear programming-based solution is formulated for obtaining the optimal solution. Furthermore, a heuristic algorithm is developed for large-scale problems. To accommodate the differences of the data rates between the adjacent hardware modules, first-in first-out (FIFO) buffers are indispensable, but their overheads are non-negligible. A parallelism-aware FIFO sizing method is also introduced to determine the optimal sizes of FIFOs. Experimental results on seven real-world applications demonstrate that the algorithms in the synthesis flow can make effective design trade-offs and find superior solutions in a short time compared with existing approaches. Furthermore, the algorithms achieve optimal results in most cases with subsecond running time. © 2016 ACM.",FIFO sizing; Parallelization; Partition; Streaming applications; System-level design and optimization,Economic and social effects; Heuristic algorithms; Integer programming; Optimization; Partitions (building); Pipelines; Reconfigurable hardware; Design constraints; FIFO sizing; First in first outs; Large-scale problem; Mixed integer linear programming; Parallelizations; Streaming applications; System level design; C (programming language)
Exploring soft-error robust and energy-efficient register file in GPGPUs using resistive memory,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957063699&doi=10.1145%2f2827697&partnerID=40&md5=31a3ae0ef8fcaff77cf740a103688422,"The increasing adoption of graphics processing units (GPUs) for high-performance computing raises the reliability challenge, which is generally ignored in traditional GPUs. GPUs usually support thousands of parallel threads and require a sizable register file. Such large register file is highly susceptible to soft errors and power-hungry. Although ECC has been adopted to register file in modern GPUs, it causes considerable power overhead, which further increases the power stress. Thus, an energy-efficient soft-error protection mechanism is more desirable. Besides its extremely low leakage power consumption, resistive memory (e.g., spin-transfer torque RAM) is also immune to the radiation induced soft errors due to its magnetic field based storage. In this article, we propose to LEverage reSistive memory to enhance the Soft-error robustness and reduce the power consumption (LESS) of registers in the General-Purpose computing on GPUs (GPGPUs). Since resistive memory experiences longer write latency compared to SRAM, we explore the unique characteristics of GPGPU applications to obtain the win-win gains: achieving the near-full soft-error protection for the register file, and meanwhile substantially reducing the energy consumption with negligible performance degradation. Our experimental results show that LESS is able to mitigate the registers soft-error vulnerability by 86% and achieve 61% energy savings with negligible (e.g., 1%) performance degradation. © 2016 ACM.",Energy efficiency; GPGPU; Register file; Reliability; Resistive memory; Soft error,Computer graphics; Electric power utilization; Energy conservation; Energy utilization; Error correction; Errors; Magnetic leakage; Magnetic storage; Program processors; Radiation hardening; Random access storage; Reliability; Static random access storage; General-purpose computing; GPGPU; Graphics processing units; High performance computing; Performance degradation; Register files; Resistive memory; Soft error; Energy efficiency
Yield and speedup improvements in extensible processors by allocating extra cycles to some custom instructions,2016,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957067867&doi=10.1145%2f2830566&partnerID=40&md5=fdaa40f0077e03d129ddc2de706b6ca2,"In this article, we investigate the application of different techniques for mitigating the impact of process variations on the custom functional unit (CFU) of extensible processors. The techniques include using extra cycles for the CFU and extending the clock period for the extensible processor. The former technique is based on providing an extra clock cycle to those custom instructions (CIs) that have timing yields smaller than one. For this purpose, we make use of a lookup table (LUT) for each fabricated processor. Based on a post-fabrication analysis, the need for an extra clock cycle for some CIs is determined. Consequently, the CI timing violations are prevented, and all manufactured extensible processors will work with a predefined clock cycle time. To study the effect of the objective function (used during the CI selection phase) on the efficacy of the suggested architectural technique, we investigate three different objective functions. In the second technique, the clock period extension is used to guarantee a design yield of one. Our results demonstrate that combining both techniques helps increase the speedup achieved by the extensible processor. To assess the efficacies of the proposed methods, several benchmarks from different application domains are used. Results of the study reveal that the suggested techniques provide considerable improvements in the speedups of the extensible processors when compared to those of approaches that do not consider the impact of process variations. © 2016 ACM.",Extensible processor; Process variation; Speedup; Yield,Benchmarking; Clocks; Table lookup; Clock cycle time; Custom instruction; Extensible Processors; Functional units; Objective functions; Process Variation; Speedup; Yield; Embedded systems
Removal of SAT-Hard Instances in Logic Obfuscation Through Inference of Functionality,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199345098&doi=10.1145%2f3674903&partnerID=40&md5=964825c0f854052f615e815669bab4e4,"Logic obfuscation is a prominent approach to protect intellectual property within integrated circuits during fabrication. Many attacks on logic locking have been proposed, particularly in the Boolean satifiability (SAT) attack family, leading to the development of stronger obfuscation techniques. Some obfuscation techniques, including Full-Lock and InterLock, resist SAT attacks by inserting SAT-hard instances into the design, making the SAT attack infeasible. In this work, we observe that this class of obfuscation leaves most of the original design topology visible to an attacker, who can reverse-engineer the original design given the functionality of the SAT-hard instance. We show that an attacker can expose the SAT-hard instance functionality of Full-Lock or InterLock with a polynomial number of queries of its inputs and outputs. We then develop a mathematical framework showing how the functionality can be inferred using only a black-box oracle, as is commonly used in attacks in the literature. Using this framework, we develop a novel attack that allows a SAT-capable attacker to efficiently unlock designs obfuscated with Full-Lock. Our attack recovers the intellectual property from these obfuscation techniques that were previously thought secure. We empirically demonstrate the potency of our novel sensitization attack against benchmark circuits obfuscated with Full-Lock.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",full-lock; Logic obfuscation; reverse engineering; untrusted foundry,
An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199310672&doi=10.1145%2f3664652&partnerID=40&md5=2d867d35113f3f5997166d3ea957ffec,"Parameterizable machine learning (ML) accelerators are the product of recent breakthroughs in ML. To fully enable their design space exploration (DSE), we propose a physical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN ML algorithms. It adopts a unified approach that combines power, performance, and area (PPA) analysis with frontend performance simulation, thereby achieving a realistic estimation of both backend PPA and system metrics such as runtime and energy. In addition, our framework includes a fully automated DSE technique, which optimizes backend and system metrics through an automated search of architectural and backend parameters. Experimental studies show that our approach consistently predicts backend PPA and system metrics with an average 7% or less prediction error for the ASIC implementation of two deep learning accelerator platforms, VTA and VeriGOOD-ML, in both a commercial 12 nm process and a research-oriented 45 nm process.  Copyright © 2024 held by the owner/author(s).",design space exploration; ML accelerator; PPA prediction,Computer aided design; Forecasting; Learning systems; Area prediction; Design space exploration; Machine learning accelerator; Machine-learning; Open-source; Optimization framework; Performance prediction; Power performance; Power predictions; System metrics; Deep neural networks
"A Single Bitline Highly Stable, Low Power With High Speed Half-Select Disturb Free 11T SRAM Cell",2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199269381&doi=10.1145%2f3653675&partnerID=40&md5=90e87681890d889a7f8f21bef8875f38,"A half-select disturb-free 11T (HF11T) static random access memory (SRAM) cell with low power, better stability and high speed is presented in this paper. The proposed SRAM cell works well with bit-interleaving design, which enhances soft-error immunity. A comparison of the proposed HF11T cell with other cutting-edge designs such as single-ended HS free 11T (SEHF11T), a shared-pass-gate 11T (SPG11T), data-dependent stack PMOS switching 10T (DSPS10T), a single-ended half-selected robust 12T (HSR12T), and 11T SRAM cells has been made. It exhibits 4.85×/9.19× less read delay (TRA) and write delay (TWA), respectively as compared to other considered SRAM cells. It achieves 1.07×/1.02× better read and write stability, respectively than the considered SRAM cells. It shows maximum reduction of 1.68×/4.58×/94.72×/9×/145× leakage power, read power, write power consumption, read power delay product (PDP) and write PDP respectively, than the considered SRAM cells. In addition, the proposed HF11T cell achieves 10.14× higher Ion/Ioff ratio than the other compared cells. These improvements come with a trade-off, resulting in 1.13× more TRA compared to SPG11T. The simulation is performed with Cadence Virtuoso 45nm CMOS technology at supply voltage (VDD) of 0.6 V. Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",half-select disturb-free; hold static noise margin (HSNM) and write ability; I<sub>on</sub>/I<sub>off</sub> ratio; power consumption; read and write delay; read static noise margin (RSNM); Single-bitline SRAM,Cells; Cytology; Economic and social effects; Radiation hardening; Static random access storage; Disturb free; Half-select disturb-free; Hold static noise margin  and write ability; Ion/Ioff ratio; Read and write delay; Read static noise margin; Single bit lines; Single-bitline static random access memory; Static noise margin; Static random access memory; Electric power utilization
Load Balanced PIM-Based Graph Processing,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199376811&doi=10.1145%2f3659951&partnerID=40&md5=d3c7cdb798891a058690fbedb57ce778,"Graph processing is widely used for many modern applications, such as social networks, recommendation systems, and knowledge graphs. However, processing large-scale graphs on traditional Von Neumann architectures is challenging due to the irregular graph data and memory-bound graph algorithms. Processing-in-memory (PIM) architecture has emerged as a promising approach for accelerating graph processing by enabling computation to be performed directly on memory. Despite having many processing units and high local memory bandwidth, PIM often suffers from insufficient global communication bandwidth and high synchronization overhead due to load imbalance. This article proposes GraphB, a novel PIM-based graph processing system, to address all these issues. From the algorithm perspective, we propose a degree-aware graph partitioning algorithm that can generate balanced partitioning at a low cost. From the architecture perspective, we introduce tile buffers incorporated with an on-chip 2D-Mesh, which provides high bandwidth for inter-node data transfer. Dataflow in GraphB is designed to enable computation-communication overlap and dynamic load balancing. In a PyMTL3-based cycle-accurate simulator with five real-world graphs and three common algorithms, GraphB achieves an average 2.2× and maximum 2.8× speedup compared to the SOTA PIM-based graph processing system GraphQ.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph analytics; load balancing; on-chip network; processing-in-memory,Bandwidth; Data flow analysis; Data transfer; Graphic methods; Knowledge graph; Network architecture; Graph processing; Graph-analytic; Knowledge graphs; Large-scales; Load-balanced; Load-Balancing; Modern applications; On-chip networks; Processing systems; Processing-in-memory; Memory architecture
Semi-Permanent Stuck-At Fault injection attacks on Elephant and GIFT lightweight ciphers,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199314448&doi=10.1145%2f3662734&partnerID=40&md5=5483826b0ee8b60d8700ff2012f9c38c,"Fault attacks pose a potent threat to modern cryptographic implementations, particularly those used in physically approachable embedded devices in IoT environments. Information security in such resource-constrained devices is ensured using lightweight ciphers, where combinational circuit implementations of SBox are preferable over look-up tables as they are more efficient regarding area, power, and memory requirements. Most existing fault analysis techniques focus on fault injection in memory cells and registers. Recently, a novel fault model and analysis technique, namely Semi-Permanent Stuck-At (SPSA) fault analysis, has been proposed to evaluate the security of ciphers with combinational circuit implementation of Substitution layer elements, SBox. In this work, we propose optimized techniques to recover the key in a minimum number of ciphertexts in such implementations of lightweight ciphers. Based on the proposed techniques, a key recovery attack on the NIST lightweight cryptography (NIST-LWC) standardization process finalist, Elephant AEAD, has been proposed. The proposed key recovery attack is validated on two versions of Elephant cipher. The proposed fault analysis approach recovered the secret key within 85-240 ciphertexts, calculated over 1,000 attack instances. To the best of our knowledge, this is the first work on fault analysis attacks on the Elephant scheme. Furthermore, an optimized combinational circuit implementation of Spongent SBox (SBox used in Elephant cipher) is proposed, having a smaller gate count than the optimized implementation reported in the literature. The proposed fault analysis techniques are validated on primary and optimized versions of Spongent SBox through Verilog simulations. Further, we pinpoint SPSA hotspots in the lightweight GIFT cipher SBox architecture. We observe that GIFT SBox exhibits resilience toward the proposed SPSA fault analysis technique under the single fault adversarial model. However, eight SPSA fault patterns reduce the nonlinearity of the SBox to zero, rendering it vulnerable to linear cryptanalysis. Conclusively, SPSA faults may adversely affect the cryptographic properties of an SBox, thereby leading to trivial key recovery. The GIFT cipher is used as an example to focus on two aspects: (i) its SBox construction is resilient to the proposed SPSA analysis and therefore characterizing such constructions for SPSA resilience and (ii) an SBox even though resilient to the proposed SPSA analysis, may exhibit vulnerabilities toward other classical analysis techniques when subjected to SPSA faults. Our work reports new vulnerabilities in fault analysis in the combinational circuit implementations of cryptographic protocols.  Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",Combinational circuit; Elephant AEAD; Fault attacks; GIFT cipher; Lightweight ciphers; Semi-permanent faults; Stuck-at faults,Network security; Software testing; Table lookup; Timing circuits; Analysis techniques; Elephant AEAD; Fault analysis; Faults attacks; GIFT cipher; Lightweight ciphers; Permanent faults; Semi permanents; Semi-permanent fault; Stuck-at faults; Side channel attack
Capacity-Aware Wash Optimization with Dynamic Fluid Scheduling and Channel Storage for Continuous-Flow Microfluidic Biochips,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199314330&doi=10.1145%2f3659952&partnerID=40&md5=16671026a548f5a8da84f81055c92982,"Continuous-flow microfluidic biochips are gaining increasing attention with promising applications for automatically executing various laboratory procedures in biology and biochemistry. Biochips with distributed channel-storage architectures enable each channel to switch between the roles of transportation and storage. Consequently, fluid transportation, caching, and fetch can occur concurrently through different flow paths. When two dissimilar types of fluidic flows occur through the same channels in a time-interleaved manner, it may cause contamination to the latter as some residues of the former flow may be stuck at the channel wall during transportation. To remove the residues, wash operations are introduced as an essential step to avoid incorrect assay outcomes. However, existing work has been considered that the washing capacity of a buffer fluid is unlimited. In the actual scenario, a fixed-volume buffer fluid irrefutably possesses a limited washing capacity, which can be successively consumed while washing away residues from the channels. Hence, capacity-aware wash scheme is a basic requirement to fulfil the dynamic fluid scheduling and channel storage. In this paper, we formulate a practical wash optimization problem for microfluidic biochips, which considers the requirements of dynamic fluid scheduling, channel storage, as well as washing capacity constraints of buffer fluids simultaneously, and present an efficient design flow to solve this problem systematically. Given the high-level synthesis result of a biochemical application and the corresponding component placement solution, our goal is to complete a contamination-aware flow-path planning with short flow-channel length. Meanwhile, the biochemical application can be executed efficiently and correctly with an optimized capacity-aware wash scheme. Experimental results show that compared to a state-of-the-art washing method, the proposed method achieves an average reduction of 26.1%, 43.1%, and 34.1% across all the benchmarks with respect to the total channel length, total wash time, and execution time of bioassays, respectively.  Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",channel storage; Continuous-flow microfluidic biochips; fluid scheduling; wash optimization; washing capacity,Biochips; Buffer storage; High level synthesis; Microfluidics; Washing; Capacity-aware; Channel storage; Continuous-flow; Continuous-flow microfluidic biochip; Dynamic fluids; Fluid scheduling; Micro fluidic biochips; Optimisations; Wash optimization; Washing capacities; Channel flow
Modeling Retention Errors of 3D NAND Flash for Optimizing Data Placement,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199353528&doi=10.1145%2f3659101&partnerID=40&md5=297c78fe6791b50f2006abe4a52a5d43,"Considering 3D NAND flash has a new property of process variation (PV), which causes different raw bit error rates (RBER) among different layers of the flash block. This article builds a mathematical model for estimating the retention errors of flash cells, by considering the factor of layer-to-layer PV in 3D NAND flash memory, as well as the factors of program/erase (P/E) cycle and retention time of data. Then, it proposes classifying the layers of flash block in 3D NAND flash memory into profitable and unprofitable categories, according to the error correction overhead. After understanding the retention error variation of different layers in 3D NAND flash, we design a mechanism of data placement, which maps the write data onto a suitable layer of flash block, according to the data hotness and the error correction overhead of layers, to boost read performance of 3D NAND flash. The experimental results demonstrate that our proposed retention error estimation model can yield a R2 value of 0.966 on average, verifying the accuracy of the model. Based on the estimated retention error rates of layers, the proposed data placement mechanism can noticeably reduce the read latency by 29.8% on average, compared with state-of-the-art methods against retention errors for 3D NAND flash memory.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D flash memories; ECC; layer RBER variation; modeling; reliability; Solid-state drivers,Bit error rate; Flash memory; Memory architecture; NAND circuits; 3d flash memory; Data placement; ECC; Layer raw bit error rate variation; Modeling; NAND Flash; NAND flash memory; Rate variation; Raw bit error rates; Solid-state driver; Error correction
Enhanced Watermarking for Paper-Based Digital Microfluidic Biochips,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199382215&doi=10.1145%2f3661309&partnerID=40&md5=5ac54570b602fde232884e0c75dce677,"Paper-based digital microfluidic biochip (PB-DMFB) technology provides a promising solution to many biochemical applications. However, the PB-DMFB manufacturing process may suffer from potential security threats. For example, a Trojan insertion attack may affect the functionality of PB-DMFBs. To ensure the correct functionality of PB-DMFBs, we propose a watermarking scheme to hide information in the PB-DMFB layout, which allows users to check design integrity and authenticate the source of the PB-DMFB design. As a result, the proposed method serves as a countermeasure against Trojan insertion attacks in addition to proof of authorship.  © 2024 Copyright held by the owner/author(s).",Paper-based DMFB; security; Trojan; watermarking,Biochips; Digital microfluidics; Digital watermarking; Malware; Bio-chemical applications; Biochip technologies; Digital microfluidic biochips; Manufacturing process; Paper-based DMFB; Security; Security threats; Trojans; Watermarking schemes; Watermarking
"Survey of Machine Learning for Software-assisted Hardware Design Verification: Past, Present, and Prospect",2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199295479&doi=10.1145%2f3661308&partnerID=40&md5=bbbcff5fb2f60b3bbc84b950d79b4430,"With the ever-increasing hardware design complexity comes the realization that efforts required for hardware verification increase at an even faster rate. Driven by the push from the desired verification productivity boost and the pull from leap-ahead capabilities of machine learning (ML), recent years have witnessed the emergence of exploiting ML-based techniques to improve the efficiency of hardware verification. In this article, we present a panoramic view of how ML-based techniques are embraced in hardware design verification, from formal verification to simulation-based verification, from academia to industry, and from current progress to future prospects. We envision that the adoption of ML-based techniques will pave the road for more scalable, more intelligent, and more productive hardware verification.  Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",formal verification; Hardware verification; machine learning; simulation-based verification,Computer aided design; Computer software; Machine learning; 'current; Design complexity; Design verification; Faster rates; Hardware design; Hardware verification; Machine-learning; Panoramic views; Productivity boost; Simulation based verification; Formal verification
Root-Cause Analysis with Semi-Supervised Co-Training for Integrated Systems,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193467283&doi=10.1145%2f3649313&partnerID=40&md5=3232d7474fa0b6270f76b5370a5a9b2a,"Root-cause analysis for integrated systems has become increasingly challenging due to their growing complexity. To tackle these challenges, machine learning (ML) has been applied to enhance root-cause analysis. Nonetheless, ML-based root-cause analysis usually requires abundant training data with root causes labeled by human experts, which are difficult or even impossible to obtain. To overcome this drawback, a semi-supervised co-training method is proposed for root-cause analysis in this article, which only requires a small portion of labeled data. First, a random forest is trained with labeled data. Next, we propose a co-training technique to learn from unlabeled data with semi-supervised learning, which pre-labels a subset of these data automatically and then retrains each decision tree in the random forest. In addition, a robust framework is proposed to avoid over-fitting. We further apply initialization by clustering and feature selection to improve the diagnostic performance. With two case studies from industry, the proposed approach shows superior performance against other state-of-the-art methods by saving up to 67% of labeling efforts.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",co-training; random forest; Root-cause analysis; semi-supervised learning,Integrated control; Co-training; Integrated systems; Labeled data; Machine-learning; Random forests; Root cause; Root cause analysis; Semi-supervised; Semi-supervised learning; Training data; Decision trees
Security Evaluation of State Space Obfuscation of Hardware IP through a Red Team-Blue Team Practice,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193200212&doi=10.1145%2f3640461&partnerID=40&md5=65006f03e96330e1eb5a5e65144d97d6,"Due to the inclination towards a fab-less model of integrated circuit (IC) manufacturing, several untrusted entities get white-box access to the proprietary intellectual property (IP) blocks from diverse vendors. To this end, the untrusted entities pose security-breach threats in the form of piracy, cloning, and reverse-engineering, sometimes threatening national security. Hardware obfuscation is a prominent countermeasure against such issues. Obfuscation allows for preventing the usage of the IP blocks without authorization from the IP owners. Due to finite state machine (FSM) transformation-based hardware obfuscation, the design's FSM gets transformed to make it difficult for an attacker to reverse-engineer the design. A secret key needs to be applied to make the FSM functional, thus preventing the usage of the IP for unintended purposes. Although several hardware obfuscation techniques have been proposed, due to the inability to analyze the techniques from the attackers' standpoint, numerous vulnerabilities inherent to the obfuscation methods go undetected unless a true adversary discovers them. In this article, we present a collaborative approach between two entities - one acting as an attacker or red team and another as a defender or blue team, the first systematic approach to replicate the real attacker-defender scenario in the hardware security domain, which in return strengthens the FSM transformation-based obfuscation technique. The blue team transforms the underlying FSM of a gate-level netlist using state space obfuscation. The red team plays the role of an adversary or evaluator and tries to unlock the design by extracting the unlocking key or recovering the obfuscation circuitries. As the key outcome of this red team-blue team effort, a robust state space obfuscation methodology is evolved showing security promises.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Hardware intellectual property; hardware reverse-engineering; protectip; state space obfuscation,Hardware security; Integrated circuit design; Integrated circuits; Intellectual property; National security; Finite states machine; Hardware intellectual property; Hardware obfuscations; Hardware reverse-engineering; Integrated circuit manufacturing; Protectip; Security evaluation; State space obfuscation; State-space; Transformation based; Reverse engineering
A High-Performance Accelerator for Real-Time Super-Resolution on Edge FPGAs,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193204257&doi=10.1145%2f3652855&partnerID=40&md5=be45ea64fc7479f7b3de83e92f0d0133,"In the digital era, the prevalence of low-quality images contrasts with the widespread use of high-definition displays, primarily due to low-resolution cameras and compression technologies. Image super-resolution (SR) techniques, particularly those leveraging deep learning, aim to enhance these images for high-definition presentation. However, real-time execution of deep neural network (DNN)-based SR methods at the edge poses challenges due to their high computational and storage requirements. To address this, field-programmable gate arrays (FPGAs) have emerged as a promising platform, offering flexibility, programmability, and adaptability to evolving models. Previous FPGA-based SR solutions have focused on reducing computational and memory costs through aggressive simplification techniques, often sacrificing the quality of the reconstructed images. This paper introduces a novel SR network specifically designed for edge applications, which maintains reconstruction performance while managing computation costs effectively. Additionally, we propose an architectural design that enables the real-time and end-to-end inference of the proposed SR network on embedded FPGAs. Our key contributions include a tailored SR algorithm optimized for embedded FPGAs, a DSP-enhanced design that achieves a significant four-fold speedup, a novel scalable cache strategy for handling large feature maps, optimization of DSP cascade consumption, and a constraint optimization approach for resource allocation. Experimental results demonstrate that our FPGA-specific accelerator surpasses existing solutions, delivering superior throughput, energy efficiency, and image quality.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",FPGA; Real-time System; Super-Resolution,Constrained optimization; Deep neural networks; Energy efficiency; Field programmable gate arrays (FPGA); Image enhancement; Integrated circuit design; Interactive computer systems; Optical resolving power; Digital era; Embedded fields; Field programmables; Field-programmable gate array; Low qualities; Performance; Programmable gate array; Real - Time system; Real- time; Superresolution; Real time systems
Enhancing Lifetime and Performance of MLC NVM Caches Using Embedded Trace Buffers,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193244877&doi=10.1145%2f3659102&partnerID=40&md5=a2f62ead5e8e4a72d9f7d60d30d9e846,"Large volumes of on-chip and off-chip memory are required by contemporary applications. Emerging non-volatile memory technologies including STT-RAM, PCM, and ReRAM are becoming popular for on-chip and off-chip memories as a result of their desirable properties. Compared to traditional memory technologies such as SRAM and DRAM, they have minimal leakage current and high packing density. Non Volatile Memories (NVM), however, have a low write endurance, a high write latency, and high write energy. Non-volatile Single Level Cell (SLC) memories can store a single bit of data in each memory cell, whereas Multi Level Cells (MLC) can store two or more bits in each memory cell. Although MLC NVMs have substantially higher packing density than SLCs, their lifetime and access speed are key concerns. For a given cache size, MLC caches consume 1.84× less space and 2.62× less leakage power than SLC caches. We propose Trace buffer Assisted Non-volatile Memory Cache (TANC), an approach that increases the lifespan and performance of MLC-based last-level caches using the underutilized Embedded Trace Buffers (ETB). TANC improves the lifetime of MLC LLCs up to 4.36× and decreases average memory access time by 4% compared to SLC NVM LLCs and by 6.41× and 11%, respectively, compared to baseline MLC LLCs.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",multi level cell; Non volatile memory; STT-RAM; wear-leveling,Dynamic random access storage; Memory architecture; Nonvolatile storage; Semiconductor storage; Static random access storage; Wear of materials; ChIP-chip; Multi level cell; Multilevels; Non-volatile memory; Off-chip memory; On chips; Performance; Single level cells; Stt rams; Wear-Leveling; Cache memory
"WCPNet: Jointly Predicting Wirelength, Congestion and Power for FPGA Using Multi-Task Learning",2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193264746&doi=10.1145%2f3656170&partnerID=40&md5=64242bb3546508ee976db9986de8a169,"To speed up the design closure and improve the QoR of FPGA, supervised single-task machine learning techniques have been used to predict individual design metric based on placement results. However, the design objective is to achieve optimal performance while considering multiple conflicting metrics. The single-task approaches predict each metric in isolation and neglect the potential correlations or dependencies among them. To address the limitations, this article proposes a multi-task learning approach to jointly predict wirelength, congestion and power. By sharing the common feature representations and adopting the joint optimization strategy, the novel WCPNet models (including WCPNet-HS and WCPNet-SS) cannot only predict the three metrics of different scales simultaneously, but also outperform the majority of single-task models in terms of both prediction performance and time cost, which are demonstrated by the results of the cross design experiment. By adopting the cross-stitch structure in the encoder, WCPNet-SS outperforms WCPNet-HS in prediction performance, but WCPNet-HS is faster because of the simpler parameters sharing structure. The significance of the feature imagepinUtilization on predicting power and wirelength are demonstrated by the ablation experiment.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",design metrics prediction; FPGA; multi-task learning; physical design,Field programmable gate arrays (FPGA); Integrated circuit design; Learning systems; Structural design; Design closure; Design metric prediction; Design metrics; Machine learning techniques; Multitask learning; Physical design; Power; Prediction performance; Speed up; Wire length; Forecasting
Deep Reinforcement Learning-based Mining Task Offloading Scheme for Intelligent Connected Vehicles in UAV-aided MEC,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193283141&doi=10.1145%2f3653451&partnerID=40&md5=d98d6ccff5e8c578d655e33bd73107ba,"The convergence of unmanned aerial vehicle (UAV)-aided mobile edge computing (MEC) networks and blockchain transforms the existing mobile networking paradigm. However, in the temporary hotspot scenario for intelligent connected vehicles (ICVs) in UAV-aided MEC networks, deploying blockchain-based services and applications in vehicles is generally impossible due to its high computational resource and storage requirements. One possible solution is to offload part of all the computational tasks to MEC servers wherever possible. Unfortunately, due to the limited availability and high mobility of the vehicles, there is still lacking simple solutions that can support low-latency and higher reliability networking services for ICVs. In this article, we study the task offloading problem of minimizing the total system latency and the optimal task offloading scheme, subject to constraints on the hover position coordinates of the UAV, the fixed bonuses, flexible transaction fees, transaction rates, mining difficulty, costs and battery energy consumption of the UAV. The problem is confirmed to be a challenging linear integer planning problem, we formulate the problem as a constrained Markov decision process. Deep Reinforcement Learning (DRL) has excellently solved sequential decision-making problems in dynamic ICVs environment, therefore, we propose a novel distributed DRL-based P-D3QN approach by using Prioritized Experience Replay strategy and the dueling double deep Q-network (D3QN) algorithm to solve the optimal task offloading policy effectively. Finally, experiment results show that compared with the benchmark scheme, the P-D3QN algorithm can bring about 26.24% latency improvement and increase about 42.26% offloading utility.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep reinforcement learning (DRL); Intelligent Connected Vehicles (ICVs); Mining task offloading; Mobile edge computing (MEC); Unmanned aerial vehicle (UAV),Antennas; Blockchain; Computation offloading; Decision making; Deep learning; Energy utilization; Markov processes; Mobile edge computing; Unmanned aerial vehicles (UAV); Aerial vehicle; Block-chain; Deep reinforcement learning; Intelligent connected vehicle; Mining task offloading; Mining tasks; Mobile edge computing; Reinforcement learnings; Task offloading; Unmanned aerial vehicle; Reinforcement learning
Incremental Concolic Testing of Register-Transfer Level Designs,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193541440&doi=10.1145%2f3655621&partnerID=40&md5=ee5b91780db21012aef5d8c1a43e1963,"Concolic testing is a scalable solution for automated generation of directed tests for validation of hardware designs. Unfortunately, concolic testing fails to cover complex corner cases such as hard-to-activate branches. In this article, we propose an incremental concolic testing technique to cover hard-to-activate branches in register-transfer level (RTL) models. We show that a complex branch condition can be viewed as a sequence of easy-to-activate events. We map the branch coverage problem to the coverage of a sequence of events. We propose an efficient algorithm to cover the sequence of events using concolic testing. Specifically, the test generated to activate the current event is used as the starting point to activate the next event in the sequence. Experimental results demonstrate that our approach can be used to generate directed tests to cover complex corner cases in RTL models while state-of-the-art methods fail to activate them.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Concolic testing; directed test generation; RTL functional validation,Concolic testing; Corner case; Directed test generation; Functional validation; Level design; Level model; Register-transfer level; Register-transfer level functional validation; Sequence of events; Test generations; Automation
SEDONUT: A Single Event Double Node Upset Tolerant SRAM for Terrestrial Applications,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193202815&doi=10.1145%2f3651985&partnerID=40&md5=0eb9be5770935f63b4c8639320e293cf,"Radiation and its effect on neighboring nodes are critical not only for space applications but also for terrestrial applications at modern lower-technology nodes. This may cause static random-access memory (SRAM) failures due to single- and multi-node upset. Hence, this article proposes a 14T radiation-hardened-based SRAM cell to overcome soft errors for space and critical terrestrial applications. Simulation results show that the proposed cell can be resilient to any single event upset and single event double node upset at its storage nodes. This cell uses less power than others. The hold, read, and write stability increases compared with most considered cells. The higher critical charge of the proposed SRAM increases radiation resistance. Simulation results demonstrate that out of all compared SRAMs, only DNUSRM and the proposed SRAM show 0% probability of logical flipping. Also, other parameters such as total critical charge, write stability, read stability, hold stability, area, power, sensitive area, write speed, and read speed of the proposed SRAM are improved by -19.1%, 5.22%, 25.7%, -5.46%, 22.5%, 50.6%, 60.0%, 17.91%, and 0.74% compared with DNUSRM SRAM. Hence, the better balance among the parameters makes the proposed cell more suitable for space and critical terrestrial applications. Finally, the post-layout and Monte Carlo simulation validate the efficiency of SRAMs.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Critical charge; high stability; low power loss; radiation-hardened SRAM; stability,Cells; Cytology; Hardening; Intelligent systems; Monte Carlo methods; Radiation hardening; Space applications; Static random access storage; Transients; Critical charge; High stability; Low power loss; Neighbouring nodes; Radiation-hardened; Radiation-hardened static random-access memory; Single event; Static random access memory; Technology nodes; Terrestrial application; Stability
Floorplanning with Edge-aware Graph Attention Network and Hindsight Experience Replay,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193203190&doi=10.1145%2f3653453&partnerID=40&md5=fe82b8ff28f1c16356fe84b5ece95dab,"In this article, we focus on chip floorplanning, which aims to determine the location and orientation of circuit macros simultaneously, so the chip area and wirelength are minimized. As the highest level of abstraction in hierarchical physical design, floorplanning bridges the gap between the system-level design and the physical synthesis, whose quality directly influences downstream placement and routing. To tackle chip floorplanning, we propose an end-to-end reinforcement learning (RL) methodology with a hindsight experience replay technique. An edge-aware graph attention network (EAGAT) is developed to effectively encode the macro and connection features of the netlist graph. Moreover, we build a hierarchical decoder architecture mainly consisting of transformer and attention pointer mechanism to output floorplan actions. Since the RL agent automatically extracts knowledge about the solution space, the previously learned policy can be quickly transferred to optimize new unseen netlists. Experimental results demonstrate that, compared with state-of-the-art floorplanners, the proposed end-to-end methodology significantly optimizes area and wirelength on public GSRC and MCNC benchmarks.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Floorplanning; Graph Attention Network; Reinforcement Learning; Transformer,Edge aware; End to end; Experience replay; Floor-planning; Graph attention network; Netlist; On chips; Reinforcement learnings; Transformer; Wire length; Reinforcement learning
Comparative Analysis of Dynamic Power Consumption of Parallel Prefix Adder,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193200731&doi=10.1145%2f3651984&partnerID=40&md5=cf88a0105993a40af42cd07b333cf2bd,"The Newcomb-Benford law, also known as Benford's law, is the law of anomalous numbers stating that in many real-life numerical datasets, including physical and statistical ones, numbers have a small initial digit. Numbers irregularity observed in nature leads to the question, is the arithmetical-logical unit, responsible for performing calculations in computers, optimal? Are there other architectures, not as regular as commonly used Parallel Prefix Adders, that can perform better, especially when operating on the datasets that are not purely random, but irregular? In this article, structures of a propagate-generate tree are compared including regular and irregular configurations - various structures are examined: regular, irregular, with gray cells only, with both gray and black, and with higher valency cells. Performance is evaluated in terms of energy consumption. The evaluation was performed using the extended power model of static CMOS gates. The model is based on changes of vectors, naturally taking into account spatio-temporal correlations. The energy parameters of the designed cells were calculated on the basis of electrical (Spice) simulation. Designs and simulations were done in the Cadence environment and calculations of the power dissipation were performed in MATLAB. The results clearly show that there are PPA structures that perform much better for a specific type of numerical data. Negligent design can lead to an increase greater than two times of power consumption. The novel architectures of PPA described in this work might find practical applications in specialized adders dealing with numerical datasets, such as, for example, sine functions commonly used in digital signal processing.  © 2024 Copyright held by the owner/author(s).",CMOS technology; integrated circuits; low-power design; parallel prefix adder; Power consumption,Adders; CMOS integrated circuits; Digital signal processing; Electric power supplies to apparatus; Integrated circuit design; Low power electronics; MATLAB; Parallel architectures; SPICE; Timing circuits; Benfords Law; CMOS technology; Comparative analyzes; Dynamic power consumption; Gray cells; Low-power design; Numerical datasets; One-number; Parallel prefix adder; Valencies; Electric power utilization
H3D-Transformer: A Heterogeneous 3D (H3D) Computing Platform for Transformer Model Acceleration on Edge Devices,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191095612&doi=10.1145%2f3649219&partnerID=40&md5=f9cc02c65d72e4e73e531ab6c25fc2aa,"Prior hardware accelerator designs primarily focused on single-chip solutions for 10 MB-class computer vision models. The GB-class transformer models for natural language processing (NLP) impose challenges on existing accelerator design due to the massive number of parameters and the diverse matrix multiplication (MatMul) workloads involved. This work proposes a heterogeneous 3D-based accelerator design for transformer models, which adopts an interposer substrate with multiple 3D memory/logic hybrid cubes optimized for accelerating different MatMul workloads. An approximate computing scheme is proposed to take advantage of heterogeneous computing paradigms of mixed-signal compute-in-memory (CIM) and digital tensor processing units (TPU). From the system-level evaluation results, 10 TOPS/W energy efficiency is achieved for the BERT and GPT2 model, which is about 2.6× ∼3.1× higher than the baseline with 7 nm TPU and stacked FeFET memory.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Compute-in-memory; DNN accelerator; heterogeneous 3D integration; multi-head self-attention; transformer,Computer aided design; Computer hardware; Natural language processing systems; Tensors; 3-D integration; Accelerator design; Compute-in-memory; DNN accelerator; Heterogeneous 3d integration; MAtrix multiplication; Multi-head self-attention; Processing units; Transformer; Transformer modeling; Energy efficiency
Two-dimensional Search Space for Extracting Broadside Tests from Functional Test Sequences,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193201477&doi=10.1145%2f3650207&partnerID=40&md5=d253d0f76c0e38444dba8cd05405cfcf,"Testing for delay faults after chip manufacturing is critical to correct chip operation. Tests for delay faults are applied using scan chains that provide access to internal memory elements. As a result, a circuit may operate under non-functional operation conditions during test application. This may lead to overtesting. The extraction of broadside tests from functional test sequences ensures that the tests create functional operation conditions. When N functional test sequences of length L+1 are available, the number of broadside tests that can be extracted is N · L. Depending on the source of the functional test sequences, the value of N · L may be large. In this case, it is important to select a subset of n ≤ N sequences and consider only the first l ≤ L clock cycles of every sequence for the extraction of n · l N · L broadside tests. The two-dimensional N × L search space for broadside tests is the subject of this article. Using a static procedure that considers fixed values of N and l, the article demonstrates that, for the same value of N · L, different circuits benefit from different values of N and l. It also describes a dynamic procedure that matches the parameters N and l to the circuit. The discussion is supported by experimental results for transition faults in benchmark circuits.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Broadside tests; functional test sequences; state diagram; test extraction; transition faults.,Broadside tests; Delay faults; Functional operation; Functional test sequences; Operation conditions; Search spaces; State diagram; Test extraction; Transition fault.; Transition faults; Extraction
VeriGen: A Large Language Model for Verilog Code Generation,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190854770&doi=10.1145%2f3643681&partnerID=40&md5=bd65ec7406ab3d2f4416eee4c7b631ec,"In this study, we explore the capability of Large Language Models (LLMs) to automate hardware design by automatically completing partial Verilog code, a common language for designing and modeling digital systems. We fine-tune pre-existing LLMs on Verilog datasets compiled from GitHub and Verilog textbooks. We evaluate the functional correctness of the generated Verilog code using a specially designed test suite, featuring a custom problem set and testing benches. Here, our fine-tuned open-source CodeGen-16B model outperforms the commercial state-of-the-art GPT-3.5-turbo model with a 1.1% overall increase. Upon testing with a more diverse and complex problem set, we find that the fine-tuned model shows competitive performance against state-of-the-art gpt-3.5-turbo, excelling in certain scenarios. Notably, it demonstrates a 41% improvement in generating syntactically correct Verilog code across various problem categories compared to its pre-trained counterpart, highlighting the potential of smaller, in-house LLMs in hardware design automation. We release our training/evaluation scripts and LLM checkpoints as open-source contributions.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",EDA; GPT; large language models; Transformers; verilog,Codes (symbols); Computational linguistics; Electronic design automation; Modeling languages; Open source software; Open systems; EDA; GPT; Hardware design; Language model; Large language model; Open-source; Problem sets; State of the art; Transformer; Verilog code; Computer hardware description languages
Reduced On-chip Storage of Seeds for Built-in Test Generation,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193204191&doi=10.1145%2f3643810&partnerID=40&md5=b1b4d34f61648a17f39968f8c3b3b3ed,"Logic built-in self-test (LBIST) approaches use an on-chip logic block for test generation and thus enable in-field testing. Recent reports of silent data corruption underline the importance of in-field testing. In a class of storage-based LBIST approaches, compressed tests are stored on-chip and decompressed by an on-chip decompression logic. The on-chip storage requirements may become a bottleneck when the number of compressed tests is large. In this case, using each compressed test for applying several different tests allows the storage requirements to be reduced. However, producing different tests from each compressed test has a hardware overhead. This article suggests a new on-chip storage scheme for compressed tests that eliminates the additional hardware overhead. Under the new storage scheme, a set of N B-bit compressed tests targeting a set of faults F0 is translated into a sequence S of N · B bits. Every B consecutive bits of S are considered as a compressed test. The sequence S thus yields close to N ·B compressed tests, magnifying the test data stored in S almost B times. Taking advantage of the extra tests, the article describes a software procedure that is applied offline to reduce S without losing fault coverage of F0. Experimental results for benchmark circuits demonstrate significant reductions in the storage requirements of S and significant increases in the fault coverage of a second set of faults, F1 © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Linear-feedback shift-register (LFSR); logic built-in self-test (LBIST); on-chip storage; on-chip test generation; test data compression,Built-in self test; Computer circuits; Data compression; Digital storage; Integrated circuit testing; Software testing; Linear feedback shift registers; Linear-feedback shift-register; Logic build-in self-test; Logic built-in self test; On chips; On-chip storage; On-chip test generation; On-chip tests; Test Data Compression; Test generations; Shift registers
An Efficient FPGA Architecture with Turn-Restricted Switch Boxes,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193202333&doi=10.1145%2f3643809&partnerID=40&md5=0554f15661c80f02f53584f9d22b9b01,"Abstract. Field-Programmable Gate Arrays (FPGAs) employ a large number of SRAM cells to provide a flexible routing architecture which have a significant impact on the FPGA's area and power consumption. This flexible routing allows for a rather easy realization of the desired functionality, but our evaluations show that the full routing flexibility is not required in many occasions. In this work, we focus on what is actually needed and introduce a new switch-box realization what we call Turn-Restricted Switch-Boxes which supports only a subset of possible turns. The proposed method increases the utilization rate of FPGA switch-boxes by eliminating the unemployed resources. Experimental evaluations confirm that the area and average power consumption can be reduced by 12.8% and 14.1%, on average, respectively and the FPGA routing susceptibility to SEU and MBU can be improved by 18.2%, on average, by imposing negligible performance.1 © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",FPGA; switch-box,Electric power utilization; Static random access storage; Array architecture; Field programmables; Field-programmable gate array; Flexible routing; Programmable gate array; Routing architecture; Routing flexibility; SRAM Cell; Switch box; Utilization rates; Field programmable gate arrays (FPGA)
Detecting Adversarial Examples Utilizing Pixel Value Diversity,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193200033&doi=10.1145%2f3636460&partnerID=40&md5=97228378c683c49aafd48cf1bc8f6b2f,"In this article, we introduce two novel methods to detect adversarial examples utilizing pixel value diversity. First, we propose the concept of pixel value diversity (which reflects the spread of pixel values in an image) and two independent metrics (UPVR and RPVR) to assess the pixel value diversity separately. Then we propose two methods to detect adversarial examples based on the threshold method and Bayesian method respectively. Experimental results show that compared to an excellent prior method LID, our proposed methods achieve better performances in detecting adversarial examples. We also show the robustness of our proposed work against an adaptive attack method.  © 2024 Copyright held by the owner/author(s).",adversarial detection; Adversarial examples; deep learning; security of neural networks,Deep learning; Pixels; Adversarial detection; Adversarial example; Bayesian methods; Deep learning; Example based; Neural-networks; Novel methods; Pixel values; Security of neural network; Threshold methods; Bayesian networks
EPHA: An Energy-efficient Parallel Hybrid Architecture for ANNs and SNNs,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193199978&doi=10.1145%2f3643134&partnerID=40&md5=324a59dff3000ac846e9f031a0dd059c,"Artificial neural networks (ANNs) and spiking neural networks (SNNs) are two general approaches to achieve artificial intelligence (AI). The former have been widely used in academia and industry fields; the latter, SNNs, are more similar to biological neural networks and can realize ultra-low power consumption, thus have received widespread research attention. However, due to their fundamental differences in computation formula and information coding, the two methods often require different and incompatible platforms. Alongside the development of AI, a general platform that can support both ANNs and SNNs is necessary. Moreover, there are some similarities between ANNs and SNNs, which leaves room to deploy different networks on the same architecture. However, there is little related research on this topic. Accordingly, this article presents an energy-efficient, scalable, and non-Von Neumann architecture (EPHA) for ANNs and SNNs. Our study combines device-, circuit-, architecture-, and algorithm-level innovations to achieve a parallel architecture with ultra-low power consumption. We use the compensated ferrimagnet to act as both synapses and neurons to store weights and perform dot-product operations, respectively. Moreover, we propose a novel computing flow to reduce the operations across multiple crossbar arrays, which enables our design to conduct large and complex tasks. On a suite of ANN and SNN workloads, the EPHA is 1.6× more power-efficient than a state-of-the-art design, NEBULA, in the ANN mode. In the SNN mode, our design is 4 orders of magnitude more than the Loihi in power efficiency.  © 2024 Copyright held by the owner/author(s).",AI chip; Artificial intelligence chips; neural network hardware; neuromorphic computing,Computing power; Electric power utilization; Energy efficiency; Low power electronics; Network architecture; Parallel architectures; Artificial intelligence chip; Biological neural networks; Energy efficient; Hybrid architectures; Neural network hardware; Neural-networks; Neuromorphic computing; Parallel hybrids; Ultra-low power consumption; Neural networks
D3PBO: Dynamic Domain Decomposition-based Parallel Bayesian Optimization for Large-scale Analog Circuit Sizing,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193201561&doi=10.1145%2f3643811&partnerID=40&md5=4ea70b67b7a3835c025088749923fde8,"Bayesian optimization (BO) is an efficient global optimization method for expensive black-box functions, but the expansion for high-dimensional problems and large sample budgets still remains a severe challenge. In order to extend BO for large-scale analog circuit synthesis, a novel computationally efficient parallel BO method, D3PBO, is proposed for high-dimensional problems in this work. We introduce the dynamic domain decomposition method based on maximum variance between clusters. The search space is decomposed into subdomains progressively to limit the maximal number of observations in each domain. The promising domain is explored by multi-trust region-based batch BO with the local Gaussian process (GP) model. As the domain decomposition progresses, the basin-shaped domain is identified using a GP-assisted quadratic regression method and exploited by the local search method BOBYQA to achieve a faster convergence rate. The time complexity of D3PBO is constant for each iteration. Experiments demonstrate that D3PBO obtains better results with significantly less runtime consumption compared to state-of-the-art methods. For the circuit optimization experiments, D3PBO achieves up to 10× runtime speedup compared to TuRBO with better solutions.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Domain decomposition; high-dimensional optimization; maximum variance between clusters; parallel bayesian optimization,Analog circuits; Budget control; Domain decomposition methods; Global optimization; Regression analysis; Timing circuits; Bayesian optimization; Between clusters; Domain decompositions; Dynamic domain decomposition; High-dimensional optimization; Higher-dimensional problems; Large-scales; Maximum variance; Maximum variance between cluster; Parallel bayesian optimization; Iterative methods
Optimal Model Partitioning with Low-Overhead Profiling on the PIM-based Platform for Deep Learning Inference,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190981817&doi=10.1145%2f3628599&partnerID=40&md5=934f8879b12f7e0018aeb573639dde82,"Recently Processing-in-Memory (PIM) has become a promising solution to achieve energy-efficient computation in data-intensive applications by placing computation near or inside the memory. In most Deep Learning (DL) frameworks, a user manually partitions a model’s computational graph (CG) onto the computing devices by considering the devices’ capability and the data transfer. The Deep Neural Network (DNN) models become increasingly complex for improving accuracy; thus, it is exceptionally challenging to partition the execution to achieve the best performance, especially on a PIM-based platform requiring frequent offloading of large amounts of data. This article proposes two novel algorithms for DL inference to resolve the challenge: low-overhead profiling and optimal model partitioning. First, we reconstruct CG by considering the devices’ capability to represent all the possible scheduling paths. Second, we develop a profiling algorithm to find the required minimum profiling paths to measure all the node and edge costs of the reconstructed CG. Finally, we devise the model partitioning algorithm to get the optimal minimum execution time using the dynamic programming technique with the profiled data. We evaluated our work by executing the BERT, RoBERTa, and GPT-2 models on the ARM multicores with the PIM-modeled FPGA platform with various sequence lengths. For three computing devices in the platform, i.e., CPU serial/parallel and PIM executions, we could find all the costs only in four profile runs, three for node costs and one for edge costs. Also, our model partitioning algorithm achieved the highest performance in all the experiments over the execution with manually assigned device priority and the state-of-the-art greedy approach. © 2024 Copyright held by the owner/author(s).",computational graph; optimal scheduling; PIM-based execution; profiling,Computation offloading; Data transfer; Dynamic programming; Energy efficiency; Energy utilization; Inference engines; Learning systems; Neural network models; Computational graph; Computing devices; Device capabilities; Low overhead; Model partitioning; Optimal model; Optimal scheduling; Processing-in-memory; Processing-in-memory-based execution; Profiling; Deep neural networks
"Security of Electrical, Optical, and Wireless On-chip Interconnects: A Survey",2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189189445&doi=10.1145%2f3631117&partnerID=40&md5=6d496cc0cca5d4f44a22264f2ebae0ab,"The advancement of manufacturing technologies has enabled the integration of more intellectual property (IP) cores on the same system-on-chip (SoC). Scalable and high throughput on-chip communication architecture has become a vital component in today’s SoCs. Diverse technologies such as electrical, wireless, optical, and hybrid are available for on-chip communication with different architectures supporting them. On-chip communication sub-system is shared across all the IPs and continuously used throughout the lifetime of the SoC. Therefore, the security of the on-chip communication is crucial, because exploiting any vulnerability would be a goldmine for an attacker. In this survey, we provide a comprehensive review of threat models, attacks, and countermeasures over diverse on-chip communication technologies as well as sophisticated architectures. © 2024 Copyright held by the owner/author(s).","Network-on-chip security, communication security","Computer architecture; Integrated circuit interconnects; Network architecture; Network security; Optical communication; Programmable logic controllers; Servers; Communication architectures; Communications security; High-throughput; Manufacturing technologies; Network-on-chip security, communication security; Networks on chips; On chip communication; Optical and wireless; Security communication; Systems-on-Chip; Network-on-chip"
A Module-Level Configuration Methodology for Programmable Camouflaged Logic,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190271477&doi=10.1145%2f3640462&partnerID=40&md5=9af99081fc2f7407d285688f5565690a,"Logic camouflage is a widely adopted technique that mitigates the threat of intellectual property (IP) piracy and overproduction in the integrated circuit (IC) supply chain. Camouflaged logic achieves functional obfuscation through physical-level ambiguity and post-manufacturing programmability. However, discussions on programmability are confined to the level of logic cells/gates, limiting the broader-scale application of logic camouflage. In this work, we propose a novel module-level configuration methodology for programmable camouflaged logic that can be implemented without additional hardware ports and with negligible resources. We prove theoretically that the configuration of the programmable camouflaged logic cells can be achieved through the inputs and netlist of the original module. Further, we propose a novel lightweight ferroelectric FET (FeFET)-based reconfigurable logic gate (rGate) family and apply it to the proposed methodology. With the flexible replacement and the proposed configuration-aware conversion algorithm, this work is characterized by the input-only programming scheme as well as the combination of high output error rate and point-function-like defense. Evaluations show an average of >95% of the alternative rGate location for camouflage, which is sufficient for the security-aware design. We illustrate the exponential complexity in function state traversal and the enhanced defense capability of locked blackbox against Boolean Satisfiability (SAT) attacks compared with key-based methods. We also preserve an evident output Hamming distance and introduce negligible hardware overheads in both gate-level and module-level evaluations under typical benchmarks. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ferroelectric FET; Hardware security; IP protection; logic camouflage; nonvolatile memory; reconfiguration,Function evaluation; Hamming distance; Logic devices; Network security; Reconfigurable hardware; Supply chains; Ferroelectric FeFET; Intellectual property protection; Logic camouflage; Logic cells; Non-volatile memory; Nonvolatile memory; Physical level; Programmability; Reconfigurable logic; Reconfiguration; Hardware security
Scalable and Accelerated Self-healing Control Circuit Using Evolvable Hardware,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190605470&doi=10.1145%2f3634682&partnerID=40&md5=e06d1dd9f88c6461ec79035a1ab981a5,"Controllers are mission-critical components of any electronic design. By sending control signals, they decide which and when other data path elements must operate. Faults, especially Single Event Upset (SEU) occurrence in these components, can lead to functional/mission failure of the system when deployed in harsh environments. Hence, competence to self-heal from SEU is highly required in the control path of the digital system. Reconfiguration is critical for recovering from a faulty state to a non-faulty state. Compared to native reconfiguration, the Virtual Reconfigurable Circuit (VRC) is an FPGA-generic reconfiguration mechanism. The non-partial reconfiguration in VRC and extensive architecture are considered hindrances in extending the VRC-based Evolvable Hardware (EHW) to real-time fault mitigation. To confront this challenge, we have proposed an intrinsic constrained evolution to improve the scalability and accelerate the evolution process for VRC-based fault mitigation in mission-critical applications. Experimentation is conducted on complex ACM/SIGDA benchmark circuits and real-time circuits used in space missions, which are not included in related works. In addition, a comparative study is made between existing and proposed methodologies for brushless DC motor control circuits. The hardware utilization in the multiplexer has been significantly reduced, resulting in up to a 77% reduction in the existing VRC architecture. The proposed methodology employs a fault localization approach to narrow the search space effectively. This approach has yielded an 87% improvement on average in convergence speed, as measured by the evolution time, compared to the existing work. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",evolvable hardware; genetic algorithm; Hardwired control circuit; scalability; SEU fault mitigation; virtual reconfigurable circuit,Brushless DC motors; Electric network analysis; Fault tolerance; Genetic algorithms; Radiation hardening; Reconfigurable hardware; Self-healing materials; Timing circuits; Control circuits; Evolvable hardware; Fault mitigations; Hardwired control circuit; Real- time; Self healing control; Single event upset fault mitigation; Single event upset faults; Single event upsets; Virtual reconfigurable circuit; Scalability
Energy-Constrained Scheduling for Weakly Hard Real-Time Systems Using Standby-Sparing,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190598772&doi=10.1145%2f3631587&partnerID=40&md5=e409c2e9b255e452ec91208b5f3698c0,"For real-time embedded systems, QoS (Quality of Service), fault tolerance, and energy budget constraint are among the primary design concerns. In this research, we investigate the problem of energy constrained standby-sparing for both periodic and aperiodic tasks in a weakly hard real-time environment. The standby-sparing systems adopt a primary processor and a spare processor to provide fault tolerance for both permanent and transient faults. For such kind of systems, we firstly propose several novel standby-sparing schemes for the periodic tasks which can ensure the system feasibility under tighter energy budget constraint than the traditional ones. Then based on them integrated approachs for both periodic and aperiodic tasks are proposed to minimize the aperiodic response time whilst achieving better energy and QoS performance under the given energy budget constraint. The evaluation results demonstrated that the proposed techniques significantly outperformed the existing state-of-the-art approaches in terms of feasibility and system performance while ensuring QoS and fault tolerance under the given energy budget constraint. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Energy constraint; fault tolerance; quality of service; real-time scheduling; standby-sparing,Budget control; Embedded systems; Fault tolerance; Interactive computer systems; Real time systems; Aperiodic task; Budget constraint; Constrained scheduling; Energy budgets; Energy constraint; Energy-constrained; Periodic tasks; Quality-of-service; Real time scheduling; Standby-sparing; Quality of service
DeepFlow: A Cross-Stack Pathfinding Framework for Distributed AI Systems,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190604006&doi=10.1145%2f3635867&partnerID=40&md5=b70fc6460d0340b4b88cd0f3076a513d,"Over the past decade, machine learning model complexity has grown at an extraordinary rate, as has the scale of the systems training such large models. However, there is an alarmingly low hardware utilization (5–20%) in large scale AI systems. The low system utilization is a cumulative effect of minor losses across different layers of the stack, exacerbated by the disconnect between engineers designing different layers spanning across different industries. To address this challenge, in this work we designed a cross-stack performance modelling and design space exploration framework. First, we introduce CrossFlow, a novel framework that enables cross-layer analysis all the way from the technology layer to the algorithmic layer. Next, we introduce DeepFlow (built on top of CrossFlow using machine learning techniques) to automate the design space exploration and co-optimization across different layers of the stack. We have validated CrossFlow’s accuracy with distributed training on real commercial hardware and showcase several DeepFlow case studies demonstrating pitfalls of not optimizing across the technology-hardware-software stack for what is likely the most important workload driving large development investments in all aspects of computing stack. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",design space exploration; Distributed AI systems; hardware-software co-optimization; performance modelling,AI systems; Co-optimization; Cross flows; Design space exploration; Different layers; Distributed AI; Distributed AI system; Hardware-software co-optimization; Hardware/software; Performance Modeling; Machine learning
RGMU: A High-flexibility and Low-cost Reconfigurable Galois Field Multiplication Unit Design Approach for CGRCA,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190597616&doi=10.1145%2f3639820&partnerID=40&md5=75ad8bf30921443446b4e28f09abadf1,"Finite field multiplication is a non-linear transformation operator that appears in the majority of symmetric cryptographic algorithms. Numerous specified finite field multiplication units have been proposed as a fundamental module in the coarse-grained reconfigurable cipher logic array to support more cryptographic algorithms; however, it will introduce low flexibility and high overhead, resulting in reduced performance of the coarse-grained reconfigurable cipher logic array. In this article, a high-flexibility and low-cost reconfigurable Galois field multiplication unit (RGMU) is proposed to balance the tradeoffs between the function, delay, and area. All the finite field multiplication operations, including maximum distance separable matrix multiplication, parallel update of Fibonacci linear feedback shift register, parallel update of Galois linear feedback shift register, and composite field multiplication, are analyzed and two basic operation components are abstracted. Further, a reconfigurable finite field multiplication computational model is established to demonstrate the efficacy of reconfigurable units and guide the design of RGMU with high performance. Finally, the overall architecture of RGMU and two multiplication circuits are introduced. Experimental results show that the RGMU can not only reduce the hardware overhead and power consumption but also has the unique advantage of satisfying all the finite field multiplication operations in symmetric cryptography algorithms. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Coarse-grained reconfigurable cipher logic arrays; finite field multiplication; parallelism; symmetric cipher,Computation theory; Cost reduction; Cryptography; Linear transformations; Reconfigurable architectures; Shift registers; Coarse-grained reconfigurable; Coarse-grained reconfigurable cipher logic array; Finite field multiplication; Galois's fields; High flexibility; High-low; Low-costs; Parallelism; Reconfigurable; Symmetric cipher; Computer circuits
Mixed Integer Programming based Placement Refinement by RSMT Model with Movable Pins,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190618998&doi=10.1145%2f3639365&partnerID=40&md5=a368de94dbe0355d69a0c1b9c89667a3,"Placement is a critical step in the physical design for digital application specific integrated circuits (ASICs), as it can directly affect the design qualities such as wirelength and timing. For many domain specific designs, the demands for high performance parallel computing result in repetitive hardware instances, such as the processing elements in the neural network accelerators. As these instances can dominate the area of the designs, the runtime of the complete design’s placement can be traded for optimizing and reusing one instance’s placement to achieve higher quality. Therefore, this work proposes a mixed integer programming (MIP)-based placement refinement algorithm for the repetitive instances. By efficiently modeling the rectilinear steiner tree wirelength, the placement can be precisely refined for better quality. Besides, the MIP formulations for timing-driven placement are proposed. A theoretical proof is then provided to show the correctness of the proposed wirelength model. For the instances in various popular fields, the experiments show that given the placement from the commercial placers, the proposed algorithm can perform further placement refinement to reduce 3.76%/3.64% detailed routing wirelength and 1.68%/2.42% critical path delay under wirelength/timing-driven mode, respectively, and also outperforms the state-of-the-art previous work. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",mixed integer programming; physical design; Placement,Application specific integrated circuits; Computer aided design; Integrated circuit design; Application-specific integrated circuits; Critical steps; Design Quality; Digital applications; Domain specific design; High performance parallel computing; Mixed-Integer Programming; Physical design; Placement; Wire length; Integer programming
TROP: TRust-aware OPportunistic Routing in NoC with Hardware Trojans,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190266404&doi=10.1145%2f3639821&partnerID=40&md5=1e70f40c4f36805269c3f8568af86045,"Multiple software and hardware intellectual property (IP) components are combined on a single chip to form Multi-Processor Systems-on-Chips (MPSoCs). Due to the rigid time-to-market constraints, some of the IPs are from outsourced third parties. Due to the supply-chain management of IP blocks being handled by unreliable third-party vendors, security has grown as a crucial design concern in the MPSoC. These IPs may get exposed to certain unwanted practises like the insertion of malicious circuits called Hardware Trojan (HT) leading to security threats and attacks, including sensitive data leakage or integrity violations. A Network-on-Chip (NoC) connects various units of an MPSoC. Since it serves as the interface between various units in an MPSoC, it has complete access to all the data flowing through the system. This makes NoC security a paramount design issue. Our research focuses on a threat model where the NoC is infiltrated by multiple HTs that can corrupt packets. Data integrity verified at the destination’s network interface (NI) triggers re-transmissions of packets if the verification results in an error. In this article, we propose an opportunistic trust-aware routing strategy that efficiently avoids HT while ensuring that the packets arrive at their destination unaltered. Experimental results demonstrate the successful movement of packets through opportunistically selected neighbours along a trust-aware path free from the HT effect. We also observe a significant reduction in the rate of packet retransmissions and latency at the expense of incurring minimum area and power overhead. © 2024 Copyright held by the owner/author(s).",Hardware Trojans; opportunistic routing; packet integrity; trustable path,Integrated circuit design; Internet protocols; Malware; Multiprocessing systems; Network security; Network-on-chip; Sensitive data; Supply chain management; Data integrity; In networks; Multi processor system on chips; Networks on chips; Opportunistic routing; Packet integrity; Software and hardwares; Software intellectual properties; Trust-aware; Trustable path; Hardware security
Enhanced Real-time Scheduling of AVB Flows in Time-Sensitive Networking,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190586844&doi=10.1145%2f3637878&partnerID=40&md5=85c336fca9f7ec90a27c09aeb9e0aeb1,"Time-Sensitive Networking (TSN) realizes high bandwidth and time determinism for data transmission and thus becomes the crucial communication technology in time-critical systems. The Gate Control List (GCL) is used to control the transmission of different classes of traffic in TSN, including Time-Triggered (TT) flows, Audio-Video-Bridging (AVB) flows, and Best-Effort (BE) flows. Most studies focus on optimizing GCL synthesis by reserving the preceding time slots to serve TT flows with the strict delay requirement, but ignore the deadlines of non-TT flows and cause the large delay. Therefore, this paper proposes a comprehensive scheduling method to enhance the real-time scheduling of AVB flows while guaranteeing the time determinism of TT flows. This method first optimizes GCL synthesis to reserve the preceding time slots for AVB flows, and then introduces the Earliest Deadline First (EDF) method to further improve the transmission of AVB flows by considering their deadlines. Moreover, the worst-case delay (WCD) analysis method is proposed to verify the effectiveness of the proposed method. Experimental results show that the proposed method improves the transmission of AVB flows compared to the state-of-the-art methods. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Audio-video-bridging (AVB); real-time scheduling; time-sensitive networking (TSN); worst-case delay (WCD) analysis,Delay-sensitive applications; Audio videos; Audio-video-bridging; Bandwidth and time; Gate control; High bandwidth; Real time scheduling; Time triggered; Time-sensitive networking; Timeslots; Worst-case delay analysis; Response time (computer systems)
Pareto Optimization of Analog Circuits Using Reinforcement Learning,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190269196&doi=10.1145%2f3640463&partnerID=40&md5=b04c36f6e8cf0a257e77aa561bac2026,"Analog circuit optimization and design presents a unique set of challenges in the IC design process. Many applications require the designer to optimize for multiple competing objectives, which poses a crucial challenge. Motivated by these practical aspects, we propose a novel method to tackle multi-objective optimization for analog circuit design in continuous action spaces. In particular, we propose to (i) extrapolate current techniques in Multi-Objective Reinforcement Learning to continuous state and action spaces and (ii) provide for a dynamically tunable trained model to query user defined preferences in multi-objective optimization in the analog circuit design context. © 2024 Copyright held by the owner/author(s).",Analog circuit optimization; machine learning; Pareto optimization,Analog circuits; Integrated circuit manufacture; Pareto principle; Reinforcement learning; Timing circuits; Action spaces; Analog Circuit Design; Analog circuit optimization; Circuit designs; Circuit optimization; Continuous actions; Machine-learning; Multi-objectives optimization; Pareto-optimization; Reinforcement learnings; Multiobjective optimization
Application-level Validation of Accelerator Designs Using a Formal Software/Hardware Interface,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190583623&doi=10.1145%2f3639051&partnerID=40&md5=7203818b3500d498ce4abecebbc6858b,"Ideally, accelerator development should be as easy as software development. Several recent design languages/tools are working toward this goal, but actually testing early designs on real applications end-to-end remains prohibitively difficult due to the costs of building specialized compiler and simulator support. We propose a new first-in-class, mostly automated methodology termed “3LA” to enable end-to-end testing of prototype accelerator designs on unmodified source applications. A key contribution of 3LA is the use of a formal software/hardware interface that specifies an accelerator’s operations and their semantics. Specifically, we leverage the Instruction-level Abstraction (ILA) formal specification for accelerators that has been successfully used thus far for accelerator implementation verification. We show how the ILA for accelerators serves as a software/hardware interface, similar to the Instruction Set Architecture for processors, that can be used for automated development of compilers and instruction-level simulators. Another key contribution of this work is to show how ILA-based accelerator semantics enables extending recent work on equality saturation to auto-generate basic compiler support for prototype accelerators in a technique we term “flexible matching.” By combining flexible matching with simulators auto-generated from ILA specifications, our approach enables end-to-end evaluation with modest engineering effort. We detail several case studies of 3LA, which uncovered an unknown flaw in a recently published accelerator and facilitated its fix. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Application programs; Computer architecture; Formal specification; Program compilers; Semantics; Accelerator design; Application level; Design languages; Early designs; End to end; Flexible matching; Hardware interfaces; Instruction-level; Language tools; Software/hardware; Software design
GAN-Place: Advancing Open Source Placers to Commercial-quality Using Generative Adversarial Networks and Transfer Learning,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190293396&doi=10.1145%2f3636461&partnerID=40&md5=8ea69c693376c21cc4f7e3420e85b207,"Recently, GPU-accelerated placers such as DREAMPlace and Xplace have demonstrated their superiority over traditional CPU-reliant placers by achieving orders of magnitude speed up in placement runtime. However, due to their limited focus in placement objectives (e.g., wirelength and density), the placement quality achieved by DREAMPlace or Xplace is not comparable to that of commercial tools. In this article, to bridge the gap between open source and commercial placers, we present a novel placement optimization framework named GAN-Place that employs generative adversarial learning to transfer the placement quality of the industry-leading commercial placer, Synopsys ICC2, to existing open source GPU-accelerated placers (DREAMPlace and Xplace). Without the knowledge of the underlying proprietary algorithms or constraints used by the commercial tools, our framework facilitates transfer learning to directly enhance the open source placers by optimizing the proposed differentiable loss that denotes the “similarity” between DREAMPlace- or Xplace-generated placements and those in commercial databases. Experimental results on seven industrial designs not only show that our GAN-Place immediately improves the Power, Performance, and Area metrics at the placement stage but also demonstrates that these improvements last firmly to the post-route stage, where we observe improvements by up to 8.3% in wirelength, 7.4% in power, and 37.6% in Total Negative Slack on a commercial CPU benchmark. © 2024 Copyright held by the owner/author(s).",generative adversarial learning; Placement optimization; transfer learning,Benchmarking; Placers; Adversarial learning; Commercial qualities; Commercial tools; Generative adversarial learning; GPU-accelerated; Networks learning; Open-source; Placement optimization; Transfer learning; Wire length; Generative adversarial networks
SparGD: A Sparse GEMM Accelerator with Dynamic Dataflow,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190573500&doi=10.1145%2f3634703&partnerID=40&md5=a962fe9b88689233cc38a51ced25b5b6,"Deep learning has become a highly popular research field, and previously deep learning algorithms ran primarily on CPUs and GPUs. However, with the rapid development of deep learning, it was discovered that existing processors could not meet the specific large-scale computing requirements of deep learning, and custom deep learning accelerators have become popular. The majority of the primary workloads in deep learning are general matrix-matrix multiplications (GEMMs), and emerging GEMMs are highly sparse and irregular. The TPU and SIGMA are typical GEMM accelerators in recent years, but the TPU does not support sparsity, and both the TPU and SIGMA have insufficient utilization rates of the Processing Element (PE). We design and implement SparGD, a sparse GEMM accelerator with dynamic dataflow. SparGD has specific PE structures, flexible distribution networks and reduction networks, and a simple dataflow switching module. When running sparse and irregular GEMMs, SparGD can maintain high PE utilization while utilizing sparsity, and can switch to the optimal dataflow according to the computing environment. For sparse, irregular GEMMs, our experimental results show that SparGD outperforms systolic arrays by 30 times and SIGMA by 3.6 times. © 2024 Copyright held by the owner/author(s).",accelerators; dynamic dataflow; GEMM; sparsity,Learning algorithms; Matrix algebra; Program processors; Systolic arrays; Am generals; Dataflow; Dynamic dataflow; General matrix-matrix multiplication; Large-scale computing; Matrix-matrix multiplications; Processing elements; Research fields; Sparsity; Utilization rates; Deep learning
An Efficient Reinforcement Learning Based Framework for Exploring Logic Synthesis,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189623569&doi=10.1145%2f3632174&partnerID=40&md5=3d895f38a6ae89a53bf24ee93f18b304,"Logic synthesis is a crucial step in electronic design automation tools. The rapid developments of reinforcement learning (RL) have enabled the automated exploration of logic synthesis. Existing RL based methods may lead to data inefficiency, and the exploration approaches for FPGA and ASIC technology mapping in recent works lack the flexibility of the learning process. This work proposes ESE, a reinforcement learning based framework to efficiently learn the logic synthesis process. The framework supports the modeling of logic optimization and technology mapping for FPGA and ASIC. The optimization for the execution time of the synthesis script is also considered. For the modeling of FPGA mapping, the logic optimization and technology mapping are combined to be learned in a flexible way. For the modeling of ASIC mapping, the standard cell based optimization and LUT optimization operations are incorporated into the ASIC synthesis flow. To improve the utilization of samples, the Proximal Policy Optimization model is adopted. Furthermore, the framework is enhanced by supporting MIG based synthesis exploration. Experiments show that for FPGA technology mapping on the VTR benchmark, the average LUT-Level-Product and script runtime are improved by more than 18.3% and 12.4% respectively than previous works. For ASIC mapping on the EPFL benchmark, the average Area-Delay-Product is improved by 14.5%. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",And-inverter graph; logic optimization; Majority-inverter graph; Reinforcement learning; technology mapping,Codes (symbols); Computer circuits; Field programmable gate arrays (FPGA); Learning systems; Logic gates; Mapping; Reinforcement learning; And-inverte graph; Electronic design automation tools; FPGA technology; FPGAs and ASICs; Learning-based methods; Logic optimization; Logic technology; Majority-inverte graph; Reinforcement learnings; Technology-mapping; Logic Synthesis
RSPP: Restricted Static Pseudo-Partitioning for Mitigation of Cross-Core Covert Channel Attacks,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190588174&doi=10.1145%2f3637222&partnerID=40&md5=a456fac7407e40c4b0b57739760c1fa2,"Cache timing channel attacks exploit the inherent properties of cache memories: hit and miss time along with the shared nature of the cache to leak secret information. The side channel and covert channel are the two well-known cache timing channel attacks. In this article, we propose Restricted Static Pseudo-Partitioning (RSPP), an effective partition-based mitigation mechanism that restricts the cache access of only the adversaries involved in the attack. It has an insignificant impact of only 1% in performance, as the benign processes have access to the full cache and restrictions are limited only to the suspicious processes and cache sets. It can be implemented with a maximum storage overhead of 1.45% of the total Last-Level Cache (LLC) size. This article presents three variations of the proposed attack mitigation mechanism: RSPP, simplified-RSPP (S-RSPP) and corewise-RSPP (C-RSPP) with different hardware overheads. A full system simulator is used for evaluating the performance impact of RSPP. A detailed experimental analysis with different LLC and attack parameters is also discussed. RSPP is also compared with the existing defense mechanisms effective against cross-core covert channel attacks. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cache partitioning; Cache timing channel attacks; cross-core cache attacks; secure cache architecture,Cache memory; Side channel attack; Cache architecture; Cache attack; Cache partitioning; Cache timing channel attack; Covert channels; Cross-core cache attack; Last-level caches; Property; Secure cache architecture; Timing channels; Timing circuits
A Survey on Approximate Multiplier Designs for Energy Efficiency: From Algorithms to Circuits,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182587033&doi=10.1145%2f3610291&partnerID=40&md5=d47a7a57be4e3fcb572e1f962f8650e2,"Given the stringent requirements of energy efficiency for Internet-of-Things edge devices, approximate multipliers, as a basic component of many processors and accelerators, have been constantly proposed and studied for decades, especially in error-resilient applications. The computation error and energy efficiency largely depend on how and where the approximation is introduced into a design. Thus, this article aims to provide a comprehensive review of the approximation techniques in multiplier designs ranging from algorithms and architectures to circuits. We have implemented representative approximate multiplier designs in each category to understand the impact of the design techniques on accuracy and efficiency. The designs can then be effectively deployed in high-level applications, such as machine learning, to gain energy efficiency at the cost of slight accuracy loss. Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesApproximate computing; algorithm; architecture; circuit; multiplier,Approximation algorithms; Timing circuits; Additional key word and phrasesapproximate computing; Approximation techniques; Computation energy; Computation errors; Design for energy efficiencies; Error-resilient; Key words; Multiplier; Multiplier design; Stringent requirement; Energy efficiency
Dynamic Adaptation Using Deep Reinforcement Learning for Digital Microfluidic Biochips,2024,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190576930&doi=10.1145%2f3633458&partnerID=40&md5=127340918c39dda3dc9fa0e8e08774d6,"We describe an exciting new application domain for deep reinforcement learning (RL): droplet routing on digital microfluidic biochips (DMFBs). A DMFB consists of a two-dimensional electrode array, and it manipulates droplets of liquid to automatically execute biochemical protocols for clinical chemistry. However, a major problem with DMFBs is that electrodes can degrade over time. The transportation of droplet transportation over these degraded electrodes can fail, thereby adversely impacting the integrity of the bioassay outcome. We demonstrated that the formulation of droplet transportation as an RL problem enables the training of deep neural network policies that can adapt to the underlying health conditions of electrodes and ensure reliable fluidic operations. We describe an RL-based droplet routing solution that can be used for various sizes of DMFBs. We highlight the reliable execution of an epigenetic bioassay with the RL droplet router on a fabricated DMFB. We show that the use of the RL approach on a simple micro-computer (Raspberry Pi 4) leads to acceptable performance for time-critical bioassays. We present a simulation environment based on the OpenAI Gym Interface for RL-guided droplet routing problems on DMFBs. We present results on our study of electrode degradation using fabricated DMFBs. The study supports the degradation model used in the simulator. © 2024 Copyright held by the owner/author(s).",Biochips; Biological system modeling; Real-time systems; Reinforcement learning,Biochemistry; Biochips; Biological systems; Deep neural networks; Digital microfluidics; Drops; Electrodes; Interactive computer systems; Real time systems; Applications domains; Biological system modeling; Digital microfluidic biochips; Droplet routing; Dynamic adaptations; Electrode arrays; New applications; Real - Time system; Reinforcement learnings; Two-dimensional electrode; Reinforcement learning
Construction of All Multilayer Monolithic RSMTs and Its Application to Monolithic 3D IC Routing,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182596400&doi=10.1145%2f3626958&partnerID=40&md5=1a9762294e00045f987966ed634dfe6c,"Monolithic three-dimensional (M3D) integration allows ultra-thin silicon tier stacking in a single package. The high-density stacking is acquiring interest and is becoming more popular for smaller footprint areas, shorter wirelength, higher performance, and lower power consumption than the conventional planar fabrication technologies. The physical design of M3D integrated circuits requires several design steps, such as three-dimensional (3D) placement, 3D clock-tree synthesis, 3D routing, and 3D optimization. Among these, 3D routing is significantly time consuming due to countless routing blockages. Therefore, 3D routers proposed in the literature insert monolithic interlayer vias (MIVs) and perform tier-by-tier routing in two substeps. In this article, we propose an algorithm to build a routing topology database (DB) used to construct all multilayer monolithic rectilinear Steiner minimum trees on the 3D Hanan grid. To demonstrate the effectiveness of the DB in various applications, we use the DB to construct timing-driven 3D routing topologies and perform congestion-aware global routing on 3D designs. We anticipate that the algorithm and the DB will help 3D routers reduce the runtime of the MIV insertion step and improve the quality of the 3D routing. © 2023 Association for Computing Machinery. All rights reserved.",3D position sequence; 3D potentially optimal Steiner tree; 3D rectilinear Steiner minimum tree; Additional Key Words and PhrasesMonolithic three-dimensional integration; monolithic inter-layer vias; multilayer monolithic rectilinear Steiner minimum trees; tier sequence,Integrated circuit design; Three dimensional integrated circuits; Trees (mathematics); 3d position sequence; 3D positions; 3d potentially optimal steiner tree; 3d rectilinear steiner minimum tree; Additional key word and phrasesmonolithic three-dimensional integration; Inter-layers; Key words; Monolithic inter-layer vias; Monolithics; Multilayer monolithic rectilinear steiner minimum tree; Rectilinear Steiner minimum trees; Steiner trees; Three dimensional integration; Tier sequence; Multilayers
Flip: Data-centric Edge CGRA Accelerator,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183778643&doi=10.1145%2f3631118&partnerID=40&md5=94b55ddcc131d97520e1e8768b67179a,"Coarse-Grained Reconfigurable Arrays (CGRA) are promising edge accelerators due to the outstanding balance in flexibility, performance, and energy efficiency. Classic CGRAs statically map compute operations onto the processing elements (PE) and route the data dependencies among the operations through the Network-on-Chip. However, CGRAs are designed for fine-grained static instruction-level parallelism and struggle to accelerate applications with dynamic and irregular data-level parallelism, such as graph processing. To address this limitation, we present Flip, a novel accelerator that enhances traditional CGRA architectures to boost the performance of graph applications. Flip retains the classic CGRA execution model while introducing a special data-centric mode for efficient graph processing. Specifically, it leverages the inherent data parallelism of graph algorithms by mapping graph vertices onto PEs rather than the operations and supporting dynamic routing of temporary data according to the runtime evolution of the graph frontier. Experimental results demonstrate that Flip achieves up to 36× speedup with merely 19% more area compared to classic CGRAs. Compared to state-of-the-art large-scale graph processors, Flip has similar energy efficiency and 2.2× better area efficiency at a much-reduced power/area budget.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Accelerator; Coarse-grained Reconfigurable Array; Graph Processing,Budget control; Data handling; Memory architecture; Network architecture; Network-on-chip; Coarse-grained reconfigurable arrays; Data centric; Data dependencies; Fine grained; Graph processing; Instruction level parallelism; Networks on chips; Performance; Processing elements; Processing Route; Energy efficiency
A Machine Learning Approach to Improving Timing Consistency between Global Route and Detailed Route,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182606810&doi=10.1145%2f3626959&partnerID=40&md5=0b1059d13f8cc6dce15b70e3c5cc950c,"Due to the unavailability of routing information in design stages prior to detailed routing (DR), the tasks of timing prediction and optimization pose major challenges. Inaccurate timing prediction wastes design effort, hurts circuit performance, and may lead to design failure. This work focuses on timing prediction after clock tree synthesis and placement legalization, which is the earliest opportunity to time and optimize a ""complete""netlist. The article first documents that having ""oracle knowledge""of the final post-DR parasitics enables post-global routing (GR) optimization to produce improved final timing outcomes. To bridge the gap between GR-based parasitic and timing estimation and post-DR results during post-GR optimization, machine learning (ML)-based models are proposed, including the use of features for macro blockages for accurate predictions for designs with macros. Based on a set of experimental evaluations, it is demonstrated that these models show higher accuracy than GR-based timing estimation. When used during post-GR optimization, the ML-based models show demonstrable improvements in post-DR circuit performance. The methodology is applied to two different tool flows - OpenROAD and a commercial tool flow - and results on an open-source 45nm bulk and a commercial 12nm FinFET enablement show improvements in post-DR timing slack metrics without increasing congestion. The models are demonstrated to be generalizable to designs generated under different clock period constraints and are robust to training data with small levels of noise. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and Phrasesmachine learning; static timing analysis; timing optimization,Clocks; Computer aided design; Electric network analysis; Machine learning; Timing circuits; Additional key word and phrasesmachine learning; Circuit performance; Detailed routing; Global routing; Key words; Parasitics; Routing optimization; Static timing analysis; Timing estimation; Timing optimization; Forecasting
Mathematical Framework for Optimizing Crossbar Allocation for ReRAM-based CNN Accelerators,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182601613&doi=10.1145%2f3631523&partnerID=40&md5=1a6d6a56c77bc350487bde871103733a,"The resistive random-access memory (ReRAM) has widely been used to accelerate convolutional neural networks (CNNs) thanks to its analog in-memory computing capability. ReRAM crossbars not only store layers' weights, but also perform in-situ matrix-vector multiplications which are core operations of CNNs. To boost the performance of ReRAM-based CNN accelerators, crossbars can be duplicated to explore more intra-layer parallelism. The crossbar allocation scheme can significantly influence both the computing throughput and bandwidth requirements of ReRAM-based CNN accelerators. Under the resource constraints (i.e., crossbars and memory bandwidths), how to find the optimal number of crossbars for each layer to maximize the inference performance for an entire CNN is an unsolved problem. In this work, we find the optimal crossbar allocation scheme by mathematically modeling the problem as a constrained optimization problem and solving it with a dynamic programming based solver. Experiments demonstrate that our model for CNN inference time is almost precise, and the proposed framework can obtain solutions with near-optimal inference time. We also emphasize that communication (i.e., data access) is an important factor and must also be considered when determining the optimal crossbar allocation scheme. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and PhrasesReRAM crossbars; CNN accelerator; crossbar allocation; mathematical framework,Bandwidth; Constrained optimization; Convolutional neural networks; RRAM; Additional key word and phrasesreram crossbar; Computing capability; Convolutional neural network; Convolutional neural network accelerator; Crossbar allocation; Key words; Mathematical frameworks; Matrix vector multiplication; Performance; Random access memory; Dynamic programming
BOOM-Explorer: RISC-V BOOM Microarchitecture Design Space Exploration,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182598293&doi=10.1145%2f3630013&partnerID=40&md5=7953b43932ff93c868786cb89d16e526,"Microarchitecture parameters tuning is critical in the microprocessor design cycle. It is a non-trivial design space exploration (DSE) problem due to the large solution space, cycle-accurate simulators' modeling inaccuracy, and high simulation runtime for performance evaluations. Previous methods require massive expert efforts to construct interpretable equations or high computing resource demands to train black-box prediction models. This article follows the black-box methods due to better solution qualities than analytical methods in general. We summarize two learned lessons and propose BOOM-Explorer accordingly. First, embedding microarchitecture domain knowledge in the DSE improves the solution quality. Second, BOOM-Explorer makes the microarchitecture DSE for register-transfer-level designs within the limited time budget feasible. We enhance BOOM-Explorer with the diversity-guidance, further improving the algorithm performance. Experimental results with RISC-V Berkeley-Out-of-Order Machine under 7-nm technology show that our proposed methodology achieves an average of 18.75% higher Pareto hypervolume, 35.47% less average distance to reference set, and 65.38% less overall running time compared to previous approaches. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and PhrasesMicroprocessor; design space exploration; microarchitecture,Computer architecture; Domain Knowledge; Integrated circuit design; Additional key word and phrasesmicroprocessor; Design cycle; Design space exploration; Key words; Micro architectures; Micro-architecture design; Microprocessor designs; Non-trivial; Parameters tuning; Solution quality; Budget control
Introduction to the Special Issue on Design for Testability and Reliability of Security-aware Hardware,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182657591&doi=10.1145%2f3631476&partnerID=40&md5=370a80b69597e0c420013e86f67c1da6,[No abstract available],,
NeuroCool: Dynamic Thermal Management of 3D DRAM for Deep Neural Networks through Customized Prefetching,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182587621&doi=10.1145%2f3630012&partnerID=40&md5=92f3774f47efec0f6bf206c13bf9c5f8,"Deep neural network (DNN) implementations are typically characterized by huge datasets and concurrent computation, resulting in a demand for high memory bandwidth due to intensive data movement between processors and off-chip memory. Performing DNN inference on general-purpose cores/edge is gaining attraction to enhance user experience and reduce latency. The mismatch in the CPU and conventional DRAM speed leads to under-utilization of the compute capabilities, causing increased inference time. 3D DRAM is a promising solution to effectively fulfill the bandwidth requirement of high-throughput DNNs. However, due to high power density in stacked architectures, 3D DRAMs need dynamic thermal management (DTM), resulting in performance overhead due to memory-induced CPU throttling.We study the thermal impact of DNN applications running on a 3D DRAM system, and make a case for a memory temperature-aware customized prefetch mechanism to reduce DTM overheads and significantly improve performance. In our proposed NeuroCool DTM policy, we intelligently place either DRAM ranks or tiers in low power state, using the DNN layer characteristics and access rate. We establish the generalization of our approach through training and test datasets comprising diverse data points from widely used DNN applications. Experimental results on popular DNNs show that NeuroCool results in a average performance gain of 44% (as high as 52%) and memory energy improvement of 43% (as high as 69%) over general-purpose DTM policies. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and Phrases3D DRAM; customized prefetching; dynamic thermal management,Bandwidth; Dynamic random access storage; Temperature control; Additional key word and phrases3d DRAM; Concurrent computation; Customized prefetching; Data movements; Dynamic thermal management; High memory bandwidth; Key words; Management policy; Neural network application; Prefetching; Deep neural networks
"A Reliability-Aware Splitting Duty-Cycle Physical Unclonable Function Based on Trade-off Process, Voltage, and Temperature Variations",2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182609360&doi=10.1145%2f3594667&partnerID=40&md5=c9eb939650b1d40a544c5511eae946b3,"The physical unclonable function (PUF) is a hardware security primitive that can be used to prevent malicious attacks aimed at obtaining device information at the hardware level. The ring oscillator (RO) PUF has attracted considerable research attention. To improve the reliability of the RO PUF under voltage and temperature changes, the response of the duty-cycle (DC) PUF was obtained by comparing the duty cycle of the RO rather than the period. However, this method reduces the effective utilization of process variations, which limits its implementation in mature advanced manufacturing processes. In this study, a splitting duty-cycle (SDC) PUF was proposed to balance the effective extraction of process variations and robustness under voltage and temperature changes. The sensibility formula between the performance of SDC PUF and process, voltage, and temperature was established through a circuit model and statistical methodology, and the comprehensive characteristics of SDC PUF were analyzed theoretically. Next, 16 SDC PUFs with 128-bit responses were implemented and measured on a Xilinx Virtex-7 device. The experimental results revealed that the average native reliability of SDC PUF was 98.97%, and the reliability was 97.32% under various voltage and temperature conditions. This result revealed advantages over the DC PUF implemented in the same device. The uniqueness of the SDC PUF was 50.42%, and it passed the NIST SP 800-22 randomness and autocorrelation function tests.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesHardware security; manufacturing process variations; physical unclonable function; ring oscillator,Cryptography; Economic and social effects; Network security; Reliability; Additional key word and phraseshardware security; Duty-cycle; Key words; Manufacturing process variations; Process Variation; Ring oscillator; Splittings; Temperature changes; Undervoltage; Voltage change; Hardware security
Yield Optimization for Analog Circuits over Multiple Corners via Bayesian Neural Networks: Enhancing Circuit Reliability under Environmental Variation,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182589719&doi=10.1145%2f3626321&partnerID=40&md5=038b07b98c3c05fdc12df0633d1ca24e,"The reliability of circuits is significantly affected by process variations in manufacturing and environmental variation during operation. Current yield optimization algorithms take process variations into consideration to improve circuit reliability. However, the influence of environmental variations (e.g., voltage and temperature variations) is often ignored in current methods because of the high computational cost. In this article, a novel and efficient approach named BNN-BYO is proposed to optimize the yield of analog circuits in multiple environmental corners. First, we use a Bayesian Neural Network (BNN) to simultaneously model the yields and performances of interest in multiple corners efficiently. Next, the multi-corner yield optimization can be performed by embedding BNN into a Bayesian optimization framework. Since the correlation among yields and performances of interest in different corners is implicitly encoded in the BNN model, it provides great modeling capabilities for yields and their uncertainties to improve the efficiency of yield optimization. Our experimental results demonstrate that the proposed method can save up to 45.3% of simulation cost compared to other baseline methods to achieve the same target yield. In addition, for the same simulation cost, our proposed method can find better design points with 3.2% yield improvement. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and PhrasesBayesian neural network; analog circuits; Bayesian optimization; hardware reliability; PVT corners; yield optimization,Analog circuits; Bayesian networks; Computer aided design; Reliability; Timing circuits; Uncertainty analysis; Additional key word and phrasesbayesian neural network; Bayesian neural networks; Bayesian optimization; Circuit reliability; Environmental variations; Hardware reliability; Key words; Neural-networks; PVT corner; Yield optimization; Neural networks
A High Throughput STR-based TRNG by Jitter Precise Quantization Superposing,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182596421&doi=10.1145%2f3606373&partnerID=40&md5=d6a5bf8a71a357507990faf48b7d3f50,"With the rapid development of integrated circuits and the continuous progress of computing capability, higher demands have been placed on the security and speed of data encryption in security systems. As a basic hardware security primitive, the true random number generator (TRNG) plays an important role in the encryption system, which requires higher throughput and randomness with lower hardware overhead. However, the throughput of TRNG is related to the entropy source's quality and the randomness extraction methodology. To quantify the randomness of the entropy source with higher efficiency and quality, we utilize the independent jitter of the self-timed ring (STR) to generate original entropy and propose a high throughput jitter-based TRNG which can extract random information at the pulse of oscillation signal by jitter precise quantization superposing and random oscillation sampling. The proposed TRNG has been implemented on Artix-7 and Virtex-6 FPGAs. The generated true random number successfully passes the NIST SP800-22 and NIST SP800-90B tests while also exhibiting a minimum entropy greater than 0.9947. The most prominent superiority of our proposed TRNG is that it achieves a high throughput of 330 Mbps with an ultra-low hardware overhead of only 35 LUTs and 12 DFFs. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and PhrasesHardware security; FPGA; throughput; true random number generator,Cryptography; Entropy; Field programmable gate arrays (FPGA); Number theory; Quantization (signal); Random number generation; Random processes; Additional key word and phraseshardware security; Entropy sources; Hardware overheads; High-throughput; Key words; Quantisation; Random number generators; Self-timed; True random number generator; True randoms; Jitter
Test Compression for Launch-on-Capture Transition Fault Testing,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182598166&doi=10.1145%2f3597433&partnerID=40&md5=d43965f76972d7b313504056500bfe5c,"A new low-power test compression scheme, called Dcompress, is proposed for launch-on-capture transition fault testing by using a new seed encoding scheme, a new design for testability architecture, and a new low-power test application procedure. The new seed encoding scheme generates seeds for all tests by selecting a primitive polynomial that encodes all tests of a compact test set. A software-defined linear feedback shift register architecture, called SLFSR, is proposed to make the new method conform to the current flow of design and test. Experimental results on benchmark circuits show that test data volume can be compressed up to 6300X with the well-compacted baseline test set for a design with 11.8M gates and more than 1.1M scan flip-flops.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLaunch-on-capture transition fault testing; deterministic BIST; low-power test compression for LOC delay testing; software-defined linear-feedback shift register (SLFSR),Computer aided design; Design for testability; Flip flop circuits; Shift registers; Signal encoding; Software testing; Additional key word and phraseslaunch-on-capture transition fault testing; Delay testing; Deterministic BIST; Fault testing; Key words; Linear feedback shift registers; Low-power test compression for LOC delay testing; Low-power tests; Software-defined linear-feedback shift register; Test compression; Transition faults; Encoding (symbols)
Lightning: Leveraging DVFS-induced Transient Fault Injection to Attack Deep Learning Accelerator of GPUs,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182595449&doi=10.1145%2f3617893&partnerID=40&md5=a38cfeb98b4082dfff6aa461832fea43,"Graphics Processing Units (GPU) are widely used as deep learning accelerators because of its high performance and low power consumption. Additionally, it remains secure against hardware-induced transient fault injection attacks, a classic type of attacks that have been developed on other computing platforms. In this work, we demonstrate that well-trained machine learning models are robust against hardware fault injection attacks when the faults are generated randomly. However, we discover that these models have components, which we refer to as sensitive targets, that are vulnerable to faults. By exploiting this vulnerability, we propose the Lightning attack, which precisely strikes the model's sensitive targets with hardware-induced transient faults based on the Dynamic Voltage and Frequency Scaling (DVFS). We design a sensitive targets search algorithm to find the most critical processing units of Deep Neural Network (DNN) models determining the inference results, and develop a genetic algorithm to automatically optimize the attack parameters for DVFS to induce faults. Experiments on three commodity Nvidia GPUs for four widely-used DNN models show that the proposed Lightning attack can reduce the inference accuracy by 69.1% on average for non-targeted attacks, and, more interestingly, achieve a success rate of 67.9% for targeted attacks. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and PhrasesHardware faults; deep learning trustworthiness; DVFS; GPU accelerator,Computer graphics; Deep neural networks; Fault detection; Genetic algorithms; Inference engines; Lightning; Network security; Neural network models; Program processors; Software testing; Voltage scaling; Additional key word and phraseshardware fault; Deep learning trustworthiness; Dynamic voltage and frequency scaling; Fault injection; Fault injection attacks; Graphic processing unit accelerator; High-low; Key words; Neural network model; Transient faults; Graphics processing unit
AD2VNCS: Adversarial Defense and Device Variation-tolerance in Memristive Crossbar-based Neuromorphic Computing Systems,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182593542&doi=10.1145%2f3600231&partnerID=40&md5=f8453aa5b22c2b36d58fea93eef54e68,"In recent years, memristive crossbar-based neuromorphic computing systems (NCS) have obtained extremely high performance in neural network acceleration. However, adversarial attacks and conductance variations of memristors bring reliability challenges to NCS design. First, adversarial attacks can fool the neural network and pose a serious threat to security critical applications. However, device variations lead to degradation of the network accuracy. In this article, we propose DFS (Deep neural network Feature importance Sampling) and BFS (Bayesian neural network Feature importance Sampling) training strategies, which consist of Bayesian Neural Network (BNN) prior setting, clustering-based loss function, and feature importance sampling techniques, to simultaneously combat device variation, white-box attack, and black-box attack challenges. Experimental results clearly demonstrate that the proposed training framework can improve the NCS reliability.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNeuromorphic; adversarial defense; memristor; variation tolerance,Barium compounds; Deep neural networks; Importance sampling; Network security; Additional key word and phrasesneuromorphic; Adversarial defense; Computing system; Device variations; Key words; Memristor; Neural network features; Neural-networks; Neuromorphic computing; Variation tolerances; Memristors
MOEA/D vs. NSGA-II: A Comprehensive Comparison for Multi/Many Objective Analog/RF Circuit Optimization through a Generic Benchmark,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182592457&doi=10.1145%2f3626096&partnerID=40&md5=120f8559c4df94e40cc71f49d45ae801,"Thanks to the enhanced computational capacity of modern computers, even sophisticated analog/radio frequency (RF) circuit sizing problems can be solved via electronic design automation (EDA) tools. Recently, several analog/RF circuit optimization algorithms have been successfully applied to automatize the analog/RF circuit design process. Conventionally, metaheuristic algorithms are widely used in optimization process. Among various nature-inspired algorithms, evolutionary algorithms (EAs) have been more preferred due to their superiorities (robustness, efficiency, accuracy etc.) over the other algorithms. Furthermore, EAs have been diversified and several distinguished analog/RF circuit optimization approaches for single-, multi-, and many-objective problems have been reported in the literature. However, there are conflicting claims on the performance of these algorithms and no objective performance comparison has been revealed yet. In the previous work, only a few case study circuits have been under test to demonstrate the superiority of the utilized algorithm, so a limited comparison has been made for only these specific circuits. The underlying reason is that the literature lacks a generic benchmark for analog/RF circuit sizing problem. To address these issues, we propose a comprehensive comparison of the most popular two evolutionary computation algorithms, namely Non-Sorting Genetic Algorithm-II and Multi-Objective Evolutionary Algorithm based Decomposition, in this article. For that purpose, we introduce two ad hoc testbenches for analog and RF circuits including the common building blocks. The comparison has been made at both multi- and many-objective domains and the performances of algorithms have been quantitatively revealed through the well-known Pareto-optimal front quality metrics. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and PhrasesAnalog; CAD; design automation; EDA; evolutionary algorithms; many-objective; metaheuristics; MOEA/D; NSGA-II; optimization; RF; testbench,Biomimetics; Electric network analysis; Electronic design automation; Genetic algorithms; Integrated circuit manufacture; Timing circuits; Additional key word and phrasesanalog; Design automations; Electronics design automation; Key words; Many-objective; Metaheuristic; MOEA/D; NSGA-II; Optimisations; Radiofrequencies; Test-bench; Pareto principle
An Efficient Ring Oscillator PUF Using Programmable Delay Units on FPGA,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182595220&doi=10.1145%2f3593807&partnerID=40&md5=4fae392bec8f828618d5caf7ee8beb70,"The ring oscillator (RO) PUF can be implemented on different FPGA platforms with high uniqueness and reliability. To decrease the hardware cost of conventional RO PUFs, a new design using the programmable delay units is proposed, namely, PRO PUF. The programmable interconnect points (PIPs) of programmable delay units are used to enhance the configurability. The PUF cell of the proposed design has the ability to be efficiently programmed to an RO PUF at any stage by adjusting the propagation paths of the delay units. A significant number of responses can be generated by the proposed PRO PUF while consuming fewer hardware resources. To verify the performance, the proposed design has been implemented on Xilinx FPGAs and also simulated using a standard 40nm technology. The experimental results have shown that the proposed design achieves high uniqueness, reliability, and hardware efficiency. Moreover, the PRO PUF has been evaluated using a machine learning attack, the CMA-ES attack. The results have shown that the proposed structure is more resistant to common modeling attacks when compared to conventional RO-related PUF designs.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPUF; FPGAs; hardware security; programmable delay units; ring oscillator,Cryptography; Hardware security; Integrated circuit design; Additional key word and phrasespuf; Configurability; Delay units; Hardware cost; Hardware resources; Key words; Programmable delay units; Programmable interconnect points; Propagation paths; Ring oscillator; Field programmable gate arrays (FPGA)
The Resistance Analysis Attack and Security Enhancement of the IMC LUT Based on the Complementary Resistive Switch Cells,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182604385&doi=10.1145%2f3616870&partnerID=40&md5=305a4c51d9d203b14a615b23105a86e0,"Rresistive random access memory (RRAM) based in-memory computing (IMC) is an emerging architecture to address the challenge of the ""memory wall""problem. The complementary resistive switch (CRS) cell connects two bipolar RRAM elements anti-serially to reduce the sneak current in the crossbar array. The CRS array is a generic computing platform, for the arbitrary logic functions can be implemented in it. The IMC CRS LUT consumes fewer CRS cells than the static CRS LUT. The CRS array has built-in polymorphic characteristics because the correct logic function cannot be distinguished based on the circuit layout. However, the logic state of every CRS cell can be readout after each operation. It helps the attacker to recover the correct function of the IMC CRS LUT. This work discusses the resistance analysis attack of the IMC LUT based on the CRS array. The proposed resistance analysis attack method is able to be applied to different computation styles based on the CRS array, such as the CRS IMPLY, CRS NOR-OR/NAND-AND, and so on. The attacker can recover the logic function of the LUT by tracing the states of CRS cells. Furthermore, an improved IMC CRS LUT method is proposed and discussed to enhance security. The simulation and analysis results show that the improved IMC CRS LUT can resist various attacks, and it maintains the polymorphic characteristics of the IMC CRS LUT. And the N-bit full adder circuit based on the improved IMC CRS NOR-OR LUTs achieves the best performance compared with the previous counterparts. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and PhrasesIn-memory computing; complementary resistive switch (CRS); hardware security; look-up table (LUT); resistance analysis attack,Computation theory; Computer circuits; Cytology; RRAM; Table lookup; Additional key word and phrasesin-memory computing; Complementary resistive switch; Complementary resistive switches; Key words; Logic functions; Look-up table; Lookup tables (LUTs); Resistance analyse attack; Resistance analysis; Switch arrays; Cells
On-chip ESD Protection Design Methodologies by CAD Simulation,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182270931&doi=10.1145%2f3593808&partnerID=40&md5=2abb5309e48b1ffacbeac62345a8adc7,"Electrostatic discharge (ESD) can cause malfunction or failure of integrated circuits (ICs). On-chip ESD protection design is a major IC design-for-reliability (DfR) challenge, particularly for complex chips made in advanced technology nodes. Traditional trial-and-error approaches become unacceptable to practical ESD protection designs for advanced ICs. Full-chip ESD protection circuit design optimization, prediction, and verification become essential to advanced chip designs, which highly depends on CAD algorithm and simulation that has been a constant research topic for decades. This paper reviews recent advances in CAD-enabled on-chip ESD protection circuit simulation design technologies and ESD-IC co-design methodologies. Key challenges of ESD CAD design practices are outlined. Practical ESD protection simulation design examples are discussed.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesESD protection; CDM; co-design; HBM; HMM; IC; IEC; RF; TCAD; TLP; VFTLP,Circuit simulation; Electronic design automation; Electrostatic devices; Integrated circuit design; Integrated circuit manufacture; Integrated circuits; Additional key word and phrasesesd protection; CDM; Co-designs; HBM; HMM; IEC; Key words; RF; TCAD; TLP; VFTLP; Timing circuits
NPU-Accelerated Imitation Learning for Thermal Optimization of QoS-Constrained Heterogeneous Multi-Cores,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182592356&doi=10.1145%2f3626320&partnerID=40&md5=31c646fb93e3bebf648b361ab0522ac6,"Thermal optimization of a heterogeneous clustered multi-core processor under user-defined QoS targets requires application migration and DVFS. However, selecting the core to execute each application and the VF levels of each cluster is a complex problem because (1) the diverse characteristics and QoS targets of applications require different optimizations, and (2) per-cluster DVFS requires a global optimization considering all running applications. State-of-the-art resource management for power or temperature minimization either relies on measurements that are commonly not available (such as power) or fails to consider all the dimensions of the optimization (e.g., by using simplified analytical models). To solve this, ML methods can be employed. In particular, IL leverages the optimality of an oracle policy, yet at low run-time overhead, by training a model from oracle demonstrations. We are the first to employ IL for temperature minimization under QoS targets. We tackle the complexity by training NN at design time and accelerate the run-time NN inference using NPU. While such NN accelerators are becoming increasingly widespread, they are so far only used to accelerate user applications. In contrast, we use for the first time an existing accelerator on a real platform to accelerate NN-based resource management. To show the superiority of IL compared to RL in our targeted problem, we also develop multi-agent RL-based management. Our evaluation on a HiKey 970 board with an Arm big.LITTLE CPU and NPU shows that IL achieves significant temperature reductions at a negligible run-time overhead. We compare TOP-IL against several techniques. Compared to ondemand Linux governor, TOP-IL reduces the average temperature by up to 17 C at minimal QoS violations for both techniques. Compared to the RL policy, our TOP-IL achieves 63 % to 89 % fewer QoS violations while resulting similar average temperatures. Moreover, TOP-IL outperforms the RL policy in terms of stability. We additionally show that our IL-based technique also generalizes to different software (unseen applications) and even hardware (different cooling) than used for training. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and PhrasesMachine learning; AI accelerators; imitation learning; neural networks; processor scheduling; quality of service; task migration; thermal management,Application programs; Complex networks; Computer operating systems; Constrained optimization; Global optimization; Multi agent systems; Natural resources management; Program compilers; Resource allocation; Additional key word and phrasesmachine learning; AI accelerator; Imitation learning; Key words; Neural-networks; Processor scheduling; Quality-of-service; Runtimes; Task migration; Thermal optimization; Quality of service
Heterogeneous Integration Supply Chain Integrity Through Blockchain and CHSM,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182604404&doi=10.1145%2f3625823&partnerID=40&md5=f6f341899de5f74d3b15a7a651e7cc33,"Over the past few decades, electronics have become commonplace in government, commercial, and social domains. These devices have developed rapidly, as seen in the prevalent use of system-on-chips rather than separate integrated circuits on a single circuit board. As the semiconductor community begins conversations over the end of Moore's law, an approach to further increase both functionality per area and yield using segregated functionality dies on a common interposer die, labeled a System in Package (SiP), is gaining attention. Thus, the chiplet and SiP space has grown to meet this demand, creating a new packaging paradigm, advanced packaging, and a new supply chain. This new distributed supply chain with multiple chiplet developers and foundries has augmented counterfeit vulnerabilities. Chiplets are currently available on an open market, and their origin and authenticity consequently are difficult to ascertain. With this lack of control over the stages of the supply chain, counterfeit threats manifest at the chiplet, interposer, and SiP levels. In this article, we identify counterfeit threats in the SiP domain, and we propose a mitigating framework utilizing blockchain for the effective traceability of SiPs to establish provenance. Our framework utilizes the Chiplet Hardware Security Module to authenticate a SiP throughout its life. To accomplish this, we leverage SiP information including electronic chip identification of chiplets, combating die and IC recycling sensor information, documentation, test patterns and/or electrical measurements, grade, and part number of the SiP. We detail the structure of the blockchain and establish protocols for both enrolling trusted information into the blockchain network and authenticating the SiP. Our framework mitigates SiP counterfeit threats including recycled, remarked, cloned, overproduced interposer, forged documentation, and substituted chiplet while detecting of out-of-spec and defective SiPs. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and PhrasesSecure heterogeneous integration; blockchain; chiplet HSM; electronics supply chain; system in package security,Blockchain; Cloning; Crime; Internet protocols; Recycling; Supply chains; Additional key word and phrasessecure heterogeneous integration; Block-chain; Chiplet HSM; Electronics supply chain; Heterogeneous integration; Key words; Social domains; Supply chain integrities; System in package security; Systems in packages; System-in-package
ProtFe: Low-Cost Secure Power Side-Channel Protection for General and Custom FeFET-Based Memories,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182601532&doi=10.1145%2f3604589&partnerID=40&md5=23a7895f3247592ff7a05a73d5806eb8,"Ferroelectric Field Effect Transistors (FeFETs) have spurred increasing interest in both memories and computing applications, thanks to their CMOS compatibility, low-power operation, and high scalability. However, new security threats to the FeFET-based memories also arise. A major threat is the power analysis side-channel attack (P-SCA), which exploits the power traces of the memory access to obtain data information. There have been several effective efforts on resistive nonvolatile memories (NVMs), but they fail to meet the requirements for secure FeFET-based memories due to the different capacitive FeFETs load. Directly applying these existing countermeasures to the P-SCA protection for FeFETs induces huge challenges, especially for the balance between power side-channel resistance and corresponding overheads.To address this issue, we leverage the unique features of FeFETs and propose ProtFe, namely the protection methods for FeFET-based memories, including the pipelined multi-step write strategy (PiMWrite) and the split array design (SpA). PiMWrite is proposed for general FeFET-based memories, and inserts specially designed intermediate states to mitigate information leakage with pipelined steps to reduce overheads. SpA is proposed for custom FeFET-based memories, and simultaneously writes two split portions of the array with shared minimized peripherals to go beyond the balance between security and overheads. Simulation results show that PiMWrite expands the search space of a single power trace to 21× and involves nearly zero hardware penalties. SpA presents 33× search space improvement with negligible latency, 0.6% area, and only 7.1% energy overhead. ProtFe achieves improved balance between security and overheads, compared with the state-of-the-art works.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPower analysis; data privacy; ferroelectric field effect transistors; hardware security; nonvolatile memory,Computing power; Costs; Ferroelectricity; Field effect transistors; Hardware security; Pipelines; Side channel attack; Additional key word and phrasespower analyse; Array design; Ferroelectric fieldeffect transistors (FeFET); Key words; Multisteps; Non-volatile memory; Nonvolatile memory; Power; Side-channel; Write strategy; Data privacy
ICP-RL: Identifying Critical Paths for Fault Diagnosis Using Reinforcement Learning,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182592380&doi=10.1145%2f3610294&partnerID=40&md5=80247605fb1d6352b0b8ef4ee61b2911,"Identifying the critical paths is crucial to reducing the complexity of performance analysis and reliability calculation for logic circuits. In this article, we propose a method for identifying the critical path in a combination circuit using a reinforcement learning framework to enhance its applicability and compatibility. Initially, we configured the learning environment of the model based on circuit structure information to provide valuable information for decision-making on time. Subsequently, the upper confidence bound applied to trees (UCT) algorithm is employed to construct the behavior decision strategy of the model, which avoids invalid traversal and reduces computing costs. Then, a goal-oriented reward and punishment function is constructed based on the distance from the circuit primary outputs. Finally, based on the parallel computing strategy, we construct an adaptive training method to improve the model's prediction accuracy by using finite sampling, which speeds up the convergence speed and enhances the quality of the model. Experimental results on benchmark circuits show that, with the functional timing analysis method as the reference, the average accuracy of the proposed method is as high as 99.39% and the single average calculation speed is 18.07 times faster than that of the reference method. Compared with the Monte Carlo model, the proposed method has a higher critical path hit rate, and the average calculation speed is 928.75 times faster. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and PhrasesUpper confidence bound applied to tree (UCT); circuit reliability; critical path; parallel training; reinforcement learning,Computer aided instruction; Computer circuits; Decision making; Electric network analysis; Learning systems; Logic circuits; Monte Carlo methods; Reliability analysis; Timing circuits; Trees (mathematics); Additional key word and phrasesupper confidence bound applied to tree (UCT); Calculation speed; Circuit reliability; Confidence bounds; Critical Paths; Faults diagnosis; Key words; Parallel training; Performance reliability; Reinforcement learnings; Reinforcement learning
A Compact TRNG Design for FPGA Based on the Metastability of RO-driven Shift Registers,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182600571&doi=10.1145%2f3610295&partnerID=40&md5=4ca1d855179974ad91c0240b35544351,"True random number generators (TRNGs), as an important component of security systems, have received a lot of attention for their related research. The previous researches have provided a large number of TRNG solutions, however, they still failed to reach an excellent tradeoff in various performance metrics. This article presents a shift-registers metastability-based TRNG, which is implemented by compact reference units and comparison units. By forcing the D flip-flops in the shift-registers into the metastable state, it optimizes the problem that the conventional metastability entropy sources consume excessive hardware resources. And a new method of metastable randomness extraction is used to reduce the bias of metastable output. The proposed TRNG is implemented in Xilinx Spartan-6 and Virtex-6 FPGAs, which generate random sequences that pass the NIST SP800-22, NIST SP800-90B tests and show excellent robustness to voltage and temperature variations. This TRNG can consume only 3 slices of the FPGA, but it has a high throughput rate of 25 Mbit/s. In comparison with state-of-the-art FPGA-compatible TRNGs, the proposed TRNG achieves the highest figure of merit FOM, which means that the proposed TRNG significantly outperforms previous researches in terms of hardware resources, throughput rate, and operating frequency tradeoffs. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and PhrasesTrue random number generator; field programmable gate array (FPGA); metastability; shift-register,Field programmable gate arrays (FPGA); Flip flop circuits; Integrated circuit design; Number theory; Random number generation; Additional key word and phrasestrue random number generator; Field programmable gate array; Field programmables; Hardware resources; Key words; Metastabilities; Metastables; Programmable gate array; Random number generators; True randoms; Shift registers
Design of Enhanced Reversible 9T SRAM Design for the Reduction in Sub-threshold Leakage Current with14nm FinFET Technology,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178019506&doi=10.1145%2f3616538&partnerID=40&md5=fde8132d348dcf52d5a47e1b1559aed8,"Power dissipation is considered one of the important issues in low power Very-large-scale integration (VLSI) circuit design and is related to the threshold voltage. Generally, the sub-threshold leakage current and the leakage power dissipation are increased by reducing the threshold voltage. The overall performance of the circuit completely depends on this leakage power dissipation because this leakage and power consumption causes the components that are functioning by the battery for a long period to be washed-out rapidly. In this research, the reversible logic gate-based 9T static random access memory (SRAM) is designed in 14nm FinFET technology to reduce leakage power consumption in memory related applications. The Schmitt-trigger (ST)-based 9T SRAM cell is designed to attain high read-write stability and low power consumption using a single bit line structure. The reversible logic gates of Feynman (FG) and Fredkin gate (FRG) are combined to develop a row and column decoder in an SRAM design to diminish the leakage power. Moreover, the transistor stacking effect is applied to the proposed memory design to reduce the leakage power in active mode. The proposed reversible logic and transistor stacking based SRAM design is implemented in Tanner EDA Tool version 16.0. It also performs both read and write operations using the proposed circuit. The performance measures of read access time (RAT), write access time (WAT), read, write, and static power by varying supply voltage and temperature, delay and stability analysis (read/write static noise margin) are examined and compared with existing SRAM designs.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Feynman (FG); Fredkin gate (FRG); Schmitt-trigger,Computer circuits; Electric losses; Electric power utilization; Field programmable gate arrays (FPGA); FinFET; Integrated circuit design; Integrated circuit manufacture; Leakage currents; Logic gates; Low power electronics; Static random access storage; VLSI circuits; Feynman (FG); Fredkin gate; Leakage power; Leakage power dissipations; Memory design; Reversible logic gates; Schmitt trigger; Static random access memory; Sub-threshold leakage currents; Threshold voltage
Surrogate Lagrangian Relaxation: A Path to Retrain-Free Deep Neural Network Pruning,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178075415&doi=10.1145%2f3624476&partnerID=40&md5=b8ada09bb0c7c65218dd50b5a7be32fb,"Network pruning is a widely used technique to reduce computation cost and model size for deep neural networks. However, the typical three-stage pipeline (i.e., training, pruning, and retraining (fine-tuning)) significantly increases the overall training time. In this article, we develop a systematic weight-pruning optimization approach based on surrogate Lagrangian relaxation (SLR), which is tailored to overcome difficulties caused by the discrete nature of the weight-pruning problem. We further prove that our method ensures fast convergence of the model compression problem, and the convergence of the SLR is accelerated by using quadratic penalties. Model parameters obtained by SLR during the training phase are much closer to their optimal values as compared to those obtained by other state-of-the-art methods. We evaluate our method on image classification tasks using CIFAR-10 and ImageNet with state-of-the-art multi-layer perceptron based networks such as MLP-Mixer; attention-based networks such as Swin Transformer; and convolutional neural network based models such as VGG-16, ResNet-18, ResNet-50, ResNet-110, and MobileNetV2. We also evaluate object detection and segmentation tasks on COCO, the KITTI benchmark, and the TuSimple lane detection dataset using a variety of models. Experimental results demonstrate that our SLR-based weight-pruning optimization approach achieves a higher compression rate than state-of-the-art methods under the same accuracy requirement and also can achieve higher accuracy under the same compression rate requirement. Under classification tasks, our SLR approach converges to the desired accuracy × faster on both of the datasets. Under object detection and segmentation tasks, SLR also converges 2× faster to the desired accuracy. Further, our SLR achieves high model accuracy even at the hardpruning stage without retraining, which reduces the traditional three-stage pruning into a two-stage process. Given a limited budget of retraining epochs, our approach quickly recovers the model's accuracy.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",image classification; model compression; object detection and segmentation; Surrogate Lagrangian relaxation; weight pruning,Budget control; Classification (of information); Deep neural networks; Image segmentation; Lagrange multipliers; Multilayer neural networks; Network layers; Object detection; Object recognition; Images classification; Lagrangian relaxations; Model compression; Network pruning; Objects detection; Objects segmentation; Optimization approach; State-of-the-art methods; Surrogate lagrangian relaxation; Weight pruning; Image classification
Sequential Routing-based Time-division Multiplexing Optimization for Multi-FPGA Systems,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178016540&doi=10.1145%2f3626322&partnerID=40&md5=d97c07b0f1d7679436bd41f7caa4bc04,"Multi-field programming gate array (FPGA) systems are widely used in various circuit design-related areas, such as hardware emulation, virtual prototypes, and chiplet design methodologies. However, a physical resource clash between inter-FPGA signals and I/O pins can create a bottleneck in a multi-FPGA system. Specifically, inter-FPGA signals often outnumber I/O pins in a multi-FPGA system. To solve this problem, time-division multiplexing (TDM) is introduced. However, undue time delay caused by TDM may impair the performance of a multi-FPGA system. Therefore, a more efficient TDM solution is needed. In this work, we propose a new routing sequence strategy to improve the efficiency of TDM. Our strategy consists of two parts: a weighted routing algorithm and TDM assignment optimization. The algorithm takes into account the weight of the net to generate a high-quality routing topology. Then, a net-based TDM assignment is performed to obtain a lower TDM ratio for the multi-FPGA system. Experiments on the public dataset of CAD Contest 2019 at ICCAD showed that our routing sequence strategy achieved good results. Especially in those testcases of unbalanced designs, the performance of multi-FPGA systems was improved up to 2.63. Moreover, we outperformed the top two contest finalists as to TDM results in most of the testcases.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Multi-FPGA system; sequential routing; time-division multiplexing,Computer aided design; Field programmable gate arrays (FPGA); Integrated circuit design; Array signals; Array systems; Field-programming gate arrays; Multi-field; Multi-field programming gate array system; Optimisations; Performance; Routings; Sequential routing; Time-division multiplexing; Time division multiplexing
"Task Modules Partitioning, Scheduling and Floorplanning for Partially Dynamically Reconfigurable Systems with Heterogeneous Resources",2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178083969&doi=10.1145%2f3625295&partnerID=40&md5=ee64b9c12312e96d32bf1527a29543cf,"Some field programmable gate arrays (FPGAs) can be partially dynamically reconfigurable with heterogeneous resources distributed on the chip. FPGA-based partially dynamically reconfigurable system (FPGA-PDRS) can be used to accelerate computing and improve computing flexibility. However, the traditional design of FPGA-PDRS is based on manual design. Implementing the automation of FPGA-PDRS needs to solve the problems of task modules partitioning, scheduling, and floorplanning on heterogeneous resources. Existing works only partly solve problems for the automation process of FPGA-PDRS or model homogeneous resources for FPGA-PDRS. To better solve the problems in the automation process of FPGA-PDRS and narrow the gap between algorithm and application, in this paper, we propose a complete workflow including three parts: pre-processing to generate the lists of task module candidate shapes according to the resource requirements, exploration process to search the solution of task modules partitioning, scheduling, and floorplanning, and post-optimization to improve the floorplan success rate. Experimental results show that, compared with state-of-the-art work, the pre-processing process can reduce the occupied area of task modules by 6% on average; the proposed complete workflow can improve performance by 9.6%, and reduce communication cost by 14.2% with improving the resources reuse rate of the heterogeneous resources on the chip. Based on the solution generated by the exploration process, the post-optimization process can improve the floorplan success rate by 11%.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Floorplan; Heterogeneous resources; ILP; partition; PDRS; RRs; schedule,Field programmable gate arrays (FPGA); Integrated circuit design; Structural design; Field programmables; Floor-planning; Floorplans; Heterogeneous resources; ILP; Partition; PDRS; Programmable gate array; RR; Schedule; Automation
Automatic Synthesis of FSMs for Enforcing Non-functional Requirements on MPSoCs Using Multi-objective Evolutionary Algorithms,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178032720&doi=10.1145%2f3617832&partnerID=40&md5=720424c2a4749b7ab4704fe82e081995,"Embedded system applications often require guarantees regarding non-functional properties when executed on a given MPSoC platform. Examples of such requirements include real-time, energy, or safety properties on corresponding programs. One option to implement the enforcement of such requirements is by a reactive control loop, where an enforcer decides based on a system response (feedback) how to control the system, e.g., by adapting the number of cores allocated to a program or by scaling the voltage/frequency mode of involved processors.Typically, a violation of a requirement must either never happen in case of strict enforcement, or only happen temporally (in case of so-called loose enforcement). However, it is a challenge to design enforcers for which it is possible to give formal guarantees with respect to requirements, especially in the presence of typically largely varying environmental input (workload) per execution. Technically, an enforcement strategy can be formally modeled by a finite state machine (FSM) and the uncertain environment determining the workload by a discrete-time Markov chain. It has been shown in previous work that this formalization allows the formal verification of temporal properties (verification goals) regarding the fulfillment of requirements for a given enforcement strategy.In this article, we consider the so-far-unsolved problem of design space exploration and automatic synthesis of enforcement automata that maximize a number of deterministic and probabilistic verification goals formulated on a given set of non-functional requirements. For the design space exploration (DSE), an approach based on multi-objective evolutionary algorithms is proposed in which enforcement automata are encoded as genes of states and state transition conditions. For each individual, the verification goals are evaluated using probabilistic model checking. At the end, the DSE returns a set of efficient FSMs in terms of probabilities of meeting given requirements. As experimental results, we present three use cases while considering requirements on latency and energy consumption.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",design space exploration; Enforcement FSM; evolutionary algorithm; finite state machine; genetic algorithm; markov chain; MPSoC; optimization; PCTL; probabilistic model cheking; runtime requirement enforcement; steady state; verification,Automata theory; Computer aided software engineering; Formal verification; Genetic algorithms; Markov processes; Model checking; Multiprocessing systems; System-on-chip; Design space exploration; Enforcement finite state machine; Finite states machine; MPSoC; Optimisations; PCTL; Probabilistic model cheking; Probabilistic models; Runtime requirement enforcement; Runtimes; Steady state; Energy utilization
A High-performance Masking Design Approach for Saber against High-order Side-channel Attack,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178049276&doi=10.1145%2f3611670&partnerID=40&md5=7cffaf2b7a166b3bdf384bcbd9c1f165,"Post-quantum cryptography (PQC) has become the most promising cryptographic scheme against the threat of quantum computing to conventional public-key cryptographic schemes. Saber, as the finalist in the third round of the PQC standardization procedure, presents an appealing option for embedded systems due to its high encryption efficiency and accessibility. However, side-channel attack (SCA) can easily reveal confidential information by analyzing the physical manifestations, and several works demonstrate that Saber is vulnerable to SCAs. In this work, a ciphertext comparison method for masking design based on the bitslicing technique and zerotest is proposed, which balances the tradeoff between the performance and security of comparing two arrays. The mathematical description of the proposed ciphertext comparison method is provided, and its correctness and security metrics are analyzed under the concept of PINI. Moreover, a high-order masking approach based on the state of the art, including the hash functions, centered binomial sampling, masking conversions, and proposed ciphertext comparison, is presented, using the bitslicing technique to improve throughput. As a proof of concept, the proposed implementation of Saber is on the ARM Cortex-M4. The performance results show that the runtime overhead factor of 1st-, 2nd-, and 3rd-order masking is 3.01×, 5.58×, and 8.68×, and the dynamic memory used for 1st-, 2nd-, and 3rd-order masking is 17.4kB, 24.0kB, and 30.2kB, respectively. The SCA-resilience evaluation results illustrate that the 1st-order Test Vectors Leakage Assessment (TVLA) result fails to reveal the secret key with 100,000 traces.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",high-order masking; PINI; Post-quantum cryptography; Saber; test vector leakage assessment,Design; Embedded systems; Hash functions; Quantum computers; Quantum cryptography; High-order; High-order masking; Higher-order; Leakage assessment; Performance; PINI; Post quantum cryptography; Saber; Test vector leakage assessment; Test vectors; Side channel attack
"A Reconfigurable 7T SRAM Bit Cell for High Speed, Power Saving and Low Voltage Application",2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178023840&doi=10.1145%2f3616872&partnerID=40&md5=10b71ec834a95f6f660212a627cd9645,"The decreasing operational voltage and scaled technology node for memory designing has widened the gap between two crucial parameters for an SRAM - delay and power. As the demand for internet of things is increasing, the need for round the clock connectivity is increasing. This mandates designing a cell with a capability to switch between low power and high speed operation. Thus, this paper presents the design of dual mode operational 7T cell that can switch between single port and dual port configuration. The proposed reconfigurable cell can operate as single port or dual port cell single ended 7T cell. The reconfigurability in the cell is realized using control signals. The noise stability of the bit cell is obtained to be 333, 333, and 470 mV for read, hold, and write modes, respectively. The robustness of the cell against temperature variation, process variation and voltage variation is also analyzed. The performance variation in each parameter will not have a dramatic impact as it is within manageable limit. Its write time is 0.14 ns, while 5 ps are required for a successful read operation. The dual port configuration of the cell supports pipelining and thus operates faster than its single port configuration.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",high speed; internet of things; low power; process tolerant; Reconfigurable cell,Cells; Computer aided design; Cytology; Integrated circuit design; Static random access storage; Cell-be; Cell/B.E; Cell/BE; Dual port; High Speed; Low Power; Process tolerant; Reconfigurable; Reconfigurable cell; SRAM bit cell; Internet of things
Programmable In-memory Computing Circuit of Fast Hartley Transform,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178071882&doi=10.1145%2f3618112&partnerID=40&md5=70a3be9dd33867c5879bd7ec3a8d5237,"Discrete Hartley transform is a core component of digital signal processing because of its advantages of fast computing speed and less power consumption. Traditional FPGA-based implementation methods have the disadvantage of high latency, which cannot meet the needs of energy-efficient computing in the Internet of Things era. Therefore, A programmable analog memory computing circuit is proposed to accelerate FHT and IFHT calculations for large-scale one-step matrix computation. By adjusting the weight of memristor, different scales of FHT calculation can be achieved. PSPICE simulation results show that the average accuracy of the proposed circuit can reach 99.9%, and the speed can also reach the level of 0.1 μs. The robustness analysis shows that the circuit can tolerate a certain degree of programming error and resistance tolerance. The designed analog circuit is applied to image compression processing, and the image compression accuracy can reach 99.9%.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",circuit design; FHT; IFHT; in-memory computing; memristor,Digital signal processing; Energy efficiency; Green computing; Image compression; Integrated circuit manufacture; Matrix algebra; SPICE; Timing circuits; Circuit designs; Computing circuits; Core components; Discrete Hartley transforms; Fast hartley transforms; FHT; IFHT; Images compression; In-memory computing; Memristor; Memristors
Self Adaptive Logical Split Cache Techniques for Delayed Aging of NVM LLC,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178021021&doi=10.1145%2f3616871&partnerID=40&md5=9d4edd98830412d882118329477d5a1d,"Due to the technological advancements in the last few decades, several applications have emerged that demand more computing power and on-chip and off-chip memories. However, the scaling of memory technologies is not at par with computing throughput of modern day multi-core processors. Conventional memory technologies such as SRAM and DRAM have technological limitations to meet large on-chip memory requirements owing to their low packaging density and high leakage power. In order to meet the ever-increasing demand for memory, researchers came up with alternative solutions, such as emerging non-volatile memory technologies such as STT-RAM, PCM, and ReRAM. However, these memory technologies have limited write endurance and high write energy. This emphasizes the need for a policy that will reduce the writes or distribute the writes uniformly across the memory thereby enhancing its lifetime by delaying the early wear out of memory cells due to frequent writes. We propose two techniques, Enhanced-Virtually Split Cache (E-ViSC) and Protean-Virtually Split Cache (P-ViSC), which dynamically adjust the cache configuration to distribute the writes uniformly across the memory to enhance the lifetime. Experimental studies show that E-ViSC and P-ViSC improve lifetime of NVM L2 caches by upto 2.31× and 1.97× respectively.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",last-level cache; Non volatile memory; STT-RAM; wear-leveling,Cache memory; Computing power; Nonvolatile storage; Static random access storage; Wear of materials; Computing power; Delayed aging; Last-level caches; Memory technology; Non-volatile memory; Power chips; Split cache; Stt rams; Technological advancement; Wear-Leveling; Dynamic random access storage
Modified Decoupled Sense Amplifier with Improved Sensing Speed for Low-Voltage Differential SRAM,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178032824&doi=10.1145%2f3611672&partnerID=40&md5=838d29736ce5f5e0229c03dfeb5b28eb,"A modified decoupled sense amplifier (MDSA) and modified decoupled sense amplifier with NMOS foot-switch is proposed for improved sensing in differential SRAM for low-voltage operation at the 22-nm technology node. The MDSA and MDSANF both offer notable improvements to read delay over conventional voltage and current sense amplifiers. At an operating voltage of 0.8 V, the MDSA exhibited a reduced delay of 28.6%, 41.79%, 37.74%, and 30.94% compared to modified clamped sense amplifier (MCSA), double tail sense amplifier (DTSA), modified hybrid sense amplifier (MHSA), and conventional latch-type sense amplifier (LSA), respectively. Similarly, the MDSANF demonstrated a delay reduction of 26.13%, 39.78%, 35.58%, and 28.55% over MCSA, DTSA, MHSA, and LSA, respectively. To validate the performance, the MDSA and MDSANF are evaluated using the variation in delay and power consumption across various supply voltages, process corners, input differential bit line voltage (ΔVBL), bit line capacitance (CBL), and the sizing of decoupling transistors. Monte Carlo simulations were conducted to analyse the impact of voltage threshold variations on transistor mismatch which leads to an increased occurrence of read failures and a decline in SRAM yield. The performance analysis of various voltage and current sense amplifiers is presented along with MDSA and MDSANF. Area consideration for selection of sensing scheme is important and as such layout of MDSA and MDSANF was performed conforming to the design rules and estimated area for MDSA is 0.297 μm2 whereas MDSANF occupies 0.5192 μm2  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Low voltage SRAM; sense amplifier; sensing delay; SRAM sensing; transient analysis,Capacitance; Differential amplifiers; Intelligent systems; Monte Carlo methods; Current sense amplifiers; Low voltage operation; Low voltages; Low-voltage SRAM; Sense amplifier; Sensing delay; Sensing speed; SRAM sensing; Technology nodes; Voltage differentials; Transient analysis
Enhanced PATRON: Fault Injection and Power-aware FSM Encoding Through Linear Programming,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178080302&doi=10.1145%2f3611669&partnerID=40&md5=5008d68bffcb60febea26116e9a338c9,"Since finite state machines (FSMs) regulate the control flow in circuits, a computing system's security might be breached by attacking the FSM. Physical attacks are especially worrisome because they can bypass software countermeasures. For example, an attacker can gain illegal access to the sensitive states of an FSM through fault injection, leading to privilege escalation and/or information leakage. Laser fault injection (LFI) provides one of the most effective attack vectors by enabling adversaries to precisely overturn single flip-flops states. Although conventional error correction/detection methodologies have been employed to improve FSM resiliency, their substantial overhead makes them unattractive to circuit designers. In our prior work, a novel decision diagram-based FSM encoding scheme called PATRON was proposed to resist LFI according to attack parameters, e.g., number of simultaneous faults. Although PATRON bested traditional encodings keeping overhead minimum, it provided numerous candidates for FSM designs requiring exhaustive and manual effort to select one optimum candidate. In this article, we automatically select an optimum candidate by enhancing PATRON using linear programming (LP). First, we exploit the proportionality between dynamic power dissipation and switching activity in digital CMOS circuits. Thus, our LP objective minimizes the number of FSM bit switches per transition, for comparatively lower switching activity and hence total power consumption. Second, additional LP constraints along with incorporating the original PATRON rules, systematically enforce bidirectionality to at least two state elements per FSM transition. This bestows protection against different types of fault injection, which we capture with a new unidirectional metric. Enhanced PATRON (EP) achieves superior security at lower power consumption in average compared to PATRON, error-coding, and traditional FSM encoding on five popular benchmarks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Electric power utilization; Error correction; Flip flop circuits; Linear programming; Power management; Signal encoding; Software testing; Computing system; Control-flow; Encodings; Fault injection; Finite states machine; Laser fault injections; Linear-programming; Power-aware; Switching activities; System security; Encoding (symbols)
SoC Protocol Implementation Verification Using Instruction-Level Abstraction Specifications,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178027141&doi=10.1145%2f3610292&partnerID=40&md5=60b7380df72062bd1e1231007bc3a8fe,"In modern systems-on-chips, several hardware protocols are used for communication and interaction among different modules. These protocols are complex and need to be implemented correctly for correct operation of the system-on-chip. Therefore, protocol verification has received significant attention. However, this verification is often limited to checking high-level properties on a protocol specification or an implementation. Verifying these properties directly on an implementation faces scalability challenges due to its size and design complexity. Further, even after some high-level properties are verified, there is no guarantee that an implementation fully complies with a given specification, even if the same properties have also been checked on the specification. We address these challenges and gaps by adding a layer of component specifications, one for each component in the protocol implementation, and specifying and verifying the interactions at the interfaces between each pair of communicating components. We use the recently proposed formal model termed Instruction-Level Abstraction (ILA) as a component specification, which includes an interface specification for the interactions in composing different components. The use of ILA models as component specifications allows us to decompose the complete verification task into two sub-tasks: checking that the composition of ILAs is sequentially equivalent to a verified formal protocol specification, and checking that the protocol implementation is a refinement of the ILA composition. This check requires that each component implementation is a refinement of its ILA specification and includes interface checks guaranteeing that components interact with each other as specified. We have applied the proposed ILA-based methodology for protocol verification to several third-party design case studies. These include an AXI on-chip communication protocol, an off-chip communication protocol, and a cache coherence protocol. For each system, we successfully detected bugs in the implementation, and show that the full formal verification can be completed in reasonable time and effort.  © 2023 Copyright held by the owner/author(s).",formal verification; hardware protocol specification; instruction-level abstraction; refinement checking; sequential equivalence checking; System-on-chip,Abstracting; Application specific integrated circuits; Formal verification; Integrated circuit design; Program debugging; Programmable logic controllers; Specifications; Component specification; Hardware protocol specification; Instruction-level; Instruction-level abstraction; Property; Protocol implementation; Protocol specifications; Refinement checking; Sequential equivalence checking; Systems-on-Chip; System-on-chip
Systemization of Knowledge: Robust Deep Learning using Hardware-software co-design in Centralized and Federated Settings,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178017766&doi=10.1145%2f3616868&partnerID=40&md5=8f114c0aa12b969f39d481493e80cac3,"Deep learning (DL) models are enabling a significant paradigm shift in a diverse range of fields, including natural language processing and computer vision, as well as the design and automation of complex integrated circuits. While the deep models - and optimizations based on them, e.g., Deep Reinforcement Learning (RL) - demonstrate a superior performance and a great capability for automated representation learning, earlier works have revealed the vulnerability of DL to various attacks. The vulnerabilities include adversarial samples, model poisoning, and fault injection attacks. On the one hand, these security threats could divert the behavior of the DL model and lead to incorrect decisions in critical tasks. On the other hand, the susceptibility of DL to potential attacks might thwart trustworthy technology transfer as well as reliable DL deployment. In this work, we investigate the existing defense techniques to protect DL against the above-mentioned security threats. Particularly, we review end-to-end defense schemes for robust deep learning in both centralized and federated learning settings. Our comprehensive taxonomy and horizontal comparisons reveal an important fact that defense strategies developed using DL/software/hardware co-design outperform the DL/software-only counterparts and show how they can achieve very efficient and latency-optimized defenses for real-world applications. We believe our systemization of knowledge sheds light on the promising performance of hardware-software co-design of DL security methodologies and can guide the development of future defenses.  © 2023 Copyright held by the owner/author(s).",federated learning; Machine learning; robustness; security,Computer hardware; Learning algorithms; Learning systems; Natural language processing systems; Network security; Reinforcement learning; Security systems; Software design; Technology transfer; Centralised; Federated learning; Hardware/software codesign; Learning models; Learning software; Machine-learning; Performance; Robustness; Security; Security threats; Deep learning
A General Layout Pattern Clustering Using Geometric Matching-based Clip Relocation and Lower-bound Aided Optimization,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178039204&doi=10.1145%2f3610293&partnerID=40&md5=bd097f3c5ae8fa5013ac5c076f44d6c4,"With the continuous shrinking of feature size, detection of lithography hotspots has been raised as one of the major concerns in Design-for-Manufacturability (DFM) of semiconductor processing. Hotspot detection, along with other DFM measures, trades off turnaround time for the yield of IC manufacturing, and thus a simplified but wide-ranging pattern definition is a key to the problem. Layout pattern clustering methods, which group geometrically similar layout clips into clusters, have been vastly proposed to identify layout patterns efficiently. To minimize the clustering number for subsequent DFM processing, in this article, we propose a geometric-matching-based clip relocation technique to increase the opportunity of pattern clustering. Particularly, we formulate the lower bound of the clustering number as a maximum-clique problem, and we have also proved that the clustering problem can be solved by the result of the maximum-clique very efficiently. Compared with the experimental results of the state-of-the-art approaches on ICCAD 2016 Contest benchmarks, the proposed method can achieve the optimal solutions for all benchmarks with very competitive runtime. To evaluate the scalability, the ICCAD 2016 Contest benchmarks are extended and evaluated. Moreover, experimental results on the extended benchmarks demonstrate that our method can reduce the cluster number by 16.59% on average, while the runtime is 74.11% faster on large-scale benchmarks compared with previous works.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",clip pattern clustering; clip relocation; cluster number minimization; Design for manufacturability; lithography hotspot,Clustering algorithms; Computer aided design; Design for manufacturability; Lithography; Clip pattern clustering; Clip relocation; Cluster number minimization; Cluster numbers; Design-for-manufacturability; Geometric matching; Layout patterns; Lithography Hotspots; Minimisation; Pattern clustering; Machine design
Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178029067&doi=10.1145%2f3611673&partnerID=40&md5=55c5e679bec08f192de58ca239eda1ba,"The unprecedented accuracy of convolutional neural networks (CNNs) across a broad range of AI tasks has led to their widespread deployment in mobile and embedded settings. In a pursuit for high-performance and energy-efficient inference, significant research effort has been invested in the design of field-programmable gate array (FPGA)-based CNN accelerators. In this context, single computation engines constitute a popular design approach that enables the deployment of diverse models without the overhead of fabric reconfiguration. Nevertheless, this flexibility often comes with significantly degraded performance on memory-bound layers and resource underutilisation due to the suboptimal mapping of certain layers on the engine's fixed configuration. In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at runtime. We refer to these approaches as on-the-fly. This article presents unzipFPGA, a novel CNN inference system that counteracts the limitations of existing CNN engines. The proposed framework comprises a novel CNN hardware architecture that introduces a weights generator module that enables the on-chip on-the-fly generation of weights, alleviating the negative impact of limited bandwidth on memory-bound layers. We further enhance unzipFPGA with an automated hardware-aware methodology that tailors the weights generation mechanism to the target CNN-device pair, leading to an improved accuracy-performance balance. Finally, we introduce an input selective processing element (PE) design that balances the load between PEs in suboptimally mapped layers. Quantitative evaluation shows that the proposed framework yields hardware designs that achieve an average of 2.57× performance efficiency gain over highly optimised GPU designs for the same power constraints and up to 3.94× higher performance density over a diverse range of state-of-the-art FPGA-based CNN accelerators.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",hardware accelerator; Neural networks; weights generation,Convolution; Energy efficiency; Engines; Field programmable gate arrays (FPGA); Integrated circuit design; Machine design; Convolutional neural network; Field programmables; Hardware accelerators; Memory bounds; Memory wall; Network engines; Neural-networks; Performance; Programmable gate array; Weights generations; Convolutional neural networks
TMDS: Temperature-aware Makespan Minimizing DAG Scheduler for Heterogeneous Distributed Systems,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178063939&doi=10.1145%2f3616869&partnerID=40&md5=a2b355802b8b32d2a570cf915c72ab9b,"To meet application-specific performance demands, recent embedded platforms often involve the use of intricate micro-architectural designs and very small feature sizes leading to complex chips with multi-million gates. Such ultra-high gate densities often make these chips susceptible to inappropriate surges in core temperatures. Temperature surges above a specific threshold may throttle processor performance, enhance cooling costs, and reduce processor life expectancy. This work proposes a generic temperature management strategy that can be easily employed to adapt existing state-of-the-art task graph schedulers so that schedules generated by them never violate stipulated thermal bounds. The overall temperature-aware task graph scheduling problem has first been formally modeled as a constraint optimization formulation whose solution is shown to be prohibitively expensive in terms of computational overheads. Based on insights obtained through the formal model, a new fast and efficient heuristic algorithm called TMDS has been designed. Experimental evaluation over diverse test case scenarios shows that TMDS is able to deliver lower schedule lengths compared to the temperature-aware versions of four prominent makespan minimizing algorithms, namely, HEFT, PEFT, PPTS, and PSLS. Additionally, a case study with an adaptive cruise controller in automotive systems has been included to exhibit the applicability of TMDS in real-world settings.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",DAG scheduling; distributed systems; heterogeneous platforms; list scheduling; makespan; temperature-aware,Adaptive control systems; Adaptive cruise control; Heuristic algorithms; Application specific; DAG scheduling; Distributed systems; Embedded platforms; Heterogeneous distributed systems; Heterogeneous platforms; List-scheduling; Makespan; Performance; Temperature aware; Constrained optimization
Multi-target Fluid Mixing in MEDA Biochips: Theory and an Attempt toward Waste Minimization,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178039539&doi=10.1145%2f3622785&partnerID=40&md5=c8c0958410304d7521e608736da23970,"Sample preparation is an inherent procedure of many biochemical applications, and digital microfluidic biochips (DMBs) have proved to be very effective in performing such a procedure. In a single mixing step, conventional DMBs can mix two droplets in a 1:1 ratio only. Due to this limitation, DMBs suffer from heavy fluid wastage and often require a lot of mixing steps. However, the next-generation DMBs, i.e., micro-electrode-dot-array (MEDA) biochips, can realize multiple mixing ratios, which in general helps in minimizing the number of mixing operations. In this article, we present a heuristic-based sample preparation algorithm, specifically a mixing algorithm called Division by Factor Method for MEDA that exploits the mixing models of MEDA biochips. We propose another mixing algorithm for MEDA biochips called Single Target Waste Minimization (STWM), which minimizes the wastage of fluids and determines an efficient mixing graph. We also propose an advanced methodology for multiple target reagent mixing problems called Multi-target Waste Minimization (MTWM), which determines efficient mixing graphs for different target ratios by maximizing the sharing of fluids and minimizing the fluid wastage. Simulation results suggest that the proposed STWM and MTWM methods outperform the state-of-the-art methods in terms of minimizing the amount of fluid wastage, reducing the total usage of reagent fluids, and minimizing the number of mixing operations.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Biochips; digital microfluidics; MEDA biochips; mixing; sample preparation,Biochips; Digital microfluidics; Electrodes; Heuristic methods; Digital microfluidic biochips; Dot array; Fluid mixing; Micro-electrode-dot-array biochip; Micro-electrodes; Minimizing the number of; Mixing algorithms; Multi-targets; Sample preparation; Waste minimization; Mixing
QuanDA: GPU Accelerated Quantitative Deep Neural Network Analysis,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178040139&doi=10.1145%2f3611671&partnerID=40&md5=8bdcdda86b4ae09dde57da3fbb6e86da,"Over the past years, numerous studies demonstrated the vulnerability of deep neural networks (DNNs) to make correct classifications in the presence of small noise. This motivated the formal analysis of DNNs to ensure that they delineate acceptable behavior. However, in the case that the DNN's behavior is unacceptable for the desired application, these qualitative approaches are ill equipped to determine the precise degree to which the DNN behaves unacceptably. We propose a novel quantitative DNN analysis framework, QuanDA, which not only checks whether the DNN delineates certain behavior but also provides the estimated probability of the DNN to delineate this particular behavior. Unlike the (few) available quantitative DNN analysis frameworks, QuanDA does not use any implicit assumptions on the probability distribution of the hidden nodes, which enables the framework to propagate close to real probability distributions of the hidden node values to each proceeding DNN layer. Furthermore, our framework leverages CUDA to parallelize the analysis, enabling high-speed GPU implementation for fast analysis. The applicability of the framework is demonstrated using the ACAS Xu benchmark, to provide reachability probability estimates for all network nodes. This paper also provides potential applications of QuanDA for the analysis of DNN safety properties.  © 2023 Copyright held by the owner/author(s).",ACAS xu; confidence interval; GPU; neural networks; quantitative analysis; safety properties,Graphics processing unit; Network security; Probability distributions; ACAS xu; Analysis frameworks; Confidence interval; Formal analysis; GPU-accelerated; Hidden nodes; Neural network analysis; Neural-networks; Probability: distributions; Safety property; Deep neural networks
Uncertainty-aware Energy Harvest Prediction and Management for IoT Devices,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171773738&doi=10.1145%2f3606372&partnerID=40&md5=d3f2dd08bd7fb4d42c15360401e3a521,"Internet of things (IoT) devices are popular in several high-impact applications such as mobile healthcare and digital agriculture. However, IoT devices have limited operating lifetime due to their small form factor. Harvesting energy from ambient sources is an effective method to supplement the battery. Energy harvesting necessitates development of energy management policies to manage the harvested energy. Designing optimal policies for energy management is challenging for two key reasons: (1) ambient energy sources are highly stochastic; therefore, energy management policies must consider the associated uncertainty; (2) energy management policies must consider future energy availability while making decisions to ensure that sufficient energy is available when there is no ambient energy. Prior approaches typically consider energy in the immediate future (e.g., 1 hour) and do not account for the uncertainty in future energy harvest. This article proposes novel machine learning and dynamic optimization-based approaches to handle the two challenges. Specifically, we first develop a novel set of features and use it in a low-power neural network architecture to predict future energy availability and uncertainty. The energy predictions and uncertainty are used in a dynamic optimization algorithm to optimally allocate the harvested energy. Experiments on solar energy data over 5 years from Golden, Colorado, show that the proposed energy prediction model achieves 3.4 J mean absolute error while having a coverage of 80%. Moreover, our energy management algorithm provides energy allocations that are within 2.5 J of an optimal Oracle with 2.65 mJ to 36.54 mJ of energy overhead.  © 2023 Copyright held by the owner/author(s).",Energy harvesting; internet of things; wearable devices,Deep neural networks; Digital devices; Energy policy; Forecasting; Internet of things; Low power electronics; Network architecture; Solar energy; Stochastic systems; Wearable technology; Ambients; Dynamic optimization; Energy; Energy availability; Energy prediction; Future energies; High impact; Management policy; Uncertainty; Wearable devices; Energy harvesting
Introduction to the Special Section on Advances in Physical Design Automation,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172664642&doi=10.1145%2f3604593&partnerID=40&md5=68ef25416fa165a5cc3c50ad6272f81d,[No abstract available],,
A Brain-Inspired Hardware Architecture for Evolutionary Algorithms Based on Memristive Arrays,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171792874&doi=10.1145%2f3598421&partnerID=40&md5=0a25c4e396b57298eae0e21380d33b25,"Brain-inspired computing takes inspiration from the brain to create energy-efficient hardware systems for information processing, capable of performing highly sophisticated tasks. Systems built with emerging electronics, such as memristive devices, can achieve gains in speed and energy by mimicking the distributed topology of the brain. In this work, a brain-inspired hardware architecture for evolutionary algorithms is proposed based on memristive arrays, which can realize sparse and approximate computing as a result of the parallel analog computing characteristic of the memristive arrays. On this basis, an efficient evolvable brain-inspired hardware system is implemented. We experimentally show that the approach can offer at least a four orders of magnitude speed improvement. We also use experimentally grounded simulations to explore fault tolerance and different parameter settings in the implemented hardware system. The experimental results show that the evolvable hardware system, implemented based on the proposed hardware architecture, can continuously evolve toward a better system even if there are failures or parameter changes in the memristive arrays, demonstrating that the proposed hardware architecture has good adaptability and fault tolerance.  © 2023 Copyright held by the owner/author(s).",brain-inspired architecture; circuit implementation; evolutionary algorithms; Memristor; parallel analog computing,Brain; Energy efficiency; Fault tolerance; Memristors; Parallel architectures; Timing circuits; Brain-inspired; Brain-inspired architecture; Brain-inspired computing; Circuit implementation; Energy; Energy efficient; Hardware architecture; Hardware system; Memristor; Parallel analog computing; Evolutionary algorithms
Mitigating Mode-switch through Run-time Computation of Response Time,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171784919&doi=10.1145%2f3597432&partnerID=40&md5=6dac7763dab8cf17bac1b459bcdab978,"Mixed-critical systems consist of applications with different criticality. In these systems, different confidence levels of Worst-Case Execution Time (WCET) estimations are used. Dual criticality systems use a less pessimistic, but with lower level of assurance, WCET estimation, and a safe, but pessimistic, WCET estimation. Initially, both high and low criticality tasks are executed. When a high criticality task exceeds its less pessimistic WCET, the system switches mode and low criticality tasks are usually dropped, reducing the overall system Quality of Service (QoS). To postpone mode switch, and thus, improve QoS, existing approaches explore the slack, created dynamically, when the actual execution of a task is faster than its WCET. However, existing approaches observe this slack only after the task has finished execution. To enhance dynamic slack exploitation, we propose a fine-grained approach that is able to expose the slack during the progress of a task, and safely uses it to postpone mode switch. The evaluation results show that the proposed approach has lower cost and achieves significant improvements in avoiding mode-switch, compared to existing approaches.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",interference-sensitive; multi-cores; response time analysis; run-time adaptation; time-triggered; Worst-Case Execution Time,Criticality (nuclear fission); Embedded systems; Response time (computer systems); Interference-sensitive; Mode switches; Multi-cores; Quality-of-service; Response-time analysis; Runtime adaptation; Runtimes; Time estimation; Time triggered; Worst-case execution time; Quality of service
Hardware Security Risks and Threat Analyses in Advanced Manufacturing Industry,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171770898&doi=10.1145%2f3603502&partnerID=40&md5=80f3a439eb39d3ee2c61601e70f2c7b4,"The advanced manufacturing industry (AMI) faces many unique challenges from the cyber-physical domain. Security threats are originated from two integral parts: software and hardware. Over the past decade, software security has been addressed extensively, but hardware security has not received enough attention. This work analyzes the security vulnerabilities of typical electronic devices deployed to AMI and proposes three attack models for sensing nodes, local storage and processing edge devices, and wired/wireless communication interfaces, respectively. Practical security attacks on hardware are demonstrated in this work to inspire the development of feasible countermeasures against hardware Trojans, fault injection attacks, and external signal interference. Moreover, this work highlights the unique security challenges posed by advanced manufacturing applications. To mitigate those security attacks in AMI, this work suggests guidelines for the defense method design that can effectively protect the hardware in AMI.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",communication; cybersecurity; hardware Trojan; Security threats; sensor; side-channel analysis,Hardware security; Malware; Network security; Risk assessment; Security systems; Side channel attack; Advanced manufacturing; Cyber physicals; Cyber security; Manufacturing industries; Physical domain; Security attacks; Security risk analysis; Security threats; Side-channel analysis; Threats analysis; Cybersecurity
Analytical Placement with 3D Poisson's Equation and ADMM-based Optimization for Large-scale 2.5D Heterogeneous FPGAs,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171801157&doi=10.1145%2f3582554&partnerID=40&md5=c2d96d356d0a92dc84367fa0ba35331c,"As design complexity keeps increasing, the 2.5D field-programmable gate array (FPGA) with large logic capacity has become popular in modern circuit applications. A 2.5D FPGA consists of multiple dies connected through super long lines (SLLs) on an interposer. Each die contains heterogeneous logic blocks and ASIC-like clocking architectures to achieve better skew and timing. Existing works consider these problems separately and thus may lead to serious timing issues or routing failure. This article presents an analytical placement algorithm for the 2.5D FPGA to simultaneously minimize the number of inter-die SLL signals and intra-die clocking violations. Using a lifting dimension technique, we first formulate the 2.5D global placement problem as a three-dimensional continuous and differential minimization problem, where the SLL-aware block distribution is modeled by 3D Poisson's equation and directly solved to obtain an analytical solution. Then, we further reformulate the minimization problem as a separable optimization problem with linear constraints. Based on the proximal alternating direction method of multipliers optimization method, we efficiently optimize the separable subproblems one by one in an alternating fashion. Finally, clock-aware legalization and detailed placement are applied to legalize and improve our placement results. Compared with the state-of-the-art works, experimental results show that our algorithm can resolve all clocking constraints and reduce the number of SLL crossing signals by 36.9% with similar wirelength in a comparable running time.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",2.5D heterogeneous FPGAs; Additional Key Words and PhrasesPhysical design; ADMM-based optimization; placement,Clocks; Computer circuits; Dies; Field programmable gate arrays (FPGA); Poisson distribution; Poisson equation; 2.5d heterogeneous FPGA; Additional key word and phrasesphysical design; ADMM-based optimization; Field programmables; Key words; Long line; Optimisations; Placement; Programmable gate array; Super longs; Integrated circuit design
CBDC-PUF: A Novel Physical Unclonable Function Design Framework Utilizing Configurable Butterfly Delay Chain Against Modeling Attack,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171782304&doi=10.1145%2f3588435&partnerID=40&md5=9addf2b7c568ae04d6c364ff2e8f1405,"Physical unclonable function (PUF) is a promising security-based primitive, which provides an extremely large number of responses for key generation and authentication applications. Various PUFs have been developed as central building blocks in cryptographic protocols and security architectures, however, the existing PUFs and their improvements are still vulnerable to modeling attacks (MA) with refined machine learning algorithms. In this article, a configurable butterfly delay chain-based PUF design framework is proposed to meet the requirements of randomness, reliability, uniqueness, and MA-resistance metrics. A configurable butterfly delay chain is introduced to create multiple pairs of symmetric paths and a strong PUF relying on the intrinsic delay fluctuations of two identical paths is built. Furthermore, a secure hash function is used to insert non-linearities into the PUF, and a BCH-based error correction algorithm is utilized to recover the actual responses under noisy environments. The proposed PUF is implemented on Xilinx FPGAs and three machine learning algorithms are used to evaluate the resistance against MA. Experimental results show that the randomness, reliability, and uniqueness of the proposed PUF are close to the ideal value (49.6%, 99.9%, and 49.9%, respectively), and the prediction accuracy reaches 50% that indicating a desirable resilient to MA.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cryptographic hash function; machine learning algorithm; modeling attack; Physical unclonable function,Error correction; Hardware security; Hash functions; Learning algorithms; Machine learning; Network architecture; Building blockes; Cryptographic hash functions; Cryptographic protocols; Delay chains; Design frameworks; Function designs; Key authentication; Key generation; Machine learning algorithms; Modeling attack; Random processes
ILP-based Substrate Routing with Mismatched Via Dimension Consideration for Wire-bonding FBGA Package Design,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171730277&doi=10.1145%2f3579843&partnerID=40&md5=7e5a0069368137e404bffad320c5ab48,"With the rapidly growing demand for system-level integration, package substrates have become one of the most important carriers in semiconductor industry. Fine pitch ball grid array (FBGA) packaging is a widely used technology thanks to its relative cost-effectiveness compared to other advanced packaging technologies. In addition, it is also widely used in space-constrained applications, such as mobile and handheld devices. These packaging substrate interconnections are usually customized by layout engineers taking many complex and stringent design rules into consideration. However, fully net-by-net manual design for FBGA is time-consuming and error-prone. In this article, we propose an integer linear programming (ILP)-based router for wire-bonding FBGA packaging design. Our ILP formulation not only can handle design-dependent constraints but also take the problem of mismatched via dimension into account, which is caused by the mechanical processes and greatly increases design complexity. In addition to the ILP formulation for substrate routing, three optimization stages and several ILP constraint reduction techniques are also developed to boost the run time of ILP solver. Experimental results indicate that the proposed framework can achieve high routing completion rates, which could effectively reduce the cycle time of substrate layout design. In addition, in combination with the proposed optimization strategies, 278× speedup can be achieved compared to the ILP constraint optimized router.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPackage substrate routing; integer linear programming; via mismatch,Ball grid arrays; Chip scale packages; Constraint programming; Cost effectiveness; Integer programming; Additional key word and phrasespackage substrate routing; Ball grid array packaging; Fine pitch; Integer Linear Programming; Integer linear programming formulation; Key words; Programming constraints; Routings; Via mismatch; Wirebonding; Substrates
DRC-SG 2.0: Efficient Design Rule Checking Script Generation via Key Information Extraction,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171748109&doi=10.1145%2f3594666&partnerID=40&md5=fc449159b267327c6b7eb4787322f9ff,"Design Rule Checking (DRC) is a critical step in integrated circuit design. DRC requires formatted scripts as the input to design rule checkers. However, these scripts are manually generated in the foundry, which is tedious and error prone for generation of thousands of rules in advanced technology nodes. To mitigate this issue, we propose the first DRC script generation framework, leveraging a deep learning-based key information extractor to automatically identify essential arguments from rules and a script translator to organize the extracted arguments into executable DRC scripts. We further enhance the performance of the extractor with three specific design rule generation techniques and a multi-task learning-based rule classification module. Experimental results demonstrate that the framework can generate a single rule script in 5.46 ms on average, with the extractor achieving 91.1% precision and 91.8% recall on the key information extraction. Compared with the manual generation, our framework can significantly reduce the turnaround time and speed up process design closure.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Design Rule Checking; key information extraction; natural language processing,Design; Information retrieval; Natural language processing systems; Critical steps; Design rule checker; Design rule checking; Efficient designs; Error prones; Key information extraction; Language processing; Natural language processing; Natural languages; Script generation; Deep learning
Global Interconnect Optimization,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161127282&doi=10.1145%2f3587044&partnerID=40&md5=596ce2e92666f31746ed002ba4b3640f,"We propose a new comprehensive solution to global interconnect optimization. Traditional buffering algorithms mostly insert repeaters on a net-by-net basis based on slacks and possibly guided by global wires. We show how to integrate routing congestion, placement congestion, global timing constraints, power consumption, and additional constraints into a single resource sharing formulation. The core of our algorithm is a new buffered routing subroutine. Given a net and Lagrangean resource prices for routing, timing, placement, and power, it computes a buffered Steiner tree. The resource sharing framework provides a special multiplicative price update for fast convergence. Our algorithm is fast enough for practical instances. We demonstrate experimentally on 7nm microprocessor units that it significantly improves timing while reducing netlength and power consumption in an industrial design flow. Our implementation scales well under parallelization with up to 128 threads.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesInterconnect optimization; buffer insertion; global routing; wire synthesis,Computer aided design; Product design; Additional key word and phrasesinterconnect optimization; Buffer insertion; Global interconnects; Global routing; Interconnect optimization; Key words; Optimisations; Resources sharing; Routings; Wire synthesis; Electric power utilization
A Constructive Approach for Threshold Function Identification,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171760488&doi=10.1145%2f3606371&partnerID=40&md5=252d5f5c586a833984e3de205dbbca0b,"Threshold Function (TF) is a subset of Boolean function that can be represented with a single linear threshold gate (LTG). In the research about threshold logic, the identification of TF is an important task that determines whether a given function is a TF or not. In this article, we propose a sufficient and necessary condition for a function being a TF. With the proposed sufficient and necessary condition, we devise a TF identification algorithm. The experimental results show that the proposed approach saves 80% CPU time for identifying all the 8-input NP-class TFs as compared with the state-of-the-art. Furthermore, the LTGs corresponding to the identified TFs obtained by the proposed approach have smaller weights and threshold values than the state-of-the-art.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",linear threshold logic gate; threshold function identification; Threshold logic,Computer circuits; Logic gates; Constructive approach; Function identification; Identification algorithms; Linear threshold gates; Linear threshold logic gate; State of the art; Sufficient and necessary condition; Threshold function identification; Threshold functions; Threshold logic gates; Boolean functions
Improving the Performance of CNN Accelerator Architecture under the Impact of Process Variations,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171747803&doi=10.1145%2f3604236&partnerID=40&md5=4e0943f79acb0fe1abbc24cafd3fe2fc,"Convolutional neural network (CNN) accelerators are popular specialized platforms for efficient CNN processing. As semiconductor manufacturing technology scales down to nano scale, process variation dramatically affects the chip's quality. Process variation causes delay variation within the chip due to transistor parameter differences. CNN accelerators adopt a large number of processing elements (PEs) for parallel computing, which are highly susceptible to process variation effects. Fast CNN processing desires consistent performance among PEs; otherwise the processing speed is limited by the slowest PE within the chip. In this work, we first quantitatively model and analyze the impact of process variation on CNN accelerators' operating frequency. We further analyze the utilization of CNN accelerators and the characteristics of CNN models. We then leverage the PE underutilization to propose a sub-matrix reformation mechanism and leverage the pixel similarity of images to propose a weight transfer technique. Both techniques are able to tolerate the low-frequency PEs and achieve performance improvement at chip level. Furthermore, a novel resilience-aware mapping technique that exploits the diversity in the importance of weights is also proposed to improve the performance. Evaluation results show that our techniques are able to achieve significant processing speed improvement with negligible accuracy loss.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CNN accelerator; Process variation; systolic array,Convolutional neural networks; Nanotechnology; Semiconductor device manufacture; Accelerator architectures; Convolutional neural network; Convolutional neural network accelerator; Manufacturing technologies; Neural-network processing; Performance; Process Variation; Processing elements; Processing speed; Semiconductor manufacturing; Systolic arrays
CRP2.0: A Fast and Robust Cooperation between Routing and Placement in Advanced Technology Nodes,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171749133&doi=10.1145%2f3590962&partnerID=40&md5=9a47300f7d3540a73fee33087bfbf339,"Traditionally, the placement and routing stages of a physical design are performed separately. Because of the additional complexities arising in advanced technology nodes, they have become more interdependent. Therefore, creating efficient cooperation between the routing and placement steps has become an important topic in Electronic Design Automation (EDA). In this article, a framework that allows cooperation between routing and placement is proposed. The main objective of the proposed framework is to improve the detailed routing solution by combining routing and placement. The core of this framework is the Cooperation between Routing and Placement (CRP2.0)1 engine including techniques to combine routing and placement. The key contributions of CRP2.0 include an Integer Linear Programming (ILP)-based Detailed Placement (ILP-DP), net classification, and two Cost and Net Caching techniques. The efficacy of the proposed framework is evaluated on the official ACM/IEEE International Symposium on Physical Design (ISPD) 2018 and 2019 contest benchmarks. In this article, we show that by using the Cost Caching technique, the global routing runtime compared with state-of-the-art algorithms was reduced by 28.56%, on average. Moreover, numerical results show that when working with advanced technology nodes, the proposed framework can improve the detailed routing score by an average of 0.3% while only moving 0.7% of the cells, on average. The proposed engine can be employed as an add-on to the physical design flow between the global routing and detailed routing steps.  © 2023 Copyright held by the owner/author(s).",detailed placement; detailed routing; Electronic design automation; physical synthesis; placement; routing,Automation; Computer aided design; Integer programming; Advanced technology; Caching technique; Detailed placement; Detailed routing; Electronics design automation; Physical design; Physical synthesis; Placement; Routings; Technology nodes; Engines
GNN-based Multi-bit Flip-flop Clustering and Post-clustering Design Optimization for Energy-efficient 3D ICs,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171730087&doi=10.1145%2f3588570&partnerID=40&md5=7d78f32d5cdca1bc1b757a328adc662f,"In high-performance three-dimensional Integrated Circuits (3D ICs), clock networks consume a large portion of the full-chip power. However, no previous 3D IC work has ever optimized 3D clock networks for both power and performance simultaneously, which results in sub-optimal 3D designs. To overcome this issue, in this article, we propose a GNN-based flip-flop clustering algorithm that merges single-bit flip-flops into multi-bit flip-flops in an unsupervised manner, which jointly optimizes the power and performance metrics of clock networks. Moreover, we integrate our algorithm into the state-of-the-art 3D physical design flow and verify the integration, which leads to a better 3D full-chip design. Experimental results on eight industrial benchmarks demonstrate that the algorithm achieves improvements up to 18% in total power and 8.2% in performance over the state-of-the-art 3D flow.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Flip-flop clustering; graph neural network; Pseudo-3D design flow; Random forest,Benchmarking; Clocks; Clustering algorithms; Computer aided design; Flip flop circuits; Flow graphs; Graph neural networks; Three dimensional integrated circuits; Timing circuits; 3-d designs; Clock network; Clusterings; Design flows; Flip-flop clustering; Graph neural networks; Multi-bits; Performance; Pseudo-3d design flow; Random forests; Energy efficiency
A PPA Study of Reinforced Placement Parameter Autotuning: Pseudo-3D vs. True-3D Placers,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171770769&doi=10.1145%2f3582007&partnerID=40&md5=33633682b797201316c4ae2582836fdc,"D Place and Route (P&R) flows either involve true-3D placement algorithms or use commercial 2D tools to transform a 2D design into a 3D design. Irrespective of the nature of the placers, several placement parameters in these tools affect the quality of the final 3D designs. Different parameter settings work well with different circuits, and it is impossible to manually tune them for a particular circuit. Automated approaches involving reinforcement learning have been shown to adapt and learn the parameter settings and create trained models. However, their effectiveness depends on the input dataset quality. Using a set of 10 netlists and 10-21 handpicked placement parameters in P&R flows involving pseudo-3D or true-3D placement, the dataset quality is analyzed. The datasets are the design metrics obtained through different P&R stages, such as placement optimization, clock tree synthesis, or 3D partitioning and global routing. The training runtime and the quality of the final design metrics are compared. On a pseudo-3D flow, the training takes around 126-290 hours, whereas, on a true-3D placer-based flow, it takes around 305-410 hours. It is observed that the datasets obtained from different stages lead to drastically different final design results. With the RL-based training processes, the quality of results in 3D designs improves by up to 23.7% compared to their corresponding untrained P&R flows.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPhysical design for 3D ICs; pseudo 3D placement; RL for 3D physical design; true 3D placement,Computer aided design; Placers; Reinforcement learning; Three dimensional integrated circuits; 3-D placements; Additional key word and phrasesphysical design for 3d IC; Key words; Physical design; Place and route; Pseudo 3d placement; RL for 3d physical design; True 3d; True 3d placement; Timing circuits
A Generalized Methodology for Well Island Generation and Well-tap Insertion in Analog/Mixed-signal Layouts,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151571279&doi=10.1145%2f3580477&partnerID=40&md5=ffcf5b990d0d0a5de1dc79bd8cc4e6b2,"Well island generation and well tap placement is an important problem in analog/mixed-signal (AMS) circuits. Well taps can only prevent latchups within a certain radius of influence within a well island, and hence must be appropriately inserted to cover all devices. However, existing automated AMS layout paradigms typically defer the insertion of well taps and creation of well islands to a post-processing step after placement. This alters the placement, resulting in increased area and wire length, as well as circuit performance degradation. Therefore, there is a strong need for a solution that generates well islands and inserts well taps during placement so the placer can account for well overheads in optimizing placement metrics. In this work, we propose a modular solution using a graph-based optimization scheme that can be used within multiple placement paradigms with minimal intrusion. We demonstrate the integration of this scheme into stochastic, analytical, and designer-driven row-based placement. The method is demonstrated in advanced FinFET technologies. Layouts generated using this scheme show better area, wire length, and performance metrics at the cost of a marginal runtime degradation when compared to the post-processing approach. Using our scheme, there is an average improvement of 3% and 4% and a maximum improvement of 23% and 11% in area and wirelength, respectively, of layouts of various classes of AMS circuits at the cost of 17% average and 29% maximum increase in total runtime.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesWell island generation; AMS layout automation; analog circuits; circle graph; Jordan curve; mixed-signal circuits; well tap sharing,Analog circuits; Computer aided design; Electric signal systems; Graphic methods; Mixed signal integrated circuits; Stochastic systems; Timing circuits; Additional key word and phraseswell island generation; Analog/mixed-signal circuits; Analog/mixed-signal layout automation; Circle graphs; Jordan curves; Key words; Mixed signal; Mixed-signal circuits; Well tap sharing; Wire length; Taps
A Fast Optimal Double-row Legalization Algorithm,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171743496&doi=10.1145%2f3579844&partnerID=40&md5=9fd7373c36ef8d414598c42778658ca6,"In Placement Legalization, it is often assumed that (almost) all standard cells possess the same height and can therefore be aligned in cell rows, which can then be treated independently. However, this is no longer true for recent technologies, where a substantial number of cells of double- or even arbitrary multiple-row height is to be expected. Due to interdependencies between the cell placements within several rows, the legalization task becomes considerably harder. In this article, we show how to optimize squared cell movement for pairs of adjacent rows comprising cells of single- as well as double-row height with a fixed left-to-right ordering in time (n · log (n)), where n denotes the number of cells involved. Opposed to prior works, we do not artificially bound the maximum cell movement and can guarantee to find an optimum solution. Our approach also allows us to include gridding and movebound constraints for the cells. Experimental results show an average percental decrease of over 26% in the total squared movement when compared to a legalization approach that fixes cells of more than single-row height after Global Placement.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPlacement; double-row-height cells; legalization,Authentication; Contracts; Cytology; Additional key word and phrasesplacement; Cell movement; Cell placement; Double rows; Double-row-height cell; Key words; Legalization; Multiple rows; Placement legalization; Standard-cell; Cells
MEDUSA: A Multi-Resolution Machine Learning Congestion Estimation Method for 2D and 3D Global Routing,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169027679&doi=10.1145%2f3590768&partnerID=40&md5=a9e167384167fec12d8d61eaa8cb7f33,"Routing congestion is one of the many factors that need to be minimized during the physical design phase of large integrated circuits. In this article, we propose a novel congestion estimation method, called MEDUSA, that consists of three parts: (1) a feature extraction and ""hyper-image""encoding; (2) a congestion estimation method using a fixed-resolution convolutional neural network model that takes a tile of this hyper-image as input and makes accurate congestion predictions for a small region of the circuit; and (3) a sliding-window method for repeatedly applying this convolutional neural network on a layout, thereby producing higher-resolution congestion maps for arbitrarily large circuits. The proposed congestion estimation approach works with both 2D (collapsed) and 3D global routing. Using both quantitative metrics and qualitative visual inspection, congestion maps produced with MEDUSA show better accuracy than prior estimation techniques. Global routers typically use estimation techniques during their first router iteration and then switch to using actual congestion information extracted from the intermediate router solutions. Experimental results within the same global router infrastructure show a significant impact on quality after the first routing iteration; other estimation techniques result in an average of 22% to 54% higher initial overflow counts. This initial quality improvement carries through to the final global routing solution, with other estimation techniques needing up to 5% more routing iterations and up to 3× more runtime, on average. Compared with other global routers, MEDUSA achieves comparable wire length results and lower total overflow counts (more legal global routing solutions) and is typically faster.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CNNs; Congestion estimation; convolutional neural networks; global routing; machine learning,Convolution; Convolutional neural networks; Iterative methods; Congestion estimation; Congestion maps; Convolutional neural network; Estimation methods; Estimation techniques; Global routers; Global routing; Machine-learning; Routing congestion; Routings; Machine learning
Boosting VLSI Design Flow Parameter Tuning with Random Embedding and Multi-objective Trust-region Bayesian Optimization,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171738846&doi=10.1145%2f3597931&partnerID=40&md5=8d518a4fa8e53bf46ca6f1ebeaf32dd3,"Modern very large-scale integration (VLSI) design requires the implementation of integrated circuits using electronic design automation (EDA) tools. Due to the complexity of EDA algorithms, there are numerous tool parameters that have imperative impacts on the chip design quality. Manual selection of parameter values is excessively laborious and constrained by experts' experience. Due to the high complexity and lack of parallelization, most existing parameter tuning methods cannot make sufficient exploration in a large search space. In this article, we boost the efficiency and performance of parameter tuning with random embedding and multi-objective trust-region Bayesian optimization. Random embedding can effectively cut down the number of variables in the search process and thus reduce the runtime of Bayesian optimization. Multi-objective trust-region Bayesian optimization allows the algorithm to explore diverse solutions with excellent parallelism. Due to the ability to do more exploration in limited runtime, the proposed framework can achieve better performance than existing methods in our experiments.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bayesian optimization; parameter tuning; Physical design; VLSI design flow,Computational complexity; Computer aided design; Integrated circuit design; Multiobjective optimization; Tuning; VLSI circuits; Bayesian optimization; Design flows; Embeddings; Multi objective; Parameters tuning; Physical design; Runtimes; Trust region; Very large scale integration designs; Very large-scale integration design flow; Embeddings
Dynamic Power Management in Large Manycore Systems: A Learning-to-Search Framework,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171778945&doi=10.1145%2f3603501&partnerID=40&md5=5c29d056a75b7d5071d6abe8a5c9cee7,"The complexity of manycore System-on-chips (SoCs) is growing faster than our ability to manage them to reduce the overall energy consumption. Further, as SoC design moves toward three-dimensional (3D) architectures, the core's power density increases leading to unacceptable high peak chip temperatures. In this article, we consider the optimization problem of dynamic power management (DPM) in manycore SoCs for an allowable performance penalty (say, 5%) and admissible peak chip temperature. We employ a machine learning- (ML) based DPM policy, which selects the voltage/frequency levels for different cluster of cores as a function of the application workload features such as core computation and inter-core traffic, and so on. We propose a novel learning-to-search (L2S) framework to automatically identify an optimized sequence of DPM decisions from a large combinatorial space for joint energy-thermal optimization for one or more given applications. The optimized DPM decisions are given to a supervised learning algorithm to train a DPM policy, which mimics the corresponding decision-making behavior. Our experiments on two different manycore architectures designed using wireless interconnect and monolithic 3D demonstrate that principles behind the L2S framework are applicable for more than one configuration. Moreover, L2S-based DPM policies achieve up to 30% energy-delay product savings and reduce the peak chip temperature by up to 17 °C compared to the state-of-the-art ML methods for an allowable performance overhead of only 5%.  © 2023 Copyright held by the owner/author(s).",Dynamic power management; large manycore systems; machine learning; thermal-aware; voltage frequency island,Decision making; Energy utilization; Learning algorithms; Machine learning; Memory architecture; Power management; Programmable logic controllers; Chip temperature; Dynamic power management; Large manycore system; Machine-learning; Management decisions; Management policy; Manycore systems; Systems-on-Chip; Thermal-Aware; Voltage-frequency islands; System-on-chip
DAGSizer: A Directed Graph Convolutional Network Approach to Discrete Gate Sizing of VLSI Graphs,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171394908&doi=10.1145%2f3577019&partnerID=40&md5=bdda1569fab0bc41785ba7d03f436511,"The objective of a leakage recovery step is to make use of positive slack and reduce power by performing appropriate standard-cell swaps such as threshold-voltage (Vth) or channel-length reassignments. The resulting engineering change order netlist needs to be timing clean. Because this recovery step is performed several times in a physical design flow and involves long runtimes and high tool-license usage, previous works have proposed graph neural network–based frameworks that restrict feature aggregation to three-hop neighborhoods and do not fully consider the directed nature of netlist graphs. As a result, the intermediate node embeddings do not capture the complete structure of the timing graph. In this article, we propose DAGSizer, a framework that exploits the directed acyclic nature of timing graphs to predict cell reassignments in the discrete gate sizing task. Our DAGSizer (Sizer for DAGs) framework is based on a node ordering-aware recurrent message-passing scheme for generating the latent node embeddings. The generated node embeddings absorb the complete information from the fanin cone (predecessors) of the node. To capture the fanout information into the node embeddings, we enable a bidirectional message-passing mechanism. The concatenated latent node embeddings from the forward and reverse graphs are then translated to nodewise delta-delay predictions using a teacher sampling mechanism. With eight possible cell-assignments, the experimental results demonstrate that our model can accurately estimate design-level leakage recovery with an absolute relative error ϵmodel under 5.4%. As compared to our previous work, GRA-LPO, we also demonstrate a significant improvement in the model mean squared error. © 2023 Association for Computing Machinery.",directed graph convolution; Discrete gate sizing; leakage optimization; sequential message passing,Convolution; Flow graphs; Graphic methods; Integrated circuit design; Logic gates; Message passing; Molecular biology; Recovery; Threshold voltage; VLSI circuits; Convolutional networks; Directed graph convolution; Discrete gate sizing; Embeddings; Gate sizing; Latent nodes; Leakage optimization; Message-passing; Netlist; Sequential message passing; Mean square error
ECO-GNN: Signoff Power Prediction Using Graph Neural Networks with Subgraph Approximation,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183462399&doi=10.1145%2f3569942&partnerID=40&md5=51aa9dbd5a4c2df6e0036daa538667af,"Modern electronic design automation flows depend on both implementation and signoff tools to perform timing-constrained power optimization through Engineering Change Orders (ECOs), which involve gate sizing and threshold-voltage (Vth)-assignment of standard cells. However, the signoff ECO optimization is highly time-consuming, and the power improvement is hard to predict in advance. Ever since the industrial benchmarks released by the ISPD-2012 gate-sizing contest, active research has been conducted extensively to improve the optimization process. Nonetheless, previous works were mostly based on heuristics or analytical methods whose timing models were oversimplified and lacked of formal validations from commercial signoff tools. In this article, we propose ECO-graph neural networks (GNN), a transferable graph-learning-based framework, which harnesses GNNs to perform commercial-quality signoff power optimization through discrete Vth-assignment. One of the highlights of our framework is that it generates tool-accurate optimization results instantly on unseen netlists that are not utilized in the training process. Furthermore, we propose a subgraph approximation technique to improve training and inferencing time of the proposed GNN model. We show that design instances with non-overlapping subgraphs can be optimized in parallel so as to improve the inference time of the learning-based model. Finally, we implement a GNN-based explanation method to interpret the optimization results achieved by our framework. Experimental results on 14 industrial designs, including a RISC-V-based multi-core system and the renowned ISPD-2012 benchmarks, demonstrate that our framework achieves up to 14× runtime improvement with similar signoff power optimization quality compared with Synopsys PrimeTime, an industry-leading signoff tool. © 2023 Association for Computing Machinery.",Engineering Change Order (ECO); Graph Neural Networks (GNNs); power prediction,Benchmarking; Cell engineering; Computer aided design; Constrained optimization; Flow graphs; Graph neural networks; Industrial research; Threshold voltage; Engineering change order; Engineering change orders; Gate sizing; Graph neural network; Graph neural networks; Order graph; Power Optimization; Power predictions; Sign-off; Subgraphs; Forecasting
SwitchX: Gmin-Gmax Switching for Energy-efficient and Robust Implementation of Binarized Neural Networks on ReRAM Xbars,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186458486&doi=10.1145%2f3576195&partnerID=40&md5=ec5686d0912fd8bed53e17a34ba2659a,"Memristive crossbars can efficiently implement Binarized Neural Networks (BNNs) wherein the weights are stored in high-resistance states (HRS) and low-resistance states (LRS) of the synapses. We propose SwitchX mapping of BNN weights onto ReRAM crossbars such that the impact of crossbar non-idealities, that lead to degradation in computational accuracy, are minimized. Essentially, SwitchX maps the binary weights in such a manner that a crossbar instance comprises of more HRS than LRS synapses. We find BNNs mapped onto crossbars with SwitchX to exhibit better robustness against adversarial attacks than the standard crossbar-mapped BNNs, the baseline. Finally, we combine SwitchX with state-aware training (that further increases the feasibility of HRS states during weight mapping) to boost the robustness of a BNN on hardware. We find that this approach yields stronger defense against adversarial attacks than adversarial training, a state-of-the-art software defense. We perform experiments on a VGG16 BNN with benchmark datasets (CIFAR-10, CIFAR-100 and TinyImagenet) and use Fast Gradient Sign Method (ϵ = 0.05 to 0.3) and Projected Gradient Descent (ϵ = 2/255 to 32/255, α = 2/255) adversarial attacks. We show that SwitchX combined with state-aware training can yield upto ∼35% improvements in clean accuracy and ∼6–16% in adversarial accuracies against conventional BNNs. Furthermore, an important by-product of SwitchX mapping is increased crossbar power savings, owing to an increased proportion of HRS synapses, which is furthered with state-aware training. We obtain upto ∼21–22% savings in crossbar power consumption for state-aware trained BNN mapped via SwitchX on 16 × 16 and 32 × 32 crossbars using the CIFAR-10 and CIFAR-100 datasets. © 2023 Association for Computing Machinery.",adversarial robustness; Binarized neural network; non-idealities; ReRAM crossbar; switched-mapping,Energy efficiency; Gradient methods; Network security; RRAM; Adversarial robustness; Binarized neural network; Energy efficient; High-resistance state; Low-resistance state; Neural-networks; Nonideality; ReRAM crossbar; State resistance; Switched-mapping; Mapping
Polling-Based Memory Interface,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160007909&doi=10.1145%2f3572919&partnerID=40&md5=073e4ead749d2f7f514b0004aca5bcc2,"Non-volatile memory has been extensively researched as the alternative for a DRAM-based system; however, the traditional memory controller cannot efficiently track and schedule operations for all the memory devices in heterogeneous systems due to different timing requirements and complex architecture supports of various memory technologies. To address this issue, we propose a hybrid memory architecture framework called POMI (POlling-based Memory Interface). It uses a small buffer chip inserted on each DIMM (Dual In-line Memory Module) to decouple operation scheduling from the controller to enable the support for diverse memory technologies in the system. Unlike the conventional DRAM-based system, POMI uses a polling-based memory bus protocol for communication and to resolve any bus conflicts between memory modules. The buffer chip on each DIMM will provide feedback information to the main memory controller so that the polling overhead is trivial. We propose two unique designs. The first one adds additional bus lines for sending the feedback information, and the second one utilizes the Command/Address bus. The framework provides several benefits: a technology-independent memory system, higher parallelism, and better scalability. Our experimental results show that POMI can efficiently support both homogeneous and heterogeneous systems. Compared with the conventional DDR4-2400 implementation, our scheme improves the performance of memory-intensive workloads by 3.7% on average. Compared with an existing interface for hybrid memory systems, Twin-Load, it also improves performance by 22.0% on average for memory-intensive workloads.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesHybrid memory system; decentralized memory controller; diverse memory technology; interoperability,Buses; Controllers; Feedback; Interoperability; Memory architecture; Additional key word and phraseshybrid memory system; Decentralised; Decentralized memory controller; Diverse memory technology; Heterogeneous systems; Key words; Memory controller; Memory interface; Memory systems; Memory technology; Dynamic random access storage
Test Point Insertion for Multi-Cycle Power-On Self-Test,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160021164&doi=10.1145%2f3563552&partnerID=40&md5=8ee9ba68bacb6742787febfd79232c80,"Under the functional safety standard ISO26262, automotive systems require testing in the field, such as the power-on self-test (POST). Unlike the production test, the POST requires reducing the test application time to meet the indispensable test quality (e.g., >90% of latent fault metric) of ISO26262. This article proposes a test point insertion technique for multi-cycle power-on self-test to reduce the test application time under the indispensable test quality. The main difference to the existing test point insertion techniques is to solve the fault masking problem and the fault detection degradation problem under the multi-cycle test. We also present the method to identify a user-specified amount of test points that could achieve the most scan-in pattern reduction for attaining a target test coverage. The experimental results on ISCAS89 and ITC99 benchmarks show 24.4X pattern reduction on average to achieve 90% stuck-at fault coverage confirming the effectiveness of the proposed method.  © 2023 Copyright held by the owner/author(s).",functional safety; ISO26262; LBIST; multi-cycle test; POST,Built-in self test; Embedded systems; Integrated circuit testing; Safety testing; Functional Safety; LBIST; Multi cycle; Multi cycle tests; Power; Power-on self-test; Self test; Test application time; Test point insertion; Test quality; Fault detection
Towards LDPC Read Performance of 3D Flash Memories with Layer-induced Error Characteristics,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160023244&doi=10.1145%2f3585075&partnerID=40&md5=48edd07a8e1fbac2c4e386e15c8ff97f,"3D flash memories have been widely developed to further increase the storage capacity of SSDs by vertically stacking multiple layers. However, this special physical structure brings new error characteristics. Existing studies have discovered that there exist significant Raw Bit Error Rate (RBER) variations among different layers and RBER similarity inside the same layer due to the manufacturing process. These error characteristics would introduce a new data reliability issue. Currently, Low-Density Parity-Check (LDPC) code has been widely used to ensure the data reliability of flash memories. It can provide stronger error correction capability for high RBERs by trading with longer read latency. Traditional LDPC codes designed for planar flash memories do not consider the layer RBER characteristics of 3D flash memories, which may induce sub-optimal read performance.This article first investigates the effect of RBER characteristics of 3D flash memories on read performance and then obtains two observations. On one hand, we observe that LDPC read latencies are largely diverse in different flash layers and increase in diverse speeds along with data retention. This phenomenon is caused by the inter-layer RBER variation. On the other hand, we also compare RBERs between different pages of the same flash layer and observe that read latencies with LDPC codes are quite similar, which is caused by the intra-layer RBER similarity. Then, by exploiting these two observation results, this article proposes a Multi-Granularity LDPC (MG-LDPC) read method to adapt read latency increase characteristics across 3D flash layers. In detail, we design five LDPC decoding engines with varied read level increase granularity (higher level induces higher latency) and assign these engines to each layer dynamically according to prior information, or in a fixed way. A series of experimental results demonstrate that the fixed and dynamic MG-LDPC methods can reduce SSD read response time by 21% and 51% on average, respectively.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",3D flash memories; layer RBERs variation; Low-Density Parity-Check codes; read performance,Engines; Flash memory; Forward error correction; Signal encoding; 3d flash memory; Data reliability; Error characteristics; Layer RBER variation; Low-density parity-check; Low-density parity-check codes; Rate variation; Raw bit error rates; Read latencies; Read performance; Bit error rate
Fast Area Optimization Approach for XNOR/OR-based Fixed Polarity Reed-Muller Logic Circuits based on Multi-strategy Wolf Pack Algorithm,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160007730&doi=10.1145%2f3587818&partnerID=40&md5=cf4e07674de0b96381f31562ac819a80,"Area optimization is one of the most important contents of circuits logic synthesis. The smaller area has stronger testability and lower cost. However, searching for a circuit with the smallest area in a large-scale space of polarity is a combinatorial optimization problem. The existing optimization approaches are inefficient and do not consider the time cost. In this paper, we propose a multi-strategy wolf pack algorithm (MWPA) to solve high-dimension combinatorial optimization problems. MWPA performs global search based on the proposed global exploration strategy, extends the search area based on the Levy flight strategy, and performs local search based on the proposed deep exploitation strategy. In addition, we propose a fast area optimization approach (FAOA) for fixed polarity Reed-Muller (FPRM) logic circuits based on MWPA, which searches the best polarity corresponding to a FPRM circuit. The experimental results confirm that FAOA is highly effective and can be used as a promising EDA tool.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Area optimization; combinatorial optimization problem; FPRM logic circuits; Levy flight; wolf pack algorithm,Combinatorial optimization; Computer circuits; Logic Synthesis; Timing circuits; Area optimization; Circuit logic; Combinatorial optimization problems; Fixed polarity reed-mulle logic circuit; Levy flights; Optimization approach; Reed-muller logic; Search-based; Small area; Wolf pack algorithm; Logic circuits
Accurately Measuring Contention in Mesh NoCs in Time-Sensitive Embedded Systems,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160021535&doi=10.1145%2f3582006&partnerID=40&md5=400a3e2e2bd221c11e5456ce8741450b,"The computing capacity demanded by embedded systems is on the rise as software implements more functionalities, ranging from best-effort entertainment functions to performance-guaranteed safety-related functions. Heterogeneous manycore processors, using wormhole mesh (wmesh) Network-on-Chips (NoCs) as the main communication means, and contention block among applications, are increasingly considered to deliver the required computing performance. Most research efforts on software timing analysis have focused on deriving bounds (estimates) to the contention that tasks can suffer when accessing wmesh NoCs. However, less effort has been devoted to an equally important problem, namely, accurately measuring the actual contention tasks generate each other on the wmesh which is instrumental during system validation to diagnose any software timing misbehavior and determine which tasks are particularly affected by contention on specific wmesh routers. In this article, we work on the foundations of contention measuring in wmesh NoCs and propose and explain the rationale of a golden metric, called task PairWise Contention (PWC). PWC allows ascribing the actual share of the contention a given task suffers in the wmesh to each of its co-runner tasks at packet level. We also introduce and formalize a Golden Reference Value (GRV) for PWC that specifically defines a criterion to fairly break down the contention suffered by a task among its co-runner tasks in the wmesh. Our evaluation shows that GRV effectively captures how contention occurs by identifying the actual core (task) causing contention and whether contention is caused by local or remote interference in the wmesh.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contention; contention breakdown; NoCs; Timing validation & verification,Embedded systems; Mesh generation; Routers; Timing circuits; Best-effort; Computing capacity; Contention; Contention breakdown; Embedded-system; MeshNetworks; Networks on chips; Reference values; Software implement; Timing validation & verification; Network-on-chip
Virtuoso: Energy- and Latency-aware Streamlining of Streaming Videos on Systems-on-Chips,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160007954&doi=10.1145%2f3564289&partnerID=40&md5=05fa321734d29ef20e2496060b81b362,"Efficient and adaptive computer vision systems have been proposed to make computer vision tasks, such as image classification and object detection, optimized for embedded or mobile devices. These solutions, quite recent in their origin, focus on optimizing the model (a deep neural network) or the system by designing an adaptive system with approximation knobs. Despite several recent efforts, we show that existing solutions suffer from two major drawbacks. First, while mobile devices or systems-on-chips usually come with limited resources including battery power, most systems do not consider the energy consumption of the models during inference. Second, they do not consider the interplay between the three metrics of interest in their configurations, namely, latency, accuracy, and energy. In this work, we propose an efficient and adaptive video object detection system - Virtuoso, which is jointly optimized for accuracy, energy efficiency, and latency. Underlying Virtuoso is a multi-branch execution kernel that is capable of running at different operating points in the accuracy-energy-latency axes, and a lightweight runtime scheduler to select the best fit execution branch to satisfy the user requirement. We position this work as a first step in understanding the suitability of various object detection kernels on embedded boards in the accuracy-latency-energy axes, opening the door for further development in solutions customized to embedded systems and for benchmarking such solutions. Virtuoso is able to achieve up to 286 FPS on the NVIDIA Jetson AGX Xavier board, which is up to 45× faster than the baseline EfficientDet D3 and 15× faster than the baseline EfficientDet D0. In addition, we also observe up to 97.2% energy reduction using Virtuoso compared to the baseline YOLO (v3) - a widely used object detector designed for mobiles. To fairly compare with Virtuoso, we benchmark 15 state-of-the-art or widely used protocols, including Faster R-CNN (FRCNN) [NeurIPS'15], YOLO v3 [CVPR'16], SSD [ECCV'16], EfficientDet [CVPR'20], SELSA [ICCV'19], MEGA [CVPR'20], REPP [IROS'20], FastAdapt [EMDL'21], and our in-house adaptive variants of FRCNN+, YOLO+, SSD+, and EfficientDet+ (our variants have enhanced efficiency for mobiles). With this comprehensive benchmark, Virtuoso has shown superiority to all the above protocols, leading the accuracy frontier at every efficiency level on NVIDIA Jetson mobile GPUs. Specifically, Virtuoso has achieved an accuracy of 63.9%, which is more than 10% higher than some of the popular object detection models, FRCNN at 51.1% and YOLO at 49.5%.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesAdaptive inference; configuration tuning; efficiency-aware analytics; embedded computing; energy consumption; mobile GPUs; video object detection,Computer vision; Deep neural networks; Embedded systems; Energy efficiency; Green computing; Object detection; Object recognition; Program processors; System-on-chip; Video streaming; Additional key word and phrasesadaptive inference; Configuration tuning; Efficiency-aware analytic; Embedded computing; Energy; Energy-consumption; Key words; Mobile GPU; Objects detection; Video object detections; Energy utilization
Introduction to the Special Issue on Machine Learning for CAD/EDA,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152478569&doi=10.1145%2f3586208&partnerID=40&md5=c390e96361d3be14892135e8b43a9b6f,"Recent advances in machine learning (ML) have brought revolutions for a variety of applications like computer vision, recommendation systems, and robotics. Many researches have been exploring the applications of ML to CAD/EDA problems. However, the design processes in the CAD flow present challenges to achieve high accuracy, generality, and efficiency. Compared to traditional ML applications such as computer vision, parallel advances in ML and CAD are often required to achieve effectiveness in the design processes. This special issue on Machine Learning for CAD/EDA focuses on concepts and methods for applying machine learning techniques to improve design performance and speed up design closure in the CAD flow. © 2023 Association for Computing Machinery. All rights reserved.",,Computer aided design; Computer vision; CAD flow; CAD/EDA; Design performance; Design speed; Design-process; High-accuracy; Machine learning applications; Machine learning techniques; Machine-learning; On-machines; Machine learning
Design of Synthesis-time Vectorized Arithmetic Hardware for Tapered Floating-point Addition and Subtraction,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160004412&doi=10.1145%2f3567423&partnerID=40&md5=5fdb57af138fc9ad2177ae9a45ff5bb6,"Energy efficiency has become the new performance criterion in this era of pervasive embedded computing; thus, accelerator-rich multi-processor system-on-chips are commonly used in embedded computing hardware. Once computationally intensive machine learning applications gained much traction, they are now deployed in many application domains due to abundant and cheaply available computational capacity. In addition, there is a growing trend toward developing hardware accelerators for machine learning applications for embedded edge devices where performance and energy efficiency are critical. Although these hardware accelerators frequently use floating-point operations for accuracy, reduced-width floating-point formats are also used to reduce hardware complexity; thus, power consumption while maintaining accuracy. Vectorization concepts can also be used to improve performance, energy efficiency, and memory bandwidth. We propose the design of a vectorized floating-point adder/subtractor that supports arbitrary length floating-point formats with varying exponent and mantissa widths in this article. In comparison to existing designs in the literature, the proposed design is 2.57× area- and 1.56× power-efficient, and it supports true vectorization with no restrictions on exponent and mantissa widths.  © 2023 Association for Computing Machinery.",application specific integrated circuits (ASICs); digital circuits; floating-point hardware; Hardware accelerators; intellectual property (IP); register transfer level (RTL),Application specific integrated circuits; Computing power; Digital circuits; Energy efficiency; Machine learning; System-on-chip; Timing circuits; Application specific integrated circuit; Application-specific integrated circuits; Embedded computing; Floating point hardware; Floating points; Hardware accelerators; Intellectual property; Machine learning applications; Register transfer level; Register-transfer level; Integrated circuit design
Auto-tuning Fixed-point Precision with TVM on RISC-V Packed SIMD Extension,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160012604&doi=10.1145%2f3569939&partnerID=40&md5=4c0013d615e2ad26358cefbdb1b0aa6c,"Today, as deep learning (DL) is applied more often in daily life, dedicated processors such as CPUs and GPUs have become very important for accelerating model executions. With the growth of technology, people are becoming accustomed to using edge devices, such as mobile phones, smart watches, and VR devices in their daily lives. A variety of technologies using DL are gradually being applied to these edge devices. However, there is a large number of computations in DL. It faces a challenging problem how to provide solutions in the edge devices. In this article, the proposed method enables a flow with the RISC-V Packed extension (P extension) in TVM. TVM, an open deep learning compiler for neural network models, is growing as a key infrastructure for DL computing. RISC-V is an open instruction set architecture (ISA) with customized and flexible features. The Packed-SIMD extension is a RISC-V extension that enables subword single-instruction multiple-data (SIMD) computations in RISC-V architectures to support fallback engines in AI computing. In the proposed flow, a fixed-point type that is supported by an integer of 16-bit type and saturation instructions is added to replace the original 32-bit float type. In addition, an auto-tuning method is proposed to use a uniform selector mechanism (USM) to find the binary point position for fixed-point type use. The tensorization feature of TVM can be used to optimize specific hardware such as subword SIMD instructions with RISC-V P extension. With our experiment on the Spike simulator, the proposed method with the USM can improve performance by approximately 2.54 to 6.15× in terms of instruction counts with little accuracy loss.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesTVM; fixed-point; LLVM; RISC-V P extension; Subword SIMD,Network architecture; Program processors; Additional key word and phrasestvm; Daily lives; Data extension; Fixed points; Key words; LLVM; Multiple data; RISC-V packed extension; Sub words; Subword single-instruction multiple-data; Deep learning
Efficient Test Chip Design via Smart Computation,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152909584&doi=10.1145%2f3558393&partnerID=40&md5=2b3e49aade77c5d07cc60b46f5710b5c,"Submitted to the Special Issue on Machine Learning for CAD (ML-CAD). Competitive strength in semiconductor field depends on yield. The challenges associated with designing and manufacturing of leading-edge integrated circuits (ICs) have increased that reduce yield. Test chips, especially full-flow logic test chips, are increasingly employed to investigate the complex interaction between layout features and the process that improves the total process quality before and during initial mass production. However, designing a high-quality full-flow logic test chip can be time-consuming due to the huge design space and complex process to search for optimal result. This work describes a new design flow that significantly accelerates the logic test chip design process. First, we deploy random forest classification technique to predict potential synthesis outcome for test chip design exploration. Next, a new method is described to efficiently solve the integer programming problem involved in the design process. Various experiments with industrial design have demonstrated that the proposed two methods greatly improve the design efficiency.  © 2023 Association for Computing Machinery.",integer programming; Random forest; test chip design,Computation theory; Computer aided design; Computer circuits; Integrated circuit design; Integrated circuits; Chip design; Flow logic; Initial mass; Integer Program- ming; Machine-learning; On-machines; Process quality; Random forests; Test chip design; Test chips; Integer programming
Multi-Objective Optimization for Safety-Related Available E/E Architectures Scoping Highly Automated Driving Vehicles,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160010976&doi=10.1145%2f3582004&partnerID=40&md5=120dab1dee2dcd66c0c9cc9d1032f28d,"Megatrends such as Highly Automated Driving (HAD) (SAE ≥ Level 3), electrification, and connectivity are reshaping the automotive industry. Together with the new technologies, the business models will also evolve, opening up new possibilities and new fields of competition. To cope with the ongoing advances, new Electric/Electronic (E/E) architecture patterns are emerging in the sector, distributing the vehicle functions across several processing devices and enhancing the connectivity between them via Ethernet-based networks. Upcoming systems will demand Safety-Related Availability (SaRA) requirements in mixed-critical E/E architectures that challenge the concept of freedom from interference defined in ISO 26262. This work explores the concepts of SaRA system development according to ISO 26262, building a framework based on model-based systems engineering to evaluate feasible next-generation automotive E/E architecture designs with a multi-objective analysis. Additionally, we propose a pattern template for SaRA systems to automate the architecture synthesis. To illustrate the framework created, we evaluate a set of automotive E/E architectures synthesized to support mixed-critical vehicle features, including SaRA SAE Level-3 functions, considering the communication networks' performance as well as hardware and safety-related development costs. This work presents a methodology for original equipment manufacturers and Tier-1 suppliers that enables them to make the trade-offs arising in the design of E/E architectures based on quantified information.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated driving; automotive; availability; design patterns; design space exploration; E/E architecture; model based systems engineering; safety; time-sensitive networking,Automation; Automotive industry; Competition; Economic and social effects; Function evaluation; Multiobjective optimization; Network architecture; Safety engineering; Vehicle to vehicle communications; Automated driving; Automotives; Design Patterns; Design space exploration; Electric electronics; Electric/electronic architecture; Electronic architecture; Model-based system engineerings; Safety-Related; Time-sensitive networking; Vehicles
Machine-learning-driven Architectural Selection of Adders and Multipliers in Logic Synthesis,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152894001&doi=10.1145%2f3560712&partnerID=40&md5=980ab504a57d9171705b8d4ebff00e69,"Designing high-performance adders and multiplier components for diverse specifications and constraints is of practical concern. However, selecting the best architecture for adder or multiplier, which largely affects the performance of synthesized circuits, is difficult. To tackle this difficulty, a machine-learning-driven approach is proposed for automatic architectural selection of adders and multipliers. It trains a machine learning model for classification through learning a number of existing design schemes and their performance data. Experimental results show that the proposed approach based on a multi-perception neural network achieves as high as 94% prediction accuracy with negligible inference time. On a CPU server, the proposed approach runs about 4× faster than a brute-force approach trying four candidate architectures and consumes 10%∼20% less runtime than the DesignWare datapath generator for obtaining the optimal adder/multiplier circuit. The adder (multiplier) generated with the proposed approach achieves performance metrics close to the optimal and has 1.6% (5.2%) less area and 2.2% (7.1%) more worst negative slack averagely than that generated with the DesignWare datapath generator. Our experiment also shows that the proposed approach is not sensitive to the size of training subset.  © 2023 Copyright held by the owner/author(s).",adder/multiplier; architectural selection; datapath; Logic synthesis; machine learning,Adders; Codes (symbols); Computer circuits; Logic Synthesis; Network architecture; Adder/multiplier; Architectural selection; Data paths; Design scheme; DesignWare; High performance adders; Machine learning models; Machine-learning; Performance; Synthesised; Machine learning
A Survey and Perspective on Artificial Intelligence for Security-Aware Electronic Design Automation,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152894309&doi=10.1145%2f3563391&partnerID=40&md5=e781f6d9398d90f45a19b235a29e936e,"Artificial intelligence (AI) and machine learning (ML) techniques have been increasingly used in several fields to improve performance and the level of automation. In recent years, this use has exponentially increased due to the advancement of high-performance computing and the ever increasing size of data. One of such fields is that of hardware design - specifically the design of digital and analog integrated circuits, where AI/ ML techniques have been extensively used to address ever-increasing design complexity, aggressive time to market, and the growing number of ubiquitous interconnected devices. However, the security concerns and issues related to integrated circuit design have been highly overlooked. In this article, we summarize the state-of-the-art in AI/ML for circuit design/optimization, security and engineering challenges, research in security-aware computer-aided design/electronic design automation, and future research directions and needs for using AI/ML for security-aware circuit design.  © 2023 Association for Computing Machinery.",deep learning; Integrated circuit; reinforcement learning; security primitive,Analog integrated circuits; Automation; Computer aided design; Deep learning; Digital devices; Digital integrated circuits; E-learning; Integrated circuit design; Integrated circuit manufacture; Reinforcement learning; Artificial intelligence learning; Circuit designs; Deep learning; Electronics design automation; Improve performance; Machine learning techniques; Machine-learning; Reinforcement learnings; Security primitives; Security-aware; Timing circuits
A Comprehensive Survey on Electronic Design Automation and Graph Neural Networks: Theory and Applications,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152615243&doi=10.1145%2f3543853&partnerID=40&md5=3f1634c1cbd9c3459f7c35131ef4c368,"Driven by Moore's law, the chip design complexity is steadily increasing. Electronic Design Automation (EDA) has been able to cope with the challenging very large-scale integration process, assuring scalability, reliability, and proper time-to-market. However, EDA approaches are time and resource demanding, and they often do not guarantee optimal solutions. To alleviate these, Machine Learning (ML) has been incorporated into many stages of the design flow, such as in placement and routing. Many solutions employ Euclidean data and ML techniques without considering that many EDA objects are represented naturally as graphs. The trending Graph Neural Networks (GNNs) are an opportunity to solve EDA problems directly using graph structures for circuits, intermediate Register Transfer Levels, and netlists. In this article, we present a comprehensive review of the existing works linking the EDA flow for chip design and GNNs. We map those works to a design pipeline by defining graphs, tasks, and model types. Furthermore, we analyze their practical implications and outcomes. We conclude by summarizing challenges faced when applying GNNs within the EDA design flow.  © 2023 Association for Computing Machinery.",Electronic Design Automation; Graph Neural Networks; machine learning; register-transfer level; very large-scale integration,Automation; E-learning; Electronic design automation; Graph neural networks; Chip design; Design complexity; Design flows; Design graphs; Electronics design automation; Graph neural networks; Machine-learning; Moore Law; Register-transfer level; Verylarge-scale integrations (VLSI); Machine learning
DRAGON: Dynamic Recurrent Accelerator for Graph Online Convolution,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147258409&doi=10.1145%2f3524124&partnerID=40&md5=932a21d86bc02f18d67e842eb9b42bea,"Despite the extraordinary applicative potentiality that dynamic graph inference may entail, its practical-physical implementation has been a topic seldom explored in literature. Although graph inference through neural networks has received plenty of algorithmic innovation, its transfer to the physical world has not found similar development. This is understandable since the most preeminent Euclidean acceleration techniques from CNN have little implication in the non-Euclidean nature of relational graphs. Instead of coping with the challenges arising from forcing naturally sparse structures into more inflexible stochastic arrangements, in DRAGON, we embrace this characteristic in order to promote acceleration. Inspired by high-performance computing approaches like Parallel Multi-moth Flame Optimization for Link Prediction (PMFO-LP), we propose and implement a novel efficient architecture, capable of producing similar speed-up and performance than baseline but at a fraction of its hardware requirements and power consumption. We leverage the hidden parallelistic capacity of our previously developed static graph convolutional processor ACE-GCN and expanded it with RNN structures, allowing the deployment of a multi-processing network referenced around a common pool of proximity-based centroids. Experimental results demonstrate outstanding acceleration. In comparison with the fastest CPU-based software implementation available in the literature, DRAGON has achieved roughly 191× speed-up. Under the largest configuration and dataset, DRAGON was also able to overtake a more power-hungry PMFO-LP by almost 1.59× in speed, and at around 89.59% in power efficiency. More importantly than raw acceleration, we demonstrate the unique functional qualities of our approach as a flexible and fault-tolerant solution that makes it an interesting alternative for an anthology of applicative scenarios.  © 2023 Association for Computing Machinery.",Convolutional neural networks; dynamic graphs; embedded systems; HW accelerator,Convolutional neural networks; Deep neural networks; Dynamics; Embedded systems; Energy efficiency; Online systems; Recurrent neural networks; Stochastic systems; Algorithmics; Convolutional neural network; Dynamic graph; Embedded-system; HW accelerator; Link prediction; Neural-networks; Optimisations; Physical world; Speed up; Convolution
Structured Dynamic Precision for Deep Neural Networks Quantization,2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147251787&doi=10.1145%2f3549535&partnerID=40&md5=79fcf986ebf8707d45e59f376391663b,"Deep Neural Networks (DNNs) have achieved remarkable success in various Artificial Intelligence applications. Quantization is a critical step in DNNs compression and acceleration for deployment. To further boost DNN execution efficiency, many works explore to leverage the input-dependent redundancy with dynamic quantization for different regions. However, the sensitive regions in the feature map are irregularly distributed, which restricts the real speed up for existing accelerators. To this end, we propose an algorithm-architecture co-design, named Structured Dynamic Precision (SDP). Specifically, we propose a quantization scheme in which the high-order bit part and the low-order bit part of data can be masked independently. And a fixed number of term parts are dynamically selected for computation based on the importance of each term in the group. We also present a hardware design to enable the algorithm efficiently with small overheads, whose inference time mainly scales with the precision proportionally. Evaluation experiments on extensive networks demonstrate that compared to the state-of-the-art dynamic quantization accelerator DRQ, our SDP can achieve 29% performance gain and 51% energy reduction for the same level of model accuracy. © 2023 Association for Computing Machinery.",algorithm-architecture co-design; compression and acceleration; Neural networks; systolic array,Inference engines; Memory architecture; Network architecture; Systolic arrays; Algorithm/architecture co-design; Compression and acceleration; Critical steps; Dynamic precision; Dynamic quantization; Feature map; Network compression; Neural-networks; Quantisation; Sensitive regions; Deep neural networks
"Memory-aware Partitioning, Scheduling, and Floorplanning for Partially Dynamically Reconfigurable Systems",2023,ACM Transactions on Design Automation of Electronic Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147255444&doi=10.1145%2f3534968&partnerID=40&md5=a928b0a4386541b2d8c95c93abef7bb7,"Partially dynamic reconfiguration (PDR) technology can accelerate the reconfiguration process and overcome hardware resource constraints when facing the challenge of high performance with respect to applications and resources constraints on field-programmable gate arrays (FPGAs). On FPGAs with PDR technology, the available on-chip Block RAM (BRAM) resources may not satisfy the memory requirements for all data. If we reserve more BRAM resources, then the total area of the dynamically reconfigurable region (DRR) that is used for calculation will decrease, with a reduction in system performance. We propose a memory-aware optimization framework to search for the optimal solution considering partitioning, scheduling, and floorplanning, where we make a tradeoff between performance and on-chip memory resources utilization. We then propose methods for memory allocation: An ILP model and a heuristic algorithm are provided to determine the minimum memory requirements and the number of corresponding memory blocks for data, as well as to determine whether the memory block with its stored data is assigned on-chip or off-chip by formulating the problem into a 0-1 knapsack problem and solving it using dynamic programming. Experimental results show that the memory-aware optimization framework and methods of memory allocation can increase the amount of on-chip data access to 29.65% of the total data volume with guaranteed performance.  © 2023 Association for Computing Machinery.",Memory allocation; memory block allocation; on-chip/off-chip memory block assignment; partially dynamically reconfigurabe systems,Combinatorial optimization; Dynamic models; Field programmable gate arrays (FPGA); Heuristic algorithms; Heuristic methods; Integrated circuit design; Memory architecture; Random access storage; Structural design; Block allocation; Chip-off; Memory aware; Memory block allocation; Memory blocks; Off-chip memory; On chips; On-chip/off-chip memory block assignment; Partially dynamically reconfigurabe system; Dynamic programming
