Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
ASCOS++: An asymmetric similarity measure for weighted networks to address the problem of SimRank,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945178846&doi=10.1145%2f2776894&partnerID=40&md5=cfef12f2015f0ec2c94707578286997d,"In this article, we explore the relationships among digital objects in terms of their similarity based on vertex similarity measures. We argue that SimRank-a famous similarity measure-and its families, such as P-Rank and SimRank++, fail to capture similar node pairs in certain conditions, especially when two nodes can only reach each other through paths of odd lengths. We present new similarity measures ASCOS and ASCOS++ to address the problem. ASCOS outputs a more complete similarity score than SimRank and SimRank's families. ASCOS++ enriches ASCOS to include edge weight into the measure, giving all edges and network weights an opportunity to make their contribution. We show that both ASCOS++ and ASCOS can be reformulated and applied on a distributed environment for parallel contribution. Experimental results show that ASCOS++ reports a better score than SimRank and several famous similarity measures. Finally, we re-examine previous use cases of SimRank, and explain appropriate and inappropriate use cases. We suggest future SimRank users following the rules proposed here before naï¿½vely applying it. We also discuss the relationship between ASCOS++ and PageRank. ï¿½ 2015 ACM.",ASCOS++; Coauthor network; Link analysis; Link prediction; SimRank; Vertex similarity,ASCOS; Co-author networks; Link analysis; Link prediction; Simrank; Vertex similarities; Data mining
GLAD: Group anomaly detection in social media analysis,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946022328&doi=10.1145%2f2811268&partnerID=40&md5=f87a019525d744640fe93b2591cc3445,"Traditional anomaly detection on social media mostly focuses onindividual point anomalies while anomalous phenomena usually occur in groups. Therefore, it is valuable to study the collective behavior of individuals and detect group anomalies. Existing group anomaly detection approaches rely on the assumption that the groups are known, which can hardly be true in real world social media applications. In this article, we take a generative approach by proposing a hierarchical Bayes model: Group Latent Anomaly Detection (GLAD) model. GLAD takes both pairwise and point-wise data as input, automatically infers the groups and detects group anomalies simultaneously. To account for the dynamic properties of the social media data, we further generalize GLAD to its dynamic extension d-GLAD. We conduct extensive experiments to evaluate our models on both synthetic and real world datasets. The empirical results demonstrate that our approach is effective and robust in discovering latent groups and detecting group anomalies. © 2015 ACM.",Community detection; Group anomaly; Topic modeling,Bayesian networks; Social networking (online); Anomalous phenomena; Collective behavior; Community detection; Group anomaly; Hierarchical Bayes models; Real-world datasets; Social media analysis; Topic Modeling; Anomaly detection
Context-aware recommendation using role-based trust network,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945190735&doi=10.1145%2f2751562&partnerID=40&md5=3de9c3e50c860d6a11e63b8831f3fe50,"Recommender systems have been studied comprehensively in both academic and industrial fields over the past decade. As user interests can be affected by context at any time and any place in mobile scenarios, rich context information becomes more and more important for personalized context-aware recommendations. Although existing context-aware recommender systems can make context-aware recommendations to some extent, they suffer several inherent weaknesses: (1) Users' context-aware interests are not modeled realistically, which reduces the recommendation quality; (2) Current context-aware recommender systems ignore trust relations among users. Trust relations are actually context-aware and associated with certain aspects (i.e., categories of items) in mobile scenarios. In this article, we define a term role to model common context-aware interests among a group of users. We propose an efficient role mining algorithm to mine roles from a ""user-context-behavior"" matrix, and a role-based trust model to calculate context-aware trust value between two users. During online recommendation, given a user u in a context c, an efficient weighted set similarity query (WSSQ) algorithm is designed to build u's role-based trust network in context c. Finally, we make recommendations to u based on u's role-based trust network by considering both context-aware roles and trust relations. Extensive experiments demonstrate that our recommendation approach outperforms the state-of-the-art methods in both effectiveness and efficiency. © 2015 ACM.",Context-aware; Recommendation; Role; Trust network,Recommender systems; Context-Aware; Context-aware recommendations; Context-aware recommender systems; Effectiveness and efficiencies; Recommendation; Role; State-of-the-art methods; Trust networks; Behavioral research
Socializing by gaming: Revealing social relationships in Multiplayer Online Games,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945200502&doi=10.1145%2f2736698&partnerID=40&md5=78a1a7b81b1f483a74097db334578458,"Multiplayer Online Games (MOGs) like Defense of the Ancients and StarCraft II have attracted hundreds of millions of users who communicate, interact, and socialize with each other through gaming. In MOGs, rich social relationships emerge and can be used to improve gaming services such as match recommendation and game population retention, which are important for the user experience and the commercial value of the companies who run these MOGs. In this work, we focus on understanding social relationships in MOGs. We propose a graph model that is able to capture social relationships of a variety of types and strengths. We apply our model to real-world data collected from three MOGs that contain in total over ten years of behavioral history for millions of players and matches. We compare social relationships in MOGs across different game genres and with regular online social networks like Facebook. Taking match recommendation as an example application of our model, we propose SAMRA, a Socially Aware Match Recommendation Algorithm that takes social relationships into account. We show that our model not only improves the precision of traditional link prediction approaches, but also potentially helps players enjoy games to a higher extent. © 2015 ACM.",Graph model; Multiplayer Online Games (MOGs); Social relationship; User interaction,Graph theory; Social networking (online); User experience; Graph model; Link prediction; Multi-player online games; On-line social networks; Real-world; Recommendation algorithms; Social relationships; User interaction; Social aspects
User identification across social media,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945185139&doi=10.1145%2f2747880&partnerID=40&md5=51ebfb07c8fd3935a1963bcd90ca67b4,"People use various social media sites for different purposes. The information on each site is often partial. When sources of complementary information are integrated, abetter profile of a user canbe built. This profile can help improve online services such as advertising across sites. To integrate these sources of information, it is necessary to identify individuals across social media sites. This paper aims to address the cross-media user identification problem. We provide evidence on the existence of a mapping among identities of individuals across social media sites, study the feasibility of finding this mapping, and illustrate and develop means for finding this mapping. Our studies show that effective approaches that exploit information redundancies due to users' unique behavioral patterns can be utilized to find such a mapping. This study paves the way for analysis and mining across social networking sites, and facilitates the creation of novel online services across sites. In particular, recommending friends and advertising across networks, analyzing information diffusion across sites, and studying specific user behavior such as user migration across sites in social media are one of the many areas that can benefit from the results of this study. © 2015 ACM.",Behavioral modeling; Cross-media analysis; Social network analysis; User identification,Behavioral research; Mapping; Marketing; Behavioral model; Cross-media; Effective approaches; Information diffusion; Information redundancies; Social networking sites; Sources of informations; User identification; Social networking (online)
A Bayesian perspective on locality sensitive hashing with extensions for kernel methods,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945191989&doi=10.1145%2f2778990&partnerID=40&md5=f8b3b2195fd6acd8429a86835ade22c7,"Given a collection of objects and an associated similarity measure, the all-pairs similarity search problem asks us to find all pairs of objects with similarity greater than a certain user-specified threshold. In order to reduce the number of candidates to search, locality-sensitive hashing (LSH) based indexing methods are very effective. However, most such methods only use LSH for the first phase of similarity search-that is, efficient indexing for candidate generation. In this article, we present BayesLSH, a principled Bayesian algorithm for the subsequent phase of similarity search-performing candidate pruning and similarity estimation using LSH. A simpler variant, BayesLSH-Lite, which calculates similarities exactly, is also presented. Our algorithms are able to quickly prune away a large majority of the false positive candidate pairs, leading to significant speedups over baseline approaches. For BayesLSH, we also provide probabilistic guarantees on the quality of the output, both in terms of accuracy and recall. Finally, the quality of BayesLSH's output can be easily tuned and does not require any manual setting of the number of hashes to use for similarity estimation, unlike standard approaches. For two state-of-the-art candidate generation algorithms, AllPairs and LSH, BayesLSH enables significant speedups, typically in the range 2×-20× for a wide variety of datasets. We also extend the BayesLSH algorithm for kernel methods-in which the similarity between two data objects is defined by a kernel function. Since the embedding of data points in the transformed kernel space is unknown, algorithms such as AllPairs which rely on building inverted index structure for fast similarity search do not work with kernel functions. Exhaustive search across all possible pairs is also not an option since the dataset can be huge and computing the kernel values for each pair can be prohibitive. We propose K-BayesLSH an all-pairs similarity search problem for kernel functions. K-BayesLSH leverages a recently proposed idea-kernelized locality sensitive hashing (KLSH)-for hash bit computation and candidate generation, and uses the aforementioned BayesLSH idea for candidate pruning and similarity estimation. We ran a broad spectrum of experiments on a variety of datasets drawn from different domains and with distinct kernels and find a speedup of 2×-7× over vanilla KLSH. © 2015 ACM.",All-pairs similarity search; Bayesian inference; Kernel similarity measure; Locality-sensitive hashing,Bayesian networks; Inference engines; Bayesian inference; Inverted index structures; Kernelized locality-sensitive hashing; Locality sensitive hashing; Probabilistic guarantees; Similarity estimation; Similarity measure; Similarity search; Indexing (of information)
Recommending users and communities in social media,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946127386&doi=10.1145%2f2757282&partnerID=40&md5=1e7114915ca52584a0d6c1adfaad5339,"Social media has become increasingly prevalent in the last few years, not only enabling people to connect with each other by social links, but also providing platforms for people to share information and interact over diverse topics. Rich user-generated information, for example, users relationships and daily posts, are often available in most social media service websites. Given such information, a challenging problem is to provide reasonable user and community recommendation for a target user, and consequently, help the target user engage in the daily discussions and activities with his/her friends or like-minded people. In this article, we propose a unified framework of recommending users and communities that utilizes the information in social media. Given a users profile or a set of keywords as input, our framework is capable of recommending influential users and topic-cohesive interactive communities that are most relevant to the given user or keywords. With the proposed framework, users can find other individuals or communities sharing similar interests, and then have more interaction with these users or within the communities. We present a generative topic model to discover user-oriented and community-oriented topics simultaneously, which enables us to capture the exact topical interests of users, as well as the focuses of communities. Extensive experimental evaluation and case studies on a dataset collected from Twitter demonstrate the effectiveness of our proposed framework compared with other probabilistic-topic-model-based recommendation methods.",Community recommendation; Social media; User recommendation; Usercommunity-topic model,Social networking (online); Community recommendations; Experimental evaluation; Probabilistic topic models; Recommendation methods; Social media; Social media services; Topic Modeling; User recommendations; Recommender systems
Data-aware vaccine allocation over large networks,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945185873&doi=10.1145%2f2803176&partnerID=40&md5=334aca4aea445f85bbdac425228f9248,"Givenagraph, likeasocial/computer networkorthe blogosphere, in which an infection (or meme or virus) has been spreading for some time, how to select the k best nodes for immunization/quarantining immediately? Most previous works for controlling propagation (say via immunization) have concentrated on developing strategies for vaccination preemptively before the start of the epidemic. While very useful to provide insights in to which baseline policies can best control an infection, they may not be ideal to make real-time decisions as the infection is progressing. In this paper, we study how to immunize healthy nodes, in the presence of already infected nodes. Efficient algorithms for such a problem can help public-health experts make more informed choices, tailoring their decisions to the actual distribution of the epidemic on the ground. First we formulate the Data-Aware Vaccination problem, and prove it is NP-hard and also that it is hard to approximate. Secondly, we propose three effective polynomial-time heuristics DAVA, DAVA-prune and DAVA-fast, of varying degrees of efficiency and performance. Finally, we also demonstrate the scalability and effectiveness of our algorithms through extensive experiments on multiple real networks including large epidemiology datasets (containing millions of interactions). Our algorithms show substantial gains of up to ten times more healthy nodes at the end against many other intuitive and nontrivial competitors. © 2015 ACM.",Diffusion; Graph mining; Immunization; Social networks,Diffusion; Epidemiology; Immunization; Large dataset; Polynomial approximation; Social networking (online); Viruses; Developing strategy; Efficiency and performance; Graph mining; Large networks; Polynomial time heuristics; Public health experts; Real networks; Real time decisions; Vaccines
Refining social graph connectivity via shortcut edge addition,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945193193&doi=10.1145%2f2757281&partnerID=40&md5=5ddc96e3bde6150bd387308c973758e6,"Small changes on the structure of a graph can have a dramatic effect on its connectivity. While in the traditional graph theory, the focus is on well-defined properties of graph connectivity, such as biconnectivity, in the context of a social graph, connectivity is typically manifested by its ability to carry on social processes. In this paper, we consider the problem of adding a small set of nonexisting edges (shortcuts) in a social graph with the main objective of minimizing its characteristic path length. This property determines the average distance between pairs of vertices and essentially controls how broadly information can propagate through a network. We formally define the problem of interest, characterize its hardness and propose a novel method, path screening, which quickly identifies important shortcuts to guide the augmentation of the graph. We devise a sampling-based variant of our method that can scale up the computation in larger graphs. The claims of our methods are formally validated. Through experiments on real and synthetic data, we demonstrate that our methods are a multitude of times faster than standard approaches, their accuracy outperforms sensible baselines and they can ease the spread of information in a network, for a varying range of conditions. © 2015 ACM.",Conductance; Graph augmentation; Mixing time; Network engineering; Propagation; Shortcuts; Social networks,Computation theory; Electric conductance; Mixer circuits; Social networking (online); Wave propagation; Average Distance; Characteristic path lengths; Graph augmentation; Graph connectivity; Mixing time; Network engineering; Shortcuts; Spread of informations; Graph theory
Occupancy-based frequent pattern mining,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945201262&doi=10.1145%2f2753765&partnerID=40&md5=b40347fd98fe50e2290c795c0cd9f1d5,"Frequent pattern mining is an important data mining problem with many broad applications. Most studies in this field use support (frequency) to measure the popularity of a pattern, namely the fraction of transactions or sequences that include the pattern in a data set. In this study, we introduce a new interesting measure, namely occupancy, to measure the completeness of a pattern in its supporting transactions or sequences. This is motivated by some real-world pattern recommendation applications in which an interesting pattern should not only be frequent, but also occupies a large portion of its supporting transactions or sequences. With the definition of occupancy we call a pattern dominant if its occupancy value is above a user-specified threshold. Then, our task is to identify the qualified patterns which are both dominant and frequent. Also, we formulate the problem of mining top-k qualified patterns, that is, finding k qualified patterns with maximum values on a user-defined function of support and occupancy, for example, weighted sum of support and occupancy. The challenge to these tasks is that the value of occupancy does not change monotonically when more items are appended to a given pattern. Therefore, we propose a general algorithm called DOFRA (DOminant and FRequent pattern mining Algorithm) for mining these qualified patterns, which explores the upper bound properties on occupancy to drastically reduce the search process. Finally, we show the effectiveness of DOFRA in two real-world applications and also demonstrate the efficiency of DOFRA on several real and large synthetic datasets. © 2015 ACM.",Bound estimation; Dominant pattern; Occupancy,Large dataset; Bound estimation; Broad application; Data mining problems; Dominant pattern; Frequent pattern mining; Occupancy; Synthetic datasets; User Defined Functions; Data mining
Discovering information propagation patterns in microblogging services,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938295069&doi=10.1145%2f2742801&partnerID=40&md5=df533658b50188d41f7a47fb225a99b5,"During the last decade, microblog has become an important social networking service with billions of users all over the world, acting as a novel and efficient platform for the creation and dissemination of real-time information. Modeling and revealing the information propagation patterns in microblogging services cannot only lead to more accurate understanding of user behaviors and provide insights into the underlying sociology, but also enable useful applications such as trending prediction, recommendation and filtering, spam detection and viral marketing. In this article, we aim to reveal the information propagation patterns in Sina Weibo, the biggest microblogging service in China. First, the cascade of each message is represented as a tree based on its retweeting process. Afterwards, we divide the information propagation pattern into two levels, that is, the macro level and the micro level. On one hand, the macro propagation patterns refer to general propagation modes that are extracted by grouping propagation trees based on hierarchical clustering. On the other hand, the micro propagation patterns are frequent information flow patterns that are discovered using tree-based mining techniques. Experimental results show that several interesting patterns are extracted, such as popular message propagation, artificial propagation, and typical information flows between different types of users. © 2015 ACM.",Algorithms; H.1.2 [user/machine systems]: Human factors; H.2.8 [database applications]: Data mining; Human factors; Information propagation pattern; Message cascade; Microblogging services; Propagation tree,Algorithms; Behavioral research; Chemical detection; Data mining; Hierarchical clustering; Human engineering; Information dissemination; Sociology; Trees (mathematics); Database applications; Information propagation; Message propagation; Micro-blogging services; Propagation pattern; Real-time information; Social networking services; User/machine systems; Information filtering
Algorithms for mining the coevolving relational motifs in dynamic networks,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938358965&doi=10.1145%2f2733380&partnerID=40&md5=ada7440ca7b9ac68311a1012b23faf79,"Computational methods and tools that can efficiently and effectively analyze the temporal changes in dynamic complex relational networks enable us to gain significant insights regarding the entity relations and their evolution. This article introduces a new class of dynamic graph patterns, referred to as coevolving relational motifs (CRMs), which are designed to identify recurring sets of entities whose relations change in a consistent way over time. CRMs can provide evidence to the existence of, possibly unknown, coordination mechanisms by identifying the relational motifs that evolve in a similar and highly conserved fashion. We developed an algorithm to efficiently analyze the frequent relational changes between the entities of the dynamic networks and capture all frequent coevolutions as CRMs. Our algorithm follows a depth-first exploration of the frequent CRM lattice and incorporates canonical labeling for redundancy elimination. Experimental results based on multiple real world dynamic networks show that the method is able to efficiently identify CRMs. In addition, a qualitative analysis of the results shows that the discovered patterns can be used as features to characterize the dynamic network. © 2015 ACM.","Algorithms; Dynamic networks; Evolving pattern mining; G.2.2 [graph theory]: Graph mining; I.2.8 [problem solving, control methods, and search]: Graph mining; Network evolution",Algorithms; Computation theory; Data mining; Graph theory; Control methods; Dynamic network; Evolving patterns; Graph mining; Network evolution; Graph algorithms
Utility-theoretic ranking for semiautomated text classification,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938385374&doi=10.1145%2f2742548&partnerID=40&md5=8b1040b8633a76cb4b08a9d94e30b628,"Semiautomated Text Classification (SATC) may be defined as the task of ranking a set D of automatically labelled textual documents in such a way that, if a human annotator validates (i.e., inspects and corrects where appropriate) the documents in a top-ranked portion of D with the goal of increasing the overall labelling accuracy of D, the expected increase is maximized. An obvious SATC strategy is to rank D so that the documents that the classifier has labelled with the lowest confidence are top ranked. In this work, we show that this strategy is suboptimal. We develop new utility-theoretic ranking methods based on the notion of validation gain, defined as the improvement in classification effectiveness that would derive by validating a given automatically labelled document. We also propose a new effectiveness measure for SATC-oriented ranking methods, based on the expected reduction in classification error brought about by partially validating a list generated by a given ranking method. We report the results of experiments showing that, with respect to the baseline method mentioned earlier, and according to the proposed measure, our utility-theoretic ranking methods can achieve substantially higher expected reductions in classification error. © 2015 ACM.",Algorithm; Cost-sensitive learning; Design; Experimentation; H. [Information retrieval]: Information systems; I. [Machine learning]: Computing methodologies; Learning paradigms-supervised learning; Measurements; Ranking; Retrieval tasks and goals-clustering and classification; Semiautomated text classification; Supervised learning; Text classification,Algorithms; Automation; Classification (of information); Design; Information retrieval systems; Measurement; Search engines; Supervised learning; Computing methodologies; Cost-sensitive learning; Experimentation; Learning paradigms; Ranking; Text classification; Text processing
Social influence based clustering and optimization over heterogeneous information networks,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938406989&doi=10.1145%2f2717314&partnerID=40&md5=4d2dfd654465a7829d19cb3b413bed4a,"Social influence analysis has shown great potential for strategic marketing decision. It is well known that people influence one another based on both their social connections and the social activities that they have engaged in the past. In this article, we develop an innovative and high-performance social influence based graph clustering framework with four unique features. First, we explicitly distinguish social connection based influence (self-influence) and social activity based influence (co-influence). We compute the self-influence similarity between two members based on their social connections within a single collaboration network, and compute the co-influence similarity by taking into account not only the set of activities that people participate but also the semantic association between these activities. Second, we define the concept of influence-based similarity by introducing a unified influence-based similarity matrix that employs an iterative weight update method to integrate self-influence and co-influence similarities. Third, we design a dynamic learning algorithm, called SI-CLUSTER, for social influence based graph clustering. It iteratively partitions a large social collaboration network into K clusters based on both the social network itself and the multiple associated activity information networks, each representing a category of activities that people have engaged. To make the SI-CLUSTER algorithm converge fast, we transform sophisticated nonlinear fractional programming problem with respect to multiple weights into a straightforward nonlinear parametric programming problem of single variable. Finally, we develop an optimization technique of diagonalizable-matrix approximation to speed up the computation of self-influence similarity and co-influence similarities. Our SI-Cluster-Opt significantly improves the efficiency of SI-Cluster on large graphs while maintaining high quality of clustering results. Extensive experimental evaluation on three real-world graphs shows that, compared to existing representative graph clustering algorithms, our SI-CLUSTER-OPT approach not only achieves a very good balance between self-influence and co-influence similarities but also scales extremely well for clustering large graphs in terms of time complexity while meeting the guarantee of high density, low entropy and low Davies-Bouldin Index. © 2015 ACM.",Algorithms; Experimentation; Graph clustering; H.2.8 [Database management]: Database applications-data mining; Heterogeneous information network; Performance; Social influence,Algorithms; Data mining; Economic and social effects; Graph algorithms; Information management; Information services; Iterative methods; Learning algorithms; Mathematical programming; Mathematical transformations; Matrix algebra; Petroleum reservoir evaluation; Semantics; Experimentation; Graph clustering; H.2.8 [database management]: database applications - data minings; Heterogeneous information; Performance; Social influence; Clustering algorithms
Measuring temporal patterns in dynamic social networks,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938355062&doi=10.1145%2f2749465&partnerID=40&md5=8b78b6735da8db9a185012a09a5bcd16,"Given social networks over time, how can we measure network activities across different timesteps with a limited number of metrics? We propose two classes of dynamic metrics for assessing temporal evolution patterns of agents in terms of persistency and emergence. For each class of dynamic metrics, we implement it using three different temporal aggregation models ranging from the most commonly used Average Aggregation Model to more the complex models such as the Exponential Aggregation Model. We argue that the problem of measuring temporal patterns can be formulated using Recency and Primacy effect, which is a concept used to characterize human cognitive processes. Experimental results show that the way metrics model Recency-Primacy effect is closely related to their abilities to measure temporal patterns. Furthermore, our results indicate that future network agent activities can be predicted based on history information using dynamic metrics. By conducting multiple experiments, we are also able to find an optimal length of history information that is most relevant to future activities. This optimal length is highly consistent within a dataset and can be used as an intrinsic metric to evaluate a dynamic social network. © 2015, Association for Computing Machinery. All rights reserved.",Aggregating method; Algorithms; Data mining; Measurement; Performance; Primacy and recency effects; Social network analysis; Temporal analysis,Algorithms; Computer science; Measurement; Social networking (online); Aggregating methods; Dynamic social networks; History informations; Network activities; Performance; Recency effects; Temporal aggregation; Temporal analysis; Data mining
"Hierarchical density estimates for data clustering, visualization, and outlier detection",2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938375176&doi=10.1145%2f2733381&partnerID=40&md5=b007c8cdf92edf0224a337c604e62c66,"An integrated framework for density-based cluster analysis, outlier detection, and data visualization is introduced in this article. The main module consists of an algorithm to compute hierarchical estimates of the level sets of a density, following Hartigan's classic model of density-contour clusters and trees. Such an algorithm generalizes and improves existing density-based clustering techniques with respect to different aspects. It provides as a result a complete clustering hierarchy composed of all possible density-based clusters following the nonparametric model adopted, for an infinite range of density thresholds. The resulting hierarchy can be easily processed so as to provide multiple ways for data visualization and exploration. It can also be further postprocessed so that: (i) a normalized score of ""outlierness"" can be assigned to each data object, which unifies both the global and local perspectives of outliers into a single definition; and (ii) a ""flat""(i.e., nonhierarchical) clustering solution composed of clusters extracted from local cuts through the cluster tree (possibly corresponding to different density thresholds) can be obtained, either in an unsupervised or in a semisupervised way. In the unsupervised scenario, the algorithm corresponding to this postprocessing module provides a global, optimal solution to the formal problem of maximizing the overall stability of the extracted clusters. If partially labeled objects or instance-level constraints are provided by the user, the algorithm can solve the problem by considering both constraints violations/satisfactions and cluster stability criteria. An asymptotic complexity analysis, both in terms of running time and memory space, is described. Experiments are reported that involve a variety of synthetic and real datasets, including comparisons with state-of-the-art, density-based clustering and (global and local) outlier detection methods. © 2015 ACM.",Algorithms; Clustering; Data mining; Data visualization; Density-based clustering; Global/local outliers; H.2.8 [database mamagement]: Database applications - Data mining; H.3.3 [information storage and retrieval]: Information search and retrieval - Clustering; Hierarchical and nonhierarchical clustering; I.5.3 [pattern recognition]: Clustering - Algorithms; Outlier detection; Unsupervised and semisupervised clustering,Algorithms; Anomaly detection; Data mining; Data visualization; Digital storage; Hierarchical clustering; Pattern recognition; Stability criteria; Statistics; Trees (mathematics); Visualization; Clustering; Database applications; Density-based Clustering; Global/Local; H.3.3 [information storage and retrieval]: information search and retrievals; Hierarchical and nonhierarchical clustering; Semi-supervised Clustering; Clustering algorithms
ParCube: Sparse parallelizable CANDECOMP-PARAFAC tensor decomposition,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938355609&doi=10.1145%2f2729980&partnerID=40&md5=5eb8a90f6a67763a94e519eb338bfc86,"How can we efficiently decompose a tensor into sparse factors, when the data do not fit in memory? Tensor decompositions have gained a steadily increasing popularity in data-mining applications; however, the current state-of-art decomposition algorithms operate on main memory and do not scale to truly large datasets. In this work, we propose PARCUBE, a new and highly parallelizable method for speeding up tensor decompositions that is well suited to produce sparse approximations. Experiments with even moderately large data indicate over 90% sparser outputs and 14 times faster execution, with approximation error close to the current state of the art irrespective of computation and memory requirements. We provide theoretical guarantees for the algorithm's correctness and we experimentally validate our claims through extensive experiments, including four different real world datasets (ENRON, LBNL, FACEBOOK and NELL), demonstrating its effectiveness for data-mining practitioners. In particular, we are the first to analyze the very large NELL dataset using a sparse tensor decomposition, demonstrating that PARCUBE enables us to handle effectively and efficiently very large datasets. Finally, we make our highly scalable parallel implementation publicly available, enabling reproducibility of our work. © 2015 ACM.",Algorithms; H.2.8 [database applications]: Data mining; H.3.3 [information search and retrieval]: Clustering; PARAFA decomposition; Parallel algorithms; Performance; Randomized algorithms; Sampling; Sparsity; Tensors,Algorithms; Arts computing; Clustering algorithms; Large dataset; Parallel algorithms; Sampling; Tensors; Database applications; Information search and retrieval; Performance; Randomized Algorithms; Sparsity; Data mining
Large-scale cross-language web page classification via dual knowledge transfer using fast nonnegative matrix trifactorization,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938358416&doi=10.1145%2f2710021&partnerID=40&md5=d0d338496ca0cfd4e5f8198447c09143,"With the rapid growth of modern technologies, Internet has reached almost every corner of the world. As a result, it becomes more and more important to manage and mine information contained in Web pages in different languages. Traditional supervised learning methods usually require a large amount of training data to obtain accurate and robust classification models. However, labeled Web pages did not increase as fast as the growth of Internet. The lack of sufficient training Web pages in many languages, especially for those in uncommonly used languages, makes it a challenge for traditional classification algorithms to achieve satisfactory performance. To address this, we observe that Web pages for a same topic from different languages usually share some common semantic patterns, though in different representation forms. In addition, we also observe that the associations between word clusters and Web page classes are another type of reliable carriers to transfer knowledge across languages. With these recognitions, in this article we propose a novel joint nonnegative matrix trifactorization (NMTF) based Dual Knowledge Transfer (DKT) approach for cross-language Web page classification. Our approach transfers knowledge from the auxiliary language, in which abundant labeled Web pages are available, to the target languages, in which we want to classify Web pages, through two different paths: word cluster approximation and the associations between word clusters and Web page classes. With the reinforcement between these two different knowledge transfer paths, our approach can achieve better classification accuracy. In order to deal with the large-scale real world data, we further develop the proposed DKT approach by constraining the factor matrices of NMTF to be cluster indicator matrices. Due to the nature of cluster indicator matrices, we can decouple the proposed optimization objective and the resulted subproblems are of much smaller sizes involving much less matrix multiplications, which make our new approach much more computationally efficient. We evaluate the proposed approach in extensive experiments using a real world cross-language Web page data set. Promising results have demonstrated the effectiveness of our approach that are consistent with our theoretical analyses. © 2015 ACM.",Algorithms; Cluster indicator matrix; Cross-language classification; Experimentation; H.3.1 [Information storage and retrieval]: Content analysis and indexing; Knowledge transfer; Large-scale data; Nonnegative matrix trifactorization,Algorithms; Digital storage; Electronic document exchange; Knowledge management; Learning systems; Matrix algebra; Semantics; Websites; Content analysis; Cross languages; Experimentation; Indicator matrix; Knowledge transfer; Large scale data; Non-negative matrix; Classification (of information)
Smart multitask Bregman clustering and multitask Kernel clustering,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938331799&doi=10.1145%2f2747879&partnerID=40&md5=85c580c33cbf867bdba277a88b33ae21,"Traditional clustering algorithms deal with a single clustering task on a single dataset. However, there are many related tasks in the real world, which motivates multitask clustering. Recently some multitask clustering algorithms have been proposed, and among them multitask Bregman clustering (MBC) is a very applicable method. MBC alternatively updates clusters and learns relationships between clusters of different tasks, and the two phases boost each other. However, the boosting does not always have positive effects on improving the clustering performance, it may also cause negative effects. Another issue of MBC is that it cannot deal with nonlinear separable data. In this article, we show that in MBC, the process of using cluster relationship to boost the cluster updating phase may cause negative effects, that is, cluster centroids may be skewed under some conditions. We propose a smart multitask Bregman clustering (S-MBC) algorithm which can identify the negative effects of the boosting and avoid the negative effects if they occur. We then propose a multitask kernel clustering (MKC) framework for nonlinear separable data by using a similar framework like MBC in the kernel space. We also propose a specific optimization method, which is quite different from that of MBC, to implement the MKC framework. Since MKC can also cause negative effects like MBC, we further extend the framework of MKC to a smart multitask kernel clustering (S-MKC) framework in a similar way that S-MBC is extended from MBC. We conduct experiments on 10 real world multitask clustering datasets to evaluate the performance of S-MBC and S-MKC. The results on clustering accuracy show that: (1) compared with the original MBC algorithm MBC, S-MBC and S-MKC perform much better; (2) compared with the convex discriminative multitask relationship clustering (DMTRC) algorithms DMTRC-L and DMTRC-R which also avoid negative transfer, S-MBC and S-MKC perform worse in the (ideal) case in which different tasks have the same cluster number and the empirical label marginal distribution in each task distributes evenly, but better or comparable in other (more general) cases. Moreover, S-MBC and S-MKC can work on the datasets in which different tasks have different number of clusters, violating the assumptions of DMTRC-L and DMTRC-R. The results on efficiency show that S-MBC and S-MKC consume more computational time than MBC and less computational time than DMTRC-L and DMTRC-R. Overall S-MBC and S-MKC are competitive compared with the state-of-the-art multitask clustering algorithms in synthetical terms of accuracy, efficiency and applicability. 2015 Copyright is held by the owner/author(s).",Algorithms; Bregman divergence; I.5.3 [pattern recognition]: Clustering; Mercer kernel; Multitask clustering; Negative transfer; Performance,Algorithms; Efficiency; Pattern recognition; Bregman divergences; I.5.3 [pattern recognition]: Clustering; Mercer Kernel; Negative transfer; Performance; Clustering algorithms
Rationality analytics from trajectories,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938388849&doi=10.1145%2f2735634&partnerID=40&md5=c8352f5515dad83300050760e58dc022,"The availability of trajectories tracking the geographical locations of people as a function of time offers an opportunity to study human behaviors. In this article, we study rationality from the perspective of user decision on visiting a point of interest (POI) which is represented as a trajectory. However, the analysis of rationality is challenged by a number of issues, for example, how to model a trajectory in terms of complex user decision processes? and how to detect hidden factors that have significant impact on the rational decision making? In this study, we propose Rationality Analysis Model (RAM) to analyze rationality from trajectories in terms of a set of impact factors. In order to automatically identify hidden factors, we propose a method, Collective Hidden Factor Retrieval (CHFR), which can also be generalized to parse multiple trajectories at the same time or parse individual trajectories of different time periods. Extensive experimental study is conducted on three large-scale real-life datasets (i.e., taxi trajectories, user shopping trajectories, and visiting trajectories in a theme park). The results show that the proposed methods are efficient, effective, and scalable. We also deploy a system in a large theme park to conduct a field study. Interesting findings and user feedback of the field study are provided to support other applications in user behavior mining and analysis, such as business intelligence and user management for marketing purposes. © 2015 ACM.",Algorithms; Data mining; Decision model; Design; Performance; Rationality analytics; Trajectory,Algorithms; Data mining; Decision making; Design; Large dataset; Taxicabs; Trajectories; Decision modeling; Function of time; Geographical locations; Performance; Point of interest; Rational decision making; Rationality analytics; Real life datasets; Behavioral research
Process discovery under precedence constraints,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930791285&doi=10.1145%2f2710020&partnerID=40&md5=37a4b62addf125e8ec6ca137942a1df5,"Process discovery has emerged as a powerful approach to support the analysis and the design of complex processes. It consists of analyzing a set of traces registering the sequence of tasks performed along several enactments of a transactional system, in order to build a process model that can explain all the episodes recorded over them. An approach to accomplish this task is presented that can benefit from the background knowledge that, in many cases, is available to the analysts taking care of the process (re-)design. The approach is based on encoding the information gathered from the log and the (possibly) given background knowledge in terms of precedence constraints, that is, of constraints over the topology of the resulting process models. Mining algorithms are eventually formulated in terms of reasoning problems over precedence constraints, and the computational complexity of such problems is thoroughly analyzed by tracing their tractability frontier. Solution algorithms are proposed and their properties analyzed. These algorithms have been implemented in a prototype system, and results of a thorough experimental activity are discussed. © 2015 ACM.",Computational complexity; Graph analysis; Process mining,Computational complexity; Back-ground knowledge; Experimental activities; Graph analysis; Precedence constraints; Process mining; Reasoning problems; Solution algorithms; Transactional systems; Data mining
Density-aware clustering based on aggregated heat kernel and its transformation,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930798076&doi=10.1145%2f2700385&partnerID=40&md5=c4c64b821c7cadeb9f2d973bf0ce818f,"Current spectral clustering algorithms suffer from the sensitivity to existing noise and parameter scaling and may not be aware of different density distributions across clusters. If these problems are left untreated, the consequent clustering results cannot accurately represent true data patterns, in particular, for complex real-world datasets with heterogeneous densities. This article aims to solve these problems by proposing a diffusion-based Aggregated Heat Kernel (AHK) to improve the clustering stability, and a Local Density Affinity Transformation (LDAT) to correct the bias originating from different cluster densities. AHK statistically models the heat diffusion traces along the entire time scale, so it ensures robustness during the clustering process, while LDAT probabilistically reveals the local density of each instance and suppresses the local density bias in the affinity matrix. Our proposed framework integrates these two techniques systematically. As a result, it not only provides an advanced noise-resisting and density-aware spectral mapping to the original dataset but also demonstrates the stability during the processing of tuning the scaling parameter (which usually controls the range of neighborhood). Furthermore, our framework works well with the majority of similarity kernels, which ensures its applicability to many types of data and problem domains. The systematic experiments on different applications show that our proposed algorithm outperforms state-of-the-art clustering algorithms for the data with heterogeneous density distributions and achieves robust clustering performance with respect to tuning the scaling parameter and handling various levels and types of noise. © 2015 ACM.",Aggregated heat kernel; Local density affinity transformation,Parameter estimation; Photomapping; Affinity transformation; Clustering stability; Density distributions; Different densities; Heat kernel; Real-world datasets; Spectral clustering algorithms; Systematic experiment; Clustering algorithms
Improving top-N recommendation for cold-start users via cross-domain information,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930813163&doi=10.1145%2f2724720&partnerID=40&md5=7f90acd6b05f97b59964df7fc068e69c,"Making accurate recommendations for cold-start users is a challenging yet important problem in recommendation systems. Including more information from other domains is a natural solution to improve the recommendations. However, most previous work in cross-domain recommendations has focused on improving prediction accuracy with several severe limitations. In this article, we extend our previous work on clustering-based matrix factorization in single domains into cross domains. In addition, we utilize recent results on unobserved ratings. Our new method can more effectively utilize data from auxiliary domains to achieve better recommendations, especially for cold-start users. For example, our method improves the recall to 21% on average for cold-start users, whereas previous methods result in only 15% recall in the cross-domain Amazon dataset. We also observe almost the same improvements in the Epinions dataset. Considering that it is often difficult to make even a small improvement in recommendations, for cold- start users in particular, our result is quite significant. © 2015 ACM.",Cold start; Collaborative filtering; Matrix factorization; Recommendation system,Collaborative filtering; Factorization; Recommender systems; Cold start; Cross-domain; Cross-domain recommendations; Matrix factorizations; Prediction accuracy; Single domains; Matrix algebra
Chromatic correlation clustering,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930804710&doi=10.1145%2f2728170&partnerID=40&md5=27428e3f9873d0b9a03a0e19fc3c6272,"We study a novel clustering problem in which the pairwise relations between objects are categorical. This problem can be viewed as clustering the vertices of a graph whose edges are of different types (colors). We introduce an objective function that ensures the edges within each cluster have, as much as possible, the same color. We show that the problem is NP-hard and propose a randomized algorithm with approximation guarantee proportional to the maximum degree of the input graph. The algorithm iteratively picks a random edge as a pivot, builds a cluster around it, and removes the cluster from the graph. Although being fast, easy to implement, and parameter-free, this algorithm tends to produce a relatively large number of clusters. To overcome this issue we introduce a variant algorithm, which modifies how the pivot is chosen and how the cluster is built around the pivot. Finally, to address the case where a fixed number of output clusters is required, we devise a third algorithm that directly optimizes the objective function based on the alternating-minimization paradigm. We also extend our objective function to handle cases where object's relations are described by multiple labels. We modify our randomized approximation algorithm to optimize such an extended objective function and show that its approximation guarantee remains proportional to the maximum degree of the graph. We test our algorithms on synthetic and real data from the domains of social media, protein-interaction networks, and bibliometrics. Results reveal that our algorithms outperform a baseline algorithm both in the task of reconstructing a ground-truth clustering and in terms of objective-function value. © 2015 ACM.",Clustering; Correlation clustering; Edge-labeled graphs,Approximation algorithms; Graph algorithms; Graph structures; Iterative methods; Optimization; Alternating minimization; Clustering; Correlation clustering; Labeled graphs; Objective function values; Protein interaction networks; Randomized approximation; Synthetic and real data; Clustering algorithms
Mathematical modeling and analysis of product rating with partial information,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930792120&doi=10.1145%2f2700386&partnerID=40&md5=fa49a3cd36f26b585103b074a2601bcf,"Many Web services like Amazon, Epinions, and TripAdvisor provide historical product ratings so that users can evaluate the quality of products. Product ratings are important because they affect how well a product will be adopted by the market. The challenge is that we only have partial information on these ratings: each user assigns ratings to only a small subset of products. Under this partial information setting, we explore a number of fundamental questions. What is the minimum number of ratings a product needs so that one can make a reliable evaluation of its quality? How may users' misbehavior, such as cheating in product rating, affect the evaluation result? To answer these questions, we present a probabilistic model to capture various important factors (e.g., rating aggregation rules, rating behavior) that may influence the product quality assessment under the partial information setting. We derive the minimum number of ratings needed to produce a reliable indicator on the quality of a product. We extend our model to accommodate users' misbehavior in product rating. We derive the maximum fraction of misbehaving users that a rating aggregation rule can tolerate and the minimum number of ratings needed to compensate. We carry out experiments using both synthetic and real-world data (from Amazon and TripAdvisor). We not only validate our model but also show that the ""average rating rule"" produces more reliable and robust product quality assessments than the ""majority rating rule"" and the ""median rating rule"" in aggregating product ratings. Last, we perform experiments on two movie rating datasets (from Flixster and Netflix) to demonstrate how to apply our framework to improve the applications of recommender systems. © 2015 ACM.",Bias; Minimum number of ratings; Misbehavior; Product rating; Rating aggregation rule; True quality,Web services; Bias; Evaluation results; Misbehavior; Partial information; Probabilistic modeling; Product ratings; Quality assessment; Rating aggregation; Quality control
Optimizing text quantifiers for multivariate loss functions,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930792020&doi=10.1145%2f2700406&partnerID=40&md5=62bef7f3151ca7efe2feb7823df66af0,"We address the problem of quantification, a supervised learning task whose goal is, given a class, to estimate the relative frequency (or prevalence) of the class in a dataset of unlabeled items. Quantification has several applications in data and text mining, such as estimating the prevalence of positive reviews in a set of reviews of a given product or estimating the prevalence of a given support issue in a dataset of transcripts of phone calls to tech support. So far, quantification has been addressed by learning a general-purpose classifier, counting the unlabeled items that have been assigned the class, and tuning the obtained counts according to some heuristics. In this article, we depart from the tradition of using general-purpose classifiers and use instead a supervised learning model for structured prediction, capable of generating classifiers directly optimized for the (multivariate and nonlinear) function used for evaluating quantification accuracy. The experiments that we have run on 5,500 binary high-dimensional datasets (averaging more than 14,000 documents each) show that this method is more accurate, more stable, and more efficient than existing state-of-the-art quantification methods. © 2015 ACM.",Kullback-Leibler divergence; Loss functions; Prevalence estimation; Prior estimation; Quantification; Supervised learning; Text classification,Classification (of information); Optimization; Supervised learning; Kullback Leibler divergence; Loss functions; Prevalence estimation; Quantification; Text classification; Text mining
Supporting exploratory hypothesis testing and analysis,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930802584&doi=10.1145%2f2701430&partnerID=40&md5=80e3ad3ed19dfb953f5a9dd81172e10b,"Conventional hypothesis testing is carried out in a hypothesis-driven manner. A scientist must first formulate a hypothesis based on what he or she sees and then devise a variety of experiments to test it. Given the rapid growth of data, it has become virtually impossible for a person to manually inspect all data to find all of the interesting hypotheses for testing. In this article, we propose and develop a data-driven framework for automatic hypothesis testing and analysis. We define a hypothesis as a comparison between two or more subpopulations. We find subpopulations for comparison using frequent pattern mining techniques and then pair them up for statistical hypothesis testing. We also generate additional information for further analysis of the hypotheses that are deemed significant. The number of hypotheses generated can be very large, and many of them are very similar. We develop algorithms to remove redundant hypotheses and present a succinct set of significant hypotheses to users. We conducted a set of experiments to show the efficiency and effectiveness of the proposed algorithms. The results show that our system can help users (1) identify significant hypotheses efficiently, (2) isolate the reasons behind significant hypotheses efficiently, and (3) find confounding factors that form Simpson's paradoxes with discovered significant hypotheses. © 2015 ACM.",Actionable knowledge; Comparative data analysis; Exploratory data mining; Exploratory hypothesis testing,Computer science; Actionable knowledge; Comparative data; Data driven; Exploratory data mining; Frequent pattern mining; Hypothesis testing; Rapid growth; Statistical hypothesis testing; Data mining
Classification with streaming features: An emerging-pattern mining approach,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930813519&doi=10.1145%2f2700409&partnerID=40&md5=1d6b31e34bed83920f886309f5bfa233,"Many datasets from real-world applications have very high-dimensional or increasing feature space. It is a new research problem to learn and maintain a classifier to deal with very high dimensionality or streaming features. In this article, we adapt the well-known emerging-pattern-based classification models and propose a semi-streaming approach. For streaming features, it is computationally expensive or even prohibitive to mine long-emerging patterns, and it is nontrivial to integrate emerging-pattern mining with feature selection. We present an online feature selection step, which is capable of selecting and maintaining a pool of effective features from a feature stream. Then, in our offline step, separated from the online step, we periodically compute and update emerging patterns from the pool of selected features from the online step. We evaluate the effectiveness and efficiency of the proposed method using a series of benchmark datasets and a real-world case study on Mars crater detection. Our proposed method yields classification performance comparable to the state-of-art static classification methods. Most important, the proposed method is significantly faster and can efficiently handle datasets with streaming features. © 2015 ACM.",Classification; Emerging patterns; Feature selection; Streaming features,Data mining; Feature extraction; Classification models; Classification performance; Effectiveness and efficiencies; Emerging patterns; High dimensionality; Online feature selection; Static classification; Streaming approach; Classification (of information)
Information measures in statistical privacy and data processing applications,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930813442&doi=10.1145%2f2700407&partnerID=40&md5=8e1e08cdfeba7937720a725ebb1c9deb,"In statistical privacy, utility refers to two concepts: information preservation, how much statistical information is retained by a sanitizing algorithm, and usability, how (and with how much difficulty) one extracts this information to build statistical models, answer queries, and so forth. Some scenarios incentivize a separation between information preservation and usability, so that the data owner first chooses a sanitizing algorithm to maximize a measure of information preservation, and, afterward, the data consumers process the sanitized output according to their various individual needs [Ghosh et al. 2009; Williams and McSherry 2010]. We analyze the information-preserving properties of utility measures with a combination of two new and three existing utility axioms and study how violations of an axiom can be fixed. We show that the average (over possible outputs of the sanitizer) error of Bayesian decision makers forms the unique class of utility measures that satisfy all of the axioms. The axioms are agnostic to Bayesian concepts such as subjective probabilities and hence strengthen support for Bayesian views in privacy research. In particular, this result connects information preservation to aspects of usability - if the information preservation of a sanitizing algorithm should be measured as the average error of a Bayesian decision maker, shouldn't Bayesian decision theory be a good choice when it comes to using the sanitized outputs for various purposes? We put this idea to the test in the unattributed histogram problem where our decision-theoretic postprocessing algorithm empirically outperforms previously proposed approaches. © 2015 ACM.",Decision theory; Differential privacy; Information measures; Minimax; Privacy; Utility,Data handling; Data privacy; Decision making; Query processing; Bayesian decision theory; Data processing applications; Differential privacies; Information measures; Information preservations; Minimax; Postprocessing algorithms; Utility; Decision theory
Modeling location-based user rating profiles for personalized recommendation,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927512948&doi=10.1145%2f2663356&partnerID=40&md5=95bbe5b688fac945ed3c28063a089c4a,"This article proposes LA-LDA, a location-aware probabilistic generative model that exploits location-based ratings to model user profiles and produce recommendations. Most of the existing recommendation models do not consider the spatial information of users or items; however, LA-LDA supports three classes of locationbased ratings, namely spatial user ratings for nonspatial items, nonspatial user ratings for spatial items, and spatial user ratings for spatial items. LA-LDA consists of two components, ULA-LDA and ILA-LDA, which are designed to take into account user and item location information, respectively. The component ULA-LDA explicitly incorporates and quantifies the influence from local public preferences to produce recommendations by considering user home locations, whereas the component ILA-LDA recommends items that are closer in both taste and travel distance to the querying users by capturing item co-occurrence patterns, as well as item location co-occurrence patterns. The two components of LA-LDA can be applied either separately or collectively, depending on the available types of location-based ratings. To demonstrate the applicability and flexibility of the LA-LDA model, we deploy it to both top-k recommendation and cold start recommendation scenarios. Experimental evidence on large-scale real-world data, including the data from Gowalla (a location-based social network), DoubanEvent (an event-based social network), and MovieLens (a movie recommendation system), reveal that LA-LDA models user profiles more accurately by outperforming existing recommendation models for top-k recommendation and the cold start problem. © 2015 ACM.",Cold start; Location-based services; Probabilistic generative model; Recommender system; User profile,Recommender systems; Social networking (online); Telecommunication services; Co-occurrence pattern; Cold start; Cold-start Recommendations; Generative model; Location-based social networks; Personalized recommendation; Top-K recommendations; User profile; Location based services
Use of local group information to identify communities in networks,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927515534&doi=10.1145%2f2700404&partnerID=40&md5=8ad12b4c12f8017c153144245c45b724,"The recent interest in networks has inspired a broad range of work on algorithms and techniques to characterize, identify, and extract communities from networks. Such efforts are complicated by a lack of consensus on what a ""community"" truly is, and these disagreements have led to a wide variety of mathematical formulations for describing communities. Often, these mathematical formulations, such as modularity and conductance, have been founded in the general principle that communities, like a G(n, p) graph, are ""round,"" with connections throughout the entire community, and so algorithms were developed to optimize such mathematical measures. More recently, a variety of algorithms have been developed that, rather than expecting connectivity through the entire community, seek out very small groups of well-connected nodes and then connect these groups into larger communities. In this article, we examine seven real networks, each containing external annotation that allows us to identify ""annotated communities."" A study of these annotated communities gives insight into why the second category of community detection algorithms may be more successful than the first category. We then present a flexible algorithm template that is based on the idea of joining together small sets of nodes. In this template, we first identify very small, tightly connected ""subcommunities"" of nodes, each corresponding to a single node's ""perception"" of the network around it. We then create a new network in which each node represents such a subcommunity, and then identify communities in this new network. Because each node can appear in multiple subcommunities, this method allows us to detect overlapping communities.When evaluated on real data, we show that our template outperforms many other state-of-the-art algorithms.",Communities; Social networks,Data mining; Ecosystems; Social networking (online); Community detection algorithms; In networks; Local groups; Mathematical formulation; Real networks; State-of-the-art algorithms; Algorithms
Who influenced you? Predicting retweet via social influence locality,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927509532&doi=10.1145%2f2700398&partnerID=40&md5=ba32434805307356a8dff329affd7beb,"Social influence occurs when one's opinions, emotions, or behaviors are affected by others in a social network. However, social influence takes many forms, and its underlying mechanism is still unclear. For example, how is one's behavior influenced by a group of friends who know each other and by the friends from different ego friend circles? In this article, we study the social influence problem in a large microblogging network. Particularly, we consider users' (re)tweet behaviors and focus on investigating how friends in one's ego network influence retweet behaviors. We propose a novel notion of social influence locality and develop two instantiation functions based on pairwise influence and structural diversity. The defined influence locality functions have strong predictive power.Without any additional features, we can obtain an F1-score of 71.65% for predicting users' retweet behaviors by training a logistic regression classifier based on the defined influence locality functions. We incorporate social influence locality into a factor graph model, which can further leverage the network-based correlation. Our experiments on the large microblogging network show that the model significantly improves the precision of retweet prediction. Our analysis also reveals several intriguing discoveries. For example, if you have six friends retweeting a microblog, the average likelihood that you will also retweet it strongly depends on the structure among the six friends: The likelihood will significantly drop (only 1/6 ) when the six friends do not know each other, compared with the case when the six friends know each other. © 2015 ACM.",Microblog network; Retweet prediction; Social influence; Social network,Forecasting; Social networking (online); Ego networks; Factor graphs; Logistic regression classifier; Micro-blog; Microblogging; Network-based; Social influence; Structural diversity; Economic and social effects
Unbiased characterization of node pairs over large graphs,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927520376&doi=10.1145%2f2700393&partnerID=40&md5=2a897639086d28c47e87aae85b2f00ea,"Characterizing user pair relationships is important for applications such as friend recommendation and interest targeting in online social networks (OSNs). Due to the large-scale nature of such networks, it is infeasible to enumerate all user pairs and thus sampling is used. In this article, we show that it is a great challenge for OSN service providers to characterize user pair relationships, even when they possess the complete graph topology. The reason is that when sampling techniques (i.e., uniform vertex sampling (UVS) and random walk (RW)) are naively applied, they can introduce large biases, particularly for estimating similarity distribution of user pairs with constraints like existence of mutual neighbors, which is important for applications such as identifying network homophily. Estimating statistics of user pairs is more challenging in the absence of the complete topology information, as an unbiased sampling technique like UVS is usually not allowed and exploring the OSN graph topology is expensive. To address these challenges, we present unbiased sampling methods to characterize user pair properties based on UVS and RW techniques.We carry out an evaluation of our methods to show their accuracy and efficiency. Finally, we apply our methods to three OSNs-Foursquare, Douban, and Xiami-and discover that significant homophily is present in these networks. © 2015 ACM.",Graph sampling; Homophily; Random walks; Social network,Graph theory; Importance sampling; Random processes; Sampling; Topology; Characterization of nodes; Friend recommendations; Graph samplings; Homophily; Online social networks (OSNs); Random Walk; Similarity distribution; Topology information; Social networking (online)
Universal and distinct properties of communication dynamics: How to generate realistic inter-event times,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927557489&doi=10.1145%2f2700399&partnerID=40&md5=c78a6e9afcbae3c438235f7501ae3a2f,"With the advancement of information systems, means of communications are becoming cheaper, faster, and more available. Today, millions of people carrying smartphones or tablets are able to communicate practically any time and anywhere they want. They can access their e-mails, comment on weblogs, watch and post videos and photos (as well as comment on them), and make phone calls or text messages almost ubiquitously. Given this scenario, in this article, we tackle a fundamental aspect of this new era of communication: How the time intervals between communication events behave for different technologies and means of communications. Are there universal patterns for the Inter-Event Time Distribution (IED)? How do inter-event times behave differently among particular technologies? To answer these questions, we analyzed eight different datasets from real andmodern communication data and found four well-defined patterns seen in all the eight datasets. Moreover, we propose the use of the Self-Feeding Process (SFP) to generate inter-event times between communications. The SFP is an extremely parsimonious point process that requires at most two parameters and is able to generate inter-event times with all the universal properties we observed in the data. We also show three potential applications of the SFP: as a framework to generate a synthetic dataset containing realistic communication events of any one of the analyzed means of communications, as a technique to detect anomalies, and as a building block for more specific models that aim to encompass the particularities seen in each of the analyzed systems.",Communication dynamics; Generative model; Inter-event times,Building blockes; Communication data; Generative model; Inter-event times; Realistic communication; Time distribution; Universal patterns; Universal properties; Data mining
Pairwised specific distance learning from physical linkages,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927520753&doi=10.1145%2f2700405&partnerID=40&md5=2ee2cbc72f20506af1257524fa14a1a5,"In real tasks, usually a good classification performance can only be obtained when a good distance metric is obtained; therefore, distance metric learning has attracted significant attention in the past few years. Typical studies of distance metric learning evaluate how to construct an appropriate distance metric that is able to separate training data points from different classes or satisfy a set of constraints (e.g., must-links and/or cannot-links). It is noteworthy that this task becomes challenging when there are only limited labeled training data points and no constraints are given explicitly.Moreover,most existing approaches aim to construct a global distance metric that is applicable to all data points. However, different data points may have different properties and may require different distance metrics. We notice that data points in real tasks are often connected by physical links (e.g., people are linked with each other in social networks; personal webpages are often connected to other webpages, including nonpersonal webpages), but the linkage information has not been exploited in distance metric learning. In this article, we develop a pairwised specific distance (PSD) approach that exploits the structures of physical linkages and in particular captures the key observations that nonmetric and clique linkages imply the appearance of different or unique semantics, respectively. It is noteworthy that, rather than generating a global distance, PSD generates different distances for different pairs of data points; this property is desired in applications involving complicated data semantics. We mainly present PSD for multi-class learning and further extend it to multi-label learning. Experimental results validate the effectiveness of PSD, especially in the scenarios in which there are very limited labeled training data points and no explicit constraints are given. © 2015 ACM.",Distance metric learning; Multi-class learning; Multi-label learning; Nonmetric linkage; Physical linkages; Unlabeled data,Learning algorithms; Learning systems; Semantics; Websites; Distance Metric Learning; Multi-class learning; Multi-label learning; Non-Metric; Unlabeled data; Distance education
On data publishing with clustering preservation,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927509664&doi=10.1145%2f2700403&partnerID=40&md5=e2232dbd9b5759c44d5de9f346021c07,"The emergence of cloud-based storage services is opening up new avenues in data exchange and data dissemination. This has amplified the interest in right-protection mechanisms to establish ownership in the event of data leakage. Current right-protection technologies, however, rarely provide strong guarantees on dataset utility after the protection process. This work presents techniques that explicitly address this topic and provably preserve the outcome of certain mining operations. In particular, we take special care to guarantee that the outcome of hierarchical clustering operations remains the same before and after right protection. Our approach considers all prevalent hierarchical clustering variants: single-, complete-, and average-linkage. We imprint the ownership in a dataset using watermarking principles, and we derive tight bounds on the expansion/contraction of distances incurred by the process.We leverage our analysis to design fast algorithms for right protection without exhaustively searching the vast design space. Finally, because the right-protection process introduces a user-tunable distortion on the dataset, we explore the possibility of using this mechanism for data obfuscation. We quantify the tradeoff between obfuscation and utility for spatiotemporal datasets and discover very favorable characteristics of the process. An additional advantage is that when one is interested in both right-protecting and obfuscating the original data values, the proposed mechanism can accomplish both tasks simultaneously. © 2015 ACM.",Distance-Based Mining; Distortion Estimation; Restricted Isometry Property; Watermarking,Data mining; Data privacy; Digital watermarking; Electronic data interchange; Matrix algebra; Data dissemination; Distance-based; Distortion estimation; Hier-archical clustering; Protection mechanisms; Protection technologies; Restricted isometry properties; Spatiotemporal datasets; Digital storage
Hierarchical bayesian inference and recursive regularization for large-scale classification,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929156906&doi=10.1145%2f2629585&partnerID=40&md5=4219efde9b2b8c8bea3647fc5a1971cd,"In this article, we address open challenges in large-scale classification, focusing on how to effectively leverage the dependency structures (hierarchical or graphical) among class labels, and how to make the inference scalable in jointly optimizing all model parameters. We propose two main approaches, namely the hierarchical Bayesian inference framework and the recursive regularization scheme. The key idea in both approaches is to reinforce the similarity among parameter across the nodes in a hierarchy or network based on the proximity and connectivity of the nodes. For scalability, we develop hierarchical variational inference algorithms and fast dual coordinate descent training procedures with parallelization. In our experiments for classification problems with hundreds of thousands of classes and millions of training instances with terabytes of parameters, the proposed methods show consistent and statistically significant improvements over other competing approaches, and the best results on multiple benchmark datasets for large-scale classification. © 2015 ACM.",Bayesian methods; Hierarchical classification; Large-scale optimization,Bayesian networks; Classification (of information); Bayesian methods; Dependency structures; Hierarchical bayesian; Hierarchical classification; Large scale classifications; Large-scale optimization; Regularization schemes; Variational inference; Inference engines
Introduction to the special issue ACM SIGKDD 2013,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923649730&doi=10.1145%2f2700993&partnerID=40&md5=00bddf2561e4a5fbe594dc5c6e11636f,[No abstract available],,
A framework of mining trajectories from untrustworthy data in cyber-physical system,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923698501&doi=10.1145%2f2700394&partnerID=40&md5=58bc0f3b9259278a9ce5839af8a83b44,"A cyber-physical system (CPS) integrates physical (i.e., sensor) devices with cyber (i.e., informational) components to form a context-sensitive system that responds intelligently to dynamic changes in real-world situations. The CPS has wide applications in scenarios such as environment monitoring, battlefield surveillance, and traffic control. One key research problem of CPS is called mining lines in the sand. With a large number of sensors (sand) deployed in a designated area, the CPS is required to discover all trajectories (lines) of passing intruders in real time. There are two crucial challenges that need to be addressed: (1) the collected sensor data are not trustworthy, and (2) the intruders do not send out any identification information. The system needs to distinguish multiple intruders and track their movements. This study proposes a method called LiSM (Line-in-the-Sand Miner) to discover trajectories from untrustworthy sensor data. LiSM constructs a watching network from sensor data and computes the locations of intruder appearances based on the link information of the network. The system retrieves a cone model from the historical trajectories to track multiple intruders. Finally, the system validates the mining results and updates sensors' reliability scores in a feedback process. In addition, LoRM (Line-on-the-Road Miner) is proposed for trajectory discovery on road networks-mining lines on the roads. LoRM employs a filtering-and-refinement framework to reduce the distance computational overhead on road networks and uses a shortest-path-measure to track intruders. The proposed methods are evaluated with extensive experiments on big datasets. The experimental results show that the proposed methods achieve higher accuracy and efficiency in trajectory mining tasks. © 2015 ACM.",Cyber-physical system; Sensor network; Trajectory,Embedded systems; Miners; Roads and streets; Sensor networks; Traffic control; Trajectories; Battlefield surveillance; Computational overheads; Context sensitive; Cyber-physical systems (CPS); Environment monitoring; Link informations; Real world situations; Trajectory minings; Cyber Physical System
A space-efficient streaming algorithm for estimating transitivity and triangle counts using the birthday paradox,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923684324&doi=10.1145%2f2700395&partnerID=40&md5=52658bec8ca7d32af9b668283ae17b15,"We design a space-efficient algorithm that approximates the transitivity (global clustering coefficient) and total triangle count with only a single pass through a graph given as a stream of edges. Our procedure is based on the classic probabilistic result, the birthday paradox. When the transitivity is constant and there are more edges than wedges (common properties for social networks), we can prove that our algorithm requires O(√n) space (n is the number of vertices) to provide accurate estimates. We run a detailed set of experiments on a variety of real graphs and demonstrate that the memory requirement of the algorithm is a tiny fraction of the graph. For example, even for a graph with 200 million edges, our algorithm stores just 40,000 edges to give accurate results. Being a single pass streaming algorithm, our procedure also maintains a real-time estimate of the transitivity/number of triangles of a graph by storing a minuscule fraction of edges. © 2015 ACM.",Birthday paradox; Clustering coefficient; Streaming algorithms; Streaming graphs; Transitivity; Triangle counting,Graph algorithms; Birthday paradoxes; Clustering coefficient; Streaming algorithm; Transitivity; Triangle counting; Clustering algorithms
Querying discriminative and representative samples for batch mode active learning,2015,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923696228&doi=10.1145%2f2700408&partnerID=40&md5=48ddbfbd4e7cbb984d1df97dbec44ee0,"Empirical risk minimization (ERM) provides a principled guideline for many machine learning and data mining algorithms. Under the ERM principle, one minimizes an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, ERM requires that the training data are i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where one selects the most informative samples to label, and these data may not follow the source distribution. In this article, we generalize the ERM principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound,we develop a practical batch mode active learning method. The proposed formulation involves a nonconvex integer programming optimization problem. We solve it efficiently by an alternating optimization method. Our method is shown to query the most informative samples while preserving the source distribution as much as possible, thus identifying the most uncertain and representative queries. We further extend our method to multiclass active learning by introducing novel pseudolabels in the multiclass case and developing an efficient algorithm. Experiments on benchmark datasets and real-world applications demonstrate the superior performance of our proposed method compared to state-of-the-art methods. © 2015 ACM.",Active learning; Empirical risk minimization; Maximum mean discrepancy; Representative and discriminative,Benchmarking; Data mining; Integer programming; Learning algorithms; Active Learning; Alternating optimizations; Batch mode active learning; Empirical risk minimization; Maximum mean discrepancy; Representative and discriminative; Representative sample; State-of-the-art methods; Machine learning
MDL4BMF: Minimum description length for boolean matrix factorization,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908225464&doi=10.1145%2f2601437&partnerID=40&md5=65c874b26e145fdb4d8a2bea985dab31,"Matrix factorizations-where a given data matrix is approximated by a product of two or more factor matrices-are powerful data mining tools. Among other tasks, matrix factorizations are often used to separate global structure from noise. This, however, requires solving the ""model order selection problem"" of determining the proper rank of the factorization, that is, to answer where fine-grained structure stops, and where noise starts. Boolean Matrix Factorization (BMF)-where data, factors, and matrix product are Boolean-has in recent years received increased attention from the data mining community. The technique has desirable properties, such as high interpretability and natural sparsity. Yet, so far no method for selecting the correct model order for BMF has been available. In this article, we propose the use of the Minimum Description Length (MDL) principle for this task. Besides solving the problem, this well-founded approach has numerous benefits; for example, it is automatic, does not require a likelihood function, is fast, and, as experiments show, is highly accurate. We formulate the description length function for BMF in general-making it applicable for any BMF algorithm. We discuss how to construct an appropriate encoding: starting from a simple and intuitive approach, we arrive at a highly efficient data-to-model-based encoding for BMF. We extend an existing algorithm for BMF to use MDL to identify the best Boolean matrix factorization, analyze the complexity of the problem, and perform an extensive experimental evaluation to study its behavior.",Boolean matrix factorization; Boolean rank; MDL; Minimum description length principle; Model order selection; Model selection; Parameter free; Pattern sets; Summarization,Data mining; Encoding (symbols); Factorization; Natural language processing systems; Signal encoding; Boolean Matrix; Boolean rank; Minimum description length principle; Model Selection; Model-order selection; Parameter free; Pattern set; Summarization; Matrix algebra
Probabilistic reframing for cost-sensitive regression,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908253871&doi=10.1145%2f2641758&partnerID=40&md5=471c558a1d9f24822cf91b74031d2018,"Common-day applications of predictive models usually involve the full use of the available contextual information. When the operating context changes, one may fine-tune the by-default (incontextual) prediction or may even abstain from predicting a value (a reject). Global reframing solutions, where the same function is applied to adapt the estimated outputs to a new cost context, are possible solutions here. An alternative approach, which has not been studied in a comprehensive way for regression in the knowledge discovery and data mining literature, is the use of a local (e.g., probabilistic) reframing approach, where decisions are made according to the estimated output and a reliability, confidence, or probability estimation. In this article, we advocate for a simple two-parameter (mean and variance) approach, working with a normal conditional probability density. Given the conditional mean produced by any regression technique, we develop lightweight ""enrichment"" methods that produce good estimates of the conditional variance, which are used by the probabilistic (local) reframingmethods.We apply these methods to some very common families of costsensitive problems, such as optimal predictions in (auction) bids, asymmetric loss scenarios, and rejection rules.",asymmetric loss; calibration; conditional density estimation; Cost-sensitive regression; reframing; reliability estimation in regression,Calibration; Data mining; Forecasting; Predictive analytics; Conditional density; Conditional probability density; Contextual information; Cost-sensitive; Knowledge discovery and data minings; Probability estimation; Reframing; Reliability estimation; Cost estimating
Discovering social circles in directed graphs,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908238072&doi=10.1145%2f2641759&partnerID=40&md5=855ff0cb8d52e764809268e4f0308651,"We examine the problem of identifying social circles, or sets of cohesive and mutually aware nodes surrounding an initial query set, in directed graphs where the complete graph is not known beforehand. This problem differs from local community mining, in that the query set defines the circle of interest. We explicitly handle edge direction, as in many cases relationships are not symmetric, and focus on the local context because many real-world graphs cannot be feasibly known. We outline several issues that are unique to this context, introduce a quality function to measure the value of including a particular node in an emerging social circle, and describe a greedy social circle discovery algorithm. We demonstrate the effectiveness of this approach on artificial benchmarks, large networks with topical community labels, and several real-world case studies.",Directed graphs; Local community search; Social circles,Graphic methods; Community labels; Complete graphs; Discovery algorithm; Edge direction; Local community; Local contexts; Real-world graphs; Social circles; Directed graphs
Random projections for linear support vector machines,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908246259&doi=10.1145%2f2641760&partnerID=40&md5=3665807cb9a0febbe06bc450dc2964fe,"Let X be a data matrix of rank p, whose rows represent n points in d-dimensional space. The linear support vector machine constructs a hyperplane separator that maximizes the 1-norm soft margin. We develop a new oblivious dimension reduction technique that is precomputed and can be applied to any input matrix X. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within ε-relative error, ensuring comparable generalization as in the original space in the case of classification. For regression, we show that the margin is preserved to -relative error with high probability. We present extensive experiments with real and synthetic data to support our theory.",Classification; dimensionality reduction; support vector machines,Classification (of information); Dimensionality reduction; Matrix algebra; D-dimensional spaces; Dimension reduction techniques; High probability; Linear Support Vector Machines; Minimum enclosing ball; Random projections; Relative errors; Synthetic data; Support vector machines
Feature selection for social media data,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908237935&doi=10.1145%2f2629587&partnerID=40&md5=fa2b58d9e293448d8723759bc48443d4,"Feature selection is widely used in preparing high-dimensional data for effective data mining. The explosive popularity of social media producesmassive and high-dimensional data at an unprecedented rate, presenting new challenges to feature selection. Social media data consists of (1) traditional high-dimensional, attributevalue data such as posts, tweets, comments, and images, and (2) linked data that provides social context for posts and describes the relationships between social media users as well as who generates the posts, and so on. The nature of social media also determines that its data is massive, noisy, and incomplete, which exacerbates the already challenging problem of feature selection. In this article, we study a novel feature selection problem of selecting features for social media data with its social context. In detail, we illustrate the differences between attribute-value data and social media data, investigate if linked data can be exploited in a new feature selection framework by taking advantage of social science theories. We design and conduct experiments on datasets from real-world social media Web sites, and the empirical results demonstrate that the proposed framework can significantly improve the performance of feature selection. Further experiments are conducted to evaluate the effects of user-user and user-post relationships manifested in linked data on feature selection, and research issues for future work will be discussed.",Feature selection; social context; social media data,Clustering algorithms; Data mining; Linked data; Social networking (online); Attribute values; Feature selection problem; High dimensional data; High-dimensional; Selection framework; Social context; Social media datum; Social science theory; Feature extraction
Reconstructing graphs from neighborhood data,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908248408&doi=10.1145%2f2641761&partnerID=40&md5=c31ec2925632a67b881841395e895f01,"Consider a social network and suppose that we are only given the number of common friends between each pair of users. Can we reconstruct the underlying network? Similarly, consider a set of documents and the words that appear in them. If we only know the number of common words for every pair of documents, as well as the number of common documents for every pair of words, can we infer which words appear in which documents? In this article, we develop a general methodology for answering questions like these. We formalize these questions in what we call the RECONSTRUCT problem: given information about the common neighbors of nodes in a network, our goal is to reconstruct the hidden binary matrix that indicates the presence or absence of relationships between individual nodes. In fact, we propose two different variants of this problem: one where the number of connections of every node (i.e., the degree of every node) is known and a second one where it is unknown. We call these variants the degree-aware and the degree-oblivious versions of the RECONSTRUCT problem, respectively. Our algorithms for both variants exploit the properties of the singular value decomposition of the hidden binary matrix. More specifically, we show that using the available neighborhood information, we can reconstruct the hidden matrix by finding the components of its singular value decomposition and then combining them appropriately. Our extensive experimental study suggests that our methods are able to reconstruct binary matrices of different characteristics with up to 100% accuracy.",Adjacency matrix; Bipartite graph reconstruction; Singular value decomposition,Computer science; Data mining; Adjacency matrices; Binary matrix; Bipartite graphs; General methodologies; Hidden matrix; Neighborhood information; Underlying networks; Singular value decomposition
Uncovering hierarchical and overlapping communities with a local-first approach,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908150669&doi=10.1145%2f2629511&partnerID=40&md5=2f49df2ef563c0ca88b5c86965af88da,"Community discovery in complex networks is the task of organizing a network's structure by grouping together nodes related to each other. Traditional approaches are based on the assumption that there is a global-level organization in the network. However, in many scenarios, each node is the bearer of complex information and cannot be classified in disjoint clusters. The top-down global view of the partition approach is not designed for this. Here, we represent this complex information as multiple latent labels, and we postulate that edges in the networks are created among nodes carrying similar labels. The latent labels are the communities a node belongs to and we discover them with a simple local-first approach to community discovery. This is achieved by democratically letting each node vote for the communities it sees surrounding it in its limited view of the global system, its ego neighborhood, using a label propagation algorithm, assuming that each node is aware of the label it shares with each of its connections. The local communities are merged hierarchically, unveiling the modular organization of the network at the global level and identifying overlapping groups and groups of groups. We tested this intuition against the state-of-the-art overlapping community discovery and found that our new method advances in the chosen scenarios in the quality of the obtained communities. We perform a test on benchmark and on real-world networks, evaluating the quality of the community coverage by using the extracted communities to predict the metadata attached to the nodes, which we consider external information about the latent labels. We also provide an explanation about why real-world networks contain overlapping communities and how our logic is able to capture them. Finally, we show how our method is deterministic, is incremental, and has a limited time complexity, so that it can be used on real-world scale networks. © 2014 ACM.",Community discovery; Complex networks; Data mining,Benchmarking; Data mining; Partial discharges; Community discoveries; Complex information; External informations; Modular organizations; Overlapping communities; Overlapping groups; Real-world networks; Traditional approaches; Complex networks
An optimization framework for combining ensembles of classifiers and clusterers with applications to nontransductive semisupervised learning and transfer learning,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908216012&doi=10.1145%2f2601435&partnerID=40&md5=83e9fbb5185a6630608ef5bb121d66e8,"Unsupervised models can provide supplementary soft constraints to help classify new ""target"" data because similar instances in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place, as in transfer learning settings. This article describes a general optimization framework that takes as input class membership estimates from existing classifiers learned on previously encountered ""source"" (or training) data, as well as a similarity matrix from a cluster ensemble operating solely on the target (or test) data to be classified, and yields a consensus labeling of the target data. More precisely, the application settings considered are nontransductive semisupervised and transfer learning scenarios where the training data are used only to build an ensemble of classifiers and are subsequently discarded before classifying the target data. The framework admits a wide range of loss functions and classification/ clustering methods. It exploits properties of Bregman divergences in conjunction with Legendre duality to yield a principled and scalable approach. A variety of experiments show that the proposed framework can yield results substantially superior to those provided by näively applying classifiers learned on the original task to the target data. In addition, we show that the proposed approach, even not being conceptually transductive, can provide better results compared to some popular transductive learning techniques. © 2014 ACM.",Classification; Clustering; Ensembles; Semisupervised learning; Transductive learning; Transfer learning,Learning systems; Semi-supervised learning; Transfer learning; Bregman divergences; Clustering; Ensemble of classifiers; Ensembles; Ensembles of classifiers; General optimizations; Optimization framework; Transductive learning; Classification (of information)
Bayesian variable selection in linear regression in one pass for large datasets,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908154088&doi=10.1145%2f2629617&partnerID=40&md5=5161a2d455c187d39e26613e9d42bfee,"Bayesian models are generally computed with Markov Chain Monte Carlo (MCMC) methods. The main disadvantage of MCMC methods is the large number of iterations they need to sample the posterior distributions of model parameters, especially for large datasets. On the other hand, variable selection remains a challenging problem due to its combinatorial search space, where Bayesian models are a promising solution. In this work, we study how to accelerate Bayesian model computation for variable selection in linear regression. We propose a fast Gibbs sampler algorithm, a widely used MCMC method that incorporates several optimizations. We use a Zellner prior for the regression coefficients, an improper prior on variance, and a conjugate prior Gaussian distribution, which enable dataset summarization in one pass, thus exploiting an augmented set of sufficient statistics. Thereafter, the algorithm iterates inmainmemory. Sufficient statistics are indexed with a sparse binary vector to efficiently compute matrix projctions based on selected variables. Discovered variable subsets probabilities, selecting and discarding each variable, are stored on a hash table for fast retrieval in future iterations. We study how to integrate our algorithm into a Database Management System (DBMS), exploiting aggregate User-Defined Functions for parallel data summarization and stored procedures to manipulate matrices with arrays. An experimental evaluation with real datasets evaluates accuracy and time performance, comparing our DBMS-based algorithm with the R package. Our algorithm is shown to produce accurate results, scale linearly on dataset size, and run orders of magnitude faster than the R package. © 2014 ACM.",Gibbs sampler; MCMC; On-line algorithm; Sufficient statistics; Variable selection,Bayesian networks; Database systems; Genetic algorithms; Information management; Markov chains; Matrix algebra; Monte Carlo methods; Gibbs samplers; MCMC; On-line algorithms; Sufficient statistics; Variable selection; Large dataset
Anomaly detection from incomplete data,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907536998&doi=10.1145%2f2629668&partnerID=40&md5=169c90b4c0dcbc20e59b9c98a9ef2ddb,"Anomaly detection (a.k.a., outlier or burst detection) is a well-motivated problem and a major data mining and knowledge discovery task. In this article, we study the problem of population anomaly detection, one of the key issues related to event monitoring and population management within a city. Through studying detected population anomalies, we can trace and analyze these anomalies, which could help to model city traffic design and event impact analysis and prediction. Although a significant and interesting issue, it is very hard to detect population anomalies and retrieve anomaly trajectories, especially given that it is difficult to get actual and sufficient population data. To address the difficulties of a lack of real population data, we take advantage of mobile phone networks, which offer enormous spatial and temporal communication data on persons. More importantly, we claim that we can utilize these mobile phone data to infer and approximate population data. Thus, we can study the population anomaly detection problem by taking advantages of unique features hidden in mobile phone data. In this article, we present a system to conduct Population Anomaly Detection (PAD). First, we propose an effective clustering method, correlation-based clustering, to cluster the incomplete location information from mobile phone data (i.e., from mobile call volume distribution to population density distribution). Then, we design an adaptive parameter-free detection method, R-scan, to capture the distributed dynamic anomalies. Finally, we devise an efficient algorithm, BT-miner, to retrieve anomaly trajectories. The experimental results from real-life mobile phone data confirm the effectiveness and efficiency of the proposed algorithms. Finally, the proposed methods are realized as a pilot system in a city in China. © 2014 ACM.",Anomaly detection; Anomaly trajectory; Correlation-based clustering; Mobile phone,Cellular telephones; Cellular telephones; Data mining; Population statistics; Trajectories; Correlation based clustering; Data mining and knowledge discovery; Distributed dynamics; Effectiveness and efficiencies; Mobile phone networks; Population density distribution; Population management; Volume distributions; Anomaly detection
User vulnerability and its reduction on a social networking site,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907502828&doi=10.1145%2f2630421&partnerID=40&md5=dc88f65da6661033ec89a89bbdfa8703,"Privacy and security are major concerns for many users of social media. When users share information (e.g., data and photos) with friends, they can make their friends vulnerable to security and privacy breaches with dire consequences. With the continuous expansion of a user's social network, privacy settings alone are often inadequate to protect a user's profile. In this research, we aim to address some critical issues related to privacy protection: (1) How can we measure and assess individual users' vulnerability? (2) With the diversity of one's social network friends, how can one figure out an effective approach to maintaining balance between vulnerability and social utility? In this work, first we present a novel way to define vulnerable friends from an individual use's perspective. User vulnerability is dependent on whether or not the use's friends' privacy settings protect the friend and the individual's network of friends (which includes the user). We show that it is feasible to measure and assess user vulnerability and reduce one's vulnerability without changing the structure of a social networking site. The approach is to unfriend one's most vulnerable friends. However, when such a vulnerable friend is also socially important, unfriending him or her would significantly reduce one's own social status. We formulate this novel problem as vulnerability minimization with social utility constraints. We formally define the optimization problem and provide an approximation algorithm with a proven bound. Finally, we conduct a large-scale evaluation of a new framework using a Facebook dataset. We resort to experiments and observe how much vulnerability an individual user can be decreased by unfriending a vulnerable friend. We compare performance of different unfriending strategies and discuss the security risk of new friend requests. Additionally, by employing different forms of social utility, we confirm that the balance between user vulnerability and social utility can be practically achieved. This work is supported by grants of ARO (025071), ONR (N000141010091, N000141410095), and AFOSR (FA95500810132). This work was also funded, in part, by OSD-T&E (Office of Secretary Defense-Test and Evaluation), DefenseWide/PE0601120D8Z National Defense Education Program (NDEP)/BA-1, Basic Research; SMART Program Office, www.asee.org/fellowships/smart, Grant Number N00244-09-1-0081. The majority of this work was conducted when Geoffrey Barbier was affiliated with the Computer Science Denartment at Arizona Sate University. © 2014 ACM.",Privacy; Social network; Vulnerability,Approximation algorithms; Data privacy; Large dataset; Social networking (online); Software testing; Effective approaches; Optimization problems; Privacy and security; Privacy protection; Security and privacy; Social networking sites; Test and evaluation; Vulnerability; Network security
A generic multilabel learning-based classification algorithm recommendation method,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908151901&doi=10.1145%2f2629474&partnerID=40&md5=5b2f983dd6c704c4f9fd278c920a6b89,"As more and more classification algorithms continue to be developed, recommending appropriate algorithms to a given classification problem is increasingly important. This article first distinguishes the algorithm recommendationmethods by two dimensions: (1)meta-features,which are a set ofmeasures used to characterize the learning problems, and (2) meta-target, which represents the relative performance of the classification algorithms on the learning problem. In contrast to the existing algorithm recommendation methods whose meta-target is usually in the form of either the ranking of candidate algorithms or a single algorithm, this article proposes a new and natural multilabel form to describe the meta-target. This is due to the fact that there would be multiple algorithms being appropriate for a given problem in practice. Furthermore, a novel multilabel learning-based generic algorithm recommendation method is proposed, which views the algorithm recommendation as a multilabel learning problem and solves the problem by the mature multilabel learning algorithms. To evaluate the proposed multilabel learning-based recommendation method, extensive experiments with 13 well-known classification algorithms, two kinds of meta-targets such as algorithm ranking and single algorithm, and five different kinds of meta-features are conducted on 1,090 benchmark learning problems. The results show the effectiveness of our proposed multilabel learning-based recommendation method. © 2014 ACM.",Algorithm automatic recommendation; Multilabel classification; Multilabel k nearest neighbors; Multiple comparison procedure,Computer programming; Learning systems; Nearest neighbor search; Classification algorithm; Multi-label classifications; Multi-label K-nearest neighbors; Multi-label learning; Multiple algorithms; Multiple-comparison procedures; Recommendation methods; Relative performance; Learning algorithms
GBAGC: A general bayesian framework for attributed graph clustering,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908165238&doi=10.1145%2f2629616&partnerID=40&md5=e0c3598426632ca98f20e09e093d0329,"Graph clustering, also known as community detection, is a long-standing problem in data mining. In recent years, with the proliferation of rich attribute information available for objects in real-world graphs, how to leverage not only structural but also attribute information for clustering attributed graphs becomes a new challenge. Most existing works took a distance-based approach. They proposed various distance measures to fuse structural and attribute information and then applied standard techniques for graph clustering based on these distance measures. In this article, we take an alternative view and propose a novel Bayesian framework for attributed graph clustering. Our framework provides a general and principled solution to modeling both the structural and the attribute aspects of a graph. It avoids the artificial design of a distance measure in existing methods and, furthermore, can seamlessly handle graphs with different types of edges and vertex attributes. We develop an efficient variational method for graph clustering under this framework and derive two concrete algorithms for clustering unweighted and weighted attributed graphs. Experimental results on large real-world datasets show that our algorithms significantly outperform the state-of-the-art distance-based method, in terms of both effectiveness and efficiency. © 2014 ACM.",Attributed graph clustering; Bayesian method; Model-based clustering,Bayesian networks; Clustering algorithms; Data mining; Graphic methods; Large dataset; Attribute information; Attributed graph clustering; Bayesian methods; Community detection; Distance-based methods; Effectiveness and efficiencies; Model-based clustering; Variational methods; Graph algorithms
Physics-based anomaly detection defined on manifold space,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907512451&doi=10.1145%2f2641574&partnerID=40&md5=f9e762c4d6aa4ec58698601bb30c148f,"Current popular anomaly detection algorithms are capable of detecting global anomalies but often fail to distinguish local anomalies from normal instances. Inspired by contemporary physics theory (i.e., heat diffusion and quantum mechanics), we propose two unsupervised anomaly detection algorithms. Building on the embedding manifold derived from heat diffusion, we devise Local Anomaly Descriptor (LAD), which faithfully reveals the intrinsic neighborhood density. It uses a scale-dependent umbrella operator to bridge global and local properties, which makes LAD more informative within an adaptive scope of neighborhood. To offer more stability of local density measurement on scaling parameter tuning, we formulate Fermi Density Descriptor (FDD), which measures the probability of a fermion particle being at a specific location. By choosing the stable energy distribution function, FDD steadily distinguishes anomalies from normal instances with any scaling parameter setting. To further enhance the efficacy of our proposed algorithms, we explore the utility of anisotropic Gaussian kernel (AGK), which offers better manifold-aware affinity information. We also quantify and examine the effect of different Laplacian normalizations for anomaly detection. Comprehensive experiments on both synthetic and benchmark datasets verify that our proposed algorithms outperform the existing anomaly detection algorithms. © 2014 ACM.",Anomaly detection; Heat diffusion; Laplace operator; Quantum mechanics,Diffusion; Distribution functions; Laplace transforms; Quantum theory; Signal detection; Anomaly-detection algorithms; Benchmark datasets; Contemporary physics; Energy distribution functions; Heat diffusions; Laplace operator; Specific location; Unsupervised anomaly detection; Anomaly detection
Efficiently estimating motif statistics of large networks,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907537343&doi=10.1145%2f2629564&partnerID=40&md5=1a620ee43731b7adc528b276bb9fe009,"Exploring statistics of locally connected subgraph patterns (also known as network motifs) has helped researchers better understand the structure and function of biological and Online Social Networks (OSNs). Nowadays, the massive size of some critical networks -often stored in already overloaded relational databases -effectively limits the rate at which nodes and edges can be explored, making it a challenge to accurately discover subgraph statistics. In this work, we propose sampling methods to accurately estimate subgraph statistics from as few queried nodes as possible. We present sampling algorithms that efficiently and accurately estimate subgraph properties of massive networks. Our algorithms require no precomputation or complete network topology information. At the same time, we provide theoretical guarantees of convergence. We perform experiments using widely known datasets and show that, for the same accuracy, our algorithms require an order of magnitude less queries (samples) than the current state-of-the-art algorithms. © 2014 ACM.",Graph sampling; Network motifs; Random walks; Social network; Subgraph patterns,Graph theory; Social networking (online); Graph samplings; Network motif; Online social networks (OSNs); Random Walk; State-of-the-art algorithms; Subgraph statistics; Subgraphs; Theoretical guarantees; Sampling
Structured sparse boosting for graph classification,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908216847&doi=10.1145%2f2629328&partnerID=40&md5=8414351c82f6594f724d5264e580e10b,"Boosting is a highly effective algorithm that produces a linear combination of weak classifiers (a.k.a. base learners) to obtain high-quality classification models. In this article, we propose a generalized logit boost algorithm in which base learners have structural relationships in the functional space. Although such relationships are generic, our work is particularlymotivated by the emerging topic of pattern-based classification for semistructured data including graphs. Toward an efficient incorporation of the structure information, we have designed a general model in which we use an undirected graph to capture the relationship of subgraphbased base learners. In our method, we employ both L1 and Laplacian-based L2 regularization to logit boosting to achieve model sparsity and smoothness in the functional space spanned by the base learners. We have derived efficient optimization algorithms based on coordinate descent for the new boosting formulation and theoretically prove that it exhibits a natural grouping effect for nearby spatial or overlapping base learners and that the resulting estimator is consistent. Additionally, motivated by the connection between logit boosting and logistic regression, we extend our structured sparse regularization framework to logistic regression for vectorial data in which features are structured. Using comprehensive experimental study and comparing our work with the state-of-the-art, we have demonstrated the effectiveness of the proposed learning method. © 2014 ACM.",Boosting; Feature selection; Graph classification; Logistic regression; Regularization; Semistructured data; Structural sparsity,Classification (of information); Feature extraction; Learning systems; Boosting; Graph classification; Regularization; Semi structured data; Structural sparsity; Logistic regression
Selecting the right correlation measure for binary data,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907509019&doi=10.1145%2f2637484&partnerID=40&md5=782d556a80ef337b5c75c8736849a5de,"Finding the most interesting correlations among items is essential for problems in many commercial, medical, and scientific domains. Although there are numerous measures available for evaluating correlations, different correlation measures provide drastically different results. Piatetsky-Shapiro provided three mandatory properties for any reasonable correlation measure, and Tan et al. proposed several properties to categorize correlation measures; however, it is still hard for users to choose the desirable correlation measures according to their needs. In order to solve this problem, we explore the effectiveness problem in three ways. First, we propose two desirable properties and two optional properties for correlation measure selection and study the property satisfaction for different correlation measures. Second, we study different techniques to adjust correlation measures and propose two new correlation measures: the Simplified x2 with Continuity Correction and the Simplified x2 with Support. Third, we analyze the upper and lower bounds of different measures and categorize them by the bound differences. Combining these three directions, we provide guidelines for users to choose the proper measure according to their needs. © 2014 ACM.",Association rules; Correlation; Knowledge discovery,Association rules; Computer science; Correlation methods; Data mining; Binary data; Continuity corrections; Correlation measures; New correlations; Property satisfaction; Upper and lower bounds; Medical problems
Toward personalized context recognition for mobile users: A semisupervised bayesian HMM approach,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907555913&doi=10.1145%2f2629504&partnerID=40&md5=ab6893a68137819b61df4945f5aaf5e5,"The problem of mobile context recognition targets the identification of semantic meaning of context in a mobile environment. This plays an important role in understanding mobile user behaviors and thus provides the opportunity for the development of better intelligent context-aware services. A key step of context recognition is to model the personalized contextual information of mobile users. Although many studies have been devoted to mobile context modeling, limited efforts have been made on the exploitation of the sequential and dependency characteristics of mobile contextual information. Also, the latent semantics behind mobile context are often ambiguous and poorly understood. Indeed, a promising direction is to incorporate some domain knowledge of common contexts, such as ""waiting for a bus"" or ""having dinner,"" by modeling both labeled and unlabeled context data from mobile users because there are often few labeled contexts available in practice. To this end, in this article, we propose a sequence-based semisupervised approach to modeling personalized context for mobile users. Specifically, we first exploit the Bayesian Hidden Markov Model (BHMM) for modeling context in the form of probabilistic distributions and transitions of raw context data. Also, we propose a sequential model by extending B-HMM with the prior knowledge of contextual features to model context more accurately. Then, to efficiently learn the parameters and initial values of the proposed models, we develop a novel approach for parameter estimation by integrating the Dirichlet Process Mixture (DPM) model and the Mixture Unigram (MU) model. Furthermore, by incorporating both user-labeled and unlabeled data, we propose a semisupervised learning-based algorithm to identify and model the latent semantics of context. Finally, experimental results on real-world data clearly validate both the efficiency and effectiveness of the proposed approaches for recognizing personalized context of mobile users. © 2014 ACM.",Context recognition; Hidden markov model,Hidden Markov models; Information services; Learning algorithms; Mixtures; Probability distributions; Semantics; Context aware services; Context recognition; Contextual feature; Contextual information; Dirichlet process mixtures (DPM); Labeled and unlabeled data; Mobile environments; Probabilistic distribution; Behavioral research
A framework for hierarchical ensemble clustering,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907515134&doi=10.1145%2f2611380&partnerID=40&md5=566237b55d051af79987961874ebf8fa,"Ensemble clustering, as an important extension of the clustering problem, refers to the problem of combining different (input) clusterings of a given dataset to generate a final (consensus) clustering that is a better fit in some sense than existing clusterings. Over the past few years, many ensemble clustering approaches have been developed. However, most of them are designed for partitional clustering methods, and few research efforts have been reported for ensemble hierarchical clustering methods. In this article, a hierarchical ensemble clustering framework that can naturally combine both partitional clustering and hierarchical clustering results is proposed. In addition, a novel method for learning the ultra-metric distance from the aggregated distance matrices and generating final hierarchical clustering with enhanced cluster separation is developed based on the ultra-metric distance for hierarchical clustering. We study three important problems: dendrogram description, dendrogram combination, and dendrogram selection. We develop two approaches for dendrogram selection based on tree distances, and we investigate various dendrogram distances for representing dendrograms. We provide a systematic empirical study of the ensemble hierarchical clustering problem. Experimental results demonstrate the effectiveness of our proposed approaches. © 2014 ACM.",Ensemble selection; Hierarchical ensemble clustering; Ultra-metric,Clustering algorithms; Aggregated distances; Clustering problems; Ensemble clustering; Ensemble selections; Hierarchical clustering methods; Hierarchical ensemble; Partitional clustering; Ultra-metric; Hierarchical clustering
A framework for exploiting local information to enhance density estimation of data streams,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908205316&doi=10.1145%2f2629618&partnerID=40&md5=2fae2c8d39cc58430c4711dbbe4a79f3,"The Probability Density Function (PDF) is the fundamental data model for a variety of stream mining algorithms. Existing works apply the standard nonparametric Kernel Density Estimator (KDE) to approximate the PDF of data streams. As a result, the stream-based KDEs cannot accurately capture complex local density features. In this article, we propose the use of Local Region (LRs) to model local density information in univariate data streams. In-depth theoretical analyses are presented to justify the effectiveness of the LR-based KDE. Based on the analyses, we develop the General Local rEgion AlgorithM (GLEAM) to enhance the estimation quality of structurally complex univariate distributions for existing stream-based KDEs. A set of algorithmic optimizations is designed to improve the query throughput of GLEAM and to achieve its linear order computation. Additionally, a comprehensive suite of experiments was conducted to test the effectiveness and efficiency of GLEAM. © 2014 ACM.",General Local rEgion AlgorithM (GLEAM); Local region information,Probability density function; Algorithmic optimization; Density estimation; Effectiveness and efficiencies; Estimation quality; Kernel density estimators; Local region; Probability density function (pdf); Univariate distributions; Data streams
Efficient discovery of association rules and frequent itemsets through sampling with tight performance guarantees,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908233917&doi=10.1145%2f2629586&partnerID=40&md5=177f4eae8de3f064906571bc63cf9a36,"The tasks of extracting (top-K) Frequent Itemsets (FIs) and Association Rules (ARs) are fundamental primitives in data mining and database applications. Exact algorithms for these problems exist and are widely used, but their running time is hindered by the need of scanning the entire dataset, possibly multiple times. High-quality approximations of FIs and ARs are sufficient for most practical uses. Sampling techniques can be used for fast discovery of approximate solutions, but works exploring this technique did not provide satisfactory performance guarantees on the quality of the approximation due to the difficulty of bounding the probability of under- or oversampling any one of an unknown number of frequent itemsets. We circumvent this issue by applying the statistical concept of Vapnik-Chervonenkis (VC) dimension to develop a novel technique for providing tight bounds on the sample size that guarantees approximation of the (top-K) FIs and ARs within user-specified parameters. The resulting sample size is linearly dependent on the VC-dimension of a range space associated with the dataset. We analyze the VC-dimension of this range space and show that it is upper bounded by an easy-to-compute characteristic quantity of the dataset, the d-index, namely, the maximum integer d such that the dataset contains at least d transactions of length at least d such that no one of them is a superset of or equal to another. We show that this bound is tight for a large class of datasets. The resulting sample size is a significant improvement over previous known results. We present an extensive experimental evaluation of our technique on real and artificial datasets, demonstrating the practicality of our methods, and showing that they achieve even higher quality approximations than what is guaranteed by the analysis.",Association rules; Data mining; Frequent itemsets; Sampling; VCdimension,Association rules; Quality control; Sampling; Approximate solution; Database applications; Experimental evaluation; Item sets; Performance guarantees; Statistical concepts; Vapnik-Chervonenkis dimensions; VC-dimension; Data mining
Predicting and identifying missing node information in social networks,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903693170&doi=10.1145%2f2536775&partnerID=40&md5=5bde11b5df69fd35e1139d18a08d9815,"In recent years, social networks have surged in popularity. One key aspect of social network research is identifying important missing information that is not explicitly represented in the network, or is not visible to all. To date, this line of research typically focused on finding the connections that are missing between nodes, a challenge typically termed as the link prediction problem. This article introduces the missing node identification problem, where missing members in the social network structure must be identified. In this problem, indications of missing nodes are assumed to exist. Given these indications and a partial network, we must assess which indications originate from the same missing node and determine the full network structure. Toward solving this problem, we present the missing node identification by spectral clustering algorithm (MISC), an approach based on a spectral clustering algorithm, combined with nodes' pairwise affinity measures that were adopted from link prediction research. We evaluate the performance of our approach in different problem settings and scenarios, using real-life data from Facebook. The results show that our approach has beneficial results and can be effective in solving the missing node identification problem. In addition, this article also presents R-MISC, which uses a sparse matrix representation, efficient algorithms for calculating the nodes' pairwise affinity, and a proprietary dimension reduction technique to enable scaling the MISC algorithm to large networks of more than 100,000 nodes. Last, we consider problem settings where some of the indications are unknown. Two algorithms are suggested for this problem: speculative MISC, based on MISC, and missing link completion, based on classical link prediction literature. We show that speculative MISC outperforms missing link completion. © 2014 ACM.",Missing nodes; Social networks; Spectral clustering,Clustering algorithms; Forecasting; Research; Social networking (online); Dimension reduction techniques; Missing information; Missing nodes; Network structures; Node identifications; Social network structures; Spectral clustering; Spectral clustering algorithms; Problem solving
Multiresolution tensor decompositions with mode hierarchies,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903691277&doi=10.1145%2f2532169&partnerID=40&md5=c13f5da8b2ea250ae5fb3ab1840c062c,"Tensors (multidimensional arrays) are widely used for representing high-order dimensional data, in applications ranging from social networks, sensor data, and Internet traffic. Multiway data analysis techniques, in particular tensor decompositions, allow extraction of hidden correlations among multiway data and thus are key components of many data analysis frameworks. Intuitively, these algorithms can be thought of as multiway clustering schemes, which consider multiple facets of the data in identifying clusters, their weights, and contributions of each data element. Unfortunately, algorithms for fitting multiway models are, in general, iterative and very time consuming. In this article, we observe that, in many applications, there is a priori background knowledge (or metadata) about one or more domain dimensions. This metadata is often in the form of a hierarchy that clusters the elements of a given data facet (or mode). We investigate whether such single-mode data hierarchies can be used to boost the efficiency of tensor decomposition process, without significant impact on the final decomposition quality. We consider each domain hierarchy as a guide to help provide higher- or lower-resolution views of the data in the tensor on demand and we rely on these metadata-induced multiresolution tensor representations to develop a multiresolution approach to tensor decomposition. In this article, we focus on an alternating least squares (ALS)-based implementation of the two most important decomposition models such as the PARAllel FACtors (PARAFAC, which decomposes a tensor into a diagonal tensor and a set of factor matrices) and the Tucker (which produces as result a core tensor and a set of dimension-subspaces matrices). Experiment results show that, when the available metadata is used as a rough guide, the proposed multiresolution method helps fit both PARAFAC and Tucker models with consistent (under different parameters settings) savings in execution time and memory consumption, while preserving the quality of the decomposition. © 2014 ACM.",Multiresolution; PARAFAC; Tensor decomposition; Tucker,Iterative methods; Matrix algebra; Metadata; Social networking (online); Alternating least squares; Multi-resolution approach; Multi-way data analysis; Multiresolution; Multiresolution methods; PARAFAC; Tensor decomposition; Tucker; Tensors
Optimizing data misuse detection,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903694150&doi=10.1145%2f2611520&partnerID=40&md5=fa9dd8f22787deb6092f34c3a622e2fb,"Data misuse may be performed by entities such as an organization's employees and business partners who are granted access to sensitive information and misuse their privileges. We assume that users can be either trusted or untrusted. The access of untrusted parties to data objects (e.g., client and patient records) should be monitored in an attempt to detect misuse. However, monitoring data objects is resource intensive and time-consuming and may also cause disturbance or inconvenience to the involved employees. Therefore, the monitored data objects should be carefully selected. In this article, we present two optimization problems carefully designed for selecting specific data objects formonitoring, such that the detection rate is maximized and the monitoring effort is minimized. In the first optimization problem, the goal is to select data objects for monitoring that are accessed by at most c trusted agents while ensuring access to at least k monitored objects by each untrusted agent (both c and k are integer variable). As opposed to the first optimization problem, the goal of the second optimization problem is to select monitored data objects that maximize the number of monitored data objects accessed by untrusted agents while ensuring that each trusted agent does not access more than d monitored data objects (d is an integer variable as well). Two efficient heuristic algorithms for solving these optimization problems are proposed, and experiments were conducted simulating different scenarios to evaluate the algorithms' performance.Moreover, we compared the heuristic algorithms' performance to the optimal solution and conducted sensitivity analysis on the three parameters (c, k, and d) and on the ratio between the trusted and untrusted agents. © 2014 ACM.",Data misuse; Data monitoring; Honeytokens; Information security,C (programming language); Heuristic algorithms; Hierarchical systems; Integer programming; Optimization; Problem solving; Security of data; Data misuse; Data monitoring; Honeytokens; Integer variables; Optimal solutions; Optimization problems; Second optimization; Sensitive informations; Monitoring
Ranking metric anomaly in invariant networks,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903747545&doi=10.1145%2f2601436&partnerID=40&md5=3e69629ec9dd13c0774670163be6457b,"The management of large-scale distributed information systems relies on the effective use and modeling of monitoring data collected at various points in the distributed information systems. A traditional approach to model monitoring data is to discover invariant relationships among the monitoring data. Indeed, we can discover all invariant relationships among all pairs of monitoring data and generate invariant networks, where a node is a monitoring data source (metric) and a link indicates an invariant relationship between two monitoring data. Such an invariant network representation can help system experts to localize and diagnose the system faults by examining those broken invariant relationships and their related metrics, since system faults usually propagate among the monitoring data and eventually lead to some broken invariant relationships. However, at one time, there are usually a lot of broken links (invariant relationships) within an invariant network. Without proper guidance, it is difficult for system experts to manually inspect this large number of broken links. To this end, in this article, we propose the problem of ranking metrics according to the anomaly levels for a given invariant network, while this is a nontrivial task due to the uncertainties and the complex nature of invariant networks. Specifically, we propose two types of algorithms for ranking metric anomaly by link analysis in invariant networks. Along this line, we first define two measurements to quantify the anomaly level of each metric, and introduce the mRank algorithm. Also, we provide a weighted score mechanism and develop the gRank algorithm, which involves an iterative process to obtain a score to measure the anomaly levels. In addition, some extended algorithms based on mRank and gRank algorithms are developed by taking into account the probability of being broken as well as noisy links. Finally, we validate all the proposed algorithms on a large number of real-world and synthetic data sets to illustrate the effectiveness and efficiency of different algorithms. © 2014 ACM.",Invariant networks; Link analysis; Metric anomaly ranking,Algorithms; Complex networks; Information systems; Iterative methods; World Wide Web; Distributed information systems; Effectiveness and efficiencies; Link analysis; Metric anomaly ranking; Network representation; Non-trivial tasks; Synthetic datasets; Traditional approaches; Monitoring
Discovering general prominent streaks in sequence data,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903736247&doi=10.1145%2f2601439&partnerID=40&md5=6cb994742eb42bc06c0eb546538808e9,"This article studies the problem of prominent streak discovery in sequence data. Given a sequence of values, a prominent streak is a long consecutive subsequence consisting of only large (small) values, such as consecutive games of outstanding performance in sports, consecutive hours of heavy network traffic, and consecutive days of frequent mentioning of a person in social media. Prominent streak discovery provides insightful data patterns for data analysis in many real-world applications and is an enabling technique for computational journalism. Given its real-world usefulness and complexity, the research on prominent streaks in sequence data opens a spectrum of challenging problems. A baseline approach to finding prominent streaks is a quadratic algorithm that exhaustively enumerates all possible streaks and performs pairwise streak dominance comparison. For more efficient methods, we make the observation that prominent streaks are in fact skyline points in two dimensions-streak interval length and minimum value in the interval. Our solution thus hinges on the idea to separate the two steps in prominent streak discovery: candidate streak generation and skyline operation over candidate streaks. For candidate generation, we propose the concept of local prominent streak (LPS). We prove that prominent streaks are a subset of LPSs and the number of LPSs is less than the length of a data sequence, in comparison with the quadratic number of candidates produced by the brute-force baseline method. We develop efficient algorithms based on the concept of LPS. The nonlinear local prominent streak (NLPS)-based method considers a superset of LPSs as candidates, and the linear local prominent streak (LLPS)-based method further guarantees to consider only LPSs. The proposed properties and algorithms are also extended for discovering general top-k, multisequence, and multidimensional prominent streaks. The results of experiments using multiple real datasets verified the effectiveness of the proposed methods and showed orders of magnitude performance improvement against the baseline method. © 2014 ACM.",Computational journalism; Sequence database; Skyline query; Time series database,Computer science; Data mining; Candidate generation; Computational journalism; Heavy network traffic; Orders of magnitude; Quadratic algorithms; Sequence database; Skyline query; Time Series Database; Algorithms
A regularization approach to learning task relationships in multitask learning,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903750522&doi=10.1145%2f2538028&partnerID=40&md5=5b03c966ea4f8d4846c3b6631c962b98,"Multitask learning is a learning paradigm that seeks to improve the generalization performance of a learning task with the help of some other related tasks. In this article, we propose a regularization approach to learning the relationships between tasks inmultitask learning. This approach can be viewed as a novel generalization of the regularized formulation for single-task learning. Besides modeling positive task correlation, our approach-multitask relationship learning (MTRL)-can also describe negative task correlation and identify outlier tasks based on the same underlying principle. By utilizing a matrix-variate normal distribution as a prior on the model parameters of all tasks, our MTRL method has a jointly convex objective function. For efficiency, we use an alternating method to learn the optimal model parameters for each task as well as the relationships between tasks. We study MTRL in the symmetric multitask learning setting and then generalize it to the asymmetric setting as well.We also discuss some variants of the regularization approach to demonstrate the use of other matrix-variate priors for learning task relationships. Moreover, to gain more insight into our model, we also study the relationships between MTRL and some existing multitask learning methods. Experiments conducted on a toy problem as well as several benchmark datasets demonstrate the effectiveness of MTRL as well as its high interpretability revealed by the task covariance matrix. © 2014 ACM.",Multitask learning; Regularization framework; Task relationship,Normal distribution; Alternating method; Benchmark datasets; Generalization performance; Multitask learning; Regularization approach; Regularization framework; Task relationship; Underlying principles; Covariance matrix
Robust manifold nonnegative matrix factorization,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903737038&doi=10.1145%2f2601434&partnerID=40&md5=cccd9320dfe15f35967edf826eb1110a,"Nonnegative Matrix Factorization (NMF) has been one of the most widely used clustering techniques for exploratory data analysis. However, since each data point enters the objective function with squared residue error, a few outliers with large errors easily dominate the objective function. In this article, we propose a Robust Manifold Nonnegative Matrix Factorization (RMNMF)method using l2,1-norm and integrating NMF and spectral clustering under the same clustering framework. We also point out the solution uniqueness issue for the existing NMF methods and propose an additional orthonormal constraint to address this problem. With the new constraint, the conventional auxiliary function approach no longer works. We tackle this difficult optimization problem via a novel Augmented Lagrangian Method (ALM)-based algorithm and convert the original constrained optimization problem on one variable into a multivariate constrained problem. The new objective function then can be decomposed into several subproblems that each has a closedform solution. More importantly, we reveal the connection of our method with robust K-means and spectral clustering, and we demonstrate its theoretical significance. Extensive experiments have been conducted on nine benchmark datasets, and all empirical results show the effectiveness of our method. © 2014 ACM.",Clustering; Manifold; Nonnegative matrix factorization (NMF); Robust NMF,Lagrange multipliers; Augmented Lagrangian methods; Clustering; Clustering techniques; Constrained optimi-zation problems; Exploratory data analysis; Manifold; Nonnegative matrix factorization; Robust NMF; Constrained optimization
On the sample complexity of random fourier features for online learning: How many random fourier features do we need?,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903690669&doi=10.1145%2f2611378&partnerID=40&md5=597517997823555dcf78809c2a35b453,"We study the sample complexity of random Fourier features for online kernel learning-that is, the number of random Fourier features required to achieve good generalization performance.We show that when the loss function is strongly convex and smooth, online kernel learning with random Fourier features can achieve an O(log T/T) bound for the excess risk with only O(1/γ 2) random Fourier features, where T is the number of training examples and γ is the modulus of strong convexity. This is a significant improvement compared to the existing result for batch kernel learning that requires O(T) random Fourier features to achieve a generalization bound O(1/√T). Our empirical study verifies that online kernel learning with a limited number of random Fourier features can achieve similar generalization performance as online learning using full kernel matrix. We also present an enhanced online learning algorithm with random Fourier features that improves the classification performance by multiple passes of training examples and a partial average. © 2014 ACM.",Kernel learning; Nyström; Sampling complexity,Fourier transforms; Classification performance; Empirical studies; Generalization bound; Generalization performance; Kernel learning; Online kernel learning; Online learning algorithms; Strong convexities; E-learning
Network sampling: From static to streaming graphs,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903733505&doi=10.1145%2f2601438&partnerID=40&md5=b786870e8beb5eabc079bf09c18b3635,"Network sampling is integral to the analysis of social, information, and biological networks. Since many real-world networks are massive in size, continuously evolving, and/or distributed in nature, the network structure is often sampled in order to facilitate study. For these reasons, a more thorough and complete understanding of network sampling is critical to support the field of network science. In this paper, we outline a framework for the general problem of network sampling by highlighting the different objectives, population and units of interest, and classes of network sampling methods. In addition, we propose a spectrum of computational models for network sampling methods, ranging from the traditionally studied model based on the assumption of a static domain to a more challenging model that is appropriate for streaming domains. We design a family of sampling methods based on the concept of graph induction that generalize across the full spectrum of computational models (from static to streaming) while efficiently preserving many of the topological properties of the input graphs. Furthermore, we demonstrate how traditional static sampling algorithms can be modified for graph streams for each of the three main classes of sampling methods: node, edge, and topology-based sampling. Experimental results indicate that our proposed family of sampling methods more accurately preserve the underlying properties of the graph in both static and streaming domains. Finally, we study the impact of network sampling algorithms on the parameter estimation and performance evaluation of relational classification algorithms. © 2014 ACM.",Graph streams; Network sampling; Relational classification; Social network analysis,Computational methods; Learning algorithms; Social networking (online); Biological networks; Classification algorithm; Computational model; Graph streams; Network samplings; Real-world networks; Sampling algorithm; Topological properties; Topology
Efficient discovery of the most interesting associations,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903700552&doi=10.1145%2f2601433&partnerID=40&md5=8bfba922772bf5150ee7327dad815380,"Self-sufficient itemsets have been proposed as an effective approach to summarizing the key associations in data. However, their computation appears highly demanding, as assessing whether an itemset is selfsufficient requires consideration of all pairwise partitions of the itemset into pairs of subsets as well as consideration of all supersets. This article presents the first published algorithm for efficiently discovering self-sufficient itemsets. This branch-and-bound algorithm deploys two powerful pruning mechanisms based on upper bounds on itemset value and statistical significance level. It demonstrates that finding top-k productive and nonredundant itemsets, with postprocessing to identify those that are not independently productive, can efficiently identify small sets of key associations. We present extensive evaluation of the strengths and limitations of the technique, including comparisons with alternative approaches to finding the most interesting associations. © 2014 ACM.",Association mining; Interestingness; Itemset mining; Statistical association mining,Algorithms; Association mining; Branch-and-bound algorithms; Effective approaches; Interestingness; Itemset mining; Non-redundant; Statistical significance; Upper Bound; Branch and bound method
Scalable and axiomatic ranking of network role similarity,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896998405&doi=10.1145%2f2518176&partnerID=40&md5=9b94611743416fd7c3248b6e9e877f13,"A key task in analyzing social networks and other complex networks is role analysis: describing and categorizing nodes according to how they interact with other nodes. Two nodes have the same role if they interact with equivalent sets of neighbors. The most fundamental role equivalence is automorphic equivalence. Unfortunately, the fastest algorithms known for graph automorphism are nonpolynomial. Moreover, since exact equivalence is rare, a more meaningful task is measuring the role similarity between any two nodes. This task is closely related to the structural or link-based similarity problem that SimRank addresses. However, SimRank and other existing similarity measures are not sufficient because they do not guarantee to recognize automorphically or structurally equivalent nodes. This article makes two contributions. First, we present and justify several axiomatic properties necessary for a role similarity measure or metric. Second, we present RoleSim, a new similarity metric that satisfies these axioms and can be computed with a simple iterative algorithm. We rigorously prove that RoleSim satisfies all of these axiomatic properties. We also introduce Iceberg RoleSim, a scalable algorithm that discovers all pairs with RoleSim scores above a user-defined threshold θ. We demonstrate the interpretative power of RoleSim on both both synthetic and real datasets.",Automorphic equivalence; Complex network; Ranking; Role similarity; Social network; Vertex similarity,Complex networks; Sea ice; Social networking (online); Automorphic equivalence; Graph automorphisms; Iterative algorithm; Ranking; Role similarity; Scalable algorithms; Similarity metrics; Vertex similarities; Algorithms
User behavior learning and transfer in composite social networks,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896983834&doi=10.1145%2f2556613&partnerID=40&md5=5b57fc5d780dcbdd6ceb3f61e230cd26,"Accurate prediction of user behaviors is important for many social media applications, including social marketing, personalization, and recommendation. A major challenge lies in that although many previous worksmodel user behavior from only historical behavior logs, the available user behavior data or interactions between users and items in a given social network are usually very limited and sparse (e.g., ≤99.9% empty), which makes models overfit the rare observations and fail to provide accurate predictions. We observe that many people are members of several social networks in the same time, such as Facebook, Twitter, and Tencent's QQ. Importantly, users' behaviors and interests in different networks influence one another. This provides an opportunity to leverage the knowledge of user behaviors in different networks by considering the overlapping users in different networks as bridges, in order to alleviate the data sparsity problem, and enhance the predictive performance of user behavior modeling. Combining different networks ""simply and naively"" does not work well. In this article, we formulate the problem to model multiple networks as ""adaptive composite transfer"" and propose a framework called ComSoc. ComSoc first selects the most suitable networks inside a composite social network via a hierarchical Bayesian model, parameterized for individual users. It then builds topic models for user behavior prediction using both the relationships in the selected networks and related behavior data.With different relational regularization, we introduce different implementations, corresponding to different ways to transfer knowledge from composite social relations. To handle big data, we have implemented the algorithm using Map/Reduce. We demonstrate that the proposed composite network-based user behavior models significantly improve the predictive accuracy over a number of existing approaches on several real-world applications, including a very large social networking dataset from Tencent Inc.",Composite social network; Social network analysis; Transfer learning,Bayesian networks; Knowledge management; Social networking (online); Accurate prediction; Adaptive composite; Data sparsity problems; Hierarchical Bayesian modeling; Predictive accuracy; Predictive performance; Transfer learning; User behavior modeling; Behavioral research
"Introduction to special issue on computational aspects of social and information networks: Theory, methodologies, and applications (TKDD-CASIN)",2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896993065&doi=10.1145%2f2556608&partnerID=40&md5=c8b1d538c26e37304a3999829daf73ca,[No abstract available],,
A separability framework for analyzing community structure,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896962839&doi=10.1145%2f2527231&partnerID=40&md5=3b2c362a9500c00ca8d66c1ab3ee9bb0,"Four major factors govern the intricacies of community extraction in networks: (1) the literature offers a multitude of disparate community detection algorithms whose output exhibits high structural variability across the collection, (2) communities identified by algorithms may differ structurally from real communities that arise in practice, (3) there is no consensus characterizing how to discriminate communities from noncommunities, and (4) the application domain includes a wide variety of networks of fundamentally different natures. In this article, we present a class separability framework to tackle these challenges through a comprehensive analysis of community properties. Our approach enables the assessment of the structural dissimilarity among the output of multiple community detection algorithms and between the output of algorithms and communities that arise in practice. In addition, our method provides us with a way to organize the vast collection of community detection algorithms by grouping those that behave similarly. Finally, we identify themost discriminative graph-theoretical properties of community signature and the small subset of properties that account for most of the biases of the different community detection algorithms. We illustrate our approach with an experimental analysis, which reveals nuances of the structure of real and extracted communities. In our experiments, we furnish our framework with the output of 10 different community detection procedures, representative of categories of popular algorithms available in the literature, applied to a diverse collection of large-scale real network datasets whose domains span biology, online shopping, and social systems. We also analyze communities identified by annotations that accompany the data, which reflect exemplar communities in various domain.We characterize these communities using a broad spectrum of community properties to produce the different structural classes. As our experiments show that community structure is not a universal concept, our framework enables an informed choice of the most suitable community detection method for identifying communities of a specific type in a given network and allows for a comparison of existing community detection algorithms while guiding the design of new ones.",Class separability; Community structure; Detection algorithms; Networks,Algorithms; Electronic commerce; Experiments; Graph theory; Networks (circuits); Population dynamics; Signal detection; Class separability; Community detection algorithms; Community structures; Comprehensive analysis; Detection algorithm; Experimental analysis; Structural dissimilarities; Structural variability; Computer networks
IHypR: Prominence ranking in networks of collaborations with hyperedges1,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891852169&doi=10.1145%2f2541268.2541269&partnerID=40&md5=ab70c4f60d4081804312e3d64106153f,"We present a new algorithm called iHypR for computing prominence of actors in social networks of collaborations. Our algorithm builds on the assumption that prominent actors collaborate on prominent objects, and prominent objects are naturally grouped into prominent clusters or groups (hyperedges in a graph). iHypR makes use of the relationships between actors, objects, and hyperedges to compute a global prominence score for the actors in the network. We do not assume the hyperedges are given in advance. Hyperedges computed by our method can perform as well or even better than ''true'' hyperedges. Our algorithm is customized for networks of collaborations, but it is generally applicable without further tuning. We show, through extensive experimentation with three real-life data sets and multiple external measures of prominence, that our algorithm outperforms existing well-known algorithms. Our work is the first to offer such an extensive evaluation. We show that unlike most existing algorithms, the performance is robust across multiple measures of performance. Further, we give a detailed study of the sensitivity of our algorithm to different data sets and the design choices within the algorithm that a user may wish to change. Our article illustrates the various trade-offs that must be considered in computing prominence in collaborative social networks. © 2013 ACM.",Collaboration; Prominence; Social networks,Social networking (online); Social sciences computing; Collaboration; Hyperedges; In networks; Measures of performance; Prominence; Prominent objects; Real life datasets; Clustering algorithms
Discovering social circles in ego networks,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896914521&doi=10.1145%2f2556612&partnerID=40&md5=dfdf16c512b5861a3f2e97920cf7bd7e,"People's personal social networks are big and cluttered, and currently there is no good way to automatically organize them. Social networking sites allow users to manually categorize their friends into social circles (e.g., ""circles"" on Google+, and ""lists"" on Facebook and Twitter). However, circles are laborious to construct and must be manually updated whenever a user's network grows. In this article, we study the novel task of automatically identifying users' social circles. We pose this task as a multimembership node clustering problem on a user's ego network, a network of connections between her friends. We develop a model for detecting circles that combines network structure as well as user profile information. For each circle, we learn its members and the circle-specific user profile similarity metric.Modeling nodemembership tomultiple circles allows us to detect overlapping as well as hierarchically nested circles. Experiments show that our model accurately identifies circles on a diverse set of data from Facebook, Google+, and Twitter, for all of which we obtain hand-labeled ground truth.",Community detection; Ego networks; Machine learning; Social circles,Computer science; Data mining; Learning systems; Community detection; Ego networks; Ground truth; Network structures; Node clustering; Social circles; Social networking sites; User profile; Social networking (online)
Classification in p2p networks with cascade support vector machines,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891883230&doi=10.1145%2f2541268.2541273&partnerID=40&md5=c6e65565c6c2977d8d7e359fe9d5326b,"Classification in Peer-to-Peer (P2P) networks is important to many real applications, such as distributed intrusion detection, distributed recommendation systems, and distributed antispam detection. However, it is very challenging to perform classification in P2P networks due tomany practical issues, such as scalability, peer dynamism, and asynchronism. This article investigates the practical techniques of constructing Support Vector Machine (SVM) classifiers in the P2P networks. In particular, we demonstrate how to efficiently cascade SVM in a P2P network with the use of reduced SVM. In addition, we propose to fuse the concept of cascade SVM with bootstrap aggregation to effectively balance the trade-off between classification accuracy, model construction, and prediction cost. We provide theoretical insights for the proposed solutions and conduct an extensive set of empirical studies on a number of large-scale datasets. Encouraging results validate the efficacy of the proposed approach. © 2013 ACM.",Bootstrap aggregation; Cascade SVM; Distributed classification; P2P networks,Distributed computer systems; Support vector machines; Cascade SVM; Classification accuracy; Distributed classification; Distributed intrusion detection; Distributed recommendation systems; Large-scale datasets; P2P network; Peer to Peer (P2P) network; Peer to peer networks
Social trust prediction using heterogeneous networks,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891871273&doi=10.1145%2f2541268.2541270&partnerID=40&md5=87d6fb37bd0880fc5824a33b922324ff,"Along with increasing popularity of socialwebsites, online users rely more on the trustworthiness information to make decisions, extract and filter information, and tag and build connections with other users. However, such social network data often suffer from severe data sparsity and are not able to provide users with enough information. Therefore, trust prediction has emerged as an important topic in social network research. Traditional approaches are primarily based on exploring trust graph topology itself. However, research in sociology and our life experience suggest that people who are in the same social circle often exhibit similar behaviors and tastes. To take advantage of the ancillary information for trust prediction, the challenge then becomes what to transfer and how to transfer. In this article, we address this problem by aggregating heterogeneous social networks and propose a novel joint social networks mining (JSNM) method. Our new joint learning model explores the user-group-level similarity between correlated graphs and simultaneously learns the individual graph structure; therefore, the shared structures and patterns from multiple social networks can be utilized to enhance the prediction tasks. As a result, we not only improve the trust prediction in the target graph but also facilitate other information retrieval tasks in the auxiliary graphs. To optimize the proposed objective function, we use the alternative technique to break down the objective function into several manageable subproblems. We further introduce the auxiliary function to solve the optimization problems with rigorously proved convergence. The extensive experiments have been conducted on both synthetic and real- world data. All empirical results demonstrate the effectiveness of our method. © 2013 ACM.",Nonnegative matrix factorization; Social network; Transfer learning; Trust prediction,Behavioral research; Filtration; Forecasting; Heterogeneous networks; Optimization; Social networking (online); Auxiliary functions; Nonnegative matrix factorization; Objective functions; Optimization problems; Social networks minings; Traditional approaches; Transfer learning; Trust predictions; Topology
Solving inverse frequent itemset mining with infrequency constraints via large-scale linear programs,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891852921&doi=10.1145%2f2541268.2541271&partnerID=40&md5=c2e100c1be5e2eb8a3c7665475f21ba8,"Inverse frequent set mining (IFM) is the problem of computing a transaction database D satisfying given support constraints for some itemsets, which are typically the frequent ones. This article proposes a new formulation of IFM, called IFMI (IFM with infrequency constraints), where the itemsets that are not listed as frequent are constrained to be infrequent; that is, they must have a support less than or equal to a specified unique threshold. An instance of IFMI can be seen as an instance of the original IFM by making explicit the infrequency constraints for the minimal infrequent itemsets, corresponding to the so-called negative generator border defined in the literature. The complexity increase from PSPACE (complexity of IFM) to NEXP (complexity of IFMI) is caused by the cardinality of the negative generator border, which can be exponential in the original input size. Therefore, the article introduces a specific problem parameter ? that computes an upper bound to this cardinality using a hypergraph interpretation for which minimal infrequent itemsets correspond to minimal transversals. By fixing a constant k, the article formulates a k-bounded definition of the problem, called k-IFMI, that collects all instances for which the value of the parameter ? is less than or equal to k-its complexity is in PSPACE as for IFM. The bounded problem is encoded as an integer linear program with a large number of variables (actually exponential w.r.t. the number of constraints), which is thereafter approximated by relaxing integer constraints-the decision problem of solving the linear program is proven to be in NP. In order to solve the linear program, a column generation technique is used that is a variation of the simplex method designed to solve large-scale linear programs, in particular with a huge number of variables. The method at each step requires the solution of an auxiliary integer linear program, which is proven to be NP hard in this case and for which a greedy heuristic is presented. The resulting overall column generation solution algorithm enjoys very good scaling as evidenced by the intensive experimentation, thereby paving the way for its application in real-life scenarios. © 2013 ACM.",Column generation simplex; Frequent itemset mining; Inverse problem; Minimal hypergraph transversals,Distributed computer systems; Integer programming; Inverse problems; Linear programming; Column generation; Frequent itemset mining; Frequent set mining; Hypergraph transversals; Integer linear programs; Solution algorithms; Specific problems; Transaction database; Constraint satisfaction problems
Uncovering social network sybils in the wild,2014,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896964777&doi=10.1145%2f2556609&partnerID=40&md5=726ad2b12afc54e68b2aa5d461e72929,"Sybil accounts are fake identities created to unfairly increase the power or resources of a single malicious user. Researchers have long known about the existence of Sybil accounts in online communities such as file-sharing systems, but they have not been able to perform large-scale measurements to detect them or measure their activities. In this article, we describe our efforts to detect, characterize, and understand Sybil account activity in the Renren Online Social Network (OSN). We use ground truth provided by Renren Inc. to build measurement-based Sybil detectors and deploy them on Renren to detect more than 100,000 Sybil accounts. Using our full dataset of 650,000 Sybils, we examine several aspects of Sybil behavior. First, we study their link creation behavior and find that contrary to prior conjecture, Sybils in OSNs do not form tight-knit communities. Next, we examine the fine-grained behaviors of Sybils on Renren using clickstream data. Third, we investigate behind-the-scenes collusion between large groups of Sybils. Our results reveal that Sybils with no explicit social ties still act in concert to launch attacks. Finally, we investigate enhanced techniques to identify stealthy Sybils. In summary, our study advances the understanding of Sybil behavior on OSNs and shows that Sybils can effectively avoid existing community-based Sybil detectors.We hope that our results will foster new research on Sybil detection that is based on novel types of Sybil features.",Measurement; Online social networks; Spam; Sybil attacks; User behavior,Measurements; Online systems; Social networking (online); File-sharing system; Large-scale measurement; On-line communities; On-line social networks; Online social networks (OSN); Spam; Sybil attack; User behaviors; Behavioral research
Formal and computational properties of the confidence boost of association rules,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887079072&doi=10.1145%2f2541268.2541272&partnerID=40&md5=38bcff4e2b804c1f93e46906e4c815ad,"Some existing notions of redundancy among association rules allow for a logical-style characterization and lead to irredundant bases of absolutely minimum size. We push the intuition of redundancy further to find an intuitive notion of novelty of an association rule, with respect to other rules. Namely, an irredundant rule is so because its confidence is higher than what the rest of the rules would suggest; then, one can ask: how much higher? We propose tomeasure such a sort of novelty through the confidence boost of a rule. Acting as a complement to confidence and support, the confidence boost helps to obtain small and crisp sets of mined association rules and solves the well-known problem that, in certain cases, rules of negative correlation may pass the confidence bound. We analyze the properties of two versions of the notion of confidence boost, one of them a natural generalization of the other. We develop algorithms to filter rules according to their confidence boost, compare the concept to some similar notions in the literature, and describe the results of some experimentation employing the new notions on standard benchmark datasets. We describe an open source association mining tool that embodies one of our variants of confidence boost in such a way that the data mining process does not require the user to select any value for any parameter. © 2013 ACM.",Association rule mining; Association rule quality; Confidence,Data mining; Filtration; Redundancy; Benchmark datasets; Computational properties; Confidence; Confidence bounds; Data mining process; Natural generalization; Negative correlation; Open sources; Association rules
Parallel field ranking,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885003530&doi=10.1145%2f2513092.2513096&partnerID=40&md5=aea96a5d891039f9e7f3f8713855c247,"Recently, ranking data with respect to the intrinsic geometric structure (manifold ranking) has received considerable attentions, with encouraging performance in many applications in pattern recognition, information retrieval and recommendation systems. Most of the existing manifold ranking methods focus on learning a ranking function that varies smoothly along the data manifold. However, beyond smoothness, a desirable ranking function should vary monotonically along the geodesics of the data manifold, such that the ranking order along the geodesics is preserved. In this article, we aim to learn a ranking function that varies linearly and therefore monotonically along the geodesics of the data manifold. Recent theoretical work shows that the gradient field of a linear function on the manifold has to be a parallel vector field. Therefore, we propose a novel ranking algorithm on the data manifolds, called Parallel Field Ranking. Specifically, we try to learn a ranking function and a vector field simultaneously. We require the vector field to be close to the gradient field of the ranking function, and the vector field to be as parallel as possible. Moreover, we require the value of the ranking function at the query point to be the highest, and then decrease linearly along the manifold. Experimental results on both synthetic data and real data demonstrate the effectiveness of our proposed algorithm. © 2013 ACM.",Manifold; Ranking; Vector field,Algorithms; Pattern recognition; Vectors; Geometric structure; Linear functions; Manifold; Parallel vectors; Ranking; Ranking algorithm; Ranking functions; Vector fields; Information retrieval
Active sampling for entity matching with guarantees,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885006501&doi=10.1145%2f2500490&partnerID=40&md5=fdf13e63f8394aa4bb7f86ddafb3eb93,"In entity matching, a fundamental issue while training a classifier to label pairs of entities as either duplicates or nonduplicates is the one of selecting informative training examples. Although active learning presents an attractive solution to this problem, previous approaches minimize the misclassification rate (0-1 loss) of the classifier, which is an unsuitable metric for entity matching due to class imbalance (i.e., many more nonduplicate pairs than duplicate pairs). To address this, a recent paper [Arasu et al. 2010] proposes to maximize recall of the classifier under the constraint that its precision should be greater than a specified threshold. However, the proposed technique requires the labels of all n input pairs in the worst case. Our main result is an active learning algorithm that approximately maximizes recall of the classifier while respecting a precision constraint with provably sub linear label complexity (under certain distributional assumptions). Our algorithm uses as a black box any active learning module that minimizes 0-1 loss. We show that label complexity of our algorithm is at most log n times the label complexity of the black box, and also bound the difference in the recall of classifier learnt by our algorithm and the recall of the optimal classifier satisfying the precision constraint. We provide an empirical evaluation of our algorithm on several real-world matching data sets that demonstrates the effectiveness of our approach. © 2013 ACM.",Active learning; Deduplication; Entity matching; Imbalanced data,Computer science; Data mining; Active Learning; Active-learning algorithm; Attractive solutions; De duplications; Empirical evaluations; Entity matching; Imbalanced data; Misclassification rates; Algorithms
Addressing big data time series: Mining trillions of time series subsequences under dynamic time warping,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884994717&doi=10.1145%2f2500489&partnerID=40&md5=9ed987bc2afff6e1dc0f96456bfc5439,"Most time series data mining algorithms use similarity search as a core subroutine, and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithms, including classification, clustering, motif discovery, anomaly detection, and so on. The difficulty of scaling a search to large datasets explains to a great extent why most academic work on time series data mining has plateaued at considering a few millions of time series objects, while much of industry and science sits on billions of time series objects waiting to be explored. In this work we show that by using a combination of four novel ideas we can search and mine massive time series for the first time. We demonstrate the following unintuitive fact: in large datasets we can exactly search under Dynamic Time Warping (DTW) much more quickly than the current state-of-the-art Euclidean distance search algorithms. We demonstrate our work on the largest set of time series experiments ever attempted. In particular, the largest dataset we consider is larger than the combined size of all of the time series datasets considered in all data mining papers ever published. We explain how our ideas allow us to solve higher-level time series data mining problems such as motif discovery and clustering at scales that would otherwise be untenable. Moreover, we show how our ideas allow us to efficiently support the uniform scaling distance measure, a measure whose utility seems to be underappreciated, but which we demonstrate here. In addition to mining massive datasets with up to one trillion data points, we will show that our ideas also have implications for real-time monitoring of data streams, allowing us to handle much faster arrival rates and/or use cheaper and lower powered devices than are currently possible. © 2013 ACM.",Lower bounds; Similarity search; Time series,Algorithms; Time series; Time series analysis; Dynamic time warping; Euclidean distance; Lower bounds; Real time monitoring; Similarity search; Time series data mining; Time series subsequences; Time-series experiments; Data mining
PathSelClus: Integrating meta-path selection with user-guided Object clustering in heterogeneous information networks,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884996575&doi=10.1145%2f2500492&partnerID=40&md5=a23c13cf88bd59cc4443787de87ba92d,"Real-world, multiple-typed objects are often interconnected, forming heterogeneous information networks. A major challenge for link-based clustering in such networks is their potential to generate many different results, carrying rather diverse semantic meanings. In order to generate desired clustering, we propose to use meta-path, a path that connects object types via a sequence of relations, to control clustering with distinct semantics. Nevertheless, it is easier for a user to provide a few examples (seeds) than a weighted combination of sophisticated meta-paths to specify her clustering preference. Thus, we propose to integrate meta-path selection with user-guided clustering to cluster objects in networks, where a user first provides a small set of object seeds for each cluster as guidance. Then the system learns the weight for each meta path that is consistent with the clustering result implied by the guidance, and generates clusters under the learned weights of meta-paths. A probabilistic approach is proposed to solve the problem, and an effective and efficient iterative algorithm, PathSelClus, is proposed to learn the model, where the clustering quality and the meta-path weights mutually enhance each other. Our experiments with several clustering tasks in two real networks and one synthetic network demonstrate the power of the algorithm in comparison with the baselines. © 2013 ACM.",Heterogeneous information networks; Meta-path selection; User-guided clustering,Clustering algorithms; Semantics; Clustering quality; Clustering results; Heterogeneous information; Iterative algorithm; Meta-path selection; Probabilistic approaches; Synthetic networks; User-guided; Information services
Introduction to the special issue ACM SIGKDD 2012,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885013894&doi=10.1145%2f2513092.2513093&partnerID=40&md5=4ba7662d8b67b0c16bde9e0fd1e338b8,[No abstract available],,
Batch mode active sampling based on marginal probability distribution matching,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884991180&doi=10.1145%2f2513092.2513094&partnerID=40&md5=b916f9405c965a68f33d79fc2ee88a98,"Active Learning is a machine learning and data mining technique that selects the most informative samples for labeling and uses them as training data; it is especially useful when there are large amount of unlabeled data and labeling them is expensive. Recently, batch-mode active learning, where a set of samples are selected concurrently for labeling, based on their collective merit, has attracted a lot of attention. The objective of batch-mode active learning is to select a set of informative samples so that a classifier learned on these samples has good generalization performance on the unlabeled data. Most of the existing batch-mode active learning methodologies try to achieve this by selecting samples based on certain criteria. In this article we propose a novel criterion which achieves good generalization performance of a classifier by specifically selecting a set of query samples that minimize the difference in distribution between the labeled and the unlabeled data, after annotation. We explicitly measure this difference based on all candidate subsets of the unlabeled data and select the best subset. The proposed objective is an NP-hard integer programming optimization problem. We provide two optimization techniques to solve this problem. In the first one, the problem is transformed into a convex quadratic programming problem and in the second method the problem is transformed into a linear programming problem. Our empirical studies using publicly available UCI datasets and two biomedical image databases demonstrate the effectiveness of the proposed approach in comparison with the state-of-the-art batch-mode active learning methods. We also present two extensions of the proposed approach, which incorporate uncertainty of the predicted labels of the unlabeled data and transfer learning in the proposed formulation. In addition, we present a joint optimization framework for performing both transfer and active learning simultaneously unlike the existing approaches of learning in two separate stages, that is, typically, transfer learning followed by active learning. We specifically minimize a common objective of reducing distribution difference between the domain adapted source, the queried and labeled samples and the rest of the unlabeled target domain data. Our empirical studies on two biomedical image databases and on a publicly available 20 Newsgroups dataset show that incorporation of uncertainty information and transfer learning further improves the performance of the proposed active learning based classifier. Our empirical studies also show that the proposed transfer-active method based on the joint optimization framework performs significantly better than a framework which implements transfer and active learning in two separate stages. © 2013 ACM.",Active learning; Marginal probability distribution; Maximum mean discrepancy; Transfer learning,Classification (of information); Integer programming; Linear programming; Optimization; Probability distributions; Active Learning; Biomedical image database; Convex quadratic programming; Generalization performance; Linear programming problem; Maximum mean discrepancy; Transfer learning; Uncertainty informations; Problem solving
Instance annotation for multi-instance multi-label learning,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885006033&doi=10.1145%2f2500491&partnerID=40&md5=b9efc3ae165aca54df10831c29b072dd,"Multi-instance multi-label learning (MIML) is a framework for supervised classification where the objects to be classified are bags of instances associated with multiple labels. For example, an image can be represented as a bag of segments and associated with a list of objects it contains. Prior work on MIML has focused on predicting label sets for previously unseen bags. We instead consider the problem of predicting instance labels while learning from data labeled only at the bag level. We propose a regularized rank-loss objective designed for instance annotation, which can be instantiated with different aggregation models connecting instance-level labels with bag-level label sets. The aggregation models that we consider can be factored as a linear function of a ""support instance"" for each class, which is a single feature vector representing a whole bag. Hence we name our proposed methods rank-loss Support Instance Machines (SIM). We propose two optimization methods for the rank-loss objective, which is nonconvex. One is a heuristic method that alternates between updating support instances, and solving a convex problem in which the support instances are treated as constant. The other is to apply the constrained concave-convex procedure (CCCP), which can also be interpreted as iteratively updating support instances and solving a convex problem. To solve the convex problem, we employ the Pegasos framework of primal subgradient descent, and prove that it finds an ε-suboptimal solution in runtime that is linear in the number of bags, instances, and 1/ε . Additionally, we suggest a method of extending the linear learning algorithm to nonlinear classification, without increasing the runtime asymptotically. Experiments on artificial and real-world datasets including images and audio show that the proposed methods achieve higher accuracy than other loss functions used in prior work, e.g., Hamming loss, and recent work in ambiguous label classification. © 2013 ACM.",Bioacoustics; Image annotation; Instance annotation; Multi-instance; Multi-label; Subgradient; Support vector machine,Bioacoustics; Classification (of information); Heuristic methods; Image retrieval; Iterative methods; Learning algorithms; Learning systems; Support vector machines; Image annotation; Instance annotation; Multi-instance; Multi-label; Subgradient; Problem solving
Confidence weighted mean reversion strategy for online portfolio selection,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878528879&doi=10.1145%2f2435209.2435213&partnerID=40&md5=97eb11a87fd0beed8b87686eb3b7d6fb,"Online portfolio selection has been attracting increasing attention from the data mining and machine learning communities. All existing online portfolio selection strategies focus on the first order information of a portfolio vector, though the second order information may also be beneficial to a strategy. Moreover, empirical evidence shows that relative stock prices may follow the mean reversion property, which has not been fully exploited by existing strategies. This article proposes a novel online portfolio selection strategy named Confidence Weighted Mean Reversion (CWMR). Inspired by the mean reversion principle in finance and confidence weighted online learning technique in machine learning, CWMR models the portfolio vector as a Gaussian distribution, and sequentially updates the distribution by following the mean reversion trading principle. CWMR's closed-form updates clearly reflect the mean reversion trading idea. We also present several variants of CWMR algorithms, including a CWMR mixture algorithm that is theoretical universal. Empirically, CWMR strategy is able to effectively exploit the power of mean reversion for online portfolio selection. Extensive experiments on various real markets show that the proposed strategy is superior to the state-of-the-art techniques. The experimental testbed including source codes and data sets is available online.1 © 2013 ACM.",Confidence weighted learning; Mean reversion; Online learning; Portfolio selection,Algorithms; Commerce; Confidence-weighted learning; Experimental testbed; Machine learning communities; Mean reversion; Mixture algorithm; Online learning; Portfolio selection; State-of-the-art techniques; Learning systems
Comparative document summarization via discriminative sentence selection,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878535007&doi=10.1145%2f2435209.2435211&partnerID=40&md5=7766d5dd9f24bc99df491b04d30b0a3b,"Given a collection of document groups, a natural question is to identify the differences among these groups. Although traditional document summarization techniques can summarize the content of the document groups one by one, there exists a great necessity to generate a summary of the differences among the document groups. In this article, we study a novel problem of summarizing the differences between document groups. A discriminative sentence selection method is proposed to extract the most discriminative sentences that represent the specific characteristics of each document group. Experiments and case studies on real-world data sets demonstrate the effectiveness of our proposed method. © 2013 ACM.",Comparative document summarization; Discriminative sentence selection,Computer science; Data mining; Collection of documents; Document summarization; Experiments and case studies; Real-world; Sentence selection; Virtual reality
Nearest neighbor-based classification of uncertain data,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878524067&doi=10.1145%2f2435209.2435210&partnerID=40&md5=948a2b6e51f73b9a81bee877b0d853ae,"This work deals with the problem of classifying uncertain data. With this aim we introduce the Uncertain Nearest Neighbor (UNN) rule, which represents the generalization of the deterministic nearest neighbor rule to the case in which uncertain objects are available. The UNN rule relies on the concept of nearest neighbor class, rather than on that of nearest neighbor object. The nearest neighbor class of a test object is the class that maximizes the probability of providing its nearest neighbor. The evidence is that the former concept is much more powerful than the latter in the presence of uncertainty, in that it correctly models the right semantics of the nearest neighbor decision rule when applied to the uncertain scenario. An effective and efficient algorithm to perform uncertain nearest neighbor classification of a generic (un)certain test object is designed, based on properties that greatly reduce the temporal cost associated with nearest neighbor class probability computation. Experimental results are presented, showing that the UNN rule is effective and efficient in classifying uncertain data. © 2013 ACM.",Classification; Nearest neighbor; Nearest neighbor rule; Probability density functions; Uncertain data,Algorithms; Classification (of information); Semantics; Nearest neighbor classification; Nearest neighbor decision rules; Nearest neighbor rule; Nearest neighbors; Test object; Uncertain datas; Probability density function
Message-passing algorithms for sparse network alignment,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878595437&doi=10.1145%2f2435209.2435212&partnerID=40&md5=7e16fa71a5dd4687d327361768d79381,"Network alignment generalizes and unifies several approaches for forming a matching or alignment between the vertices of two graphs. We study a mathematical programming framework for network alignment problem and a sparse variation of it where only a small number of matches between the vertices of the two graphs are possible.We propose a new message passing algorithm that allows us to compute, very efficiently, approximate solutions to the sparse network alignment problems with graph sizes as large as hundreds of thousands of vertices. We also provide extensive simulations comparing our algorithms with two of the best solvers for network alignment problems on two synthetic matching problems, two bioinformatics problems, and three large ontology alignment problems including a multilingual problem with a known labeled alignment. © 2014 ACM.",Belief propagation; Graph matching; Message-passing; Network alignment,Algorithms; Bioinformatics; Computer simulation; Mathematical programming; Message passing; Pattern matching; Approximate solution; Belief propagation; Extensive simulations; Graph matchings; Matching problems; Message passing algorithm; Network alignments; Ontology alignment; Graph theory
Efficient online learning for multitask feature selection,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893288034&doi=10.1145%2f2499907.2499909&partnerID=40&md5=a884cbde13221777aa0c322a35219ff9,"Learning explanatory features across multiple related tasks, or MultiTask Feature Selection (MTFS), is an important problem in the applications of data mining, machine learning, and bioinformatics. Previous MTFS methods fulfill this task by batch-mode training. This makes them inefficient when data come sequentially or when the number of training data is so large that they cannot be loaded into the memory simultaneously. In order to tackle these problems, we propose a novel online learning framework to solve the MTFS problem. A main advantage of the online algorithm is its efficiency in both time complexity and memory cost. The weights of the MTFS models at each iteration can be updated by closed-form solutions based on the average of previous subgradients. This yields the worst-case bounds of the time complexity and memory cost at each iteration, both in the order of O(d × Q), where d is the number of feature dimensions and Q is the number of tasks. Moreover, we provide theoretical analysis for the average regret of the online learning algorithms, which also guarantees the convergence rate of the algorithms. Finally, we conduct detailed experiments to show the characteristics and merits of the online learning algorithms in solving several MTFS problems.©2013 ACM 1556-4681/2013/07- ART8 $15.00.",Dual averaging method; Feature selection; Multitask learning; Online learning; Supervised learning,Bioinformatics; Feature extraction; Iterative methods; Learning algorithms; Problem solving; Supervised learning; Closed form solutions; Convergence rates; Dual averaging; Feature dimensions; Multitask learning; On-line algorithms; Online learning; Online learning algorithms; E-learning
Exploiting fisher and fukunaga-koontz transforms in chernoff dimensionality reduction,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896953516&doi=10.1145%2f2499907.2499911&partnerID=40&md5=5c5dd39484a82b190f0860d12a65e1be,"Knowledge discovery from big data demands effective representation of data. However, big data are often characterized by high dimensionality, which makes knowledge discovery more difficult. Many techniques for dimensionality reudction have been proposed, including well-known Fisher 's Linear Discriminant Analysis (LDA). However, the Fisher criterion is incapable of dealing with heteroscedasticity in the data. A technique based on the Chernoff criterion for linear dimensionality reduction has been proposed that is capable of exploiting heteroscedastic information in the data. While the Chernoff criterion has been shown to outperform the Fisher 's, a clear understanding of its exact behavior is lacking. In this article, we show precisely what can be expected from the Chernoff criterion. In particular, we show that the Chernoff criterion exploits the Fisher and Fukunaga-Koontz transforms in computing its linear discriminants. Furthermore, we show that a recently proposed decomposition of the data space into four subspaces is incomplete.We provide arguments on how to best enrich the decomposition of the data space in order to account for heteroscedasticity in the data. Finally, we provide experimental results validating our theoretical analysis.©2013 ACM 1556-4681/2013/07-ART8 $15.00.",Chernoff distance; Dimensionality reduction; Feature evaluation and selection; FKT; LDA,Discriminant analysis; Face recognition; Chernoff distance; Dimensionality reduction; Feature evaluation and selection; FKT; LDA; Mathematical transformations
Multilabel relationship learning,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896928346&doi=10.1145%2f2499907.2499910&partnerID=40&md5=d37d20b3dd3ae8f59317bc42c31a5a79,"Multilabel learning problems are commonly found in many applications. A characteristic shared by many multilabel learning problems is that some labels have significant correlations between them. In this article, we propose a novel multilabel learning method, called MultiLabel Relationship Learning (MLRL), which extends the conventional support vector machine by explicitly learning and utilizing the relationships between labels. Specifically, we model the label relationships using a label covariance matrix and use it to define a new regularization term for the optimization problem. MLRL learns the model parameters and the label covariance matrix simultaneously based on a unified convex formulation. To solve the convex optimization problem, we use an alternating method in which each subproblem can be solved efficiently. The relationship between MLRL and two widely used maximum margin methods for multilabel learning is investigated. Moreover, we also propose a semisupervised extension of MLRL, called SSMLRL, to demonstrate how to make use of unlabeled data to help learn the label covariance matrix. Through experiments conducted on some multilabel applications, we find that MLRL not only gives higher classification accuracy but also has better interpretability as revealed by the label covariance matrix.©2013 ACM 1556-4681/2013/07-ART8 $15.00.",Label relationship; Multilabel learning,Convex optimization; Alternating method; Classification accuracy; Convex optimization problems; Interpretability; Model parameters; Multi-label learning; Optimization problems; Regularization terms; Covariance matrix
Learning to predict reciprocity and triadic closure in social networks,2013,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891535149&doi=10.1145%2f2499907.2499908&partnerID=40&md5=28f8bb55ca82ec023ab3be1f1f3a922f,"We study how links are formed in social networks. In particular, we focus on investigating how a reciprocal (two-way) link, the basic relationship in social networks, is developed from a parasocial (one-way) relationship and how the relationships further develop into triadic closure, one of the fundamental processes of link formation. We first investigate how geographic distance and interactions between users influence the formation of link structure among users. Then we study how social theories including homophily, social balance, and social status are satisfied over networks with parasocial and reciprocal relationships. The study unveils several interesting phenomena. For example, "" friend 's friend is a friend"" indeed exists in the reciprocal relationship network, but does not hold in the parasocial relationship network. We propose a learning framework to formulate the problems of predicting reciprocity and triadic closure into a graphical model.We demonstrate that it is possible to accurately infer 90% of reciprocal relationships in a Twitter network. The proposed model also achieves better performance (+20-30% in terms of F1-measure) than several alternative methods for predicting the triadic closure formation.©2013 ACM 1556-4681/2013/07-ART5 $15.00.",Link prediction; Predictive model; Reciprocal relationship; Social influence; Social network; Twitter,Forecasting; Link prediction; Predictive modeling; Reciprocal relationship; Social influence; Twitter; Social networking (online)
Special issue on best of SIGKDD 2011,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878613010&doi=10.1145%2f2382577.2382578&partnerID=40&md5=11fdaf760ef0cfcb58cb08d052fb6c59,[No abstract available],,
Substantial improvements in the set-covering projection classifier CHIRP (composite hypercubes on iterated random projections),2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878562629&doi=10.1145%2f2382577.2382583&partnerID=40&md5=83989a20cf47a4856490fa2218a2593b,"In Wilkinson et al. [2011] we introduced a new set-covering random projection classifier that achieved average error lower than that of other classifiers in the Weka platform. This classifier was based on an L∞ norm distance function and exploited an iterative sequence of three stages (projecting, binning, and covering) to deal with the curse of dimensionality, computational complexity, and nonlinear separability. We now present substantial changes that improve robustness and reduce training and testing time by almost an order of magnitude without jeopardizing CHIRP's outstanding error performance. © 2012 ACM.",Random projections; Supervised classification,Data mining; Curse of dimensionality; Distance functions; Error performance; Iterative sequences; Nonlinear separability; Random projections; Supervised classification; Training and testing; Computer science
Summarizing data succinctly with the most informative itemsets,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878575950&doi=10.1145%2f2382577.2382580&partnerID=40&md5=3de4631694d87ebbe8181201b2e83a28,"Knowledge discovery from data is an inherently iterative process. That is, what we know about the data greatly determines our expectations, and therefore, what results we would find interesting and/or surprising. Given new knowledge about the data, our expectations will change. Hence, in order to avoid redundant results, knowledge discovery algorithms ideally should follow such an iterative updating procedure. With this in mind, we introduce a well-founded approach for succinctly summarizing data with the most informative itemsets; using a probabilistic maximum entropy model, we iteratively find the itemset that provides us the most novel information-that is, for which the frequency in the data surprises us the most- and in turn we update our model accordingly. As we use the maximum entropy principle to obtain unbiased probabilistic models, and only include those itemsets that are most informative with regard to the current model, the summaries we construct are guaranteed to be both descriptive and nonredundant. The algorithm that we present, called MTV, can either discover the top-k most informative itemsets, or we can employ either the Bayesian Information Criterion (BIC) or the Minimum Description Length (MDL) principle to automatically identify the set of itemsets that together summarize the data well. In other words, our method will ""tell you what you need to know"" about the data. Importantly, it is a one-phase algorithm: rather than picking itemsets from a user-provided candidate set, itemsets and their supports are mined on-the-fly. To further its applicability, we provide an efficient method to compute the maximum entropy distribution using Quick Inclusion-Exclusion. Experiments on our method, using synthetic, benchmark, and real data, show that the discovered summaries are succinct, and correctly identify the key patterns in the data. The models they form attain high likelihoods, and inspection shows that they summarize the data well with increasingly specific, yet nonredundant itemsets. © 2012 ACM.",BIC; Frequent itemsets; Inclusion-exclusion; Maximum entropy; MDL; Minimum description length principle; Pattern sets; Summarization,Algorithms; Information theory; Maximum entropy methods; Natural language processing systems; BIC; Inclusion-exclusion; Item sets; Maximum entropy; MDL; Minimum description length principle; Pattern set; Summarization; Iterative methods
Multisource domain adaptation and its application to early detection of fatigue,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878555728&doi=10.1145%2f2382577.2382582&partnerID=40&md5=a1eb8072b2157e44db674daacc8b949d,"We consider the characterization of muscle fatigue through a noninvasive sensing mechanism such as Surface ElectroMyoGraphy (SEMG). While changes in the properties of SEMG signals with respect to muscle fatigue have been reported in the literature, the large variation in these signals across different individuals makes the task of modeling and classification of SEMG signals challenging. Indeed, the variation in SEMG parameters from subject to subject creates differences in the data distribution. In this article, we propose two transfer learning frameworks based on the multisource domain adaptation methodology for detecting different stages of fatigue using SEMG signals, that addresses the distribution differences. In the proposed frameworks, the SEMG data of a subject represent a domain; data from multiple subjects in the training set form the multiple source domains and the test subject data form the target domain. SEMG signals are predominantly different in conditional probability distribution across subjects. The key feature of the first framework is a novel weighting scheme that addresses the conditional probability distribution differences across multiple domains (subjects) and the key feature of the second framework is a two-stage domain adaptation methodology which combines weighted data from multiple sources based on marginal probability differences (first stage) as well as conditional probability differences (second stage), with the target domain data. The weights for minimizing the marginal probability differences are estimated independently, while the weights for minimizing conditional probability differences are computed simultaneously by exploiting the potential interaction among multiple sources. We also provide a theoretical analysis on the generalization performance of the proposed multisource domain adaptation formulation using the weighted Rademacher complexity measure. We have validated the proposed frameworks on Surface ElectroMyoGram signals collected from 8 people during a fatigue-causing repetitive gripping activity. Comprehensive experiments on the SEMG dataset demonstrate that the proposed method improves the classification accuracy by 20% to 30% over the cases without any domain adaptation method and by 13% to 30% over existing state-of-the-art domain adaptation methods. © 2012 ACM.",Multisource domain adaption; Subject-based variability; Surface electromyogram; Transfer learning,Classification (of information); Electromyography; Muscle; Probability distributions; Conditional probabilities; Conditional probability distributions; Domain adaptions; Generalization performance; Subject-based variability; Surface electromyogram; Surface electromyography; Transfer learning; Signal detection
Triangle listing in massive networks,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866038305&doi=10.1145%2f2382577.2382581&partnerID=40&md5=2687cdfa7a1549c86f2f1fe6d67bcaac,"Triangle listing is one of the fundamental algorithmic problems whose solution has numerous applications especially in the analysis of complex networks, such as the computation of clustering coefficients, transitivity, triangular connectivity, trusses, etc. Existing algorithms for triangle listing are mainly in-memory algorithms, whose performance cannot scale with the massive volume of today's fast growing networks. When the input graph cannot fit in main memory, triangle listing requires random disk accesses that can incur prohibitively huge I/O cost. Some streaming, semistreaming, and sampling algorithms have been proposed but these are approximation algorithms. We propose an I/O-efficient algorithm for triangle listing. Our algorithm is exact and avoids random disk access. Our results show that our algorithm is scalable and outperforms the state-of-the-art in-memory and local triangle estimation algorithms. © 2012 ACM.",Clustering coefficients; Large graphs; Massive networks; Triangle counting; Triangle listing,Computer science; Data mining; Clustering coefficient; Large graphs; Massive networks; Triangle counting; Triangle listing; Approximation algorithms
"Leakage in data mining: Formulation, detection, and avoidance",2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878552624&doi=10.1145%2f2382577.2382579&partnerID=40&md5=08730c3bad82e5f663e1f9a6914d3761,"Deemed ""one of the top ten data mining mistakes"", leakage is the introduction of information about the data mining target that should not be legitimately available to mine from. In addition to our own industry experience with real-life projects, controversies around several major public data mining competitions held recently such as the INFORMS 2010 Data Mining Challenge and the IJCNN 2011 Social Network Challenge are evidence that this issue is as relevant today as it has ever been. While acknowledging the importance and prevalence of leakage in both synthetic competitions and real-life data mining projects, existing literature has largely left this idea unexplored. What little has been said turns out not to be broad enough to cover more complex cases of leakage, such as those where the classical independently and identically distributed (i.i.d.) assumption is violated, that have been recently documented. In our new approach, these cases and others are explained by explicitly defining modeling goals and analyzing the broader framework of the data mining problem. The resulting definition enables us to derive general methodology for dealing with the issue. We show that it is possible to avoid leakage with a simple specific approach to data management followed by what we call a learn-predict separation, and present several ways of detecting leakage when the modeler has no control over how the data have been collected. We also offer an alternative point of view on leakage that is based on causal graph modeling concepts. © 2012 ACM.",Data mining; Leakage; Predictive modeling,Information management; Leakage (fluid); Data mining problems; General methodologies; Independently and identically distributed; Industry experience; New approaches; Point of views; Predictive modeling; Social Networks; Data mining
CIForager: Incrementally discovering regions of correlated change in evolving graphs,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869415360&doi=10.1145%2f2362383.2362385&partnerID=40&md5=dbc4039e85232147b386d1b067e88d97,"Data mining techniques for understanding how graphs evolve over time have become increasingly important. Evolving graphs arise naturally in diverse applications such as computer network topologies, multiplayer games and medical imaging. A natural and interesting problem in evolving graph analysis is the discovery of compact subgraphs that change in a similar manner. Such subgraphs are known as regions of correlated change and they can both summarise change patterns in graphs and help identify the underlying events causing these changes. However, previous techniques for discovering regions of correlated change suffer from limited scalability, making them unsuitable for analysing the evolution of very large graphs. In this paper, we introduce a new algorithm called ciForager, that addresses this scalability challenge and offers considerable improvements. The efficiency of ciForager is based on the use of new incremental techniques for detecting change, as well as the use of Voronoi representations for efficiently determining distance. We experimentally show that ciForager can achieve speedups of up to 1000 times over previous approaches. As a result, it becomes feasible for the first time to discover regions of correlated change in extremely large graphs, such as the entire BGP routing topology of the Internet. © 2012 ACM.",Connected components; Correlated change; Dynamic graph mining; Fault detection; Shortest path distance,Data mining; Electric network topology; Fault detection; Graphic methods; Medical imaging; Scalability; Change patterns; Computer network topologies; Connected component; Correlated change; Data mining techniques; Diverse applications; Dynamic graph; Evolving graphs; Incremental techniques; Large graphs; Multiplayer games; Routing topology; Shortest path; Subgraphs; Voronoi; Topology
Comparative document summarization via discriminative sentence selection,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869390620&doi=10.1145%2f2362383.2362386&partnerID=40&md5=48f7c24cfb9fc7de71767a2a62dfe519,"Given a collection of document groups, a natural question is to identify the differences among them. Although traditional document summarization techniques can summarize the content of the document groups one by one, there exists a great necessity to generate a summary of the differences among the document groups. In this article, we study a novel problem, that of summarizing the differences between document groups. A discriminative sentence selection method is proposed to extract the most discriminative sentences which represent the specific characteristics of each document group. Experiments and case studies on real-world data sets demonstrate the effectiveness of our proposed method. © 2012 ACM.",Comparative document summarization; Discriminative sentence selection,Computer science; Data mining; Collection of documents; Document summarization; Experiments and case studies; Real world data; Sentence selection; Virtual reality
Learning Bayesian networks from Markov random fields: An efficient algorithm for linear models,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869412863&doi=10.1145%2f2362383.2362384&partnerID=40&md5=ffd7b7c72f4781acfbe54a3737ed1eab,"Dependency analysis is a typical approach for Bayesian network learning, which infers the structures of Bayesian networks by the results of a series of conditional independence (CI) tests. In practice, testing independence conditioning on large sets hampers the performance of dependency analysis algorithms in terms of accuracy and running time for the following reasons. First, testing independence on large sets of variables with limited samples is not stable. Second, for most dependency analysis algorithms, the number of CI tests grows at an exponential rate with the sizes of conditioning sets, and the running time grows of the same rate. Therefore, determining how to reduce the number of CI tests and the sizes of conditioning sets becomes a critical step in dependency analysis algorithms. In this article, we address a two-phase algorithm based on the observation that the structures of Markov random fields are similar to those of Bayesian networks. The first phase of the algorithm constructs a Markov random field from data, which provides a close approximation to the structure of the true Bayesian network; the second phase of the algorithm removes redundant edges according to CI tests to get the true Bayesian network. Both phases use Markov blanket information to reduce the sizes of conditioning sets and the number of CI tests without sacrificing accuracy. An empirical study shows that the two-phase algorithm performs well in terms of accuracy and efficiency. © 2012 ACM.",Bayesian networks; Causal modeling; Graphical models,Approximation algorithms; Image segmentation; Testing; Bayesian network learning; Causal modeling; Close approximation; Conditional independences; Critical steps; Dependency analysis; Empirical studies; Exponential rates; GraphicaL model; Learning Bayesian networks; Markov Blankets; Markov Random Fields; Running time; Second phase; Two phase algorithm; Bayesian networks
Forecasting in the NBA and other team sports: Network effects in action,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869445659&doi=10.1145%2f2362383.2362387&partnerID=40&md5=c728763fe6c8bb838280c8894e7b703d,"The multi-million sports-betting market is based on the fact that the task of predicting the outcome of a sports event is very hard. Even with the aid of an uncountable number of descriptive statistics and background information, only a few can correctly guess the outcome of a game or a league. In this work, our approach is to move away from the traditional way of predicting sports events, and instead to model sports leagues as networks of players and teams where the only information available is the work relationships among them. We propose two network-based models to predict the behavior of teams in sports leagues. These models are parameter-free, that is, they do not have a single parameter, and moreover are sport-agnostic: they can be applied directly to any team sports league. First, we view a sports league as a network in evolution, and we infer the implicit feedback behind network changes and properties over the years. Then, we use this knowledge to construct the network-based prediction models, which can, with a significantly high probability, indicate how well a team will perform over a season. We compare our proposed models with other prediction models in two of the most popular sports leagues: the National Basketball Association (NBA) and the Major League Baseball (MLB). Our model shows consistently good results in comparison with the other models and, relying upon the network properties of the teams, we achieved a ≈ 14% rank prediction accuracy improvement over our best competitor. © 2012 ACM.",Complex networks; Social networks; Sports analytics,Forecasting; Mathematical models; Recreational facilities; Background information; Complex networks; Descriptive statistics; High probability; Implicit feedback; National basketball associations; Network effects; Network properties; Network-based; Prediction accuracy; Prediction model; Single parameter; Social Networks; Sports events; SportS
Cross-guided clustering: Transfer of relevant supervision across tasks,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866333225&doi=10.1145%2f2297456.2297461&partnerID=40&md5=4dd2cd0197ebd59a0bca41fd4164ceb2,"Lack of supervision in clustering algorithms often leads to clusters that are not useful or interesting to human reviewers. We investigate if supervision can be automatically transferred for clustering a target task, by providing a relevant supervised partitioning of a dataset from a different source task. The target clustering is made more meaningful for the human user by trading-off intrinsic clustering goodness on the target task for alignment with relevant supervised partitions in the source task, wherever possible. We propose a cross-guided clustering algorithm that builds on traditional k-means by aligning the target clusters with source partitions. The alignment process makes use of a cross-task similarity measure that discovers hidden relationships across tasks. When the source and target tasks correspond to different domains with potentially different vocabularies, we propose a projection approach using pivot vocabularies for the cross-domain similarity measure. Using multiple real-world and synthetic datasets, we show that our approach improves clustering accuracy significantly over traditional k-means and state-of-the-art semi-supervised clustering baselines, over a wide range of data characteristics and parameter settings. © 2012 ACM.",Cluster alignment; Multitask; Transfer,Alignment; Clustering accuracy; Cross-domain; Data characteristics; Data sets; Different domains; Human users; K-means; Multitask; Parameter setting; Semi-supervised Clustering; Similarity measure; Synthetic datasets; Transfer; Clustering algorithms
A sequential sampling framework for spectral κ-means based on efficient bootstrap accuracy estimations: Application to distributed clustering,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866338646&doi=10.1145%2f2297456.2297457&partnerID=40&md5=9ab7d7ad6a7521f4529e412de8bb4172,"The scalability of learning algorithms has always been a central concern for data mining researchers, and nowadays, with the rapid increase in data storage capacities and availability, its importance has increased. To this end, sampling has been studied by several researchers in an effort to derive sufficiently accurate models using only small data fractions. In this article we focus on spectral κ-means, that is, the κ-means approximation as derived by the spectral relaxation, and propose a sequential sampling framework that iteratively enlarges the sample size until the κ-means results (objective function and cluster structure) become indistinguishable from the asymptotic (infinite-data) output. In the proposed framework we adopt a commonly applied principle in data mining research that considers the use of minimal assumptions concerning the data generating distribution. This restriction imposes several challenges, mainly related to the efficiency of the sequential sampling procedure. These challenges are addressed using elements of matrix perturbation theory and statistics. Moreover, although the main focus is on spectral κ-means, we also demonstrate that the proposed framework can be generalized to handle spectral clustering. The proposed sequential sampling framework is consecutively employed for addressing the distributed clustering problem, where the task is to construct a global model for data that resides in distributed network nodes. The main challenge in this context is related to the bandwidth constraints that are commonly imposed, thus requiring that the distributed clustering algorithm consumes a minimal amount of network load. This illustrates the applicability of the proposed approach, as it enables the determination of aminimal sample size that can be used for constructing an accurate clustering model that entails the distributional characteristics of the data. As opposed to the relevant distributed κ-means approaches, our framework takes into account the fact that the choice of the number of clusters has a crucial effect on the required amount of communication. More precisely, the proposed algorithm is able to derive a statistical estimation of the required relative sizes for all possible values of κ. This unique feature of our distributed clustering framework enables a network administrator to choose an economic solution that identifies the crude cluster structure of a dataset and not devote excessive network resources for identifying all the ""correct"" detailed clusters. © 2012 ACM.",Asymptotic convergence; Bootstrapping; Clustering; Distributed clustering; Matrix perturbation theory; Sampling; Spectral,Clustering algorithms; Data mining; Learning algorithms; Research; Sampling; Asymptotic convergence; Bootstrapping; Clustering; Distributed clustering; Matrix perturbation theory; Spectral; Perturbation techniques
The latent maximum entropy principle,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866340966&doi=10.1145%2f2297456.2297460&partnerID=40&md5=8a5b8a8ba2651e57b12a0b2ef31878ed,"We present an extension to Jaynes' maximum entropy principle that incorporates latent variables. The principle of latent maximum entropy we propose is different from both Jaynes' maximum entropy principle and maximum likelihood estimation, but can yield better estimates in the presence of hidden variables and limited training data. We first show that solving for a latent maximum entropy model poses a hard nonlinear constrained optimization problem in general. However, we then show that feasible solutions to this problem can be obtained efficiently for the special case of log-linear models-which forms the basis for an efficient approximation to the latent maximum entropy principle. We derive an algorithm that combines expectation-maximization with iterative scaling to produce feasible log-linear solutions. This algorithm can be interpreted as an alternating minimization algorithm in the information divergence, and reveals an intimate connection between the latent maximum entropy and maximum likelihood principles. To select a final model, we generate a series of feasible candidates, calculate the entropy of each, and choose the model that attains the highest entropy. Our experimental results show that estimation based on the latent maximum entropy principle generally gives better results than maximum likelihood when estimating latent variable models on small observed data samples. © 2012 ACM.",Expectation maximization; Information geometry; Iterative scaling; Latent variable models; Maximum entropy,Algorithms; Entropy; Equivalence classes; Estimation; Maximum likelihood estimation; Regression analysis; Expectation Maximization; Information geometry; Iterative scaling; Latent variable models; Maximum entropy; Maximum principle
A model for information growth in collective wisdom processes,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866339901&doi=10.1145%2f2297456.2297458&partnerID=40&md5=b429f91087383f3ba3380a46c9359c28,"Collaborative media such as wikis have become enormously successful venues for information creation. Articles accrue information through the asynchronous editing of users who arrive both seeking information and possibly able to contribute information. Most articles stabilize to high-quality, trusted sources of information representing the collective wisdom of all the users who edited the article. We propose a model for information growth which relies on two main observations: (i) as an article's quality improves, it attracts visitors at a faster rate (a rich-get-richer phenomenon); and, simultaneously, (ii) the chances that a new visitor will improve the article drops (there is only so much that can be said about a particular topic). Our model is able to reproduce many features of the edit dynamics observed on Wikipedia; in particular, it captures the observed rise in the edit rate, followed by 1/t decay. Despite differences in the media, we also document similar features in the comment rates for a segment of the LiveJournal blogosphere. © 2012 ACM.",Collective intelligence; Dynamical systems; Social networks,Dynamical systems; Blogospheres; Collective intelligences; Faster rates; High quality; Information creation; Social Networks; Wikipedia; Websites
Generative models for evolutionary clustering,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866341270&doi=10.1145%2f2297456.2297459&partnerID=40&md5=42cf73a30247132737d4ae46aea92c04,"This article studies evolutionary clustering, a recently emerged hot topic with many important applications, noticeably in dynamic social network analysis. In this article, based on the recent literature on nonparametric Bayesian models, we have developed two generative models: DPChain and HDP-HTM. DPChain is derived from the Dirichlet process mixture (DPM) model, with an exponential decaying component along with the time. HDP-HTM combines the hierarchical dirichlet process (HDP) with a hierarchical transition matrix (HTM) based on the proposed Infinite hierarchical Markov state model (iHMS). Both models substantially advance the literature on evolutionary clustering, in the sense that not only do they both perform better than those in the existing literature, but more importantly, they are capable of automatically learning the cluster numbers and explicitly addressing the corresponding issues. Extensive evaluations have demonstrated the effectiveness and the promise of these two solutions compared to the state-of-the-art literature. © 2012 ACM.",DPChain; Evolutionary clustering; HDP-HTM; Hierarchical transition matrix; IHMS,Bayesian networks; DPChain; Evolutionary clustering; HDP-HTM; Hierarchical transition; IHMS; Social networking (online)
From context to distance: Learning dissimilarity for categorical data clustering,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859378493&doi=10.1145%2f2133360.2133361&partnerID=40&md5=fe53c985fd8adec23c2b0a863dcab1d0,"Clustering data described by categorical attributes is a challenging task in data mining applications. Unlike numerical attributes, it is difficult to define a distance between pairs of values of a categorical attribute, since the values are not ordered. In this article, we propose a framework to learn a context-based distance for categorical attributes. The key intuition of this work is that the distance between two values of a categorical attribute A i can be determined by the way in which the values of the other attributes A j are distributed in the dataset objects: if they are similarly distributed in the groups of objects in correspondence of the distinct values of A i a low value of distance is obtained. We propose also a solution to the critical point of the choice of the attributes A j. We validate our approach by embedding our distance learning framework in a hierarchical clustering algorithm. We applied it on various real world and synthetic datasets, both low and high-dimensional. Experimental results show that our method is competitive with respect to the state of the art of categorical data clustering approaches. We also show that our approach is scalable and has a low impact on the overall computational time of a clustering task. © 2012 ACM 1556-4681/2012/03-ART1 $10.00.",Algorithms; Theory,Algorithms; Distance education; Categorical attributes; Categorical data clustering; Clustering data; Computational time; Context-based; Critical points; Data mining applications; Data sets; Hierarchical clustering algorithms; High-dimensional; Numerical attributes; State of the art; Synthetic datasets; Theory; Clustering algorithms
Large linear classification when data cannot fit in memory,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863266107&doi=10.1145%2f2086737.2086743&partnerID=40&md5=a76c760e9221a1a1ff97504dfa6a0f45,"Recent advances in linear classification have shown that for applications such as document classification, the training process can be extremely efficient. However, most of the existing training methods are designed by assuming that data can be stored in the computer memory. These methods cannot be easily applied to data larger than the memory capacity due to the random access to the disk. We propose and analyze a block minimization framework for data larger than the memory size. At each step a block of data is loaded from the disk and handled by certain learning methods. We investigate two implementations of the proposed framework for primal and dual SVMs, respectively. Because data cannot fit in memory, many design considerations are very different from those for traditional algorithms. We discuss and compare with existing approaches that are able to handle data larger than memory. Experiments using data sets 20 times larger than the memory demonstrate the effectiveness of the proposed method. © 2012 ACM.",Block minimization methods; Large-scale learning; Linear classification; Support vector machines,Information retrieval systems; Support vector machines; Computer memories; Data sets; Design considerations; Document Classification; Large-scale learning; Learning methods; Linear classification; Memory capacity; Memory size; Minimization methods; Random access; Support vector; Training methods; Training process; Computers
A modular machine learning system for flow-level traffic classification in large networks,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859393329&doi=10.1145%2f2133360.2133364&partnerID=40&md5=bd886179bc1599f2b930763d051fb631,"The ability to accurately and scalably classify network traffic is of critical importance to a wide range of management tasks of large networks, such as tier-1 ISP networks and global enterprise networks. Guided by the practical constraints and requirements of traffic classification in large networks, in this article, we explore the design of an accurate and scalable machine learning based flow-level traffic classification system, which is trained on a dataset of flow-level data that has been annotated with application protocol labels by a packet-level classifier. Our system employs a lightweight modular architecture, which combines a series of simple linear binary classifiers, each of which can be efficiently implemented and trained on vast amounts of flow data in parallel, and embraces three key innovative mechanisms, weighted threshold sampling, logistic calibration, and intelligent data partitioning, to achieve scalability while attaining high accuracy. Evaluations using real traffic data from multiple locations in a large ISP show that our system accurately reproduces the labels of the packet level classifier when runs on (unlabeled) flow records, while meeting the scalability and stability requirements of large ISP networks. Using training and test datasets that are two months apart and collected from two different locations, the flow error rates are only 3% for TCP flows and 0.4% for UDP flows. We further show that such error rates can be reduced by combining the information of spatial distributions of flows, or collective traffic statistics, during classification. We propose a novel two-step model, which seamlessly integrates these collective traffic statistics into the existing traffic classification system. Experimental results display performance improvement on all traffic classes and an overall error rate reduction by 15%. In addition to a high accuracy, at runtime, our implementation easily scales to classify traffic on 10Gbps links. © 2012 ACM 1556-4681/2012/03-ART4 $10.00.",Algorithms; Measurement; Theory,Algorithms; Learning systems; Measurements; Scalability; Traffic surveys; Transmission control protocol; Application protocols; Data sets; Display performance; Error rate; Error rate reduction; Flow data; Flow-level; Global enterprise networks; Intelligent data; Large networks; Linear binary classifiers; Management tasks; Modular architectures; Modular machines; Network traffic; Packet level; Real traffic; Runtimes; Stability requirements; TCP flows; Theory; Traffic class; Traffic classification; Traffic statistics; TWo-step model; UDP flows; Telecommunication traffic
Efficient mining of gap-constrained subsequences and its various applications,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859393676&doi=10.1145%2f2133360.2133362&partnerID=40&md5=581dc99592a1db8d748e1380ef03f075,"Mining frequent subsequence patterns is a typical data-mining problem and various efficient sequential pattern mining algorithms have been proposed. In many application domains (e.g., biology), the frequent subsequences confined by the predefined gap requirements are more meaningful than the general sequential patterns. In this article, we propose two algorithms, Gap-BIDE for mining closed gap-constrained subsequences from a set of input sequences, and Gap-Connect for mining repetitive gap-constrained subsequences from a single input sequence. Inspired by some state-of-the-art closed or constrained sequential pattern mining algorithms, the Gap-BIDE algorithm adopts an efficient approach to finding the complete set of closed sequential patterns with gap constraints, while the Gap-Connect algorithm efficiently mines an approximate set of long patterns by connecting short patterns. We also present several methods for feature selection from the set of gap-constrained patterns for the purpose of classification and clustering. Our extensive performance study shows that our approaches are very efficient in mining frequent subsequences with gap constraints, and the gap-constrained pattern based classification/clustering approaches can achieve high-quality results. © 2012 ACM 1556-4681/2012/03-ART2 $10.00.",Algorithms,Data mining; Intelligent networks; Application domains; Classification and clustering; Classification/clustering; Closed sequential patterns; Data mining problems; High quality; Input sequence; Performance study; Sequential pattern mining algorithm; Sequential patterns; Single input; Algorithms
Isolation-based anomaly detection,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859412430&doi=10.1145%2f2133360.2133363&partnerID=40&md5=76bde6c1ef9df63ae25bb121df2b9cd1,"Anomalies are data points that are few and different. As a result of these properties, we show that, anomalies are susceptible to a mechanism called isolation. This article proposes a method called Isolation Forest (iForest), which detects anomalies purely based on the concept of isolation without employing any distance or density measure-fundamentally different from all existing methods. As a result, iForest is able to exploit subsampling (i) to achieve a low linear time-complexity and a small memory-requirement and (ii) to deal with the effects of swamping and masking effectively. Our empirical evaluation shows that iForest outperforms ORCA, one-class SVM, LOF and Random Forests in terms of AUC, processing time, and it is robust against masking and swamping effects. iForest also works well in high dimensional problems containing a large number of irrelevant attributes, and when anomalies are not available in training sample. © 2012 ACM 1556-4681/2012/03-ART3 $10.00.",Algorithms; Design; Experimentation,Algorithms; Decision Making; Design; Forestry; Algorithms; Decision trees; Design; Anomaly detection; Data points; Empirical evaluations; Experimentation; High-dimensional problems; One class-SVM; Processing time; Random forests; Training sample; Forestry
Sequential modeling of topic dynamics with multiple timescales,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857693080&doi=10.1145%2f2086737.2086739&partnerID=40&md5=08ac07a7161e785186b89672c8f3ef6f,"We propose an online topic model for sequentially analyzing the time evolution of topics in document collections. Topics naturally evolve with multiple timescales. For example, some words may be used consistently over one hundred years, while other words emerge and disappear over periods of a few days. Thus, in the proposed model, current topic-specific distributions over words are assumed to be generated based on the multiscale word distributions of the previous epoch. Considering both the long- and short-timescale dependency yields a more robust model. We derive efficient online inference procedures based on a stochastic EM algorithm, in which the model is sequentially updated using newly obtained data; this means that past data are not required to make the inference. We demonstrate the effectiveness of the proposed method in terms of predictive performance and computational efficiency by examining collections of real documents with timestamps. © 2012 ACM.",Multiscale; Online learning; Time-series analysis; Topic model,Algorithms; Harmonic analysis; Document collection; EM algorithms; Multiple timescales; Multiscales; Online learning; Predictive performance; Robust models; Sequential modeling; Time evolutions; Time stamps; Topic model; Stochastic models
Discriminative topic modeling based on manifold learning,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857717020&doi=10.1145%2f2086737.2086740&partnerID=40&md5=ad932bc2dc6f36214f030e5a1b4fdffd,"Topic modeling has become a popular method used for data analysis in various domains including text documents. Previous topic model approaches, such as probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA), have shown impressive success in discovering low-rank hidden structures for modeling text documents. These approaches, however do not take into account the manifold structure of the data, which is generally informative for nonlinear dimensionality reduction mapping. More recent topic model approaches, Laplacian PLSI (LapPLSI) and Locally-consistent Topic Model (LTM), have incorporated the local manifold structure into topic models and have shown resulting benefits. But they fall short of achieving full discriminating power of manifold learning as they only enhance the proximity between the low-rank representations of neighboring pairs without any consideration for non-neighboring pairs. In this article, we propose a new approach, Discriminative Topic Model (DTM), which separates non-neighboring pairs from each other in addition to bringing neighboring pairs closer together, thereby preserving the global manifold structure as well as improving local consistency. We also present a novel model-fitting algorithm based on the generalized EM algorithm and the concept of Pareto improvement. We empirically demonstrate the success of DTM in terms of unsupervised clustering and semisupervised classification accuracies on text corpora and robustness to parameters compared to state-of-the-art techniques. © 2012 ACM.",Dimensionality reduction; Document clustering and classification; Semisupervised learning; Topic modeling,Character recognition; Statistics; Dimensionality reduction; Discriminating power; Document Clustering; Generalized EM algorithms; Hidden structures; Laplacians; Latent dirichlet allocations; Local consistency; Local manifold structure; Manifold learning; Model fitting; Nonlinear dimensionality reduction; Pareto improvements; Probabilistic latent semantic analysis; Semi-supervised classification; Semi-supervised learning; Text corpora; Text document; Topic model; Unsupervised clustering; Text processing
Connecting two (or less) dots: Discovering structure in news articles,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857764188&doi=10.1145%2f2086737.2086744&partnerID=40&md5=acf88ea8797a3b2b14ebf445a430668f,"Finding information is becoming a major part of our daily life. Entire sectors, from Web users to scientists and intelligence analysts, are increasingly struggling to keep up with the larger and larger amounts of content published every day. With this much data, it is often easy to miss the big picture. In this article, we investigate methods for automatically connecting the dots-providing a structured, easy way to navigate within a new topic and discover hidden connections. We focus on the news domain: given two news articles, our systemautomatically finds a coherent chain linking themtogether. For example, it can recover the chain of events starting with the decline of home prices (January 2007), and ending with the health care debate (2009). We formalize the characteristics of a good chain and provide a fast search-driven algorithm to connect two fixed endpoints. We incorporate user feedback into our framework, allowing the stories to be refined and personalized. We also provide a method to handle partially-specified endpoints, for users who do not know both ends of a story. Finally, we evaluate our algorithm over real news data. Our user studies demonstrate that the objective we propose captures the users' intuitive notion of coherence, and that our algorithm effectively helps users understand the news. © 2012 ACM.",Coherence,Coherent light; Health care; Daily lives; Intelligence analysts; News articles; News domain; User feedback; User study; Web users; Algorithms
Learning incoherent sparse and low-rank patterns from multiple tasks,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863232691&doi=10.1145%2f2086737.2086742&partnerID=40&md5=9720fde163fdd77c6006e8d48c64280d,"We consider the problem of learning incoherent sparse and low-rank patterns from multiple tasks. Our approach is based on a linear multitask learning formulation, in which the sparse and low-rank patterns are induced by a cardinality regularization term and a low-rank constraint, respectively. This formulation is nonconvex; we convert it into its convex surrogate, which can be routinely solved via semidefinite programming for small-size problems. We propose employing the general projected gradient scheme to efficiently solve such a convex surrogate; however, in the optimization formulation, the objective function is nondifferentiable and the feasible domain is nontrivial. We present the procedures for computing the projected gradient and ensuring the global convergence of the projected gradient scheme. The computation of the projected gradient involves a constrained optimization problem; we show that the optimal solution to such a problem can be obtained via solving an unconstrained optimization subproblem and a Euclidean projection subproblem. We also present two projected gradient algorithms and analyze their rates of convergence in detail. In addition, we illustrate the use of the presented projected gradient algorithms for the proposed multitask learning formulation using the least squares loss. Experimental results on a collection of real-world data sets demonstrate the effectiveness of the proposed multitask learning formulation and the efficiency of the proposed projected gradient algorithms. © 2012 ACM.",Low-rank and sparse patterns; Multitask learning; Trace norm,Constrained optimization; Mathematical programming; Virtual reality; Cardinalities; Constrained optimization problems; Euclidean; Global convergence; Least Square; Low-rank and sparse patterns; Multiple tasks; Multitask learning; Non-differentiable; Nonconvex; Objective functions; Optimal solutions; Optimization formulations; Projected gradient; Rates of convergence; Real world data; Semi-definite programming; Trace-norms; Unconstrained optimization; Algorithms
Inferring networks of diffusion and influence,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857768001&doi=10.1145%2f2086737.2086741&partnerID=40&md5=1e86041f733befa9562f9902cc52134a,"Information diffusion and virus propagation are fundamental processes taking place in networks. While it is often possible to directly observe when nodes become infected with a virus or publish the information, observing individual transmissions (who infects whom, or who influences whom) is typically very difficult. Furthermore, in many applications, the underlying network over which the diffusions and propagations spread is actually unobserved. We tackle these challenges by developing a method for tracing paths of diffusion and influence through networks and inferring the networks over which contagions propagate. Given the times when nodes adopt pieces of information or become infected, we identify the optimal network that best explains the observed infection times. Since the optimization problem is NP-hard to solve exactly, we develop an efficient approximation algorithm that scales to large datasets and finds provably near-optimal networks. We demonstrate the effectiveness of our approach by tracing information diffusion in a set of 170 million blogs and news articles over a one year period to infer how information flows through the online media space. We find that the diffusion network of news for the top 1,000 media sites and blogs tends to have a core-periphery structure with a small set of core media sites that diffuse information to the rest of the Web. These sites tend to have stable circles of influence with more general news media sites acting as connectors between them. © 2012 ACM.",Blogs; Information cascades; Meme-tracking; Networks of diffusion; News media; Social networks,Approximation algorithms; Blogs; Crack propagation; Diffusion; Internet; Optimization; Viruses; Core peripheries; Diffusion networks; Efficient approximation algorithms; Information diffusion; Information flows; Large datasets; Meme-tracking; News articles; News media; NP-hard; Online media; Optimal networks; Optimization problems; Social Networks; Underlying networks; Virus propagation; Computer viruses
Guest Editorial for special issue KDD'10,2012,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857761042&doi=10.1145%2f2086737.2086738&partnerID=40&md5=9407d1c8c1318c9e38105479bb8ea8a0,[No abstract available],,
Integrating Document clustering and multidocument summarization,2011,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051959804&doi=10.1145%2f1993077.1993078&partnerID=40&md5=8c3d8e5458e95590bfa012d2517c916a,"Document understanding techniques such as document clustering and multidocument summarization have been receiving much attention recently. Current document clustering methods usually represent the given collection of documents as a document-term matrix and then conduct the clustering process. Although many of these clustering methods can group the documents effectively, it is still hard for people to capture the meaning of the documents since there is no satisfactory interpretation for each document cluster. A straightforward solution is to first cluster the documents and then summarize each document cluster using summarization methods. However, most of the current summarization methods are solely based on the sentence-term matrix and ignore the context dependence of the sentences. As a result, the generated summaries lack guidance from the document clusters. In this article, we propose a new language model to simultaneously cluster and summarize documents by making use of both the document-term and sentenceterm matrices. By utilizing the mutual influence of document clustering and summarization, our method makes; (1) a better document clustering method with more meaningful interpretation; and (2) an effective document summarization method with guidance from document clustering. Experimental results on various document datasets show the effectiveness of our proposed method and the high interpretability of the generated summaries. © 2011 ACM.",Document clustering; Multidocument summarization; Nonnegative matrix factorization with given bases,Abstracting; Clustering algorithms; Computational linguistics; Information retrieval; Matrix algebra; Clustering methods; Clustering process; Collection of documents; Context dependences; Document Clustering; Document datasets; Document summarization; Interpretability; Language model; matrix; Multi-document summarization; Nonnegative matrix factorization; Cluster analysis
Can the utility of anonymized data be used for privacy breaches?,2011,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052013720&doi=10.1145%2f1993077.1993080&partnerID=40&md5=7bd4290055aa9bd225826ebfeda8acd9,"Group based anonymization is the most widely studied approach for privacy-preserving data publishing. Privacy models/definitions using group based anonymization includes k-anonymity, ℓ-diversity, and t-closeness, to name a few. The goal of this article is to raise a fundamental issue regarding the privacy exposure of the approaches using group based anonymization. This has been overlooked in the past. The group based anonymization approach by bucketization basically hides each individual record behind a group to preserve data privacy. If not properly anonymized, patterns can actually be derived from the published data and be used by an adversary to breach individual privacy. For example, from the medical records released, if patterns such as that people from certain countries rarely suffer from some disease can be derived, then the information can be used to imply linkage of other people in an anonymized group with this disease with higher likelihood. We call the derived patterns from the published data the foreground knowledge. This is in contrast to the background knowledge that the adversary may obtain from other channels, as studied in some previous work. Finally, our experimental results show such an attack is realistic in the privacy benchmark dataset under the traditional group based anonymization approach. © 2011 ACM.",ℓ-diversity; Data publishing; K-anonymity; Privacy preservation,Anonymization; Background knowledge; Benchmark datasets; Data publishing; Group-based; Individual privacy; K-Anonymity; Medical record; Privacy breaches; Privacy models; Privacy preservation; Privacy preserving; T-closeness; Data privacy
Indexing network structure with shortest-path trees,2011,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051996890&doi=10.1145%2f1993077.1993079&partnerID=40&md5=cfc5eb028c782f1a4a5c7a04ca9144e0,"The ability to discover low-cost paths in networks has practical consequences for knowledge discovery and social network analysis tasks. Many analytic techniques for networks require finding low-cost paths, but exact methods for search become prohibitive for large networks, and data sets are steadily increasing in size. Short paths can be found efficiently by utilizing an index of network structure, which estimates network distances and enables rapid discovery of short paths. Through experiments on synthetic networks, we demonstrate that one such novel network structure index based on the shortest-path tree outperforms other previously proposed indices. We also show that it generalizes across arbitrarily weighted networks of various structures and densities, provides accurate estimates of distance, and has efficient time and space complexity. We present results on real data sets for several applications, including navigation, diameter estimation, centrality computation, and clustering-all made efficient by virtue of the network structure index. © 2011 ACM.",Knowledge discovery in graphs; Network structure index; Social network analysis; Weighted networks,Electric network analysis; Estimation; Plant extracts; Analytic technique; Data sets; Diameter estimation; Exact methods; Large networks; Network distance; Network structures; Real data sets; Short-path; Shortest-path; Social Network Analysis; Space complexity; Synthetic networks; Weighted networks; Cost benefit analysis
Community discovery via MetaGraph Factorization,2011,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052016150&doi=10.1145%2f1993077.1993081&partnerID=40&md5=2bdafb1459fe596646ad041c728a9523,"This work aims at discovering community structure in rich media social networks through analysis of timevarying, multirelational data. Community structure represents the latent social context of user actions. It has important applications such as search and recommendation. The problem is particularly useful in the enterprise domain, where extracting emergent community structure on enterprise social media can help in forming new collaborative teams, in expertise discovery, and in the long term reorganization of enterprises based on collaboration patterns. There are several unique challenges: (a) In social media, the context of user actions is constantly changing and coevolving; hence the social context contains time-evolving multidimensional relations. (b) The social context is determined by the available system features and is unique in each social media platform; hence the analysis of such data needs to flexibly incorporate various system features. In this article we propose MetaFac (MetaGraph Factorization), a framework that extracts community structures from dynamic, multidimensional social contexts and interactions. Our work has three key contributions: (1) metagraph, a novel relational hypergraph representation for modeling multirelational and multidimensional social data; (2) an efficient multirelational factorization method for community extraction on a given metagraph; (3) an online method to handle time-varying relations through incremental metagraph factorization. Extensive experiments on real-world social data collected from an enterprise and the public Digg social media Web site suggest that our technique is scalable and is able to extract meaningful communities from social media contexts. We illustrate the usefulness of our framework through two prediction tasks: (1) in the enterprise dataset, the task is to predict users' future interests on tag usage, and (2) in the Digg dataset, the task is to predict users' future interests in voting and commenting on Digg stories. Our prediction significantly outperforms baseline methods (including aspect model and tensor analysis), indicating the promising direction of using metagraphs for handling time-varying social relational contexts. © 2011 ACM.",Community discovery; Dynamic social network analysis; MetaFac; MetaGraph Factorization; Nonnegative tensor factorization; Relational hypergraph,Data mining; Electric network analysis; Forecasting; Industry; Social networking (online); Social sciences; Tensors; Time varying systems; User interfaces; Community discovery; Hypergraph; Meta-graph; MetaFac; Nonnegative tensor factorizations; Factorization
Temporal link prediction using matrix and tensor factorizations,2011,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952552980&doi=10.1145%2f1921632.1921636&partnerID=40&md5=afd9f1039436b2ddb0e1d52c0ac89f75,"The data in many disciplines such as social networks, Web analysis, etc. is link-based, and the link structure can be exploited for many different data mining tasks. In this article, we consider the problem of temporal link prediction: Given link data for times 1 through T, can we predict the links at time T +1? If our data has underlying periodic structure, can we predict out even further in time, i.e., links at time T + 2, T + 3, etc.? In this article, we consider bipartite graphs that evolve over time and consider matrix-and tensor-based methods for predicting future links. We present a weight-based method for collapsing multiyear data into a single matrix. We show how the well-known Katz method for link prediction can be extended to bipartite graphs and, moreover, approximated in a scalable way using a truncated singular value decomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we illustrate the usefulness of exploiting the natural three-dimensional structure of temporal link data. Through several numerical experiments, we demonstrate that both matrix-and tensor-based techniques are effective for temporal link prediction despite the inherent difficulty of the problem. Additionally, we show that tensor-based techniques are particularly effective for temporal data with varying periodic patterns. © 2011 ACM.",CANDECOMP; Evolution; Link mining; Link prediction; PARAFAC; Tensor factorization,Factorization; Graph theory; Periodic structures; Singular value decomposition; Tensors; Candecomp; Evolution; Link mining; Link prediction; PARAFAC; Tensor factorization; Forecasting
HADI: Mining radii of large graphs,2011,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952540632&doi=10.1145%2f1921632.1921634&partnerID=40&md5=81ba900e437279b143293fa2d2c6e453,"Given large, multimillion-node graphs (e.g., Facebook, Web-crawls, etc.), how do they evolve over time? How are they connected? What are the central nodes and the outliers? In this article we define the Radius plot of a graph and show how it can answer these questions. However, computing the Radius plot is prohibitively expensive for graphs reaching the planetary scale. There are two major contributions in this article: (a) We propose HADI (HAdoop Diameter and radii estimator), a carefully designed and fine-tuned algorithm to compute the radii and the diameter of massive graphs, that runs on the top of the Hadoop/MapReduce system, with excellent scale-up on the number of available machines (b) We run HADI on several real world datasets including YahooWeb (6B edges, 1/8 of a Terabyte), one of the largest public graphs ever analyzed. Thanks to HADI, we report fascinating patterns on large networks, like the surprisingly small effective diameter, the multimodal/bimodal shape of the Radius plot, and its palindrome motion over time. © 2011 ACM.",Graph mining; HADI; Hadoop; Radius plot; Small web,Graph mining; HADI; Hadoop; Radius plot; Small web; Graphic methods
Fast algorithms for approximating the singular value decomposition,2011,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952555116&doi=10.1145%2f1921632.1921639&partnerID=40&md5=18d318d1721c12b937727b9c64b820e0,"A low-rank approximation to a matrix A is a matrix with significantly smaller rank than A, and which is close to A according to some norm. Many practical applications involving the use of large matrices focus on low-rank approximations. By reducing the rank or dimensionality of the data, we reduce the complexity of analyzing the data. The singular value decomposition is the most popular low-rank matrix approximation. However, due to its expensive computational requirements, it has often been considered intractable for practical applications involving massive data. Recent developments have tried to address this problem, with several methods proposed to approximate the decomposition with better asymptotic runtime. We present an empirical study of these techniques on a variety of dense and sparse datasets. We find that a sampling approach of Drineas, Kannan and Mahoney is often, but not always, the best performing method. This method gives solutions with high accuracy much faster than classical SVD algorithms, on large sparse datasets in particular. Other modern methods, such as a recent algorithm by Rokhlin and Tygert, also offer savings compared to classical SVD algorithms. The older sampling methods of Achlioptas and McSherry are shown to sometimes take longer than classical SVD. © 2011 ACM.",Experimental evaluation; Low rank approximation; Singular value decomposition,Algorithms; Computational requirements; Data sets; Empirical studies; Experimental evaluation; Fast algorithms; Low rank approximation; Low rank approximations; Low-rank matrices; Massive data; matrix; Runtimes; Sampling method; Singular values; Singular value decomposition
Robust record linkage blocking using suffix arrays and bloom filters,2011,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952543891&doi=10.1145%2f1921632.1921635&partnerID=40&md5=1e90eda38fed91425e0ff67eab4ebcdd,"Record linkage is an important data integration task that has many practical uses for matching, merging and duplicate removal in large and diverse databases. However, quadratic scalability for the brute force approach of comparing all possible pairs of records necessitates the design of appropriate indexing or blocking techniques. The aim of these techniques is to cheaply remove candidate record pairs that are unlikely to match. We design and evaluate an eficient and highly scalable blocking approach based on sufix arrays. Our suffix grouping technique exploits the ordering used by the index to merge similar blocks at marginal extra cost, resulting in a much higher accuracy while retaining the high scalability of the base sufix array method. Efficiently grouping similar suffixes is carried out with the use of a sliding window technique. We carry out an in-depth analysis of our method and show results from experiments using real and synthetic data, which highlight the importance of using eficient indexing and blocking in real-world applications where datasets contain millions of records. We extend our disk-based methods with the capability to utilise main memory based storage to construct Bloom filters, which we have found to cause significant speedup by reducing the number of costly database queries by up to 70% in real data. We give practical implementation details and show how Bloom filters can be easily applied to Suffix Array based indexing. © 2011 ACM.",Blocking; Record linkage; Suffix arrays,Blooms (metal); Data structures; Indexing (of information); Query languages; Query processing; Scalability; Blocking; Blocking technique; Bloom filters; Brute-force approach; Data integration; Data sets; Database queries; Disk-based; Grouping technique; High scalabilities; In-depth analysis; Main memory; Practical implementation; Practical use; Real-world application; Record linkage; Sliding window techniques; Suffix arrays; Synthetic data; Data handling
Clustering large attributed graphs: A balance between structural and attribute similarities,2011,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952562794&doi=10.1145%2f1921632.1921638&partnerID=40&md5=10dedb55067211f723fc99dcd3932b8c,"Social networks, sensor networks, biological networks, and many other information networks can be modeled as a large graph. Graph vertices represent entities, and graph edges represent their relationships or interactions. In many large graphs, there is usually one or more attributes associated with every graph vertex to describe its properties. In many application domains, graph clustering techniques are very useful for detecting densely connected groups in a large graph as well as for understanding and visualizing a large graph. The goal of graph clustering is to partition vertices in a large graph into different clusters based on various criteria such as vertex connectivity or neighborhood similarity. Many existing graph clustering methods mainly focus on the topological structure for clustering, but largely ignore the vertex properties, which are often heterogenous. In this article, we propose a novel graph clustering algorithm, SA-Cluster, which achieves a good balance between structural and attribute similarities through a unified distance measure. Our method partitions a large graph associated with attributes into k clusters so that each cluster contains a densely connected subgraph with homogeneous attribute values. An effective method is proposed to automatically learn the degree of contributions of structural similarity and attribute similarity. Theoretical analysis is provided to show that SA-Cluster is converging quickly through iterative cluster refinement. Some optimization techniques on matrix computation are proposed to further improve the efficiency of SA-Cluster on large graphs. Extensive experimental results demonstrate the effectiveness of SA-Cluster through comparisons with the state-of-the-art graph clustering and summarization methods. © 2011 ACM.",Attribute similarity; Graph clustering; Structural proximity,Clustering algorithms; Information services; Matrix algebra; Natural language processing systems; Sensor networks; Application domains; Attribute similarity; Attribute values; Attributed graphs; Biological networks; Distance measure; Graph clustering; Graph edges; Graph vertex; Information networks; K cluster; Large graphs; Matrix computation; Optimization techniques; Social Networks; Structural proximity; Structural similarity; Subgraphs; Topological structure; Vertex connectivity; Graph theory
Introduction to special issue on large-scale data mining,2011,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952567356&doi=10.1145%2f1921632.1921633&partnerID=40&md5=f78385b2a43ff6746e5d1658dfab3665,[No abstract available],,
Enhancing clustering quality through landmark-based dimensionality reduction,2011,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952566833&doi=10.1145%2f1921632.1921637&partnerID=40&md5=d7532908d58912ae35c3f9e8873b1146,"Scaling up data mining algorithms for data of both high dimensionality and cardinality has been lately recognized as one of the most challenging problems in data mining research. The reason is that typical data mining tasks, such as clustering, cannot produce high quality results when applied on high-dimensional and/or large (in terms of cardinality) datasets. Data preprocessing and in particular dimensionality reduction constitute promising tools to deal with this problem. However, most of the existing dimensionality reduction algorithms share also the same disadvantages with data mining algorithms, when applied on large datasets of high dimensionality. In this article, we propose a fast and efficient dimensionality reduction algorithm (FEDRA), which is particularly scalable and therefore suitable for challenging datasets. FEDRA follows the landmark-based paradigm for embedding data objects in a low-dimensional projection space. By means of a theoretical analysis, we prove that FEDRA is efficient, while we demonstrate the achieved quality of results through experiments on datasets of higher cardinality and dimensionality than those employed in the evaluation of competitive algorithms. The obtained results prove that FEDRA manages to retain or ameliorate clustering quality while projecting in less than 10% of the initial dimensionality Moreover, our algorithm produces embeddings that enable the faster convergence of clustering algorithms Therefore, FEDRA emerges as a powerful and generic tool for data pre-processing, which can be integrated in other data mining algorithms, thus enhancing their performance. © 2011 ACM.",Clustering quality; Dimensionality reduction; Landmarks,Cluster analysis; Convergence of numerical methods; Data mining; Data processing; Cardinalities; Clustering quality; Competitive algorithms; Data mining algorithm; Data mining tasks; Data objects; Data preprocessing; Data sets; Dimensionality reduction; Dimensionality reduction algorithms; Embeddings; Faster convergence; High dimensionality; High quality; High-dimensional; Landmarks; Large datasets; Projection space; Quality of results; Scaling-up; Clustering algorithms
A combination approach to web user profiling,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650756423&doi=10.1145%2f1870096.1870098&partnerID=40&md5=b7ff980eae7313ab61bd4d85ecbc4871,"In this article, we study the problem of Web user profiling, which is aimed at finding, extracting, and fusing the ""semantic""-based user profile from the Web. Previously, Web user profiling was often undertaken by creating a list of keywords for the user, which is (sometimes even highly) insufficient for main applications. This article formalizes the profiling problem as several subtasks: profile extraction, profile integration, and user interest discovery. We propose a combination approach to deal with the profiling tasks. Specifically, we employ a classification model to identify relevant documents for a user from the Web and propose a Tree-Structured Conditional Random Fields (TCRF) to extract the profile information from the identified documents; we propose a unified probabilistic model to deal with the name ambiguity problem (several users with the same name) when integrating the profile information extracted from different sources; finally, we use a probabilistic topic model to model the extracted user profiles, and construct the user interest model. Experimental results on an online system show that the combination approach to different profiling tasks clearly outperforms several baseline methods. The extracted profiles have been applied to expert finding, an important application on the Web. Experiments show that the accuracy of expert finding can be improved (ranging from +6% to +26% in terms of MAP) by taking advantage of the profiles.",Information extraction; Name disambiguation; Social network; Text mining; Topic modeling; User profiling,Information retrieval systems; Integration; Models; Semantic Web; Semantics; Websites; Information Extraction; Name disambiguation; Social Networks; Text mining; Topic modeling; User profiling; Data mining
Modeling social annotation: A bayesian approach,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650751030&doi=10.1145%2f1870096.1870100&partnerID=40&md5=9f000a87b40330d5a1caaabcbbe03e62,"Collaborative tagging systems, such as Delicious, CiteULike, and others, allow users to annotate resources, for example, Web pages or scientific papers, with descriptive labels called tags. The social annotations contributed by thousands of users can potentially be used to infer categorical knowledge, classify documents, or recommend new relevant information. Traditional text inference methods do not make the best use of social annotation, since they do not take into account variations in individual users' perspectives and vocabulary. In a previous work, we introduced a simple probabilistic model that takes the interests of individual annotators into account in order to find hidden topics of annotated resources. Unfortunately, that approach had one major shortcoming: the number of topics and interests must be specified a priori. To address this drawback, we extend the model to a fully Bayesian framework, which offers a way to automatically estimate these numbers. In particular, the model allows the number of interests and topics to change as suggested by the structure of the data. We evaluate the proposed model in detail on the synthetic and real-world data by comparing its performance to Latent Dirichlet Allocation on the topic extraction task. For the latter evaluation, we apply the model to infer topics of Web resources from social annotations obtained from Delicious in order to discover new resources similar to a specified one. Our empirical results demonstrate that the proposed model is a promising method for exploiting social knowledge contained in user-generated annotations. © 2010 ACM.",Collaborative tagging; Probabilistic model; Resource discovery; Social annotation; Social information processing,Data processing; Labels; Multi agent systems; World Wide Web; Collaborative tagging; Probabilistic models; Resource discovery; Social annotation; Social information processing; Bayesian networks
A framework for computing the privacy scores of users in online social networks,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650743404&doi=10.1145%2f1870096.1870102&partnerID=40&md5=16ece857b708edef64daf92c37f3c598,"A large body of work has been devoted to address corporate-scale privacy concerns related to social networks. Most of this work focuses on how to share social networks owned by organizations without revealing the identities or the sensitive relationships of the users involved. Not much attention has been given to the privacy risk of users posed by their daily information-sharing activities. In this article, we approach the privacy issues raised in online social networks from the individual users' viewpoint: we propose a framework to compute the privacy score of a user. This score indicates the user's potential risk caused by his or her participation in the network. Our definition of privacy score satisfies the following intuitive properties: the more sensitive information a user discloses, the higher his or her privacy risk. Also, the more visible the disclosed information becomes in the network, the higher the privacy risk. We develop mathematical models to estimate both sensitivity and visibility of the information. We apply our methods to synthetic and real-world data and demonstrate their efficacy and practical utility. © 2010 ACM.",Expectation maximization; Information propagation; Item-response theory; Maximum-likelihood estimation; Social networks,Estimation; Mathematical models; Maximum likelihood estimation; Maximum principle; Online systems; Optimization; Expectation Maximization; Information propagation; Item Response Theory; Online social networks; Potential risks; Privacy concerns; Privacy issue; Privacy risks; Real world data; Sensitive informations; Social Networks; Work Focus; Social networking (online)
Discovering knowledge-sharing communities in question-answering forums,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650728690&doi=10.1145%2f1870096.1870099&partnerID=40&md5=7fa17b12f7a662d504e604daea923418,"In this article, we define a knowledge-sharing community in a question-answering forum as a set of askers and authoritative users such that, within each community, askers exhibit more homogeneous behavior in terms of their interactions with authoritative users than elsewhere. A procedure for discovering members of such a community is devised. As a case study, we focus on Yahoo! Answers, a large and diverse online question-answering service. Our contribution is twofold. First, we propose a method for automatic identification of authoritative actors in Yahoo! Answers. To this end, we estimate and then model the authority scores of participants as a mixture of gamma distributions. The number of components in the mixture is determined using the Bayesian Information Criterion (BIC), while the parameters of each component are estimated using the Expectation-Maximization (EM) algorithm. This method allows us to automatically discriminate between authoritative and nonauthoritative users. Second, we represent the forum environment as a type of transactional data such that each transaction summarizes the interaction of an asker with a specific set of authoritative users. Then, to group askers on the basis of their interactions with authoritative users, we propose a parameter-free transaction data clustering algorithm which is based on a novel criterion function. The identified clusters correspond to the communities that we aim to discover. To evaluate the suitability of our clustering algorithm, we conduct a series of experiments on both synthetic data and public real-life data. Finally, we put our approach to work using data from Yahoo! Answers which represent users' activities over one full year. © 2010 ACM.",Clustering; Mixture models; Transaction data,Automation; Mixtures; Parameter estimation; Automatic identification; Bayesian information criterion; Clustering; Community IS; Expectation-maximization algorithms; Gamma distribution; Knowledge-sharing; Mixture models; Novel criterion; Number of components; Question Answering; Real life data; Synthetic data; Transaction data; Transactional data; Clustering algorithms
Fast discovery of group lag correlations in streams,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650733369&doi=10.1145%2f1870096.1870101&partnerID=40&md5=cc6a3988a17bed1c3f9f3b632b5a8921,"The study of data streams has received considerable attention in various communities (theory, databases, data mining, networking), due to several important applications, such as network analysis, sensor monitoring, financial data analysis, and moving object tracking. Our goal in this article is to monitor multiple numerical streams and determine which pairs are correlated with lags, as well as the value of each such lag. Lag correlations and anticorrelations are frequent and very interesting in practice. For example, a decrease in interest rates typically precedes an increase in house sales by a few months; higher amounts of fluoride in drinking water may lead to fewer dental cavities some years later. Other lag settings include network analysis, sensor monitoring, financial data analysis, and tracking of moving objects. Such data streams are often correlated or anticorrelated, but with unknown lag. We propose BRAID, a method of detecting lag correlations among data streams. BRAID can handle data streams of semi-infinite length incrementally, quickly, and with small resource consumption. However, BRAID requires space and time quadratic on a number of streams k. We also propose ThinBRAID, which is even faster than BRAID, requiring O(k) space and time per time tick. Our theoretical analysis shows that BRAID/ThinBRAID can estimate lag correlations with little or, often, with no error. Our experiments on real and realistic data show that BRAID and ThinBRAID detect the correct lag perfectly most of the time (the largest relative error was about 1%), while they are significantly faster (up to 40,000 times) than the näive implementation. © 2010 ACM.",Cross-correlation; Data streams; Time-series,Data communication systems; Data mining; Electric network analysis; Potable water; Sensors; Tracking (position); Weaving; Cross correlations; Data stream; Dental cavities; Drinking water; Financial Data Analysis; Interest rates; Moving object tracking; Moving objects; Network analysis; Realistic data; Relative errors; Resource consumption; Sensor monitoring; Space and time; Data handling
ACM TKDD special issue on knowledge discovery for web intelligence,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650719883&doi=10.1145%2f1870096.1870097&partnerID=40&md5=91838de0f6c6d60f94d2b25e7b94f9b5,[No abstract available],,
Centralized and distributed anonymization for high-dimensional healthcare data,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149335185&doi=10.1145%2f1857947.1857950&partnerID=40&md5=3c2e4ac8d2aad733e72646e30588a5df,"Sharing healthcare data has become a vital requirement in healthcare system management; however, inappropriate sharing and usage of healthcare data could threaten patients' privacy. In this article, we study the privacy concerns of sharing patient information between the Hong Kong Red Cross Blood Transfusion Service (BTS) and the public hospitals. We generalize their information and privacy requirements to the problems of centralized anonymization and distributed anonymization, and identify the major challenges that make traditional data anonymization methods not applicable. Furthermore, we propose a new privacy model called LKC- privacy to overcome the challenges and present two anonymization algorithms to achieve LKC- privacy in both the centralized and the distributed scenarios. Experiments on real-life data demonstrate that our anonymization algorithms can effectively retain the essential information in anonymous data for data analysis and is scalable for anonymizing large datasets. © 2010 ACM.",,
Multilabel dimensionality reduction via dependence maximization,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049346655&doi=10.1145%2f1839490.1839495&partnerID=40&md5=7466290379b1d14dc5281f8be8e3bc46,"Multilabel learning deals with data associated with multiple labels simultaneously. Like other data mining and machine learning tasks, multilabel learning also suffers from the curse of dimensionality. Dimensionality reduction has been studied for many years, however, multilabel dimensionality reduction remains almost untouched. In this article, we propose a multilabel dimensionality reduction method, MDDM, with two kinds of projection strategies, attempting to project the original data into a lower-dimensional feature space maximizing the dependence between the original feature description and the associated class labels. Based on the Hilbert-Schmidt Independence Criterion, we derive a eigen-decomposition problem which enables the dimensionality reduction process to be efficient. Experiments validate the performance of MDDM. © 2010 ACM.",Dimensionality reduction; Multilabel learning,Class labels; Curse of dimensionality; Dimensionality reduction; Dimensionality reduction method; Eigen decomposition; Feature description; Feature space; Hilbert; Machine-learning; Multi-label; Multiple labels; Optimization
Behavioral targeting: The art of scaling up simple algorithms,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149339681&doi=10.1145%2f1857947.1857949&partnerID=40&md5=aa9f3c8bd7a6ad921492033005a23433,"Behavioral targeting (BT) leverages historical user behavior to select the ads most relevant to users to display. The state-of-the-art of BT derives a linear Poisson regression model from fine- grained user behavioral data and predicts click-through rate (CTR) from user history. We designed and implemented a highly scalable and efficient solution to BT using Hadoop MapReduce frame-work. With our parallel algorithm and the resulting system, we can build above 450 BT- category models from the entire Yahoo's user base within one day, the scale that one can not even imagine with prior systems. Moreover, our approach has yielded 20% CTR lift over the existing production system by leveraging the well-grounded probabilistic model fitted from a much larger training dataset. Specifically, our major contributions include: (1) A MapReduce statistical learning algorithm and implementation that achieve optimal data parallelism, task parallelism, and load balance in spite of the typically skewed distribution of domain data. (2) An in- place feature vector generation algorithm with strict linear-time complexity O(n) regardless of the granularity of sliding target window. (3) An in- memory caching scheme that significantly reduces the number of disk IOs to make large-scale learning practical. (4) Highly efficient data structures and sparse representations of models and data to enable fast model updates. We believe that our work makes significant contributions to solving large-scale machine learning problems of industrial relevance in general. Finally, we report comprehensive experimental results, using industrial proprietary codebase and datasets. © 2010 ACM.",,
Bayesian browsing model: Exact inference of document relevance from petabyte-scale data,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149296325&doi=10.1145%2f1857947.1857951&partnerID=40&md5=1a4a4a7347eea179ebc9ca224d67058a,"A fundamental challenge in utilizing Web search click data is to infer user-perceived relevance from the search log. Not only is the inference a difficult problem involving statistical reasonings but the bulky size, together with the ever-increasing nature, of the log data imposes extra requirements on scalability. In this paper, we propose the Bayesian Browsing Model (BBM), which performs exact inference of the document relevance, only requires a single pass of the data (i.e., the optimal scalability), and is shown effective. We present two sets of experiments to evaluate the model effectiveness and scalability. On the first set of over 50 million search instances of 1.1 million distinct queries, BBM outperforms the state-of-the-art competitor by 29.2% in log-likelihood while being 57 times faster. On the second click log set, spanning a quarter of petabyte, we showcase the scalability of BBM: we implemented it on a commercial MapReduce cluster, and it took only 3 hours to compute the relevance for 1.15 billion distinct query-URL pairs. © 2010 ACM.",,
TKDD special issue SIGKDD 2009,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149316489&doi=10.1145%2f1857947.1857948&partnerID=40&md5=e31812e2b84dabec0022f27ada8d2cd7,[No abstract available],,
A model-agnostic framework for fast spatial anomaly detection,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149318703&doi=10.1145%2f1857947.1857952&partnerID=40&md5=6e10d93ebe4be13144f757bbbf614100,"Given a spatial dataset placed on an n x n grid, our goal is to find the rectangular regions within which subsets of the dataset exhibit anomalous behavior. We develop algorithms that, given any user-supplied arbitrary likelihood function, conduct a likelihood ratio hypothesis test (LRT) over each rectangular region in the grid, rank all of the rectangles based on the computed LRT statistics, and return the top few most interesting rectangles. To speed this process, we develop methods to prune rectangles without computing their associated LRT statistics. © 2010 ACM.",,
Efficient algorithms for large-scale local triangle counting,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049335266&doi=10.1145%2f1839490.1839494&partnerID=40&md5=468a1a43e23fc10cf5d5c48bd8bc6fdd,"In this article, we study the problem of approximate local triangle counting in large graphs. Namely, given a large graph G = (V, E) we want to estimate as accurately as possible the number of triangles incident to every node ν ∈ V in the graph. We consider the question both for undirected and directed graphs. The problem of computing the global number of triangles in a graph has been considered before, but to our knowledge this is the first contribution that addresses the problem of approximate local triangle counting with a focus on the efficiency issues arising in massive graphs and that also considers the directed case. The distribution of the local number of triangles and the related local clustering coefficient can be used in many interesting applications. For example, we show that the measures we compute can help detect the presence of spamming activity in largescale Web graphs, as well as to provide useful features for content quality assessment in social networks. For computing the local number of triangles (undirected and directed), we propose two approximation algorithms, which are based on the idea of min-wise independent permutations [Broder et al. 1998]. Our algorithms operate in a semi-streaming fashion, using O(|V|) space in main memory and performing O(log|V|) sequential scans over the edges of the graph. The first algorithm we describe in this article also uses O(|E|) space of external memory during computation, while the second algorithm uses only main memory. We present the theoretical analysis as well as experimental results on large graphs, demonstrating the practical efficiency of our approach. © 2010 ACM.",Clustering coefficient; Massive-graph computing; Social networks; Web com- puting,Drug products plants; Graphic methods; Social networking (online); Clustering coefficient; Content qualities; Directed graphs; Efficient algorithm; External memory; Large graphs; Local clustering; Main memory; Massive graph; Massive-graph computing; Min-wise independent permutations; Number of triangles; Social Networks; Web graphs; Web-Com; Approximation algorithms
Learning multiple nonredundant clusterings,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049323340&doi=10.1145%2f1839490.1839496&partnerID=40&md5=04730edb17bff646f86af28a6f93d617,"Real-world applications often involve complex data that can be interpreted in many different ways. When clustering such data, there may exist multiple groupings that are reasonable and interesting from different perspectives. This is especially true for high-dimensional data, where different feature subspaces may reveal different structures of the data. However, traditional clustering is restricted to finding only one single clustering of the data. In this article, we propose a new clustering paradigm for exploratory data analysis: find all non-redundant clustering solutions of the data, where data points in the same cluster in one solution can belong to different clusters in other partitioning solutions. We present a framework to solve this problem and suggest two approaches within this framework: (1) orthogonal clustering, and (2) clustering in orthogonal subspaces. In essence, both approaches find alternative ways to partition the data by projecting it to a space that is orthogonal to the current solution. The first approach seeks orthogonality in the cluster space, while the second approach seeks orthogonality in the feature space. We study the relationship between the two approaches. We also combine our framework with techniques for automatically finding the number of clusters in the different solutions, and study stopping criteria for determining when all meaningful solutions are discovered. We test our framework on both synthetic and high-dimensional benchmark data sets, and the results show that indeed our approaches were able to discover varied clustering solutions that are interesting and meaningful. © 2010 ACM.",Disparate clustering; Diverse clustering; Nonredundant clustering; Orthogonalization,Belong to; Benchmark data; Clustering solutions; Clusterings; Complex data; Data points; Different structure; Disparate clustering; Diverse clustering; Exploratory data analysis; Feature space; Feature subspace; High dimensional data; High-dimensional; Non-redundant clustering; Number of clusters; Orthogonal subspaces; Orthogonality; Orthogonalization; Real-world application; Stopping criteria; Traditional clustering; Cluster analysis
SCOAL: A framework for Simultaneous CO-clustering and Learning from complex data,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049323817&doi=10.1145%2f1839490.1839492&partnerID=40&md5=29615758403ee37786de110b20edb296,"For difficult classification or regression problems, practitioners often segment the data into relatively homogeneous groups and then build a predictive model for each group. This two-step procedure usually results in simpler, more interpretable and actionable models without any loss in accuracy. In this work, we consider problems such as predicting customer behavior across products, where the independent variables can be naturally partitioned into two sets, that is, the data is dyadic in nature. A pivoting operation now results in the dependent variable showing up as entries in a ""customer by product"" data matrix. We present the Simultaneous CO-clustering And Learning (SCOAL) framework, based on the key idea of interleaving co-clustering and construction of prediction models to iteratively improve both cluster assignment and fit of the models. This algorithm provably converges to a local minimum of a suitable cost function. The framework not only generalizes co-clustering and collaborative filtering to model-based co-clustering, but can also be viewed as simultaneous co-segmentation and classification or regression, which is typically better than independently clustering the data first and then building models. Moreover, it applies to a wide range of bi-modal or multimodal data, and can be easily specialized to address classification and regression problems. We demonstrate the effectiveness of our approach on both these problems through experimentation on a variety of datasets. © 2010 ACM.",Classification; CO-clustering; Dyadic data; Multimodal data; Predictive modeling; Regression,Customer satisfaction; Mathematical models; Classification; Co-clustering; Dyadic data; Multi-modal data; Predictive modeling; Regression; Regression analysis
BISC: A bitmap itemset support counting approach for efficient frequent itemset mining,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049324078&doi=10.1145%2f1839490.1839493&partnerID=40&md5=2ecba2df8e9b6b66d4f5b5617262e163,"The performance of a depth-first frequent itemset (FI) miming algorithm is closely related to the total number of recursions. In previous approaches this is mainly decided by the total number of FIs, which results in poor performance when a large number of FIs are involved. To solve this problem, a three-strategy adaptive algorithm, bitmap itemset support counting (BISC), is presented. The core strategy, BISC1, is used in the innermost steps of the recursion. For a database D with only s frequent items, a depth-first approach need up to s levels of recursions to detect all the FIs (up to 2s). BISC1 completely replaces these recursions with a special summation that directly calculates the supports of all the possible 2s candidate itemsets. With BISC1 the run-time is entirely independent of the database after one database scan, and the per-candidate cost is only s. To offset the exponential growth of cost (both time and space) with BISC1 as s increases, a second strategy, BISC2, is introduced to effectively double the acceptable range of s. BISC2 divides an itemset into prefix and suffix and improves the performance by pruning all the itemsets with infrequent prefixes. If the total number of frequent items in D is high, the classic database projection strategy is used. In this case for the first s items a single run of BISC (1 or 2) is applied. For each of the remaining items, a projected database is created and the mining process proceeds recursively. To achieve optimal performance, BISC adaptively decides which strategy to use based on the dataset and minimum support. Experiments show that BISC outperforms previous approaches in all the datasets tested. Even though this does not guarantee that BISC will always perform the best, the result is impressive given the fact that most existing algorithms are only efficient in some types of datasets. The memory usage of BISC is also comparable to those of other algorithms. © 2010 ACM.",Association rule mining; Data mining algorithms; Frequent itemset mining,Adaptive algorithms; Association rules; Associative processing; Database systems; Mining; Association rule mining; Core strategy; Data mining algorithms; Data sets; Database scans; Depth first; Exponential growth; Frequent items; Frequent itemset; Frequent itemset mining; Item sets; Itemset; Memory usage; Minimum support; Mining process; Optimal performance; Other algorithms; Poor performance; Projected database; Recursions; Runtimes; Time and space; Data mining
Margin: Maximal frequent subgraph mining,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049342020&doi=10.1145%2f1839490.1839491&partnerID=40&md5=065649a24630f1e807a257499d443aa7,"The exponential number of possible subgraphs makes the problem of frequent subgraph mining a challenge. The set of maximal frequent subgraphs is much smaller to that of the set of frequent subgraphs providing ample scope for pruning. MARGIN is a maximal subgraph mining algorithm that moves among promising nodes of the search space along the ""border"" of the infrequent and frequent subgraphs. This drastically reduces the number of candidate patterns in the search space. The proof of correctness of the algorithm is presented. Experimental results validate the efficiency and utility of the technique proposed. © 2010 ACM.",Graph mining; Maximal frequent subgraph mining,Candidate patterns; Exponential numbers; Frequent subgraph mining; Frequent subgraphs; Graph mining; Proof of correctness; Search spaces; Subgraph mining; Subgraphs
Data mining for discrimination discovery,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953188096&doi=10.1145%2f1754428.1754432&partnerID=40&md5=44747c685dac92709533d46b949f5bd4,"In the context of civil rights law, discrimination refers to unfair or unequal treatment of people based on membership to a category or a minority, without regard to individual merit. Discrimination in credit, mortgage, insurance, labor market, and education has been investigated by researchers in economics and human sciences. With the advent of automatic decision support systems, such as credit scoring systems, the ease of data collection opens several challenges to data analysts for the fight against discrimination. In this article, we introduce the problem of discovering discrimination through data mining in a dataset of historical decision records, taken by humans or by automatic systems. We formalize the processes of direct and indirect discrimination discovery by modelling protected-by-law groups and contexts where discrimination occurs in a classification rule based syntax. Basically, classification rules extracted from the dataset allow for unveiling contexts of unlawful discrimination, where the degree of burden over protected-by-law groups is formalized by an extension of the lift measure of a classification rule. In direct discrimination, the extracted rules can be directly mined in search of discriminatory contexts. In indirect discrimination, the mining process needs some background knowledge as a further input, for example, census data, that combined with the extracted rules might allow for unveiling contexts of discriminatory decisions. A strategy adopted for combining extracted classification rules with background knowledge is called an inference model. In this article, we propose two inference models and provide automatic procedures for their implementation. An empirical assessment of our results is provided on the German credit dataset and on the PKDD Discovery Challenge 1999 financial dataset. © 2010 ACM.",Classification rules; Discrimination,Artificial intelligence; Data mining; Decision support systems; Decision theory; Employment; Mining; Population statistics; Automatic decision; Automatic procedures; Automatic systems; Background knowledge; Census data; Civil rights; Classification rules; Credit scoring; Data analysts; Data collection; Data sets; Empirical assessment; Human science; Inference models; Labor markets; Mining process; Classification (of information)
A shared-subspace learning framework for multi-label classification,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953216761&doi=10.1145%2f1754428.1754431&partnerID=40&md5=9aeb0dca3dd8e612bbbea77416e4f191,"Multi-label problems arise in various domains such as multi-topic document categorization, protein function prediction, and automatic image annotation. One natural way to deal with such problems is to construct a binary classifier for each label, resulting in a set of independent binary classification problems. Since multiple labels share the same input space, and the semantics conveyed by different labels are usually correlated, it is essential to exploit the correlation information contained in different labels. In this paper, we consider a general framework for extracting shared structures in multi-label classification. In this framework, a common subspace is assumed to be shared among multiple labels. We show that the optimal solution to the proposed formulation can be obtained by solving a generalized eigenvalue problem, though the problem is nonconvex. For high-dimensional problems, direct computation of the solution is expensive, and we develop an efficient algorithm for this case. One appealing feature of the proposed framework is that it includes several well-known algorithms as special cases, thus elucidating their intrinsic relationships. We further show that the proposed framework can be extended to the kernel-induced feature space. We have conducted extensive experiments on multi-topic web page categorization and automatic gene expression pattern image annotation tasks, and results demonstrate the effectiveness of the proposed formulation in comparison with several representative algorithms. © 2010 ACM.",Gene expression pattern image annotation; Kernel methods; Least squares loss; Multi-label classification; Shared subspace; Singular value decomposition; Web page categorization,Algorithms; Computational efficiency; Eigenvalues and eigenfunctions; Gene expression; Image analysis; Security of data; Singular value decomposition; World Wide Web; Gene expression patterns; Kernel methods; Least Square; Least squares loss; Multi-label; Shared subspace; Web page; Labels
CSNL: A cost-sensitive non-linear decision tree algorithm,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953193051&doi=10.1145%2f1754428.1754429&partnerID=40&md5=31cf6db3796502dd1b51a84e77e15365,"This article presents a new decision tree learning algorithm called CSNL that induces Cost-Sensitive Non-Linear decision trees. The algorithm is based on the hypothesis that nonlinear decision nodes provide a better basis than axis-parallel decision nodes and utilizes discriminant analysis to construct nonlinear decision trees that take account of costs of misclassification. The performance of the algorithm is evaluated by applying it to seventeen datasets and the results are compared with those obtained by two well known cost-sensitive algorithms, ICET and MetaCost, which generate multiple trees to obtain some of the best results to date. The results show that CSNL performs at least as well, if not better than these algorithms, in more than twelve of the datasets and is considerably faster. The use of bagging with CSNL further enhances its performance showing the significant benefits of using nonlinear decision nodes. © 2010 ACM.",Cost-sensitive learning; Decision tree learning,Costs; Decision trees; Discriminant analysis; Learning algorithms; Cost-sensitive; Cost-sensitive algorithm; Cost-sensitive learning; Data sets; Decision tree learning; Decision tree learning algorithm; Misclassifications; Multiple trees; Non-linear; Cost benefit analysis
Analyzing knowledge communities using foreground and background clusters,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953175973&doi=10.1145%2f1754428.1754430&partnerID=40&md5=724078a58ac629235d51b5ec942de7f4,"Insight into the growth (or shrinkage) of ""knowledge communities"" of authors that build on each other's work can be gained by studying the evolution over time of clusters of documents. We cluster documents based on the documents they cite in common using the Streemer clustering method, which finds cohesive foreground clusters (the knowledge communities) embedded in a diffuse background. We build predictive models with features based on the citation structure, the vocabulary of the papers, and the affiliations and prestige of the authors and use these models to study the drivers of community growth and the predictors of how widely a paper will be cited. We find that scientific knowledge communities tend to grow more rapidly if their publications build on diverse information and use narrow vocabulary and that papers that lie on the periphery of a community have the highest impact, while those not in any community have the lowest impact. © 2010 ACM.",Citation analysis; Clustering; Community evolution; Knowledge communities; Text mining,Biology; Citation analysis; Cluster documents; Clustering methods; Community evolution; Diffuse background; Knowledge communities; Predictive models; Scientific knowledge; Text mining; Technical writing
Mining multidimensional and multilevel sequential patterns,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955640556&doi=10.1145%2f1644873.1644877&partnerID=40&md5=5a732466aab5cfcc456e475fbc418cf6,"Multidimensional databases have been designed to provide decision makers with the necessary tools to help them understand their data. This framework is different from transactional data as the datasets contain huge volumes of historicized and aggregated data defined over a set of dimensions that can be arranged through multiple levels of granularities. Many tools have been proposed to query the data and navigate through the levels of granularity. However, automatic tools are still missing to mine this type of data in order to discover regular specific patterns. In this article, we present a method for mining sequential patterns from multidimensional databases, at the same time taking advantage of the different dimensions and levels of granularity, which is original compared to existing work. The necessary definitions and algorithms are extended from regular sequential patterns to this particular case. Experiments are reported, showing the significance of this approach. © 2010 ACM.",Frequent patterns; Hierarchy; Multidimensional databases; Multilevel patterns; Sequential patterns,Frequent patterns; Hierarchy; Multi-level pattern; Multidimensional database; Sequential patterns; Database systems
Self-sufficient itemsets: An approach to screening potentially interesting associations between items,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955631169&doi=10.1145%2f1644873.1644876&partnerID=40&md5=0702f21d884b230bd03ea5cbe26580d4,"Self-sufficient itemsets are those whose frequency cannot be explained solely by the frequency of either their subsets or of their supersets. We argue that itemsets that are not self-sufficient will often be of little interest to the data analyst, as their frequency should be expected once that of the itemsets on which their frequency depends is known. We present tests for statistically sound discovery of self-sufficient itemsets, and computational techniques that allow those tests to be applied as a post-processing step for any itemset discovery algorithm. We also present a measure for assessing the degree of potential interest in an itemset that complements these statistical measures. © 2010 ACM.",Association discovery; Association rules; Itemset discovery; Itemset screening; Statistical evaluation,Association rules; Associative processing; Association discoveries; Computational technique; Data analysts; Discovery algorithm; Item sets; Itemset; Post processing; Statistical evaluation; Statistical measures; Supersets; Function evaluation
Factor in the neighbors: Scalable and accurate collaborative filtering,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955644905&doi=10.1145%2f1644873.1644874&partnerID=40&md5=06f46086dc47168b8e459b7671b81800,"Recommender systems provide users with personalized suggestions for products or services. These systems often rely on collaborating filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The most common approach to CF is based on neighborhood models, which originate from similarities between products or users. In this work we introduce a new neighborhood model with an improved prediction accuracy. Unlike previous approaches that are based on heuristic similarities,we model neighborhood relations by minimizing a global cost function. Further accuracy improvements are achieved by extending the model to exploit both explicit and implicit feedback by the users. Past models were limited by the need to compute all pairwise similarities between items or users, which grow quadratically with input size. In particular, this limitation vastly complicates adopting user similarity models, due to the typical large number of users. Our new model solves these limitations by factoring the neighborhood model, thus making both item-item and user-user implementations scale linearly with the size of the data. The methods are tested on the Netflix data, with encouraging results. © 2010 ACM.",Collaborative filtering; Netflix Prize; Recommender systems,Accuracy Improvement; Collaborative filtering; Implicit feedback; Input size; Neighborhood model; Neighborhood relation; Netflix Prize; New model; Prediction accuracy; Recommender systems; Similarity models; Signal filtering and prediction
Motif discovery in physiological datasets: A methodology for inferring predictive elements,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955605527&doi=10.1145%2f1644873.1644875&partnerID=40&md5=f92bfaf773260f338ab50e20dc394335,"In this article, we propose a methodology for identifying predictive physiological patterns in the absence of prior knowledge.We use the principle of conservation to identify activity that consistently precedes an outcome in patients, and describe a two-stage process that allows us to efficiently search for such patterns in large datasets. This involves first transforming continuous physiological signals from patients into symbolic sequences, and then searching for patterns in these reduced representations that are strongly associated with an outcome. Our strategy of identifying conserved activity that is unlikely to have occurred purely by chance in symbolic data is analogous to the discovery of regulatory motifs in genomic datasets. We build upon existing work in this area, generalizing the notion of a regulatory motif and enhancing current techniques to operate robustly on non-genomic data.We also address two significant considerations associated with motif discovery in general: computational efficiency and robustness in the presence of degeneracy and noise. To deal with these issues, we introduce the concept of active regions and new subset-based techniques such as a two-layer Gibbs sampling algorithm. These extensions allow for a framework for information inference, where precursors are identified as approximately conserved activity of arbitrary complexity preceding multiple occurrences of an event. We evaluated our solution on a population of patients who experienced sudden cardiac death and attempted to discover electrocardiographic activity that may be associated with the endpoint of death. To assess the predictive patterns discovered, we compared likelihood scores for motifs in the sudden death population against control populations of normal individuals and those with non-fatal supraventricular arrhythmias. Our results suggest that predictive motif discovery may be able to identify clinically relevant information even in the absence of significant prior knowledge. © 2010 ACM.",Data mining; Gibbs sampling; Inference; Knowledge discovery; Motifs; Physiological signals,Computational efficiency; Data mining; Genes; Gibbs sampling; Inference; Knowledge Discovery; Motifs; Physiological signals; Physiology
VOGUE: A variable order hidden Markov model with duration based on frequent sequence mining,2010,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955634103&doi=10.1145%2f1644873.1644878&partnerID=40&md5=b0f690f54bba02ac83ece67bc7f28266,"We present VOGUE, a novel, variable order hidden Markov model with state durations, that combines two separate techniques for modeling complex patterns in sequential data: pattern mining and data modeling. VOGUE relies on a variable gap sequence mining method to extract frequent patterns with different lengths and gaps between elements. It then uses these mined sequences to build a variable order hidden Markov model (HMM), that explicitly models the gaps. The gaps implicitly model the order of the HMM, and they explicitly model the duration of each state. We apply VOGUE to a variety of real sequence data taken from domains such as protein sequence classification, Web usage logs, intrusion detection, and spelling correction. We show that VOGUE has superior classification accuracy compared to regular HMMs, higher-order HMMs, and even special purpose HMMs like HMMER, which is a state-of-the-art method for protein classification. The VOGUE implementation and the datasets used in this article are available as open-source. © 2010 ACM.",Hidden Markov models; Higher-order HMM; HMM with duration; Sequence mining and modeling; Variable-order HMM,Intrusion detection; Mining; Object recognition; Proteins; Classification accuracy; Complex pattern; Data modeling; Data sets; Frequent patterns; Frequent sequences; Higher order; HMM with duration; Open-source; Pattern mining; Protein Classification; Protein sequence classification; Sequence data; Sequence mining; Sequential data; Spelling correction; State-of-the-art methods; Variable order; Variable-order HMM; Web usage; Hidden Markov models
On evolutionary spectral clustering,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449164947&doi=10.1145%2f1631162.1631165&partnerID=40&md5=74f1b77fc52b98ab2eced3f0b5e930cf,"Evolutionary clustering is an emerging research area essential to important applications such as clustering dynamic Web and blog contents and clustering data streams. In evolutionary clustering, a good clustering result should fit the current data well, while simultaneously not deviate too dramatically from the recent history. To fulfill this dual purpose, a measure of temporal smoothness is integrated in the overall measure of clustering quality. In this article, we propose two frameworks that incorporate temporal smoothness in evolutionary spectral clustering. For both frameworks, we start with intuitions gained from the well-known k-means clustering problem, and then propose and solve corresponding cost functions for the evolutionary spectral clustering problems. Our solutions to the evolutionary spectral clustering problems provide more stable and consistent clustering results that are less sensitive to short-term noises while at the same time are adaptive to long-term cluster drifts. Furthermore, we demonstrate that our methods provide the optimal solutions to the relaxed versions of the corresponding evolutionary k-means clustering problems. Performance experiments over a number of real and synthetic data sets illustrate our evolutionary spectral clustering methods provide more robust clustering results that are not sensitive to noise and can adapt to data drifts. © 2009 ACM.",Evolutionary spectral clustering; Preserving cluster membership; Preserving cluster quality; Temporal smoothness,Cost functions; Water supply systems; Cluster memberships; Clustering data; Clustering quality; Clustering results; Current data; Evolutionary clustering; K-means clustering; Optimal solutions; Performance experiment; Research areas; Robust clustering; Spectral clustering; Spectral clustering methods; Synthetic datasets; Clustering algorithms
Constructing comprehensive summaries of large event sequences,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449201090&doi=10.1145%2f1631162.1631169&partnerID=40&md5=ee4735a334e71571321787589d6a0657,"Event sequences capture system and user activity over time. Prior research on sequence mining has mostly focused on discovering local patterns appearing in a sequence. While interesting, these patterns do not give a comprehensive summary of the entire event sequence. Moreover, the number of patterns discovered can be large. In this article, we take an alternative approach and build short summaries that describe an entire sequence, and discover local dependencies between event types. We formally define the summarization problem as an optimization problem that balances shortness of the summary with accuracy of the data description. We show that this problem can be solved optimally in polynomial time by using a combination of two dynamic-programming algorithms. We also explore more efficient greedy alternatives and demonstrate that they work well on large datasets. Experiments on both synthetic and real datasets illustrate that our algorithms are efficient and produce high-quality results, and reveal interesting local structures in the data. © 2009 ACM.",Event sequences; Log mining; Summarization,Polynomial approximation; Alternative approach; Capture system; Event sequence; High quality; Large datasets; Local patterns; Local structure; Log mining; Optimization problems; Polynomial-time; Programming algorithms; Real data sets; Sequence mining; User activity; Graphical user interfaces
ACM TKDD Special Issue: ACM SIGKDD 2007 and ACM SIGKDD 2008,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449177900&doi=10.1145%2f1631162.1631163&partnerID=40&md5=3b76fc455a43850861ed05b2154921b0,[No abstract available],,
An event-based framework for characterizing the evolutionary behavior of interaction graphs,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449133333&doi=10.1145%2f1631162.1631164&partnerID=40&md5=5e40b7d67c922c2bcddd05d406118dfa,"Interaction graphs are ubiquitous in many fields such as bioinformatics, sociology and physical sciences. There have been many studies in the literature targeted at studying and mining these graphs. However, almost all of them have studied these graphs from a static point of view. The study of the evolution of these graphs over time can provide tremendous insight on the behavior of entities, communities and the flow of information among them. In this work, we present an event-based characterization of critical behavioral patterns for temporally varying interaction graphs. We use nonoverlapping snapshots of interaction graphs and develop a framework for capturing and identifying interesting events from them. We use these events to characterize complex behavioral patterns of individuals and communities over time. We show how semantic information can be incorporated to reason about community-behavior events. We also demonstrate the application of behavioral patterns for the purposes of modeling evolution, link prediction and influence maximization. Finally, we present a diffusion model for evolving networks, based on our framework. © 2009 ACM.",Diffusion of innovations; Dynamic interaction networks; Evolutionary analysis,Active networks; Bioinformatics; Behavioral patterns; Diffusion model; Diffusion of innovations; Dynamic interaction; Event-based; Evolving networks; Interaction graphs; Link prediction; Physical science; Semantic information; Static point; Diffusion
Reflect and correct: A misclassification prediction approach to active inference,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449142164&doi=10.1145%2f1631162.1631168&partnerID=40&md5=e5698cd4ccc6a10b9edb4febd41b2ad6,"Information diffusion, viral marketing, graph-based semi-supervised learning, and collective classification all attempt to model and exploit the relationships among nodes in a network to improve the performance of node labeling algorithms. However, sometimes the advantage of exploiting the relationships can become a disadvantage. Simple models like label propagation and iterative classification can aggravate a misclassification by propagating mistakes in the network, while more complex models that define and optimize a global objective function, such as Markov random fields and graph mincuts, can misclassify a set of nodes jointly. This problem can be mitigated if the classification system is allowed to ask for the correct labels for a few of the nodes during inference. However, determining the optimal set of labels to acquire is intractable under relatively general assumptions, which forces us to resort to approximate and heuristic techniques. We describe three such techniques in this article. The first one is based on directly approximating the value of the objective function of label acquisition and greedily acquiring the label that provides the most improvement. The second technique is a simple technique based on the analogy we draw between viral marketing and label acquisition. Finally, we propose a method, which we refer to as reflect and correct, that can learn and predict when the classification system is likely to make mistakes and suggests acquisitions to correct those mistakes. We empirically show on a variety of synthetic and real-world datasets that the reflect and correct method significantly outperforms the other two techniques, as well as other approaches based on network structural measures such as node degree and network clustering. © 2009 ACM.",Active inference; Collective classification; Information diffusion; Label acquisition; Viral marketing,Diffusion; Heuristic methods; Hidden Markov models; Labels; Learning algorithms; Marketing; Mergers and acquisitions; Supervised learning; Active inference; Classification system; Complex model; Global objective; Graph-based; Heuristic techniques; Information diffusion; Label propagation; Labeling algorithms; Markov Random Fields; Misclassifications; Network Clustering; Node degree; Objective functions; Optimal sets; Real-world datasets; Semi-supervised learning; Simple model; Structural measures; Viral marketing; Classification (of information)
Fast likelihood search for hidden Markov models,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449122835&doi=10.1145%2f1631162.1631166&partnerID=40&md5=f91bdcb48aecaa6ce47ecccdefc89af2,"Hidden Markov models (HMMs) are receiving considerable attention in various communities and many applications that use HMMs have emerged such as mental task classification, biological analysis, traffic monitoring, and anomaly detection. This article has two goals; The first goal is exact and efficient identification of the model whose state sequence has the highest likelihood for the given query sequence (more precisely, no HMM that actually has a high-probability path for the given sequence is missed by the algorithm), and the second goal is exact and efficient monitoring of streaming data sequences to find the best model. We propose SPIRAL, a fast search method for HMM datasets. SPIRAL is based on three ideas; (1) it clusters states of models to compute approximate likelihood, (2) it uses several granularities and approximates likelihood values in search processing, and (3) it focuses on just the promising likelihood computations by pruning out low-likelihood state sequences. Experiments verify the effectiveness of SPIRAL and show that it is more than 490 times faster than the naive method. © 2009 ACM.",Hidden Markov model; Likelihood; Upper bound,Feature extraction; Object recognition; Anomaly detection; Best model; Biological analysis; Data sets; Efficient monitoring; Fast search method; Likelihood; Likelihood computation; Mental task classification; Query sequence; State sequences; Streaming data; Traffic monitoring; Upper Bound; Hidden Markov models
Efficient algorithms for genome-wide association study,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449161607&doi=10.1145%2f1631162.1631167&partnerID=40&md5=e4df8737949097ed3561447b0fbfeae0,"Studying the association between quantitative phenotype (such as height or weight) and single nucleotide polymorphisms (SNPs) is an important problem in biology. To understand underlying mechanisms of complex phenotypes, it is often necessary to consider joint genetic effects across multiple SNPs. ANOVA (analysis of variance) test is routinely used in association study. Important findings from studying gene-gene (SNP-pair) interactions are appearing in the literature. However, the number of SNPs can be up to millions. Evaluating joint effects of SNPs is a challenging task even for SNP-pairs. Moreover, with large number of SNPs correlated, permutation procedure is preferred over simple Bonferroni correction for properly controlling family-wise error rate and retaining mapping power, which dramatically increases the computational cost of association study. In this article, we study the problem of finding SNP-pairs that have significant associations with a given quantitative phenotype. We propose an efficient algorithm, FastANOVA, for performing ANOVA tests on SNP-pairs in a batch mode, which also supports large permutation test. We derive an upper bound of SNP-pair ANOVA test, which can be expressed as the sum of two terms. The first term is based on single-SNP ANOVA test. The second term is based on the SNPs and independent of any phenotype permutation. Furthermore, SNP-pairs can be organized into groups, each of which shares a common upper bound. This allows for maximum reuse of intermediate computation, efficient upper bound estimation, and effective SNP-pair pruning. Consequently, FastANOVA only needs to perform the ANOVA test on a small number of candidate SNP-pairs without the risk of missing any significant ones. Extensive experiments demonstrate that FastANOVA is orders of magnitude faster than the brute-force implementation of ANOVA tests on all SNP pairs. The principles used in FastANOVA can be applied to categorical phenotypes and other statistics such as Chi-square test. © 2009 ACM.",ANOVA test; Association study; Permutation test,Algorithms; Analysis of variance (ANOVA); Biology; Computational efficiency; Error correction; Fluorine containing polymers; Regression analysis; Risk perception; ANOVA (analysis of variance); ANOVA test; Batch modes; Bonferroni correction; Chi-square tests; Complex phenotype; Computational costs; Efficient algorithm; Error rate; Genetic effects; Genome-wide association; Joint effect; Orders of magnitude; Other statistics; Permutation procedures; Permutation tests; Single nucleotide polymorphisms; Underlying mechanism; Upper Bound; Testing
Author name disambiguation in MEDLINE,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73349139831&doi=10.1145%2f1552303.1552304&partnerID=40&md5=4d29d56410af1020cfab40b2c64bbd70,"Background: We recently described ""Author-ity,"" a model for estimating the probability that two articles in MEDLINE, sharing the same author name, were written by the same individual. Features include shared title words, journal name, coauthors, medical subject headings, language, affiliations, and author name features (middle initial, suffix, and prevalence in MEDLINE). Here we test the hypothesis that the Author-ity model will suffice to disambiguate author names for the vast majority of articles in MEDLINE. Methods: Enhancements include: (a) incorporating first names and their variants, email addresses, and correlations between specific last names and affiliation words; (b) new methods of generating large unbiased training sets; (c) new methods for estimating the prior probability; (d) a weighted least squares algorithm for correcting transitivity violations; and (e) a maximum likelihood based agglomerative algorithm for computing clusters of articles that represent inferred author-individuals. Results: Pairwise comparisons were computed for all author names on all 15.3 million articles in MEDLINE (2006 baseline), that share last name and first initial, to create Author-ity 2006, a database that has each name on each article assigned to one of 6.7 million inferred author-individual clusters. Recall is estimated at ∼98.8%. Lumping (putting two different individuals into the same cluster) affects ∼0.5% of clusters, whereas splitting (assigning articles written by the same individual to >1 cluster) affects ∼2% of articles. Impact: The Author-ity model can be applied generally to other bibliographic databases. Author name disambiguation allows information retrieval and data integration to become person-centered, not just document-centered, setting the stage for new data mining and social network tools that will facilitate the analysis of scholarly publishing and collaboration behavior. Availability: The Author-ity 2006 database is available for nonprofit academic research, and can be freely queried via http://arrowsmith.psych.uic.edu.",Bibliographic databases; Name disambiguation,Computation theory; Data handling; Information retrieval; Information services; Maximum likelihood estimation; Vocabulary control; Academic research; Agglomerative algorithm; Bibliographic database; Computing clusters; Data integration; E-mail address; Medical subject headings; Medline; Name disambiguation; Pair-wise comparison; Prior probability; Scholarly publishing; Social Networks; Training sets; Weighted-least-squares algorithm; Database systems
Density-based clustering of data streams at multiple resolutions,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73349121934&doi=10.1145%2f1552303.1552307&partnerID=40&md5=d7ddab436ebd96572502938520199f5f,"In data stream clustering, it is desirable to have algorithms that are able to detect clusters of arbitrary shape, clusters that evolve over time, and clusters with noise. Existing stream data clustering algorithms are generally based on an online-offline approach: The online component captures synopsis information from the data stream (thus, overcoming real-time and memory constraints) and the offline component generates clusters using the stored synopsis. The online-offline approach affects the overall performance of stream data clustering in various ways: the ease of deriving synopsis from streaming data; the complexity of data structure for storing and managing synopsis; and the frequency at which the offline component is used to generate clusters. In this article, we propose an algorithm that (1) computes and updates synopsis information in constant time; (2) allows users to discover clusters at multiple resolutions; (3) determines the right time for users to generate clusters from the synopsis information; (4) generates clusters of higher purity than existing algorithms; and (5) determines the right threshold function for density-based clustering based on the fading model of stream data. To the best of our knowledge, no existing data stream algorithms has all of these features. Experimental results show that our algorithm is able to detect arbitrarily shaped, evolving clusters with high quality.",Data mining algorithms; Density based clustering; Evolving data streams,Data communication systems; Data mining; Data structures; Hydraulics; Arbitrary shape; Constant time; Data mining algorithm; Data stream; Data stream algorithms; Data stream clustering; Density-based Clustering; Fading models; High quality; Memory constraints; Multiple resolutions; Off-line approaches; Offline; Online components; Stream data; Stream data clustering; Streaming data; Threshold functions; Clustering algorithms
Link spam target detection using page farms,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73349120904&doi=10.1145%2f1552303.1552306&partnerID=40&md5=78ca7059779429b262a7ff57bd5ca296,"Currently, most popular Web search engines adopt some link-based ranking methods such as PageRank. Driven by the huge potential benefit of improving rankings of Web pages, many tricks have been attempted to boost page rankings. The most common way, which is known as link spam, is to make up some artificially designed link structures. Detecting link spam effectively is a big challenge. In this article, we develop novel and effective detection methods for link spam target pages using page farms. The essential idea is intuitive: whether a page is the beneficiary of link spam is reflected by how it collects its PageRank score. Technically, how a target page collects its PageRank score is modeled by a page farm, which consists of pages contributing a major portion of the PageRank score of the target page. We propose two spamicity measures based on page farms. They can be used as an effective measure to check whether the pages are link spam target pages. An empirical study using a newly available real dataset strongly suggests that our method is effective. It outperforms the state-of-the-art methods like SpamRank and SpamMass in both precision and recall.",Link Spam; Page Farm; PageRank,Farms; Information retrieval; Search engines; Targets; Websites; AS-links; Data sets; Detection methods; Effective measures; Empirical studies; Link structure; Link-based ranking; Page Farm; Page ranking; PageRank; Pagerank score; Potential benefits; Precision and recall; State-of-the-art methods; Target detection; Web page; Web search engines; Internet
Stream data clustering based on grid density and attraction,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73349087457&doi=10.1145%2f1552303.1552305&partnerID=40&md5=5409a027e7bb05c7645f48517152b74e,"Clustering real-time stream data is an important and challenging problem. Existing algorithms such as CluStream are based on the k-means algorithm. These clustering algorithms have difficulties finding clusters of arbitrary shapes and handling outliers. Further, they require the knowledge of k and user-specified time window. To address these issues, this article proposes D-Stream, a framework for clustering stream data using a density-based approach. Our algorithm uses an online component that maps each input data record into a grid and an offline component that computes the grid density and clusters the grids based on the density. The algorithm adopts a density decaying technique to capture the dynamic changes of a data stream and a attraction-based mechanism to accurately generate cluster boundaries. Exploiting the intricate relationships among the decay factor, attraction, data density, and cluster structure, our algorithm can efficiently and effectively generate and adjust the clusters in real time. Further, a theoretically sound technique is developed to detect and remove sporadic grids mapped by outliers in order to dramatically improve the space and time efficiency of the system. The technique makes high-speed data stream clustering feasible without degrading the clustering quality. The experimental results show that our algorithm has superior quality and efficiency, can find clusters of arbitrary shapes, and can accurately recognize the evolving behaviors of real-time data streams.",Clustering; Data mining; Density-based algorithms; Stream data,Data communication systems; Data mining; Hydraulics; A-density; Arbitrary shape; Cluster boundaries; Cluster structure; Clustering data; Clustering quality; Data density; Data stream; Decay factor; Density-based algorithm; Dynamic changes; Grid density; High-speed data; Input datas; k-Means algorithm; Offline; Online components; Real time; Real-time data streams; Space and time; Stream data; Stream data clustering; Time windows; Clustering algorithms
Analyzing communities and their evolutions in dynamic social networks,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73349107745&doi=10.1145%2f1514888.1514891&partnerID=40&md5=f2ed7256d7a0dd65aea8254c781f115d,"We discover communities from social network data and analyze the community evolution. These communities are inherent characteristics of human interaction in online social networks, as well as paper citation networks. Also, communities may evolve over time, due to changes to individuals' roles and social status in the network as well as changes to individuals' research interests. We present an innovative algorithm that deviates from the traditional two-step approach to analyze community evolutions. In the traditional approach, communities are first detected for each time slice, and then compared to determine correspondences. We argue that this approach is inappropriate in applications with noisy data. In this paper, we propose FacetNet for analyzing communities and their evolutions through a robust unified process. This novel framework will discover communities and capture their evolution with temporal smoothness given by historic community structures. Our approach relies on formulating the problem in terms of maximum a posteriori (MAP) estimation, where the community structure is estimated both by the observed networked data and by the prior distribution given by historic community structures. Then we develop an iterative algorithm, with proven low time complexity, which is guaranteed to converge to an optimal solution. We perform extensive experimental studies, on both synthetic datasets and real datasets, to demonstrate that our method discovers meaningful communities and provides additional insights not directly obtainable from traditional methods.",Community; Community net; Evolution; Evolution net; Nonnegative matrix factorization; Soft membership,Factorization; Light measurement; Social networking (online); Community evolution; Community structures; Experimental studies; Human interactions; Innovative algorithms; Iterative algorithm; Maximum a posteriori estimation; Noisy data; Nonnegative matrix factorization; Online social networks; Optimal solutions; Paper citations; Prior distribution; Real data sets; Social Networks; Social status; Soft membership; Synthetic datasets; Time complexity; Time slice; Two-step approach; Unified process; Social sciences
Modeling information-seeker satisfaction in community question answering,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449134746&doi=10.1145%2f1514888.1514893&partnerID=40&md5=718f0edf8627b3132c27c94e94a83e6c,"Question Answering Communities such as Naver, Baidu Knows, and Yahoo Answers have emerged as popular, and often effective, means of information seeking on the web. By posting questions for other participants to answer, information seekers can obtain specific answers to their questions. Users of CQA portals have already contributed millions of questions, and received hundreds of millions of answers from other participants. However, CQA is not always effective: in some cases, a user may obtain a perfect answer within minutes, and in others it may require hours - and sometimes days - until a satisfactory answer is contributed. We investigate the problem of predicting information seeker satisfaction in collaborative question answering communities, where we attempt to predict whether a question author will be satisfied with the answers submitted by the community participants. We present a general prediction model, and develop a variety of content, structure, and community-focused features for this task. Our experimental results, obtained from a large-scale evaluation over thousands of real questions and user ratings, demonstrate the feasibility of modeling and predicting asker satisfaction. We complement our results with a thorough investigation of the interactions and information seeking patterns in question answering communities that correlate with information seeker satisfaction. We also explore personalized models of asker satisfaction, and show that when sufficient interaction history exists, personalization can significantly improve prediction accuracy over a ""one-size-fits-all"" model. Our models and predictions could be useful for a variety of applications, such as user intent inference, answer ranking, interface design, and query suggestion and routing.",Community question answering; Information seeker satisfaction,Mathematical models; Information seeking; Intent inference; Interaction history; Interface designs; Personalizations; Personalized model; Prediction accuracy; Prediction model; Query suggestion; Question Answering; User rating; Information use
"Introduction to special issue on social computing, behavioral modeling, and prediction",2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73349114575&doi=10.1145%2f1514888.1514889&partnerID=40&md5=3597e8bb3935d355c4ce88b1ef7a7880,[No abstract available],,
Expanding network communities from representative examples,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73349119355&doi=10.1145%2f1514888.1514890&partnerID=40&md5=b1e67123334e7f573fdbd15affac9b06,"We present an approach to leverage a small subset of a coherent community within a social network into a much larger, more representative sample. Our problem becomes identifying a small conductance subgraph containing many (but not necessarily all) members of the given seed set. Starting with an initial seed set representing a sample of a community, we seek to discover as much of the full community as possible. We present a general method for network community expansion, demonstrating that our methods work well in expanding communities in real world networks starting from small given seed groups (20 to 400 members). Our approach is marked by incremental expansion from the seeds with retrospective analysis to determine the ultimate boundaries of our community. We demonstrate how to increase the robustness of the general approach through bootstrapping multiple random partitions of the input set into seed and evaluation groups. We go beyond statistical comparisons against gold standards to careful subjective evaluations of our expanded communities. This process explains the causes of most disagreement between our expanded communities and our gold-standards - arguing that our expansion methods provide more reliable communities than can be extracted from reference sources/gazetteers such as Wikipedia.",Artificial intelligence; Community discovery; Discrete mathematics; Graph theory; News analysis; Social networks,Artificial intelligence; Function evaluation; Graph theory; Human computer interaction; Mathematical techniques; Community discovery; Discrete mathematics; Expansion methods; General approach; General method; Gold standards; Input set; Network communities; Random partitions; Real-world networks; Representative sample; Retrospective analysis; Seed set; Social Networks; Statistical comparisons; Subgraphs; Subjective evaluations; Wikipedia; Seed
Blocking links to minimize contamination spread in a social network,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549088338&doi=10.1145%2f1514888.1514892&partnerID=40&md5=fabda09b578ba1a985d7fc8efd8511fe,"We address the problem of minimizing the propagation of undesirable things, such as computer viruses or malicious rumors, by blocking a limited number of links in a network, which is converse to the influence maximization problem in which the most influential nodes for information diffusion is searched in a social network. This minimization problem is more fundamental than the problem of preventing the spread of contamination by removing nodes in a network. We introduce two definitions for the contamination degree of a network, accordingly define two contamination minimization problems, and propose methods for efficiently finding good approximate solutions to these problems on the basis of a naturally greedy strategy. Using large social networks, we experimentally demonstrate that the proposed methods outperform conventional link-removal methods. We also show that unlike the case of blocking a limited number of nodes, the strategy of removing nodes with high out-degrees is not necessarily effective for these problems.",Contamination diffusion; Link analysis; Social networks,Contamination; Diffusion; Optimization; Approximate solution; Contamination degree; Greedy strategies; Information diffusion; Link analysis; Maximization problem; Minimization problems; Removal method; Social Networks; Computer viruses
DOLPHIN: An efficient algorithm for mining distance-based outliers in very large datasets,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149118905&doi=10.1145%2f1497577.1497581&partnerID=40&md5=128abc75a821a00a555d88391992084b,"In this work a novel distance-based outlier detection algorithm, named DOLPHIN, working on disk-resident datasets and whose I/O cost corresponds to the cost of sequentially reading the input dataset file twice, is presented. It is both theoretically and empirically shown that the main memory usage of DOLPHIN amounts to a small fraction of the dataset and that DOLPHIN has linear time performance with respect to the dataset size. DOLPHIN gains efficiency by naturally merging together in a unified schema three strategies, namely the selection policy of objects to be maintained in main memory, usage of pruning rules, and similarity search techniques. Importantly, similarity search is accomplished by the algorithm without the need of preliminarily indexing the whole dataset, as other methods do. The algorithm is simple to implement and it can be used with any type of data, belonging to either metric or nonmetric spaces. Moreover, a modification to the basic method allows DOLPHIN to deal with the scenario in which the available buffer of main memory is smaller than its standard requirements. DOLPHIN has been compared with state-of-the-art distance-based outlier detection algorithms, showing that it is much more efficient. © 2009 ACM.",Data mining; Distance-based outliers; Outlier detection,Algorithms; Information management; Mining; Signal detection; Data set size; Data sets; Disk-resident datasets; Distance-based; Distance-based outliers; Efficient algorithm; Large datasets; Linear time; Main memory; Non-metric spaces; Outlier detection; Outlier detection algorithm; Similarity search; Standard requirements; Dolphins (structures)
"Clustering high-dimensional data: A survey on subspace clustering, pattern-based clustering, and correlation clustering",2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149084291&doi=10.1145%2f1497577.1497578&partnerID=40&md5=d02809d0d2c023adff30be49605020f4,"As a prolific research area in data mining, subspace clustering and related problems induced a vast quantity of proposed solutions. However, many publications compare a new proposition if at all with one or two competitors, or even with a so-called naïve ad hoc solution, but fail to clarify the exact problem definition. As a consequence, even if two solutions are thoroughly compared experimentally, it will often remain unclear whether both solutions tackle the same problem or, if they do, whether they agree in certain tacit assumptions and how such assumptions may influence the outcome of an algorithm. In this survey, we try to clarify: (i) the different problem definitions related to subspace clustering in general; (ii) the specific difficulties encountered in this field of research; (iii) the varying assumptions, heuristics, and intuitions forming the basis of different approaches; and (iv) how several prominent solutions tackle different problems. © 2009 ACM.",Clustering; High-dimensional data; Survey,Competition; Information management; Mining; Surveys; Clustering; Correlation clustering; High-dimensional data; Pattern-based clustering; Problem definition; Research areas; Subspace clustering; Clustering algorithms
Semi-analytical method for analyzing models and model selection measures based on moment analysis,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149141458&doi=10.1145%2f1497577.1497579&partnerID=40&md5=5b04f2e1d3203c6775115847125b6a08,"In this article we propose a moment-based method for studying models and model selection measures. By focusing on the probabilistic space of classifiers induced by the classification algorithm rather than on that of datasets, we obtain efficient characterizations for computing the moments, which is followed by visualization of the resulting formulae that are too complicated for direct interpretation. By assuming the data to be drawn independently and identically distributed from the underlying probability distribution, and by going over the space of all possible datasets, we establish general relationships between the generalization error, hold-out-set error, cross-validation error, and leave-one-out error. We later exemplify the method and the results by studying the behavior of the errors for the naive Bayes classifier. © 2009 ACM.",Classification; Generalization error; Model selection,Classifiers; Learning systems; Probability distributions; Classification; Classification algorithm; Cross validation; Data sets; Generalization error; Independently and identically distributed; Leave-one-out error; Model selection; Moment analysis; Naive Bayes classifiers; Probabilistic space; Semi-analytical methods; Method of moments
Bellwether analysis: Searching for cost-effective query-defined predictors in large databases,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149144177&doi=10.1145%2f1497577.1497582&partnerID=40&md5=1134d1745bfcea8089922619dab045ab,"How to mine massive datasets is a challenging problem with great potential value. Motivated by this challenge, much effort has concentrated on developing scalable versions of machine learning algorithms. However, the cost of mining large datasets is not just computational; preparing the datasets into the right form so that learning algorithms can be applied is usually costly, due to the human labor that is typically required and a large number of choices in data preparation, which include selecting different subsets of data and aggregating data at different granularities. We make the key observation that, for a number of practically motivated problems, these choices can be defined using database queries and analyzed in an automatic and systematic manner. Specifically, we propose a new class of data-mining problem, called bellwether analysis, in which the goal is to find a few query-defined predictors (e.g., first week sales of Peoria, IL of an item) that can be used to accurately predict the result of a target query (e.g., first year worldwide sales of the item) from a large number of queries that define candidate predictors. To make a prediction for a new item, the data needed to generate such predictors has to be collected (e.g., selling the new item in Peoria, IL for a week and collecting the sales data). A useful predictor is one that has high prediction accuracy and a low data-collection cost. We call such a cost-effective predictor a bellwether. This article introduces bellwether analysis, which integrates database query processing and predictive modeling into a single framework, and provides scalable algorithms for large datasets that cannot fit in main memory. Through a series of extensive experiments, we show that bellwethers do exist in real-world databases, and that our computation techniques achieve good efficiency on large datasets. © 2009 ACM.",Bellwether; Cost-effective prediction; Data cube; OLAP queries; Predictive models; Scalable algorithms,Cost benefit analysis; Cost effectiveness; Costs; Data acquisition; Education; Learning systems; Mathematical models; Mining; Predictive control systems; Query processing; Rough set theory; Sales; Bellwether; Data cube; OLAP queries; Predictive models; Scalable algorithms; Learning algorithms
Closed patterns meet n-ary relations,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149121799&doi=10.1145%2f1497577.1497580&partnerID=40&md5=ab1137887c2e4470276674fb04de610a,"Set pattern discovery from binary relations has been extensively studied during the last decade. In particular, many complete and efficient algorithms for frequent closed set mining are now available. Generalizing such a task to n-ary relations (n ≥ 2) appears as a timely challenge. It may be important for many applications, for example, when adding the time dimension to the popular objects × features binary case. The generality of the task (no assumption being made on the relation arity or on the size of its attribute domains) makes it computationally challenging. We introduce an algorithm called Data-Peeler. From an n-ary relation, it extracts all closed n-sets satisfying given piecewise (anti) monotonic constraints. This new class of constraints generalizes both monotonic and antimonotonic constraints. Considering the special case of ternary relations, Data-Peeler outperforms the state-of-the-art algorithms CubeMiner and Trias by orders of magnitude. These good performances must be granted to a new clever enumeration strategy allowing to efficiently enforce the closeness property. The relevance of the extracted closed n-sets is assessed on real-life 3-and 4-ary relations. Beyond natural 3-or 4-ary relations, expanding a relation with an additional attribute can help in enforcing rather abstract constraints such as the robustness with respect to binarization. Furthermore, a collection of closed n-sets is shown to be an excellent starting point to compute a tiling of the dataset. © 2009 ACM.",Closed patterns; Constraint properties; Constraint-based mining; N-ary relations; Tiling,Mining; Closed patterns; Constraint properties; Constraint-based mining; N-ary relations; Tiling; Set theory
Weighted cluster ensembles: Methods and analysis,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67549123292&doi=10.1145%2f1460797.1460800&partnerID=40&md5=649681a3f510ebf5dc91ae598b60f48b,"Cluster ensembles offer a solution to challenges inherent to clustering arising from its ill-posed nature. Cluster ensembles can provide robust and stable solutions by leveraging the consensus across multiple clustering results, while averaging out emergent spurious structures that arise due to the various biases to which each participating algorithm is tuned. In this article, we address the problem of combining multiple weighted clusters that belong to different subspaces of the input space. We leverage the diversity of the input clusterings in order to generate a consensus partition that is superior to the participating ones. Since we are dealing with weighted clusters, our consensus functions make use of the weight vectors associated with the clusters. We demonstrate the effectiveness of our techniques by running experiments with several real datasets, including high-dimensional text data. Furthermore, we investigate in depth the issue of diversity and accuracy for our ensemble methods. Our analysis and experimental results show that the proposed techniques are capable of producing a partition that is as good as or better than the best individual clustering. © 2009 ACM.",Accuracy and diversity measures; Cluster ensembles; Consensus functions; Data mining; Subspace clustering; Text data,Clustering algorithms; Information management; Mining; Accuracy and diversity measures; Cluster ensembles; Consensus functions; Subspace clustering; Text data; Cluster analysis
Mining frequent cross-graph quasi-cliques,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67549083110&doi=10.1145%2f1460797.1460799&partnerID=40&md5=15697db62043430c82b5117ac1c77e4d,"Joint mining of multiple datasets can often discover interesting, novel, and reliable patterns which cannot be obtained solely from any single source. For example, in bioinformatics, jointly mining multiple gene expression datasets obtained by different labs or during various biological processes may overcome the heavy noise in the data. Moreover, by joint mining of gene expression data and protein-protein interaction data, we may discover clusters of genes which show coherent expression patterns and also produce interacting proteins. Such clusters may be potential pathways. In this article, we investigate a novel data mining problem, mining frequent cross-graph quasi-cliques, which is generalized from several interesting applications in bioinformatics, cross-market customer segmentation, social network analysis, and Web mining. In a graph, a set of vertices S is a -quasi-clique (0 < 1) if each vertex v in S directly connects to at least (S - 1) other vertices in S. Given a set of graphs G1, , Gn and parameter min-sup (0 < min-sup 1), a set of vertices S is a frequent cross-graph quasi-clique if S is a -quasi-clique in at least min-sup n graphs, and there does not exist a proper superset of S having the property. We build a general model, show why the complete set of frequent cross-graph quasi-cliques cannot be found by previous data mining methods, and study the complexity of the problem. While the problem is difficult, we develop practical algorithms which exploit several interesting and effective techniques and heuristics to efficaciously mine frequent cross-graph quasi-cliques. A systematic performance study is reported on both synthetic and real data sets. We demonstrate some interesting and meaningful frequent cross-graph quasi-cliques in bioinformatics. The experimental results also show that our algorithms are efficient and scalable. © 2009 ACM.",Bioinformatics; Clique; Graph mining; Joint mining,Bioactivity; Bioinformatics; Data mining; Electric network analysis; Gene expression; Information management; Mining; Biological process; Clique; Customer segmentation; Data mining methods; Data mining problems; Data sets; Expression patterns; Gene Expression Data; General model; Graph mining; Joint mining; Multiple genes; Performance study; Practical algorithms; Protein-protein interactions; Single source; Social Network Analysis; Synthetic and real data; Web Mining; Graph theory
Feature-preserved sampling over streaming data,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67549144164&doi=10.1145%2f1460797.1460798&partnerID=40&md5=e7fd99aa1ea61b626f367ac892de38e6,"In this article, we explore a novel sampling model, called feature preserved sampling (FPS) that sequentially generates a high-quality sample over sliding windows. The sampling quality we consider refers to the degree of consistency between the sample proportion and the population proportion of each attribute value in a window. Due to the time-variant nature of real-world datasets, users are more likely to be interested in the most recent data. However, previous works have not been able to generate a high-quality sample over sliding windows that precisely preserves up-to-date population characteristics. Motivated by this shortcoming, we have developed the FPS algorithm, which has several advantages: (1) it sequentially generates a sample from a time-variant data source over sliding windows; (2) the execution time of FPS is linear with respect to the database size; (3) the relative proportional differences between the sample proportions and population proportions of most distinct attribute values are guaranteed to be below a specified error threshold, ε, while the relative proportion differences of the remaining attribute values are as close to ε as possible, which ensures that the generated sample is of high quality; (4) the sample rate is close to the user specified rate so that a high quality sampling result can be obtained without increasing the sample size; (5) by a thorough analytical and empirical study, we prove that FPS has acceptable space overheads, especially when the attribute values have Zipfian distributions, and FPS can also excellently preserve the population proportion of multivariate features in the sample; and (6) FPS can be applied to infinite streams and finite datasets equally, and the generated samples can be used for various applications. Our experiments on both real and synthetic data validate that FPS can effectively obtain a high quality sample of the desired size. In addition, while using the sample generated by FPS in various mining applications, a significant improvement in efficiency can be achieved without compromising the model's precision. © 2009 ACM.",Sampling; Streaming mining,Communication channels (information theory); Mining; Windows; Attribute values; Data sets; Data source; Database size; Empirical studies; Error threshold; Execution time; High quality; Population characteristics; Real-world datasets; Sample rate; Sample sizes; Sampling model; Sampling quality; Sliding Window; Space overhead; Streaming data; Streaming mining; Synthetic data; Time variant; Population statistics
On domination game analysis for microeconomic data mining,2009,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67549107020&doi=10.1145%2f1460797.1460801&partnerID=40&md5=2879b5e3bb2aa3e2903f2f25c68d1503,"Game theory is a powerful tool for analyzing the competitions among manufacturers in a market. In this article, we present a study on combining game theory and data mining by introducing the concept of domination game analysis. We present a multidimensional market model, where every dimension represents one attribute of a commodity. Every product or customer is represented by a point in the multidimensional space, and a product is said to dominate a customer if all of its attributes can satisfy the requirements of the customer. The expected market share of a product is measured by the expected number of the buyers in the customers, all of which are equally likely to buy any product dominating him. A Nash equilibrium is a configuration of the products achieving stable expected market shares for all products. We prove that Nash equilibrium in such a model can be computed in polynomial time if every manufacturer tries to modify its product in a round robin manner. To further improve the efficiency of the computation, we also design two algorithms for the manufacturers to efficiently find their best response to other products in the market. © 2009 ACM.",Data mining; Domination game; Game theory,Ad hoc networks; Commerce; Competition; Customer satisfaction; Graph theory; Information management; Mining; Polynomial approximation; Sales; Best response; Domination game; Game analysis; Market model; Market share; Microeconomic data mining; Multi-dimensional space; Nash Equilibrium; Polynomial-time; Round Robin; Game theory
On disclosure risk analysis of anonymized itemsets in the presence of prior knowledge,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149114917&doi=10.1145%2f1409620.1409623&partnerID=40&md5=5db1b05236585083054cff2bed0a0260,"Decision makers of companies often face the dilemma of whether to release data for knowledge discovery, vis-a-vis the risk of disclosing proprietary or sensitive information. Among the various methods employed for ""sanitizing"" the data prior to disclosure, we focus in this article on anonymization, given its widespread use in practice. We do due diligence to the question ""just how safe is the anonymized data?"" We consider both those scenarios when the hacker has no information and, more realistically, when the hacker may have partial information about items in the domain. We conduct our analyses in the context of frequent set mining and address the safety question at two different levels: (i) how likely of being cracked (i.e., re-identified by a hacker), are the identities of individual items and (ii) how likely are sets of items cracked? For capturing the prior knowledge of the hacker, we propose a belief function, which amounts to an educated guess of the frequency of each item. For various classes of belief functions which correspond to different degrees of prior knowledge, we derive formulas for computing the expected number of cracks of single items and for itemsets, the probability of cracking the itemsets. While obtaining, exact values for more general situations is computationally hard, we propose a series of heuristics called the O-estimates. They are easy to compute and are shown fairly accurate, justified by empirical results on real benchmark datasets. Based on the O-estimates, we propose a recipe for the decision makers to resolve their dilemma. Our recipe operates at two different levels, depending on whether the data owner wants to reason in terms of single items or sets of items (or both). Finally, we present techniques for ascertaining a hacker's knowledge of correlation in terms of co-occurrence of items likely. This information regarding the hacker's knowledge can be incorporated into our framework of disclosure risk analysis and we present experimental results demonstrating how this knowledge affects the heuristic estimates we have developed. © 2008 ACM.",Anonymization; Belief function; Bipartite graphs; Correlation; Disclosure risk; Frequent itemsets; Hacker; Matching; Prior knowledge; Sampling,Computer crime; Graph theory; Heuristic methods; Personal computing; Probability density function; Risk analysis; Risk assessment; Safety factor; Anonymization; Belief function; Bipartite graphs; Correlation; Disclosure risk; Frequent itemsets; Hacker; Matching; Prior knowledge; Knowledge based systems
Privacy-preserving decision trees over vertically partitioned data,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149091380&doi=10.1145%2f1409620.1409624&partnerID=40&md5=f3707771934eae96c391c91ad4538c20,"Privacy and security concerns can prevent sharing of data, derailing data-mining projects. Distributed knowledge discovery, if done correctly, can alleviate this problem. We introduce a generalized privacy-preserving variant of the ID3 algorithm for vertically partitioned data distributed over two or more parties. Along with a proof of security, we discuss what would be necessary to make the protocols completely secure. We also provide experimental results, giving a first demonstration of the practical complexity of secure multiparty computation-based data mining. © 2008 ACM.",Decision tree classification; Privacy,Decision support systems; Decision theory; Decision trees; Information management; Mining; Trees (mathematics); Data distributed; Data-mining; Distributed knowledge discoveries; Privacy; Privacy and securities; Secure multiparty computations; Security of data
Privacy-preserving classification of vertically partitioned data via random kernels,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149100301&doi=10.1145%2f1409620.1409622&partnerID=40&md5=07fc4ab72442aff0b70d22f875a6a3ac,"We propose a novel privacy-preserving support vector machine (SVM) classifier for a data matrix A whose input feature columns are divided into groups belonging to different entities. Each entity is unwilling to share its group of columns or make it public. Our classifier is based on the concept of a reduced kernel K(A, B′), where B′ is the transpose of a random matrix B. The column blocks of B corresponding to the different entities are privately generated by each entity and never made public. The proposed linear or nonlinear SVM classifier, which is public but does not reveal any of the privately held data, has accuracy comparable to that of an ordinary SVM classifier that uses the entire set of input features directly. © 2008 ACM.",Privacy preserving classification; Support vector machines; Vertically partitioned data,Classifiers; Columns (structural); Gears; Image retrieval; Learning systems; Multilayer neural networks; Vectors; Data matrixes; Group of columns; Input features; Privacy preserving classification; Random matrixes; Support vector machine classifiers; Svm classifiers; Vertically partitioned data; Support vector machines
Incremental tensor analysis: Theory and applications,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149108152&doi=10.1145%2f1409620.1409621&partnerID=40&md5=ed08a3fcc2b2ae9111db5be6bb8e9602,"How do we find patterns in author-keyword associations, evolving over time? Or in data cubes (tensors), with product-branchcustomer sales information? And more generally, how to summarize high-order data cubes (tensors)? How to incrementally update these patterns over time? Matrix decompositions, like principal component analysis (PCA) and variants, are invaluable tools for mining, dimensionality reduction, feature selection, rule identification in numerous settings like streaming data, text, graphs, social networks, and many more settings. However, they have only two orders (i.e., matrices, like author and keyword in the previous example). We propose to envision such higher-order data as tensors, and tap the vast literature on the topic. However, these methods do not necessarily scale up, let alone operate on semi-infinite streams. Thus, we introduce a general framework, incremental tensor analysis (ITA), which efficiently computes a compact summary for high-order and high-dimensional data, and also reveals the hidden correlations. Three variants of ITA are presented: (1) dynamic tensor analysis (DTA); (2) streaming tensor analysis (STA); and (3) window-based tensor analysis (WTA). In paricular, we explore several fundamental design trade-offs such as space efficiency, computational cost, approximation accuracy, time dependency, and model complexity. We implement all our methods and apply them in several real settings, such as network anomaly detection, multiway latent semantic indexing on citation networks, and correlation study on sensor measurements. Our empirical studies show that the proposed methods are fast and accurate and that they find interesting patterns and outliers on the real datasets. © 2008 ACM.",Multilinear algebra; Stream mining; Tensor,Algebra; Computational complexity; Correlation methods; Dynamic analysis; Information theory; Matrix algebra; Principal component analysis; Sensor networks; And models; Approximation accuracies; Citation networks; Computational costs; Correlation studies; Data cubes; Dimensional datums; Dimensionality reductions; Empirical studies; Feature selections; Fundamental designs; General frameworks; Keyword associations; Latent Semantic Indexing; Matrix decompositions; Multilinear algebra; Network anomaly detections; Order datums; Out-liers; Principal components; Real data sets; Real settings; Rule identifications; Scale-up; Sensor measurements; Social networks; Space efficiencies; Stream mining; Streaming datums; Tensor analysis; Time dependencies; Tensors
"Joint Cluster Analysis of Attribute Data and Relationship Data: The Connected k-Center Problem, Algorithms and Applications",2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859109801&doi=10.1145%2f1376815.1376816&partnerID=40&md5=79a8f5432f841f8f62ea1dc993b2393b,"Attribute data and relationship data are two principal types of data, representing the intrinsic and extrinsic properties of entities. While attribute data have been the main source of data for cluster analysis, relationship data such as social networks or metabolic networks are becoming increasingly available. It is also common to observe both data types carry complementary information such as in market segmentation and community identification, which calls for a joint cluster analysis of both data types so as to achieve better results. In this article, we introduce the novel Connected k-Center (CkC) problem, a clustering model taking into account attribute data as well as relationship data. We analyze the complexity of the problem and prove its NP-hardness. Therefore, we analyze the approximability of the problem and also present a constant factor approximation algorithm. For the special case of the CkC problem where the relationship data form a tree structure, we propose a dynamic programming method giving an optimal solution in polynomial time. We further present NetScan, a heuristic algorithm that is efficient and effective for large real databases. Our extensive experimental evaluation on real datasets demonstrates the meaningfulness and accuracy of the NetScan results. © 2008, ACM. All rights reserved.",Algorithms; Management; Performance,
Tree Model Guided Candidate Generation for Mining Frequent Subtrees from XML Documents,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962804101&doi=10.1145%2f1376815.1376818&partnerID=40&md5=fb0eae081157058243ee18fa0dd34ae8,"Due to the inherent flexibilities in both structure and semantics, XML association rules mining faces few challenges, such as: a more complicated hierarchical data structure and ordered data context. Mining frequent patterns from XML documents can be recast as mining frequent tree structures from a database of XML documents. In this study, we model a database of XML documents as a database of rooted labeled ordered subtrees. In particular, we are mainly concerned with mining frequent induced and embedded ordered subtrees. Our main contributions are as follows. We describe our unique embedding list representation of the tree structure, which enables efficient implementation of our Tree Model Guided (TMG) candidate generation. TMG is an optimal, nonredundant enumeration strategy that enumerates all the valid candidates that conform to the structural aspects of the data. We show through a mathematical model and experiments that TMG has better complexity compared to the commonly used join approach. In this article, we propose two algorithms, MB3-Miner and iMB3-Miner. MB3-Miner mines embedded subtrees. iMB3-Miner mines induced and/or embedded subtrees by using the maximum level of embedding constraint. Our experiments with both synthetic and real datasets against two well-known algorithms for mining induced and embedded subtrees, demonstrate the effectiveness and the efficiency of the proposed techniques. © 2008, ACM. All rights reserved.",Algorithms; Experimentation; FREQT; Performance; TMG; Tree mining; tree model guided; TreeMiner,
Bregman Bubble Clustering: A Robust Framework for Mining Dense Clusters,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025380737&doi=10.1145%2f1376815.1376817&partnerID=40&md5=fdf91ff3327caee4ecc006420b0eca77,"In classical clustering, each data point is assigned to at least one cluster. However, in many applications only a small subset of the available data is relevant for the problem and the rest needs to be ignored in order to obtain good clusters. Certain nonparametric density-based clustering methods find the most relevant data as multiple dense regions, but such methods are generally limited to low-dimensional data and do not scale well to large, high-dimensional datasets. Also, they use a specific notion of “distance”, typically Euclidean or Mahalanobis distance, which further limits their applicability. On the other hand, the recent One Class Information Bottleneck (OC-IB) method is fast and works on a large class of distortion measures known as Bregman Divergences, but can only find a single dense region. This article presents a broad framework for finding Κ dense clusters while ignoring the rest of the data. It includes a seeding algorithm that can automatically determine a suitable value for Κ. When Κ is forced to 1, our method gives rise to an improved version of OC-IB with optimality guarantees. We provide a generative model that yields the proposed iterative algorithm for finding Κ dense regions as a special case. Our analysis reveals an interesting and novel connection between the problem of finding dense regions and exponential mixture models; a hard model corresponding to Κ exponential mixtures with a uniform background results in a set of Κ dense clusters. The proposed method describes a highly scalable algorithm for finding multiple dense regions that works with any Bregman Divergence, thus extending density based clustering to a variety of non-Euclidean problems not addressable by earlier methods. We present empirical results on three artificial, two microarray and one text dataset to show the relevance and effectiveness of our methods. © 2008, ACM. All rights reserved.",Algorithms; Bregman divergences; Density-based clustering; expectation maximization; Experimentation; exponential family; One Class classification,
Semantic Text Similarity Using Corpus-Based Word Similarity and String Similarity,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866976063&doi=10.1145%2f1376815.1376819&partnerID=40&md5=fa68580a5d3a838cd81f1b58e9d915ab,"We present a method for measuring the semantic similarity of texts using a corpus-based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm. Existing methods for computing text similarity have focused mainly on either large documents or individual words. We focus on computing the similarity between two sentences or two short paragraphs. The proposed method can be exploited in a variety of applications involving textual knowledge representation and knowledge discovery. Evaluation results on two different data sets show that our method outperforms several competing methods. © 2008, ACM. All rights reserved.",Algorithms; corpusbased measures; Experimentation; Performance; Semantic similarity of words; similarity of short texts,
Tree model guided candidate generation for mining frequent subtrees from XML documents,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49149120315&partnerID=40&md5=a168fd783177eaff77c7b4a4b2e6267b,"Due to the inherent flexibilities in both structure and semantics, XML association rules mining faces few challenges, such as: a more complicated hierarchical data structure and ordered data context. Mining frequent patterns from XML documents can be recast as mining frequent tree structures from a database of XML documents. In this study, we model a database of XML documents as a database of rooted labeled ordered subtrees. In particular, we are mainly concerned with mining frequent induced and embedded ordered subtrees. Our main contributions are as follows. We describe our unique embedding list representation of the tree structure, which enables efficient implementation of our Tree Model Guided (TMG) candidate generation. TMG is an optimal, nonredundant enumeration strategy that enumerates all the valid candidates that conform to the structural aspects of the data. We show through a mathematical model and experiments that TMG has better complexity compared to the commonly used join approach. In this article, we propose two algorithms, MB3-Miner and iMB3-Miner. MB3-Miner mines embedded subtrees. iMB3-Miner mines induced and/or embedded subtrees by using the maximum level of embedding constraint. Our experiments with both synthetic and real datasets against two well-known algorithms for mining induced and embedded subtrees, demonstrate the effectiveness and the efficiency of the proposed techniques. © 2008 ACM.",FREQT; TMG; Tree mining; Tree model guided; TreeMiner,Associative processing; Chlorine compounds; Data mining; Data structures; Database systems; Experiments; File organization; Information management; Information theory; Markup languages; Mathematical models; Miners; Mining; XML; Association rules mining; Candidate generation; Efficient implementation; FREQT; Frequent patterns; Hierarchical data structures; Ordered data; Real datasets; Structural aspects; Subtrees; TMG; Tree mining; Tree model guided; Tree structures; TreeMiner; XML documents; Trees (mathematics)
"Joint cluster analysis of attribute data and relationship data: The connected k-center problem, algorithms and applications",2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49149121323&partnerID=40&md5=916be6a09b93f1d4b1b194634036da3e,"Attribute data and relationship data are two principal types of data, representing the intrinsic and extrinsic properties of entities. While attribute data have been the main source of data for cluster analysis, relationship data such as social networks or metabolic networks are becoming increasingly available. It is also common to observe both data types carry complementary information such as in market segmentation and community identification, which calls for a joint cluster analysis of both data types so as to achieve better results. In this article, we introduce the novel Connected k-Center (CkC) problem, a clustering model taking into account attribute data as well as relationship data. We analyze the complexity of the problem and prove its NP-hardness. Therefore, we analyze the approximability of the problem and also present a constant factor approximation algorithm. For the special case of the CkC problem where the relationship data form a tree structure, we propose a dynamic programming method giving an optimal solution in polynomial time. We further present NetScan, a heuristic algorithm that is efficient and effective for large real databases. Our extensive experimental evaluation on real datasets demonstrates the meaningfulness and accuracy of the NetScan results. © 2008 ACM.",Approximation algorithms; Attribute data; Community identification; Document clustering; Joint cluster analysis; Market segmentation; NP-hardness; Relationship data,Approximation algorithms; Chlorine compounds; Cluster analysis; Heuristic algorithms; Heuristic programming; Mathematical programming; Measurement theory; Nuclear propulsion; Polynomial approximation; Random variables; Systems engineering; Approximability; AS relationship; Attribute data; Clustering models; Community identification; Complementary information; Constant-factor approximation algorithms; Data typing; Document clustering; Experimental evaluations; Joint cluster analysis; K-center; K-center problem; Market segmentation; Metabolic networks; NP-hardness; Optimal solutions; Polynomial-time; Real databases; Real datasets; Relationship data; Social networks; Tree structures; Trees (mathematics)
Semantic text similarity using corpus-based word similarity and string similarity,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49149122316&partnerID=40&md5=052fd31558beb9c438141f91f8a5a4e1,We present a method for measuring the semantic similarity of texts using a corpus-based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm. Existing methods for computing text similarity have focused mainly on either large documents or individual words. We focus on computing the similarity between two sentences or two short paragraphs. The proposed method can be exploited in a variety of applications involving textual knowledge representation and knowledge discovery. Evaluation results on two different data sets show that our method outperforms several competing methods. © 2008 ACM.,Corpus-based measures; Semantic similarity of words; Similarity of short texts,Artificial intelligence; Information theory; Semantics; Corpus-based measures; Data sets; Evaluation results; Knowledge discovery; Longest common subsequence; Semantic similarity; Semantic similarity of words; Similarity of short texts; String matching algorithms; Text similarity; Word similarity; Knowledge representation
Bregman bubble clustering: A robust framework for mining dense clusters,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49149092575&partnerID=40&md5=e6c60377edc74db03cbd08902daa946b,"In classical clustering, each data point is assigned to at least one cluster. However, in many applications only a small subset of the available data is relevant for the problem and the rest needs to be ignored in order to obtain good clusters. Certain nonparametric density-based clustering methods find the most relevant data as multiple dense regions, but such methods are generally limited to low-dimensional data and do not scale well to large, high-dimensional datasets. Also, they use a specific notion of ""distance"", typically Euclidean or Mahalanobis distance, which further limits their applicability. On the other hand, the recent One Class Information Bottleneck (OC-IB) method is fast and works on a large class of distortion measures known as Bregman Divergences, but can only find a single dense region. This article presents a broad framework for finding k dense clusters while ignoring the rest of the data. It includes a seeding algorithm that can automatically determine a suitable value for k. When k is forced to 1, our method gives rise to an improved version of OC-IB with optimality guarantees. We provide a generative model that yields the proposed iterative algorithm for finding k dense regions as a special case. Our analysis reveals an interesting and novel connection between the problem of finding dense regions and exponential mixture models; a hard model corresponding to k exponential mixtures with a uniform background results in a set of k dense clusters. The proposed method describes a highly scalable algorithm for finding multiple dense regions that works with any Bregman Divergence, thus extending density based clustering to a variety of non-Euclidean problems not addressable by earlier methods. We present empirical results on three artificial, two microarray and one text dataset to show the relevance and effectiveness of our methods. © 2008 ACM.",Bregman divergences; Density-based clustering; Expectation maximization; Exponential family; One class classification,Biological materials; Chlorine compounds; Cluster analysis; Clustering algorithms; Flow of solids; Bregman divergences; Class information; Data points; Data-sets; Dense clusters; Dense regions; Density-based clustering; Dimensional data; Empirical results; Euclidean; Expectation maximization; Exponential family; Exponential mixtures; Generative modeling; High-dimensional; Iterative algorithms; Mahalanobis distance; Micro arrays; Mixture modelling; Non-Euclidean; Non-parametric; One class classification; Optimality; Scalable algorithms; Iterative methods
Adaptive discriminant analysis for microarray-based classification,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149123935&doi=10.1145%2f1342320.1342325&partnerID=40&md5=0bf0547dd5cc5b888d973e66d1986276,"Microarray technology has generated enormous amounts of high-dimensional gene expression data, providing a unique platform for exploring gene regulatory networks. However, the curse of dimensionality plagues effort to analyze these high throughput data. Linear Discriminant Analysis (LDA) and Biased Discriminant Analysis (BDA) are two popular techniques for dimension reduction, which pay attention to different roles of the positive and negative samples in finding discriminating subspace. However, the drawbacks of these two methods are obvious: LDA has limited efficiency in classifying sample data from subclasses with different distributions, and BDA does not account for the underlying distribution of negative samples. In this paper, we propose a novel dimension reduction technique for microarray analysis: Adaptive Discriminant Analysis (ADA), which effectively exploits favorable attributes of both BDA and LDA and avoids their unfavorable ones. ADA can find a good discriminative subspace with adaptation to different sample distributions. It not only alleviates the problem of high dimensionality, but also enhances the classification performance in the subspace with nave Bayes classifier. To learn the best model fitting the real scenario, boosted Adaptive Discriminant Analysis is further proposed. Extensive experiments on the yeast cell cycle regulation data set, and the expression data of the red blood cell cycle in malaria parasite Plasmodium falciparum demonstrate the superior performance of ADA and boosted ADA. We also present some putative genes of specific functional classes predicted by boosted ADA. Their potential functionality is confirmed by independent predictions based on Gene Ontology, demonstrating that ADA and boosted ADA are effective dimension reduction methods for microarray-based classification. © 2008 ACM.",ADA; BDA; Boosted ADA; Dimension reduction; LDA; Microarray,Bayesian networks; Gene expression; Mathematical models; Microarrays; Problem solving; ADA; BDA; Boosted ADA; Dimension reduction; LDA; Discriminant analysis
Compositional mining of multirelational biological datasets,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149157382&doi=10.1145%2f1342320.1342322&partnerID=40&md5=c65f32cb5f7326305668552f48f70eb1,"High-throughput biological screens are yielding ever-growing streams of information about multiple aspects of cellular activity. As more and more categories of datasets come online, there is a corresponding multitude of ways in which inferences can be chained across them, motivating the need for compositional data mining algorithms. In this article, we argue that such compositional data mining can be effectively realized by functionally cascading redescription mining and biclustering algorithms as primitives. Both these primitives mirror shifts of vocabulary that can be composed in arbitrary ways to create rich chains of inferences. Given a relational database and its schema, we show how the schema can be automatically compiled into a compositional data mining program, and how different domains in the schema can be related through logical sequences of biclustering and redescription invocations. This feature allows us to rapidly prototype new data mining applications, yielding greater understanding of scientific datasets. We describe two applications of compositional data mining: (i) matching terms across categories of the Gene Ontology and (ii) understanding the molecular mechanisms underlying stress response in human cells. © 2008 ACM.",Biclustering; Bioinformatics; Compositional data mining; Inductive logic programming; Redescription mining,Algorithms; Bioinformatics; Database systems; Ontology; Software prototyping; Biclustering; Compositional data mining; Inductive logic programming; Redescription mining; Data mining
A new efficient probabilistic model for mining labeled ordered trees applied to glycobiology,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149183175&doi=10.1145%2f1342320.1342326&partnerID=40&md5=1a2fca24e658e7a7904a50cba27c0f15,"Mining frequent patterns from large datasets is an important issue in data mining. Recently, complex and unstructured (or semi-structured) datasets have appeared as targets for major data mining applications, including text mining, web mining and bioinformatics. Our work focuses on labeled ordered trees, which are typically semi-structured datasets. In bioinformatics, carbohydrate sugar chains, or glycans, can be modeled as labeled ordered trees. Glycans are the third major class of biomolecules, having important roles in signaling and recognition. For mining labeled ordered trees, we propose a new probabilistic model and its efficient learning scheme which significantly improves the time and space complexity of an existing probabilistic model for labeled ordered trees. We evaluated the performance of the proposed model, comparing it with those of other probabilistic models, using synthetic as well as real datasets from glycobiology. Experimental results showed that the proposed model drastically reduced the computation time of the competing model, keeping the predictive power and avoiding overfitting to the training data. Finally, we assessed our results on real data from a variety of biological viewpoints, verifying known facts in glycobiology. © 2008 ACM.",Expectation-maximization; Labeled ordered trees; Maximum likelihood; Probabilistic models,Mathematical models; Maximum likelihood; Probability; Trees (mathematics); Expectation-maximization; Labeled ordered trees; Probabilistic models; Data mining
Discovering semantic biomedical relations utilizing the Web,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149129644&doi=10.1145%2f1342320.1342323&partnerID=40&md5=317c7a15ce0e7b7ca51c38fd2af0d4a4,"To realize the vision of a Semantic Web for Life Sciences, discovering relations between resources is essential. It is very difficult to automatically extract relations from Web pages expressed in natural language formats. On the other hand, because of the explosive growth of information, it is difficult to manually extract the relations. In this paper we present techniques to automatically discover relations between biomedical resources from the Web. For this purpose we retrieve relevant information from Web Search engines and Pubmed database using various lexico-syntactic patterns as queries over SOAP web services. The patterns are initially handcrafted but can be progressively learnt. The extracted relations can be used to construct and augment ontologies and knowledge bases. Experiments are presented for general biomedical relation discovery and domain specific search to show the usefulness of our technique. © 2008 ACM.",Ontology construction; Relation identification,Database systems; Resource allocation; Search engines; Syntactics; Websites; Life Sciences; Ontology construction; Relation identification; Semantic Web
Developmental stage annotation of Drosophila gene expression pattern images via an entire solution path for LDA,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149163917&doi=10.1145%2f1342320.1342324&partnerID=40&md5=e03f976c8f32ec277d757910ce400b3e,"Gene expression in a developing embryo occurs in particular cells (spatial patterns) in a time-specific manner (temporal patterns), which leads to the differentiation of cell fates. Images of a Drosophila melanogaster embryo at a given developmental stage, showing a particular gene expression pattern revealed by a gene-specific probe, can be compared for spatial overlaps. The comparison is fundamentally important to formulating and testing gene interaction hypotheses. Expression pattern comparison is most biologically meaningful when images from a similar time point (developmental stage) are compared. In this paper, we present LdaPath, a novel formulation of Linear Discriminant Analysis (LDA) for automatic developmental stage range classification. It employs multivariate linear regression with the L1-norm penalty controlled by a regularization parameter for feature extraction and visualization. LdaPath computes an entire solution path for all values of regularization parameter with essentially the same computational cost as fitting one LDA model. Thus, it facilitates efficient model selection. It is based on the equivalence relationship between LDA and the least squares method for multiclass classifications. This equivalence relationship is established under a mild condition, which we show empirically to hold for many high-dimensional datasets, such as expression pattern images. Our experiments on a collection of 2705 expression pattern images show the effectiveness of the proposed algorithm. Results also show that the LDA model resulting from LdaPath is sparse, and irrelevant features may be removed. Thus, LdaPath provides a general framework for simultaneous feature selection and feature extraction. © 2008 ACM.",Dimensionality reduction; Gene expression pattern image; Linear discriminant analysis; Linear regression,Algorithms; Feature extraction; Gene expression; Linear regression; Mathematical models; Parameter estimation; Dimensionality reduction; Gene expression pattern image; Linear discriminant analysis; Image analysis
Introduction to special issue on bioinformatics,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149087300&doi=10.1145%2f1342320.1342321&partnerID=40&md5=f5f225c6828dc82b0e43fb8a9b35a317,[No abstract available],,
Finding hierarchical heavy hitters in streaming data,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149089260&doi=10.1145%2f1324172.1324174&partnerID=40&md5=8fa949b5e54bf450b506f0c6a646b43a,"Data items that arrive online as streams typically have attributes which take values from one or more hierarchies (time and geographic location, source and destination IP addresses, etc.). Providing an aggregate view of such data is important for summarization, visualization, and analysis. We develop an aggregate view based on certain organized sets of large-valued regions (heavy hitters) corresponding to hierarchically discounted frequency counts. We formally define the notion of hierarchical heavy hitters (HHHs). We first consider computing (approximate) HHHs over a data stream drawn from a single hierarchical attribute. We formalize the problem and give deterministic algorithms to find them in a single pass over the input. In order to analyze a wider range of realistic data streams (e.g., from IP traffic-monitoring applications), we generalize this problem to multiple dimensions. Here, the semantics of HHHs are more complex, since a child node can have multiple parent nodes. We present online algorithms that find approximate HHHs in one pass, with provable accuracy guarantees. The product of hierarchical dimensions forms a mathematical lattice structure. Our algorithms exploit this structure, and so are able to track approximate HHHs using only a small, fixed number of statistics per stored item, regardless of the number of dimensions. We show experimentally, using real data, that our proposed algorithms yields outputs which are very similar (virtually identical, in many cases) to offline computations of the exact solutions, whereas straightforward heavy-hitters-based approaches give significantly inferior answer quality. Furthermore, the proposed algorithms result in an order of magnitude savings in data structure size while performing competitively. © 2008 ACM.",Approximation algorithms; Data mining; Network data analysis,Approximation algorithms; Computation theory; Data mining; Data reduction; Data structures; Hierarchical heavy hitters (HHH); Network data analysis; Computer networks
Learning correlations using the mixture-of-subsets model,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149109249&doi=10.1145%2f1324172.1324175&partnerID=40&md5=c9489e23ab85c71f485e07ae1fe93c70,"Using a mixture of random variables to model data is a tried-and-tested method common in data mining, machine learning, and statistics. By using mixture modeling it is often possible to accurately model even complex, multimodal data via very simple components. However, the classical mixture model assumes that a data point is generated by a single component in the model. A lot of datasets can be modeled closer to the underlying reality if we drop this restriction. We propose a probabilistic framework, the mixture-of-subsets (MOS) model, by making two fundamental changes to the classical mixture model. First, we allow a data point to be generated by a set of components, rather than just a single component. Next, we limit the number of data attributes that each component can influence. We also propose an EM framework to learn the MOS model from a dataset, and experimentally evaluate it on real, high-dimensional datasets. Our results show that the MOS model learned from the data represents the underlying nature of the data accurately. © 2008 ACM.",EM algorithm; High-dimensional data; Mixture modeling,Data mining; Data structures; Mathematical models; Statistical methods; EM algorithm; High dimensional data; Mixture modeling; Set theory
Topic taxonomy adaptation for group profiling,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149100915&doi=10.1145%2f1324172.1324173&partnerID=40&md5=a93b29191e54aba5ea9f2e224d4354d6,"A topic taxonomy is an effective representation that describes salient features of virtual groups or online communities. A topic taxonomy consists of topic nodes. Each internal node is defined by its vertical path (i.e., ancestor and child nodes) and its horizonal list of attributes (or terms). In a text-dominant environment, a topic taxonomy can be used to flexibly describe a group's interests with varying granularity. However, the stagnant nature of a taxonomy may fail to timely capture the dynamic change of a group's interest. This article addresses the problem of how to adapt a topic taxonomy to the accumulated data that reflects the change of a group's interest to achieve dynamic group profiling. We first discuss the issues related to topic taxonomy. We next formulate taxonomy adaptation as an optimization problem to find the taxonomy that best fits the data. We then present a viable algorithm that can efficiently accomplish taxonomy adaptation. We conduct extensive experiments to evaluate our approach's efficacy for group profiling, compare the approach with some alternatives, and study its performance for dynamic group profiling. While pointing out various applications of taxonomy adaption, we suggest some future work that can take advantage of burgeoning Web 2.0 services for online targeted marketing, counterterrorism in connecting dots, and community tracking. © 2008 ACM.",Dynamic profiling; Group interest; Taxonomy adjustment; Text hierarchical classification; Topic taxonomy,Electronic crime countermeasures; Marketing; Text processing; Virtual reality; Dynamic profiling; Text hierarchical classification; Topic taxonomy; Taxonomies
A clustering framework based on subjective and objective validity criteria,2008,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149107406&doi=10.1145%2f1324172.1324176&partnerID=40&md5=77bff650b60874af37492ba85c8fcca3,"Clustering, as an unsupervised learning process is a challenging problem, especially in cases of high-dimensional datasets. Clustering result quality can benefit from user constraints and objective validity assessment. In this article, we propose a semisupervised framework for learning the weighted Euclidean subspace, where the best clustering can be achieved. Our approach capitalizes on: (i) user constraints; and (ii) the quality of intermediate clustering results in terms of their structural properties. The proposed framework uses the clustering algorithm and the validity measure as its parameters. We develop and discuss algorithms for learning and tuning the weights of contributing dimensions and defining the best clustering obtained by satisfying user constraints. Experimental results on benchmark datasets demonstrate the superiority of the proposed approach in terms of improved clustering accuracy. © 2008 ACM.",Cluster validity; Data mining; Semisupervised learning; Similarity measure learning; Space learning,Algorithms; Constraint theory; Data structures; Learning systems; User interfaces; Cluster validity; Semisupervised learning; Similarity measure learning; Data mining
Introduction to special issue ACM SIGKDD 2006,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049002623&doi=10.1145%2f1297332.1297333&partnerID=40&md5=b15093ca3c1948cc655566e4f9ce4671,[No abstract available],,
RIC: Parameter-free noise-robust clustering,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049038390&doi=10.1145%2f1297332.1297334&partnerID=40&md5=6fda4794c40c992adef067108433e79c,"How do we find a natural clustering of a real-world point set which contains an unknown number of clusters with different shapes, and which may be contaminated by noise As most clustering algorithms were designed with certain assumptions (Gaussianity), they often require the user to give input parameters, and are sensitive to noise. In this article, we propose a robust framework for determining a natural clustering of a given dataset, based on the minimum description length (MDL) principle. The proposed framework, robust information-theoretic clustering (RIC), is orthogonal to any known clustering algorithm: Given a preliminary clustering, RIC purifies these clusters from noise, and adjusts the clusterings such that it simultaneously determines the most natural amount and shape (subspace) of the clusters. Our RIC method can be combined with any clustering technique ranging from K-means and K-medoids to advanced methods such as spectral clustering. In fact, RIC is even able to purify and improve an initial coarse clustering, even if we start with very simple methods. In an extension, we propose a fully automatic stand-alone clustering method and efficiency improvements. RIC scales well with the dataset size. Extensive experiments on synthetic and real-world datasets validate the proposed RIC framework. © 2007 ACM.",Clustering; Data summarization; Noise robustness; Parameter-free data mining,Acoustic noise; Automation; Data mining; Optimization; Parameter estimation; Robustness (control systems); Data summarization; Noise robustness; Parameter-free data mining; Robust information-theoretic clustering (RIC); Clustering algorithms
Learning to detect events with Markov-modulated poisson processes,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049023828&doi=10.1145%2f1297332.1297337&partnerID=40&md5=f8d76289773bec4784bdc9c72b139609,"Time-series of count data occur in many different contexts, including Internet navigation logs, freeway traffic monitoring, and security logs associated with buildings. In this article we describe a framework for detecting anomalous events in such data using an unsupervised learning approach. Normal periodic behavior is modeled via a time-varying Poisson process model, which in turn is modulated by a hidden Markov process that accounts for bursty events. We outline a Bayesian framework for learning the parameters of this model from count time-series. Two large real-world datasets of time-series counts are used as testbeds to validate the approach, consisting of freeway traffic data and logs of people entering and exiting a building. We show that the proposed model is significantly more accurate at detecting known events than a more traditional threshold-based technique. We also describe how the model can be used to investigate different degrees of periodicity in the data, including systematic day-of-week and time-of-day effects, and to make inferences about different aspects of events such as number of vehicles or people involved. The results indicate that the Markov-modulated Poisson framework provides a robust and accurate framework for adaptively and autonomously learning how to separate unusual bursty events from traces of normal human activity. © 2007 ACM.",Event detection; Markov modulated; Poisson,Bayesian networks; Internet; Markov processes; Time series analysis; Time varying systems; Unsupervised learning; Bayesian framework; Data periodicity; Event detection; Time-varying Poisson processes; Poisson distribution
Assessing data mining results via swap randomization,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049039428&doi=10.1145%2f1297332.1297338&partnerID=40&md5=713cf7a6d854b2152a91814ca67c22c6,"The problem of assessing the significance of data mining results on high-dimensional 0 - 1 datasets has been studied extensively in the literature. For problems such as mining frequent sets and finding correlations, significance testing can be done by standard statistical tests such as chi-square, or other methods. However, the results of such tests depend only on the specific attributes and not on the dataset as a whole. Moreover, the tests are difficult to apply to sets of patterns or other complex results of data mining algorithms. In this article, we consider a simple randomization technique that deals with this shortcoming. The approach consists of producing random datasets that have the same row and column margins as the given dataset, computing the results of interest on the randomized instances and comparing them to the results on the actual data. This randomization technique can be used to assess the results of many different types of data mining algorithms, such as frequent sets, clustering, and spectral analysis. To generate random datasets with given margins, we use variations of a Markov chain approach which is based on a simple swap operation. We give theoretical results on the efficiency of different randomization methods, and apply the swap randomization method to several well-known datasets. Our results indicate that for some datasets the structure discovered by the data mining algorithms is expected, given the row and column margins of the datasets, while for other datasets the discovered structure conveys information that is not captured by the margin counts. © 2007 ACM.",0 - 1 data; Randomization tests; Significance testing; Swaps,Algorithms; Correlation methods; Problem solving; Random processes; Spectrum analysis; Statistical tests; Data mining algorithms; Randomization techniques; Randomization tests; Significance testing; Data mining
Measuring and extracting proximity graphs in networks,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049002016&doi=10.1145%2f1297332.1297336&partnerID=40&md5=e63a4f23b5d5fb2c57863167b2c71589,"Measuring distance or some other form of proximity between objects is a standard data mining tool. Connection subgraphs were recently proposed as a way to demonstrate proximity between nodes in networks. We propose a new way of measuring and extracting proximity in networks called cycle-free effective conductance (CFEC). Importantly, the measured proximity is accompanied with a proximity subgraph which allows assessing and understanding measured values. Our proximity calculation can handle more than two endpoints, directed edges, is statistically well behaved, and produces an effectiveness score for the computed subgraphs. We provide an efficient algorithm to measure and extract proximity. Also, we report experimental results and show examples for four large network datasets: a telecommunications calling graph, the IMDB actors graph, an academic coauthorship network, and a movie recommendation system. © 2007 ACM.",Connection subgraph; Cycle-free escape probability; Escape probability; Graph mining; Proximity; Proximity subgraph; Random walk,Algorithms; Data mining; Data reduction; Feature extraction; Problem solving; Random processes; Connection subgraphs; Cycle-free escape probability; Graph mining; Proximity subgraphs; Random walk; Graph theory
Semantic annotation of frequent patterns,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049034642&doi=10.1145%2f1297332.1297335&partnerID=40&md5=84b710fed0ee49d10addaa6182284105,"Using frequent patterns to analyze data has been one of the fundamental approaches in many data mining applications. Research in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns, but little attention has been paid to the important next step - -interpreting the discovered frequent patterns. Although the compression and summarization of frequent patterns has been studied in some recent work, the proposed techniques there can only annotate a frequent pattern with nonsemantical information (e.g., support), which provides only limited help for a user to understand the patterns. In this article, we study the novel problem of generating semantic annotations for frequent patterns. The goal is to discover the hidden meanings of a frequent pattern by annotating it with in-depth, concise, and structured information. We propose a general approach to generate such an annotation for a frequent pattern by constructing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach can well incorporate the user's prior knowledge, and has potentially many applications, such as generating a dictionary-like description for a pattern, finding synonym patterns, discovering semantic relations, and summarizing semantic classes of a set of frequent patterns. Experiments on different datasets show that our approach is effective in generating semantic pattern annotations. © 2007 ACM.",Frequent pattern; Pattern annotation; Pattern context; Pattern semantic analysis,Data mining; Data reduction; Feature extraction; Mathematical models; Problem solving; User interfaces; Frequent patterns; Pattern annotation; Pattern contexts; Pattern semantic analysis; Semantic Web
Mining periodic patterns with gap requirement from sequences,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548236885&doi=10.1145%2f1267066.1267068&partnerID=40&md5=bb6882ded187989276074760ec76c188,"We study a problem of mining frequently occurring periodic patterns with a gap requirement from sequences. Given a character sequence S of length L and a pattern P of length l, we consider P a frequently occurring pattern in S if the probability of observing P given a randomly picked length-l subsequence of S exceeds a certain threshold. In many applications, particularly those related to bioinformatics, interesting patterns are periodic with a gap requirement. That is to say, the characters in P should match subsequences of S in such a way that the matching characters in S are separated by gaps of more or less the same size. We show the complexity of the mining problem and discuss why traditional mining algorithms are computationally infeasible. We propose practical algorithms for solving the problem and study their characteristics. We also present a case study in which we apply our algorithms on some DNA sequences. We discuss some interesting patterns obtained from the case study. © 2007 ACM.",Gap requirement; Periodic pattern; Sequence mining,Bioinformatics; Data mining; Pattern matching; Random processes; Gap requirement; Periodic pattern; Sequence mining; Time varying systems
Twain: Two-end association miner with precise frequent exhibition periods,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548222585&doi=10.1145%2f1267066.1267069&partnerID=40&md5=9c0ae934fa066d509df0b3b433ca6640,"We investigate the general model of mining associations in a temporal database, where the exhibition periods of items are allowed to be different from one to another. The database is divided into partitions according to the time granularity imposed. Such temporal association rules allow us to observe short-term but interesting patterns that are absent when the whole range of the database is evaluated altogether. Prior work may omit some temporal association rules and thus have limited practicability. To remedy this and to give more precise frequent exhibition periods of frequent temporal itemsets, we devise an efficient algorithm Twain (standing for TWo end AssocIation miNer.) Twain not only generates frequent patterns with more precise frequent exhibition periods, but also discovers more interesting frequent patterns. Twain employs Start time and End time of each item to provide precise frequent exhibition period while progressively handling itemsets from one partition to another. Along with one scan of the database, Twain can generate frequent 2-itemsets directly according to the cumulative filtering threshold. Then, Twain adopts the scan reduction technique to generate all frequent k-itemsets (k > 2) from the generated frequent 2-itemsets. Theoretical properties of Twain are derived as well in this article. The experimental results show that Twain outperforms the prior works in the quality of frequent patterns, execution time, I/O cost, CPU overhead and scalability. © 2007 ACM.",Association; Temporal,Association rules; Database systems; Mathematical models; Time varying systems; Exhibition periods; Limited practicability; Data mining
Extrapolation errors in linear model trees,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548281605&doi=10.1145%2f1267066.1267067&partnerID=40&md5=49fe7f6addb7996e000e6dbfdb160462,"Prediction errors from a linear model tend to be larger when extrapolation is involved, particularly when the model is wrong. This article considers the problem of extrapolation and interpolation errors when a linear model tree is used for prediction. It proposes several ways to curtail the size of the errors, and uses a large collection of real datasets to demonstrate that the solutions are effective in reducing the average mean squared prediction error. The article also provides a proof that, if a linear model is correct, the proposed solutions have no undesirable effects as the training sample size tends to infinity. © 2007 ACM.",Decision tree; Prediction; Regression; Statistics,Error analysis; Interpolation; Linear systems; Mathematical models; Interpolation errors; Real datasets; Decision trees
ℓ-diversity: Privacy beyond k-anonymity,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248181923&doi=10.1145%2f1217299.1217302&partnerID=40&md5=96bcdbab9853e3202a24943654bd6d81,"Publishing data about individuals without revealing sensitive information about them is an important problem. In recent years, a new definition of privacy called k-anonymity has gained popularity. In a k-anonymized dataset, each record is indistinguishable from at least k - 1 other records with respect to certain identifying attributes. In this article, we show using two simple attacks that a k-anonymized dataset has some subtle but severe privacy problems. First, an attacker can discover the values of sensitive attributes when there is little diversity in those sensitive attributes. This is a known problem. Second, attackers often have background knowledge, and we show that k-anonymity does not guarantee privacy against attackers using background knowledge. We give a detailed analysis of these two attacks, and we propose a novel and powerful privacy criterion called ℓ-diversity that can defend against such attacks. In addition to building a formal foundation for ℓ-diversity, we show in an experimental evaluation that ℓ-diversity is practical and can be implemented efficiently. © 2007 ACM.",ℓ-diversity; Data privacy; k-anonymity; Privacy-preserving data publishing,Computational methods; Computer crime; Data structures; Electronic publishing; Information analysis; Knowledge acquisition; Data publishing; Datasets; Data privacy
Collective entity resolution in relational data,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248229658&doi=10.1145%2f1217299.1217304&partnerID=40&md5=9a3adc0c31a52ec56c68b76eb6a9d600,"Many databases contain uncertain and imprecise references to real-world entities. The absence of identifiers for the underlying entities often results in a database which contains multiple references to the same entity. This can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. These problems can be alleviated through the use of entity resolution. Entity resolution involves discovering the underlying entities and mapping each database reference to these entities. Traditionally, entities are resolved using pairwise similarity over the attributes of references. However, there is often additional relational information in the data. Specifically, references to different entities may cooccur. In these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. We propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. We investigate the impact that different relational similarity measures have on entity resolution quality. We evaluate our collective entity resolution algorithm on multiple real-world databases. We show that it improves entity resolution performance over both attribute-based baselines and over algorithms that consider relational information but do not resolve entities collectively. In addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute-based algorithms. © 2007 ACM.",Data cleaning; Entity resolution; Graph clustering; Record linkage,Clustering algorithms; Computational methods; Data mining; Knowledge acquisition; Query processing; Real time systems; Graph clustering; Relational clustering algorithms; Relational database systems
Clustering aggregation,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248168069&doi=10.1145%2f1217299.1217303&partnerID=40&md5=a3d9a4fbc97e6c25e310da87faf8c636,"We consider the following problem: given a set of clusterings, find a single clustering that agrees as much as possible with the input clusterings. This problem, clustering aggregation, appears naturally in various contexts. For example, clustering categorical data is an instance of the clustering aggregation problem; each categorical attribute can be viewed as a clustering of the input rows where rows are grouped together if they take the same value on that attribute. Clustering aggregation can also be used as a metaclustering method to improve the robustness of clustering by combining the output of multiple algorithms. Furthermore, the problem formulation does not require a priori information about the number of clusters; it is naturally determined by the optimization function. In this article, we give a formal statement of the clustering aggregation problem, and we propose a number of algorithms. Our algorithms make use of the connection between clustering aggregation and the problem of correlation clustering. Although the problems we consider are NP-hard, for several of our methods, we provide theoretical guarantees on the quality of the solutions. Our work provides the best deterministic approximation algorithm for the variation of the correlation clustering problem we consider. We also show how sampling can be used to scale the algorithms for large datasets. We give an extensive empirical evaluation demonstrating the usefulness of the problem and of the solutions. © 2007 ACM.",Clustering aggregation; Clustering categorical data; Correlation clustering; Data clustering,Approximation algorithms; Computational complexity; Correlation methods; Function evaluation; Optimization; Problem solving; Clustering aggregation; Data clustering; Metaclustering; Optimization functions; Cluster analysis
ACM Transactions on Knowledge Discovery from Data: Introduction,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248175725&doi=10.1145%2f1217299.1217300&partnerID=40&md5=157a3771af60e537d042610b0da9d9d8,[No abstract available],,
Graph evolution: Densification and shrinking diameters,2007,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248205060&doi=10.1145%2f1217299.1217301&partnerID=40&md5=61863ea4c272efd6fb1730c99ca4019e,"How do real graphs evolve over time What are normal growth patterns in social, technological, and information networks Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time. Here we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time with the number of edges growing superlinearly in the number of nodes. Second, the average distance between nodes often shrinks over time in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O(log n) or O(log(log n)). Existing graph generation models do not exhibit these types of behavior even at a qualitative level. We provide a new graph generator, based on a forest fire spreading process that has a simple, intuitive justification, requires very few parameters (like the flammability of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study. We also notice that the forest fire model exhibits a sharp transition between sparse graphs and graphs that are densifying. Graphs with decreasing distance between the nodes are generated around this transition point. Last, we analyze the connection between the temporal evolution of the degree distribution and densification of a graph. We find that the two are fundamentally related. We also observe that real networks exhibit this type of relation between densification and the degree distribution. © 2007 ACM.",Densification power laws; Graph generators; Graph mining; Heavy-tailed distributions; Small-world phenomena,Computational methods; Data mining; Graphic methods; Mathematical models; Parameter estimation; Probability distributions; Densification power laws; Graph generation models; Graph generators; Graph mining; Computer graphics
Online Learning Bipartite Matching with Non-stationary Distributions,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131159329&doi=10.1145%2f3502734&partnerID=40&md5=b43e94cd26ec2d0fa32f54e17b7f5f50,"Online bipartite matching has attracted wide interest since it can successfully model the popular online car-hailing problem and sharing economy. Existing works consider this problem under either adversary setting or i.i.d. setting. The former is too pessimistic to improve the performance in the general case; the latter is too optimistic to deal with the varying distribution of vertices. In this article, we initiate the study of the non-stationary online bipartite matching problem, which allows the distribution of vertices to vary with time and is more practical. We divide the non-stationary online bipartite matching problem into two subproblems, the matching problem and the selecting problem, and solve them individually. Combining Batch algorithms and deep Q-learning networks, we first construct a candidate algorithm set to solve the matching problem. For the selecting problem, we use a classical online learning algorithm, Exp3, as a selector algorithm and derive a theoretical bound. We further propose CDUCB as a selector algorithm by integrating distribution change detection into UCB. Rigorous theoretical analysis demonstrates that the performance of our proposed algorithms is no worse than that of any candidate algorithms in terms of competitive ratio. Finally, extensive experiments show that our proposed algorithms have much higher performance for the non-stationary online bipartite matching problem comparing to the state-of-the-art. © 2022 Association for Computing Machinery.",non-stationary distributions; Online bipartite matching; online learning; reinforcement learning,Deep learning; E-learning; Learning algorithms; Bipartite matching problems; Bipartite matchings; Non-stationary distribution; Nonstationary; Online bipartite matching; Online learning; Optimistics; Performance; Stationary distribution; Reinforcement learning
Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131152494&doi=10.1145%2f3494566&partnerID=40&md5=837db321b162973d86f64186023d8542,"Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis. © 2022 Association for Computing Machinery.",Intelligent data; tourism industry,Classification (of information); Data handling; Data mining; Extended Kalman filters; Filtration; Classification of data; Data mining methods; Data-mining techniques; Intelligent data; Intelligent data analysis; Optimisations; Pattern analysis; Pre-processed data; Support vectors machine; Tourism industry; Support vector machines
Stochastic Variational Optimization of a Hierarchical Dirichlet Process Latent Beta-Liouville Topic Model,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131206122&doi=10.1145%2f3502727&partnerID=40&md5=8fd1a6d4af18eba82b4969b4d1b30852,"In topic models, collections are organized as documents where they arise as mixtures over latent clusters called topics. A topic is a distribution over the vocabulary. In large-scale applications, parametric or finite topic mixture models such as LDA (latent Dirichlet allocation) and its variants are very restrictive in performance due to their reduced hypothesis space. In this article, we address the problem related to model selection and sharing ability of topics across multiple documents in standard parametric topic models. We propose as an alternative a BNP (Bayesian nonparametric) topic model where the HDP (hierarchical Dirichlet process) prior models documents topic mixtures through their multinomials on infinite simplex. We, therefore, propose asymmetric BL (Beta-Liouville) as a diffuse base measure at the corpus level DP (Dirichlet process) over a measurable space. This step illustrates the highly heterogeneous structure in the set of all topics that describes the corpus probability measure. For consistency in posterior inference and predictive distributions, we efficiently characterize random probability measures whose limits are the global and local DPs to approximate the HDP from the stick-breaking formulation with the GEM (Griffiths-Engen-McCloskey) random variables. Due to the diffuse measure with the BL prior as conjugate to the count data distribution, we obtain an improved version of the standard HDP that is usually based on symmetric Dirichlet (Dir). In addition, to improve coordinate ascent framework while taking advantage of its deterministic nature, our model implements an online optimization method based on stochastic, at document level, variational inference to accommodate fast topic learning when processing large collections of text documents with natural gradient. The high value in the predictive likelihood per document obtained when compared to the performance of its competitors is also consistent with the robustness of our fully asymmetric BL-based HDP. While insuring the predictive accuracy of the model using the probability of the held-out documents, we also added a combination of metrics such as the topic coherence and topic diversity to improve the quality and interpretability of the topics discovered. We also compared the performance of our model using these metrics against the standard symmetric LDA. We show that online HDP-LBLA (Latent BL Allocation)'s performance is the asymptote for parametric topic models. The accuracy in the results (improved predictive distributions of the held out) is a product of the model's ability to efficiently characterize dependency between documents (topic correlation) as now they can easily share topics, resulting in a much robust and realistic compression algorithm for information modeling. © 2022 Association for Computing Machinery.",Bayesian nonparametric topic model; Beta-Liouville distribution; Hierarchical dirichlet process; predictive distributions; stochastic and variational optimizations,Data mining; Mixtures; Probability distributions; Statistics; Stochastic models; Bayesian nonparametric topic model; Bayesian nonparametrics; Beta-liouville distribution; Hierarchical Dirichlet process; Liouville; Performance; Predictive distributions; Stochastic optimizations; Topic Modeling; Variational optimization; Stochastic systems
Who will Win the Data Science Competition? Insights from KDD Cup 2019 and Beyond,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131169490&doi=10.1145%2f3511896&partnerID=40&md5=03b40a16dc518f546e2fb748a67d8805,"Data science competitions are becoming increasingly popular for enterprises collecting advanced innovative solutions and allowing contestants to sharpen their data science skills. Most existing studies about data science competitions have a focus on improving task-specific data science techniques, such as algorithm design and parameter tuning. However, little effort has been made to understand the data science competition itself. To this end, in this article, we shed light on the team's competition performance, and investigate the team's evolving performance in the crowd-sourcing competitive innovation context. Specifically, we first acquire and construct multi-sourced datasets of various data science competitions, including the KDD Cup 2019 machine learning competition and beyond. Then, we conduct an empirical analysis to identify and quantify a rich set of features that are significantly correlated with teams' future performances. By leveraging team's rank as a proxy, we observe ""the stronger, the stronger""rule; that is, top-ranked teams tend to keep their advantages and dominate weaker teams for the rest of the competition. Our results also confirm that teams with diversified backgrounds tend to achieve better performances. After that, we formulate the team's future rank prediction problem and propose the Multi-Task Representation Learning (MTRL) framework to model both static features and dynamic features. Extensive experimental results on four real-world data science competitions demonstrate the team's future performance can be well predicted by using MTRL. Finally, we envision our study will not only help competition organizers to understand the competition in a better way, but also provide strategic implications to contestants, such as guiding the team formation and designing the submission strategy. © 2022 Association for Computing Machinery.",Data science competition prediction; deep representation learning; multi-task learning,Data Science; Algorithm design; Algorithm parameters; Data science competition prediction; Deep representation learning; Design tuning; Future performance; Innovative solutions; Multi tasks; Performance; Task representation; Deep learning
BhBF: A Bloom Filter Using BhSequences for Multi-set Membership Query,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131200383&doi=10.1145%2f3502735&partnerID=40&md5=239ae0c3b70f9f4399c1b7108d37cf32,"Multi-set membership query is a fundamental issue for network functions such as packet processing and state machines monitoring. Given the rigid query speed and memory requirements, it would be promising if a multi-set query algorithm can be designed based on Bloom filter (BF), a space-efficient probabilistic data structure. However, existing efforts on multi-set query based on BF suffer from at least one of the following drawbacks: low query speed, low query accuracy, limitation in only supporting insertion and query operations, or limitation in the set size. To address the issues, we design a novel Bh sequence-based Bloom filter (BhBF) for multi-set query, which supports four operations: insertion, query, deletion, and update. In BhBF, the set ID is encoded as a code in a Bh sequence. Exploiting good properties of Bh sequences, we can correctly decode the BF cells to obtain the set IDs even when the number of hash collisions is high, which brings high query accuracy. In BhBF, we propose two strategies to further speed up the query speed and increase the query accuracy. On the theoretical side, we analyze the false positive and classification failure rate of our BhBF. Our results from extensive experiments over two real datasets demonstrate that BhBF significantly advances state-of-the-art multi-set query algorithms. © 2022 Association for Computing Machinery.",bloom filter; Multi-set membership query,Failure analysis; Bloom filters; Machine monitoring; Multi-set membership query; Multi-sets; Network functions; Packet processing; Processing machines; Query algorithms; Set membership query; State-machine; Data structures
Graph Neural News Recommendation with User Existing and Potential Interest Modeling,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128903782&doi=10.1145%2f3511708&partnerID=40&md5=2cf587bb3bdf20aa5d99f0fb91774006,"Personalized news recommendations can alleviate the information overload problem. To enable personalized recommendation, one critical step is to learn a comprehensive user representation to model her/his interests. Many existing works learn user representations from the historical clicked news articles, which reflect their existing interests. However, these approaches ignore users' potential interests and pay less attention to news that may interest the users in the future. To address this problem, we propose a novel Graph neural news Recommendation model with user Existing and Potential interest modeling, named GREP. Different from existing works, GREP introduces three modules to jointly model users' existing and potential interests: (1) Existing Interest Encoding module mines user historical clicked news and applies the multi-head self-attention mechanism to capture the relatedness among the news; (2) Potential Interest Encoding module leverages the graph neural network to explore the user potential interests on the knowledge graph; and (3) Bi-directional Interaction module dynamically builds a news-entity bipartite graph to further enrich two interest representations. Finally, GREP combines the existing and potential interest representations to represent the user and leverages a prediction layer to estimate the clicking probability of the candidate news. Experiments on two real-world large-scale datasets demonstrate the state-of-the-art performance of GREP. © 2022 Association for Computing Machinery.",graph neural networks; knowledge graph; Recommendation,Encoding (symbols); Graph neural networks; Large dataset; Recommender systems; Signal encoding; User profile; Encoding modules; Graph neural networks; Information overloads; Knowledge graphs; Learn+; News recommendation; Overload problems; Personalized news; Personalized recommendation; Recommendation; Knowledge graph
On the Robustness of Metric Learning: An Adversarial Perspective,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131173224&doi=10.1145%2f3502726&partnerID=40&md5=b6009cc8d95a549421c5aa0d848616ef,"Metric learning aims at automatically learning a distance metric from data so that the precise similarity between data instances can be faithfully reflected, and its importance has long been recognized in many fields. An implicit assumption in existing metric learning works is that the learned models are performed in a reliable and secure environment. However, the increasingly critical role of metric learning makes it susceptible to a risk of being malicious attacked. To well understand the performance of metric learning models in adversarial environments, in this article, we study the robustness of metric learning to adversarial perturbations, which are also known as the imperceptible changes to the input data that are crafted by an attacker to fool a well-learned model. However, different from traditional classification models, metric learning models take instance pairs rather than individual instances as input, and the perturbation on one instance may not necessarily affect the prediction result for an instance pair, which makes it more difficult to study the robustness of metric learning. To address this challenge, in this article, we first provide a definition of pairwise robustness for metric learning, and then propose a novel projected gradient descent-based attack method (called AckMetric) to evaluate the robustness of metric learning models. To further explore the capability of the attacker to change the prediction results, we also propose a theoretical framework to derive the upper bound of the pairwise adversarial loss. Finally, we incorporate the derived bound into the training process of metric learning and design a novel defense method to make the learned models more robust. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed methods. © 2022 Association for Computing Machinery.",adversarial perturbations; Metric learning; robustness,Learning systems; Adversarial environments; Adversarial perturbation; Classification models; Distance metrics; Input datas; Learning models; Metric learning; Performance; Projected gradient; Robustness; Gradient methods
Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131199163&doi=10.1145%2f3502736&partnerID=40&md5=1c6856c8855b7b400bc4da1b0fe9314c,"Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Applied research; classification; clustering; domain knowledge; estimation; graphical data mining; machine learning; predictive analytics; scientific applications,Classification (of information); Computational efficiency; Data mining; Efficiency; Learning systems; Predictive analytics; Semantics; Applied research; Clusterings; Computational estimation; Condition; Domain knowledge; Estimation approaches; Graphical data; Graphical data mining; Scientific applications; Scientific data; Domain Knowledge
Combining Filtering and Cross-Correlation Efficiently for Streaming Time Series,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131183892&doi=10.1145%2f3502738&partnerID=40&md5=71e5d12b6a58fa4ef494bf034cf040bd,"Monitoring systems have hundreds or thousands of distributed sensors gathering and transmitting real-time streaming data. The early detection of events in these systems, such as an earthquake in a seismic monitoring system, is the base for essential tasks as warning generations. To detect such events is usual to compute pairwise correlation across the disparate signals generated by the sensors. Since the data sources (e.g., sensors) are spatially separated, it is essential to consider the lagged correlation between the signals. Besides, many applications require to process a specific band of frequencies depending on the event's type, demanding a pre-processing step of filtering before computing correlations. Due to the high speed of data generation and a large number of sensors in these systems, the operations of filtering and lagged cross-correlation need to be efficient to provide real-time responses without data losses. This article proposes a technique named FilCorr that efficiently computes both operations in one single step. We achieve an order of magnitude speedup by maintaining frequency transforms over sliding windows. Our method is exact, devoid of sensitive parameters, and easily parallelizable. Besides our algorithm, we also provide a publicly available real-time system named Seisviz that employs FilCorr in its core mechanism for monitoring a seismometer network. We demonstrate that our technique is suitable for several monitoring applications as seismic signal monitoring, motion monitoring, and neural activity monitoring. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",correlation; filtering; real-time monitoring systems; seismic monitoring; streaming; Time series,Earthquakes; Interactive computer systems; Neurons; Real time systems; Time series; Correlation; Cross-correlations; Distributed sensor; Monitoring system; Real time monitoring system; Real time streaming; Seismic monitoring; Streaming; Streaming data; Times series; Monitoring
Asymmetric Multi-Task Learning with Local Transference,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131160373&doi=10.1145%2f3514252&partnerID=40&md5=71ceecf5d2ab29364aae55dbe669cbd7,"In this article, we present the Group Asymmetric Multi-Task Learning (GAMTL) algorithm that automatically learns from data how tasks transfer information among themselves at the level of a subset of features. In practice, for each group of features GAMTL extracts an asymmetric relationship supported by the tasks, instead of assuming a single structure for all features. The additional flexibility promoted by local transference in GAMTL allows any two tasks to have multiple asymmetric relationships. The proposed method leverages the information present in these multiple structures to bias the training of individual tasks towards more generalizable models.The solution to the GAMTL's associated optimization problem is an alternating minimization procedure involving tasks parameters and multiple asymmetric relationships, thus guiding to convex smaller sub-problems. GAMTL was evaluated on both synthetic and real datasets. To evidence GAMTL versatility, we generated a synthetic scenario characterized by diverse profiles of structural relationships among tasks. GAMTL was also applied to the problem of Alzheimer's Disease (AD) progression prediction. Our experiments indicated that the proposed approach not only increased prediction performance, but also estimated scientifically grounded relationships among multiple cognitive scores, taken here as multiple regression tasks, and regions of interest in the brain, directly associated here with groups of features. We also employed stability selection analysis to investigate GAMTL's robustness to data sampling rate and hyper-parameter configuration. GAMTL source code is available on GitHub: https://github.com/shgo/gamtl. © 2022 Association for Computing Machinery.",Multi-task learning; structural learning; structural sparsity,Learning systems; Additional flexibilities; Feature groups; Learn+; Multiple structures; Multitask learning; Single structure; Structural learning; Structural sparsity; Task transfer; Transfer information; Neurodegenerative diseases
Evidence Transfer: Learning Improved Representations According to External Heterogeneous Task Outcomes,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131183804&doi=10.1145%2f3502732&partnerID=40&md5=327decefb1ddf32383d8719e2a0bb7c9,"Unsupervised representation learning tends to produce generic and reusable latent representations. However, these representations can often miss high-level features or semantic information, since they only observe the implicit properties of the dataset. On the other hand, supervised learning frameworks learn task-oriented latent representations that may not generalise in other tasks or domains. In this article, we introduce evidence transfer, a deep learning method that incorporates the outcomes of external tasks in the unsupervised learning process of an autoencoder. External task outcomes also referred to as categorical evidence, are represented by categorical variables, and are either directly or indirectly related to the primary dataset - in the most straightforward case they are the outcome of another task on the same dataset. Evidence transfer allows the manipulation of generic latent representations in order to include domain or task-specific knowledge that will aid their effectiveness in downstream tasks. Evidence transfer is robust against evidence of low quality and effective when introduced with related, corresponding, or meaningful evidence. © 2022 Association for Computing Machinery.",autoencoders; Deep neural networks; representation learning; transfer learning,Semantics; Auto encoders; Feature information; High level semantics; High-level features; Learn+; Property; Representation learning; Semantics Information; Task-oriented; Transfer learning; Deep neural networks
Online Scalable Streaming Feature Selection via Dynamic Decision,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131187154&doi=10.1145%2f3502737&partnerID=40&md5=75232c0a17ad4bdfd5faa474645f1065,"Feature selection is one of the core concepts in machine learning, which hugely impacts the model's performance. For some real-world applications, features may exist in a stream mode that arrives one by one over time, while we cannot know the exact number of features before learning. Online streaming feature selection aims at selecting optimal stream features at each timestamp on the fly. Without the global information of the entire feature space, most of the existing methods select stream features in terms of individual feature information or the comparison of features in pairs. This article proposes a new online scalable streaming feature selection framework from the dynamic decision perspective that is scalable on running time and selected features by dynamic threshold adjustment. Regarding the philosophy of ""Thinking-in-Threes"", we classify each new arrival feature as selecting, discarding, or delaying, aiming at minimizing the overall decision risks. With the dynamic updating of global statistical information, we add the selecting features into the candidate feature subset, ignore the discarding features, cache the delaying features into the undetermined feature subset, and wait for more information. Meanwhile, we perform the redundancy analysis for the candidate features and uncertainty analysis for the undetermined features. Extensive experiments on eleven real-world datasets demonstrate the efficiency and scalability of our new framework compared with state-of-the-art algorithms. © 2022 Association for Computing Machinery.",Feature selection; feature streams; scalable feature selection; three-way decision,Uncertainty analysis; Dynamic decision; Feature stream; Feature subset; Features selection; Modeling performance; Real-world; Scalable feature selection; Scalable streaming; Three-way decision; Time-stamp; Feature extraction
DexDeepFM: Ensemble Diversity Enhanced Extreme Deep Factorization Machine Model,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131193673&doi=10.1145%2f3505272&partnerID=40&md5=86afb37250e1d9d47f7c79943bc4eb99,"Predicting user positive response (e.g., purchases and clicks) probability is a critical task in Web applications. To identify predictive features from raw data, the state-of-the-art extreme deep factorization machine model (xDeepFM) introduces a new interaction network to leverage feature interactions at the vector-wise level explicitly. However, since each hidden layer in the interaction network is a collection of feature maps, it can be viewed essentially as an ensemble of different feature maps. In this case, only using a single objective to minimize the prediction loss may lead to overfitting and generate correlated errors. In this article, an ensemble diversity enhanced extreme deep factorization machine model (DexDeepFM) is proposed, which designs the ensemble diversity measure in each hidden layer and considers both ensemble diversity and prediction accuracy in the objective function. In addition, the attention mechanism is introduced to discriminate the importance of ensemble diversity measures with different feature interaction orders. Extensive experiments on three public real-world datasets are conducted to show the effectiveness of the proposed model. © 2022 Association for Computing Machinery.",ensemble diversity; Feature interaction; recommender systems,Factorization; Critical tasks; Diversity measure; Ensemble diversity; Factorization machines; Feature interactions; Feature map; Hidden layers; Interaction networks; Machine modelling; Model ensembles; Forecasting
Multi-relation Graph Summarization,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131167028&doi=10.1145%2f3494561&partnerID=40&md5=4ef07bc1c259c9038a98c458bacd8242,"Graph summarization is beneficial in a wide range of applications, such as visualization, interactive and exploratory analysis, approximate query processing, reducing the on-disk storage footprint, and graph processing in modern hardware. However, the bulk of the literature on graph summarization surprisingly overlooks the possibility of having edges of different types. In this article, we study the novel problem of producing summaries of multi-relation networks, i.e., graphs where multiple edges of different types may exist between any pair of nodes. Multi-relation graphs are an expressive model of real-world activities, in which a relation can be a topic in social networks, an interaction type in genetic networks, or a snapshot in temporal graphs.The first approach that we consider for multi-relation graph summarization is a two-step method based on summarizing each relation in isolation, and then aggregating the resulting summaries in some clever way to produce a final unique summary. In doing this, as a side contribution, we provide the first polynomial-time approximation algorithm based on the k-Median clustering for the classic problem of lossless single-relation graph summarization.Then, we demonstrate the shortcomings of these two-step methods, and propose holistic approaches, both approximate and heuristic algorithms, to compute a summary directly for multi-relation graphs. In particular, we prove that the approximation bound of k-Median clustering for the single relation solution can be maintained in a multi-relation graph with proper aggregation operation over adjacency matrices corresponding to its multiple relations. Experimental results and case studies (on co-authorship networks and brain networks) validate the effectiveness and efficiency of the proposed algorithms. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximation; Graph summarization; k-median; multi-relation graph,Approximation algorithms; Clustering algorithms; Heuristic algorithms; Heuristic methods; Polynomial approximation; Query processing; Approximate query processing; Approximation; Exploratory analysis; Graph summarization; Interactive analysis; K-median; K-median clustering; Multi-relation graph; Two step method; Visualization analysis; Graphic methods
MBN: Towards Multi-Behavior Sequence Modeling for Next Basket Recommendation,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131199926&doi=10.1145%2f3497748&partnerID=40&md5=6fb87474369276df2d8f118ccaab7bb4,"Next basket recommendation aims at predicting the next set of items that a user would likely purchase together, which plays an important role in e-commerce platforms. Unlike conventional item recommendation, the next basket recommendation focuses on capturing item correlations among baskets and learning the user's temporal interest from the past purchasing basket sequence. In practice, most users interact with items in various kinds of behaviors. The multi-behavior data sheds light on user's potential purchasing intention and resolves noisy signals from accidentally purchased items. In this article, we conduct an empirical study on real datasets to exploit the characteristics of multi-behavior data and confirm its positive effects on next basket recommendation. We develop a novel Multi-Behavior Network (MBN) model that captures item correlations and acquires meta-knowledge from multi-behavior basket sequences effectively. MBN employs the meta multi-behavior sequence encoder to model temporal dependencies of each individual behavior and extract meta-knowledge across different behaviors. Furthermore, we design the recurring-item-aware predictor in MBN to realize the high degree of the repeated occurrences of items, leading to better recommendation performance. We conduct extensive experiments to evaluate the performance of our proposed MBN model using real-world multi-behavior data. The results demonstrate the superior recommendation performance of MBN compared with various state-of-the-art methods. © 2022 Association for Computing Machinery.",multi-behavior data; Recommendation; sequence modeling,Data mining; User profile; Behavior network; Behavior sequences; Commerce platforms; E- commerces; Meta-knowledge; Multi-behavior data; Network models; Recommendation; Recommendation performance; Sequence models; Sales
Quality-Informed Process Mining: A Case for Standardised Data Quality Annotations,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131204925&doi=10.1145%2f3511707&partnerID=40&md5=56575409161ef743b454722489b9345d,"Real-life event logs, reflecting the actual executions of complex business processes, are faced with numerous data quality issues. Extensive data sanity checks and pre-processing are usually needed before historical data can be used as input to obtain reliable data-driven insights. However, most of the existing algorithms in process mining, a field focusing on data-driven process analysis, do not take any data quality issues or the potential effects of data pre-processing into account explicitly. This can result in erroneous process mining results, leading to inaccurate, or misleading conclusions about the process under investigation. To address this gap, we propose data quality annotations for event logs, which can be used by process mining algorithms to generate quality-informed insights. Using a design science approach, requirements are formulated, which are leveraged to propose data quality annotations. Moreover, we present the ""Quality-Informed visual Miner""plug-in to demonstrate the potential utility and impact of data quality annotations. Our experimental results, utilising both synthetic and real-life event logs, show how the use of data quality annotations by process mining techniques can assist in increasing the reliability of performance analysis results. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",annotations; data quality; metadata; Process mining; quality-informed conformance checking; quality-informed performance analysis,Data reduction; Quality control; Reliability analysis; Annotation; Conformance checking; Data quality; Event logs; Life events; Performances analysis; Process mining; Quality issues; Quality-informed conformance checking; Quality-informed performance analyse; Data mining
Exploiting Higher Order Multi-dimensional Relationships with Self-attention for Author Name Disambiguation,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131191078&doi=10.1145%2f3502730&partnerID=40&md5=3fa257abefd8d881c8da191cc08dd1ef,"Name ambiguity is a prevalent problem in scholarly publications due to the unprecedented growth of digital libraries and number of researchers. An author is identified by their name in the absence of a unique identifier. The documents of an author are mistakenly assigned due to underlying ambiguity, which may lead to an improper assessment of the author. Various efforts have been made in the literature to solve the name disambiguation problem with supervised and unsupervised approaches. The unsupervised approaches for author name disambiguation are preferred due to the availability of a large amount of unlabeled data. Bibliographic data contain heterogeneous features, thus recently, representation learning-based techniques have been used in literature to embed heterogeneous features in common space. Documents of a scholar are connected by multiple relations. Recently, research has shifted from a single homogeneous relation to multi-dimensional (heterogeneous) relations for the latent representation of document. Connections in graphs are sparse, and higher order links between documents give an additional clue. Therefore, we have used multiple neighborhoods in different relation types in heterogeneous graph for representation of documents. However, different order neighborhood in each relation type has different importance which we have empirically validated also. Therefore, to properly utilize the different neighborhoods in relation type and importance of each relation type in the heterogeneous graph, we propose attention-based multi-dimensional multi-hop neighborhood-based graph convolution network for embedding that uses the two levels of an attention, namely, (i) relation level and (ii) neighborhood level, in each relation. A significant improvement over existing state-of-the-art methods in terms of various evaluation matrices has been obtained by the proposed approach. © 2022 Association for Computing Machinery.",graph convolution networks; Multi-dimensional; multi-hop neighborhood; name disambiguation; representation learning,Digital libraries; Embeddings; Author name disambiguations; Graph convolution network; High-order; Higher-order; Multi dimensional; Multi-hop neighborhood; Multi-hops; Name disambiguation; Neighbourhood; Representation learning; Convolution
Constant Time Graph Neural Networks,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131184777&doi=10.1145%2f3502733&partnerID=40&md5=31d278c48ba4d2b5f078dcc7e7dfe32b,"The recent advancements in graph neural networks (GNNs) have led to state-of-the-art performances in various applications, including chemo-informatics, question-answering systems, and recommender systems. However, scaling up these methods to huge graphs, such as social networks and Web graphs, remains a challenge. In particular, the existing methods for accelerating GNNs either are not theoretically guaranteed in terms of the approximation error or incurred at least a linear time computation cost. In this study, we reveal the query complexity of the uniform node sampling scheme for Message Passing Neural Networks, including GraphSAGE, graph attention networks (GATs), and graph convolutional networks (GCNs). Surprisingly, our analysis reveals that the complexity of the node sampling method is completely independent of the number of the nodes, edges, and neighbors of the input and depends only on the error tolerance and confidence probability while providing a theoretical guarantee for the approximation error. To the best of our knowledge, this is the first article to provide a theoretical guarantee of approximation for GNNs within constant time. Through experiments with synthetic and real-world datasets, we investigated the speed and precision of the node sampling scheme and validated our theoretical results. © 2022 Association for Computing Machinery.",Graph neural networks; large-scale graphs,Complex networks; Errors; Message passing; Approximation errors; Constant time; Graph neural networks; Large-scale graph; Large-scales; Node sampling; Question answering systems; Sampling schemes; State-of-the-art performance; Theoretical guarantees; Graph neural networks
Multi-label Deep Convolutional Transform Learning for Non-intrusive Load Monitoring,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131180296&doi=10.1145%2f3502729&partnerID=40&md5=e997273e596390974d3eeb9c16af8620,The objective of this letter is to propose a novel computational method to learn the state of an appliance (ON / OFF) given the aggregate power consumption recorded by the smart-meter. We formulate a multi-label classification problem where the classes correspond to the appliances. The proposed approach is based on our recently introduced framework of convolutional transform learning. We propose a deep supervised version of it relying on an original multi-label cost. Comparisons with state-of-the-art techniques show that our proposed method improves over the benchmarks on popular non-intrusive load monitoring datasets. © 2022 Association for Computing Machinery.,multi-label classification; non-intrusive load monitoring; Representation learning,Classification (of information); Deep learning; Electric load management; Learn+; Multi-labels; Nonintrusive load monitoring; Representation learning; State-of-the-art techniques; Convolution
PSL: An Algorithm for Partial Bayesian Network Structure Learning,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131196755&doi=10.1145%2f3508071&partnerID=40&md5=02a2f340372ec764adb8a5ab3beb9eee,"Learning partial Bayesian network (BN) structure is an interesting and challenging problem. In this challenge, it is computationally expensive to use global BN structure learning algorithms, while only one part of a BN structure is interesting, local BN structure learning algorithms are not a favourable solution either due to the issue of false edge orientation. To address the problem, this article first presents a detailed analysis of the false edge orientation issue with local BN structure learning algorithms and then proposes PSL, an efficient and accurate Partial BN Structure Learning (PSL) algorithm. Specifically, PSL divides V-structures in a Markov blanket (MB) into two types: Type-C V-structures and Type-NC V-structures, then it starts from the given node of interest and recursively finds both types of V-structures in the MB of the current node until all edges in the partial BN structure are oriented. To further improve the efficiency of PSL, the PSL-FS algorithm is designed by incorporating Feature Selection (FS) into PSL. Extensive experiments with six benchmark BNs validate the efficiency and accuracy of the proposed algorithms. © 2022 Association for Computing Machinery.",Bayesian network; feature selection; global structure learning; local structure learning; markov blanket,Efficiency; Feature extraction; Learning algorithms; Bayesia n networks; Bayesian network structure; Features selection; Global structure; Global structure learning; Local structure; Local structure learning; Markov Blankets; Structure-learning; Bayesian networks
Machine Learning-based Short-term Rainfall Prediction from Sky Data,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141173241&doi=10.1145%2f3502731&partnerID=40&md5=a57418f342f3940d431946cc10432d1e,"To predict rainfall, our proposed model architecture combines the Convolutional Neural Network (CNN), which uses the ResNet-152 pre-training model, with the Recurrent Neural Network (RNN), which uses the Long Short-term Memory Network (LSTM) layer, for model training. By encoding the cloud images through CNN, we extract the image feature vectors in the training process and train the vectors and meteorological data as the input of RNN. After training, the accuracy of the prediction model can reach up to 82%. The result has proven not only the outperformance of our proposed rainfall prediction method in terms of cost and prediction time, but also its accuracy and feasibility compared with general prediction methods. © 2022 Association for Computing Machinery.",Convolutional Neural Network; Long Short-term Memory Network; machine learning; Rainfall prediction; Recurrent Neural Network,Brain; Convolution; Convolutional neural networks; Multilayer neural networks; Rain; Weather forecasting; Convolutional neural network; Long short-term memory network; Machine-learning; Memory network; Model training; Modeling architecture; Pre-training; Prediction methods; Rainfall prediction; Training model; Long short-term memory
Incremental Feature Spaces Learning with Label Scarcity,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141124292&doi=10.1145%2f3516368&partnerID=40&md5=02e8de4cebc356de931f7f6b6a301d39,"Recently, learning and mining from data streams with incremental feature spaces have attracted extensive attention, where data may dynamically expand over time in both volume and feature dimensions. Existing approaches usually assume that the incoming instances can always receive true labels. However, in many real-world applications, e.g., environment monitoring, acquiring the true labels is costly due to the need of human effort in annotating the data. To tackle this problem, we propose a novel incremental Feature spaces Learning with Label Scarcity (FLLS) algorithm, together with its two variants. When data streams arrive with augmented features, we first leverage the margin-based online active learning to select valuable instances to be labeled and thus build superior predictive models with minimal supervision. After receiving the labels, we combine the online passive-aggressive update rule and margin-maximum principle to jointly update the dynamic classifier in the shared and augmented feature space. Finally, we use the projected truncation technique to build a sparse but efficient model. We theoretically analyze the error bounds of FLLS and its two variants. Also, we conduct experiments on synthetic data and real-world applications to further validate the effectiveness of our proposed algorithms. © 2022 Association for Computing Machinery.",Incremental feature spaces; label scarcity; online learning,Data mining; E-learning; Active Learning; Data stream; Environment monitoring; Feature dimensions; Feature space; Incremental feature space; Label scarcity; Online learning; Predictive models; Real-world; Error analysis
Segment-Wise Time-Varying Dynamic Bayesian Network with Graph Regularization,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141131625&doi=10.1145%2f3522589&partnerID=40&md5=6d57c4d9d214f4056d9d9c372c4b0bed,"Time-varying dynamic Bayesian network (TVDBN) is essential for describing time-evolving directed conditional dependence structures in complex multivariate systems. In this article, we construct a TVDBN model, together with a score-based method for its structure learning. The model adopts a vector autoregressive (VAR) model to describe inter-slice and intra-slice relations between variables. By allowing VAR parameters to change segment-wisely over time, the time-varying dynamics of the network structure can be described. Furthermore, considering some external information can provide additional similarity information of variables. Graph Laplacian is further imposed to regularize similar nodes to have similar network structures. The regularized maximum a posterior estimation in the Bayesian inference framework is used as a score function for TVDBN structure evaluation, and the alternating direction method of multipliers (ADMM) with L-BFGS-B algorithm is used for optimal structure learning. Thorough simulation studies and a real case study are carried out to verify our proposed method's efficacy and efficiency. © 2022 Copyright held by the owner/author(s).",acyclic property; ADMM; directed acyclic graph; graph Laplacian; segment-wise change; structure learning; Time-varying dynamic Bayesian network,Directed graphs; Inference engines; Laplace transforms; Learning systems; Structural optimization; Time varying networks; Value engineering; Acyclic graphs; Acyclic property; Alternating directions method of multipliers; Directed acyclic graph; Dynamic Bayesian networks; Graph Laplacian; Property; Segment-wise change; Structure-learning; Time-varying dynamic bayesian network; Time-varying dynamics; Bayesian networks
"GrOD: Deep Learning with Gradients Orthogonal Decomposition for Knowledge Transfer, Distillation, and Adversarial Training",2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132259176&doi=10.1145%2f3530836&partnerID=40&md5=f5523dfd215ab816d616f7239bf2dceb,"Regularization that incorporates the linear combination of empirical loss and explicit regularization terms as the loss function has been frequently used for many machine learning tasks. The explicit regularization term is designed in different types, depending on its applications. While regularized learning often boost the performance with higher accuracy and faster convergence, the regularization would sometimes hurt the empirical loss minimization and lead to poor performance. To deal with such issues in this work, we propose a novel strategy, namely Gradients Orthogonal Decomposition (GrOD), that improves the training procedure of regularized deep learning. Instead of linearly combining gradients of the two terms, GrOD re-estimates a new direction for iteration that does not hurt the empirical loss minimization while preserving the regularization affects, through orthogonal decomposition. We have performed extensive experiments to use GrOD improving the commonly used algorithms of transfer learning [2], knowledge distillation [3], and adversarial learning [4]. The experiment results based on large datasets, including Caltech 256 [5], MIT indoor 67 [6], CIFAR-10 [7], and ImageNet [8], show significant improvement made by GrOD for all three algorithms in all cases. © 2022 Copyright held by the owner/author(s).",Deep neural networks; gradient-based learning; regularized deep learning,Deep neural networks; Image enhancement; Iterative methods; Knowledge management; Large dataset; Learning systems; Gradient-based learning; Knowledge transfer; Linear combinations; Loss functions; Losses minimizations; Machine-learning; Orthogonal decomposition; Regularisation; Regularization terms; Regularized deep learning; Distillation
Predicting a Person's Next Activity Region with a Dynamic Region-Relation-Aware Graph Neural Network,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141153746&doi=10.1145%2f3529091&partnerID=40&md5=c388b553c9cc5c4f81e760559d32194f,"The understanding of people's inter-regional mobility behaviors, such as predicting the next activity region (AR) or uncovering the intentions for regional mobility, is of great value to public administration or business interests. While there are numerous studies on human mobility, these studies are mainly from a statistical view or study movement behaviors within a region. The work on individual-level inter-regional mobility behavior is limited. To this end, in this article, we propose a dynamic region-relation-aware graph neural network (DRRGNN) for exploring individual mobility behaviors over ARs. Specifically, we aim at developing models that can answer three questions: (1) Which regions are the ARs? (2) Which region will be the next AR, and (3) Why do people make this regional mobility? To achieve these tasks, we first propose a method to find out people's ARs. Then, the designed model integrates a dynamic graph convolution network (DGCN) and a recurrent neural network (RNN) to depict the evolution of relations between ARs and mine the regional mobility patterns. In the learning process, the model further considers peoples' profiles and visited point-of-interest (POIs). Finally, extensive experiments on two real-world datasets show that the proposed model can significantly improve accuracy for both the next AR prediction and mobility intention prediction. © 2022 Association for Computing Machinery.",Activity region; individual mobility prediction; intention prediction; inter-regional mobility,Forecasting; Graph neural networks; Learning systems; Public administration; Activity region; Dynamic region; Graph neural networks; Human mobility; Individual mobility; Individual mobility prediction; Intention predictions; Inter-regional mobility; Mobility behavior; Mobility predictions; Recurrent neural networks
Graph-Enhanced Spatial-Temporal Network for Next POI Recommendation,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139777800&doi=10.1145%2f3513092&partnerID=40&md5=0094218336fc1ef30234f890da2cdbf0,"The task of next Point-of-Interest (POI) recommendation aims at recommending a list of POIs for a user to visit at the next timestamp based on his/her previous interactions, which is valuable for both location-based service providers and users. Recent state-of-the-art studies mainly employ recurrent neural network (RNN) based methods to model user check-in behaviors according to user's historical check-in sequences. However, most of the existing RNN-based methods merely capture geographical influences depending on physical distance or successive relation among POIs. They are insufficient to capture the high-order complex geographical influences among POI networks, which are essential for estimating user preferences. To address this limitation, we propose a novel Graph-based Spatial Dependency modeling (GSD) module, which focuses on explicitly modeling complex geographical influences by leveraging graph embedding. GSD captures two types of geographical influences, i.e., distance-based and transition-based influences from designed POI semantic graphs. Additionally, we propose a novel Graph-enhanced Spatial-Temporal network (GSTN), which incorporates user spatial and temporal dependencies for next POI recommendation. Specifically, GSTN consists of a Long Short-Term Memory (LSTM) network for user-specific temporal dependencies modeling and GSD for user spatial dependencies learning. Finally, we evaluate the proposed model using three real-world datasets. Extensive experiments demonstrate the effectiveness of GSD in capturing various geographical influences and the improvement of GSTN over state-of-the-art methods. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph embedding; LSTM; Next point-of-interest recommendation,Behavioral research; Complex networks; Embeddings; Encoding (symbols); Graphic methods; Location based services; Semantics; Telecommunication services; User profile; Check-in; Dependency model; Graph embeddings; Graph-based; Network-based; Next point-of-interest recommendation; Spatial dependencies; Spatial temporals; Temporal networks; Time-stamp; Long short-term memory
Spectral Ranking Regression,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141032457&doi=10.1145%2f3530693&partnerID=40&md5=9cfc0d6083a952d6685542f148f157d8,"We study the problem of ranking regression, in which a dataset of rankings is used to learn Plackett-Luce scores as functions of sample features. We propose a novel spectral algorithm to accelerate learning in ranking regression. Our main technical contribution is to show that the Plackett-Luce negative log-likelihood augmented with a proximal penalty has stationary points that satisfy the balance equations of a Markov Chain. This allows us to tackle the ranking regression problem via an efficient spectral algorithm by using the Alternating Directions Method of Multipliers (ADMM). ADMM separates the learning of scores and model parameters, and in turn, enables us to devise fast spectral algorithms for ranking regression via both shallow and deep neural network (DNN) models. For shallow models, our algorithms are up to 579 times faster than the Newton's method. For DNN models, we extend the standard ADMM via a Kullback-Leibler proximal penalty and show that this is still amenable to fast inference via a spectral approach. Compared to a state-of-the-art siamese network, our resulting algorithms are up to 175 times faster and attain better predictions by up to 26% Top-1 Accuracy and 6% Kendall-Tau correlation over five real-life ranking datasets. © 2022 Association for Computing Machinery.",ADMM; Markov Chain; Plackett-Luce; ranking; spectral methods,Learning algorithms; Markov processes; Neural network models; Newton-Raphson method; Regression analysis; Alternating directions method of multipliers; Learn+; Log likelihood; Neural network model; Plackett-luce; Ranking; Sample features; Spectral algorithm; Spectral methods; Technical contribution; Deep neural networks
ARIS: A Noise Insensitive Data Pre-Processing Scheme for Data Reduction Using Influence Space,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141196321&doi=10.1145%2f3522592&partnerID=40&md5=cc46aa63b20117a8382ec014f95a423a,"The extensive growth of data quantity has posed many challenges to data analysis and retrieval. Noise and redundancy are typical representatives of the above-mentioned challenges, which may reduce the reliability of analysis and retrieval results and increase storage and computing overhead. To solve the above problems, a two-stage data pre-processing framework for noise identification and data reduction, called ARIS, is proposed in this article. The first stage identifies and removes noises by the following steps: First, the influence space (IS) is introduced to elaborate data distribution. Second, a ranking factor (RF) is defined to describe the possibility that the points are regarded as noises, then, the definition of noise is given based on RF. Third, a clean dataset (CD) is obtained by removing noise from the original dataset. The second stage learns representative data and realizes data reduction. In this process, CD is divided into multiple small regions by IS. Then the reduced dataset is formed by collecting the representations of each region. The performance of ARIS is verified by experiments on artificial and real datasets. Experimental results show that ARIS effectively weakens the impact of noise and reduces the amount of data and significantly improves the accuracy of data analysis within a reasonable time cost range. © 2022 Association for Computing Machinery.",Data pre-processing scheme; data representation; influence space; noise identification; ranking factor,Digital storage; Redundancy; Reliability analysis; Data distribution; Data pre-processing scheme; Data preprocessing; Data quantity; Data representations; Influence space; Noise data; Noise identification; Ranking factor; REmove noise; Data reduction
Improving Bandit Learning Via Heterogeneous Information Networks: Algorithms and Applications,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141131221&doi=10.1145%2f3522590&partnerID=40&md5=5bc603ae9c8b6315263314b0125a117d,"Contextual bandit serves as an invaluable tool to balance the exploration vs. exploitation tradeoff in various applications such as online recommendation. In many applications, heterogeneous information networks (HINs) provide rich side information for contextual bandits, such as different types of attributes and relationships among users and items. In this article, we propose the first HIN-assisted contextual bandit framework, which utilizes a given HIN to assist contextual bandit learning. The proposed framework uses meta-paths in HIN to extract rich relations among users and items for the contextual bandit. The main challenge is how to leverage these relations, since users' preference over items, the target of our online learning, are closely related to users' preference over meta-paths. However, it is unknown which meta-path a user prefers more. Thus, both preferences are needed to be learned in an online fashion with exploration vs. exploitation tradeoff balanced. We propose the HIN-assisted upper confidence bound (HUCB) algorithm to address such a challenge. For each meta-path, the HUCB algorithm employs an independent base bandit algorithm to handle online item recommendations by leveraging the relationship captured in this meta-path. A bandit master is then employed to learn users' preference over meta-paths to dynamically combine base bandit algorithms with a balance of exploration vs. exploitation tradeoff. We theoretically prove that the HUCB algorithm can achieve similar performance compared with the optimal algorithm where each user is served according to his true preference over meta-paths (assuming the optimal algorithm knows the preference). Moreover, we prove that the HUCB algorithm benefits from leveraging HIN in achieving a smaller regret upper bound than the baseline algorithm without leveraging HIN. Experimental results on a synthetic dataset, as well as real datasets from LastFM and Yelp demonstrate the fast learning speed of the HUCB algorithm. © 2022 Association for Computing Machinery.",Contextual bandit; heterogeneous information network; online recommendation,E-learning; Information services; Bound algorithms; Contextual banditti; Heterogeneous information; Heterogeneous information network; Information networks; Network applications; Online recommendation; Optimal algorithm; Upper confidence bound; User's preferences; Learning algorithms
"Factorization of Binary Matrices: Rank Relations, Uniqueness and Model Selection of Boolean Decomposition",2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132982856&doi=10.1145%2f3522594&partnerID=40&md5=7edd3067a10f446aa98a8f9c0223e396,"The application of binary matrices are numerous. Representing a matrix as a mixture of a small collection of latent vectors via low-rank decomposition is often seen as an advantageous method to interpret and analyze data. In this work, we examine the factorizations of binary matrices using standard arithmetic (real and nonnegative) and logical operations (Boolean and ℤ2). We examine the relationships between the different ranks, and discuss when factorization is unique. In particular, we characterize when a Boolean factorization X = W ℤ H has a unique W, a unique H (for a fixed W), and when both W and H are unique, given a rank constraint. We introduce a method for robust Boolean model selection, called BMFk, and show on numerical examples that BMFk not only accurately determines the correct number of Boolean latent features but reconstruct the pre-determined factors accurately. © 2022 Association for Computing Machinery. All rights reserved.",Boolean matrix factorization; model determination; nonnegative matrix factorization; rank; unique factorization; ℤ<sub>2</sub>matrix factorization,Matrix algebra; Matrix factorization; Binary matrix; Boolean Matrix; Boolean matrix factorization; Matrix factorizations; Model determination; Nonnegative matrix factorization; Rank; Unique factorization; ℤ2matrix factorization; Numerical methods
A News-Based Framework for Uncovering and Tracking City Area Profiles: Assessment in Covid-19 Setting,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139524446&doi=10.1145%2f3532186&partnerID=40&md5=4b296258cc244f05778078124eeb4e90,"In the last years, there has been an ever-increasing interest in profiling various aspects of city life, especially in the context of smart cities. This interest has become even more relevant recently when we have realized how dramatic events, such as the Covid-19 pandemic, can deeply affect the city life, producing drastic changes. Identifying and analyzing such changes, both at the city level and within single neighborhoods, may be a fundamental tool to better manage the current situation and provide sound strategies for future planning. Furthermore, such fine-grained and up-to-date characterization can represent a valuable asset for other tools and services, e.g., web mapping applications or real estate agency platforms. In this article, we propose a framework featuring a novel methodology to model and track changes in areas of the city by extracting information from online newspaper articles. The problem of uncovering clusters of news at specific times is tackled by means of the joint use of state-of-the-art language models to represent the articles, and of a density-based streaming clustering algorithm, properly shaped to deal with high-dimensional text embeddings. Furthermore, we propose a method to automatically label the obtained clusters in a semantically meaningful way, and we introduce a set of metrics aimed at tracking the temporal evolution of clusters. A case study focusing on the city of Rome during the Covid-19 pandemic is illustrated and discussed to evaluate the effectiveness of the proposed approach. © 2022 Association for Computing Machinery.",City Areas profiling; Covid-19; NLP; online news clustering; smart cities; streaming data; text mining,Clustering algorithms; Computational linguistics; Data mining; City area profiling; Covid-19; Current situation; Fundamental tools; Neighbourhood; News clustering; Online news; Online news clustering; Streaming data; Text-mining; Smart city
Profile Decomposition Based Hybrid Transfer Learning for Cold-Start Data Anomaly Detection,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141094738&doi=10.1145%2f3530990&partnerID=40&md5=b4528ac359853989a5bd9dadb19bca7f,"Anomaly detection is an essential task for quality management in smart manufacturing. An accurate data-driven detection method usually needs enough data and labels. However, in practice, there commonly exist newly set-up processes in manufacturing, and they only have quite limited data available for analysis. Borrowing the name from the recommender system, we call this process a cold-start process. The sparsity of anomaly, the deviation of the profile, and noise aggravate the detection difficulty. Transfer learning could help to detect anomalies for cold-start processes by transferring the knowledge from more experienced processes to the new processes. However, the existing transfer learning and multi-task learning frameworks are established on task-or domain-level relatedness. We observe instead, within a domain, some components (background and anomaly) share more commonality, others (profile deviation and noise) not. To this end, we propose a more delicate component-level transfer learning scheme, i.e., decomposition-based hybrid transfer learning (DHTL): It first decomposes a domain (e.g., a data source containing profiles) into different components (smooth background, profile deviation, anomaly, and noise); then, each component's transferability is analyzed by expert knowledge; Lastly, different transfer learning techniques could be tailored accordingly. We adopted the Bayesian probabilistic hierarchical model to formulate parameter transfer for the background, and ""L2,1+L1""-norm to formulate low dimension feature-representation transfer for the anomaly. An efficient algorithm based on Block Coordinate Descend is proposed to learn the parameters. A case study based on glass coating pressure profiles demonstrates the improved accuracy and completeness of detected anomaly, and a simulation demonstrates the fidelity of the decomposition results. © 2022 Association for Computing Machinery.",Bayesian probabilistic model and regularization; Cold-start anomaly detection; Hybrid Transfer learning; Profile decomposition,Hierarchical systems; Learning systems; Manufacture; Quality management; Anomaly detection; Bayesian probabilistic model and regularization; Bayesian probabilistic models; Cold-start; Cold-start anomaly detection; Hybrid transfer learning; Profile decomposition; Regularisation; Start process; Transfer learning; Anomaly detection
Computer Science Diagram Understanding with Topology Parsing,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141028252&doi=10.1145%2f3522689&partnerID=40&md5=6943a62443bffd986e961857184d9945,"Diagram is a special form of visual expression for representing complex concepts, logic, and knowledge, which widely appears in educational scenes such as textbooks, blogs, and encyclopedias. Current research on diagrams preliminarily focuses on natural disciplines such as Biology and Geography, whose expressions are still similar to natural images. In this article, we construct the first novel geometric type of diagrams dataset in Computer Science field, which has more abstract expressions and complex logical relations. The dataset has exhaustive annotations of objects and relations for about 1,300 diagrams and 3,500 question-answer pairs. We introduce the tasks of diagram classification (DC) and diagram question answering (DQA) based on the new dataset, and propose the Diagram Paring Net (DPN) that focuses on analyzing the topological structure and text information of diagrams. We use DPN-based models to solve DC and DQA tasks, and compare the performances to well-known natural images classification models and visual question answering models. Our experiments show the effectiveness of the proposed DPN-based models on diagram understanding tasks, also indicate that our dataset is more complex compared to previous natural image understanding datasets. The presented dataset opens new challenges for research in diagram understanding, and the DPN method provides a novel perspective for studying such data. Our dataset can be available from https://github.com/WayneWong97/CSDia. © 2022 Association for Computing Machinery.",Computer Science; Diagram understanding; topology,Abstracting; Classification (of information); Graphic methods; Text processing; 'current; Diagram understanding; Logical relations; Natural images; Question Answering; Question Answering Task; Question-answer pairs; Structure information; Text information; Topological structure; Topology
Multi-objective Learning to Overcome Catastrophic Forgetting in Time-series Applications,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140926208&doi=10.1145%2f3502728&partnerID=40&md5=fc3b57e52e41ac06b185cf8c83b7b79c,"One key objective of artificial intelligence involves the continuous adaptation of machine learning models to new tasks. This branch of continual learning is also referred to as lifelong learning (LL), where a major challenge is to minimize catastrophic forgetting, or forgetting previously learned tasks. While previous work on catastrophic forgetting has been focused on vision problems; this work targets time-series data. In addition to choosing an architecture appropriate for time-series sequences, our work addresses limitations in previous work, including the handling of distribution shifts in class labels. We present multi-objective learning with three loss functions to minimize catastrophic forgetting, prediction error, and errors in generalizing across label shifts, simultaneously. We build a multi-task autoencoder network with a hierarchical convolutional recurrent architecture. The proposed method is capable of learning multiple time-series tasks simultaneously. For cases where the model needs to learn multiple new tasks, we propose sequential learning, starting with tasks that have the best individual performances. This solution was evaluated on four benchmark human activity recognition datasets collected from mobile sensing devices. A wide set of baseline comparisons is performed, and an ablation analysis is run to evaluate the impact of the different losses in the proposed multi-objective method. The results demonstrate an up to 4% performance improvement in catastrophic forgetting compared to the use of loss functions in state-of-the-art solutions while demonstrating minimal losses compared to upper bound methods of traditional fine-tuning (FT) and multi-task learning (MTL).  © 2022 Association for Computing Machinery.",autoencoders; knowledge distillation; multitask learning; Time-series,Learning systems; Network architecture; Time series; Auto encoders; Catastrophic forgetting; Continual learning; Key objective; Knowledge distillation; Loss functions; Machine learning models; Multi-objective learning; Multitask learning; Times series; Distillation
HW-Forest: Deep Forest with Hashing Screening and Window Screening,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141027923&doi=10.1145%2f3532193&partnerID=40&md5=b1d1a2a1dc5aaedf2ffea5a997a796f9,"As a novel deep learning model, gcForest has been widely used in various applications. However, current multi-grained scanning of gcForest produces many redundant feature vectors, and this increases the time cost of the model. To screen out redundant feature vectors, we introduce a hashing screening mechanism for multi-grained scanning and propose a model called HW-Forest which adopts two strategies: hashing screening and window screening. HW-Forest employs perceptual hashing algorithm to calculate the similarity between feature vectors in hashing screening strategy, which is used to remove the redundant feature vectors produced by multi-grained scanning and can significantly decrease the time cost and memory consumption. Furthermore, we adopt a self-adaptive instance screening strategy called window screening to improve the performance of our approach, which can achieve higher accuracy without hyperparameter tuning on different datasets. Our experimental results show that HW-Forest has higher accuracy than other models, and the time cost is also reduced. © 2022 Association for Computing Machinery.",deep forest; Deep learning; hashing screening; perceptual hashing; self-adaptive mechanism; window screening,Computer aided design; Deep learning; Forestry; Vectors; Deep forest; Deep learning; Features vector; Hashing screening; Perceptual hashing; Redundant features; Screening strategy; Self-adaptive mechanisms; Time cost; Window screening; Scanning
Synthesis of Longitudinal Human Location Sequences: Balancing Utility and Privacy,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141187434&doi=10.1145%2f3529260&partnerID=40&md5=1374786b4991e52c102c6f6077a7c7fd,"People's location data are continuously tracked from various devices and sensors, enabling an ongoing analysis of sensitive information that can violate people's privacy and reveal confidential information. Synthetic data have been used to generate representative location sequences yet to maintain the users' privacy. Nonetheless, the privacy-accuracy tradeoff between these two measures has not been addressed systematically. In this article, we analyze the use of different synthetic data generation models for long location sequences, including extended short-term memory networks (LSTMs), Markov Chains (MC), and variable-order Markov models (VMMs). We employ different performance measures, such as data similarity and privacy, and discuss the inherent tradeoff. Furthermore, we introduce other measurements to quantify each of these measures. Based on the anonymous data of 300 thousand cellular-phone users, our work offers a road map for developing policies for synthetic data generation processes. We propose a framework for building data generation models and evaluating their effectiveness regarding those accuracy and privacy measures. © 2022 Association for Computing Machinery.",location sequences; long short term memory network (LSTM); privacy; Synthetic data,Brain; Data privacy; Long short-term memory; Markov processes; Confidential information; Data generation models; Location data; Location sequence; Long short term memory network; Memory network; Privacy; Sensitive informations; Synthetic data; Synthetic data generations; Location
Toward Quality of Information Aware Distributed Machine Learning,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141144826&doi=10.1145%2f3522591&partnerID=40&md5=002714082cd16e3fa76ab055e10766c2,"In the era of big data, data are usually distributed across numerous connected computing and storage units (i.e., nodes or workers). Under such an environment, many machine learning problems can be reformulated as a consensus optimization problem, which consists of one objective and constraint terms splitting into N parts (each corresponds to a node). Such a problem can be solved efficiently in a distributed manner via Alternating Direction Method of Multipliers (ADMM). However, existing consensus optimization frameworks assume that every node has the same quality of information (QoI), i.e., the data from all the nodes are equally informative for the estimation of global model parameters. As a consequence, they may lead to inaccurate estimates in the presence of nodes with low QoI. To overcome this challenge, in this article, we propose a novel consensus optimization framework for distributed machine-learning that incorporates the crucial metric, QoI. Theoretically, we prove that the convergence rate of the proposed framework is linear to the number of iterations, but has a tighter upper bound compared with ADMM. Experimentally, we show that the proposed framework is more efficient and effective than existing ADMM-based solutions on both synthetic and real-world datasets due to its faster convergence rate and higher accuracy. © 2022 Association for Computing Machinery.",Distributed machine learning; quality of information,Machine learning; Alternating directions method of multipliers; Computing units; Connected computing; Distributed machine learning; Information-aware; Machine learning problem; Optimization framework; Quality of information; Storage units; Workers'; Digital storage
Streaming Data Preprocessing via Online Tensor Recovery for Large Environmental Sensor Networks,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135187654&doi=10.1145%2f3532189&partnerID=40&md5=448cf01c9876fc2d1bdad4556e672656,"Measuring the built and natural environment at a fine-grained scale is now possible with low-cost urban environmental sensor networks. However, fine-grained city-scale data analysis is complicated by tedious data cleaning including removing outliers and imputing missing data. While many methods exist to automatically correct anomalies and impute missing entries, challenges still exist on data with large spatial-temporal scales and shifting patterns. To address these challenges, we propose an online robust tensor recovery (OLRTR) method to preprocess streaming high-dimensional urban environmental datasets. A small-sized dictionary that captures the underlying patterns of the data is computed and constantly updated with new data. OLRTR enables online recovery for large-scale sensor networks that provide continuous data streams, with a lower computational memory usage compared to offline batch counterparts. In addition, we formulate the objective function so that OLRTR can detect structured outliers, such as faulty readings over a long period of time. We validate OLRTR on a synthetically degraded National Oceanic and Atmospheric Administration temperature dataset, and apply it to the Array of Things city-scale sensor network in Chicago, IL, showing superior results compared with several established online and batch-based low-rank decomposition methods. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",internet of things; multilinear analysis; outlier detection; Robust tensor recovery; tensor factorization; urban computing,Anomaly detection; Computer system recovery; Data mining; Internet of things; Sensor networks; Statistics; Tensors; City scale; Environmental sensor networks; Fine grained; Multilinear analysis; Outlier Detection; Robust tensor recovery; Streaming data preprocessings; Tensor factorization; Tensor recoveries; Urban computing; Recovery
Nested Named Entity Recognition: A Survey,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136934346&doi=10.1145%2f3522593&partnerID=40&md5=e7c78539e83616710b52214a97478630,"With the rapid development of text mining, many studies observe that text generally contains a variety of implicit information, and it is important to develop techniques for extracting such information. Named Entity Recognition (NER), the first step of information extraction, mainly identifies names of persons, locations, and organizations in text. Although existing neural-based NER approaches achieve great success in many language domains, most of them normally ignore the nested nature of named entities. Recently, diverse studies focus on the nested NER problem and yield state-of-the-art performance. This survey attempts to provide a comprehensive review on existing approaches for nested NER from the perspectives of the model architecture and the model property, which may help readers have a better understanding of the current research status and ideas. In this survey, we first introduce the background of nested NER, especially the differences between nested NER and traditional (i.e., flat) NER. We then review the existing nested NER approaches from 2002 to 2020 and mainly classify them into five categories according to the model architecture, including early rule-based, layered-based, region-based, hypergraph-based, and transition-based approaches. We also explore in greater depth the impact of key properties unique to nested NER approaches from the model property perspective, namely entity dependency, stage framework, error propagation, and tag scheme. Finally, we summarize the open challenges and point out a few possible future directions in this area. This survey would be useful for three kinds of readers: (i) Newcomers in the field who want to learn about NER, especially for nested NER. (ii) Researchers who want to clarify the relationship and advantages between flat NER and nested NER. (iii) Practitioners who just need to determine which NER technique (i.e., nested or not) works best in their applications. © 2022 Association for Computing Machinery.",information extraction; named entity recognition; natural language processing; Nested named entity recognition; text mining,Backpropagation; Character recognition; Data mining; Information retrieval; Natural language processing systems; Information extraction; Language processing; Model properties; Modeling architecture; Named entity recognition; Natural language processing; Natural languages; Nested named entity recognition; Text-mining; Surveys
MCRapper: Monte-Carlo Rademacher Averages for Poset Families and Approximate Pattern Mining,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141054213&doi=10.1145%2f3532187&partnerID=40&md5=5555ad09e94b02c6540d1a43ed08c9fc,"""I'm an MC still as honest""-Eminem, Rap GodWe present MCRapper, an algorithm for efficient computation of Monte-Carlo Empirical Rademacher Averages (MCERA) for families of functions exhibiting poset (e.g., lattice) structure, such as those that arise in many pattern mining tasks. The MCERA allows us to compute upper bounds to the maximum deviation of sample means from their expectations, thus it can be used to find both (1) statistically-significant functions (i.e., patterns) when the available data is seen as a sample from an unknown distribution, and (2)approximations of collections of high-expectation functions (e.g., frequent patterns) when the available data is a small sample from a large dataset. This flexibility offered by MCRapper is a big advantage over previously proposed solutions, which could only achieve one of the two. MCRapper uses upper bounds to the discrepancy of the functions to efficiently explore and prune the search space, a technique borrowed from pattern mining itself. To show the practical use of MCRapper, we employ it to develop an algorithm TFP-R for the task of True Frequent Pattern (TFP) mining, by appropriately computing approximations of the negative and positive borders of the collection of patterns of interest, which allow an effective pruning of the pattern space and the computation of strong bounds to the supremum deviation. TFP-R gives guarantees on the probability of including any false positives (precision) and exhibits higher statistical power (recall) than existing methods offering the same guarantees. We evaluate MCRapper and TFP-R and show that they outperform the state-of-the-art for their respective tasks. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Approximation algorithms; frequent patterns; itemsets; sampling; significant patterns; statistical learning theory; statistical testing; subgroup discovery,Approximation algorithms; Computation theory; Computational efficiency; Data mining; Large dataset; Monte Carlo methods; Set theory; Efficient computation; Frequent pattern; Itemset; Lattice structures; Pattern mining; Significant patterns; Statistical learning theory; Statistical testing; Subgroup discovery; Upper Bound; Sampling
Finding Key Structures in MMORPG Graph with Hierarchical Graph Summarization,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141163338&doi=10.1145%2f3522691&partnerID=40&md5=fef17dc72837b15587d071f2b9565a53,"What are the key structures existing in a large real-world MMORPG (Massively Multiplayer Online Role-Playing Game) graph? How can we compactly summarize an MMORPG graph with hierarchical node labels, considering substructures at different levels of hierarchy? Recent MMORPGs generate complex interactions between entities inducing a heterogeneous graph where each entity has hierarchical labels. Succinctly summarizing a heterogeneous MMORPG graph is crucial to better understand its structure; however it is a challenging task since it needs to handle complex interactions and hierarchical labels efficiently. Although there exist few methods to summarize a large-scale graph, they do not deal with heterogeneous graphs with hierarchical node labels.We propose GSHL, a novel method that summarizes a heterogeneous graph with hierarchical labels. We formulate the encoding cost of hierarchical labels using MDL (Minimum Description Length). GSHL exploits the formulation to identify and segment subgraphs, and discovers compact and consistent structures in the graph. Experiments on a large real-world MMORPG graph with multi-million edges show that GSHL is a useful and scalable tool for summarizing the graph, finding important structures in the graph, and finding similar users. © 2022 Association for Computing Machinery.",Graph summarization; hierarchical label; massively multiplayer online role-playing game; minimum description length,Data mining; Social networking (online); Game graphs; Graph summarization; Heterogeneous graph; Hierarchical graphs; Hierarchical label; Key structures; Large-scales; Massively multiplayer online role-playing games; Minimum description length; Real-world; Graph theory
Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141150179&doi=10.1145%2f3516367&partnerID=40&md5=48d2524eb134572c41e98fb0a637a63f,"Multivariate time-series data are frequently observed in critical care settings and are typically characterized by sparsity (missing information) and irregular time intervals. Existing approaches for learning representations in this domain handle these challenges by either aggregation or imputation of values, which in-turn suppresses the fine-grained information and adds undesirable noise/overhead into the machine learning model. To tackle this problem, we propose a Self-supervised Transformer for Time-Series (STraTS) model, which overcomes these pitfalls by treating time-series as a set of observation triplets instead of using the standard dense matrix representation. It employs a novel Continuous Value Embedding technique to encode continuous time and variable values without the need for discretization. It is composed of a Transformer component with multi-head attention layers, which enable it to learn contextual triplet embeddings while avoiding the problems of recurrence and vanishing gradients that occur in recurrent architectures. In addition, to tackle the problem of limited availability of labeled data (which is typically observed in many healthcare applications), STraTS utilizes self-supervision by leveraging unlabeled data to learn better representations by using time-series forecasting as an auxiliary proxy task. Experiments on real-world multivariate clinical time-series benchmark datasets demonstrate that STraTS has better prediction performance than state-of-the-art methods for mortality prediction, especially when labeled data is limited. Finally, we also present an interpretable version of STraTS, which can identify important measurements in the time-series data. Our data preprocessing and model implementation codes are available at https://github.com/sindhura97/STraTS. © 2022 Copyright held by the owner/author(s).",deep learning; healthcare; neural networks; self-supervised learning; Time-series; Transformer,Benchmarking; Continuous time systems; Data mining; Deep neural networks; Embeddings; Forecasting; Health care; Knowledge management; Clinical time series; Deep learning; Healthcare; Labeled data; Learn+; Neural-networks; Self-supervised learning; Time-series data; Times series; Transformer; Time series
The Distance Function Optimization for the Near Neighbors-Based Classifiers,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140904158&doi=10.1145%2f3434769&partnerID=40&md5=7973f60bdb67efcc505aad7133bb6bc6,"Based on the analysis of conditions for a good distance function we found four rules that should be fulfilled. Then, we introduce two new distance functions, a metric and a pseudometric one. We have tested how they fit for distance-based classifiers, especially for the IINC classifier. We rank distance functions according to several criteria and tests. Rankings depend not only on criteria or nature of the statistical test, but also whether it takes into account different difficulties of tasks or whether it considers all tasks as equally difficult. We have found that the new distance functions introduced belong among the four or five best out of 23 distance functions. We have tested them on 24 different tasks, using the mean, the median, the Friedman aligned test, and the Quade test. Our results show that a suitable distance function can improve behavior of distance-based classification rules.  © 2022 Association for Computing Machinery.",classification; distance function; metric; Near neighbors,Condition; Distance functions; Distance-based classification; Distance-based classifiers; Function Optimization; Metric; Nearest neighbor based classifiers; Nearest-neighbour; Pseudometrics; Rank distance
Dual-MGAN: An Efficient Approach for Semi-supervised Outlier Detection with Few Identified Anomalies,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141056507&doi=10.1145%2f3522690&partnerID=40&md5=672e740012977fd94e8e6406d266be47,"Outlier detection is an important task in data mining, and many technologies for it have been explored in various applications. However, owing to the default assumption that outliers are not concentrated, unsupervised outlier detection may not correctly identify group anomalies with higher levels of density. Although high detection rates and optimal parameters can usually be achieved by using supervised outlier detection, obtaining a sufficient number of correct labels is a time-consuming task. To solve these problems, we focus on semi-supervised outlier detection with few identified anomalies and a large amount of unlabeled data. The task of semi-supervised outlier detection is first decomposed into the detection of discrete anomalies and that of partially identified group anomalies, and a distribution construction sub-module and a data augmentation sub-module are then proposed to identify them, respectively. In this way, the dual multiple generative adversarial networks (Dual-MGAN) that combine the two sub-modules can identify discrete as well as partially identified group anomalies. In addition, in view of the difficulty of determining the stop node of training, two evaluation indicators are introduced to evaluate the training status of the sub-GANs. Extensive experiments on synthetic and real-world data show that the proposed Dual-MGAN can significantly improve the accuracy of outlier detection, and the proposed evaluation indicators can reflect the training status of the sub-GANs. © 2022 Association for Computing Machinery.",data augmentation; Discrete anomalies; distribution construction; partially identified group anomalies,Data handling; Data mining; Generative adversarial networks; Statistics; Data augmentation; Discrete anomaly; Distribution construction; Evaluation indicators; High detection rate; Outlier Detection; Partially identified group anomaly; Rate parameters; Semi-supervised; Submodules; Anomaly detection
Generalized Euclidean Measure to Estimate Distances on Multilayer Networks,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141073214&doi=10.1145%2f3529396&partnerID=40&md5=cb64397905910b3e8400752594209d02,"Estimating the distance covered by a spreading event on a network can lead to a better understanding of epidemics, economic growth, and human behavior. There are many methods solving this problem-which has been called Node Vector Distance (NVD)-for single layer networks. However, many phenomena are better represented by multilayer networks: networks in which nodes can connect in qualitatively different ways. In this article, we extend the literature by proposing an algorithm solving NVD for multilayer networks. We do so by adapting the Mahalanobis distance, incorporating the graph's topology via the pseudoinverse of its Laplacian. Since this is a proper generalization of the Euclidean distance in a complex space defined by the topology of the graph, and that it works on multilayer networks, we call our measure the Multi Layer Generalized Euclidean (MLGE). In our experiments, we show that MLGE is intuitive, theoretically simpler than the alternatives, performs well in recovering infection parameters, and it is useful in specific case studies. MLGE requires solving a special case of the effective resistance on the graph, which has a high time complexity. However, this needs to be done only once per network. In the experiments, we show that MLGE can cache its most computationally heavy parts, allowing it to solve hundreds of NVD problems on the same network with little to no additional runtime. MLGE is provided as a free open source tool, along with the data and the code necessary to replicate our results. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Complex networks; network distance; propagation; social networks,Behavioral research; Complex networks; Economics; Multilayers; Network layers; Open systems; Economic growths; Euclidean; Growth behavior; Human behaviors; Multi-layer network; Multi-layers; Network distance; Propagation; Social network; Vector distance; Topology
Urban Knowledge Graph Aided Mobile User Profiling,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176407156&doi=10.1145%2f3596604&partnerID=40&md5=659db3384754ab27284d45d069f6622c,"Nowadays, the explosive growth of personalized web applications and the rapid development of artificial intelligence technology have flourished the recent research on mobile user profiling, i.e., inferring the user profile from mobile behavioral data. Particularly, existing studies mainly follow the data-driven paradigm to develop feature engineering and representation learning on such data, which however suffer from the robustness issue, i.e., generalizing poorly across datasets and profiles without considering semantic knowledge therein. In comparison, the rising knowledge-driven paradigm built upon the knowledge graph (KG) offers a potential solution to mitigate such weakness. Therefore, in this article, we propose a Knowledge Graph aided framework for Mobile User Profiling (KG-MUP). Specifically, to distil semantic knowledge among data, we firstly construct an urban knowledge graph (UrbanKG) with domain entities like users, regions, point of interests (POIs), and so on. identified, as well as semantic relations for home, workplace, spatiality, and so on. extracted. Moreover, we leverage tensor decomposition and graph neural network to obtain knowledgeable user representations from UrbanKG. In addition, we introduce several customized features to quantify individual mobility characteristics for mobile user profiling. Extensive experiments on three real-world mobility datasets demonstrate that KG-MUP achieves state-of-the-art performance on user profile inference tasks. Moreover, further results also reveal the importance of various semantic knowledge to user profile inference, which provides meaningful insights on user modeling with mobile behavioral data. © 2023 Copyright held by the owner/author(s).",human mobility; knowledge graph; Mobile user profiling; representation learning,Graph neural networks; Semantics; User profile; Behavioral data; Explosive growth; Human mobility; Knowledge graphs; Mobile user profiling; Mobile users; Representation learning; Semantics knowledge; User's profiles; User's profiling; Knowledge graph
Spatio-Temporal Event Forecasting Using Incremental Multi-Source Feature Learning,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115028181&doi=10.1145%2f3464976&partnerID=40&md5=347a08acd0abf9972aa148562b12765e,"The forecasting of significant societal events such as civil unrest and economic crisis is an interesting and challenging problem which requires both timeliness, precision, and comprehensiveness. Significant societal events are influenced and indicated jointly by multiple aspects of a society, including its economics, politics, and culture. Traditional forecasting methods based on a single data source find it hard to cover all these aspects comprehensively, thus limiting model performance. Multi-source event forecasting has proven promising but still suffers from several challenges, including (1) geographical hierarchies in multi-source data features, (2) hierarchical missing values, (3) characterization of structured feature sparsity, and (4) difficulty in model's online update with incomplete multiple sources. This article proposes a novel feature learning model that concurrently addresses all the above challenges. Specifically, given multi-source data from different geographical levels, we design a new forecasting model by characterizing the lower-level features' dependence on higher-level features. To handle the correlations amidst structured feature sets and deal with missing values among the coupled features, we propose a novel feature learning model based on an Nth-order strong hierarchy and fused-overlapping group Lasso. An efficient algorithm is developed to optimize model parameters and ensure global optima. More importantly, to enable the model update in real time, the online learning algorithm is formulated and active set techniques are leveraged to resolve the crucial challenge when new patterns of missing features appear in real time. Extensive experiments on 10 datasets in different domains demonstrate the effectiveness and efficiency of the proposed models. © 2021 held by the owner/author(s). Publication rights licensed to ACM.",Event forecasting; feature selection; multiple data sources; online algorithm,Forecasting; Machine learning; Different domains; Effectiveness and efficiencies; Forecasting methods; Forecasting modeling; Model performance; Online learning algorithms; Overlapping groups; Spatio-temporal events; Learning algorithms
Dynamically Adjusting Diversity in Ensembles for the Classification of Data Streams with Concept Drift,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115011467&doi=10.1145%2f3466616&partnerID=40&md5=d8262f3ee7f1cba5b878a79c9902ab6d,"A data stream can be defined as a system that continually generates a lot of data over time. Today, processing data streams requires new demands and challenging tasks in the data mining and machine learning areas. Concept Drift is a problem commonly characterized as changes in the distribution of the data within a data stream. The implementation of new methods for dealing with data streams where concept drifts occur requires algorithms that can adapt to several scenarios to improve its performance in the different experimental situations where they are tested. This research proposes a strategy for dynamic parameter adjustment in the presence of concept drifts. Parameter Estimation Procedure (PEP) is a general method proposed for dynamically adjusting parameters which is applied to the diversity parameter (?) of several classification ensembles commonly used in the area. To this end, the proposed estimation method (PEP) was used to create Boosting-like Online Learning Ensemble with Parameter Estimation (BOLE-PE), Online AdaBoost-based M1 with Parameter Estimation (OABM1-PE), and Oza and Russell's Online Bagging with Parameter Estimation (OzaBag-PE), based on the existing ensembles BOLE, OABM1, and OzaBag, respectively. To validate them, experiments were performed with artificial and real-world datasets using Hoeffding Tree (HT) as base classifier. The accuracy results were statistically evaluated using a variation of the Friedman test and the Nemenyi post-hoc test. The experimental results showed that the application of the dynamic estimation in the diversity parameter (?) produced good results in most scenarios, i.e., the modified methods have improved accuracy in the experiments with both artificial and real-world datasets. © 2021 Association for Computing Machinery.",classification; concept drift; data stream; Ensemble; online learning,Adaptive boosting; Classification (of information); Data mining; Data streams; Learning systems; Adjusting parameters; Base classifiers; Classification ensembles; Classification of data; Dynamic estimation; Dynamic parameters; Estimation methods; Real-world datasets; Parameter estimation
Assessing Large-Scale Power Relations among Locations from Mobility Data,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115010520&doi=10.1145%2f3470770&partnerID=40&md5=f0d09bca643072a2a3ac34cef665d36c,"The pervasiveness of smartphones has shaped our lives, social norms, and the structure that dictates human behavior. They now directly influence how individuals demand resources or interact with network services. From this scenario, identifying key locations in cities is fundamental for the investigation of human mobility and also for the understanding of social problems. In this context, we propose the first graph-based methodology in the literature to quantify the power of Point-of-Interests (POIs) over its vicinity by means of user mobility trajectories. Different from literature, we consider the flow of people in our analysis, instead of the number of neighbor POIs or their structural locations in the city. Thus, we modeled POI's visits using the multiflow graph model where each POI is a node and the transitions of users among POIs are a weighted direct edge. Using this multiflow graph model, we compute the attract, support, and independence powers. The attract power and support power measure how many visits a POI gathers from and disseminate over its neighborhood, respectively. Moreover, the independence power captures the capacity of a POI to receive visitors independently from other POIs. We tested our methodology on well-known university campus mobility datasets and validated on Location-Based Social Networks (LBSNs) datasets from various cities around the world. Our findings show that in university campus: (i) buildings have low support power and attract power; (ii) people tend to move over a few buildings and spend most of their time in the same building; and (iii) there is a slight dependence among buildings, even those with high independence power receive user visits from other buildings on campus. Globally, we reveal that (i) our metrics capture places that impact the number of visits in their neighborhood; (ii) cities in the same continent have similar independence patterns; and (iii) places with a high number of visitation and city central areas are the regions with the highest degree of independence. © 2021 Association for Computing Machinery.",graph-based; Important locations; mobility; power relation,Behavioral research; Buildings; Graphic methods; Location; Degree of independence; Demand resources; Human behaviors; Location-based social networks; Network services; Point of interest; Structural locations; University campus; Graph theory
KRAN: Knowledge Refining Attention Network for Recommendation,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115038676&doi=10.1145%2f3470783&partnerID=40&md5=09847112b2a11e3a4fd0458dc4bf0a62,"Recommender algorithms combining knowledge graph and graph convolutional network are becoming more and more popular recently. Specifically, attributes describing the items to be recommended are often used as additional information. These attributes along with items are highly interconnected, intrinsically forming a Knowledge Graph (KG). These algorithms use KGs as an auxiliary data source to alleviate the negative impact of data sparsity. However, these graph convolutional network based algorithms do not distinguish the importance of different neighbors of entities in the KG, and according to Pareto's principle, the important neighbors only account for a small proportion. These traditional algorithms can not fully mine the useful information in the KG. To fully release the power of KGs for building recommender systems, we propose in this article KRAN, a Knowledge Refining Attention Network, which can subtly capture the characteristics of the KG and thus boost recommendation performance. We first introduce a traditional attention mechanism into the KG processing, making the knowledge extraction more targeted, and then propose a refining mechanism to improve the traditional attention mechanism to extract the knowledge in the KG more effectively. More precisely, KRAN is designed to use our proposed knowledge-refining attention mechanism to aggregate and obtain the representations of the entities (both attributes and items) in the KG. Our knowledge-refining attention mechanism first measures the relevance between an entity and it's neighbors in the KG by attention coefficients, and then further refines the attention coefficients using a ""richer-get-richer""principle, in order to focus on highly relevant neighbors while eliminating less relevant neighbors for noise reduction. In addition, for the item cold start problem, we propose KRAN-CD, a variant of KRAN, which further incorporates pre-trained KG embeddings to handle cold start items. Experiments show that KRAN and KRAN-CD consistently outperform state-of-the-art baselines across different settings. © 2021 Association for Computing Machinery.",attention mechanism; data sparsity; item cold start; Knowledge graph; refine,Convolution; Convolutional neural networks; Graph algorithms; Knowledge representation; Noise abatement; Pareto principle; Attention mechanisms; Cold start problems; Convolutional networks; Knowledge extraction; Recommendation performance; Recommender algorithms; Refining mechanisms; State of the art; Refining
Communication from the Editor-in-Chief: State of the ACM Transactions on Knowledge Discovery from Data,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115010674&doi=10.1145%2f3463950&partnerID=40&md5=e91348321861819b3a70dbdc2754d598,[No abstract available],,
A Robust Reputation-Based Group Ranking System and Its Resistance to Bribery,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115007825&doi=10.1145%2f3462210&partnerID=40&md5=49123b45d3a6d3468d0556ef1463e18e,"The spread of online reviews and opinions and its growing influence on people's behavior and decisions boosted the interest to extract meaningful information from this data deluge. Hence, crowdsourced ratings of products and services gained a critical role in business and governments. Current state-of-the-art solutions rank the items with an average of the ratings expressed for an item, with a consequent lack of personalization for the users, and the exposure to attacks and spamming/spurious users. Using these ratings to group users with similar preferences might be useful to present users with items that reflect their preferences and overcome those vulnerabilities. In this article, we propose a new reputation-based ranking system, utilizing multipartite rating subnetworks, which clusters users by their similarities using three measures, two of them based on Kolmogorov complexity. We also study its resistance to bribery and how to design optimal bribing strategies. Our system is novel in that it reflects the diversity of preferences by (possibly) assigning distinct rankings to the same item, for different groups of users. We prove the convergence and efficiency of the system. By testing it on synthetic and real data, we see that it copes better with spamming/spurious users, being more robust to attacks than state-of-the-art approaches. Also, by clustering users, the effect of bribery in the proposed multipartite ranking system is dimmed, comparing to the bipartite case. © 2021 Association for Computing Machinery.",briebery; clustering; data mining; graph algorithms for the web; multipartite graphs; Ranking systems; reputation-based ranking systems,Computational complexity; Clustering users; Kolmogorov complexity; Personalizations; Products and services; Ranking system; State of the art; State-of-the-art approach; Synthetic and real data; Spamming
Con&Net: A Cross-Network Anchor Link Discovery Method Based on Embedding Representation,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114985979&doi=10.1145%2f3469083&partnerID=40&md5=8abe631ee6dcf92b9207d78d17f8d36d,"Cross-network anchor link discovery is an important research problem and has many applications in heterogeneous social network. Existing schemes of cross-network anchor link discovery can provide reasonable link discovery results, but the quality of these results depends on the features of the platform. Therefore, there is no theoretical guarantee to the stability. This article employs user embedding feature to model the relationship between cross-platform accounts, that is, the more similar the user embedding features are, the more similar the two accounts are. The similarity of user embedding features is determined by the distance of the user features in the latent space. Based on the user embedding features, this article proposes an embedding representation-based method Con&Net(Content and Network) to solve cross-network anchor link discovery problem. Con&Net combines the user's profile features, user-generated content (UGC) features, and user's social structure features to measure the similarity of two user accounts. Con&Net first trains the user's profile features to get profile embedding. Then it trains the network structure of the nodes to get structure embedding. It connects the two features through vector concatenating, and calculates the cosine similarity of the vector based on the embedding vector. This cosine similarity is used to measure the similarity of the user accounts. Finally, Con&Net predicts the link based on similarity for account pairs across the two networks. A large number of experiments in Sina Weibo and Twitter networks show that the proposed method Con&Net is better than state-of-the-art method. The area under the curve (AUC) value of the receiver operating characteristic (ROC) curve predicted by the anchor link is 11% higher than the baseline method, and Precision@30 is 25% higher than the baseline method. © 2021 Association for Computing Machinery.",anchor link; embedding representation; link prediction; Social network,Social networking (online); Area under the curves; Cosine similarity; Network structures; Receiver operating characteristic curves; Research problems; State-of-the-art methods; Theoretical guarantees; User generated content (UGC); Embeddings
Wealth Flow Model: Online Portfolio Selection Based on Learning Wealth Flow Matrices,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115021838&doi=10.1145%2f3464308&partnerID=40&md5=7fa8234d162f4f9c01a382cf996a32d1,"This article proposes a deep learning solution to the online portfolio selection problem based on learning a latent structure directly from a price time series. It introduces a novel wealth flow matrix for representing a latent structure that has special regular conditions to encode the knowledge about the relative strengths of assets in portfolios. Therefore, a wealth flow model (WFM) is proposed to learn wealth flow matrices and maximize portfolio wealth simultaneously. Compared with existing approaches, our work has several distinctive benefits: (1) the learning of wealth flow matrices makes our model more generalizable than models that only predict wealth proportion vectors, and (2) the exploitation of wealth flow matrices and the exploration of wealth growth are integrated into our deep reinforcement algorithm for the WFM. These benefits, in combination, lead to a highly-effective approach for generating reasonable investment behavior, including short-term trend following, the following of a few losers, no self-investment, and sparse portfolios. Extensive experiments on five benchmark datasets from real-world stock markets confirm the theoretical advantage of the WFM, which achieves the Pareto improvements in terms of multiple performance indicators and the steady growth of wealth over the state-of-the-art algorithms. © 2021 Association for Computing Machinery.",deep reinforcement learning; Online portfolio selection; regret bound; wealth flow matrix,Benchmarking; Deep learning; E-learning; Electronic trading; Investments; Benchmark datasets; Effective approaches; Latent structures; Pareto improvements; Performance indicators; Reinforcement algorithms; Relative strength; State-of-the-art algorithms; Learning systems
Hybrid Variational Autoencoder for Recommender Systems,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114997398&doi=10.1145%2f3470659&partnerID=40&md5=90e6720675fb24c0a135372c742f3f62,"E-commerce platforms heavily rely on automatic personalized recommender systems, e.g., collaborative filtering models, to improve customer experience. Some hybrid models have been proposed recently to address the deficiency of existing models. However, their performances drop significantly when the dataset is sparse. Most of the recent works failed to fully address this shortcoming. At most, some of them only tried to alleviate the problem by considering either user side or item side content information. In this article, we propose a novel recommender model called Hybrid Variational Autoencoder (HVAE) to improve the performance on sparse datasets. Different from the existing approaches, we encode both user and item information into a latent space for semantic relevance measurement. In parallel, we utilize collaborative filtering to find the implicit factors of users and items, and combine their outputs to deliver a hybrid solution. In addition, we compare the performance of Gaussian distribution and multinomial distribution in learning the representations of the textual data. Our experiment results show that HVAE is able to significantly outperform state-of-the-art models with robust performance. © 2021 Association for Computing Machinery.",hybrid filtering; Recommender systems,Electronic commerce; Learning systems; Recommender systems; Semantics; Content information; Customer experience; Hybrid solution; Multinomial distributions; Personalized recommender systems; Robust performance; Semantic relevance; State of the art; Collaborative filtering
Community Detection in Partially Observable Social Networks,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115029831&doi=10.1145%2f3461339&partnerID=40&md5=809aa3f9adf477b74760323c4d9e3a74,"The discovery of community structures in social networks has gained significant attention since it is a fundamental problem in understanding the networks' topology and functions. However, most social network data are collected from partially observable networks with both missing nodes and edges. In this article, we address a new problem of detecting overlapping community structures in the context of such an incomplete network, where communities in the network are allowed to overlap since nodes belong to multiple communities at once. To solve this problem, we introduce KroMFac, a new framework that conducts community detection via regularized nonnegative matrix factorization (NMF) based on the Kronecker graph model. Specifically, from an inferred Kronecker generative parameter matrix, we first estimate the missing part of the network. As our major contribution to the proposed framework, to improve community detection accuracy, we then characterize and select influential nodes (which tend to have high degrees) by ranking, and add them to the existing graph. Finally, we uncover the community structures by solving the regularized NMF-aided optimization problem in terms of maximizing the likelihood of the underlying graph. Furthermore, adopting normalized mutual information (NMI), we empirically show superiority of our KroMFac approach over two baseline schemes by using both synthetic and real-world networks. © 2021 Association for Computing Machinery.",Community detection; influential node; Kronecker graph model; matrix factorization,Factorization; Matrix algebra; Population dynamics; Community detection; Community structures; Incomplete networks; Nonnegative matrix factorization; Normalized mutual information; Optimization problems; Overlapping communities; Real-world networks; Graph theory
A Synopsis Based Approach for Itemset Frequency Estimation over Massive Multi-Transaction Stream,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115008473&doi=10.1145%2f3465238&partnerID=40&md5=45918c0d50b472717ffc521f79c82968,"The streams where multiple transactions are associated with the same key are prevalent in practice, e.g., a customer has multiple shopping records arriving at different time. Itemset frequency estimation on such streams is very challenging since sampling based methods, such as the popularly used reservoir sampling, cannot be used. In this article, we propose a novel k-Minimum Value (KMV) synopsis based method to estimate the frequency of itemsets over multi-transaction streams. First, we extract the KMV synopses for each item from the stream. Then, we propose a novel estimator to estimate the frequency of an itemset over the KMV synopses. Comparing to the existing estimator, our method is not only more accurate and efficient to calculate but also follows the downward-closure property. These properties enable the incorporation of our new estimator with existing frequent itemset mining (FIM) algorithm (e.g., FP-Growth) to mine frequent itemsets over multi-transaction streams. To demonstrate this, we implement a KMV synopsis based FIM algorithm by integrating our estimator into existing FIM algorithms, and we prove it is capable of guaranteeing the accuracy of FIM with a bounded size of KMV synopsis. Experimental results on massive streams show our estimator can significantly improve on the accuracy for both estimating itemset frequency and FIM compared to the existing estimators. © 2021 Association for Computing Machinery.",close frequent itemset mining; Data stream mining; downward-closure estimator; itemset frequency estimation; k-minimum value synopsis; massive multi-transaction stream data,Computer science; Data mining; Bounded size; Downward closure properties; Frequent itemset mining; Minimum value; Multi-transactions; Multiple transactions; Reservoir samplings; Sampling-based method; Frequency estimation
Embedding Heterogeneous Information Network in Hyperbolic Spaces,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115016158&doi=10.1145%2f3468674&partnerID=40&md5=b18488b7b17d4cca69b9841cf0124933,"Heterogeneous information network (HIN) embedding, aiming to project HIN into a low-dimensional space, has attracted considerable research attention. Most of the existing HIN embedding methods focus on preserving the inherent network structure and semantic correlations in Euclidean spaces. However, one fundamental problem is whether the Euclidean spaces are the intrinsic spaces of HINo?Recent researches find the complex network with hyperbolic geometry can naturally reflect some properties, e.g., hierarchical and power-law structure. In this article, we make an effort toward embedding HIN in hyperbolic spaces. We analyze the structures of three HINs and discover some properties, e.g., the power-law distribution, also exist in HINs. Therefore, we propose a novel HIN embedding model HHNE. Specifically, to capture the structure and semantic relations between nodes, HHNE employs the meta-path guided random walk to sample the sequences for each node. Then HHNE exploits the hyperbolic distance as the proximity measurement. We also derive an effective optimization strategy to update the hyperbolic embeddings iteratively. Since HHNE optimizes different relations in a single space, we further propose the extended model HHNE++. HHNE++ models different relations in different spaces, which enables it to learn complex interactions in HINs. The optimization strategy of HHNE++ is also derived to update the parameters of HHNE++ in a principle manner. The experimental results demonstrate the effectiveness of our proposed models. © 2021 Association for Computing Machinery.",Heterogeneous information network; network embedding; social network analysis,Embeddings; Geometry; Information services; Iterative methods; Semantics; Heterogeneous information; Hyperbolic distances; Hyperbolic geometry; Low-dimensional spaces; Network structures; Optimization strategy; Power law distribution; Semantic relations; Complex networks
Constrained Dual-Level Bandit for Personalized Impression Regulation in Online Ranking Systems,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115012590&doi=10.1145%2f3461340&partnerID=40&md5=fd139755fdffc3aa7bbe17fe5818c5e7,"Impression regulation plays an important role in various online ranking systems, e.g., e-commerce ranking systems always need to achieve local commercial demands on some pre-labeled target items like fresh item cultivation and fraudulent item counteracting while maximizing its global revenue. However, local impression regulation may cause ""butterfly effects""on the global scale, e.g., in e-commerce, the price preference fluctuation in initial conditions (overpriced or underpriced items) may create a significantly different outcome, thus affecting shopping experience and bringing economic losses to platforms. To prevent ""butterfly effects"", some researchers define their regulation objectives with global constraints, by using contextual bandit at the page-level that requires all items on one page sharing the same regulation action, which fails to conduct impression regulation on individual items. To address this problem, in this article, we propose a personalized impression regulation method that can directly makes regulation decisions for each user-item pair. Specifically, we model the regulation problem as a Constrained Dual-level Bandit (CDB) problem, where the local regulation action and reward signals are at the item-level while the global effect constraint on the platform impression can be calculated at the page-level only. To handle the asynchronous signals, we first expand the page-level constraint to the item-level and then derive the policy updating as a second-order cone optimization problem. Our CDB approaches the optimal policy by iteratively solving the optimization problem. Experiments are performed on both offline and online datasets, and the results, theoretically and empirically, demonstrate CDB outperforms state-of-the-art algorithms. © 2021 Association for Computing Machinery.",constrained dual-level bandit; Online ranking systems; personalized impression regulation,Chaos theory; Electronic commerce; Iterative methods; Losses; Optimization; Asynchronous signals; Contextual bandits; Global constraints; Initial conditions; Local regulations; Optimization problems; Regulation problems; State-of-the-art algorithms; Online systems
Learning Sentence-to-Hashtags Semantic Mapping for Hashtag Recommendation on Microblogs,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114988746&doi=10.1145%2f3466876&partnerID=40&md5=a402694ecfd00ce39fa4f688514f43da,"The growing use of microblogging platforms is generating a huge amount of posts that need effective methods to be classified and searched. In Twitter and other social media platforms, hashtags are exploited by users to facilitate the search, categorization, and spread of posts. Choosing the appropriate hashtags for a post is not always easy for users, and therefore posts are often published without hashtags or with hashtags not well defined. To deal with this issue, we propose a new model, called HASHET (HAshtag recommendation using Sentence-to-Hashtag Embedding Translation), aimed at suggesting a relevant set of hashtags for a given post. HASHET is based on two independent latent spaces for embedding the text of a post and the hashtags it contains. A mapping process based on a multi-layer perceptron is then used for learning a translation from the semantic features of the text to the latent representation of its hashtags. We evaluated the effectiveness of two language representation models for sentence embedding and tested different search strategies for semantic expansion, finding out that the combined use of BERT (Bidirectional Encoder Representation from Transformer) and a global expansion strategy leads to the best recommendation results. HASHET has been evaluated on two real-world case studies related to the 2016 United States presidential election and COVID-19 pandemic. The results reveal the effectiveness of HASHET in predicting one or more correct hashtags, with an average F-score up to 0.82 and a recommendation hit-rate up to 0.92. Our approach has been compared to the most relevant techniques used in the literature (generative models, unsupervised models, and attention-based supervised models) by achieving up to 15% improvement in F-score for the hashtag recommendation task and 9% for the topic discovery task. © 2021 Association for Computing Machinery.",Deep neural networks; hashtag recommendation; sentence embedding; social media; word embedding,Embeddings; Mapping; Multilayer neural networks; Semantics; Social networking (online); Micro-blogging platforms; Multi layer perceptron; Presidential election; Representation model; Search strategies; Semantic expansion; Semantic features; Social media platforms; Translation (languages)
On-Shelf Utility Mining of Sequence Data,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115030599&doi=10.1145%2f3457570&partnerID=40&md5=7931211b7daa3e74e6b2f3fce72712e3,"Utility mining has emerged as an important and interesting topic owing to its wide application and considerable popularity. However, conventional utility mining methods have a bias toward items that have longer on-shelf time as they have a greater chance to generate a high utility. To eliminate the bias, the problem of on-shelf utility mining (OSUM) is introduced. In this article, we focus on the task of OSUM of sequence data, where the sequential database is divided into several partitions according to time periods and items are associated with utilities and several on-shelf time periods. To address the problem, we propose two methods, OSUM of sequence data (OSUMS) and OSUMS+, to extract on-shelf high-utility sequential patterns. For further efficiency, we also design several strategies to reduce the search space and avoid redundant calculation with two upper bounds time prefix extension utility (TPEU) and time reduced sequence utility (TRSU). In addition, two novel data structures are developed for facilitating the calculation of upper bounds and utilities. Substantial experimental results on certain real and synthetic datasets show that the two methods outperform the state-of-the-art algorithm. In conclusion, OSUMS may consume a large amount of memory and is unsuitable for cases with limited memory, while OSUMS+ has wider real-life applications owing to its high efficiency. © 2021 Association for Computing Machinery.",data mining; On-shelf utility mining; sequence data; utility mining,Mining; High-efficiency; Limited memory; Real-life applications; Sequential database; Sequential patterns; State-of-the-art algorithms; Synthetic datasets; Utility mining; Efficiency
Opinion Dynamics Optimization by Varying Susceptibility to Persuasion via Non-Convex Local Search,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114998150&doi=10.1145%2f3466617&partnerID=40&md5=76aa25873b6c9dcc29d99e336814b54c,"A long line of work in social psychology has studied variations in people's susceptibility to persuasion - the extent to which they are willing to modify their opinions on a topic. This body of literature suggests an interesting perspective on theoretical models of opinion formation by interacting parties in a network: in addition to considering interventions that directly modify people's intrinsic opinions, it is also natural to consider interventions that modify people's susceptibility to persuasion.In this work, motivated by this fact, we propose an influence optimization problem. Specifically, we adopt a popular model for social opinion dynamics, where each agent has some fixed innate opinion, and a resistance that measures the importance it places on its innate opinion; agents influence one another's opinions through an iterative process. Under certain conditions, this iterative process converges to some equilibrium opinion vector. For the unbudgeted variant of the problem, the goal is to modify the resistance of any number of agents (within some given range) such that the sum of the equilibrium opinions is minimized; for the budgeted variant, in addition the algorithm is given upfront a restriction on the number of agents whose resistance may be modified.We prove that the objective function is in general non-convex. Hence, formulating the problem as a convex program as in an early version of this work (Abebe et al., KDD'18) might have potential correctness issues. We instead analyze the structure of the objective function, and show that any local optimum is also a global optimum, which is somehow surprising as the objective function might not be convex. Furthermore, we combine the iterative process and the local search paradigm to design very efficient algorithms that can solve the unbudgeted variant of the problem optimally on large-scale graphs containing millions of nodes. Finally, we propose and evaluate experimentally a family of heuristics for the budgeted variant of the problem. © 2021 held by the owner/author(s). Publication rights licensed to ACM.",non-convex local search; Opinion dynamics; susceptibility to persuasion,Budget control; Convex optimization; Graph algorithms; Local search (optimization); Convex programs; Global optimum; Iterative process; Objective functions; Opinion dynamics; Opinion formation; Optimization problems; Social psychology; Iterative methods
Exploiting Heterogeneous Graph Neural Networks with Latent Worker/Task Correlation Information for Label Aggregation in Crowdsourcing,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114993652&doi=10.1145%2f3460865&partnerID=40&md5=c0b4475cd2e7ea944a9f342d9ea5ec87,"Crowdsourcing has attracted much attention for its convenience to collect labels from non-expert workers instead of experts. However, due to the high level of noise from the non-experts, a label aggregation model that infers the true label from noisy crowdsourced labels is required. In this article, we propose a novel framework based on graph neural networks for aggregating crowd labels. We construct a heterogeneous graph between workers and tasks and derive a new graph neural network to learn the representations of nodes and the true labels. Besides, we exploit the unknown latent interaction between the same type of nodes (workers or tasks) by adding a homogeneous attention layer in the graph neural networks. Experimental results on 13 real-world datasets show superior performance over state-of-the-art models. © 2021 Association for Computing Machinery.",Crowdsourcing; graph neural network; label aggregation,Crowdsourcing; Graph theory; Graph neural networks; Heterogeneous graph; Label aggregation; Real-world datasets; State of the art; Multilayer neural networks
Establishing Smartphone User Behavior Model Based on Energy Consumption Data,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115026982&doi=10.1145%2f3461459&partnerID=40&md5=cd58cf3b8e20c308986c738db875318f,"In smartphone data analysis, both energy consumption modeling and user behavior mining have been explored extensively, but the relationship between energy consumption and user behavior has been rarely studied. Such a relationship is explored over large-scale users in this article. Based on energy consumption data, where each users' feature vector is represented by energy breakdown on hardware components of different apps, User Behavior Models (UBM) are established to capture user behavior patterns (i.e., app preference, usage time). The challenge lies in the high diversity of user behaviors (i.e., massive apps and usage ways), which leads to high dimension and dispersion of data. To overcome the challenge, three mechanisms are designed. First, to reduce the dimension, apps are ranked with the top ones identified as typical apps to represent all. Second, the dispersion is reduced by scaling each users' feature vector with typical apps to unit g.,""1 norm. The scaled vector becomes Usage Pattern, while the g.,""1 norm of vector before scaling is treated as Usage Intensity. Third, the usage pattern is analyzed with a two-layer clustering approach to further reduce data dispersion. In the upper layer, each typical app is studied across its users with respect to hardware components to identify Typical Hardware Usage Patterns (THUP). In the lower layer, users are studied with respect to these THUPs to identify Typical App Usage Patterns (TAUP). The analytical results of these two layers are consolidated into Usage Pattern Models (UPM), and UBMs are finally established by a union of UPMs and Usage Intensity Distributions (UID). By carrying out experiments on energy consumption data from 18,308 distinct users over 10 days, 33 UBMs are extracted from training data. With the test data, it is proven that these UBMs cover 94% user behaviors and achieve up to 20% improvement in accuracy of energy representation, as compared with the baseline method, PCA. Besides, potential applications and implications of these UBMs are illustrated for smartphone manufacturers, app developers, network providers, and so on. © 2021 Association for Computing Machinery.",Data mining; smartphone energy consumption; user behavior modeling,Behavioral research; Dispersions; Energy utilization; Smartphones; Vectors; Clustering approach; Energy consumption datum; Energy consumption model; Energy representations; Hardware components; Intensity distribution; User behavior modeling; User behavior patterns; Data reduction
High-Value Token-Blocking: Efficient Blocking Method for Record Linkage,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114989206&doi=10.1145%2f3450527&partnerID=40&md5=e47653f664182986b37c2c4dbb3bb5f4,"Data integration is an important component of Big Data analytics. One of the key challenges in data integration is record linkage, that is, matching records that represent the same real-world entity. Because of computational costs, methods referred to as blocking are employed as a part of the record linkage pipeline in order to reduce the number of comparisons among records. In the past decade, a range of blocking techniques have been proposed. Real-world applications require approaches that can handle heterogeneous data sources and do not rely on labelled data. We propose high-value token-blocking (HVTB), a simple and efficient approach for blocking that is unsupervised and schema-agnostic, based on a crafted use of Term Frequency-Inverse Document Frequency. We compare HVTB with multiple methods and over a range of datasets, including a novel unstructured dataset composed of titles and abstracts of scientific papers. We thoroughly discuss results in terms of accuracy, use of computational resources, and different characteristics of datasets and records. The simplicity of HVTB yields fast computations and does not harm its accuracy when compared with existing approaches. It is shown to be significantly superior to other methods, suggesting that simpler methods for blocking should be considered before resorting to more sophisticated methods. © 2021 Association for Computing Machinery.",Blocking; entity resolution; record linkage,Advanced Analytics; Data Analytics; Inverse problems; Text processing; Blocking technique; Computational costs; Computational resources; Heterogeneous data sources; Multiple methods; Real-world entities; Scientific papers; Term frequency-inverse document frequencies; Data integration
S2OSC: A Holistic Semi-Supervised Approach for Open Set Classification,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114990151&doi=10.1145%2f3468675&partnerID=40&md5=2442457b135a034a11e22cb77b183e0b,"Open set classification (OSC) tackles the problem of determining whether the data are in-class or out-of-class during inference, when only provided with a set of in-class examples at training time. Traditional OSC methods usually train discriminative or generative models with the owned in-class data, and then utilize the pre-trained models to classify test data directly. However, these methods always suffer from the embedding confusion problem, i.e., partial out-of-class instances are mixed with in-class ones of similar semantics, making it difficult to classify. To solve this problem, we unify semi-supervised learning to develop a novel OSC algorithm, S2OSC, which incorporates out-of-class instances filtering and model re-training in a transductive manner. In detail, given a pool of newly coming test data, S2OSC firstly filters the mostly distinct out-of-class instances using the pre-trained model, and annotates super-class for them. Then, S2OSC trains a holistic classification model by combing in-class and out-of-class labeled data with the remaining unlabeled test data in a semi-supervised paradigm. Furthermore, considering that data are usually in the streaming form in real applications, we extend S2OSC into an incremental update framework (I-S2OSC), and adopt a knowledge memory regularization to mitigate the catastrophic forgetting problem in incremental update. Despite the simplicity of proposed models, the experimental results show that S2OSC achieves state-of-the-art performance across a variety of OSC tasks, including 85.4% of F1 on CIFAR-10 with only 300 pseudo-labels. We also demonstrate how S2OSC can be expanded to incremental OSC setting effectively with streaming data. © 2021 Association for Computing Machinery.",embedding confusion; incremental learning; Open set classification; semi-supervised learning,Semantics; Catastrophic forgetting problem; Classification models; Generative model; Incremental updates; Real applications; Semi-supervised; State-of-the-art performance; Streaming data; Semi-supervised learning
"Transfer Learning across Graph Convolutional Networks: Methods, Theory, and Applications",2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176408038&doi=10.1145%2f3617376&partnerID=40&md5=8df7bf69347d6d413f88a456af5f4484,"Graph neural networks have been widely used for learning representations of nodes for many downstream tasks on graph data. Existing models were designed for the nodes on a single graph, which would not be able to utilize information across multiple graphs. The real world does have multiple graphs where the nodes are often partially aligned. For examples, knowledge graphs share a number of named entities though they may have different relation schema; collaboration networks on publications and awarded projects share some researcher nodes who are authors and investigators, respectively; people use multiple web services, shopping, tweeting, rating movies, and some may register the same e-mail account across the platforms. In this article, we propose partially aligned graph convolutional networks to learn node representations across the models. We provide multiple methods such as model sharing, regularization, and alignment reconstruction, as well as theoretical analysis to positively transfer knowledge across the set of partially aligned nodes. Extensive experiments on real-world knowledge graphs, collaboration networks, and bipartite rating graphs show the superior performance of our proposed methods on relation classification, link prediction, and item recommendation. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph data; graph neural network; transfer learning,Convolution; Convolutional neural networks; Graph structures; Graph theory; Graphic methods; Knowledge graph; Knowledge management; Learning systems; Transfer learning; Web services; Collaboration network; Convolutional networks; Down-stream; Graph data; Graph neural networks; Knowledge graphs; Network applications; Network methods; Real-world; Transfer learning; Graph neural networks
Measuring the Network Vulnerability Based on Markov Criticality,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115020380&doi=10.1145%2f3464390&partnerID=40&md5=439095f81d0bc10a06d3973234b7054f,"Vulnerability assessment - a critical issue for networks - attempts to foresee unexpected destructive events or hostile attacks in the whole system. In this article, we consider a new Markov global connectivity metric - Kemeny constant, and take its derivative called Markov criticality to identify critical links. Markov criticality allows us to find links that are most influential on the derivative of Kemeny constant. Thus, we can utilize it to identity a critical link (i, j) from node i to node j, such that removing it leads to a minimization of networks' global connectivity, i.e., the Kemeny constant. Furthermore, we also define a novel vulnerability index to measure the average speed by which we can disconnect a specified ratio of links with network decomposition. Our method is of high efficiency, which can be easily employed to calculate the Markov criticality in real-life networks. Comprehensive experiments on several synthetic and real-life networks have demonstrated our method's better performance by comparing it with state-of-the-art baseline approaches. © 2021 Association for Computing Machinery.",Complex networks; edge centrality; Kemeny constant; Markov chain; vulnerability assessment,Criticality (nuclear fission); Critical issues; Global connectivity; Network decomposition; Network vulnerability; Real-life networks; State of the art; Vulnerability assessments; Vulnerability index; Network security
Adaptive Neighbor Graph Aggregated Graph Attention Network for Heterogeneous Graph Embedding,2022,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176751464&doi=10.1145%2f3616377&partnerID=40&md5=7ca20ef71911067cae0c92a57fdba37a,"Graph attention network can generate effective feature embedding by specifying different weights to different nodes. The key of the research on heterogeneous graph embedding is the way to combine its rich structural information with semantic relations to aggregate the neighborhood information. Most of the existing heterogeneous graph representation learning methods guide the selection of neighbors by defining various meta-paths on heterogeneous graphs. However, these models only consider the information contained in the nodes under different paths and ignore the potential semantic relationships of nodes in different neighbor graph structures, which leads to the underutilization of graph structure information. In this article, we propose a novel adaptive framework named Neighbor Graph Aggregated Graph Attention Network (NGGAN) to fully exploit graph topological details in heterogeneous graph, and aggregates their information to obtain an effective embedding. The key idea is to use different levels of sampling methods to define neighborhood, and use neighbor graphs to represent the complex structural interaction between nodes. In this way, the high-order relationship between nodes and the latent semantics of neighbor graphs can be fully explored. Afterward, a hierarchical attention mechanism is applied to adaptively learn the importance of different objects, including node information, path information, and neighbor graph information. Multiple downstream tasks are performed on four real-world heterogeneous graph datasets, and the experimental results demonstrate the effectiveness of NGGAN.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph attention network; graph neural network; graph representation learning; heterogeneous graphs,Graph embeddings; Graph neural networks; Graph structures; Graph theory; Graphic methods; Semantics; Feature embedding; Graph attention network; Graph embeddings; Graph neural networks; Graph representation; Graph representation learning; Graph structures; Heterogeneous graph; Neighbor graph; Structural information; Aggregates
HCBST: An Efficient Hybrid Sampling Technique for Class Imbalance Problems,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130342395&doi=10.1145%2f3488280&partnerID=40&md5=b5028a8e846e893447903e9ec87ad1d4,"Class imbalance problem is prevalent in many real-world domains. It has become an active area of research. In binary classification problems, imbalance learning refers to learning from a dataset with a high degree of skewness to the negative class. This phenomenon causes classification algorithms to perform woefully when predicting positive classes with new examples. Data resampling, which involves manipulating the training data before applying standard classification techniques, is among the most commonly used techniques to deal with the class imbalance problem. This article presents a new hybrid sampling technique that improves the overall performance of classification algorithms for solving the class imbalance problem significantly. The proposed method called the Hybrid Cluster-Based Undersampling Technique (HCBST) uses a combination of the cluster undersampling technique to under-sample the majority instances and an oversampling technique derived from Sigma Nearest Oversampling based on Convex Combination, to oversample the minority instances to solve the class imbalance problem with a high degree of accuracy and reliability. The performance of the proposed algorithm was tested using 11 datasets from the National Aeronautics and Space Administration Metric Data Program data repository and University of California Irvine Machine Learning data repository with varying degrees of imbalance. Results were compared with classification algorithms such as the K-nearest neighbours, support vector machines, decision tree, random forest, neural network, AdaBoost, naïve Bayes, and quadratic discriminant analysis. Tests results revealed that for the same datasets, the HCBST performed better with average performances of 0.73, 0.67, and 0.35 in terms of performance measures of area under curve, geometric mean, and Matthews Correlation Coefficient, respectively, across all the classifiers used for this study. The HCBST has the potential of improving the performance of the class imbalance problem, which by extension, will improve on the various applications that rely on the concept for a solution.  © 2021 Association for Computing Machinery.",Class imbalance; classification; cluster undersampling technique; clustering; data sampling,Adaptive boosting; Discriminant analysis; NASA; Nearest neighbor search; Software testing; Support vector machines; Class imbalance; Class imbalance problems; Classification algorithm; Cluster undersampling technique; Cluster-based; Clusterings; Data sampling; Hybrid clusters; Performance; Under-sampling; Decision trees
Toward Understanding and Evaluating Structural Node Embeddings,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125781244&doi=10.1145%2f3481639&partnerID=40&md5=99953a4e65c3ddd9b16c616f601aae6b,"While most network embedding techniques model the proximity between nodes in a network, recently there has been significant interest in structural embeddings that are based on node equivalences, a notion rooted in sociology: equivalences or positions are collections of nodes that have similar roles - i.e., similar functions, ties or interactions with nodes in other positions - irrespective of their distance or reachability in the network. Unlike the proximity-based methods that are rigorously evaluated in the literature, the evaluation of structural embeddings is less mature. It relies on small synthetic or real networks with labels that are not perfectly defined, and its connection to sociological equivalences has hitherto been vague and tenuous. With new node embedding methods being developed at a breakneck pace, proper evaluation, and systematic characterization of existing approaches will be essential to progress. To fill in this gap, we set out to understand what types of equivalences structural embeddings capture. We are the first to contribute rigorous intrinsic and extrinsic evaluation methodology for structural embeddings, along with carefully-designed, diverse datasets of varying sizes. We observe a number of different evaluation variables that can lead to different results (e.g., choice of similarity measure, classifier, and label definitions). We find that degree distributions within nodes' local neighborhoods can lead to simple yet effective baselines in their own right and guide the future development of structural embedding. We hope that our findings can influence the design of further node embedding methods and also pave the way for more comprehensive and fair evaluation of structural embedding methods.  © 2021 Association for Computing Machinery.",Graph mining; node embedding; role discovery,Data mining; Graph embeddings; Graph theory; Embedding method; Embedding technique; Embeddings; Graph mining; Network embedding; Node embedding; Reachability; Real networks; Role discovery; Synthetic networks; Network embeddings
Overlapping Graph Clustering in Attributed Networks via Generalized Cluster Potential Game,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168421934&doi=10.1145%2f3597436&partnerID=40&md5=d1e369238af4d796b025af8f46fc69d0,"Overlapping graph clustering is essential to understand the nature and behavior of real complex systems including human interactions, technical systems and transportation network. However, in addition of topological structure, many real-world networked systems contain spare factors, i.e., attributes of networks. Despite the considerable efforts that have been made in graph clustering, they only concentrate on the topological structure, which lack a profound understanding of cluster configuration on attributed graphs. To address this great challenge, in this article, we propose a new overlapping graph clustering algorithm by integrating the topological and attributive information into a cluster potential game (CPG). Firstly, a generalized definition of the utility function is provided, which measures the payoff of each node based on different node-to-cluster distance functions. It is worth mentioning that the model we proposed is able to associate with the classic ordinal potential game well. Then, we define the measures of both tightness and the homogeneity in each cluster, and introduce a novel two-way selection mechanism. The goal is to extend the flexibility of the cluster potential game, so that one can achieve a win-win situation between nodes and clusters. Finally, a distributed and heterogeneous multiagent system (DHMAS) is carefully designed based on a fast self-learning algorithm (SLA) for attributed overlapping graph clustering. Two series of experiments are implemented in multi-types datasets and the results verify the effectiveness and the scalability after the comparison with the most advanced approaches of literature. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attributed graph; cluster potential game; optimal strategy; Overlapping graph clustering; selection mechanisms,Multi agent systems; Topology; Attributed graphs; Cluster potential game; Graph clustering; Humaninteraction; Optimal strategies; Overlapping graph clustering; Potential games; Real complex systems; Selection mechanism; Topological structure; Clustering algorithms
Scalable Mining of High-Utility Sequential Patterns With Three-Tier MapReduce Model,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127980695&doi=10.1145%2f3487046&partnerID=40&md5=384a4ca2a292ed02f619369bc32b6aad,"High-utility sequential pattern mining (HUSPM) is a hot research topic in recent decades since it combines both sequential and utility properties to reveal more information and knowledge rather than the traditional frequent itemset mining or sequential pattern mining. Several works of HUSPM have been presented but most of them are based on main memory to speed up mining performance. However, this assumption is not realistic and not suitable in large-scale environments since in real industry, the size of the collected data is very huge and it is impossible to fit the data into the main memory of a single machine. In this article, we first develop a parallel and distributed three-stage MapReduce model for mining high-utility sequential patterns based on large-scale databases. Two properties are then developed to hold the correctness and completeness of the discovered patterns in the developed framework. In addition, two data structures called sidset and utility-linked list are utilized in the developed framework to accelerate the computation for mining the required patterns. From the results, we can observe that the designed model has good performance in large-scale datasets in terms of runtime, memory, efficiency of the number of distributed nodes, and scalability compared to the serial HUSP-Span approach.  © 2021 Association for Computing Machinery.",High-utility sequential pattern mining; large-scale; MapReduce; parallel and distributed,Data mining; Large dataset; Scheduling algorithms; High-utility sequential pattern mining; Large-scales; Main-memory; Map-reduce; MapReduce models; Parallel and distributed; Performance; Property; Sequential patterns; Sequential-pattern mining; MapReduce
Graph Community Infomax,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129642634&doi=10.1145%2f3480244&partnerID=40&md5=68cc57778c2314476957aa552ea4d85b,"Graph representation learning aims at learning low-dimension representations for nodes in graphs, and has been proven very useful in several downstream tasks. In this article, we propose a new model, Graph Community Infomax (GCI), that can adversarial learn representations for nodes in attributed networks. Different from other adversarial network embedding models, which would assume that the data follow some prior distributions and generate fake examples, GCI utilizes the community information of networks, using nodes as positive(or real) examples and negative(or fake) examples at the same time. An autoencoder is applied to learn the embedding vectors for nodes and reconstruct the adjacency matrix, and a discriminator is used to maximize the mutual information between nodes and communities. Experiments on several real-world and synthetic networks have shown that GCI outperforms various network embedding methods on community detection tasks.  © 2021 Association for Computing Machinery.",attributed network; community detecton; Mutual information; representation learning,Data mining; Graph embeddings; Graph theory; Population dynamics; Adversarial networks; Attributed network; Community detecton; Down-stream; Graph representation; Infomax; Learn+; Mutual informations; Network embedding; Representation learning; Network embeddings
Network Public Opinion Detection During the Coronavirus Pandemic: A Short-Text Relational Topic Model,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159195317&doi=10.1145%2f3480246&partnerID=40&md5=b778616187f4711849a0707b103654af,"Online social media provides rich and varied information reflecting the significant concerns of the public during the coronavirus pandemic. Analyzing what the public is concerned with from social media information can support policy-makers to maintain the stability of the social economy and life of the society. In this article, we focus on the detection of the network public opinions during the coronavirus pandemic. We propose a novel Relational Topic Model for Short texts (RTMS) to draw opinion topics from social media data. RTMS exploits the feature of texts in online social media and the opinion propagation patterns among individuals. Moreover, a dynamic version of RTMS (DRTMS) is proposed to capture the evolution of public opinions. Our experiment is conducted on a real-world dataset which includes 67,592 comments from 14,992 users. The results demonstrate that, compared with the benchmark methods, the proposed RTMS and DRTMS models can detect meaningful public opinions by leveraging the feature of social media data. It can also effectively capture the evolution of public concerns during different phases of the coronavirus pandemic.  © 2021 Association for Computing Machinery.",coronavirus pandemic; dynamic topic model; Public Opinion detection; relational topic modeling for short text,Social aspects; Social networking (online); Coronavirus pandemic; Coronaviruses; Dynamic topic models; Network public opinions; Opinion detections; Public opinion detection; Public opinions; Relational topic modeling for short text; Short texts; Topic Modeling; Coronavirus
Context-aware Spatial-Temporal Neural Network for Citywide Crowd Flow Prediction via Modeling Long-range Spatial Dependency,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138042004&doi=10.1145%2f3477577&partnerID=40&md5=a35cbfb3444f530b62a141372d984fbb,"Crowd flow prediction is of great importance in a wide range of applications from urban planning, traffic control to public safety. It aims at predicting the inflow (the traffic of crowds entering a region in a given time interval) and outflow (the traffic of crowds leaving a region for other places) of each region in the city with knowing the historical flow data. In this article, we propose DeepSTN+, a deep learning-based convolutional model, to predict crowd flows in the metropolis. First, DeepSTN+ employs the ConvPlus structure to model the long-range spatial dependence among crowd flows in different regions. Further, PoI distributions and time factor are combined to express the effect of location attributes to introduce prior knowledge of the crowd movements. Finally, we propose a temporal attention-based fusion mechanism to stabilize the training process, which further improves the performance. Extensive experimental results based on four real-life datasets demonstrate the superiority of our model, i.e., DeepSTN+ reduces the error of the crowd flow prediction by approximately 10%-21% compared with the state-of-the-art baselines.  © 2021 Association for Computing Machinery.",deep learning; population flow; spatial-temporal prediction; Urban computing,Forecasting; Traffic control; Context-Aware; Crowd flows; Deep learning; Flow prediction; Long-range spatials; Population flow; Spatial temporals; Spatial-temporal prediction; Temporal prediction; Urban computing; Deep learning
Deciphering Feature Effects on Decision-Making in Ordinal Regression Problems: An Explainable Ordinal Factorization Model,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134091965&doi=10.1145%2f3487048&partnerID=40&md5=31fccf325fb3a3a799d613a86a6af7df,"Ordinal regression predicts the objects' labels that exhibit a natural ordering, which is vital to decision-making problems such as credit scoring and clinical diagnosis. In these problems, the ability to explain how the individual features and their interactions affect the decisions is as critical as model performance. Unfortunately, the existing ordinal regression models in the machine learning community aim at improving prediction accuracy rather than explore explainability. To achieve high accuracy while explaining the relationships between the features and the predictions, we propose a new method for ordinal regression problems, namely the Explainable Ordinal Factorization Model (XOFM). XOFM uses piecewise linear functions to approximate the shape functions of individual features, and renders the pairwise features interaction effects as heat-maps. The proposed XOFM captures the nonlinearity in the main effects and ensures the interaction effects' same flexibility. Therefore, the underlying model yields comparable performance while remaining explainable by explicitly describing the main and interaction effects. To address the potential sparsity problem caused by discretizing the whole feature scale into several sub-intervals, XOFM integrates the Factorization Machines (FMs) to factorize the model parameters. Comprehensive experiments with benchmark real-world and synthetic datasets demonstrate that the proposed XOFM leads to state-of-the-art prediction performance while preserving an easy-to-understand explainability.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",decision support; explainable machine learning; factorization machines; Ordinal regression,Benchmarking; Decision support systems; Diagnosis; Factorization; Forecasting; Machine learning; Piecewise linear techniques; Regression analysis; Decision supports; Explainable machine learning; Factorization machines; Factorization model; Individual features; Interaction effect; Machine-learning; Main effect; Ordinal regression; Regression problem; Decision making
A Normalizing Flow-Based Co-Embedding Model for Attributed Networks,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159170309&doi=10.1145%2f3477049&partnerID=40&md5=e77a4f543e474872de337ef4886906df,"Network embedding is a technique that aims at inferring the low-dimensional representations of nodes in a semantic space. In this article, we study the problem of inferring the low-dimensional representations of both nodes and attributes for attributed networks in the same semantic space such that the affinity between a node and an attribute can be effectively measured. Intuitively, this problem can be addressed by simply utilizing existing variational auto-encoder (VAE) based network embedding algorithms. However, the variational posterior distribution in previous VAE based network embedding algorithms is often assumed and restricted to be a mean-field Gaussian distribution or other simple distribution families, which results in poor inference of the embeddings. To alleviate the above defect, we propose a novel VAE-based co-embedding method for attributed network, F-CAN, where posterior distributions are flexible, complex, and scalable distributions constructed through the normalizing flow. We evaluate our proposed models on a number of network tasks with several benchmark datasets. Experimental results demonstrate that there are clear improvements in the qualities of embeddings generated by our model to the state-of-the-art attributed network embedding methods.  © 2021 Copyright held by the owner/author(s).",Attributed network; embedding; normalizing flow; variational auto-encoder,Data mining; Inference engines; Network coding; Semantics; Attributed network; Auto encoders; Embedding algorithms; Embeddings; Low-dimensional representation; Network embedding; Normalizing flow; Posterior distributions; Semantic Space; Variational auto-encoder; Network embeddings
NTP-Miner: Nonoverlapping Three-Way Sequential Pattern Mining,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112860397&doi=10.1145%2f3480245&partnerID=40&md5=b1e82463cb3badeb7e4a33efe52b26ed,"Nonoverlapping sequential pattern mining is an important type of sequential pattern mining (SPM) with gap constraints, which not only can reveal interesting patterns to users but also can effectively reduce the search space using the Apriori (anti-monotonicity) property. However, the existing algorithms do not focus on attributes of interest to users, meaning that existing methods may discover many frequent patterns that are redundant. To solve this problem, this article proposes a task called nonoverlapping three-way sequential pattern (NTP) mining, where attributes are categorized according to three levels of interest: strong, medium, and weak interest. NTP mining can effectively avoid mining redundant patterns since the NTPs are composed of strong and medium interest items. Moreover, NTPs can avoid serious deviations (the occurrence is significantly different from its pattern) since gap constraints cannot match with strong interest patterns. To mine NTPs, an effective algorithm is put forward, called NTP-Miner, which applies two main steps: support (frequency occurrence) calculation and candidate pattern generation. To calculate the support of an NTP, depth-first and backtracking strategies are adopted, which do not require creating a whole Nettree structure, meaning that many redundant nodes and parent-child relationships do not need to be created. Hence, time and space efficiency is improved. To generate candidate patterns while reducing their number, NTP-Miner employs a pattern join strategy and only mines patterns of strong and medium interest. Experimental results on stock market and protein datasets show that NTP-Miner not only is more efficient than other competitive approaches but can also help users find more valuable patterns. More importantly, NTP mining has achieved better performance than other competitive methods in clustering tasks.  © 2021 Association for Computing Machinery.",Apriori property; frequent pattern; gap constraint; Sequential pattern mining; three-way decisions,Apriori; Apriori property; Candidate patterns; Frequent pattern; Gap constraint; Nonoverlapping; Property; Sequential patterns; Sequential-pattern mining; Three-way decision; Data mining
DACHA: A Dual Graph Convolution Based Temporal Knowledge Graph Representation Learning Method Using Historical Relation,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159185057&doi=10.1145%2f3477051&partnerID=40&md5=d46bdd61c735b52c4d427d6a96dda2c7,"Temporal knowledge graph (TKG) representation learning embeds relations and entities into a continuous low-dimensional vector space by incorporating temporal information. Latest studies mainly aim at learning entity representations by modeling entity interactions from the neighbor structure of the graph. However, the interactions of relations from the neighbor structure of the graph are neglected, which are also of significance for learning informative representations. In addition, there still lacks an effective historical relation encoder to model the multi-range temporal dependencies. In this article, we propose a dual graph convolution network based TKG representation learning method using historical relations (DACHA). Specifically, we first construct the primal graph according to historical relations, as well as the edge graph by regarding historical relations as nodes. Then, we employ the dual graph convolution network to capture the interactions of both entities and historical relations from the neighbor structure of the graph. In addition, the temporal self-attentive historical relation encoder is proposed to explicitly model both local and global temporal dependencies. Extensive experiments on two event based TKG datasets demonstrate that DACHA achieves the state-of-the-art results.  © 2021 Association for Computing Machinery.",dual graph convolution; representation learning; Temporal knowledge graph,Data mining; Knowledge graph; Learning systems; Signal encoding; Vector spaces; Dual graph convolution; Dual graphs; Graph representation; Knowledge graphs; Learning methods; Low dimensional; Neighbor structures; Representation learning; Temporal knowledge; Temporal knowledge graph; Convolution
Time-Aware Graph Embedding: A Temporal Smoothness and Task-Oriented Approach,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159330034&doi=10.1145%2f3480243&partnerID=40&md5=c767fbbfbcd718ceba692a8e00344478,"Knowledge graph embedding, which aims at learning the low-dimensional representations of entities and relationships, has attracted considerable research efforts recently. However, most knowledge graph embedding methods focus on the structural relationships in fixed triples while ignoring the temporal information. Currently, existing time-aware graph embedding methods only focus on the factual plausibility, while ignoring the temporal smoothness, which models the interactions between a fact and its contexts, and thus can capture fine-granularity temporal relationships. This leads to the limited performance of embedding related applications. To solve this problem, this article presents a Robustly Time-aware Graph Embedding (RTGE) method by incorporating temporal smoothness. Two major innovations of our article are presented here. At first, RTGE integrates a measure of temporal smoothness in the learning process of the time-aware graph embedding. Via the proposed additional smoothing factor, RTGE can preserve both structural information and evolutionary patterns of a given graph. Secondly, RTGE provides a general task-oriented negative sampling strategy associated with temporally aware information, which further improves the adaptive ability of the proposed algorithm and plays an essential role in obtaining superior performance in various tasks. Extensive experiments conducted on multiple benchmark tasks show that RTGE can increase performance in entity/relationship/temporal scoping prediction tasks.  © 2021 Association for Computing Machinery.",graph embedding; Knowledge graph; temporal smoothness,Benchmarking; Graph embeddings; Embedding method; Graph embeddings; Knowledge graphs; Low-dimensional representation; Performance; Research efforts; Structural relationship; Task oriented approach; Temporal information; Temporal smoothness; Knowledge graph
Corpus-level and Concept-based Explanations for Interpretable Document Classification,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159151745&doi=10.1145%2f3477539&partnerID=40&md5=9ed7bb7fa80e034c6febdd78e80d0dcf,"Using attention weights to identify information that is important for models' decision making is a popular approach to interpret attention-based neural networks. This is commonly realized in practice through the generation of a heat-map for every single document based on attention weights. However, this interpretation method is fragile and it is easy to find contradictory examples. In this article, we propose a corpus-level explanation approach, which aims at capturing causal relationships between keywords and model predictions via learning the importance of keywords for predicted labels across a training corpus based on attention weights. Based on this idea, we further propose a concept-based explanation method that can automatically learn higher level concepts and their importance to model prediction tasks. Our concept-based explanation method is built upon a novel Abstraction-Aggregation Network (AAN), which can automatically cluster important keywords during an end-to-end training process. We apply these methods to the document classification task and show that they are powerful in extracting semantically meaningful keywords and concepts. Our consistency analysis results based on an attention-based Naïve Bayes classifier (NBC) also demonstrate that these keywords and concepts are important for model predictions.  © 2021 Association for Computing Machinery.",Attention mechanism; concept-based explanation; document classification; model interpretation; sentiment classification,Barium compounds; Forecasting; Information retrieval systems; Attention mechanisms; Concept-based; Concept-based explanation; Document Classification; Heat maps; Model interpretations; Model prediction; Modeling decision makings; Neural-networks; Sentiment classification; Decision making
Context-Aware Semantic Annotation of Mobility Records,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159200720&doi=10.1145%2f3477048&partnerID=40&md5=56d312ea74ea4f7a9f4649e208ace030,"The wide adoption of mobile devices has provided us with a massive volume of human mobility records. However, a large portion of these records is unlabeled, i.e., only have GPS coordinates without semantic information (e.g., Point of Interest (POI)). To make those unlabeled records associate with more information for further applications, it is of great importance to annotate the original data with POIs information based on the external context. Nevertheless, semantic annotation of mobility records is challenging due to three aspects: the complex relationship among multiple domains of context, the sparsity of mobility records, and difficulties in balancing personal preference and crowd preference. To address these challenges, we propose CAP, a context-aware personalized semantic annotation model, where we use a Bayesian mixture model to model the complex relationship among five domains of context - location, time, POI category, personal preference, and crowd preference. We evaluate our model on two real-world datasets, and demonstrate that our proposed method significantly outperforms the state-of-the-art algorithms by over 11.8%.  © 2021 Association for Computing Machinery.",graphical model; Mobility trajectories; point of interest; semantic annotation,Complex relationships; Context-Aware; GPS Coordinates; GraphicaL model; Human mobility; Mobility trajectory; Personal preferences; Point of interest; Semantic annotations; Semantics Information; Semantics
Balance-Subsampled Stable Prediction Across Unknown Test Data,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124411190&doi=10.1145%2f3477052&partnerID=40&md5=dde433a91fc2a206b6810b2259b21c27,"In data mining and machine learning, it is commonly assumed that training and test data share the same population distribution. However, this assumption is often violated in practice because of the sample selection bias, which might induce the distribution shift from training data to test data. Such a model-agnostic distribution shift usually leads to prediction instability across unknown test data. This article proposes a novel balance-subsampled stable prediction (BSSP) algorithm based on the theory of fractional factorial design. It isolates the clear effect of each predictor from the confounding variables. A design-theoretic analysis shows that the proposed method can reduce the confounding effects among predictors induced by the distribution shift, improving both the accuracy of parameter estimation and the stability of prediction across unknown test data. Numerical experiments on synthetic and real-world datasets demonstrate that our BSSP algorithm can significantly outperform the baseline methods for stable prediction across unknown test data.  © 2021 Association for Computing Machinery.",distribution shift; fractional factorial design; Stable prediction; subsampling; variable deconfounding,Data mining; Numerical methods; Population statistics; Data share; Distribution shift; Fractional factorial designs; Machine-learning; Prediction algorithms; Stable prediction; Subsampling; Test data; Training data; Variable deconfounding; Forecasting
Knowledge Distillation with Attention for Deep Transfer Learning of Convolutional Networks,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132782601&doi=10.1145%2f3473912&partnerID=40&md5=130eade758d05d37ae8f93174e92458d,"Transfer learning through fine-tuning a pre-trained neural network with an extremely large dataset, such as ImageNet, can significantly improve and accelerate training while the accuracy is frequently bottlenecked by the limited dataset size of the new target task. To solve the problem, some regularization methods, constraining the outer layer weights of the target network using the starting point as references (SPAR), have been studied. In this article, we propose a novel regularized transfer learning framework DELTA, namely DEep Learning Transfer using Feature Map with Attention. Instead of constraining the weights of neural network, DELTA aims at preserving the outer layer outputs of the source network. Specifically, in addition to minimizing the empirical loss, DELTA aligns the outer layer outputs of two networks, through constraining a subset of feature maps that are precisely selected by attention that has been learned in a supervised learning manner. We evaluate DELTA with the state-of-the-art algorithms, including L2 and L2-SP. The experiment results show that our method outperforms these baselines with higher accuracy for new tasks. Code has been made publicly available.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",algorithms; framework; knowledge distillation; Transfer learning,Deep learning; Image enhancement; Large dataset; Multilayer neural networks; Transfer learning; Convolutional networks; Data set size; Feature map; Fine tuning; Framework; Knowledge distillation; Large datasets; Outer layer; Trained neural networks; Transfer learning; Distillation
Unsupervised Adversarial Network Alignment with Reinforcement Learning,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147735402&doi=10.1145%2f3477050&partnerID=40&md5=c89f12b813c49d6b041c3e0ec9979ae5,"Network alignment, which aims at learning a matching between the same entities across multiple information networks, often suffers challenges from feature inconsistency, high-dimensional features, to unstable alignment results. This article presents a novel network alignment framework, Unsupervised Adversarial learning based Network Alignment(UANA), that combines generative adversarial network (GAN) and reinforcement learning (RL) techniques to tackle the above critical challenges. First, we propose a bidirectional adversarial network distribution matching model to perform the bidirectional cross-network alignment translations between two networks, such that the distributions of real and translated networks completely overlap together. In addition, two cross-network alignment translation cycles are constructed for training the unsupervised alignment without the need of prior alignment knowledge. Second, in order to address the feature inconsistency issue, we integrate a dual adversarial autoencoder module with an adversarial binary classification model together to project two copies of the same vertices with high-dimensional inconsistent features into the same low-dimensional embedding space. This facilitates the translations of the distributions of two networks in the adversarial network distribution matching model. Finally, we develop an RL based optimization approach to solve the vertex matching problem in the discrete space of the GAN model, i.e., directly select the vertices in target networks most relevant to the vertices in source networks, without unstable similarity computation that is sensitive to discriminative features and similarity metrics. Extensive evaluation on real-world graph datasets demonstrates the outstanding capability of UANA to address the unsupervised network alignment problem, in terms of both effectiveness and scalability.  © 2021 Association for Computing Machinery.",adversarial classification; adversarial network distribution matching; feature inconsistency; high-dimensional features; reinforcement learning; Unsupervised network alignment,Alignment; Classification (of information); Generative adversarial networks; Information services; Adversarial classifications; Adversarial network distribution matching; Adversarial networks; Distribution matching; Feature inconsistency; Higher dimensional features; Network alignments; Network distributions; Reinforcement learnings; Unsupervised network; Unsupervised network alignment; Reinforcement learning
Online and Distributed Robust Regressions with Extremely Noisy Labels,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140789508&doi=10.1145%2f3473038&partnerID=40&md5=21e65712169fae2d694ccda62943800d,"In today's era of big data, robust least-squares regression becomes a more challenging problem when considering the extremely corrupted labels along with explosive growth of datasets. Traditional robust methods can handle the noise but suffer from several challenges when applied in huge dataset including (1) computational infeasibility of handling an entire dataset at once, (2) existence of heterogeneously distributed corruption, and (3) difficulty in corruption estimation when data cannot be entirely loaded. This article proposes online and distributed robust regression approaches, both of which can concurrently address all the above challenges. Specifically, the distributed algorithm optimizes the regression coefficients of each data block via heuristic hard thresholding and combines all the estimates in a distributed robust consolidation. In addition, an online version of the distributed algorithm is proposed to incrementally update the existing estimates with new incoming data. Furthermore, a novel online robust regression method is proposed to estimate under a biased-batch corruption. We also prove that our algorithms benefit from strong robustness guarantees in terms of regression coefficient recovery with a constant upper bound on the error of state-of-the-art batch methods. Extensive experiments on synthetic and real datasets demonstrate that our approaches are superior to those of existing methods in effectiveness, with competitive efficiency.  © 2021 Association for Computing Machinery.",distributed optimization; extremely noisy labels; online robust regression; Robust regression,Crime; Large dataset; Regression analysis; Distributed optimization; Explosive growth; Extremely noisy label; Least squares regression; Noisy labels; Online robust regression; Regression coefficient; Robust least squares; Robust regressions; Optimization
Network Embedding via Motifs,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136144210&doi=10.1145%2f3473911&partnerID=40&md5=abbb4a94ca51c9b1bee19a4e41f5b031,"Network embedding has emerged as an effective way to deal with downstream tasks, such as node classification [16, 31, 42]. Most existing methods leverage multi-similarities between nodes such as connectivity, which considers vertices that are closely connected to be similar and structural similarity, which is measured by assessing their relations to neighbors; while these methods only focus on static graphs. In this work, we bridge connectivity and structural similarity in a uniform representation via motifs, and consequently present an algorithm for Learning Embeddings by leveraging Motifs Of Networks (LEMON), which aims to learn embeddings for vertices and various motifs. Moreover, LEMON is inherently capable of dealing with inductive learning tasks for dynamic graphs. To validate the effectiveness and efficiency, we conduct various experiments on two real-world datasets and five public datasets from diverse domains. Through comparison with state-of-the-art baseline models, we find that LEMON achieves significant improvements in downstream tasks.  © 2021 Association for Computing Machinery.",Motif; motif embedding; motif super-vertex; network embedding,Citrus fruits; Data mining; Down-stream; Embeddings; Inductive learning; Learn+; Motif; Motif embedding; Motif super-vertex; Multi-similarity; Network embedding; Structural similarity; Network embeddings
MULFE: Multi-Label Learning via Label-Specific Feature Space Ensemble,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111153748&doi=10.1145%2f3451392&partnerID=40&md5=d1bb4b25040614b876390fe86eb77e63,"In multi-label learning, label correlations commonly exist in the data. Such correlation not only provides useful information, but also imposes significant challenges for multi-label learning. Recently, label-specific feature embedding has been proposed to explore label-specific features from the training data, and uses feature highly customized to the multi-label set for learning. While such feature embedding methods have demonstrated good performance, the creation of the feature embedding space is only based on a single label, without considering label correlations in the data. In this article, we propose to combine multiple label-specific feature spaces, using label correlation, for multi-label learning. The proposed algorithm, multi-label-specific feature space ensemble (MULFE), takes consideration label-specific features, label correlation, and weighted ensemble principle to form a learning framework. By conducting clustering analysis on each label's negative and positive instances, MULFE first creates features customized to each label. After that, MULFE utilizes the label correlation to optimize the margin distribution of the base classifiers which are induced by the related label-specific feature spaces. By combining multiple label-specific features, label correlation based weighting, and ensemble learning, MULFE achieves maximum margin multi-label classification goal through the underlying optimization framework. Empirical studies on 10 public data sets manifest the effectiveness of MULFE. © 2021 Association for Computing Machinery.",ensemble; label correlation; label-specific features; Multi-label learning,Classification (of information); Embeddings; Clustering analysis; Label correlations; Learning frameworks; Margin distributions; Multi label classification; Multi-label learning; Optimization framework; Positive instances; Learning systems
Generic Multi-label Annotation via Adaptive Graph and Marginalized Augmentation,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111155810&doi=10.1145%2f3451884&partnerID=40&md5=19c932f44859f2b887187761cbcb1a97,"Multi-label learning recovers multiple labels from a single instance. It is a more challenging task compared with single-label manner. Most multi-label learning approaches need large-scale well-labeled samples to achieve high accurate performance. However, it is expensive to build such a dataset. In this work, we propose a generic multi-label learning framework based on Adaptive Graph and Marginalized Augmentation (AGMA) in a semi-supervised scenario. Generally speaking, AGMA makes use of a small amount of labeled data associated with a lot of unlabeled data to boost the learning performance. First, an adaptive similarity graph is learned to effectively capture the intrinsic structure within the data. Second, marginalized augmentation strategy is explored to enhance the model generalization and robustness. Third, a feature-label autoencoder is further deployed to improve inferring efficiency. All the modules are jointly trained to benefit each other. State-of-The-Art benchmarks in both traditional and zero-shot multi-label learning scenarios are evaluated. Experiments and ablation studies illustrate the accuracy and efficiency of our AGMA method. © 2021 Association for Computing Machinery.",adaptive graph; image retrieval; marginalized augmentation; multi-label annotation; Multi-label learning,Efficiency; Semi-supervised learning; Accurate performance; Adaptive similarities; Intrinsic structures; Learning performance; Model generalization; Multi-label annotation; Multi-label learning; State of the art; Learning systems
BiLabel-Specific Features for Multi-Label Classification,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111142793&doi=10.1145%2f3458283&partnerID=40&md5=0b9d4c22142e791bcc207d159c40fad3,"In multi-label classification, the task is to induce predictive models which can assign a set of relevant labels for the unseen instance. The strategy of label-specific features has been widely employed in learning from multi-label examples, where the classification model for predicting the relevancy of each class label is induced based on its tailored features rather than the original features. Existing approaches work by generating a group of tailored features for each class label independently, where label correlations are not fully considered in the label-specific features generation process. In this article, we extend existing strategy by proposing a simple yet effective approach based on BiLabel-specific features. Specifically, a group of tailored features is generated for a pair of class labels with heuristic prototype selection and embedding. Thereafter, predictions of classifiers induced by BiLabel-specific features are ensembled to determine the relevancy of each class label for unseen instance. To thoroughly evaluate the BiLabel-specific features strategy, extensive experiments are conducted over a total of 35 benchmark datasets. Comparative studies against state-of-The-Art label-specific features techniques clearly validate the superiority of utilizing BiLabel-specific features to yield stronger generalization performance for multi-label classification. © 2021 Association for Computing Machinery.",label correlations; label-specific features; Multi-label classification; pairwise comparison,Predictive analytics; Benchmark datasets; Classification models; Comparative studies; Effective approaches; Generalization performance; Multi label classification; Multi-label examples; Prototype selection; Classification (of information)
Lost in Transduction: Transductive Transfer Learning in Text Classification,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111136281&doi=10.1145%2f3453146&partnerID=40&md5=5231bce93383c3708b0b6a9e3733fbd5,"Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning(a.k.a. ""Inductive Transfer"") tries to alleviate these costs by transferring, to the ""target""domain of interest, knowledge available from a different ""source""domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik's original definition of ""transduction"", current terminology in the field is confused. In this article, we discuss how the term ""transduction""has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-The-Art algorithms for transfer learning in text classification. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",distributional hypothesis; induction; text classification; Transduction; transfer learning,Bacteriophages; Clarifiers; Classification (of information); Learning systems; Terminology; Text processing; Experimental comparison; Inductive transfer; New applications; State-of-the-art algorithms; Target domain; Text classification; Training time; Transfer learning methods; Transfer learning
Explainable Artificial Intelligence-Based Competitive Factor Identification,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111172315&doi=10.1145%2f3451529&partnerID=40&md5=d4f64fa1eda18a8c0a95a4c79640468c,"Competitor analysis is an essential component of corporate strategy, providing both offensive and defensive strategic contexts to identify opportunities and threats. The rapid development of social media has recently led to several methodologies and frameworks facilitating competitor analysis through online reviews. Existing studies only focused on detecting comparative sentences in review comments or utilized low-performance models. However, this study proposes a novel approach to identifying the competitive factors using a recent explainable artificial intelligence approach at the comprehensive product feature level. We establish a model to classify the review comments for each corresponding product and evaluate the relevance of each keyword in such comments during the classification process. We then extract and prioritize the keywords and determine their competitiveness based on relevance. Our experiment results show that the proposed method can effectively extract the competitive factors both qualitatively and quantitatively. © 2021 Association for Computing Machinery.",competitive factors; competitor analysis; LRP; mobile; XAI,Computer science; Data mining; Classification process; Competitive factor; Competitor analysis; Corporate strategies; Online reviews; Performance Model; Product feature; Social media; Artificial intelligence
Modeling Temporal Patterns with Dilated Convolutions for Time-Series Forecasting,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111125760&doi=10.1145%2f3453724&partnerID=40&md5=d9c2c14b5a042ee0bd0ae58d0367abf8,"Time-series forecasting is an important problem across a wide range of domains. Designing accurate and prompt forecasting algorithms is a non-Trivial task, as temporal data that arise in real applications often involve both non-linear dynamics and linear dependencies, and always have some mixtures of sequential and periodic patterns, such as daily, weekly repetitions, and so on. At this point, however, most recent deep models often use Recurrent Neural Networks (RNNs) to capture these temporal patterns, which is hard to parallelize and not fast enough for real-world applications especially when a huge amount of user requests are coming. Recently, CNNs have demonstrated significant advantages for sequence modeling tasks over the de-facto RNNs, while providing high computational efficiency due to the inherent parallelism. In this work, we propose HyDCNN, a novel hybrid framework based on fully Dilated CNN for time-series forecasting tasks. The core component in HyDCNN is a proposed hybrid module, in which our proposed position-Aware dilated CNNs are utilized to capture the sequential non-linear dynamics and an autoregressive model is leveraged to capture the sequential linear dependencies. To further capture the periodic temporal patterns, a novel hop scheme is introduced in the hybrid module. HyDCNN is then composed of multiple hybrid modules to capture the sequential and periodic patterns. Each of these hybrid modules targets on either the sequential pattern or one kind of periodic patterns. Extensive experiments on five real-world datasets have shown that the proposed HyDCNN is better compared with state-of-The-Art baselines and is at least 200% better than RNN baselines. The datasets and source code will be published in Github to facilitate more future work. © 2021 Association for Computing Machinery.",Convolutional neural networks; dilated convolutions; time-series forecasting,Computational efficiency; Forecasting; Time series; Auto regressive models; Forecasting algorithm; Inherent parallelism; Non-linear dynamics; Real-world datasets; Recurrent neural network (RNNs); Sequential patterns; Time series forecasting; Recurrent neural networks
Bayesian Additive Matrix Approximation for Social Recommendation,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111138570&doi=10.1145%2f3451391&partnerID=40&md5=1861ca59f8e0d78a43860e8d11edf674,"Social relations between users have been proven to be a good type of auxiliary information to improve the recommendation performance. However, it is a challenging issue to sufficiently exploit the social relations and correctly determine the user preference from both social and rating information. In this article, we propose a unified Bayesian Additive Matrix Approximation model (BAMA), which takes advantage of rating preference and social network to provide high-quality recommendation. The basic idea of BAMA is to extract social influence from social networks, integrate them to Bayesian additive co-clustering for effectively determining the user clusters and item clusters, and provide an accurate rating prediction. In addition, an efficient algorithm with collapsed Gibbs Sampling is designed to inference the proposed model. A series of experiments were conducted on six real-world social datasets. The results demonstrate the superiority of the proposed BAMA by comparing with the state-of-The-Art methods from three views, all users, cold-start users, and users with few social relations. With the aid of social information, furthermore, BAMA has ability to provide the explainable recommendation. © 2021 Association for Computing Machinery.",additive co-clustering; probabilistic graphical model; Recommendation system; social network,Additives; Inference engines; Additive matrix; Auxiliary information; Rating information; Recommendation performance; Social influence; Social information; Social relations; State-of-the-art methods; Bayesian networks
DiMBERT: Learning Vision-Language Grounded Representations with Disentangled Multimodal-Attention,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111120117&doi=10.1145%2f3447685&partnerID=40&md5=4f95d305ca60a344a3424ca12580a515,"Vision-And-language (V-L) tasks require the system to understand both vision content and natural language, thus learning fine-grained joint representations of vision and language (a.k.a. V-L representations) is of paramount importance. Recently, various pre-Trained V-L models are proposed to learn V-L representations and achieve improved results in many tasks. However, the mainstream models process both vision and language inputs with the same set of attention matrices. As a result, the generated V-L representations are entangled in one common latent space. To tackle this problem, we propose DiMBERT (short for Disentangled Multimodal-Attention BERT), which is a novel framework that applies separated attention spaces for vision and language, and the representations of multi-modalities can thus be disentangled explicitly. To enhance the correlation between vision and language in disentangled spaces, we introduce the visual concepts to DiMBERT which represent visual information in textual format. In this manner, visual concepts help to bridge the gap between the two modalities. We pre-Train DiMBERT on a large amount of image-sentence pairs on two tasks: bidirectional language modeling and sequence-To-sequence language modeling. After pre-Train, DiMBERT is further fine-Tuned for the downstream tasks. Experiments show that DiMBERT sets new state-of-The-Art performance on three tasks (over four datasets), including both generation tasks (image captioning and visual storytelling) and classification tasks (referring expressions). The proposed DiM (short for Disentangled Multimodal-Attention) module can be easily incorporated into existing pre-Trained V-L models to boost their performance, up to a 5% increase on the representative task. Finally, we conduct a systematic analysis and demonstrate the effectiveness of our DiM and the introduced visual concepts. © 2021 Association for Computing Machinery.",disentangled attention; pre-Training; Vision-And-language tasks; vision-language representations; visual concepts,Classification (of information); Computational linguistics; Modeling languages; Natural language processing systems; Classification tasks; Image captioning; Natural languages; Referring expressions; State-of-the-art performance; Systematic analysis; Visual information; Visual storytellings; Visual languages
Distributed Latent Dirichlet Allocation on Streams,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111116034&doi=10.1145%2f3451528&partnerID=40&md5=74808d67199bb17b0431e68efec70ea6,"Latent Dirichlet Allocation (LDA) has been widely used for topic modeling, with applications spanning various areas such as natural language processing and information retrieval. While LDA on small and static datasets has been extensively studied, several real-world challenges are posed in practical scenarios where datasets are often huge and are gathered in a streaming fashion. As the state-of-The-Art LDA algorithm on streams, Streaming Variational Bayes (SVB) introduced Bayesian updating to provide a streaming procedure. However, the utility of SVB is limited in applications since it ignored three challenges of processing real-world streams: Topic evolution, data turbulence, and real-Time inference. In this article, we propose a novel distributed LDA algorithm-referred to as StreamFed-LDA-to deal with challenges on streams. For topic modeling of streaming data, the ability to capture evolving topics is essential for practical online inference. To achieve this goal, StreamFed-LDA is based on a specialized framework that supports lifelong (continual) learning of evolving topics. On the other hand, data turbulence is commonly present in streams due to real-life events. In that case, the design of StreamFed-LDA allows the model to learn new characteristics from the most recent data while maintaining the historical information. On massive streaming data, it is difficult and crucial to provide real-Time inference results. To increase the throughput and reduce the latency, StreamFed-LDA introduces additional techniques that substantially reduce both computation and communication costs in distributed systems. Experiments on four real-world datasets show that the proposed framework achieves significantly better performance of online inference compared with the baselines. At the same time, StreamFed-LDA also reduces the latency by orders of magnitudes in real-world datasets. © 2021 Association for Computing Machinery.",Distributed streams; learning system; variational inference,Distributed computer systems; Distributed database systems; Modeling languages; Natural language processing systems; Statistics; Turbulence; Distributed systems; Historical information; Latent Dirichlet allocation; Latent dirichlet allocations; NAtural language processing; Orders of magnitude; Real-time inference; Real-world datasets; Data streams
New Multi-View Classification Method with Uncertain Data,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111094910&doi=10.1145%2f3458282&partnerID=40&md5=78309915694158e5310e005b07a15f48,"Multi-view classification aims at designing a multi-view learning strategy to train a classifier from multi-view data, which are easily collected in practice. Most of the existing works focus on multi-view classification by assuming the multi-view data are collected with precise information. However, we always collect the uncertain multi-view data due to the collection process is corrupted with noise in real-life application. In this case, this article proposes a novel approach, called uncertain multi-view learning with support vector machine (UMV-SVM) to cope with the problem of multi-view learning with uncertain data. The method first enforces the agreement among all the views to seek complementary information of multi-view data and takes the uncertainty of the multi-view data into consideration by modeling reachability area of the noise. Then it proposes an iterative framework to solve the proposed UMV-SVM model such that we can obtain the multi-view classifier for prediction. Extensive experiments on real-life datasets have shown that the proposed UMV-SVM can achieve a better performance for uncertain multi-view classification in comparison to the state-of-The-Art multi-view classification methods. © 2021 Association for Computing Machinery.",Multi-view classification; uncertain data,Iterative methods; Learning systems; Support vector machines; Uncertainty analysis; Classification methods; Collection process; Iterative framework; Multi-view datum; Multi-view learning; Real life datasets; Real-life applications; State of the art; Classification (of information)
Jointly Modeling Heterogeneous Student Behaviors and Interactions among Multiple Prediction Tasks,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111081455&doi=10.1145%2f3458023&partnerID=40&md5=ba61a02c7aba2ad332f9ece6f9637302,"Prediction tasks about students have practical significance for both student and college. Making multiple predictions about students is an important part of a smart campus. For instance, predicting whether a student will fail to graduate can alert the student affairs office to take predictive measures to help the student improve his/her academic performance. With the development of information technology in colleges, we can collect digital footprints that encode heterogeneous behaviors continuously. In this article, we focus on modeling heterogeneous behaviors and making multiple predictions together, since some prediction tasks are related and learning the model for a specific task may have the data sparsity problem. To this end, we propose a variant of Long-Short Term Memory (LSTM) and a soft-Attention mechanism. The proposed LSTM is able to learn the student profile-Aware representation from heterogeneous behavior sequences. The proposed soft-Attention mechanism can dynamically learn different importance degrees of different days for every student. In this way, heterogeneous behaviors can be well modeled. In order to model interactions among multiple prediction tasks, we propose a co-Attention mechanism based unit. With the help of the stacked units, we can explicitly control the knowledge transfer among multiple tasks. We design three motivating behavior prediction tasks based on a real-world dataset collected from a college. Qualitative and quantitative experiments on the three prediction tasks have demonstrated the effectiveness of our model. © 2021 Association for Computing Machinery.",attention mechanism; heterogeneous student behaviors; LSTM; multi-Task learning,Forecasting; Knowledge management; Long short-term memory; Motivation; Academic performance; Attention mechanisms; Behavior sequences; Data sparsity problems; Importance degrees; Knowledge transfer; Motivating behavior; Quantitative experiments; Students
Unsupervised Subspace Extraction via Deep Kernelized Clustering,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111097990&doi=10.1145%2f3459082&partnerID=40&md5=d2cf36e01d276ed0f9fec618e70d34d2,"Feature extraction has been widely studied to find informative latent features and reduce the dimensionality of data. In particular, due to the difficulty in obtaining labeled data, unsupervised feature extraction has received much attention in data mining. However, widely used unsupervised feature extraction methods require side information about data or rigid assumptions on the latent feature space. Furthermore, most feature extraction methods require predefined dimensionality of the latent feature space,which should be manually tuned as a hyperparameter. In this article, we propose a new unsupervised feature extraction method called Unsupervised Subspace Extractor (USE), which does not require any side information and rigid assumptions on data. Furthermore, USE can find a subspace generated by a nonlinear combination of the input feature and automatically determine the optimal dimensionality of the subspace for the given nonlinear combination. The feature extraction process of USE is well justified mathematically, and we also empirically demonstrate the effectiveness of USE for several benchmark datasets. © 2021 Association for Computing Machinery.",eigenvalue-based optimization; Feature extraction; unsupervised learning,Extraction; Feature extraction; Benchmark datasets; Feature extraction methods; Feature space; Hyper-parameter; Input features; Nonlinear combination; Optimal dimensionality; Side information; Data mining
CrowdTC: Crowd-powered Learning for Text Classification,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111149285&doi=10.1145%2f3457216&partnerID=40&md5=5c55a72f0afe2c8c2171bb11923967c3,"Text classification is a fundamental task in content analysis. Nowadays, deep learning has demonstrated promising performance in text classification compared with shallow models. However, almost all the existing models do not take advantage of the wisdom of human beings to help text classification. Human beings are more intelligent and capable than machine learning models in terms of understanding and capturing the implicit semantic information from text. In this article, we try to take guidance from human beings to classify text. We propose Crowd-powered learning for Text Classification (CrowdTC for short). We design and post the questions on a crowdsourcing platform to extract keywords in text. Sampling and clustering techniques are utilized to reduce the cost of crowdsourcing. Also, we present an attention-based neural network and a hybrid neural network to incorporate the extracted keywords as human guidance into deep neural networks. Extensive experiments on public datasets confirm that CrowdTC improves the text classification accuracy of neural networks by using the crowd-powered keyword guidance. © 2021 Association for Computing Machinery.",crowdsourcing; keyword extraction; neural networks; Text classification,Classification (of information); Crowdsourcing; Deep learning; Deep neural networks; Learning systems; Neural networks; Semantics; Clustering techniques; Content analysis; Crowdsourcing platforms; Human guidance; Hybrid neural networks; Implicit semantics; Machine learning models; Text classification; Text processing
A Joint Passenger Flow Inference and Path Recommender System for Deploying New Routes and Stations of Mass Transit Transportation,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111166057&doi=10.1145%2f3451393&partnerID=40&md5=c0470aef7fbd9c3e8c2605cba31c51eb,"In this work, a novel decision assistant system for urban transportation, called Route Scheme Assistant (RSA), is proposed to address two crucial issues that few former researches have focused on: route-based passenger flow (PF) inference and multivariant high-PF route recommendation. First, RSA can estimate the PF of arbitrary user-designated routes effectively by utilizing Deep Neural Network (DNN) for regression based on geographical information and spatial-Temporal urban informatics. Second, our proposed Bidirectional Prioritized Spanning Tree (BDPST) intelligently combines the parallel computing concept and Gaussian mixture model (GMM) for route recommendation under users' constraints running in a timely manner. We did experiments on bus-Ticket data of Tainan and Chicago and the experimental results show that the PF inference model outperforms baseline and comparative methods from 41% to 57%. Moreover, the proposed BDPST algorithm's performance is not far away from the optimal PF and outperforms other comparative methods from 39% to 71% in large-scale route recommendations. © 2021 Association for Computing Machinery.",bidirectional spanning tree; interactive route planning; passenger volume estimation; Route scheme; urban planning,Deep neural networks; Gaussian distribution; Mass transportation; Urban transportation; Algorithm's performance; Comparative methods; Gaussian Mixture Model; Geographical information; Inference models; Spatial temporals; Urban Informatics; Users' -constraints; Transportation routes
A Latent Variable Augmentation Method for Image Categorization with Insufficient Training Samples,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111167671&doi=10.1145%2f3451165&partnerID=40&md5=27ec2ae03c56a5ffb951c5093bebb5bd,"Over the past few years, we have made great progress in image categorization based on convolutional neural networks (CNNs). These CNNs are always trained based on a large-scale image data set; however, people may only have limited training samples for training CNN in the real-world applications. To solve this problem, one intuition is augmenting training samples. In this article, we propose an algorithm called Lavagan (Latent Variables Augmentation Method based on Generative Adversarial Nets) to improve the performance of CNN with insufficient training samples. The proposed Lavagan method is mainly composed of two tasks. The first task is that we augment a number latent variables (LVs) from a set of adaptive and constrained LVs distributions. In the second task, we take the augmented LVs into the training procedure of the image classifier. By taking these two tasks into account, we propose a uniform objective function to incorporate the two tasks into the learning. We then put forward an alternative two-play minimization game to minimize this uniform loss function such that we can obtain the predictive classifier. Moreover, based on Hoeffding's Inequality and Chernoff Bounding method, we analyze the feasibility and efficiency of the proposed Lavagan method, which manifests that the LV augmentation method is able to improve the performance of Lavagan with insufficient training samples. Finally, the experiment has shown that the proposed Lavagan method is able to deliver more accurate performance than the existing state-of-The-Art methods. © 2021 Association for Computing Machinery.",CNN; image recognition,Convolutional neural networks; Imaging systems; Accurate performance; Augmentation methods; Hoeffding's inequality; Image Categorization; Image Classifiers; Objective functions; State-of-the-art methods; Training procedures; Sampling
Cross-domain Recommendation with Bridge-Item Embeddings,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111083215&doi=10.1145%2f3447683&partnerID=40&md5=9f83f888415df15fb60799f4842bedab,"Web systems that provide the same functionality usually share a certain amount of items. This makes it possible to combine data from different websites to improve recommendation quality, known as the cross-domain recommendation task. Despite many research efforts on this task, the main drawback is that they largely assume the data of different systems can be fully shared. Such an assumption is unrealistic different systems are typically operated by different companies, and it may violate business privacy policy to directly share user behavior data since it is highly sensitive. In this work, we consider a more practical scenario to perform cross-domain recommendation. To avoid the leak of user privacy during the data sharing process, we consider sharing only the information of the item side, rather than user behavior data. Specifically, we transfer the item embeddings across domains, making it easier for two companies to reach a consensus (e.g., legal policy) on data sharing since the data to be shared is user-irrelevant and has no explicit semantics. To distill useful signals from transferred item embeddings, we rely on the strong representation power of neural networks and develop a new method named as NATR (short for Neural Attentive Transfer Recommendation). We perform extensive experiments on two real-world datasets, demonstrating that NATR achieves similar or even better performance than traditional cross-domain recommendation methods that directly share user-relevant data. Further insights are provided on the efficacy of NATR in using the transferred item embeddings to alleviate the data sparsity issue. © 2021 Association for Computing Machinery.",Cross-domain recommendation; privacy protection; transfer learning,Behavioral research; Data privacy; Embeddings; Semantics; Websites; Cross-domain recommendations; Data sparsity; Explicit semantics; Privacy policies; Real-world datasets; Representation power; Research efforts; User behaviors; Data Sharing
Graph-Based Stock Recommendation by Time-Aware Relational Attention Network,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111139973&doi=10.1145%2f3451397&partnerID=40&md5=cbfd4253e0060b233e619029463f7c91,"The stock market investors aim at maximizing their investment returns. Stock recommendation task is to recommend stocks with higher return ratios for the investors. Most stock prediction methods study the historical sequence patterns to predict stock trend or price in the near future. In fact, the future price of a stock is correlated not only with its historical price, but also with other stocks. In this article, we take into account the relationships between stocks (corporations) by stock relation graph. Furthermore, we propose a Time-Aware Relational Attention Network (TRAN) for graph-based stock recommendation according to return ratio ranking. In TRAN, the time-Aware relational attention mechanism is designed to capture time-varying correlation strengths between stocks by the interaction of historical sequences and stock description documents. With the dynamic strengths, the nodes of the stock relation graph aggregate the features of neighbor stock nodes by graph convolution operation. For a given group of stocks, the proposed TRAN model can output the ranking results of stocks according to their return ratios. The experimental results on several real-world datasets demonstrate the effectiveness of our TRAN for stock recommendation. © 2021 Association for Computing Machinery.",knowledge discovery; relational attention network; stock recommendation; Stock relation graph; time-Aware,Investments; Attention mechanisms; Capture time; Dynamic strength; Investment returns; Real-world datasets; Return ratio; Sequence patterns; Stock predictions; Graphic methods
MCS+: An Efficient Algorithm for Crawling the Community Structure in Multiplex Networks,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111129015&doi=10.1145%2f3451527&partnerID=40&md5=5f887a5bd85bbf69c352f253ffee0b58,"In this article, we consider the problem of crawling a multiplex network to identify the community structure of a layer-of-interest. A multiplex network is one where there are multiple types of relationships between the nodes. In many multiplex networks, some layers might be easier to explore (in terms of time, money etc.). We propose MCS+, an algorithm that can use the information from the easier to explore layers to help in the exploration of a layer-of-interest that is expensive to explore. We consider the goal of exploration to be generating a sample that is representative of the communities in the complete layer-of-interest. This work has practical applications in areas such as exploration of dark (e.g., criminal) networks, online social networks, biological networks, and so on. For example, in a terrorist network, relationships such as phone records, e-mail records, and so on are easier to collect; in contrast, data on the face-To-face communications are much harder to collect, but also potentially more valuable. We perform extensive experimental evaluations on real-world networks, and we observe that MCS+ consistently outperforms the best baseline-the similarity of the sample that MCS+ generates to the real network is up to three times that of the best baseline in some networks. We also perform theoretical and experimental evaluations on the scalability of MCS+ to network properties, and find that it scales well with the budget, number of layers in the multiplex network, and the average degree in the original network. © 2021 Association for Computing Machinery.",community detection; multi-Armed bandit; Multplex networks; network crawling,Budget control; Social networking (online); Biological networks; Community structures; Experimental evaluation; Face-to-face communications; Multiplex networks; On-line social networks; Real-world networks; Terrorist networks; Network layers
MSIPA: Multi-Scale Interval Pattern-Aware Network for ICU Transfer Prediction,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111168496&doi=10.1145%2f3458284&partnerID=40&md5=6f83d0754be368697289639223ab8dc9,"Accurate prediction of patients' ICU transfer events is of great significance for improving ICU treatment efficiency. ICU transition prediction task based on Electronic Health Records (EHR) is a temporal mining task like many other health informatics mining tasks. In the EHR-based temporal mining task, existing approaches are usually unable to mine and exploit patterns used to improve model performance. This article proposes a network based on Interval Pattern-Aware, Multi-Scale Interval Pattern-Aware (MSIPA) network. MSIPA mines different interval patterns in temporal EHR data according to the short, medium, and long intervals. MSIPA utilizes the Scaled Dot-Product Attention mechanism to query the contexts corresponding to the three scale patterns. Furthermore, Transformer will use all three types of contextual information simultaneously for ICU transfer prediction. Extensive experiments on real-world data demonstrate that an MSIPA network outperforms state-of-The-Art methods. © 2021 Association for Computing Machinery.",attention; EHR; ICU transfer prediction; interval pattern-Aware,Medical informatics; Patient treatment; Accurate prediction; Attention mechanisms; Contextual information; Electronic health record; Model performance; State-of-the-art methods; Transition prediction; Treatment efficiency; Forecasting
Recurrent Coupled Topic Modeling over Sequential Documents,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111140551&doi=10.1145%2f3451530&partnerID=40&md5=2da768ad639271042ad7201c24be2fab,"The abundant sequential documents such as online archival, social media, and news feeds are streamingly updated, where each chunk of documents is incorporated with smoothly evolving yet dependent topics. Such digital texts have attracted extensive research on dynamic topic modeling to infer hidden evolving topics and their temporal dependencies. However, most of the existing approaches focus on single-Topic-Thread evolution and ignore the fact that a current topic may be coupled with multiple relevant prior topics. In addition, these approaches also incur the intractable inference problem when inferring latent parameters, resulting in a high computational cost and performance degradation. In this work, we assume that a current topic evolves from all prior topics with corresponding coupling weights, forming the multi-Topic-Thread evolution. Our method models the dependencies between evolving topics and thoroughly encodes their complex multi-couplings across time steps. To conquer the intractable inference challenge, a new solution with a set of novel data augmentation techniques is proposed, which successfully discomposes the multi-couplings between evolving topics. A fully conjugate model is thus obtained to guarantee the effectiveness and efficiency of the inference technique. A novel Gibbs sampler with a backward-forward filter algorithm efficiently learns latent time-evolving parameters in a closed-form. In addition, the latent Indian Buffet Process compound distribution is exploited to automatically infer the overall topic number and customize the sparse topic proportions for each sequential document without bias. The proposed method is evaluated on both synthetic and real-world datasets against the competitive baselines, demonstrating its superiority over the baselines in terms of the low per-word perplexity, high coherent topics, and better document time prediction. © 2021 Association for Computing Machinery.",bayesian network; data augmentation; dropout; gibbs sampling; multiple dependency; topic coupling; topic evolution; Topic modeling,Computer science; Data mining; Compound distributions; Computational costs; Data augmentation; Effectiveness and efficiencies; Inference problem; Inference techniques; Performance degradation; Real-world datasets; Couplings
Adaptive Influence Maximization,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108965690&doi=10.1145%2f3447396&partnerID=40&md5=476b85e0b6d37b1cdc3b5f6e4722cf1e,"Influence maximization problem attempts to find a small subset of nodes that makes the expected influence spread maximized, which has been researched intensively before. They all assumed that each user in the seed set we select is activated successfully and then spread the influence. However, in the real scenario, not all users in the seed set are willing to be an influencer. Based on that, we consider each user associated with a probability with which we can activate her as a seed, and we can attempt to activate her many times. In this article, we study the adaptive influence maximization with multiple activations (Adaptive-IMMA) problem, where we select a node in each iteration, observe whether she accepts to be a seed, if yes, wait to observe the influence diffusion process; if no, we can attempt to activate her again with a higher cost or select another node as a seed. We model the multiple activations mathematically and define it on the domain of integer lattice. We propose a new concept, adaptive dr-submodularity, and show our Adaptive-IMMA is the problem that maximizing an adaptive monotone and dr-submodular function under the expected knapsack constraint. Adaptive dr-submodular maximization problem is never covered by any existing studies. Thus, we summarize its properties and study its approximability comprehensively, which is a non-trivial generalization of existing analysis about adaptive submodularity. Besides, to overcome the difficulty to estimate the expected influence spread, we combine our adaptive greedy policy with sampling techniques without losing the approximation ratio but reducing the time complexity. Finally, we conduct experiments on several real datasets to evaluate the effectiveness and efficiency of our proposed policies. © 2021 ACM.",adaptive dr-submodularity; Adaptive influence maximization; approximation algorithm; integer lattice; sampling techniques; social networks,Chemical activation; Combinatorial optimization; Approximation ratios; Diffusion process; Effectiveness and efficiencies; Influence maximizations; Knapsack constraints; Maximization problem; Sampling technique; Submodular functions; Iterative methods
A Survey on Causal Inference,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108966781&doi=10.1145%2f3444944&partnerID=40&md5=7c7a28bf5822bb82439c0ae62d131608,"Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public policy, and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the potential outcome framework, one of the well-known causal inference frameworks. The methods are divided into two categories depending on whether they require all three assumptions of the potential outcome framework or not. For each category, both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared. The plausible applications of these methods are also presented, including the applications in advertising, recommendation, medicine, and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized, which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods. © 2021 ACM.",Treatment effect estimation; Representation learning,Budget control; Economics; Education computing; Machine learning; Benchmark datasets; Causal inferences; Critical researches; Estimation methods; Observational data; Open-source code; Potential outcomes; Randomized controlled trial; Surveys
Improved Customer Lifetime Value Prediction with Sequence-To-Sequence Learning and Feature-Based Models,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108972145&doi=10.1145%2f3441444&partnerID=40&md5=90419d7986facc6dc665d0627b23ce83,"The prediction of the Customer Lifetime Value (CLV) is an important asset for tool-supported marketing by customer relationship managers. Since standard methods based on purchase recency, frequency, and past profit and revenue statistics often have limited predictive power, advanced machine learning (ML) techniques were applied to this task in recent years. However, existing approaches are often not fully capable of modeling certain temporal patterns that can be commonly found in practice, such as periodic purchasing behavior of customers. To address these shortcomings, we propose a novel method for CLV prediction based on a combination of several ML techniques. At its core, our method consists of a tailored deep learning approach based on encoder-decoder sequence-to-sequence recurrent neural networks with augmented temporal convolutions. This model is then combined with gradient boosting machines (GBMs) and a set of novel features in a hybrid framework. Empirical evaluations based on real-world data from a larger e-commerce company and a public dataset from the domain of online retail show that already the sequence-based model leads to competitive performance results. Stacking it with the GBM model is synergistic and further improves accuracy, indicating that the two models capture different patterns in the data. © 2021 ACM.",Customer lifetime value; machine learning; neural networks,Forecasting; Public relations; Recurrent neural networks; Sales; Value engineering; Competitive performance; Customer lifetime value; Customer relationships; Empirical evaluations; Feature-based model; Gradient boosting; Learning approach; Purchasing behaviors; Learning systems
Exploring BCI Control in Smart Environments: Intention Recognition Via EEG Representation Enhancement Learning,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108958418&doi=10.1145%2f3450449&partnerID=40&md5=93ecc6552ae11a5d2c9026aec2c8dc80,"The brain-computer interface (BCI) control technology that utilizes motor imagery to perform the desired action instead of manual operation will be widely used in smart environments. However, most of the research lacks robust feature representation of multi-channel EEG series, resulting in low intention recognition accuracy. This article proposes an EEG2Image based Denoised-ConvNets (called EID) to enhance feature representation of the intention recognition task. Specifically, we perform signal decomposition, slicing, and image mapping to decrease the noise from the irrelevant frequency bands. After that, we construct the Denoised-ConvNets structure to learn the colorspace and spatial variations of image objects without cropping new training images precisely. Toward further utilizing the color and spatial transformation layers, the colorspace and colored area of image objects have been enhanced and enlarged, respectively. In the multi-classification scenario, extensive experiments on publicly available EEG datasets confirm that the proposed method has better performance than state-of-the-art methods. © 2021 ACM.",brain-computer interface (BCI); electroencephalogram (EEG); intention recognition; Smart environments,Classification (of information); Convolutional neural networks; Image enhancement; Control technologies; Enhancement learning; Feature representation; Intention recognition; Multi-classification; Signal decomposition; Spatial transformation; State-of-the-art methods; Brain computer interface
Density Guarantee on Finding Multiple Subgraphs and Subtensors,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108968112&doi=10.1145%2f3446668&partnerID=40&md5=5bdd64df089b759237fc6ceac5599660,"Dense subregion (subgraph & subtensor) detection is a well-studied area, with a wide range of applications, and numerous efficient approaches and algorithms have been proposed. Approximation approaches are commonly used for detecting dense subregions due to the complexity of the exact methods. Existing algorithms are generally efficient for dense subtensor and subgraph detection, and can perform well in many applications. However, most of the existing works utilize the state-or-the-art greedy 2-approximation algorithm to capably provide solutions with a loose theoretical density guarantee. The main drawback of most of these algorithms is that they can estimate only one subtensor, or subgraph, at a time, with a low guarantee on its density. While some methods can, on the other hand, estimate multiple subtensors, they can give a guarantee on the density with respect to the input tensor for the first estimated subsensor only. We address these drawbacks by providing both theoretical and practical solution for estimating multiple dense subtensors in tensor data and giving a higher lower bound of the density. In particular, we guarantee and prove a higher bound of the lower-bound density of the estimated subgraph and subtensors. We also propose a novel approach to show that there are multiple dense subtensors with a guarantee on its density that is greater than the lower bound used in the state-of-the-art algorithms. We evaluate our approach with extensive experiments on several real-world datasets, which demonstrates its efficiency and feasibility. © 2021 ACM.",dense subgraph; Dense subtensor; density guarantee; event detection; multiple subtensor detection,Computer science; Data mining; Approximation approach; Exact methods; Its efficiencies; Lower bounds; Practical solutions; Real-world datasets; State-of-the-art algorithms; Theoretical density; Tensors
A Method for Mining Granger Causality Relationship on Atmospheric Visibility,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108968232&doi=10.1145%2f3447681&partnerID=40&md5=1437053fc3715e3fc3c5313df494bb35,"Atmospheric visibility is an indicator of atmospheric transparency and its range directly reflects the quality of the atmospheric environment. With the acceleration of industrialization and urbanization, the natural environment has suffered some damages. In recent decades, the level of atmospheric visibility shows an overall downward trend. A decrease in atmospheric visibility will lead to a higher frequency of haze, which will seriously affect people's normal life, and also have a significant negative economic impact. The causal relationship mining of atmospheric visibility can reveal the potential relation between visibility and other influencing factors, which is very important in environmental management, air pollution control and haze control. However, causality mining based on statistical methods and traditional machine learning techniques usually achieve qualitative results that are hard to measure the degree of causality accurately. This article proposed the seq2seq-LSTM Granger causality analysis method for mining the causality relationship between atmospheric visibility and its influencing factors. In the experimental part, by comparing with methods such as linear regression, random forest, gradient boosting decision tree, light gradient boosting machine, and extreme gradient boosting, it turns out that the visibility prediction accuracy based on the seq2seq-LSTM model is about 10% higher than traditional machine learning methods. Therefore, the causal relationship mining based on this method can deeply reveal the implicit relationship between them and provide theoretical support for air pollution control. © 2021 ACM.",Atmospheric visibility; deep learning; granger causality; multidimensional time series,Adaptive boosting; Air pollution; Decision trees; Environmental management; Long short-term memory; Machine learning; Statistical tests; Visibility; Atmospheric environment; Atmospheric transparency; Atmospheric visibility; Granger causality analysis; Implicit relationships; Machine learning methods; Machine learning techniques; Visibility prediction; Air pollution control
Mining Largest Maximal Quasi-Cliques,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108945646&doi=10.1145%2f3446637&partnerID=40&md5=9f1c5572274e5f2cb08e2410735a6cf2,"Quasi-cliques are dense incomplete subgraphs of a graph that generalize the notion of cliques. Enumerating quasi-cliques from a graph is a robust way to detect densely connected structures with applications in bioinformatics and social network analysis. However, enumerating quasi-cliques in a graph is a challenging problem, even harder than the problem of enumerating cliques. We consider the enumeration of top-k degree-based quasi-cliques and make the following contributions: (1) we show that even the problem of detecting whether a given quasi-clique is maximal (i.e., not contained within another quasi-clique) is NP-hard. (2) We present a novel heuristic algorithm KernelQC to enumerate the k largest quasi-cliques in a graph. Our method is based on identifying kernels of extremely dense subgraphs within a graph, followed by growing subgraphs around these kernels, to arrive at quasi-cliques with the required densities. (3) Experimental results show that our algorithm accurately enumerates quasi-cliques from a graph, is much faster than current state-of-the-art methods for quasi-clique enumeration (often more than three orders of magnitude faster), and can scale to larger graphs than current methods. © 2021 ACM.",Dense subgraph mining; heuristic algorithms; In-complete subgraphs; Large-scale graphs; Quasi-Clique enumeration,Heuristic algorithms; NP-hard; Connected structures; Dense sub-graphs; State-of-the-art methods; Subgraphs; Three orders of magnitude; Graph algorithms
Sequential Transform Learning,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108959523&doi=10.1145%2f3447394&partnerID=40&md5=bad80cbbf1338d3363206159df33e9a9,"This work proposes a new approach for dynamical modeling; we call it sequential transform learning. This is loosely based on the transform (analysis dictionary) learning formulation. This is the first work on this topic. Transform learning, was originally developed for static problems; we modify it to model dynamical systems by introducing a feedback loop. The learnt transform coefficients for the tth instant are fed back along with the t + 1st sample, thereby establishing a Markovian relationship. Furthermore, the formulation is made supervised by the label consistency cost. Our approach keeps the best of two worlds, marrying the interpretability and uncertainty measure of signal processing with the function approximation ability of neural networks. We have carried out experiments on one of the most challenging problems in dynamical modeling - stock forecasting. Benchmarking with the state-of-the-art has shown that our method excels over the rest. © 2021 ACM.",kalman filter; Sequential transform learning; stock forecasting,Dynamical systems; Signal processing; Function approximation; Interpretability; Learning formulation; State of the art; Static problems; Stock forecasting; Transform coefficients; Uncertainty measures; Learning systems
"DeepDepict: Enabling Information Rich, Personalized Product Description GenerationWith the Deep Multiple Pointer Generator Network",2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108948764&doi=10.1145%2f3446982&partnerID=40&md5=27dc23c882eb5e8fb3958ae37c6cd758,"In e-commerce platforms, the online descriptive information of products shows significant impacts on the purchase behaviors. To attract potential buyers for product promotion, numerous workers are employed to write the impressive product descriptions. The hand-crafted product descriptions are less-efficient with great labor costs and huge time consumption. Meanwhile, the generated product descriptions do not take consideration into the customization and the diversity to meet users' interests. To address these problems, we propose one generic framework, namely DeepDepict, to automatically generate the information-rich and personalized product descriptive information. Specifically, DeepDepict leverages the graph attention to retrieve the product-related knowledge from external knowledge base to enrich the diversity of products, constructs the personalized lexicon to capture the linguistic traits of individuals for the personalization of product descriptions, and utilizes multiple pointer-generator network to fuse heterogeneous data from multi-sources to generate informative and personalized product descriptions. We conduct intensive experiments on one public dataset. The experimental results show that DeepDepict outperforms existing solutions in terms of description diversity, BLEU, and personalized degree with significant margin gain, and is able to generate product descriptions with comprehensive knowledge and personalized linguistic traits. © 2021 ACM.",Personalized text generation; product description generation,Knowledge based systems; Wages; Descriptive information; External knowledge; Generic frameworks; Heterogeneous data; Personalized products; Potential buyers; Product descriptions; Users' interests; Linguistics
Critique on Natural Noise in Recommender Systems,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108951136&doi=10.1145%2f3447780&partnerID=40&md5=b3867912dc4404e2876b61a957dde977,"Recommender systems have been upgraded, tested, and applied in many, often incomparable ways. In attempts to diligently understand user behavior in certain environments, those systems have been frequently utilized in domains like e-commerce, e-learning, and tourism. Their increasing need and popularity have allowed the existence of numerous research paths on major issues like data sparsity, cold start, malicious noise, and natural noise, which immensely limit their performance. It is typical that the quality of the data that fuel those systems should be extremely reliable. Inconsistent user information in datasets can alter the performance of recommenders, albeit running advanced personalizing algorithms. The consequences of this can be costly as such systems are employed in abundant online businesses. Successfully managing these inconsistencies results in more personalized user experiences. In this article, the previous works conducted on natural noise management in recommender datasets are thoroughly analyzed. We adequately explore the ways in which the proposed methods measure improved performances and touch on the different natural noise management techniques and the attributes of the solutions. Additionally, we test the evaluation methods employed to assess the approaches and discuss several key gaps and other improvements the field should realize in the future. Our work considers the likelihood of a modern research branch on natural noise management and recommender assessment. © 2021 ACM.",evaluation metrics; natural noise management; Recommender systems,Behavioral research; Electronic commerce; Recommender systems; User experience; Cold start; Data sparsity; Evaluation methods; Noise management; Online business; User behaviors; User information; Online systems
Deep Graph Matching and Searching for Semantic Code Retrieval,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108944290&doi=10.1145%2f3447571&partnerID=40&md5=14c8a672080f2758dab68933fd5645df,"Code retrieval is to find the code snippet from a large corpus of source code repositories that highly matches the query of natural language description. Recent work mainly uses natural language processing techniques to process both query texts (i.e., human natural language) and code snippets (i.e., machine programming language), however, neglecting the deep structured features of query texts and source codes, both of which contain rich semantic information. In this article, we propose an end-to-end deep graph matching and searching (DGMS) model based on graph neural networks for the task of semantic code retrieval. To this end, we first represent both natural language query texts and programming language code snippets with the unified graph-structured data, and then use the proposed graph matching and searching model to retrieve the best matching code snippet. In particular, DGMS not only captures more structural information for individual query texts or code snippets, but also learns the fine-grained similarity between them by cross-attention based semantic matching operations. We evaluate the proposed DGMS model on two public code retrieval datasets with two representative programming languages (i.e., Java and Python). Experiment results demonstrate that DGMS significantly outperforms state-of-the-art baseline models by a large margin on both datasets. Moreover, our extensive ablation studies systematically investigate and illustrate the impact of each part of DGMS. © 2021 ACM.",graph representation; Neural networks; source code retrieval,Computer programming languages; Graph structures; Large dataset; Natural language processing systems; Semantics; Graph neural networks; Graph structured data; NAtural language processing; Natural language queries; Semantic information; Semantic matching; Source code repositories; Structural information; Semantic Web
Self-Adaptive Skeleton Approaches to Detect Self-Organized Coalitions from Brain Functional Networks through Probabilistic Mixture Models,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108973432&doi=10.1145%2f3447570&partnerID=40&md5=cf43431be330e1a52b7d79b33c7bc3f7,"Detecting self-organized coalitions from functional networks is one of the most important ways to uncover functional mechanisms in the brain. Determining these raises well-known technical challenges in terms of scale imbalance, outliers and hard-examples. In this article, we propose a novel self-adaptive skeleton approach to detect coalitions through an approximation method based on probabilistic mixture models. The nodes in the networks are characterized in terms of robust k-order complete subgraphs (k-clique) as essential substructures. The k-clique enumeration algorithm quickly enumerates all k-cliques in a parallel manner for a given network. Then, the cliques, from max-clique down to min-clique, of each order k, are hierarchically embedded into a probabilistic mixture model. They are self-adapted to the corresponding structure density of coalitions in the brain functional networks through different order k. All the cliques are merged and evolved into robust skeletons to sustain each unbalanced coalition by eliminating outliers and separating overlaps. We call this the k-CLIque Merging Evolution (CLIME) algorithm. The experimental results illustrate that the proposed approaches are robust to density variation and coalition mixture and can enable the effective detection of coalitions from real brain functional networks. There exist potential cognitive functional relations between the regions of interest in the coalitions revealed by our methods, which suggests the approach can be usefully applied in neuroscientific studies. © 2021 ACM.",Brain functional networks; probabilistic mixture model; self-organized coalition,Musculoskeletal system; Neurophysiology; Statistics; Approximation methods; Brain functional networks; Enumeration algorithms; Functional mechanisms; Functional relation; Probabilistic mixture models; Regions of interest; Technical challenges; Mixtures
Tiered Sampling: An Efficient Method for Counting Sparse Motifs in Massive Graph Streams,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108945305&doi=10.1145%2f3441299&partnerID=40&md5=9f185300f502ac6441491f0d5cac5559,"We introduce Tiered Sampling, a novel technique for estimating the count of sparse motifs in massive graphs whose edges are observed in a stream. Our technique requires only a single pass on the data and uses a memory of fixed size M, which can be magnitudes smaller than the number of edges. Our methods address the challenging task of counting sparse motifs - sub-graph patterns - that have a low probability of appearing in a sample of M edges in the graph, which is the maximum amount of data available to the algorithms in each step. To obtain an unbiased and low variance estimate of the count, we partition the available memory into tiers (layers) of reservoir samples. While the base layer is a standard reservoir sample of edges, other layers are reservoir samples of sub-structures of the desired motif. By storing more frequent sub-structures of the motif, we increase the probability of detecting an occurrence of the sparse motif we are counting, thus decreasing the variance and error of the estimate. While we focus on the designing and analysis of algorithms for counting 4-cliques, we present a method which allows generalizing Tiered Sampling to obtain high-quality estimates for the number of occurrence of any sub-graph of interest, while reducing the analysis effort due to specific properties of the pattern of interest. We present a complete analytical analysis and extensive experimental evaluation of our proposed method using both synthetic and real-world data. Our results demonstrate the advantage of our method in obtaining high-quality approximations for the number of 4 and 5-cliques for large graphs using a very limited amount of memory, significantly outperforming the single edge sample approach for counting sparse motifs in large scale graphs. © 2021 ACM.",Graph motif mining; reservoir sampling; stream computing,Graph algorithms; Graph theory; Quality control; Analysis of algorithms; Analytical analysis; Experimental evaluation; Low probability; Novel techniques; Specific properties; Sub-structures; Variance estimate; Graph structures
Dual-Embedding based Deep Latent Factor Models for Recommendation,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108964603&doi=10.1145%2f3447395&partnerID=40&md5=80eee429366cb6b1a865823075718d86,"Among various recommendation methods, latent factor models are usually considered to be state-of-the-art techniques, which aim to learn user and item embeddings for predicting user-item preferences. When applying latent factor models to the recommendation with implicit feedback, the quality of embeddings always suffers from inadequate positive feedback and noisy negative feedback. Inspired by the idea of NSVD that represents users based on their interacted items, this article proposes a dual-embedding based deep latent factor method for recommendation with implicit feedback. In addition to learning a primitive embedding for a user (resp. item), we represent each user (resp. item) with an additional embedding from the perspective of the interacted items (resp. users) and propose attentive neural methods to discriminate the importance of interacted users/items for dual-embedding learning. We design two dual-embedding based deep latent factor models, DELF and DESEQ, for pure collaborative filtering and temporal collaborative filtering (i.e., sequential recommendation), respectively. The novel attempt of the proposed models is to capture each user-item interaction with four deep representations that are subtly fused for preference prediction. We conducted extensive experiments on four real-world datasets. The results verify the effectiveness of user/item dual embeddings and the superior performance of our methods on item recommendation. © 2021 ACM.",Latent factor models; sequential recommendation,Collaborative filtering; Implicit feedback; Latent factor; Latent factor models; Real-world datasets; Recommendation methods; State-of-the-art techniques; Embeddings
Anomaly Detection with Kernel Preserving Embedding,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108962611&doi=10.1145%2f3447684&partnerID=40&md5=855eb0504c46592672ecac9a64ae60b9,"Similarity representation plays a central role in increasingly popular anomaly detection techniques, which have been successfully applied in various realistic scenes. Until now, many low-rank representation techniques have been introduced to measure the similarity relations of data; yet, they only concern to minimize reconstruction errors, without involving the structural information of data. Besides, the traditional low-rank representation methods often take nuclear norm as their low-rank constraints, easily yielding a suboptimal solution. To address the problems above, in this article, we propose a novel anomaly detection method, which exploits kernel preserving embedding, as well as the double nuclear norm, to explore the similarity relations of data. Based on the similarity relations, a kind of probability transition matrix is derived, and a tailored random walk is further adopted to reveal anomalies. The proposed method can not only preserve the manifold structural properties of the data, but also alleviate the suboptimal problem. To validate the superiority of our method, extensive experiments with eight popular anomaly detection algorithms were conducted on 12 widely used datasets. The experimental results show that our detection method outperformed the state-of-the-art anomaly detection algorithms in most cases. © 2021 ACM.",Anomaly detection; double nuclear norm; kernel preserving embedding; random walk,Embeddings; Signal detection; Anomaly detection methods; Anomaly-detection algorithms; Low-rank representations; Probability transition matrix; Reconstruction error; Similarity relations; Similarity representation; Structural information; Anomaly detection
Graph Neural Networks for Fast Node Ranking Approximation,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108973458&doi=10.1145%2f3446217&partnerID=40&md5=91c626f02bbcd61cfd849a6738237940,"Graphs arise naturally in numerous situations, including social graphs, transportation graphs, web graphs, protein graphs, etc. One of the important problems in these settings is to identify which nodes are important in the graph and how they affect the graph structure as a whole. Betweenness centrality and closeness centrality are two commonly used node ranking measures to find out influential nodes in the graphs in terms of information spread and connectivity. Both of these are considered as shortest path based measures as the calculations require the assumption that the information flows between the nodes via the shortest paths. However, exact calculations of these centrality measures are computationally expensive and prohibitive, especially for large graphs. Although researchers have proposed approximation methods, they are either less efficient or suboptimal or both. We propose the first graph neural network (GNN) based model to approximate betweenness and closeness centrality. In GNN, each node aggregates features of the nodes in multihop neighborhood. We use this feature aggregation scheme to model paths and learn how many nodes are reachable to a specific node. We demonstrate that our approach significantly outperforms current techniques while taking less amount of time through extensive experiments on a series of synthetic and real-world datasets. A benefit of our approach is that the model is inductive, which means it can be trained on one set of graphs and evaluated on another set of graphs with varying structures. Thus, the model is useful for both static graphs and dynamic graphs. Source code is available at https://github.com/sunilkmaurya/GNN_Ranking  © 2021 Owner/Author.",Betweenness centrality; closeness centrality; dynamic graphs; graph neural networks (GNNs); node ranking,Graph structures; Graph theory; Graphic methods; HTTP; Approximation methods; Betweenness centrality; Centrality measures; Closeness centralities; Exact calculations; Feature aggregation; Graph neural networks; Real-world datasets; Neural networks
Attributed Network Embedding with Micro-Meso Structure,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108417583&doi=10.1145%2f3441486&partnerID=40&md5=643e2649ccbd4149e1e18e54a1e213ce,"Recently, network embedding has received a large amount of attention in network analysis. Although some network embedding methods have been developed from different perspectives, on one hand, most of the existing methods only focus on leveraging the plain network structure, ignoring the abundant attribute information of nodes. On the other hand, for some methods integrating the attribute information, only the lower-order proximities (e.g., microscopic proximity structure) are taken into account, which may suffer if there exists the sparsity issue and the attribute information is noisy. To overcome this problem, the attribute information and mesoscopic community structure are utilized. In this article, we propose a novel network embedding method termed Attributed Network Embedding with Micro-Meso structure, which is capable of preserving both the attribute information and the structural information including the microscopic proximity structure and mesoscopic community structure. In particular, both the microscopic proximity structure and node attributes are factorized by Nonnegative Matrix Factorization (NMF), from which the low-dimensional node representations can be obtained. For the mesoscopic community structure, a community membership strength matrix is inferred by a generative model (i.e., BigCLAM) or modularity from the linkage structure, which is then factorized by NMF to obtain the low-dimensional node representations. The three components are jointly correlated by the low-dimensional node representations, from which two objective functions (i.e., ANEM_B and ANEM_M) can be defined. Two efficient alternating optimization schemes are proposed to solve the optimization problems. Extensive experiments have been conducted to confirm the superior performance of the proposed models over the state-of-the-art network embedding methods.  © 2021 ACM.",mesoscopic community structure; microscopic proximity structure; Network embedding; node attribute,Factorization; Matrix algebra; Superconducting materials; Alternating optimizations; Attribute information; Community structures; In-network analysis; Nonnegative matrix factorization; Objective functions; Optimization problems; Structural information; Embeddings
Benchmarking Unsupervised Outlier Detection with Realistic Synthetic Data,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108411004&doi=10.1145%2f3441453&partnerID=40&md5=a86a39637e203dc0ff3492fec32cc4aa,"Benchmarking unsupervised outlier detection is difficult. Outliers are rare, and existing benchmark data contains outliers with various and unknown characteristics. Fully synthetic data usually consists of outliers and regular instances with clear characteristics and thus allows for a more meaningful evaluation of detection methods in principle. Nonetheless, there have only been few attempts to include synthetic data in benchmarks for outlier detection. This might be due to the imprecise notion of outliers or to the difficulty to arrive at a good coverage of different domains with synthetic data. In this work, we propose a generic process for the generation of datasets for such benchmarking. The core idea is to reconstruct regular instances from existing real-world benchmark data while generating outliers so that they exhibit insightful characteristics. We propose and describe a generic process for the benchmarking of unsupervised outlier detection, as sketched so far. We then describe three instantiations of this generic process that generate outliers with specific characteristics, like local outliers. To validate our process, we perform a benchmark with state-of-the-art detection methods and carry out experiments to study the quality of data reconstructed in this way. Next to showcasing the workflow, this confirms the usefulness of our proposed process. In particular, our process yields regular instances close to the ones from real data. Summing up, we propose and validate a new and practical process for the benchmarking of unsupervised outlier detection.  © 2021 ACM.",benchmark; Outlier detection; synthetic data; unsupervised,Anomaly detection; Data handling; Statistics; Benchmark data; Detection methods; Different domains; Generic process; Practical process; Quality of data; State of the art; Synthetic data; Benchmarking
Variable-lag Granger Causality and Transfer Entropy for Time Series Analysis,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108435566&doi=10.1145%2f3441452&partnerID=40&md5=506f71fcf554bab07ff23c4531499563,"Granger causality is a fundamental technique for causal inference in time series data, commonly used in the social and biological sciences. Typical operationalizations of Granger causality make a strong assumption that every time point of the effect time series is influenced by a combination of other time series with a fixed time delay. The assumption of fixed time delay also exists in Transfer Entropy, which is considered to be a non-linear version of Granger causality. However, the assumption of the fixed time delay does not hold in many applications, such as collective behavior, financial markets, and many natural phenomena. To address this issue, we develop Variable-lag Granger causality and Variable-lag Transfer Entropy, generalizations of both Granger causality and Transfer Entropy that relax the assumption of the fixed time delay and allow causes to influence effects with arbitrary time delays. In addition, we propose methods for inferring both Variable-lag Granger causality and Transfer Entropy relations. In our approaches, we utilize an optimal warping path of Dynamic Time Warping to infer variable-lag causal relations. We demonstrate our approaches on an application for studying coordinated collective behavior and other real-world casual-inference datasets and show that our proposed approaches perform better than several existing methods in both simulated and real-world datasets. Our approaches can be applied in any domain of time series analysis. The software of this work is available in the R-CRAN package: VLTimeCausality.  © 2021 Owner/Author.",causal inference; Granger causality; statistical methodology; time series; transfer entropy,Entropy; Harmonic analysis; Statistical tests; Time delay; Biological science; Causal inferences; Collective behavior; Dynamic time warping; Fixed time delays; Granger Causality; Nonlinear versions; Real-world datasets; Time series analysis
Network Embedding on Hierarchical Community Structure Network,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108412439&doi=10.1145%2f3434747&partnerID=40&md5=bcee04dc56c8f6cf69eee5518bffdbdb,"Network embedding is a method of learning a low-dimensional vector representation of network vertices under the condition of preserving different types of network properties. Previous studies mainly focus on preserving structural information of vertices at a particular scale, like neighbor information or community information, but cannot preserve the hierarchical community structure, which would enable the network to be easily analyzed at various scales. Inspired by the hierarchical structure of galaxies, we propose the Galaxy Network Embedding (GNE) model, which formulates an optimization problem with spherical constraints to describe the hierarchical community structure preserving network embedding. More specifically, we present an approach of embedding communities into a low-dimensional spherical surface, the center of which represents the parent community they belong to. Our experiments reveal that the representations from GNE preserve the hierarchical community structure and show advantages in several applications such as vertex multi-class classification, network visualization, and link prediction. The source code of GNE is available online.  © 2021 ACM.",hierarchical community network; Network embedding; spherical projection,Embeddings; Galaxies; Learning systems; Hierarchical community structures; Hierarchical structures; Method of learning; Multi-class classification; Network properties; Network visualization; Optimization problems; Structural information; Computer networks
Jointly Modeling Spatiooral Dependencies and Daily Flow Correlations for Crowd Flow Prediction,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108428104&doi=10.1145%2f3439346&partnerID=40&md5=6faab628ed93f1e9f89cd979411767af,"Crowd flow prediction is a vital problem for an intelligent transportation system construction in a smart city. It plays a crucial role in traffic management and behavioral analysis, thus it has raised great attention from many researchers. However, predicting crowd flows timely and accurately is a challenging task that is affected by many complex factors such as the dependencies of adjacent regions or recent crowd flows. Existing models mainly focus on capturing such dependencies in spatial or temporal domains and fail to model relations between crowd flows of distant regions. We notice that each region has a relatively fixed daily flow and some regions (even very far away from each other) may share similar flow patterns which show strong correlations among them. In this article, we propose a novel model named Double-Encoder which follows a general encoder-decoder framework for multi-step citywide crowd flow prediction. The model consists of two encoder modules named ST-Encoder and FR-Encoder to model spatialoral dependencies and daily flow correlations, respectively. We conduct extensive experiments on two real-world datasets to evaluate the performance of the proposed model and show that our model consistently outperforms state-of-the-art methods.  © 2021 ACM.",CNN; ConvLSTM; encoder-decoder framework; latent representation; Multi-step crowd flow prediction,Forecasting; Intelligent systems; Signal encoding; Behavioral analysis; General encoder decoders; Intelligent transportation systems; Modeling relations; Real-world datasets; State-of-the-art methods; Strong correlation; Traffic management; Predictive analytics
Mobile App Cross-Domain Recommendation with Multi-Graph Neural Network,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108406437&doi=10.1145%2f3442201&partnerID=40&md5=52fb188a8e430e3445332c73a5e27079,"With the rapid development of mobile app ecosystem, mobile apps have grown greatly popular. The explosive growth of apps makes it difficult for users to find apps that meet their interests. Therefore, it is necessary to recommend user with a personalized set of apps. However, one of the challenges is data sparsity, as users' historical behavior data are usually insufficient. In fact, user's behaviors from different domains in app store regarding the same apps are usually relevant. Therefore, we can alleviate the sparsity using complementary information from correlated domains. It is intuitive to model users' behaviors using graph, and graph neural networks have shown the great power for representation learning. In this article, we propose a novel model, Deep Multi-Graph Embedding (DMGE), to learn cross-domain app embedding. Specifically, we first construct a multi-graph based on users' behaviors from different domains, and then propose a multi-graph neural network to learn cross-domain app embedding. Particularly, we present an adaptive method to balance the weight of each domain and efficiently train the model. Finally, we achieve cross-domain app recommendation based on the learned app embedding. Extensive experiments on real-world datasets show that DMGE outperforms other state-of-art embedding methods.  © 2021 ACM.",cross-domain recommendation; graph neural network; Mobile app; multi-task learning; transfer learning,Arts computing; Behavioral research; Embeddings; Graphic methods; Adaptive methods; Cross-domain recommendations; Different domains; Embedding method; Explosive growth; Graph embeddings; Graph neural networks; Real-world datasets; Neural networks
Online Sampling of Temporal Networks,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108416516&doi=10.1145%2f3442202&partnerID=40&md5=f07d866d8607052759a86bf11f8f1d8e,"Temporal networks representing a stream of timestamped edges are seemingly ubiquitous in the real world. However, the massive size and continuous nature of these networks make them fundamentally challenging to analyze and leverage for descriptive and predictive modeling tasks. In this work, we propose a general framework for temporal network sampling with unbiased estimation. We develop online, single-pass sampling algorithms, and unbiased estimators for temporal network sampling. The proposed algorithms enable fast, accurate, and memory-efficient statistical estimation of temporal network patterns and properties. In addition, we propose a temporally decaying sampling algorithm with unbiased estimators for studying networks that evolve in continuous time, where the strength of links is a function of time, and the motif patterns are temporally weighted. In contrast to the prior notion of a ĝ-3 toral motif, the proposed formulation and algorithms for counting temporally weighted motifs are useful for forecasting tasks in networks such as predicting future links, or a future time-series variable of nodes and links. Finally, extensive experiments on a variety of temporal networks from different domains demonstrate the effectiveness of the proposed algorithms. A detailed ablation study is provided to understand the impact of the various components of the proposed framework.  © 2021 ACM.",continuous-time dynamic networks; dynamic networks; Graph streams; graph summarization; higher-order networks; order sampling; priority sampling; reservoir sampling; statistical estimation; subgraph counting; triangle counting; unbiased estimation,Continuous time systems; Learning algorithms; Different domains; Memory efficient; Predictive modeling; Sampling algorithm; Statistical estimation; Temporal networks; Unbiased estimation; Unbiased estimator; Predictive analytics
User Embedding for Expert Finding in Community Question Answering,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108439102&doi=10.1145%2f3441302&partnerID=40&md5=a8e620deff0e8013de187f96a0319e4f,"The number of users who have the appropriate knowledge to answer asked questions in community question answering is lower than those who ask questions. Therefore, finding expert users who can answer the questions is very crucial and useful. In this article, we propose a framework to find experts for given questions and assign them the related questions. The proposed model benefits from users' relations in a community along with the lexical and semantic similarities between new question and existing answers. Node embedding is applied to the community graph to find similar users. Our experiments on four different Stack Exchange datasets show that adding community relations improves the performance of expert finding models.  © 2021 ACM.",community question answering; Expert finding; graph embedding; semantic text similarity,Public relations; Semantics; Community question answering; Community relations; Expert finding; Expert users; Semantic similarity; Embeddings
SAKE: Estimating Katz Centrality Based on Sampling for Large-Scale Social Networks,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108442598&doi=10.1145%2f3441646&partnerID=40&md5=e694cd358bea18b384069107cef13368,"Katz centrality is a fundamental concept to measure the influence of a vertex in a social network. However, existing approaches to calculating Katz centrality in a large-scale network are unpractical and computationally expensive. In this article, we propose a novel method to estimate Katz centrality based on graph sampling techniques, which object to achieve comparable estimation accuracy of the state-of-the-arts with much lower computational complexity. Specifically, we develop a Horvitz-Thompson estimate for Katz centrality by using a multi-round sampling approach and deriving an unbiased mean value estimator. We further propose SAKE, a Sampling-based Algorithm for fast Katz centrality Estimation. We prove that the estimator calculated by SAKE is probabilistically guaranteed to be within an additive error from the exact value. Extensive evaluation experiments based on four real-world networks show that the proposed algorithm can estimate Katz centralities for partial vertices with low sampling rate, low computation time, and it works well in identifying high influence vertices in social networks.  © 2021 ACM.",graph sampling; Katz centrality; Social network,Computer science; Data mining; Additive errors; Computation time; Evaluation experiments; Fundamental concepts; Large-scale network; Real-world networks; Sampling-based algorithms; State of the art; Petroleum reservoir evaluation
A Unified View of Causal and Non-causal Feature Selection,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108413496&doi=10.1145%2f3436891&partnerID=40&md5=be7d0c2892c385bc2455fd1aeaf54e59,"In this article, we aim to develop a unified view of causal and non-causal feature selection methods. The unified view will fill in the gap in the research of the relation between the two types of methods. Based on the Bayesian network framework and information theory, we first show that causal and non-causal feature selection methods share the same objective. That is to find the Markov blanket of a class attribute, the theoretically optimal feature set for classification. We then examine the assumptions made by causal and non-causal feature selection methods when searching for the optimal feature set, and unify the assumptions by mapping them to the restrictions on the structure of the Bayesian network model of the studied problem. We further analyze in detail how the structural assumptions lead to the different levels of approximations employed by the methods in their search, which then result in the approximations in the feature sets found by the methods with respect to the optimal feature set. With the unified view, we can interpret the output of non-causal methods from a causal perspective and derive the error bounds of both types of methods. Finally, we present practical understanding of the relation between causal and non-causal methods using extensive experiments with synthetic data and various types of real-world data.  © 2021 ACM.",Bayesian network; Causal feature selection; Markov blanket; mutual information; non-causal feature selection,Bayesian networks; Error analysis; Information theory; Bayesian network models; Feature selection methods; Feature sets; Markov Blankets; Network frameworks; Optimal feature sets; Structural assumption; Synthetic data; Feature extraction
Side Information Fusion for Recommender Systems over Heterogeneous Information Network,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108431688&doi=10.1145%2f3441446&partnerID=40&md5=4017e975256eaa3c7f34176f2f99f0a2,"Collaborative filtering (CF) has been one of the most important and popular recommendation methods, which aims at predicting users' preferences (ratings) based on their past behaviors. Recently, various types of side information beyond the explicit ratings users give to items, such as social connections among users and metadata of items, have been introduced into CF and shown to be useful for improving recommendation performance. However, previous works process different types of information separately, thus failing to capture the correlations that might exist across them. To address this problem, in this work, we study the application of heterogeneous information network (HIN), which offers a unifying and flexible representation of different types of side information, to enhance CF-based recommendation methods. However, we face challenging issues in HIN-based recommendation, i.e., how to capture similarities of complex semantics between users and items in a HIN, and how to effectively fuse these similarities to improve final recommendation performance. To address these issues, we apply metagraph to similarity computation and solve the information fusion problem with a ""matrix factorization (MF) + factorization machine (FM)""framework. For the MF part, we obtain the user-item similarity matrix from each metagraph and then apply low-rank matrix approximation to obtain latent features for both users and items. For the FM part, we apply FM with Group lasso (FMG) on the features obtained from the MF part to train the recommending model and, at the same time, identify the useful metagraphs. Besides FMG, a two-stage method, we further propose an end-to-end method, hierarchical attention fusing, to fuse metagraph-based similarities for the final recommendation. Experimental results on four large real-world datasets show that the two proposed frameworks significantly outperform existing state-of-the-art methods in terms of recommendation performance.  © 2021 ACM.",collaborative filtering; factorization machine; graph attention networks; heterogeneous information networks; matrix factorization; Recommender systems,Factorization; Frequency modulation; Information fusion; Information services; Large dataset; Matrix algebra; Semantics; Tunneling (excavation); Factorization machines; Heterogeneous information; Low-rank matrix approximations; Matrix factorizations; Recommendation methods; Recommendation performance; Similarity computation; State-of-the-art methods; Collaborative filtering
Elastic Embedding through Graph Convolution-based Regression for Semi-supervised Classification,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108448559&doi=10.1145%2f3441456&partnerID=40&md5=ffc875841cdce83753481d59624d5f39,"This article introduces a scheme for semi-supervised learning by estimating a flexible non-linear data representation that exploits Spectral Graph Convolutions structure. Structured data are exploited in order to determine non-linear and linear models. The introduced scheme takes advantage of data-driven graphs at two levels. First, it incorporates manifold smoothness that is naturally encoded by the graph itself. Second, the regression model is built on the convolved data samples that are derived from the data and their associated graph. The proposed semi-supervised embedding can tackle the problem of over-fitting on neighborhood structures for image data. The proposed Graph Convolution-based Semi-supervised Embedding paves the way to new theoretical and application perspectives related to the non-linear embedding. Indeed, building flexible models that adopt convolved data samples can enhance both the data representation and the final performance of the learning system. Several experiments are conducted on six image datasets for comparing the introduced scheme with some state-of-the-art semi-supervised approaches. This empirical evaluation shows the effectiveness of the proposed embedding scheme.  © 2021 ACM.",elastic embedding; graph convolution; graph-based embedding; Semi-supervised learning,Convolution; Embeddings; Learning systems; Regression analysis; Semi-supervised learning; Associated graph; Data representations; Empirical evaluations; Neighborhood structure; Regression model; Semi-supervised classification; State of the art; Structured data; Graph structures
Search Efficient Binary Network Embedding,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108408379&doi=10.1145%2f3436892&partnerID=40&md5=c8f5f9088bbe49546b9430d628a0dfd9,"Traditional network embedding primarily focuses on learning a continuous vector representation for each node, preserving network structure and/or node content information, such that off-the-shelf machine learning algorithms can be easily applied to the vector-format node representations for network analysis. However, the learned continuous vector representations are inefficient for large-scale similarity search, which often involves finding nearest neighbors measured by distance or similarity in a continuous vector space. In this article, we propose a search efficient binary network embedding algorithm called BinaryNE to learn a binary code for each node, by simultaneously modeling node context relations and node attribute relations through a three-layer neural network. BinaryNE learns binary node representations using a stochastic gradient descent-based online learning algorithm. The learned binary encoding not only reduces memory usage to represent each node, but also allows fast bit-wise comparisons to support faster node similarity search than using Euclidean or other distance measures. Extensive experiments and comparisons demonstrate that BinaryNE not only delivers more than 25 times faster search speed, but also provides comparable or better search quality than traditional continuous vector based network embedding methods. The binary codes learned by BinaryNE also render competitive performance on node classification and node clustering tasks. The source code of the BinaryNE algorithm is available at https://github.com/daokunzhang/BinaryNE.  © 2021 ACM.",binary coding; efficiency; Network embedding; similarity search,Binary codes; Embeddings; Encoding (symbols); Gradient methods; Machine learning; Multilayer neural networks; Nearest neighbor search; Network layers; Stochastic systems; Vector spaces; Vectors; Competitive performance; Content information; Network structures; Off-the-shelf machine; Online learning algorithms; Stochastic gradient descent; Three-layer neural networks; Vector representations; Learning algorithms
Attribute-Guided Network Sampling Mechanisms,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108437657&doi=10.1145%2f3441445&partnerID=40&md5=e95898baaec2f376e830d83929391fb1,"This article introduces a novel task-independent sampler for attributed networks. The problem is important because while data mining tasks on network content are common, sampling on internet-scale networks is costly. Link-trace samplers such as Snowball sampling, Forest Fire, Random Walk, and Metropolis-Hastings Random Walk are widely used for sampling from networks. The design of these attribute-agnostic samplers focuses on preserving salient properties of network structure, and are not optimized for tasks on node content. This article has three contributions. First, we propose a task-independent, attribute aware link-trace sampler grounded in Information Theory. Our sampler greedily adds to the sample the node with the most informative (i.e., surprising) neighborhood. The sampler tends to rapidly explore the attribute space, maximally reducing the surprise of unseen nodes. Second, we prove that content sampling is an NP-hard problem. A well-known algorithm best approximates the optimization solution within 1-1/e, but requires full access to the entire graph. Third, we show through empirical counterfactual analysis that in many real-world datasets, network structure does not hinder the performance of surprise based link-trace samplers. Experimental results over 18 real-world datasets reveal: surprise-based samplers are sample efficient and outperform the state-of-the-art attribute-agnostic samplers by a wide margin (e.g., 45% performance improvement in clustering tasks).  © 2021 ACM.",attributed networks; data mining; Task-independent sampling,Data mining; Deforestation; Graph algorithms; Information theory; NP-hard; Optimization; Random processes; Data mining tasks; Internet-scale networks; Metropolis Hastings; Network samplings; Network structures; Optimization solution; Real-world datasets; State of the art; Importance sampling
Clustering Heterogeneous Information Network by Joint Graph Embedding and Nonnegative Matrix Factorization,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108445315&doi=10.1145%2f3441449&partnerID=40&md5=3619530404b7b5e8482b1e242f83a600,"Many complex systems derived from nature and society consist of multiple types of entities and heterogeneous interactions, which can be effectively modeled as heterogeneous information network (HIN). Structural analysis of heterogeneous networks is of great significance by leveraging the rich semantic information of objects and links in the heterogeneous networks. And, clustering heterogeneous networks aims to group vertices into classes, which sheds light on revealing the structure-function relations of the underlying systems. The current algorithms independently perform the feature extraction and clustering, which are criticized for not fully characterizing the structure of clusters. In this study, we propose a learning model by joint <underline>G</underline>raph <underline>E</underline>mbedding and <underline>N</underline>onnegative <underline>M</underline>atrix <underline>F</underline>actorization (aka GEjNMF), where feature extraction and clustering are simultaneously learned by exploiting the graph embedding and latent structure of networks. We formulate the objective function of GEjNMF and transform the heterogeneous network clustering problem into a constrained optimization problem, which is effectively solved by l0-norm optimization. The advantage of GEjNMF is that features are selected under the guidance of clustering, which improves the performance and saves the running time of algorithms at the same time. The experimental results on three benchmark heterogeneous networks demonstrate that GEjNMF achieves the best performance with the least running time compared with the best state-of-the-art methods. Furthermore, the proposed algorithm is robust across heterogeneous networks from various fields. The proposed model and method provide an effective alternative for heterogeneous network clustering.  © 2021 ACM.",clustering; Heterogeneous information network; Non-negative matrix factorization,Benchmarking; Constrained optimization; Embeddings; Extraction; Factorization; Feature extraction; Heterogeneous networks; Information services; Matrix algebra; Semantics; Constrained optimi-zation problems; Heterogeneous information; Heterogeneous interactions; Network Clustering; Nonnegative matrix factorization; Objective functions; Semantic information; Structure-function relation; Clustering algorithms
A Layout-Based Classification Method for Visualizing Time-Varying Graphs,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108407698&doi=10.1145%2f3441301&partnerID=40&md5=7d82e9e1126ee873944956a911ffbd02,"Connectivity analysis between the components of large evolving systems can reveal significant patterns of interaction. The systems can be simulated by topological graph structures. However, such analysis becomes challenging on large and complex graphs. Tasks such as comparing, searching, and summarizing structures, are difficult due to the enormous number of calculations required. For time-varying graphs, the temporal dimension even intensifies the difficulty. In this article, we propose to reduce the complexity of analysis by focusing on subgraphs that are induced by closely related entities. To summarize the diverse structures of subgraphs, we build a supervised layout-based classification model. The main premise is that the graph structures can induce a unique appearance of the layout. In contrast to traditional graph theory-based and contemporary neural network-based methods of graph classification, our approach generates low costs and there is no need to learn informative graph representations. Combined with temporally stable visualizations, we can also facilitate the understanding of sub-structures and the tracking of graph evolution. The method is evaluated on two real-world datasets. The results show that our system is highly effective in carrying out visual-based analytics of large graphs.  © 2021 ACM.",simplified visualization; structural classification; Time-varying graph,Complex networks; Graph theory; Graphic methods; Classification methods; Classification models; Connectivity analysis; Graph classification; Graph representation; Real-world datasets; Significant patterns; Temporal dimensions; Graph structures
Robust Image Representation via Low Rank Locality Preserving Projection,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108405955&doi=10.1145%2f3434768&partnerID=40&md5=6d384632750224f91be97f1b899a49ba,"Locality preserving projection (LPP) is a dimensionality reduction algorithm preserving the neighhorhood graph structure of data. However, the conventional LPP is sensitive to outliers existing in data. This article proposes a novel low-rank LPP model called LR-LPP. In this new model, original data are decomposed into the clean intrinsic component and noise component. Then the projective matrix is learned based on the clean intrinsic component which is encoded in low-rank features. The noise component is constrained by the ĝ.,""1-norm which is more robust to outliers. Finally, LR-LPP model is extended to LR-FLPP in which low-dimensional feature is measured by F-norm. LR-FLPP will reduce aggregated error and weaken the effect of outliers, which will make the proposed LR-FLPP even more robust for outliers. The experimental results on public image databases demonstrate the effectiveness of the proposed LR-LPP and LR-FLPP.  © 2021 ACM.",classification; Dimensionality reduction; locality preserving projection; low rank,Dimensionality reduction; Graph algorithms; Graph structures; Statistics; Dimensionality reduction algorithms; F norms; Image representations; Locality preserving projections; Low dimensional; Noise components; Projective matrix; Public image database; Learning to rank
A Stochastic Algorithm Based on Reverse Sampling Technique to Fight against the Cyberbullying,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108445421&doi=10.1145%2f3441455&partnerID=40&md5=96b31a50f989d3b8565570d5a932dd55,"Cyberbullying has caused serious consequences especially for social network users in recent years. However, the challenge is how to fight against the cyberbullying effectively from the algorithmic perspective. In this article, we study the fighting against the cyberbullying problem, i.e., identify an initial witness set with a budget to spread the positive influence to protect the users in a specific target set such that the number of cybervictim users in the target set being activated by the seed set of cyberbullying is minimized. We first formulate this problem and show its NP-hardness. We further prove that the objective function is submodular with respect to the size of witnesses set when we convert the original problem into the maximal version. Then we propose a stochastic approach to solve this maximal version problem based on the Reverse Sampling Technique with a constant factor guarantee. In addition, we provide theoretical analysis and discuss the relationship between the optimal value and the value returned by the proposed algorithm. To evaluate the proposed approach, we implement extensive experiments on synthetic and real datasets. The experimental results show our approach is superior to the comparison methods.  © 2021 ACM.",cyberbullying; reverse sampling technique; seed selection; Social network; target users,Budget control; NP-hard; Stochastic systems; Comparison methods; Constant factors; Cyber bullying; Objective functions; Real data sets; Sampling technique; Stochastic algorithms; Stochastic approach; Computer crime
3E-LDA: Three Enhancements to Linear Discriminant Analysis,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107947522&doi=10.1145%2f3442347&partnerID=40&md5=2c7f2b6198dc6f518e2944ed2632c2e7,"Linear discriminant analysis (LDA) is one of the important techniques for dimensionality reduction, machine learning, and pattern recognition. However, in many applications, applying the classical LDA often faces the following problems: (1) sensitivity to outliers, (2) absence of local geometric information, and (3) small sample size or matrix singularity that can result in weak robustness and efficiency. Although several researchers have attempted to address one or more of the problems, little work has been done to address all of them together to produce a more effective and efficient LDA algorithm. This article proposes 3E-LDA, an enhanced LDA algorithm, that deals with all three problems as an attempt to further improve LDA. It proposes to learn a weighted median rather than the mean of the samples to deal with (1), to embed both between-class and within-class local geometric information to deal with (2), and to calculate the projection vectors in the null space of the matrix to deal with (3). Experiments on six benchmark datasets show that these three enhancements enable 3E-LDA to markedly outperform state-of-the-art LDA baselines in both accuracy and efficiency.  © 2021 ACM.",Dimensionality reduction; linear discriminant analysis; local geometric information; small sample size problem; weighted median,Dimensionality reduction; Discriminant analysis; Efficiency; Pattern recognition; Vector spaces; Benchmark datasets; Following problem; Geometric information; Linear discriminant analysis; Matrix singularity; Projection vectors; Small Sample Size; State of the art; Matrix algebra
Streaming Social Event Detection and Evolution Discovery in Heterogeneous Information Networks,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106432213&doi=10.1145%2f3447585&partnerID=40&md5=e4a2e8a3fa2f91d3a63246d21e6d41f8,"Events are happening in real world and real time, which can be planned and organized for occasions, such as social gatherings, festival celebrations, influential meetings, or sports activities. Social media platforms generate a lot of real-time text information regarding public events with different topics. However, mining social events is challenging because events typically exhibit heterogeneous texture and metadata are often ambiguous. In this article, we first design a novel event-based meta-schema to characterize the semantic relatedness of social events and then build an event-based heterogeneous information network (HIN) integrating information from external knowledge base. Second, we propose a novel Pairwise Popularity Graph Convolutional Network, named as PP-GCN, based on weighted meta-path instance similarity and textual semantic representation as inputs, to perform fine-grained social event categorization and learn the optimal weights of meta-paths in different tasks. Third, we propose a streaming social event detection and evolution discovery framework for HINs based on meta-path similarity search, historical information about meta-paths, and heterogeneous DBSCAN clustering method. Comprehensive experiments on real-world streaming social text data are conducted to compare various social event detection and evolution discovery algorithms. Experimental results demonstrate that our proposed framework outperforms other alternative social event detection and evolution discovery techniques. © 2021 ACM.",DBSCAN; event evolution; fine-grained categorization; graph convolutional network; heterogeneous information network; pairwise learning; Social event detection; streaming data,Convolutional neural networks; Information services; Knowledge based systems; Semantics; Textures; Convolutional networks; Discovery frameworks; Heterogeneous information; Historical information; Integrating information; Semantic relatedness; Semantic representation; Social media platforms; Text mining
Exploiting Temporal Dynamics in Product Reviews for Dynamic Sentiment Prediction at the Aspect Level,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105480874&doi=10.1145%2f3441451&partnerID=40&md5=aefd47284a77bfd32786fbc2cc9ab9f6,"Online reviews and ratings play an important role in shaping the purchase decisions of customers in e-commerce. Many researches have been done to make proper recommendations for users, by exploiting reviews, ratings, user profiles, or behaviors. However, the dynamic evolution of user preferences and item properties haven't been fully exploited. Moreover, it lacks fine-grained studies at the aspect level. To address the above issues, we define two concepts of user maturity and item popularity, to better explore the dynamic changes for users and items. We strive to exploit fine-grained information at the aspect level and the evolution of users and items, for dynamic sentiment prediction. First, we analyze three real datasets from both the overall level and the aspect level, to discover the dynamic changes (i.e., gradual changes and sudden changes) in user aspect preferences and item aspect properties. Next, we propose a novel model of Aspect-based Sentiment Dynamic Prediction (ASDP), to dynamically capture and exploit the change patterns with uniform time intervals. We further propose the improved model ASDP+ with a bin segmentation algorithm to set the time intervals non-uniformly based on the sudden changes. Experimental results on three real-world datasets show that our work leads to significant improvements.  © 2021 ACM.",aspect level; opinion evolution; Review mining; sentiment prediction; temporal dynamics,Computer science; Data mining; Dynamic evolution; Dynamic prediction; Item popularities; Product reviews; Purchase decision; Real-world datasets; Segmentation algorithms; Temporal dynamics; Forecasting
Utility Mining across Multi-Dimensional Sequences,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105774484&doi=10.1145%2f3446938&partnerID=40&md5=c94b3a4071ffeecf945bbd0c3ecfa8b0,"Knowledge extraction from database is the fundamental task in database and data mining community, which has been applied to a wide range of real-world applications and situations. Different from the support-based mining models, the utility-oriented mining framework integrates the utility theory to provide more informative and useful patterns. Time-dependent sequence data are commonly seen in real life. Sequence data have been widely utilized in many applications, such as analyzing sequential user behavior on the Web, influence maximization, route planning, and targeted marketing. Unfortunately, all the existing algorithms lose sight of the fact that the processed data not only contain rich features (e.g., occur quantity, risk, and profit), but also may be associated with multi-dimensional auxiliary information, e.g., transaction sequence can be associated with purchaser profile information. In this article, we first formulate the problem of utility mining across multi-dimensional sequences, and propose a novel framework named MDUS to extract <underline>M</underline>ulti-<underline>D</underline>imensional <underline>U</underline>tility-oriented <underline>S</underline>equential useful patterns. To the best of our knowledge, this is the first study that incorporates the time-dependent sequence-order, quantitative information, utility factor, and auxiliary dimension. Two algorithms respectively named MDUSEM and MDUSSD are presented to address the formulated problem. The former algorithm is based on database transformation, and the later one performs pattern joins and a searching method to identify desired patterns across multi-dimensional sequences. Extensive experiments are carried on six real-life datasets and one synthetic dataset to show that the proposed algorithms can effectively and efficiently discover the useful knowledge from multi-dimensional sequential databases. Moreover, the MDUS framework can provide better insight, and it is more adaptable to real-life situations than the current existing models. © 2021 ACM.",auxiliary dimension; Economic; pruning strategies; sequential data; utility mining,Behavioral research; Database systems; Auxiliary information; Database and data mining; Database transformation; Formulated problems; Influence maximizations; Knowledge extraction; Quantitative information; Sequential database; Data mining
Optimal Algebraic Breadth-First Search for Sparse Graphs,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107908631&doi=10.1145%2f3446216&partnerID=40&md5=050ccc7d66c628f70415d8a02b773efc,"There has been a rise in the popularity of algebraic methods for graph algorithms given the development of the GraphBLAS library and other sparse matrix methods. An exemplar for these approaches is Breadth-First Search (BFS). The algebraic BFS algorithm is simply a recurrence of matrix-vector multiplications with the n × n adjacency matrix, but the many redundant operations over nonzeros ultimately lead to suboptimal performance. Therefore an optimal algebraic BFS should be of keen interest especially if it is easily integrated with existing matrix methods. Current methods, notably in the GraphBLAS, use a Sparse Matrix masked-Sparse Vector multiplication in which the input vector is kept in a sparse representation in each step of the BFS, and nonzeros in the vector are masked in subsequent steps. This has been an area of recent research in GraphBLAS and other libraries. While in theory, these masking methods are asymptotically optimal on sparse graphs, many add work that leads to suboptimal runtime. We give a new optimal, algebraic BFS for sparse graphs, thus closing a gap in the literature. Our method multiplies progressively smaller submatrices of the adjacency matrix at each step. Let n and m refer to the number of vertices and edges, respectively. On a sparse graph, our method takes O(n) algebraic operations as opposed to O(m) operations needed by theoretically optimal sparse matrix approaches. Thus, for sparse graphs, it matches the bounds of the best-known sequential algorithm, and on a Parallel Random Access Machine, it is work-optimal. Our result holds for both directed and undirected graphs. Compared to a leading GraphBLAS library, our method achieves up to 24x faster sequential time, and for parallel computation, it can be 17x faster on large graphs and 12x faster on large-diameter graphs. © 2021 Public Domain.",Breadth-first search; graph algorithm; linear algebra; sparse matrix,Computation theory; Directed graphs; Graph algorithms; Graph structures; Graphic methods; Vectors; Algebraic operations; Asymptotically optimal; Matrix vector multiplication; Parallel random access machines; Sequential algorithm; Sparse matrix methods; Sparse representation; Sub-optimal performance; Matrix algebra
Exploring Deep Reinforcement Learning for Task Dispatching in Autonomous On-Demand Services,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105443766&doi=10.1145%2f3442343&partnerID=40&md5=f32e9c9e9934ebabbb7c461cc4812657,"Autonomous on-demand services, such as GOGOX (formerly GoGoVan) in Hong Kong, provide a platform for users to request services and for suppliers to meet such demands. In such a platform, the suppliers have autonomy to accept or reject the demands to be dispatched to him/her, so it is challenging to make an online matching between demands and suppliers. Existing methods use round-based approaches to dispatch demands. In these works, the dispatching decision is based on the predicted response patterns of suppliers to demands in the current round, but they all fail to consider the impact of future demands and suppliers on the current dispatching decision. This could lead to taking a suboptimal dispatching decision from the future perspective. To solve this problem, we propose a novel demand dispatching model using deep reinforcement learning. In this model, we make each demand as an agent. The action of each agent, i.e., the dispatching decision of each demand, is determined by a centralized algorithm in a coordinated way. The model works in the following two steps. (1) It learns the demand's expected value in each spatiotemporal state using historical transition data. (2) Based on the learned values, it conducts a Many-To-Many dispatching using a combinatorial optimization algorithm by considering both immediate rewards and expected values of demands in the next round. In order to get a higher total reward, the demands with a high expected value (short response time) in the future may be delayed to the next round. On the contrary, the demands with a low expected value (long response time) in the future would be dispatched immediately. Through extensive experiments using real-world datasets, we show that the proposed model outperforms the existing models in terms of Cancellation Rate and Average Response Time.  © 2021 Association for Computing Machinery.",deep reinforcement learning; Demand dispatching; on-demand services,Combinatorial optimization; Reinforcement learning; Centralized algorithms; Combinatorial optimization algorithm; Dispatching models; Future perspectives; Historical transition; On-demand services; Real-world datasets; Short response time; Deep learning
Fast Connectivity Minimization on Large-Scale Networks,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105481369&doi=10.1145%2f3442342&partnerID=40&md5=586a485e702028b520fb49e667262182,"The connectivity of networks has been widely studied in many high-impact applications, ranging from immunization, critical infrastructure analysis, social network mining, to bioinformatic system studies. Regardless of the end application domains, connectivity minimization has always been a fundamental task to effectively control the functioning of the underlying system. The combinatorial nature of the connectivity minimization problem imposes an exponential computational complexity to find the optimal solution, which is intractable in large systems. To tackle the computational barrier, greedy algorithm is extensively used to ensure a near-optimal solution by exploiting the diminishing returns property of the problem. Despite the empirical success, the theoretical and algorithmic challenges of the problems still remain wide open. On the theoretical side, the intrinsic hardness and the approximability of the general connectivity minimization problem are still unknown except for a few special cases. On the algorithmic side, existing algorithms are hard to balance between the optimization quality and computational efficiency. In this article, we address the two challenges by (1) proving that the general connectivity minimization problem is NP-hard and is the best approximation ratio for any polynomial algorithms, and (2) proposing the algorithm CONTAIN and its variant CONTAIN+ that can well balance optimization effectiveness and computational efficiency for eigen-function based connectivity minimization problems in large networks.  © 2021 Association for Computing Machinery.",Graph mining; network connectivity,Approximation algorithms; Efficiency; NP-hard; Optimal systems; Optimization; Polynomial approximation; Balance optimization; Best approximations; Computational barriers; Minimization problems; Near-optimal solutions; Optimization quality; Polynomial algorithm; Social network minings; Computational efficiency
Preserve Integrity in Realtime Event Summarization,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105524328&doi=10.1145%2f3442344&partnerID=40&md5=eb53334852502b5d595a1c0e444a21e4,"Online text streams such as Twitter are the major information source for users when they are looking for ongoing events. Realtime event summarization aims to generate and update coherent and concise summaries to describe the state of a given event. Due to the enormous volume of continuously coming texts, realtime event summarization has become the de facto tool to facilitate information acquisition. However, there exists a challenging yet unexplored issue in current text summarization techniques: how to preserve the integrity, i.e., the accuracy and consistency of summaries during the update process. The issue is critical since online text stream is dynamic and conflicting information could spread during the event period. For example, conflicting numbers of death and injuries might be reported after an earthquake. Such misleading information should not appear in the earthquake summary at any timestamp. In this article, we present a novel realtime event summarization framework called IAEA (i.e., Integrity-Aware Extractive-Abstractive realtime event summarization). Our key idea is to integrate an inconsistency detection module into a unified extractive-abstractive framework. In each update, important new tweets are first extracted in an extractive module, and the extraction is refined by explicitly detecting inconsistency between new tweets and previous summaries. The extractive module is able to capture the sentence-level attention which is later used by an abstractive module to obtain the word-level attention. Finally, the word-level attention is leveraged to rephrase words. We conduct comprehensive experiments on real-world datasets. To reduce efforts required for building sufficient training data, we also provide automatic labeling steps of which the effectiveness has been empirically verified. Through experiments, we demonstrate that IAEA can generate better summaries with consistent information than state-of-the-art approaches.  © 2021 Association for Computing Machinery.",data integrity; hierarchical deep neural network; real-time event summarization; Tweet summarization,Computer science; Data mining; Automatic labeling; Inconsistency detection; Information acquisitions; Information sources; Misleading informations; Real-world datasets; State-of-the-art approach; Text summarization; Earthquakes
An Improved KNN-Based Efficient Log Anomaly Detection Method with Automatically Labeled Samples,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105481653&doi=10.1145%2f3441448&partnerID=40&md5=dcaf7e9ae56c50c234ad348c7fab1984,"Logs that record system abnormal states (anomaly logs) can be regarded as outliers, and the k-Nearest Neighbor (kNN) algorithm has relatively high accuracy in outlier detection methods. Therefore, we use the kNN algorithm to detect anomalies in the log data. However, there are some problems when using the kNN algorithm to detect anomalies, three of which are: excessive vector dimension leads to inefficient kNN algorithm, unlabeled log data cannot support the kNN algorithm, and the imbalance of the number of log data distorts the classification decision of kNN algorithm. In order to solve these three problems, we propose an efficient log anomaly detection method based on an improved kNN algorithm with an automatically labeled sample set. This method first proposes a log parsing method based on N-gram and frequent pattern mining (FPM) method, which reduces the dimension of the log vector converted with Term frequency.Inverse Document Frequency (TF-IDF) technology. Then we use clustering and self-training method to get labeled log data sample set from historical logs automatically. Finally, we improve the kNN algorithm using average weighting technology, which improves the accuracy of the kNN algorithm on unbalanced samples. The method in this article is validated on six log datasets with different types.  © 2021 Association for Computing Machinery.",Anomaly detection; frequent pattern mining; kNN; N-gram; self-training,Anomaly detection; Nearest neighbor search; Statistics; Anomaly detection methods; Classification decision; Document frequency; Frequent pattern mining; K nearest neighbor algorithm; k-NN algorithm; Parsing methods; Self training; Learning algorithms
Probability Ordinal-Preserving Semantic Hashing for Large-Scale Image Retrieval,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105528496&doi=10.1145%2f3442204&partnerID=40&md5=2f4912e87d10b0d2c8f6071a14bfce3a,"Semantic hashing enables computation and memory-efficient image retrieval through learning similarity-preserving binary representations. Most existing hashing methods mainly focus on preserving the piecewise class information or pairwise correlations of samples into the learned binary codes while failing to capture the mutual triplet-level ordinal structure in similarity preservation. In this article, we propose a novel Probability Ordinal-preserving Semantic Hashing (POSH) framework, which for the first time defines the ordinal-preserving hashing concept under a non-parametric Bayesian theory. Specifically, we derive the whole learning framework of the ordinal similarity-preserving hashing based on the maximum posteriori estimation, where the probabilistic ordinal similarity preservation, probabilistic quantization function, and probabilistic semantic-preserving function are jointly considered into one unified learning framework. In particular, the proposed triplet-ordering correlation preservation scheme can effectively improve the interpretation of the learned hash codes under an economical anchor-induced asymmetric graph learning model. Moreover, the sparsity-guided selective quantization function is designed to minimize the loss of space transformation, and the regressive semantic function is explored to promote the flexibility of the formulated semantics in hash code learning. The final joint learning objective is formulated to concurrently preserve the ordinal locality of original data and explore potentials of semantics for producing discriminative hash codes. Importantly, an efficient alternating optimization algorithm with the strictly proof convergence guarantee is developed to solve the resulting objective problem. Extensive experiments on several large-scale datasets validate the superiority of the proposed method against state-of-the-art hashing-based retrieval methods.  © 2021 Association for Computing Machinery.",discriminative learning; image retrieval; Learning to hash; ordinal-preserving hashing,Computation theory; Hash functions; Image retrieval; Large dataset; Alternating optimizations; Binary representations; Non-parametric Bayesian; Posteriori estimation; Probabilistic semantics; Quantization functions; Similarity preserving; Space transformations; Semantics
TipTap: Approximate Mining of Frequent k-Subgraph Patterns in Evolving Graphs,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105490254&doi=10.1145%2f3442590&partnerID=40&md5=f0314f161d88b83a9d9aa9fbdd7364fc,"""Perhaps he could dance first and think afterwards, if it isn't too much to ask him.""S. Beckett, Waiting for GodotGiven a labeled graph, the collection of -vertex induced connected subgraph patterns that appear in the graph more frequently than a user-specified minimum threshold provides a compact summary of the characteristics of the graph, and finds applications ranging from biology to network science. However, finding these patterns is challenging, even more so for dynamic graphs that evolve over time, due to the streaming nature of the input and the exponential time complexity of the problem. We study this task in both incremental and fully-dynamic streaming settings, where arbitrary edges can be added or removed from the graph. We present TipTap, a suite of algorithms to compute high-quality approximations of the frequent -vertex subgraphs w.r.t. a given threshold, at any time (i.e., point of the stream), with high probability. In contrast to existing state-of-the-art solutions that require iterating over the entire set of subgraphs in the vicinity of the updated edge, TipTap operates by efficiently maintaining a uniform sample of connected -vertex subgraphs, thanks to an optimized neighborhood-exploration procedure. We provide a theoretical analysis of the proposed algorithms in terms of their unbiasedness and of the sample size needed to obtain a desired approximation quality. Our analysis relies on sample-complexity bounds that use Vapnik-Chervonenkis dimension, a key concept from statistical learning theory, which allows us to derive a sufficient sample size that is independent from the size of the graph. The results of our empirical evaluation demonstrates that TipTap returns high-quality results more efficiently and accurately than existing baselines.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Edge streams; graph streams; random pairing; reservoir sampling; VC-dimension,Approximation algorithms; Complex networks; Quality control; Approximation quality; Connected Subgraph; Dynamic Streaming; Empirical evaluations; Exponential time complexity; Sample complexity bounds; Statistical learning theory; Vapnik-Chervonenkis dimensions; Graph theory
Knowledge Transfer with Weighted Adversarial Network for Cold-Start Store Site Recommendation,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105458040&doi=10.1145%2f3442203&partnerID=40&md5=04b1639aa4c1acde6956360df23ea3f4,"Store site recommendation aims to predict the value of the store at candidate locations and then recommend the optimal location to the company for placing a new brick-and-mortar store. Most existing studies focus on learning machine learning or deep learning models based on large-scale training data of existing chain stores in the same city. However, the expansion of chain enterprises in new cities suffers from data scarcity issues, and these models do not work in the new city where no chain store has been placed (i.e., cold-start problem). In this article, we propose a unified approach for cold-start store site recommendation, Weighted Adversarial Network with Transferability weighting scheme (WANT), to transfer knowledge learned from a data-rich source city to a target city with no labeled data. In particular, to promote positive transfer, we develop a discriminator to diminish distribution discrepancy between source city and target city with different data distributions, which plays the minimax game with the feature extractor to learn transferable representations across cities by adversarial learning. In addition, to further reduce the risk of negative transfer, we design a transferability weighting scheme to quantify the transferability of examples in source city and reweight the contribution of relevant source examples to transfer useful knowledge. We validate WANT using a real-world dataset, and experimental results demonstrate the effectiveness of our proposed model over several state-of-the-art baseline models.  © 2021 Association for Computing Machinery.",cold-start problem; neural networks; store site recommendation; transfer learning; Urban computing,Deep learning; Knowledge management; Adversarial learning; Adversarial networks; Candidate locations; Cold start problems; Data distribution; Knowledge transfer; Learning machines; Optimal locations; Learning systems
TPmod: A Tendency-Guided Prediction Model for Temporal Knowledge Graph Completion,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105513017&doi=10.1145%2f3443687&partnerID=40&md5=3033b7a039c9b8cce3622eaa87b11ec4,"Temporal knowledge graphs (TKGs) have become useful resources for numerous Artificial Intelligence applications, but they are far from completeness. Inferring missing events in temporal knowledge graphs is a fundamental and challenging task. However, most existing methods solely focus on entity features or consider the entities and relations in a disjoint manner. They do not integrate the features of entities and relations in their modeling process. In this paper, we propose TPmod, a tendency-guided prediction model, to predict the missing events for TKGs (extrapolation). Differing from existing works, we propose two definitions for TKGs: the Goodness of relations and the Closeness of entity pairs. More importantly, inspired by the attention mechanism, we propose a novel tendency strategy to guide our aggregated process. It integrates the features of entities and relations, and assigns varying weights to different past events. What is more, we select the Gate Recurrent Unit (GRU) as our sequential encoder to model the temporal dependency in TKGs. Besides, the Softmax function is employed to generate the final decreasing group of candidate entities. We evaluate our model on two TKG datasets: GDELT-5 and ICEWS-250. Experimental results show that our method has a significant and consistent improvement compared to state-of-the-art baselines.  © 2021 Association for Computing Machinery.",completion; Temporal knowledge graph; tendency strategy,Forecasting; Knowledge representation; Attention mechanisms; Modeling process; Prediction model; State of the art; Temporal knowledge; Predictive analytics
"Understanding Persuasion Cascades in Online Product Rating Systems: Modeling, Analysis, and Inference",2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105502936&doi=10.1145%2f3440887&partnerID=40&md5=06ac62adaae3da511753a4932feec5ad,"Online product rating systems have become an indispensable component for numerous web services such as Amazon, eBay, Google Play Store, and TripAdvisor. One functionality of such systems is to uncover the product quality via product ratings (or reviews) contributed by consumers. However, a well-known psychological phenomenon called ""message-based persuasion""lead to ""biased""product ratings in a cascading manner (we call this the persuasion cascade). This article investigates: (1) How does the persuasion cascade influence the product quality estimation accuracy? (2) Given a real-world product rating dataset, how to infer the persuasion cascade and analyze it to draw practical insights?We first develop a mathematical model to capture key factors of a persuasion cascade. We formulate a high-order Markov chain to characterize the opinion dynamics of a persuasion cascade and prove the convergence of opinions. We further bound the product quality estimation error for a class of rating aggregation rules including the averaging scoring rule, via the matrix perturbation theory and the Chernoff bound. We also design a maximum likelihood algorithm to infer parameters of the persuasion cascade. We conduct experiments on both synthetic data and real-world data from Amazon and TripAdvisor. Experiment results show that our inference algorithm has a high accuracy. Furthermore, persuasion cascades notably exist, but the average scoring rule has a small product quality estimation error under practical scenarios.  © 2021 Association for Computing Machinery.",high order Markov chain; Online rating systems; persuasion cascades; product quality estimation,Estimation; Inference engines; Markov chains; Maximum likelihood estimation; Perturbation techniques; Quality control; Web services; High order Markov chain; Inference algorithm; Matrix perturbation theory; Maximum likelihood algorithm; Online products; Opinion dynamics; Quality estimation; Rating aggregation; Online systems
GGATB-LSTM: Grouping and Global Attention-based Time-aware Bidirectional LSTM Medical Treatment Behavior Prediction,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105488749&doi=10.1145%2f3441454&partnerID=40&md5=f8eaf2a3e2f9b0ba26f435251ca1ec5e,"In China, with the continuous development of national health insurance policies, more and more people have joined the health insurance. How to accurately predict patients future medical treatment behavior becomes a hotspot issue. The biggest challenge in this issue is how to improve the prediction performance by modeling health insurance data with high-dimensional time characteristics. At present, most of the research is to solve this issue by using Recurrent Neural Networks (RNNs) to construct an overall prediction model for the medical visit sequences. However, RNNs can not effectively solve the long-term dependence, and RNNs ignores the importance of time interval of the medical visit sequence. Additionally, the global model may lose some important content to different groups. In order to solve these problems, we propose a Grouping and Global Attention based Time-aware Bidirectional Long Short-Term Memory (GGATB-LSTM) model to achieve medical treatment behavior prediction. The model first constructs a heterogeneous information network based on health insurance data, and uses a tensor CANDECOMP/PARAFAC decomposition method to achieve similarity grouping. In terms of group prediction, a global attention and time factor are introduced to extend the bidirectional LSTM. Finally, the proposed model is evaluated by using real dataset, and conclude that GGATB-LSTM is better than other methods.  © 2021 Association for Computing Machinery.",Health insurance; medical treatment behavior prediction; medical visit sequence; similarity grouping,Forecasting; Health insurance; Information services; Medical problems; Patient treatment; Predictive analytics; Candecomp/parafac decompositions; Continuous development; Health insurance policies; Heterogeneous information; Long-term dependence; Prediction performance; Recurrent neural network (RNNs); Time characteristics; Long short-term memory
CoCoS: Fast and Accurate Distributed Triangle Counting in Graph Streams,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105502104&doi=10.1145%2f3441487&partnerID=40&md5=b20b513feb02e469f04ff39a81dfc4ba,"Given a graph stream, how can we estimate the number of triangles in it using multiple machines with limited storage? Specifically, how should edges be processed and sampled across the machines for rapid and accurate estimation?The count of triangles (i.e., cliques of size three) has proven useful in numerous applications, including anomaly detection, community detection, and link recommendation. For triangle counting in large and dynamic graphs, recent work has focused largely on streaming algorithms and distributed algorithms but little on their combinations for ""the best of both worlds.""In this work, we propose CoCoS, a fast and accurate distributed streaming algorithm for estimating the counts of global triangles (i.e., all triangles) and local triangles incident to each node. Making one pass over the input stream, CoCoS carefully processes and stores the edges across multiple machines so that the redundant use of computational and storage resources is minimized. Compared to baselines, CoCoS is: (a) accurate: giving up to smaller estimation error; (b) fast: up to faster, scaling linearly with the size of the input stream; and (c) theoretically sound: yielding unbiased estimates.  © 2021 Association for Computing Machinery.",distributed algorithms; Graph stream; sampling; streaming algorithms; triangle counting,Anomaly detection; Cobalt compounds; Graph algorithms; Accurate estimation; Community detection; Distributed streaming; Estimation errors; Number of triangles; Storage resources; Streaming algorithm; Unbiased estimates; Sulfur compounds
A Scalable Redefined Stochastic Blockmodel,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105465358&doi=10.1145%2f3442589&partnerID=40&md5=a6416cdc44c2db73bf8510aab7f3c173,"Stochastic blockmodel (SBM) is a widely used statistical network representation model, with good interpretability, expressiveness, generalization, and flexibility, which has become prevalent and important in the field of network science over the last years. However, learning an optimal SBM for a given network is an NP-hard problem. This results in significant limitations when it comes to applications of SBMs in large-scale networks, because of the significant computational overhead of existing SBM models, as well as their learning methods. Reducing the cost of SBM learning and making it scalable for handling large-scale networks, while maintaining the good theoretical properties of SBM, remains an unresolved problem. In this work, we address this challenging task from a novel perspective of model redefinition. We propose a novel redefined SBM with Poisson distribution and its block-wise learning algorithm that can efficiently analyse large-scale networks. Extensive validation conducted on both artificial and real-world data shows that our proposed method significantly outperforms the state-of-the-art methods in terms of a reasonable trade-off between accuracy and scalability.1  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Complex networks; redefined stochastic blockmodel; structural pattern detection,Economic and social effects; Learning systems; NP-hard; Poisson distribution; Stochastic models; Stochastic systems; Computational overheads; Interpretability; Large-scale network; Learning methods; Network representation; Network science; State-of-the-art methods; Stochastic block models; Learning algorithms
Predicting Influential Users in Online Social Network Groups,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105473771&doi=10.1145%2f3441447&partnerID=40&md5=3c1b4b5974dc931d3c7487ffad5cd93a,"The widespread adoption of Online Social Networks (OSNs), the ever-increasing amount of information produced by their users, and the corresponding capacity to influence markets, politics, and society, have led both industrial and academic researchers to focus on how such systems could be influenced. While previous work has mainly focused on measuring current influential users, contents, or pages on the overall OSNs, the problem of predicting influencers in OSNs has remained relatively unexplored from a research perspective. Indeed, one of the main characteristics of OSNs is the ability of users to create different groups types, as well as to join groups defined by other users, in order to share information and opinions. In this article, we formulate the Influencers Prediction problem in the context of groups created in OSNs, and we define a general framework and an effective methodology to predict which users will be able to influence the behavior of the other ones in a future time period, based on historical interactions that occurred within the group. Our contribution, while rooted in solid rationale and established analytical tools, is also supported by an extensive experimental campaign. We investigate the accuracy of the predictions collecting data concerning the interactions among about 800,000 users from 18 Facebook groups belonging to different categories (i.e., News, Education, Sport, Entertainment, and Work). The achieved results show the quality and viability of our approach. For instance, we are able to predict, on average, for each group, around a third of what an ex-post analysis will show being the 10 most influential members of that group. While our contribution is interesting on its own and - to the best of our knowledge - unique, it is worth noticing that it also paves the way for further research in this field.  © 2021 Copyright held by the owner/author(s).",behavior analysis; centrality measures; Influencer prediction; online social network,Forecasting; Online systems; Amount of information; Analytical tool; Ex post analysis; Experimental campaign; Influential users; On-line social networks; Online social networks (OSNs); Prediction problem; Social networking (online)
Neural Networks for Entity Matching: A Survey,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105465624&doi=10.1145%2f3442200&partnerID=40&md5=e995f84d3612e69d81eda8da9d1321a8,"Entity matching is the problem of identifying which records refer to the same real-world entity. It has been actively researched for decades, and a variety of different approaches have been developed. Even today, it remains a challenging problem, and there is still generous room for improvement. In recent years, we have seen new methods based upon deep learning techniques for natural language processing emerge. In this survey, we present how neural networks have been used for entity matching. Specifically, we identify which steps of the entity matching process existing work have targeted using neural networks, and provide an overview of the different techniques used at each step. We also discuss contributions from deep learning in entity matching compared to traditional methods, and propose a taxonomy of deep neural networks for entity matching.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data matching; Deep learning; entity matching; entity resolution; record linkage,Deep learning; Deep neural networks; Learning systems; Natural language processing systems; Surveys; Entity matching; Learning techniques; NAtural language processing; Real-world entities; Neural networks
Parallel Greedy Algorithm to Multiple Influence Maximization in Social Network,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105514730&doi=10.1145%2f3442341&partnerID=40&md5=8b8f43ba66e04a8f8412a8f51ff34428,"Influence Maximization (IM) problem is to select influential users to maximize the influence spread, which plays an important role in many real-world applications such as product recommendation, epidemic control, and network monitoring. Nowadays multiple kinds of information can propagate in online social networks simultaneously, but current literature seldom discuss about this phenomenon. Accordingly, in this article, we propose Multiple Influence Maximization (MIM) problem where multiple information can propagate in a single network with different propagation probabilities. The goal of MIM problems is to maximize the overall accumulative influence spreads of different information with the limit of seed budget . To solve MIM problems, we first propose a greedy framework to solve MIM problems which maintains an -approximate ratio. We further propose parallel algorithms based on semaphores, an inter-thread communication mechanism, which significantly improves our algorithms efficiency. Then we conduct experiments for our framework using complex social network datasets with 12k, 154k, 317k, and 1.1m nodes, and the experimental results show that our greedy framework outperforms other heuristic algorithms greatly for large influence spread and parallelization of algorithms reduces running time observably with acceptable memory overhead.  © 2021 Association for Computing Machinery.",Influence maximization; parallel algorithm; social network,Budget control; Disease control; Heuristic algorithms; Large dataset; Communication mechanisms; Complex social networks; Influence maximizations; Network Monitoring; On-line social networks; Parallelization of algorithms; Product recommendation; Propagation probabilities; Social networking (online)
Multi-View Collaborative Network Embedding,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105497231&doi=10.1145%2f3441450&partnerID=40&md5=dcd1ba7ee77ddad3bc90ee503af69bec,"Real-world networks often exist with multiple views, where each view describes one type of interaction among a common set of nodes. For example, on a video-sharing network, while two user nodes are linked, if they have common favorite videos in one view, then they can also be linked in another view if they share common subscribers. Unlike traditional single-view networks, multiple views maintain different semantics to complement each other. In this article, we propose Multi-view collAborative Network Embedding (MANE), a multi-view network embedding approach to learn low-dimensional representations. Similar to existing studies, MANE hinges on diversity and collaboration - while diversity enables views to maintain their individual semantics, collaboration enables views to work together. However, we also discover a novel form of second-order collaboration that has not been explored previously, and further unify it into our framework to attain superior node representations. Furthermore, as each view often has varying importance w.r.t. different nodes, we propose MANE, an attention-based extension of MANE, to model node-wise view importance. Finally, we conduct comprehensive experiments on three public, real-world multi-view networks, and the results demonstrate that our models consistently outperform state-of-the-art approaches.  © 2021 Association for Computing Machinery.",Multi-view networks; network embedding,Semantics; Collaborative network; Low-dimensional representation; Multiple views; Network embedding; Real-world networks; Second orders; State-of-the-art approach; Video sharing; Embeddings
Efficient and High-Quality Seeded Graph Matching: Employing Higher-order Structural Information,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105506884&doi=10.1145%2f3442340&partnerID=40&md5=509873033bb05330eac73e5b12cf75ba,"Driven by many real applications, we study the problem of seeded graph matching. Given two graphs and , and a small set of pre-matched node pairs where and , the problem is to identify a matching between and growing from , such that each pair in the matching corresponds to the same underlying entity. Recent studies on efficient and effective seeded graph matching have drawn a great deal of attention and many popular methods are largely based on exploring the similarity between local structures to identify matching pairs. While these recent techniques work provably well on random graphs, their accuracy is low over many real networks. In this work, we propose to utilize higher-order neighboring information to improve the matching accuracy and efficiency. As a result, a new framework of seeded graph matching is proposed, which employs Personalized PageRank (PPR) to quantify the matching score of each node pair. To further boost the matching accuracy, we propose a novel postponing strategy, which postpones the selection of pairs that have competitors with similar matching scores. We show that the postpone strategy indeed significantly improves the matching accuracy. To improve the scalability of matching large graphs, we also propose efficient approximation techniques based on algorithms for computing PPR heavy hitters. Our comprehensive experimental studies on large-scale real datasets demonstrate that, compared with state-of-the-art approaches, our framework not only increases the precision and recall both by a significant margin but also achieves speed-up up to more than one order of magnitude.  © 2021 Association for Computing Machinery.",Graph matching; higher-order structural information; PageRank; percolation,Approximation algorithms; Graph theory; Large dataset; Pattern matching; Approximation techniques; Local structure; Neighboring information; Personalized PageRank; Precision and recall; Real applications; State-of-the-art approach; Structural information; Graph algorithms
Reducing Cumulative Errors of Incremental CP Decomposition in Dynamic Online Social Networks,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105517660&doi=10.1145%2f3441645&partnerID=40&md5=9c9233f74c3e5a4697a6820addc7c98f,"CANDECOMP/PARAFAC (CP) decomposition is widely used in various online social network (OSN) applications. However, it is inefficient when dealing with massive and incremental data. Some incremental CP decomposition (ICP) methods have been proposed to improve the efficiency and process evolving data, by updating decomposition results according to the newly added data. The ICP methods are efficient, but inaccurate because of serious error accumulation caused by approximation in the incremental updating. To promote the wide use of ICP, we strive to reduce its cumulative errors while keeping high efficiency. We first differentiate all possible errors in ICP into two types: the cumulative reconstruction error and the prediction error. Next, we formulate two optimization problems for reducing the two errors. Then, we propose several restarting strategies to address the two problems. Finally, we test the effectiveness in three typical dynamic OSN applications. To the best of our knowledge, this is the first work on reducing the cumulative errors of the ICP methods in dynamic OSNs.  © 2021 Association for Computing Machinery.",Cumulative errors; dynamic OSNs; incremental CP decomposition; restarting methods,Efficiency; Errors; CANDECOMP/PARAFAC; Error accumulation; Incremental updating; On-line social networks; Online social networks (OSN); Optimization problems; Prediction errors; Reconstruction error; Social networking (online)
Scholar2vec: Vector Representation of Scholars for Lifetime Collaborator Prediction,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105489914&doi=10.1145%2f3442199&partnerID=40&md5=537366a40c7dad67c3d91eb8f9f3c86b,"While scientific collaboration is critical for a scholar, some collaborators can be more significant than others, e.g., lifetime collaborators. It has been shown that lifetime collaborators are more influential on a scholar's academic performance. However, little research has been done on investigating predicting such special relationships in academic networks. To this end, we propose Scholar2vec, a novel neural network embedding for representing scholar profiles. First, our approach creates scholars' research interest vector from textual information, such as demographics, research, and influence. After bridging research interests with a collaboration network, vector representations of scholars can be gained with graph learning. Meanwhile, since scholars are occupied with various attributes, we propose to incorporate four types of scholar attributes for learning scholar vectors. Finally, the early-stage similarity sequence based on Scholar2vec is used to predict lifetime collaborators with machine learning methods. Extensive experiments on two real-world datasets show that Scholar2vec outperforms state-of-the-art methods in lifetime collaborator prediction. Our work presents a new way to measure the similarity between two scholars by vector representation, which tackles the knowledge between network embedding and academic relationship mining.  © 2021 Association for Computing Machinery.",academic information retrieval; graph learning; Network embedding; scientific collaboration,Embeddings; Encoding (symbols); Forecasting; Vectors; Academic performance; Collaboration network; Machine learning methods; Novel neural network; Real-world datasets; Scientific collaboration; State-of-the-art methods; Vector representations; Learning systems
Demarcating Endogenous and Exogenous Opinion Dynamics: An Experimental Design Approach,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130095041&doi=10.1145%2f3449361&partnerID=40&md5=50f2c31615af5d545e3a6d9e65d1f11f,"The networked opinion diffusion in online social networks is often governed by the two genres of opinions-endogenous opinions that are driven by the influence of social contacts among users, and exogenous opinions which are formed by external effects like news and feeds. Accurate demarcation of endogenous and exogenous messages offers an important cue to opinion modeling, thereby enhancing its predictive performance. In this article, we design a suite of unsupervised classification methods based on experimental design approaches, in which, we aim to select the subsets of events which minimize different measures of mean estimation error. In more detail, we first show that these subset selection tasks are NP-Hard. Then we show that the associated objective functions are weakly submodular, which allows us to cast efficient approximation algorithms with guarantees. Finally, we validate the efficacy of our proposal on various real-world datasets crawled from Twitter as well as diverse synthetic datasets. Our experiments range from validating prediction performance on unsanitized and sanitized events to checking the effect of selecting optimal subsets of various sizes. Through various experiments, we have found that our method offers a significant improvement in accuracy in terms of opinion forecasting, against several competitors.  © 2021 Copyright held by the owner/author(s).",Opinion dynamics; Robust inference; Submodularity; Subset selection; Temporal point process,Approximation algorithms; Data mining; Design; Set theory; Statistics; Experimental design approaches; Opinion diffusions; Opinion dynamics; Point process; Predictive performance; Robust inference; Social contacts; Submodularity; Subset selection; Temporal point process; Social networking (online)
Unbiased measurement of feature importance in tree-based methods,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103967347&doi=10.1145%2f3429445&partnerID=40&md5=dc37f4eb14c3bab39092673b64e42cc5,"We propose a modification that corrects for split-improvement variable importance measures in Random Forests and other tree-based methods. These methods have been shown to be biased towards increasing the importance of features with more potential splits. We show that by appropriately incorporating split-improvement as measured on out of sample data, this bias can be corrected yielding better summaries and screening tools. © 2021 ACM.",Feature importance; Tree-based methods; Unbiasedness,Decision trees; Sample data; Screening tool; Tree-based methods; Variable importances; Screening
Stacked convolutional sparse auto-encoders for representation learning,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103962693&doi=10.1145%2f3434767&partnerID=40&md5=598a2d6251a1ccf9a6a62d70b43d4d6d,"Deep learning seeks to achieve excellent performance for representation learning in image datasets. However, supervised deep learning models such as convolutional neural networks require a large number of labeled image data, which is intractable in applications, while unsupervised deep learning models like stacked denoising auto-encoder cannot employ label information. Meanwhile, the redundancy of image data incurs performance degradation on representation learning for aforementioned models. To address these problems, we propose a semi-supervised deep learning framework called stacked convolutional sparse auto-encoder, which can learn robust and sparse representations from image data with fewer labeled data records. More specifically, the framework is constructed by stacking layers. In each layer, higher layer feature representations are generated by features of lower layers in a convolutional way with kernels learned by a sparse auto-encoder. Meanwhile, to solve the data redundance problem, the algorithm of Reconstruction Independent Component Analysis is designed to train on patches for sphering the input data. The label information is encoded using a Softmax Regression model for semi-supervised learning. With this framework, higher level representations are learned by layers mapping from image data. It can boost the performance of the base subsequent classifiers such as support vector machines. Extensive experiments demonstrate the superior classification performance of our framework compared to several state-of-the-art representation learning methods. © 2021 ACM.",Representation learning; Sparse auto-encoder,Convolution; Convolutional neural networks; Deep learning; Independent component analysis; Regression analysis; Semi-supervised learning; Signal encoding; Support vector machines; Classification performance; Feature representation; Label information; Learning frameworks; Performance degradation; Softmax regressions; Sparse representation; State of the art; Learning systems
HARP: A novel hierarchical attention model for relation prediction,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103393314&doi=10.1145%2f3424673&partnerID=40&md5=29a6299902d07784b71f266c3275173b,"Recent years have witnessed great advancement of representation learning (RL)-based models for the knowledge graph relation prediction task. However, they generally rely on structure information embedded in the encyclopedic knowledge graph, while the beneficial semantic information provided by lexical knowledge graph is ignored, leading the problem of shallow understanding and coarse-grained analysis for knowledge acquisition. Therefore, this article introduces concept information derived from the lexical knowledge graph (e.g., Probase), and proposes a novel Hierarchical Attention model for Relation Prediction, which consists of entity-level attention mechanism and concept-level attention mechanism, to throughly integrate multiple semantic signals. Experimental results demonstrate the efficiency of the proposed method on two benchmark datasets. © 2021 ACM.",Concept information; Hierarchical attention model; Knowledge graph; Relation prediction; Representation learning,Knowledge acquisition; Knowledge representation; Semantics; Attention mechanisms; Benchmark datasets; Encyclopedic knowledge; Knowledge graphs; Lexical knowledge; Prediction tasks; Semantic information; Structure information; Forecasting
Core interest network for click-through rate prediction,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103985290&doi=10.1145%2f3428079&partnerID=40&md5=8d0957a5c7c79f0a51124887f2a301eb,"In modern online advertising systems, the click-through rate (CTR) is an important index to measure the popularity of an item. It refers to the ratio of users who click on a specific advertisement to the number of total users who view it. Predicting the CTR of an item in advance can improve the accuracy of the advertisement recommendation. And it is commonly calculated based on users' interests. Thus, extracting users' interests is of great importance in CTR prediction tasks. In the literature, a lot of studies treat the interaction between users and items as sequential data and apply the recurrent neural network (RNN) model to extract users' interests. However, these solutions cannot handle the case when the sequence length is relatively long, e.g., over 100. This is because of the vanishing gradient problem of RNN, i.e., the model cannot learn a users' previous behaviors that are too far away from the current moment. To address this problem, we propose a new Core Interest Network (CIN) model to mitigate the problem of a long sequence in the CTR prediction task with sequential data. In brief, we first extract the core interests of users and then use the refined data as the input of subsequent learning tasks. Extensive evaluations on real dataset show that our CIN model can outperform the state-of-the-art solutions in terms of prediction accuracy. © 2021 ACM.",Computational advertising; CTR prediction; Sequential recommendation; Time series prediction; User portrait,Clustering algorithms; Data mining; Forecasting; Online systems; Click-through rate; Online advertising; Prediction accuracy; Recurrent neural network (RNN); Sequence lengths; State of the art; Users' interests; Vanishing gradient; Recurrent neural networks
Knowledge graph embedding for link prediction: A comparative analysis,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103382496&doi=10.1145%2f3424672&partnerID=40&md5=045367082d20a728bb070d953eff3ca2,"Knowledge Graphs (KGs) have found many applications in industrial and in academic settings, which in turn, have motivated considerable research efforts towards large-scale information extraction from a variety of sources. Despite such efforts, it is well known that even the largest KGs suffer from incompleteness; Link Prediction (LP) techniques address this issue by identifying missing facts among entities already in the KG. Among the recent LP techniques, those based on KG embeddings have achieved very promising performance in some benchmarks. Despite the fast-growing literature on the subject, insufficient attention has been paid to the effect of the design choices in those methods. Moreover, the standard practice in this area is to report accuracy by aggregating over a large number of test facts in which some entities are vastly more represented than others; this allows LP methods to exhibit good results by just attending to structural properties that include such entities, while ignoring the remaining majority of the KG. This analysis provides a comprehensive comparison of embedding-based LP methods, extending the dimensions of analysis beyond what is commonly available in the literature. We experimentally compare the effectiveness and efficiency of 18 state-of-the-art methods, consider a rule-based baseline, and report detailed analysis over the most popular benchmarks in the literature. © 2021 ACM.",Comparative analysis; Knowledge graph embeddings; Knowledge graphs; Link prediction,Embeddings; Industrial research; Knowledge representation; Comparative analysis; Comprehensive comparisons; Effectiveness and efficiencies; Knowledge graphs; Link prediction; Research efforts; Standard practices; State-of-the-art methods; Benchmarking
An exponential factorization machine with percentage error minimization to retail sales forecasting,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103988950&doi=10.1145%2f3426238&partnerID=40&md5=632ab7c00932453cbefb9823d7952c8e,"This article proposes a new approach to sales forecasting for new products (stock-keeping units [SKUs]) with long lead time but short product life cycle. These SKUs are usually sold for one season only, without any replenishments. An exponential factorization machine (EFM) sales forecast model is developed to solve this problem which not only takes into account SKU attributes, but also pairwise interactions. The EFM model is significantly different from the original Factorization Machines (FM) from two fold: (1) the attribute-level formulation for explanatory/input variables; and (2) exponential formulation for the positive response/output/target variable. The attribute-level formation excludes infeasible intra-attribute interactions and results in more efficient feature engineering comparing with the conventional one-hot encoding, while the exponential formulation is demonstrated more effective than the log-transformation for the positive but not skewed distributed responses. In order to estimate the parameters, percentage error squares (PES) and error squares (ES) are minimized by a proposed adaptive batch gradient descent method over the training set. To overcome the over-fitting problem, a greedy forward stepwise feature selection method is proposed to select the most useful attributes and interactions. Real-world data provided by a footwear retailer in Singapore are used for testing the proposed approach. The forecasting performance in terms of both mean absolute percentage error (MAPE) and mean absolute error (MAE) compares favorably with not only off-the-shelf models but also results reported by extant sales and demand forecasting studies. The effectiveness of the proposed approach is also demonstrated by two external public datasets. Moreover, we prove the theoretical relationships between PES and ES minimization, and present an important property of the PES minimization for regression models; that it trains models to underestimate data. This property fits the situation of sales forecasting where unit-holding cost is much greater than the unit-shortage cost (e.g., perishable products). © 2021 ACM.",Factorization machine; Forecasting; Percentage error minimization; Retail sales,Errors; Factorization; Forecasting; Gradient methods; Life cycle; Regression analysis; State assignment; Attribute interactions; Factorization machines; Feature selection methods; Forecasting performance; Gradient Descent method; Mean absolute percentage error; Over fitting problem; Pairwise interaction; Sales
Heterogeneous network approach to predict individuals' mental health,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103981475&doi=10.1145%2f3429446&partnerID=40&md5=8a54113e104fe93339a53ed56e1e13ce,"Depression and anxiety are critical public health issues affecting millions of people around the world. To identify individuals who are vulnerable to depression and anxiety, predictive models have been built that typically utilize data from one source. Unlike these traditional models, in this study, we leverage a rich heterogeneous dataset from the University of Notre Dame's NetHealth study that collected individuals' (student participants') social interaction data via smartphones, health-related behavioral data via wearables (Fitbit), and trait data from surveys. To integrate the different types of information, we model the NetHealth data as a heterogeneous information network (HIN). Then, we redefine the problem of predicting individuals' mental health conditions (depression or anxiety) in a novel manner, as applying to our HIN a popular paradigm of a recommender system (RS), which is typically used to predict the preference that a person would give to an item (e.g., a movie or book). In our case, the items are the individuals' different mental health states. We evaluate four state-of-the-art RS approaches. Also, we model the prediction of individuals' mental health as another problem type - that of node classification (NC) in our HIN, evaluating in the process four node features under logistic regression as a proof-of-concept classifier. We find that our RS and NC network methods produce more accurate predictions than a logistic regression model using the same NetHealth data in the traditional non-network fashion as well as a random-approach. Also, we find that the best of the considered RS approaches outperforms all considered NC approaches. This is the first study to integrate smartphone, wearable sensor, and survey data in a HIN manner and use RS or NC on the HIN to predict individuals' mental health conditions. © 2021 ACM.",Heterogeneous information networks; Mental health prediction,Forecasting; Health; Heterogeneous networks; Information services; Logistic regression; Smartphones; Surveys; Wearable technology; Accurate prediction; Heterogeneous information; Logistic Regression modeling; Predictive models; Public health issues; Social interactions; Traditional models; University of Notre Dame; Predictive analytics
Incremental Community Detection on Large Complex Attributed Network,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130095663&doi=10.1145%2f3451216&partnerID=40&md5=b2b71b502629d2570c754f0eb8fb9ca8,"Community detection on network data is a fundamental task, and has many applications in industry. Network data in industry can be very large, with incomplete and complex attributes, and more importantly, growing. This calls for a community detection technique that is able to handle both attribute and topological information on large scale networks, and also is incremental. In this article, we propose inc-AGGMMR, an incremental community detection framework that is able to effectively address the challenges that come from scalability, mixed attributes, incomplete values, and evolving of the network. Through construction of augmented graph, we map attributes into the network by introducing attribute centers and belongingness edges. The communities are then detected by modularity maximization. During this process, we adjust the weights of belongingness edges to balance the contribution between attribute and topological information to the detection of communities. The weight adjustment mechanism enables incremental updates of community membership of all vertices. We evaluate inc-AGGMMR on five benchmark datasets against eight strong baselines. We also provide a case study to incrementally detect communities on a PayPal payment network which contains users with transactions. The results demonstrate inc-AGGMMR's effectiveness and practicability.  © 2021 Association for Computing Machinery.",Attributed network; Community detection; Payment network; Social network,Complex networks; Social networking (online); Topology; Attribute information; Attributed network; Augmented graph; Community detection; Detection framework; Large-scale network; Network data; Payment network; Social network; Topological information; Population dynamics
3DGCN: 3-Dimensional Dynamic Graph Convolutional Network for Citywide Crowd Flow Prediction,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126979070&doi=10.1145%2f3451394&partnerID=40&md5=22b9c4d36690dbe409796715df7ec255,"Crowd flow prediction is an essential task benefiting a wide range of applications for the transportation system and public safety. However, it is a challenging problem due to the complex spatio-Temporal dependence and the complicated impact of urban structure on the crowd flow patterns. In this article, we propose a novel framework, 3-Dimensional Graph Convolution Network (3DGCN), to predict citywide crowd flow. We first model it as a dynamic spatio-Temporal graph prediction problem, where each node represents a region with time-varying flows, and each edge represents the origin-destination (OD) flow between its corresponding regions. As such, OD flows among regions are treated as a proxy for the spatial interactions among regions. To tackle the complex spatio-Temporal dependence, our proposed 3DGCN can model the correlation among graph spatial and temporal neighbors simultaneously. To learn and incorporate urban structures in crowd flow prediction, we design the GCN aggregator to be learned from both crowd flow prediction and region function inference at the same time. Extensive experiments with real-world datasets in two cities demonstrate that our model outperforms state-of-The-Art baselines by 9.6%g1/419.5% for the next-Time-interval prediction.  © 2021 Association for Computing Machinery.",Graph neural network; Traffic flow prediction; Urban computing,Complex networks; Flow graphs; Forecasting; Graph neural networks; 3-dimensional; Crowd flows; Flow prediction; Graph neural networks; Origin-destination flows; Spatio-temporal; Temporal dependence; Traffic flow prediction; Urban computing; Urban structure; Convolution
Identifying linear models in multi-resolution population data using minimum description length principle to predict household income,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103951334&doi=10.1145%2f3424670&partnerID=40&md5=088e70ad5189ad91bb97ad116ece60f8,"One shirt size cannot fit everybody, while we cannot make a unique shirt that fits perfectly for everyone because of resource limitations. This analogy is true for policy making as well. Policy makers cannot make a single policy to solve all problems for all regions because each region has its own unique issue. At the other extreme, policy makers also cannot make a policy for each small village due to resource limitations. Would it be better if we can find a set of largest regions such that the population of each region within this set has common issues and we can make a single policy for them? In this work, we propose a framework using regression analysis and Minimum Description Length (MDL) to find a set of largest areas that have common indicators, which can be used to predict household incomes efficiently. Given a set of household features, and a multi-resolution partition that represents administrative divisions, our framework reports a set C∗ of largest subdivisions that have a common predictive model for population-income prediction. We formalize the problem of finding C∗ and propose an algorithm that can find C∗ correctly. We use both simulation datasets as well as a real-world dataset of Thailand's population household information to demonstrate our framework performance and application. The results show that our framework performance is better than the baseline methods. Moreover, we demonstrate that the results of our method can be used to find indicators of income prediction for many areas in Thailand. By adjusting these indicator values via policies, we expect people in these areas to gain more incomes. Hence, the policy makers will be able to make policies by using these indicators in our results as a guideline to solve low-income issues. Our framework can be used to support policy makers in making policies regarding any other dependent variable beyond income in order to combat poverty and other issues. We provide the R package, MRReg, which is the implementation of our framework in the R language. TheMRReg package comes with a documentation for anyone who is interested in analyzing linear regression on multi-resolution population data. © 2021 Owner/Author.",Minimum description length; Model selection; Multi-resolution data; Population data; Regression analysis,Decision making; Forecasting; Predictive analytics; Dependent variables; Household features; Household income; Minimum description length; Minimum description length principle; Performance and applications; Predictive modeling; Resource limitations; Population statistics
Joint transferable dictionary learning and view adaptation for multi-view human action recognition,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103924945&doi=10.1145%2f3434746&partnerID=40&md5=207011563c4b4ba89b3252a9a64d70b7,"Multi-view human action recognition remains a challenging problem due to large view changes. In this article, we propose a transfer learning-based framework called transferable dictionary learning and view adaptation (TDVA) model for multi-view human action recognition. In the transferable dictionary learning phase, TDVA learns a set of view-specific transferable dictionaries enabling the same actions from different views to share the same sparse representations, which can transfer features of actions from different views to an intermediate domain. In the view adaptation phase, TDVA comprehensively analyzes global, local, and individual characteristics of samples, and jointly learns balanced distribution adaptation, locality preservation, and discrimination preservation, aiming at transferring sparse features of actions of different views from the intermediate domain to a common domain. In other words, TDVA progressively bridges the distribution gap among actions from various views by these two phases. Experimental results on IXMAS, ACT42, and NUCLA action datasets demonstrate that TDVA outperforms state-of-the-art methods. © 2021 ACM.",Action recognition; Multi-view; Sparse representation; Transfer learning,Computer science; Data mining; Dictionary learning; Human-action recognition; Individual characteristics; Multi-views; Sparse features; Sparse representation; State-of-the-art methods; Transfer learning
Generating artificial outliers in the absence of genuine ones - A survey,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103955785&doi=10.1145%2f3447822&partnerID=40&md5=cbbcacf8344c7f0c8f6f757201017f57,"By definition, outliers are rarely observed in reality, making them difficult to detect or analyze. Artificial outliers approximate such genuine outliers and can, for instance, help with the detection of genuine outliers or with benchmarking outlier-detection algorithms. The literature features different approaches to generate artificial outliers. However, systematic comparison of these approaches remains absent. This surveys and compares these approaches. We start by clarifying the terminology in the field, which varies from publication to publication, and we propose a general problem formulation. Our description of the connection of generating outliers to other research fields like experimental design or generative models frames the field of artificial outliers. Along with offering a concise description, we group the approaches by their general concepts and how they make use of genuine instances. An extensive experimental study reveals the differences between the generation approaches when ultimately being used for outlier detection. This survey shows that the existing approaches already cover a wide range of concepts underlying the generation, but also that the field still has potential for further development. Our experimental study does confirm the expectation that the quality of the generation approaches varies widely, for example, in terms of the dataset they are used on. Ultimately, to guide the choice of the generation approach in a specific context, we propose an appropriate general-decision process. In summary, this survey comprises, describes, and connects all relevant work regarding the generation of artificial outliers and may serve as a basis to guide further research in the field. © 2021 ACM.",Anomalies; Artificial data; Artificial outlier; Outlier detection,Anomaly detection; Data handling; Surveys; Decision process; Generative model; Outlier detection algorithm; Problem formulation; Research fields; Statistics
Sampling sparse representations with randomized measurement langevin dynamics,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103959900&doi=10.1145%2f3427585&partnerID=40&md5=9b2e739c85a22be5dc517bde2bcf6183,"Stochastic Gradient Langevin Dynamics (SGLD) have been widely used for Bayesian sampling from certain probability distributions, incorporating derivatives of the log-posterior. With the derivative evaluation of the log-posterior distribution, SGLD methods generate samples from the distribution through performing as a thermostats dynamics that traverses over gradient flows of the log-posterior with certainly controllable perturbation. Even when the density is not known, existing solutions still can first learn the kernel density models from the given datasets, then produce new samples using the SGLD over the kernel density derivatives. In this work, instead of exploring new samples from kernel spaces, a novel SGLD sampler, namely, Randomized Measurement Langevin Dynamics (RMLD) is proposed to sample the high-dimensional sparse representations from the spectral domain of a given dataset. Specifically, given a random measurement matrix for sparse coding, RMLD first derives a novel likelihood evaluator of the probability distribution from the loss function of LASSO, then samples from the high-dimensional distribution using stochastic Langevin dynamics with derivatives of the logarithm likelihood and Metropolis-Hastings sampling. In addition, new samples in low-dimensional measuring spaces can be regenerated using the sampled high-dimensional vectors and the measurement matrix. The algorithm analysis shows that RMLD indeed projects a given dataset into a high-dimensional Gaussian distribution with Laplacian prior, then draw new sparse representation from the dataset through performing SGLD over the distribution. Extensive experiments have been conducted to evaluate the proposed algorithm using real-world datasets. The performance comparisons on three real-world applications demonstrate the superior performance of RMLD beyond baseline methods. © 2021 ACM.",Compressive sensing; Hamiltonian Monte Carlo; LASSO,Dynamics; Regression analysis; Stochastic systems; Vector spaces; Derivative evaluation; Measurement matrix; Metropolis-Hastings samplings; Performance comparison; Posterior distributions; Real-world datasets; Sparse representation; Stochastic gradient; Probability distributions
High-order structure exploration on massive graphs: A local graph clustering perspective,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103941436&doi=10.1145%2f3425637&partnerID=40&md5=40775316730d33a95c19d5e6855ed9a0,"Modeling and exploring high-order connectivity patterns, also called network motifs, are essential for understanding the fundamental structures that control and mediate the behavior of many complex systems. For example, in social networks, triangles have been proven to play the fundamental role in understanding social network communities; in online transaction networks, detecting directed looped transactions helps identify money laundering activities; in personally identifiable information networks, the star-shaped structures may correspond to a set of synthetic identities. Despite the ubiquity of such high-order structures, many existing graph clustering methods are either not designed for the high-order connectivity patterns, or suffer from the prohibitive computational cost when modeling high-order structures in the large-scale networks. This article generalizes the challenges in multiple dimensions. First (Model), we introduce the notion of high-order conductance, and define the high-order diffusion core, which is based on a high-order random walk induced by the user-specified high-order network structure. Second (Algorithm), we propose a novel high-order structure-preserving graph clustering framework named HOSGRAP, which partitions the graph into structure-rich clusters in polylogarithmic time with respect to the number of edges in the graph. Third (Generalization), we generalize our proposed algorithm to solve the real-world problems on various types of graphs, such as signed graphs, bipartite graphs, and multi-partite graphs. Experimental results on both synthetic and real graphs demonstrate the effectiveness and efficiency of the proposed algorithms. © 2021 ACM.",High-order network structure; Local clustering algorithm,Clustering algorithms; Graph algorithms; Graphic methods; Information services; Social networking (online); Connectivity pattern; Effectiveness and efficiencies; Fundamental structures; High-order structure; Multiple dimensions; Network communities; Personally identifiable information; Polylogarithmic time; Graph structures
Online Tensor-Based Learning Model for Structural Damage Detection,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130097663&doi=10.1145%2f3451217&partnerID=40&md5=4c2eaa719d4569fd7c70f198082a2327,"The online analysis of multi-way data stored in a tensor has become an essential tool for capturing the underlying structures and extracting the sensitive features that can be used to learn a predictive model. However, data distributions often evolve with time and a current predictive model may not be sufficiently representative in the future. Therefore, incrementally updating the tensor-based features and model coefficients are required in such situations. A new efficient tensor-based feature extraction, named Nesterov Stochastic Gradient Descent (NeSGD), is proposed for online (CP) decomposition. According to the new features obtained from the resultant matrices of NeSGD, a new criterion is triggered for the updated process of the online predictive model. Experimental evaluation in the field of structural health monitoring using laboratory-based and real-life structural datasets shows that our methods provide more accurate results compared with existing online tensor analysis and model learning. The results showed that the proposed methods significantly improved the classification error rates, were able to assimilate the changes in the positive data distribution over time, and maintained a high predictive accuracy in all case studies.  © 2021 Association for Computing Machinery.",Anomaly detection; Incremental learning; One-class support vector machine; Online learning; Structural health monitoring,Anomaly detection; Damage detection; E-learning; Gradient methods; Stochastic systems; Structural health monitoring; Tensors; Anomaly detection; Data distribution; Incremental learning; Learning models; One-class support vector machine; Online learning; Predictive models; Stochastic gradient descent; Structural damage detection; Support vectors machine; Support vector machines
Recommending statutes: A portable method based on neural networks,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103931351&doi=10.1145%2f3424671&partnerID=40&md5=c0ccab6e28dd9ed011385a49ea6a3881,"Legal judgment prediction, which aims at predicting judgment results such as penalty, charges, and statutes for cases, has attracted much attention recently. In this article, we focus on building a recommender system to predict the associated statutes for a case given the facts of the case as input. For this purpose, we propose a two-step neural network-based machine learning framework to assist judges as well as ordinary people to reduce their effort in finding applicable statutes. The proposed model takes advantage of recurrent neural networks with a max-pooling layer to obtain contextual representations of documents, i.e., the facts associated with the cases. Moreover, an attention mechanism is used to automatically focus on the important words contributing to the prediction of statutes. In addition, we apply an encoder - decoder ranking approach to extract correlations between statutes to achieve more accurate recommendation results. We evaluate our model on a real-world dataset. Experimental results show that, compared with existing baseline methods, our method can predict statutes that are more likely to appear in real judgments. © 2021 ACM.",Learn to rank; Multi-label classification; Neural networks; Recommender system; Semantic representation; Statute recommendation,Forecasting; Multilayer neural networks; Attention mechanisms; Baseline methods; Encoder-decoder; Max-pooling; Ordinary people; Ranking approach; Real-world; Recurrent neural networks
Context-based evaluation of dimensionality reduction algorithms'experiments and statistical significance analysis,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103973301&doi=10.1145%2f3428077&partnerID=40&md5=0c0dd77e338907e27f94a9ce50a41d52,"Dimensionality reduction is a commonly used technique in data analytics. Reducing the dimensionality of datasets helps not only with managing their analytical complexity but also with removing redundancy. Over the years, several such algorithms have been proposed with their aims ranging from generating simple linear projections to complex non-linear transformations of the input data. Subsequently, researchers have defined several quality metrics in order to evaluate the performances of different algorithms. Hence, given a plethora of dimensionality reduction algorithms and metrics for their quality analysis, there is a long-existing need for guidelines on how to select the most appropriate algorithm in a given scenario. In order to bridge this gap, in this article, we have compiled 12 state-of-the-art quality metrics and categorized them into 5 identified analytical contexts. Furthermore, we assessed 15 most popular dimensionality reduction algorithms on the chosen quality metrics using a large-scale and systematic experimental study. Later, using a set of robust non-parametric statistical tests, we assessed the generalizability of our evaluation on 40 real-world datasets. Finally, based on our results, we present practitioners' guidelines for the selection of an appropriate dimensionally reduction algorithm in the present analytical contexts. © 2021 ACM.",Context-based evaluation; Dimensionality reduction; Quality metrics; Statistical significance analysis,Data Analytics; Data reduction; Dimensionality reduction; Large scale systems; Linear transformations; Mathematical transformations; Analytical context; Dimensionality reduction algorithms; Dimensionally reduction; Linear projections; Non-linear transformations; Non-parametric statistical tests; Real-world datasets; Statistical significance; Quality control
Hierarchical Concept-Driven Language Model,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130097821&doi=10.1145%2f3451167&partnerID=40&md5=59c5ce38fd3cd8aa61285dc4be959abc,"For guiding natural language generation, many semantic-driven methods have been proposed. While clearly improving the performance of the end-To-end training task, these existing semantic-driven methods still have clear limitations: for example, (i) they only utilize shallow semantic signals (e.g., from topic models) with only a single stochastic hidden layer in their data generation process, which suffer easily from noise (especially adapted for short-Text etc.) and lack of interpretation; (ii) they ignore the sentence order and document context, as they treat each document as a bag of sentences, and fail to capture the long-distance dependencies and global semantic meaning of a document. To overcome these problems, we propose a novel semantic-driven language modeling framework, which is a method to learn a Hierarchical Language Model and a Recurrent Conceptualization-enhanced Gamma Belief Network, simultaneously. For scalable inference, we develop the auto-encoding Variational Recurrent Inference, allowing efficient end-To-end training and simultaneously capturing global semantics from a text corpus. Especially, this article introduces concept information derived from high-quality lexical knowledge graph Probase, which leverages strong interpretability and anti-nose capability for the proposed model. Moreover, the proposed model captures not only intra-sentence word dependencies, but also temporal transitions between sentences and inter-sentence concept dependence. Experiments conducted on several NLP tasks validate the superiority of the proposed approach, which could effectively infer meaningful hierarchical concept structure of document and hierarchical multi-scale structures of sequences, even compared with latest state-of-The-Art Transformer-based models.  © 2021 Association for Computing Machinery.",Concept semantic information; Hierarchical language modeling; Interpretation; Language modeling; Recurrent conceptualization-enhanced gamma belief network; Representation learning; Text generation,Bayesian networks; Computational linguistics; Modeling languages; Natural language processing systems; Semantic Web; Stochastic models; Stochastic systems; Concept semantic information; End to end; Hierarchical language models; Interpretation; Language model; Natural language generation; Recurrent conceptualization-enhanced gamma belief network; Representation learning; Semantics Information; Text generations; Semantics
Page-Level Main Content Extraction from Heterogeneous Webpages,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127423977&doi=10.1145%2f3451168&partnerID=40&md5=728f26f1f75513770983167794b07982,"The main content of a webpage is often surrounded by other boilerplate elements related to the template, such as menus, advertisements, copyright notices, and comments. For crawlers and indexers, isolating the main content from the template and other noisy information is an essential task, because processing and storing noisy information produce a waste of resources such as bandwidth, storage space, and computing time. Besides, the detection and extraction of the main content is useful in different areas, such as data mining, web summarization, and content adaptation to low resolutions. This work introduces a new technique for main content extraction. In contrast to most techniques, this technique not only extracts text, but also other types of content, such as images, and animations. It is a Document Object Model-based page-level technique, thus it only needs to load one single webpage to extract the main content. As a consequence, it is efficient enough as to be used online (in real-Time). We have empirically evaluated the technique using a suite of real heterogeneous benchmarks producing very good results compared with other well-known content extraction techniques.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Block detection; Content extraction; Information retrieval; Template extraction; Web mining,Digital storage; Extraction; Information retrieval; Websites; Bandwidth storage; Block detection; Computing time; Content extraction; Spacetime; Storage spaces; Template extraction; Waste of resources; Web Mining; Web-page; Data mining
Towards automatic construction of multi-network models for heterogeneous multi-task learning,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103935703&doi=10.1145%2f3434748&partnerID=40&md5=38d95411383eeee89eff9776a5bed360,"Multi-task learning, as it is understood nowadays, consists of using one single model to carry out several similar tasks. From classifying hand-written characters of different alphabets to figuring out how to play several Atari games using reinforcement learning, multi-task models have been able to widen their performance range across different tasks, although these tasks are usually of a similar nature. In this work, we attempt to expand this range even further, by including heterogeneous tasks in a single learning procedure. To do so, we firstly formally define a multi-network model, identifying the necessary components and characteristics to allow different adaptations of said model depending on the tasks it is required to fulfill. Secondly, employing the formal definition as a starting point, we develop an illustrative model example consisting of three different tasks (classification, regression, and data sampling). The performance of this illustrative model is then analyzed, showing its capabilities. Motivated by the results of the analysis, we enumerate a set of open challenges and future research lines over which the full potential of the proposed model definition can be exploited. © 2021 ACM.",Deep neural networks; Multi-task learning; Neural architecture search,Multi-task learning; Reinforcement learning; Automatic construction; Data sampling; Formal definition; Hand-written characters; Learning procedures; Multi-task model; Network modeling; Network models; Learning systems
App2Vec: Context-Aware Application Usage Prediction,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130093905&doi=10.1145%2f3451396&partnerID=40&md5=6df1cd09d93f518b8bad034aec806059,"Both app developers and service providers have strong motivations to understand when and where certain apps are used by users. However, it has been a challenging problem due to the highly skewed and noisy app usage data. Moreover, apps are regarded as independent items in existing studies, which fail to capture the hidden semantics in app usage traces. In this article, we propose App2Vec, a powerful representation learning model to learn the semantic embedding of apps with the consideration of spatio-Temporal context. Based on the obtained semantic embeddings, we develop a probabilistic model based on the Bayesian mixture model and Dirichlet process to capture when, where, and what semantics of apps are used to predict the future usage. We evaluate our model using two different app usage datasets, which involve over 1.7 million users and 2,000+ apps. Evaluation results show that our proposed App2Vec algorithm outperforms the state-of-The-Art algorithms in app usage prediction with a performance gap of over 17.0%.  © 2021 Copyright held by the ownerauthor(s). Publication rights licensed to ACM.",App usage; Graphic models; Representation learning; Spatio-Temporal context,E-learning; Embeddings; Forecasting; App usage; Context aware applications; Graphics modelling; Independent items; Representation learning; Semantic embedding; Service provider; Spatio-temporal; Spatio-temporal context; Usage data; Semantics
The 8M algorithm from today's perspective,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103942655&doi=10.1145%2f3428078&partnerID=40&md5=62f4ae8775aa8261b65581f22c1fb15f,"We provide a detailed analysis and a first complete description of 8M - an old but virtually unknown algorithm for Boolean matrix factorization. Even though the algorithm uses a rather limited insight into the factorization problem from today's perspective, we demonstrate that its performance is reasonably good compared to the currently available algorithms. Our analysis reveals that this is due to certain concepts employed by 8M that are not exploited by the current algorithms. We discuss the prospect of these concepts, utilize them to improve two well-known current factorization algorithms, and, furthermore, propose an improvement of 8M itself, which significantly enhances the performance of the original 8M. Our findings are illustrated by experimental evaluation. © 2021 ACM.",Algorithms; Boolean matrix factorization,Computer science; Data mining; Boolean Matrix; Experimental evaluation; Factorization algorithms; M-algorithms; Factorization
Fast and Robust Dictionary-based Classification for Image Data,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124050703&doi=10.1145%2f3449360&partnerID=40&md5=9008776de80d0787dd70eb753c1e2d86,"Dictionary-based classification has been promising in knowledge discovery from image data, due to its good performance and interpretable theoretical system. Dictionary learning effectively supports both small-And large-scale datasets, while its robustness and performance depends on the atoms of the dictionary most of the time. Empirically, using a large number of atoms is helpful to obtain a robust classification, while robustness cannot be ensured when setting a small number of atoms. However, learning a huge dictionary dramatically slows down the speed of classification, which is especially worse on the large-scale datasets. To address the problem, we propose a Fast and Robust Dictionary-based Classification (FRDC) framework, which fully utilizes the learned dictionary for classification by staging-And-norms to obtain a robust sparse representation. The new objective function, on the one hand, introduces an additional-norm term upon the conventional-norm optimization, which generates a more robust classification. On the other hand, the optimization based on both-And-norms is solved in two stages, which is much easier and faster than current solutions. In this way, even when using a limited size of dictionary, which makes sure the classification runs very fast, it still can gain higher robustness for multiple types of image data. The optimization is then theoretically analyzed in a new formulation, close but distinct to elastic-net, to prove it is crucial to improve the performance under the premise of robustness. According to our extensive experiments conducted on four image datasets for face and object classification, FRDC keeps generating a robust classification no matter whether using a small or large number of atoms. This guarantees a fast and robust dictionary-based image classification. Furthermore, when simply using deep features extracted via some popular pre-Trained neural networks, it outperforms many state-of-The-Art methods on the specific datasets.  © 2021 Association for Computing Machinery.",Dictionary learning; Image classification; Regularization; Sparse representation; SVD,Atoms; Classification (of information); Image representation; Large dataset; Dictionary learning; Image data; Images classification; Large-scale datasets; Optimisations; Performance; Regularisation; Robust classification; Sparse representation; Theoretical system; Image classification
Noise corrected sampling of online social networks,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103943902&doi=10.1145%2f3434749&partnerID=40&md5=03cd173899edc227ab4e1b743ebb27d2,"In this article, we propose a new method to perform topological network sampling. Topological network sampling is a process for extracting a subset of nodes and edges from a network, such that analyses on the sample provide results and conclusions comparable to the ones they would return if run on whole structure. We need network sampling because the largest online network datasets are accessed through low-throughput application programming interface (API) systems, rendering the collection of the whole network infeasible. Our method is inspired by the literature on network backboning, specifically the noise-corrected backbone. We select the next node to explore by following the edge we identify as the one providing the largest information gain, given the topology of the sample explored so far. We evaluate our method against the most commonly used sampling methods. We do so in a realistic framework, considering a wide array of network topologies, network analysis, and features of API systems. There is no method that can provide the best sample in all possible scenarios, thus in our results section, we show the cases in which our method performs best and the cases in which it performs worst. Overall, the noise-corrected network sampling performs well: it has the best rank average among the tested methods across a wide range of applications. © 2021 ACM.",Network backboning; Network sampling; Social media; Social networks,Application programming interfaces (API); Online systems; Social networking (online); Topology; Information gain; Network samplings; Network topology; On-line network; On-line social networks; Sampling method; Topological networks; Computer systems programming
Mitigating class-boundary label uncertainty to reduce both model bias and variance,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103959131&doi=10.1145%2f3429447&partnerID=40&md5=d34a263bb354793ff06803792fe92f7a,"The study of model bias and variance with respect to decision boundaries is critically important in supervised learning and artificial intelligence. There is generally a tradeoff between the two, as fine-tuning of the decision boundary of a classification model to accommodate more boundary training samples (i.e., higher model complexity) may improve training accuracy (i.e., lower bias) but hurt generalization against unseen data (i.e., higher variance). By focusing on just classification boundary fine-tuning and model complexity, it is difficult to reduce both bias and variance. To overcome this dilemma, we take a different perspective and investigate a new approach to handle inaccuracy and uncertainty in the training data labels, which are inevitable in many applications where labels are conceptual entities and labeling is performed by human annotators. The process of classification can be undermined by uncertainty in the labels of the training data; extending a boundary to accommodate an inaccurately labeled point will increase both bias and variance. Our novel method can reduce both bias and variance by estimating the pointwise label uncertainty of the training set and accordingly adjusting the training sample weights such that those samples with high uncertainty are weighted down and those with low uncertainty are weighted up. In this way, uncertain samples have a smaller contribution to the objective function of the model's learning algorithm and exert less pull on the decision boundary. In a real-world physical activity recognition case study, the data present many labeling challenges, and we show that this new approach improves model performance and reduces model variance. © 2021 ACM.",Bias and variance; Label uncertainty; Neural networks,Artificial intelligence; Classification (of information); Learning algorithms; Sampling; Uncertainty analysis; Bias and variance; Classification boundary; Classification models; Decision boundary; Model performance; Objective functions; Physical activity; Training accuracy; Learning systems
On Modeling Influence Maximization in Social Activity Networks under General Settings,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130090112&doi=10.1145%2f3451218&partnerID=40&md5=7f0d4de46f56acd6c413c3ee79f1e025,"Finding the set of most influential users in online social networks (OSNs) to trigger the largest influence cascade is meaningful, e.g., companies may leverage the ""word-of-mouth""effect to trigger a large cascade of purchases by offering free samples/discounts to those most influential users. This task is usually modeled as an influence maximization problem, and it has been widely studied in the past decade. However, considering that users in OSNs may participate in various online activities, e.g., joining discussion groups and commenting on same pages or products, influence diffusion through online activities becomes even more significant. In this article, we study the impact of online activities by formulating social-Activity networks which contain both users and online activities, and thus induce two types of weighted edges, i.e., edges between users and edges between users and activities. To address the computation challenge, we define an influence centrality via random walks, and use the Monte Carlo framework to efficiently estimate the centrality. Furthermore, we develop a greedy-based algorithm with novel optimizations to find the most influential users for node recommendation. Experiments on real-world datasets show that our approach is very computationally efficient under different influence models, and also achieves larger influence spread by considering online activities.  © 2021 Association for Computing Machinery.",Influence maximization; OSN; Random walk; User activities,Economic and social effects; Social networking (online); User profile; Activity network; Discussion Groups; Influence maximizations; Maximization problem; Most influential users; Online activities; Random Walk; Social activities; User activity; Word-of-mouth effects; Random processes
Trajectory outlier detection: New problems and solutions for smart cities,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103937432&doi=10.1145%2f3425867&partnerID=40&md5=aa055383d61e332d15684472f7c8f3f5,"This article introduces two new problems related to trajectory outlier detection: (1) group trajectory outlier (GTO) detection and (2) deviation point detection for both individual and group of trajectory outliers. Five algorithms are proposed for the first problem by adapting DBSCAN, k nearest neighbors (kNN), and feature selection (FS). DBSCAN-GTO first applies DBSCAN to derive the micro clusters, which are considered as potential candidates. A pruning strategy based on density computation measure is then suggested to find the group of trajectory outliers. kNN-GTO recursively derives the trajectory candidates from the individual trajectory outliers and prunes them based on their density. The overall process is repeated for all individual trajectory outliers. FS-GTO considers the set of individual trajectory outliers as the set of all features, while the FS process is used to retrieve the group of trajectory outliers. The proposed algorithms are improved by incorporating ensemble learning and high-performance computing during the detection process. Moreover, we propose a general two-phase-based algorithm for detecting the deviation points, as well as a version for graphic processing units implementation using sliding windows. Experiments on a real trajectory dataset have been carried out to demonstrate the performance of the proposed approaches. The results show that they can efficiently identify useful patterns represented by group of trajectory outliers, deviation points, and that they outperform the baseline group detection algorithms. © 2021 ACM.",Data mining; Outlier detection; Road traffic management; Smart city application; Trajectory analysis,Anomaly detection; Data handling; Graphics processing unit; Learning algorithms; Nearest neighbor search; Smart city; Statistics; Detection process; Ensemble learning; Graphic processing units; High performance computing; K nearest neighbor (KNN); Problems and Solutions; Pruning strategy; Real trajectories; Trajectories
NeuSE: A Neural Snapshot Ensemble Method for Collaborative Filtering,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129844347&doi=10.1145%2f3450526&partnerID=40&md5=d9e5fd871ec13891217f2abdd147a20c,"In collaborative filtering (CF) algorithms, the optimal models are usually learned by globally minimizing the empirical risks averaged over all the observed data. However, the global models are often obtained via a performance tradeoff among users/items, i.e., not all users/items are perfectly fitted by the global models due to the hard non-convex optimization problems in CF algorithms. Ensemble learning can address this issue by learning multiple diverse models but usually suffer from efficiency issue on large datasets or complex algorithms. In this article, we keep the intermediate models obtained during global model learning as the snapshot models, and then adaptively combine the snapshot models for individual user-item pairs using a memory network-based method. Empirical studies on three real-world datasets show that the proposed method can extensively and significantly improve the accuracy (up to 15.9% relatively) when applied to a variety of existing collaborative filtering methods.  © 2021 Association for Computing Machinery.",Collaborative filtering; Neural networks; Snapshot ensemble,Convex optimization; Large dataset; Learning systems; Collaborative filtering algorithms; Empirical risks; Ensemble methods; Global models; Neural-networks; Observed data; Optimal model; Performance tradeoff; Snapshot ensemble; Snapshot model; Collaborative filtering
An instance space analysis of regression problems,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103984562&doi=10.1145%2f3436893&partnerID=40&md5=c2fd0a300e227911fe8bd8b35d5b69f6,"The quest for greater insights into algorithm strengths and weaknesses, as revealed when studying algorithm performance on large collections of test problems, is supported by interactive visual analytics tools. A recent advance is Instance Space Analysis, which presents a visualization of the space occupied by the test datasets, and the performance of algorithms across the instance space. The strengths and weaknesses of algorithms can be visually assessed, and the adequacy of the test datasets can be scrutinized through visual analytics. This article presents the first Instance Space Analysis of regression problems in Machine Learning, considering the performance of 14 popular algorithms on 4,855 test datasets from a variety of sources. The two-dimensional instance space is defined by measurable characteristics of regression problems, selected from over 26 candidate features. It enables the similarities and differences between test instances to be visualized, along with the predictive performance of regression algorithms across the entire instance space. The purpose of creating this framework for visual analysis of an instance space is twofold: one may assess the capability and suitability of various regression techniques; meanwhile the bias, diversity, and level of difficulty of the regression problems popularly used by the community can be visually revealed. This article shows the applicability of the created regression instance space to provide insights into the strengths and weaknesses of regression algorithms, and the opportunities to diversify the benchmark test instances to support greater insights. © 2021 ACM.",Algorithm selection; instance spaces; machine learning; regression; visual analytics,Benchmarking; Testing; Visualization; Algorithm performance; Level of difficulties; Performance of algorithm; Predictive performance; Regression algorithms; Regression problem; Regression techniques; Visual analytics; Machine learning
SIR-GN: A Fast Structural Iterative Representation Learning Approach for Graph Nodes,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119109977&doi=10.1145%2f3450315&partnerID=40&md5=81744ca79b1de4b827da1a5b89bb2eb6,"Graph representation learning methods have attracted an increasing amount of attention in recent years. These methods focus on learning a numerical representation of the nodes in a graph. Learning these representations is a powerful instrument for tasks such as graph mining, visualization, and hashing. They are of particular interest because they facilitate the direct use of standard machine learning models on graphs. Graph representation learning methods can be divided into two main categories: methods preserving the connectivity information of the nodes and methods preserving nodes' structural information. Connectivity-based methods focus on encoding relationships between nodes, with connected nodes being closer together in the resulting latent space. While methods preserving structure generate a latent space where nodes serving a similar structural function in the network are encoded close to each other, independently of them being connected or even close to each other in the graph. While there are a lot of works that focus on preserving node connectivity, only a few works focus on preserving nodes' structure. Properly encoding nodes' structural information is fundamental for many real-world applications as it has been demonstrated that this information can be leveraged to successfully solve many tasks where connectivity-based methods usually fail. A typical example is the task of node classification, i.e., the assignment or prediction of a particular label for a node. Current limitations of structural representation methods are their scalability, representation meaning, and no formal proof that guaranteed the preservation of structural properties. We propose a new graph representation learning method, called Structural Iterative Representation learning approach for Graph Nodes (SIR-GN). In this work, we propose two variations (SIR-GN: GMM and SIR-GN: K-Means) and show how our best variation SIR-GN: K-Means: (1) theoretically guarantees the preservation of graph structural similarities, (2) provides a clear meaning about its representation and a way to interpret it with a specifically designed attribution procedure, and (3) is scalable and fast to compute. In addition, from our experiment, we show that SIR-GN: K-Means is often better or, in the worst-case comparable than the existing structural graph representation learning methods present in the literature. Also, we empirically show its superior scalability and computational performance when compared to other existing approaches.  © 2021 Association for Computing Machinery.",Datasets; Gaze detection; Neural networks; Text tagging,Encoding (symbols); Graph theory; Graphic methods; Iterative methods; K-means clustering; Numerical methods; Scalability; Signal encoding; Dataset; Gaze detection; Graph representation; K-means; Learning approach; Learning methods; Neural-networks; Numerical representation; Structural information; Text tagging; Learning systems
Similarity Embedding Networks for Robust Human Activity Recognition,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117975234&doi=10.1145%2f3448021&partnerID=40&md5=5c07847c4ca39031a4461425cdac809a,"Deep learning models for human activity recognition (HAR) based on sensor data have been heavily studied recently. However, the generalization ability of deep models on complex real-world HAR data is limited by the availability of high-quality labeled activity data, which are hard to obtain. In this article, we design a similarity embedding neural network that maps input sensor signals onto real vectors through carefully designed convolutional and Long Short-Term Memory (LSTM) layers. The embedding network is trained with a pairwise similarity loss, encouraging the clustering of samples from the same class in the embedded real space, and can be effectively trained on a small dataset and even on a noisy dataset with mislabeled samples. Based on the learned embeddings, we further propose both nonparametric and parametric approaches for activity recognition. Extensive evaluation based on two public datasets has shown that the proposed similarity embedding network significantly outperforms state-of-The-Art deep models on HAR classification tasks, is robust to mislabeled samples in the training set, and can also be used to effectively denoise a noisy dataset.  © 2021 Association for Computing Machinery.",Embedding network; Human activity recognition; Noise robust; Pairwise loss,Classification (of information); Multilayer neural networks; Network embeddings; Network layers; Pattern recognition; Embedding network; Generalization ability; Human activity recognition; Learning models; Noise robust; Noisy datasets; Pairwise loss; Real-world; Sensors data; Similarity embedding; Long short-term memory
Multi-objective Cuckoo Search-based Streaming Feature Selection for Multi-label Dataset,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118195522&doi=10.1145%2f3447586&partnerID=40&md5=fbf32ada1f750276fdd808712da23978,"The feature selection method is the process of selecting only relevant features by removing irrelevant or redundant features amongst the large number of features that are used to represent data. Nowadays, many application domains especially social media networks, generate new features continuously at different time stamps. In such a scenario, when the features are arriving in an online fashion, to cope up with the continuous arrival of features, the selection task must also have to be a continuous process. Therefore, the streaming feature selection based approach has to be incorporated, i.e., every time a new feature or a group of features arrives, the feature selection process has to be invoked. Again, in recent years, there are many application domains that generate data where samples may belong to more than one classes called multi-label dataset. The multiple labels that the instances are being associated with, may have some dependencies amongst themselves. Finding the co-relation amongst the class labels helps to select the discriminative features across multiple labels. In this article, we develop streaming feature selection methods for multi-label data where the multiple labels are reduced to a lower-dimensional space. The similar labels are grouped together before performing the selection method to improve the selection quality and to make the model time efficient. The multi-objective version of the cuckoo search-based approach is used to select the optimal feature set. The proposed method develops two versions of the streaming feature selection method: ) when the features arrive individually and ) when the features arrive in the form of a batch. Various multi-label datasets from various domains such as text, biology, and audio have been used to test the developed streaming feature selection methods. The proposed methods are compared with many previous feature selection methods and from the comparison, the superiority of using multiple objectives and label co-relation in the feature selection process can be established.  © 2021 Association for Computing Machinery.",Label exploitation; Pareto optimal front,Data mining; Pareto principle; Applications domains; Feature selection methods; Features selection; Label exploitation; Multi-labels; Multiple labels; Pareto-optimal front; Redundant features; Relevant features; Search-based; Feature extraction
Trust Prediction for Online Social Networks with Integrated Time-Aware Similarity,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119273075&doi=10.1145%2f3447682&partnerID=40&md5=75be7102840e91749a3632daaa4b6428,"Online social networks gain increasing popularity in recent years. In online social networks, trust prediction is significant for recommendations of high reputation users as well as in many other applications. In the literature, trust prediction problem can be solved by several strategies, such as matrix factorization, trust propagation, and-NN search. However, most of the existing works have not considered the possible complementarity among these mainstream strategies to optimize their effectiveness and efficiency. In this article, we propose a novel trust prediction approach named iSim: An integrated time-Aware similarity-based collaborative filtering approach leveraging on user similarity, which integrates three kinds of factors to measure user similarity, including vector space similarity, time-Aware matrix factorization, and propagated trust. This article is the first work in the literature employing time-Aware matrix factorization and propagated trust in the study of similarity. Additionally, we use several methods like adding inverted index to reduce the time complexity of iSim, and provide its theoretical time bound. Moreover, we also provide the detailed overview and theoretical analysis of the existing works. Finally, the extensive experiments with real-world datasets show that iSim achieves great improvement for both efficiency and effectiveness over the state-of-The-Art approaches.  © 2021 Association for Computing Machinery.",K-NN; Online social network; Propagated trust; Similarity; Time-Aware latent factor; Trust prediction,Collaborative filtering; Efficiency; Forecasting; Matrix algebra; Matrix factorization; Nearest neighbor search; Vector spaces; Effectiveness and efficiencies; Latent factor; Matrix factorizations; Prediction problem; Propagated trust; Similarity; Time-aware latent factor; Trust predictions; Trust propagation; Users similarities; Social networking (online)
The Pulse of Urban Transport: Exploring the Co-evolving Pattern for Spatio-Temporal Forecasting,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115883722&doi=10.1145%2f3450528&partnerID=40&md5=c9e3d035dbadf2f8411f32481f3f36d9,"Transportation demand forecasting is a topic of large practical value. However, the model that fits the demand of one transportation by only considering the historical data of its own could be vulnerable since random fluctuations could easily impact the modeling. On the other hand, common factors like time and region attribute, drive the evolution demand of different transportation, leading to a co-evolving intrinsic property between different kinds of transportation. In this work, we focus on exploring the co-evolution between different modes of transport, e.g., taxi demand and shared-bike demand. Two significant challenges impede the discovery of the co-evolving pattern: (1) diversity of the co-evolving correlation, which varies from region to region and time to time. (2) Multi-modal data fusion. Taxi demand and shared-bike demand are time-series data, which have different representations with the external factors. Moreover, the distribution of taxi demand and bike demand are not identical. To overcome these challenges, we propose a novel method, known as co-evolving spatial temporal neural network (CEST). CEST learns a multi-view demand representation for each mode of transport, extracts the co-evolving pattern, then predicts the demand for the target transportation based on multi-scale representation, which includes fine-scale demand information and coarse-scale pattern information. We conduct extensive experiments to validate the superiority of our model over the state-of-Art models.  © 2021 Association for Computing Machinery.",Demand forecasting; Multi-modal learning; Neural network; Spatio-Temporal analysis,Bicycles; Data fusion; Data mining; Digital storage; Modal analysis; Taxicabs; Urban transportation; Demand forecasting; Evolving patterns; Mode of transport; Multi-modal learning; Neural-networks; Spatial temporals; Spatio-temporal forecasting; Spatiotemporal analysis; Temporal neural networks; Urban transport; Forecasting
Factor-Bounded Nonnegative Matrix Factorization,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108851291&doi=10.1145%2f3451395&partnerID=40&md5=4f3b746738183062c7ba87157cccc274,"Nonnegative Matrix Factorization (NMF) is broadly used to determine class membership in a variety of clustering applications. From movie recommendations and image clustering to visual feature extractions, NMF has applications to solve a large number of knowledge discovery and data mining problems. Traditional optimization methods, such as the Multiplicative Updating Algorithm (MUA), solves the NMF problem by utilizing an auxiliary function to ensure that the objective monotonically decreases. Although the objective in MUA converges, there exists no proof to show that the learned matrix factors converge as well. Without this rigorous analysis, the clustering performance and stability of the NMF algorithms cannot be guaranteed. To address this knowledge gap, in this article, we study the factor-bounded NMF problem and provide a solution algorithm with proven convergence by rigorous mathematical analysis, which ensures that both the objective and matrix factors converge. In addition, we show the relationship between MUA and our solution followed by an analysis of the convergence of MUA. Experiments on both toy data and real-world datasets validate the correctness of our proposed method and its utility as an effective clustering algorithm.  © 2021 Association for Computing Machinery.",Alternating minimization; Factor boundedness constraint; Global sequence convergence; Nonnegative matrix factorization,Clustering algorithms; Data mining; Matrix factorization; Alternating minimization; Boundedness; Clustering applications; Factor boundedness constraint; Global sequence convergences; Image clustering; matrix; Movie recommendations; Nonnegative matrix factorization; Updating algorithm; Matrix algebra
Link recommendation for social influence maximization,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111978040&doi=10.1145%2f3449023&partnerID=40&md5=400a70ba0ac4b23b9c88b09159c85d59,"Social link recommendation systems, like ""People-you-may-know"" on Facebook, ""Who-to-follow"" on Twitter, and ""Suggested-Accounts"" on Instagram assist the users of a social network in establishing new connections with other users.While these systems are becoming more and more important in the growth of social media, they tend to increase the popularity of users that are already popular. Indeed, since link recommenders aim to predict user behavior, they accelerate the creation of links that are likely to be created in the future and, consequently, reinforce social bias by suggesting few (popular) users, giving few chances to most users to create new connections and increase their popularity. In this article, we measure the popularity of a user by means of her social influence, which is her capability to influence other users' opinions, and we propose a link recommendation algorithm that evaluates the links to suggest according to their increment in social influence instead of their likelihood of being created. In detail, we give a 1 - ∈ factor approximation algorithm for the problem of maximizing the social influence of a given set of target users by suggesting a fixed number of new connections considering the Linear Threshold model asmodel for diffusion.We experimentally showthat,with fewnewlinks and small computational time, our algorithm is able to increase by far the social influence of the target users. We compare our algorithm with several baselines and show that it is the most effective one in terms of increased influence. © 2020 Lippincott Williams and Wilkins. All rights reserved.",Approximation algorithms; Experimental evaluation; Influence maximization; Link recommendation,Approximation algorithms; Behavioral research; Social networking (online); Computational time; Factor approximation algorithms; Fixed numbers; Linear threshold models; Recommendation algorithms; Social influence; Social media; User behaviors; Economic and social effects
Learning Graph Neural Networks with Positive and Unlabeled Nodes,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112548681&doi=10.1145%2f3450316&partnerID=40&md5=383589f3b25a60d37c2ad382c03a5acf,"Graph neural networks (GNNs) are important tools for transductive learning tasks, such as node classification in graphs, due to their expressive power in capturing complex interdependency between nodes. To enable GNN learning, existing works typically assume that labeled nodes, from two or multiple classes, are provided, so that a discriminative classifier can be learned from the labeled data. In reality, this assumption might be too restrictive for applications, as users may only provide labels of interest in a single class for a small number of nodes. In addition, most GNN models only aggregate information from short distances (e.g., 1-hop neighbors) in each round, and fail to capture long-distance relationship in graphs. In this article, we propose a novel GNN framework, long-short distance aggregation networks, to overcome these limitations. By generating multiple graphs at different distance levels, based on the adjacency matrix, we develop a long-short distance attention model to model these graphs. The direct neighbors are captured via a short-distance attention mechanism, and neighbors with long distance are captured by a long-distance attention mechanism. Two novel risk estimators are further employed to aggregate long-short-distance networks, for PU learning and the loss is back-propagated for model learning. Experimental results on real-world datasets demonstrate the effectiveness of our algorithm.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Attention; Graph neural networks; Positive unlabeled graph learning,Aggregates; Data mining; Graph neural networks; Graph theory; Learning systems; Attention; Attention mechanisms; Expressive power; Graph neural networks; Learning graphs; Learning tasks; Multiple class; Neural network learning; Positive unlabeled graph learning; Transductive learning; Graphic methods
Faster motif counting via succinct color coding and adaptive sampling,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116893374&doi=10.1145%2f3447397&partnerID=40&md5=b0d26f60e3ac08ef8613e2a5aaeb3e2a,"We address the problem of computing the distribution of induced connected subgraphs, aka graphlets or motifs, in large graphs. The current state-of-the-art algorithms estimate the motif counts via uniform sampling by leveraging the color coding technique by Alon, Yuster, and Zwick. In this work, we extend the applicability of this approach by introducing a set of algorithmic optimizations and techniques that reduce the running time and space usage of color coding and improve the accuracy of the counts. To this end, we first show how to optimize color coding to efficiently build a compact table of a representative subsample of all graphlets in the input graph. For 8-node motifs, we can build such a table in one hour for a graph with 65M nodes and 1.8B edges, which is 2,000 times larger than the state of the art. We then introduce a novel adaptive sampling scheme that breaks the “additive error barrier” of uniform sampling, guaranteeing multiplicative approximations instead of just additive ones. This allows us to count not only the most frequent motifs, but also extremely rare ones. For instance, on one graph we accurately count nearly 10.000 distinct 8-node motifs whose relative frequency is so small that uniform sampling would literally take centuries to find them. Our results show that color coding is still the most promising approach to scalable motif counting. © 2021 Copyright held by the owner/author(s).",Color coding; Graph mining; Graphlets; Motifs; Subgraph counting,Color; Graph theory; Graphic methods; Adaptive sampling; Colour coding; Connected Subgraph; Graph mining; Graphlets; Large graphs; Motif; Subgraph counting; Subgraphs; Uniform sampling; Additives
Maximum Likelihood Estimation of Power-law Degree Distributions via Friendship Paradox-based Sampling,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103809088&doi=10.1145%2f3451166&partnerID=40&md5=c990d002335b70e0a8b4d85cafbcb05f,"This article considers the problem of estimating a power-law degree distribution of an undirected network using sampled data. Although power-law degree distributions are ubiquitous in nature, the widely used parametric methods for estimating them (e.g., linear regression on double-logarithmic axes and maximum likelihood estimation with uniformly sampled nodes) suffer from the large variance introduced by the lack of data-points from the tail portion of the power-law degree distribution. As a solution, we present a novel maximum likelihood estimation approach that exploits the friendship paradox to sample more efficiently from the tail of the degree distribution. We analytically show that the proposed method results in a smaller bias, variance and a Cramèr-Rao lower bound compared to the vanilla maximum likelihood estimate obtained with uniformly sampled nodes (which is the most commonly used method in literature). Detailed numerical and empirical results are presented to illustrate the performance of the proposed method under different conditions and how it compares with alternative methods. We also show that the proposed method and its desirable properties (i.e., smaller bias, variance, and Cramèr-Rao lower bound compared to vanilla method based on uniform samples) extend to parametric degree distributions other than the power-law such as exponential degree distributions as well. All the numerical and empirical results are reproducible and the code is publicly available on Github.  © 2021 Association for Computing Machinery.",Degree distribution; Friendship paradox; Maximum likelihood estimation; Networks; Power-law; Sampling bias,Cramer-Rao bounds; Numerical methods; Crame-Rao lower bounds; Cramer Rao lower bound; Degree distributions; Friendship paradox; Maximum-likelihood estimation; Network; Power law degree distribution; Power-law; Sampling bias; Undirected network; Maximum likelihood estimation
Hierarchical Physician Recommendation via Diversity-enhanced Matrix Factorization,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099368895&doi=10.1145%2f3418227&partnerID=40&md5=9e6ab10fa4a4a00e31ca48b1cf483c96,"Recent studies have shown that there exhibits significantly imbalanced medical resource allocation across public hospitals. Patients, regardless of their diseases, tend to choose hospitals and physicians with a better reputation, which often overloads major hospitals while leaving others underutilized. Guiding patients to hospitals that can serve their treatment needs both timely and withgood quality can make the best use of precious medical resources. Unfortunately, it remains one of the major challenges both for research and in practice. In this article, we propose a novel diversity-enhanced hierarchical physician recommendation approach to address this issue. We adopt matrix factorization to estimate physician competency and exploit implicit similarity relationships to improve the competency estimation of physicians that we are of little information of. We then balance the patient preference and physician diversity using two novel heuristic algorithms. We evaluate ourproposed approach and compare it with the state of the art. Experiments show that our approach significantly improves both accuracy and recommendation diversity over existing approaches.  © 2020ACM.",big knowledge; enhanced matrix factorization; heuristic algorithm; Hierarchical physician recommendation,Factorization; Heuristic algorithms; Hospitals; Patient treatment; Matrix factorizations; Patient preferences; Recommendation diversities; State of the art; Matrix algebra
Dynamic Graph Mining for Multi-weight Multi-destination Route Planning with Deadlines Constraints,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099369228&doi=10.1145%2f3412363&partnerID=40&md5=d15da62ab0db8176c39ce21341b24dff,"Route planning satisfied multiple requests is an emerging branch in the route planning field and has attracted significant attention from the research community in recent years. The prevailing studies focus only on seeking a route by minimizing a single kind of Travel Cost, such as trip timeor distance, among others. In reality, most users would like to choose an appropriate route, neither fastest nor shortest route. Usually, a user may have multiple requirements, and an appropriate route would satisfy all requirements requested by the user. In fact, planning an appropriate route could be formulated as a problem of Multi-weight Multi-destination Route Planning with Deadlines Constraints (MWMDRP-DC). In this article, we propose a framework, namely, MWMD-Router, which addresses the MWMDRP-DC problem comprehensively. To consider the travel costs with time-variation, we proposenot only four novel dynamic graph miner to extract travel costs that reveal users' requirements butalso two new algorithms, namely, Basic MWMD Route Planning and Advanced MWMD Route Planning, to plan a route that satisfies deadline requirements and optimizes another criterion like travel cost with time-variation efficiently. To the best of our knowledge, this is the first work on route planning that considers handling multiple deadlines for multi-destination planning as well as optimizing multiple travel costs with time-variation simultaneously. Experimental results demonstrate that our proposed algorithms deliver excellent performance with respect to efficiency and effectiveness.  © 2020 ACM.",deadline constraint; multi-destination; Route planning; trajectory data mining,Computer science; Data mining; Dynamic graph; Research communities; Route planning; Time variations; Travel costs; Graph algorithms
Heterogeneous Graphlets,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099392939&doi=10.1145%2f3418773&partnerID=40&md5=2b95364f4d19b3d4466ed2b252308370,"In this article, we introduce a generalization of graphlets to heterogeneous networks called typed graphlets. Informally, typed graphlets are small typed induced subgraphs. Typed graphlets generalize graphlets to rich heterogeneous networks as they explicitly capture the higher-order typed connectivity patterns in such networks. To address this problem, we describe a general framework for counting the occurrences of such typed graphlets. The proposed algorithms leverage a number of combinatorial relationships for different typed graphlets. For each edge, we count a few typed graphlets, and with these counts along with the combinatorial relationships, we obtain the exact counts ofthe other typed graphlets in o(1) constant time. Notably, the worst-case time complexity of the proposed approach matches the time complexity of the best known untyped algorithm. In addition, the approach lends itself to an efficient lock-free and asynchronous parallel implementation. While thereare no existing methods for typed graphlets, there has been some work that focused on computing a different and much simpler notion called colored graphlet. The experiments confirm that our proposedapproach is orders of magnitude faster and more space-efficient than methods for computing the simpler notion of colored graphlet. Unlike these methods that take hours on small networks, the proposed approach takes only seconds on large networks with millions of edges. Notably, since typed graphlet is more general than colored graphlet (and untyped graphlets), the counts of various typed graphlets can be combined to obtain the counts of the much simpler notion of colored graphlets. The proposed methods give rise to new opportunities and applications for typed graphlets.  © 2020 ACM.",attributed graphs; heterogeneous graphlets; heterogeneous network motifs; heterogeneous networks; k-partite graphs; labeled graphlets; large networks; position-aware typed graphlets; Typed graphlets,Heterogeneous networks; Asynchronous parallel; Connectivity pattern; Induced subgraphs; Large networks; Orders of magnitude; Small networks; Space efficient; Time complexity; Complex networks
Combinatorial Algorithms for String Sanitization,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099403713&doi=10.1145%2f3418683&partnerID=40&md5=bc1805bdafeb61d64e09a569e397700c,"String data are often disseminated to support applications such as location-based service provision or DNA sequence analysis. This dissemination, however, may expose sensitive patterns that model confidential knowledge (e.g., trips to mental health clinics from a string representing a user'slocation history). In this article, we consider the problem of sanitizing a string by concealing the occurrences of sensitive patterns, while maintaining data utility, in two settings that are relevant to many common string processing tasks. In the first setting, we aim to generate the minimal-length string that preserves the order of appearance and frequency of all non-sensitive patterns. Such a string allows accurately performing tasks based on the sequential nature and pattern frequencies of the string. To construct such a string, we propose a time-optimal algorithm, TFS-ALGO. We alsopropose another time-optimal algorithm, PFS-ALGO, which preserves a partial order of appearance of non-sensitive patterns but produces a much shorter string that can be analyzed more efficiently. The strings produced by either of these algorithms are constructed by concatenating non-sensitive parts of the input string. However, it is possible to detect the sensitive patterns by ""reversing""the concatenation operations. In response, we propose a heuristic, MCSR-ALGO, which replacesletters in the strings output by the algorithms with carefully selected letters, so that sensitive patterns are not reinstated, implausible patterns are not introduced, and occurrences of spurious patterns are prevented. In the second setting, we aim to generate a string that is at minimal edit distance from the original string, in addition to preserving the order of appearance and frequency of all non-sensitive patterns. To construct such a string, we propose an algorithm, ETFS-ALGO, basedon solving specific instances of approximate regular expression matching. We implemented our sanitization approach that applies TFS-ALGO, PFS-ALGO, and then MCSR-ALGO, and experimentally show that it is effective and efficient. We also show that TFS-ALGO is nearly as effective at minimizing the edit distance as ETFS-ALGO, while being substantially more efficient than ETFS-ALGO.  © 2020 ACM.",Data privacy; data sanitization; knowledge hiding; sensitive knowledge; sequences; strings,Heuristic algorithms; Location based services; Optimization; Telecommunication services; Combinatorial algorithm; Data utilities; DNA sequence analysis; Edit distance; Minimal lengths; Regular-expression matching; String processing; Time-optimal algorithm; String searching algorithms
Automatic Recommendation of a Distance Measure for Clustering Algorithms,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099367250&doi=10.1145%2f3418228&partnerID=40&md5=130ed2abcb2ef74053d3ae7dc16f0155,"With a large number of distance measures, the appropriate choice for clustering a given data set with a specified clustering algorithm becomes an important problem. In this article, an automatic distance measure recommendation method for clustering algorithms is proposed. The recommendation method consists of the following steps: (1) metadata extraction, including meta-feature collection and meta-target identification; (2) recommendation model construction using metadata; and (3) distance measure recommendation for a new data set by the recommendation model. Two different types of meta-targets and meta-learning techniques are utilized considering the possible different requirements of users. To validate the necessity and effectiveness of the distance measure recommendation method, an empirical study is conducted with 199 publicly available data sets, 9 distance measures, and 2 widely used clustering algorithms. The experimental results indicate that distance measure significantly influences the performance of the clustering algorithm for a given data set. Furthermore,performance analysis of the proposed recommendation method proves its effectiveness.  © 2020 ACM.",clustering; Distance measure; meta-learning; recommendation,Metadata; Recommender systems; Distance measure; Empirical studies; Meta-data extractions; Meta-learning techniques; Model construction; Performance analysis; Recommendation methods; Target identification; Clustering algorithms
CrowdWT: Crowdsourcing via Joint Modeling of Workers and Tasks,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099344094&doi=10.1145%2f3421712&partnerID=40&md5=26370cb1de42871363606a78e86b6b94,"Crowdsourcing is a relatively inexpensive and efficient mechanism to collect annotations of data from the open Internet. Crowdsourcing workers are paid for the provided annotations, but the task requester usually has a limited budget. It is desirable to wisely assign the appropriate task to the right workers, so the overall annotation quality is maximized while the cost is reduced. In this article, we propose a novel task assignment strategy (CrowdWT) to capture the complex interactions between tasks and workers, and properly assign tasks to workers. CrowdWT first develops a Worker Bias Model (WBM) to jointly model the worker's bias, the ground truths of tasks, and the task features. WBM constructs a mapping between task features and worker annotations to dynamically assign the task to a group of workers, who are more likely to give correct annotations for the task. CrowdWTfurther introduces a Task Difficulty Model (TDM), which builds a Kernel ridge regressor based on task features to quantify the intrinsic difficulty of tasks and thus to assign the difficult tasks tomore reliable workers. Finally, CrowdWT combines WBM and TDM into a unified model to dynamically assign tasks to a group of workers and recall more reliable and even expert workers to annotate the difficult tasks. Our experimental results on two real-world datasets and two semi-synthetic datasetsshow that CrowdWT achieves high-quality answers within a limited budget, and has the best performance against competitive methods.<-1.5pt  © 2020 ACM.",annotation quality; Crowdsourcing; task assignment; task difficulty model; worker bias model,Budget control; Bias modeling; Ground truth; High quality; Joint modeling; Novel task; Real-world datasets; Task difficulty; Unified Modeling; Crowdsourcing
Accelerating Large-Scale Heterogeneous Interaction Graph Embedding Learning via Importance Sampling,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099332826&doi=10.1145%2f3418684&partnerID=40&md5=893736b5aab545227aa8e58f8f20b221,"In real-world problems, heterogeneous entities are often related to each other through multiple interactions, forming a Heterogeneous Interaction Graph (HIG). While modeling HIGs to deal with fundamental tasks, graph neural networks present an attractive opportunity that can make full use ofthe heterogeneity and rich semantic information by aggregating and propagating information from different types of neighborhoods. However, learning on such complex graphs, often with millions or billions of nodes, edges, and various attributes, could suffer from expensive time cost and high memory consumption. In this article, we attempt to accelerate representation learning on large-scale HIGs by adopting the importance sampling of heterogeneous neighborhoods in a batch-wise manner, which naturally fits with most batch-based optimizations. Distinct from traditional homogeneous strategies neglecting semantic types of nodes and edges, to handle the rich heterogeneous semantics within HIGs, we devise both type-dependent and type-fusion samplers where the former respectively samples neighborhoods of each type and the latter jointly samples from candidates of all types. Furthermore,to overcome the imbalance between the down-sampled and the original information, we respectively propose heterogeneous estimators including the self-normalized and the adaptive estimators to improvethe robustness of our sampling strategies. Finally, we evaluate the performance of our models for node classification and link prediction on five real-world datasets, respectively. The empirical results demonstrate that our approach performs significantly better than other state-of-the-art alternatives, and is able to reduce the number of edges in computation by up to 93%, the memory cost by up to 92% and the time cost by up to 86%.  © 2020 ACM.",Heterogeneous interaction graphs; importance sampling; large-scale graphs; type-dependent sampler; type-fusion sampler,Classification (of information); Embeddings; Neural networks; Semantics; Adaptive estimators; Graph neural networks; Heterogeneous interactions; Multiple interactions; Real-world datasets; Real-world problem; Sampling strategies; Semantic information; Importance sampling
A Reduced Network Traffic Method for IoT Data Clustering,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099345151&doi=10.1145%2f3423139&partnerID=40&md5=cad8841586586a9e694e29fa9cfa0b51,"Internet of Things (IoT) systems usually involve interconnected, low processing capacity, and low memory sensor nodes (devices) that collect data in several sorts of applications that interconnect people and things. In this scenario, mining tasks, such as clustering, have been commonly deployed to detect behavioral patterns from the collected data. The centralized clustering of IoT data demands high network traffic to transmit the data from the devices to a central node, where a clustering algorithm must be applied. This approach does not scale as the number of devices increases, and the amount of data grows. However, distributing the clustering process through the devices may not be a feasible approach as well, since the devices are usually simple and may not have the abilityto execute complex procedures. This work proposes a centralized IoT data clustering method that demands reduced network traffic and low processing power in the devices. The proposed method uses a data grid to summarize the information at the devices before transmitting it to the central node, reducing network traffic. After the data transfer, the proposed method applies a clustering algorithm that was developed to process data in the summarized representation. Tests with seven datasets provided experimental evidence that the proposed method reduces network traffic and produces results comparable to the ones generated by DBSCAN and HDBSCAN, two robust centralized clustering algorithms. © 2020 ACM.",data summarization; Data traffic reduction; distributed data mining; Internet of Things,Cluster analysis; Data transfer; Grid computing; Internet of things; Sensor nodes; Behavioral patterns; Clustering process; Complex procedure; Data clustering methods; Experimental evidence; Internet of Things (IOT); Processing capacities; Processing power; Clustering algorithms
Span-core Decomposition for Temporal Networks: Algorithms and Applications,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099346126&doi=10.1145%2f3418226&partnerID=40&md5=cc5e8c57a4375f8c0e8d843aa24709d0,"When analyzing temporal networks, a fundamental task is the identification of dense structures(i.e., groups of vertices that exhibit a large number of links), together with their temporal span (i.e., the period of time for which the high density holds). In this article, we tackle this task by introducing a notion of temporal core decomposition where each core is associated with two quantities, its coreness, which quantifies how densely it is connected, and its span, which is a temporalinterval: we call such cores span-cores. For a temporal network defined on a discrete temporal domain T, the total number of time intervals included in T is quadratic in |T|, so that the total number of span-cores is potentially quadratic in |T| as well. Our first main contribution is an algorithm that, by exploiting containment properties among span-cores, computes all the span-cores efficiently. Then, we focus on the problem of finding only the maximal span-cores, i.e., span-cores that are not dominated by any other span-core by both their coreness property and their span. We devise a very efficient algorithm that exploits theoretical findings on the maximality condition to directlyextract the maximal ones without computing all span-cores. Finally, as a third contribution, we introduce the problem of temporal community search, where a set of query vertices is given as input, and the goal is to find a set of densely-connected subgraphs containing the query vertices and covering the whole underlying temporal domain T. We derive a connection between this problem and the problem of finding (maximal) span-cores. Based on this connection, we show how temporal community search can be solved in polynomial-time via dynamic programming, and how the maximal span-cores can be profitably exploited to significantly speed-up the basic algorithm. We provide an extensive experimentation on several real-world temporal networks of widely different origins and characteristics. Our results confirm the efficiency and scalability of the proposed methods. Moreover, we showcase the practical relevance of our techniques in a number of applications on temporal networks, describing face-to-face contacts between individuals in schools. Our experiments highlight the relevance of the notion of (maximal) span-core in analyzing social dynamics, detecting/correcting anomalies in the data, and graph-embedding-based network classification.  © 2020 ACM.",community search; core decomposition; face-to-face interaction networks; maximal cores; Temporal networks,Polynomial approximation; Connected subgraphs; Containment property; Dense structures; Different origins; Graph embeddings; Network classification; Social dynamics; Temporal networks; Dynamic programming
Class Imbalance and Cost-Sensitive Decision Trees: A Unified Survey Based on a Core Similarity,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099355977&doi=10.1145%2f3415156&partnerID=40&md5=178017b02d0d7200f1e432cdf3c80aee,"Class imbalance treatment methods and cost-sensitive classification algorithms are typically treated as two independent research areas. However, many of these techniques have properties in common. After providing a background to the two fields of research, this article identifies the fundamental mechanism which is common to both. Using this mechanism, a taxonomy is created which encompasses approaches to both class imbalance treatment and cost-sensitive classification. Through this survey, we aim to bridge the gap between the two fields such that lessons from one field may be applied to the other. Many data mining tasks are naturally both class imbalanced and cost-sensitive. This survey is useful for researchers and practitioners approaching these tasks as it provides a detailed overview of approaches in both fields. Many of the surveyed techniques are classifier independent. However, we chose to focus on techniques which were either decision tree-based or compatible with decision trees. This choice was based on the popularity and novelty of their application to both fields.  © 2020 ACM.",Class imbalance; classification; cost sensitivity,Data mining; Surveys; Class imbalance; Cost sensitive classifications; Cost-sensitive; Data mining tasks; Fundamental mechanisms; Independent research; Treatment methods; Decision trees
Robust Tensor Recovery with Fiber Outliers for Traffic Events,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099345002&doi=10.1145%2f3417337&partnerID=40&md5=4a388f7a34e07b8d6a3cfd9a1a52832b,"Event detection is gaining increasing attention in smart cities research. Large-scale mobilitydata serves as an important tool to uncover the dynamics of urban transportation systems, and more often than not the dataset is incomplete. In this article, we develop a method to detect extreme events in large traffic datasets, and to impute missing data during regular conditions. Specifically,we propose a robust tensor recovery problem to recover low-rank tensors under fiber-sparse corruptions with partial observations, and use it to identify events, and impute missing data under typicalconditions. Our approach is scalable to large urban areas, taking full advantage of the spatio-temporal correlations in traffic patterns. We develop an efficient algorithm to solve the tensor recovery problem based on the alternating direction method of multipliers (ADMM) framework. Compared withexisting l1 norm regularized tensor decomposition methods, our algorithm can exactly recover the values of uncorrupted fibers of a low-rank tensor and find the positions of corrupted fibers under mild conditions. Numerical experiments illustrate that our algorithm can achieve exact recovery and outlier detection even with missing data rates as high as 40% under 5% gross corruption, depending on the tensor size and the Tucker rank of the low rank tensor. Finally, we apply our method on a real traffic dataset corresponding to downtown Nashville, TN and successfully detect the events like severe car crashes, construction lane closures, and other large events that cause significant traffic disruptions.  © 2020 ACM.",multilinear analysis; outlier detection; Robust tensor recovery; tensor factorization; traffic events; urban computing,Accidents; Fibers; Large dataset; Recovery; Statistics; Urban transportation; Alternating direction method of multipliers; Numerical experiments; Partial observation; Spatiotemporal correlation; Tensor decomposition; Tensor recoveries; Traffic disruption; Urban transportation systems; Tensors
MeSHProbeNet-P: Improving Large-scale MeSH Indexing with Personalizable MeSH Probes,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099346593&doi=10.1145%2f3421713&partnerID=40&md5=2b0f2fb6efab6016a85b7330bdf4a0ea,"Indexing biomedical research articles with Medical Subject Headings (MeSH) can greatly facilitate biomedical research and information retrieval. Currently MeSH indexing is performed by human experts. To alleviate the time consumption and monetary cost caused by manual indexing, many automatic MeSH indexing models have been developed, such as MeSHProbeNet, DeepMeSH, and NLM's official model Medical Text Indexer. In this article, we propose an end-to-end framework, MeSHProbeNet-P, which extends MeSHProbeNet with personalizable MeSH probes. In MeSHProbeNet-P, each MeSH probe carries certain aspects of biomedical knowledge and extracts related information from input articles. MeSHProbeNet-P is able to automatically personalize its MeSH probes for different input articles to ensurethat the current MeSH probes best fit the current input article and the most informative features can be extracted from the article. We demonstrate the effectiveness of MeSHProbeNet-P in a real-world large-scale MeSH indexing challenge. MeSHProbeNet-P won the first place in the first batch of Task A in the 2019 BioASQ challenge. The result on the first test set of the challenge is reported in this article. We also provide ablation studies to show the advantages of personalizable MeSH probes.  © 2020 ACM.",attention mechanism; biomedical MeSH indexing; deep learning; Large-scale,Automatic indexing; Indexing (of information); Probes; Biomedical research; Current input; Human expert; Indexing models; Manual indexing; Medical subject headings; Monetary costs; Time consumption; Mesh generation
Multi-Stage Network Embedding for Exploring Heterogeneous Edges,2021,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099346675&doi=10.1145%2f3415157&partnerID=40&md5=2378ae3418976de42b363ad9c7047d57,"The relationships between objects in a network are typically diverse and complex, leading to the heterogeneous edges with different semantic information. In this article, we focus on exploring the heterogeneous edges for network representation learning. By considering each relationship as a view that depicts a specific type of proximity between nodes, we propose a multi-stage non-negativematrix factorization (MNMF) model, committed to utilizing abundant information in multiple views tolearn robust network representations. In fact, most existing network embedding methods are closely related to implicitly factorizing the complex proximity matrix. However, the approximation error isusually quite large, since a single low-rank matrix is insufficient to capture the original information. Through a multi-stage matrix factorization process motivated by gradient boosting, our MNMF model achieves lower approximation error. Meanwhile, the multi-stage structure of MNMF gives the feasibility of designing two kinds of non-negative matrix factorization (NMF) manners to preserve network information better. The united NMF aims to preserve the consensus information between differentviews, and the independent NMF aims to preserve unique information of each view. Concrete experimental results on realistic datasets indicate that our model outperforms three types of baselines in practical applications.  © 2020 ACM.",data mining; Network embedding; non-negative matrix factorization,Complex networks; Embeddings; Factorization; Semantics; Approximation errors; Gradient boosting; Lower approximation; Matrix factorizations; Network information; Network representation; Nonnegative matrix factorization; Semantic information; Matrix algebra
Boosting Item-based Collaborative Filtering via Nearly Uncoupled Random Walks,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092709187&doi=10.1145%2f3406241&partnerID=40&md5=493c6994a28f1e8b0bc2613e3d251e0c,"Item-based models are among the most popular collaborative filtering approaches for building recommender systems. Random walks can provide a powerful tool for harvesting the rich network of interactions captured within these models. They can exploit indirect relations between the items, mitigate the effects of sparsity, ensure wider itemspace coverage, as well as increase the diversity of recommendation lists. Their potential however, can be hindered by the tendency of the walks to rapidly concentrate towards the central nodes of the graph, thereby significantly restricting the range of K-step distributions that can be exploited for personalized recommendations. In this work, we introduce RecWalk; a novel random walk-based method that leverages the spectral properties of nearly uncoupled Markov chains to provably lift this limitation and prolong the influence of users' past preferences on the successive steps of the walk - thereby allowing the walker to explore the underlying network more fruitfully. A comprehensive set of experiments on real-world datasets verify the theoretically predicted properties of the proposed approach and indicate that they are directly linked to significant improvements in top-n recommendation accuracy. They also highlight RecWalk's potential in providing a framework for boosting the performance of item-based models. RecWalk achieves state-of-the-art top-n recommendation quality outperforming several competing approaches, including recently proposed methods that rely on deep neural networks.  © 2020 ACM.",Collaborative filtering; nearly uncoupled markov chains; random walks; Top-N recommendation,Deep neural networks; Markov chains; Central nodes; Item-based collaborative filtering; Personalized recommendation; Real-world datasets; Recommendation accuracy; Spectral properties; State of the art; Underlying networks; Collaborative filtering
Large-scale Data Exploration Using Explanatory Regression Functions,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092744633&doi=10.1145%2f3410448&partnerID=40&md5=e07b8925576d5f10fab8ae31912f7baf,"Analysts wishing to explore multivariate data spaces, typically issue queries involving selection operators, i.e., range or equality predicates, which define data subspaces of potential interest. Then, they use aggregation functions, the results of which determine a subspace's interestingness for further exploration and deeper analysis. However, Aggregate Query (AQ) results are scalars and convey limited information and explainability about the queried subspaces for enhanced exploratory analysis. Analysts have no way of identifying how these results are derived or how they change w.r.t query (input) parameter values. We address this shortcoming by aiding analysts to explore and understand data subspaces by contributing a novel explanation mechanism based on machine learning. We explain AQ results using functions obtained by a three-fold joint optimization problem which assume the form of explainable piecewise-linear regression functions. A key feature of the proposed solution is that the explanation functions are estimated using past executed queries. These queries provide a coarse grained overview of the underlying aggregate function (generating the AQ results) to be learned. Explanations for future, previously unseen AQs can be computed without accessing the underlying data and can be used to further explore the queried data subspaces, without issuing more queries to the backend analytics engine. We evaluate the explanation accuracy and efficiency through theoretically grounded metrics over real-world and synthetic datasets and query workloads.  © 2020 ACM.",aggregate query explanation; data exploration; Explainability; range query explanation,Piecewise linear techniques; Aggregate function; Aggregation functions; Exploratory analysis; Joint optimization; Limited information; Piecewise linear regression; Regression function; Selection operators; Aggregates
An Approach for Concept Drift Detection in a Graph Stream Using Discriminative Subgraphs,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092699043&doi=10.1145%2f3406243&partnerID=40&md5=bf24d504c071cb9da9cc10b5dfbea870,"The emergence of mining complex networks like social media, sensor networks, and the world-wide-web has attracted considerable research interest. In a streaming scenario, the concept to be learned can change over time. However, while there has been some research done for detecting concept drift in traditional data streams, little work has been done on addressing concept drift in data represented as a graph. We propose a novel unsupervised concept-drift detection method on graph streams called Discriminative Subgraph-based Drift Detector (DSDD). The methodology starts by discovering discriminative subgraphs for each graph in the stream. We then compute the entropy of the window based on the distribution of discriminative subgraphs with respect to the graphs and then use the direct density-ratio estimation approach for detecting concept drift in the series of entropy values obtained by moving one step forward in the sliding window. The effectiveness of the proposed method is demonstrated through experiments using artificial and real-world datasets and its performance is evaluated by comparing against related baseline methods. Similarly, the usefulness of the proposed concept drift detection approach is studied by incorporating it in a popular graph stream classification algorithm and studying the impact of drift detection in classification accuracy.  © 2020 ACM.",Concept drift detection; graph stream; graph stream classification,Complex networks; Data streams; Entropy; Sensor networks; Baseline methods; Classification accuracy; Density-ratio estimations; Detection approach; Detection methods; Real-world datasets; Research interests; Stream classification; Graph algorithms
Exploiting User Preference and Mobile Peer Influence for Human Mobility Annotation,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092721922&doi=10.1145%2f3406600&partnerID=40&md5=f550ff3e9362be756da783de2e1fb146,"Human mobility annotation aims to assign mobility records the corresponding visiting Point-of-Interests (POIs). It is one of the most fundamental problems for understanding human mobile behaviors. In literature, many efforts have been devoted to annotating mobility records in a pointwise or trajectory-wise manner. However, the user preference factor is not fully explored and, worse still, the mobile peer influence factor has never been integrated. To this end, in this article, we propose a novel framework, named JEPPI, to jointly exploit user preference and mobile peer influence to tackle the problem. In our JEPPI, we first unify the two distinct factors in a behavior-driven user-POI graph. This graph enables us to model user preference with user-POI visiting relationships, and model two types of mobile peer influence with co-location and co-visiting peer relationships, respectively. Moreover, we devise an equivalence-emphasizing metric to reduce redundancy in the second-order co-visiting peer influence. In addition, a mutual augmentation learning approach is proposed to preserve the latent structures of various factors exploited. Notably, our learning approach preserves all factors in a shared representation space such that user preference is learned with mobile peer influence being considered at the same time, and vice versa. In this way, the different factors are mutually augmented and semantically integrated to enhance human mobility annotation. Finally, using two large-scale real-world datasets, we conduct extensive experiments to demonstrate the superiority of our approach compared with the state-of-the-art annotation methods.  © 2020 ACM.",Human mobility annotation; mobile analysis; network embedding; Point-of-Interest,Large dataset; Annotation methods; Latent structures; Learning approach; Point of interest; Preference factors; Real-world datasets; Shared representations; State of the art; Behavioral research
Scalable Spatial Scan Statistics for Trajectories,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092746460&doi=10.1145%2f3394046&partnerID=40&md5=5e06f6e258e6c0afd2f4ad155fd58f2a,"We define several new models for how to define anomalous regions among enormous sets of trajectories. These are based on spatial scan statistics, and identify a geometric region which captures a subset of trajectories which are significantly different in a measured characteristic from the background population. The model definition depends on how much a geometric region is contributed to by some overlapping trajectory. This contribution can be the full trajectory, proportional to the length within the spatial region, or dependent on the flux across the boundary of that spatial region. Our methods are based on and significantly extend a recent two-level sampling approach which provides high accuracy at enormous scales of data. We support these new models and algorithms with extensive experiments on millions of trajectories and also theoretical guarantees.  © 2020 ACM.",discrepancy; range spaces; Spatial scan statistics; trajectories,Population statistics; Anomalous regions; High-accuracy; Models and algorithms; Spatial regions; Spatial scan statistics; Theoretical guarantees; Trajectories
Heterogeneous Univariate Outlier Ensembles in Multidimensional Data,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092697242&doi=10.1145%2f3403934&partnerID=40&md5=856ced0be979e34ebd2b298df3ebc357,"In outlier detection, recent major research has shifted from developing univariate methods to multivariate methods due to the rapid growth of multidimensional data. However, one typical issue of this paradigm shift is that many multidimensional data often mainly contains univariate outliers, in which many features are actually irrelevant. In such cases, multivariate methods are ineffective in identifying such outliers due to the potential biases and the curse of dimensionality brought by irrelevant features. Those univariate outliers might be well detected by applying univariate outlier detectors in individually relevant features. However, it is very challenging to choose a right univariate detector for each individual feature since different features may take very different probability distributions. To address this challenge, we introduce a novel Heterogeneous Univariate Outlier Ensembles (HUOE) framework and its instance ZDD to synthesize a set of heterogeneous univariate outlier detectors as base learners to build heterogeneous ensembles that are optimized for each individual feature. Extensive results on 19 real-world datasets and a collection of synthetic datasets show that ZDD obtains 5%-14% average AUC improvement over four state-of-the-art multivariate ensembles and performs substantially more robustly w.r.t. irrelevant features.  © 2020 ACM.",anomaly detection; heterogeneous data; multidimensional data; Outlier detection; outlier ensemble; univariate outlier,Probability distributions; Statistics; Curse of dimensionality; Heterogeneous ensembles; Individual features; Multidimensional data; Multivariate methods; Real-world datasets; Relevant features; Synthetic datasets; Feature extraction
Bi-Directional Recurrent Attentional Topic Model,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092726562&doi=10.1145%2f3412371&partnerID=40&md5=556e049850ed4848b17e7d1e3ec86957,"In a document, the topic distribution of a sentence depends on both the topics of its neighbored sentences and its own content, and it is usually affected by the topics of the neighbored sentences with different weights. The neighbored sentences of a sentence include the preceding sentences and the subsequent sentences. Meanwhile, it is natural that a document can be treated as a sequence of sentences. Most existing works for Bayesian document modeling do not take these points into consideration. To fill this gap, we propose a bi-Directional Recurrent Attentional Topic Model (bi-RATM) for document embedding. The bi-RATM not only takes advantage of the sequential orders among sentences but also uses the attention mechanism to model the relations among successive sentences. To support to the bi-RATM, we propose a bi-Directional Recurrent Attentional Bayesian Process (bi-RABP) to handle the sequences. Based on the bi-RABP, bi-RATM fully utilizes the bi-directional sequential information of the sentences in a document. Online bi-RATM is proposed to handle large-scale corpus. Experiments on two corpora show that the proposed model outperforms state-of-the-art methods on document modeling and classification.  © 2020 ACM.",Bi-directional recurrent attentions; recurrent attentional Bayesian process; topic modeling,Computer science; Data mining; Attention mechanisms; Bayesian; Bi-directional; Document model; Sequential information; State-of-the-art methods; Topic distributions; Topic Modeling; Information retrieval systems
Time-Warped Sparse Non-negative Factorization for Functional Data Analysis,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111092748&doi=10.1145%2f3408313&partnerID=40&md5=36cc1c2c1cb28fa981f8d73a2056f759,"This article proposes a novel time-warped sparse non-negative factorization method for functional data analysis. The proposed method on the one hand guarantees the extracted basis functions and their coefficients to be positive and interpretable, and on the other hand is able to handle weakly correlated functions with different features. Furthermore, the method incorporates time warping into factorization and hence allows the extracted basis functions of different samples to have temporal deformations. An efficient framework of estimation algorithms is proposed based on a greedy variable selection approach. Numerical studies together with case studies on real-world data demonstrate the efficacy and applicability of the proposed methodology.  © 2020 ACM.",multivariate functional data; Non-negative functional factorization; sparse representation; time warping,Information analysis; Matrix factorization; Base function; Correlated functions; Factorization methods; Functional data analysis; Multivariate functional datum; Non negatives; Non-negative factorization; Non-negative functional factorization; Sparse representation; Time warping; Data handling
Robust Adaptive Linear Discriminant Analysis with Bidirectional Reconstruction Constraint,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092710525&doi=10.1145%2f3409478&partnerID=40&md5=29e0478cba1cafc43a8fecbe18d578d2,"Linear discriminant analysis (LDA) is a well-known supervised method for dimensionality reduction in which the global structure of data can be preserved. The classical LDA is sensitive to the noises, and the projection direction of LDA cannot preserve the main energy. This article proposes a novel feature extraction model with l2,1 norm constraint based on LDA, termed as RALDA. This model preserves within-class local structure in the latent subspace according to the label information. To reduce information loss, it learns a projection matrix and an inverse projection matrix simultaneously. By introducing an implicit variable and matrix norm transformation, the alternating direction multiple method with updating variables is designed to solve the RALDA model. Moreover, both computational complexity and weak convergence property of the proposed algorithm are investigated. The experimental results on several public databases have demonstrated the effectiveness of our proposed method.  © 2020 ACM.",adaptive self-learning weights; bidirectional reconstruction constraint; face recognition; image classification; Linear discriminant analysis,Dimensionality reduction; Discriminant analysis; Inverse problems; Linear transformations; Alternating directions; Constraint-based; Inverse projections; Label information; Linear discriminant analysis; Projection direction; Projection matrix; Supervised methods; Matrix algebra
NGUARD+: An Attention-based Game Bot Detection Framework via Player Behavior Sequences,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092702890&doi=10.1145%2f3399711&partnerID=40&md5=3d76f3f9e0752c506f2b3aef41b3a5fc,"Game bots are automated programs that assist cheating users, leading to an imbalance in the game ecosystem and the collapse of user interest. Online games provide immersive gaming experience and attract many loyal fans. However, game bots have proliferated in volume and method, evolving with the real-world detection methods and showing strong diversity, leaving game bot detection efforts extremely difficult. Existing game bot detection techniques mostly rely on handcrafted features or time-series based features instead of fully utilizing player behavior sequences. In this regard, a more reasonable way should be learning user patterns from player behavior sequences when facing the fast-changing nature of game bots. Here we propose a general game bot detection framework for massively multiplayer online role playing games termed NGUARD+ (denoting NetEase Games' Guard), which captures user patterns in order to identify game bots from player behavior sequences. NGUARD+ mainly employs attention-based methods to automatically differentiate game bots from humans. We provide a combination of supervised and unsupervised methods for game bot detection to detect game bots and new type of game bots even when the labels of game bots are limited. Specifically, we propose the following two variants for attention-based sequence modeling: Attention based Bidirectional Long Short-Term Memory Networks (ABLSTM) and Hierarchical Self-Attention Network (HSAN) as our supervised models. ABLSTM is keen on inducing certain inductive biases which makes learning more reasonable as well as capturing local dependency and global information, while HSAN could handle much longer behavior sequences with less memory and higher computational efficiency. Experiments conducted on a real-world dataset show that NGUARD+ can achieve remarkable performance improvement compared to traditional methods. Moreover, NGUARD+ can reveal outstanding robustness for game bots in mutated patterns and even in completely unseen patterns.  © 2020 ACM.",attention mechanism; auto-iteration mechanism; Game bot detection; sequence modeling; transfer learning,Computational efficiency; Social networking (online); Behavior sequences; Detection methods; Global informations; Immersive gaming; Massively multiplayer online role-playing games; Sequence modeling; Short term memory; Unsupervised method; Botnet
REMIAN: Real-Time and Error-Tolerant Missing Value Imputation,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092695392&doi=10.1145%2f3412364&partnerID=40&md5=8018c6f4c5106e937e79c5b2d9e44fd3,"Missing value (MV) imputation is a critical preprocessing means for data mining. Nevertheless, existing MV imputation methods are mostly designed for batch processing, and thus are not applicable to streaming data, especially those with poor quality. In this article, we propose a framework, called Real-time and Error-tolerant Missing vAlue ImputatioN (REMAIN), to impute MVs in poor-quality streaming data. Instead of imputing MVs based on all the observed data, REMAIN first initializes the MV imputation model based on a-RANSAC which is capable of detecting and rejecting anomalies in an efficient manner, and then incrementally updates the model parameters upon the arrival of new data to support real-time MV imputation. As the correlations among attributes of the data may change over time in unforseenable ways, we devise a deterioration detection mechanism to capture the deterioration of the imputation model to further improve the imputation accuracy. Finally, we conduct an extensive evaluation on the proposed algorithms using real-world and synthetic datasets. Experimental results demonstrate that REMAIN achieves significantly higher imputation accuracy over existing solutions. Meanwhile, REMAIN improves up to one order of magnitude in time cost compared with existing approaches.  © 2020 ACM.",Missing value; poor-quality streaming data; real-time imputation,Batch data processing; Deterioration; Detection mechanism; Imputation methods; Missing value imputation; Missing values; Model parameters; Model-based OPC; Streaming data; Synthetic datasets; Data mining
Efficient Outlier Detection in Text Corpus Using Rare Frequency and Ranking,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092731344&doi=10.1145%2f3399712&partnerID=40&md5=106bd1a82ee8a070b5faf90be3fec304,"Outlier detection in text data collections has become significant due to the need of finding anomalies in the myriad of text data sources. High feature dimensionality, together with the larger size of these document collections, presents a need for developing accurate outlier detection methods with high efficiency. Traditional outlier detection methods face several challenges including data sparseness, distance concentration, and the presence of a larger number of sub-groups when dealing with text data. In this article, we propose to address these issues by developing novel concepts such as presenting documents with the rare document frequency, finding ranking-based neighborhood for similarity computation, and identifying sub-dense local neighborhoods in high dimensions. To improve the proposed primary method based on rare document frequency, we present several novel ensemble approaches using the ranking concept to reduce the false identifications while finding the higher number of true outliers. Extensive empirical analysis shows that the proposed method and its ensemble variations improve the quality of outlier detection in document repositories as well as they are found scalable compared to the relevant benchmarking methods.  © 2020 ACM.",high dimensional data; k-occurrences; Outlier detection; ranking function; term-weighting,Benchmarking; Data handling; Statistics; Benchmarking methods; Document collection; Document frequency; Document repositories; Empirical analysis; Ensemble approaches; Local neighborhoods; Similarity computation; Anomaly detection
Influence Maximization: Seeding Based on Community Structure,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092736633&doi=10.1145%2f3399661&partnerID=40&md5=741bc405387d201c1db727f89c0ad3e8,"Influence maximization problem attempts to find a small subset of nodes in a social network that makes the expected influence maximized, which has been researched intensively before. Most of the existing literature focus only on maximizing total influence, but it ignores whether the influential distribution is balanced through the network. Even though the total influence is maximized, but gathered in a certain area of social network. Sometimes, this is not advisable. In this article, we propose a novel seeding strategy based on community structure, and formulate the Influence Maximization with Community Budget (IMCB) problem. In this problem, the number of seed nodes in each community is under the cardinality constraint, which can be classified as the problem of monotone submodular maximization under the matroid constraint. To give a satisfactory solution for IMCB problem under the triggering model, we propose the IMCB-Framework, which is inspired by the idea of continuous greedy process and pipage rounding, and derive the best approximation ratio for this problem. In IMCB-Framework, we adopt sampling techniques to overcome the high complexity of continuous greedy. Then, we propose a simplified pipage rounding algorithm, which reduces the complexity of IMCB-Framework further. Finally, we conduct experiments on three real-world datasets to evaluate the correctness and effectiveness of our proposed algorithms, as well as the advantage of IMCB-Framework against classical greedy method.  © 2020 ACM.",approximation algorithm; community structure; continuous greedy; IMCB-Framework; Influence maximization; matorid; social network,Budget control; Best approximations; Cardinality constraints; Community structures; Influence maximizations; Real-world datasets; Rounding algorithm; Satisfactory solutions; Seeding strategies; Complex networks
Probabilistic Modeling for Frequency Vectors Using a Flexible Shifted-Scaled Dirichlet Distribution Prior,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092687489&doi=10.1145%2f3406242&partnerID=40&md5=6e3c45cdc6eabce8c95b2d23bca9eba9,"Burstiness and overdispersion phenomena of count vectors pose significant challenges in modeling such data accurately. While the dependency assumption of the multinomial distribution causes its failure to model frequency vectors in several machine learning and data mining applications, researchers found that by extending the multinomial distribution to the Dirichlet Compound multinomial (DCM), both phenomena modeling can be addressed. However, Dirichlet distribution is not the best choice, as a prior, given its negative-correlation and equal-confidence requirements. Thus, we propose to use a flexible generalization of the Dirichlet distribution, namely, the shifted-scaled Dirichlet, as a prior to the multinomial, which grants the model a capability to better fit real data, and we call the new model the Multinomial Shifted-Scaled Dirichlet (MSSD). Given that the likelihood function plays a key role in statistical inference, e.g., in maximum likelihood estimation and Fisher information matrix investigation, we propose to improve the efficiency of computing the MSSD log-likelihood by approximating its function based on Bernoulli polynomials where the log-likelihood function is computed using the proposed mesh algorithm. Moreover, given the sparsity and high-dimensionality nature of count vectors, we propose to improve its computation efficiency by approximating the novel MSSD as a member of the exponential family of distribution, which we call EMSSD. The clustering is based on mixture models, and for learning a model, selection approach is seamlessly integrated with the estimation of the parameters. The merits of the proposed approach are validated via challenging real-world applications such as hate speech detection in Twitter, real-time recognition of criminal action, and anomaly detection in crowded scenes. Results reveal that the proposed clustering frameworks offer a good compromise between other state-of-the-art techniques and outperform other approaches previously used for frequency vectors modeling. Besides, comparing to the MSSD, the approximation EMSSD has reduced the computational complexity in high-dimensional feature spaces.  © 2020 ACM.",Count data; exponential approximation; mixture models; online learning; SVM kernels,Anomaly detection; Data mining; Efficiency; Fisher information matrix; Inference engines; Maximum likelihood estimation; Speech recognition; Vectors; Computation efficiency; Data mining applications; Dirichlet distributions; High-dimensional feature space; Log-likelihood functions; Multinomial distributions; Probabilistic modeling; State-of-the-art techniques; Learning systems
Efficient Approaches to k Representative G-Skyline Queries,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092327817&doi=10.1145%2f3397503&partnerID=40&md5=865d9a702e6ce359cd4b2ce691be98eb,"The G-Skyline (GSky) query is a powerful tool to analyze optimal groups in decision support. Compared with other group skyline queries, it releases users from providing an aggregate function. Besides, it can get much comprehensive results without overlooking some important results containing non-skylines. However, it is hard for the users to make sensible choices when facing so many results the GSky query returns, especially over a large, high-dimensional dataset or with a large group size. In this article, we investigate k representative G-Skyline (kGSky) queries to obtain a manageable size of optimal groups. The kGSky query can also inherit the advantage of the GSky query; its results are representative and diversified. Next, we propose three exact algorithms with novel techniques including an upper bound pruning, a grouping strategy, a layered optimum strategy, and a hybrid strategy to efficiently process the kGSky query. Consider these exact algorithms have high time complexity and the precise results are not necessary in many applications. We further develop two approximate algorithms to trade off some accuracy for efficiency. Extensive experiments on both real and synthetic datasets demonstrate the efficiency, scalability, and accuracy of the proposed algorithms.  © 2020 ACM.",Approximate algorithms; data management; group skyline query; Progressive algorithms,Decision support systems; Economic and social effects; Efficiency; Aggregate function; Approximate algorithms; Decision supports; Grouping strategies; High-dimensional dataset; Hybrid strategies; Novel techniques; Synthetic datasets; Large dataset
MiSoSouP: Mining Interesting Subgroups with Sampling and Pseudodimension,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092324824&doi=10.1145%2f3385653&partnerID=40&md5=2bc84a9871fb1d40880b6ca5c7981a40,"We present MiSoSouP, a suite of algorithms for extracting high-quality approximations of the most interesting subgroups, according to different popular interestingness measures, from a random sample of a transactional dataset. We describe a new formulation of these measures as functions of averages, that makes it possible to approximate them using sampling. We then discuss how pseudodimension, a key concept from statistical learning theory, relates to the sample size needed to obtain an high-quality approximation of the most interesting subgroups. We prove an upper bound on the pseudodimension of the problem at hand, which depends on characteristic quantities of the dataset and of the language of patterns of interest. This upper bound then leads to small sample sizes. Our evaluation on real datasets shows that MiSoSouP outperforms state-of-the-art algorithms offering the same guarantees, and it vastly speeds up the discovery of subgroups w.r.t. analyzing the whole dataset.  © 2020 ACM.",Pattern mining; statistical learning theory,Sampling; High quality; Interestingness measures; Random sample; Real data sets; Sample sizes; Small Sample Size; State-of-the-art algorithms; Statistical learning theory; Approximation algorithms
Learning Distance Metrics from Probabilistic Information,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092271369&doi=10.1145%2f3364320&partnerID=40&md5=1fdb9b3aa8fa276a556ece2f631899a3,"The goal of metric learning is to learn a good distance metric that can capture the relationships among instances, and its importance has long been recognized in many fields. An implicit assumption in the traditional settings of metric learning is that the associated labels of the instances are deterministic. However, in many real-world applications, the associated labels come naturally with probabilities instead of deterministic values, which makes it difficult for the existing metric-learning methods to work well in these applications. To address this challenge, in this article, we study how to effectively learn the distance metric from datasets that contain probabilistic information, and then propose several novel metric-learning mechanisms for two types of probabilistic labels, i.e., the instance-wise probabilistic label and the group-wise probabilistic label. Compared with the existing metric-learning methods, our proposed mechanisms are capable of learning distance metrics directly from the probabilistic labels with high accuracy. We also theoretically analyze the proposed mechanisms and conduct extensive experiments on real-world datasets to verify the desirable properties of these mechanisms.  © 2020 ACM.",distance measure; Metric learning; probabilistic labels,Computer science; Data mining; Distance metrics; High-accuracy; Metric learning; Probabilistic information; Real-world; Real-world datasets; Learning systems
Non-Redundant Subspace Clusterings with Nr-Kmeans and Nr-DipMeans,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092282722&doi=10.1145%2f3385652&partnerID=40&md5=aef8aee4525cb702d6b72b3a77590a1a,"A huge object collection in high-dimensional space can often be clustered in more than one way, for instance, objects could be clustered by their shape or alternatively by their color. Each grouping represents a different view of the dataset. The new research field of non-redundant clustering addresses this class of problems. In this article, we follow the approach that different, non-redundant k-means-like clusterings may exist in different, arbitrarily oriented subspaces of the high-dimensional space. We assume that these subspaces (and optionally a further noise space without any cluster structure) are orthogonal to each other. This assumption enables a particularly rigorous mathematical treatment of the non-redundant clustering problem and thus a particularly efficient algorithm, which we call Nr-Kmeans (for non-redundant k-means). The superiority of our algorithm is demonstrated both theoretically, as well as in extensive experiments. Further, we propose an extension of Nr-Kmeans that harnesses Hartigan's dip test to identify the number of clusters for each subspace automatically.  © 2020 ACM.",Clustering; k-means; non-redundant; subspace,Computer science; Data mining; Cluster structure; High dimensional spaces; Mathematical treatments; Noise spaces; Non-redundant; Non-redundant clustering; Number of clusters; Research fields; K-means clustering
"On Proximity and Structural Role-based Embeddings in Networks: Misconceptions, Techniques, and Applications",2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092308057&doi=10.1145%2f3397191&partnerID=40&md5=46abe8ac5306d3a1cabbe886699422c8,"Structural roles define sets of structurally similar nodes that are more similar to nodes inside the set than outside, whereas communities define sets of nodes with more connections inside the set than outside. Roles based on structural similarity and communities based on proximity are fundamentally different but important complementary notions. Recently, the notion of structural roles has become increasingly important and has gained a lot of attention due to the proliferation of work on learning representations (node/edge embeddings) from graphs that preserve the notion of roles. Unfortunately, recent work has sometimes confused the notion of structural roles and communities (based on proximity) leading to misleading or incorrect claims about the capabilities of network embedding methods. As such, this article seeks to clarify the misconceptions and key differences between structural roles and communities, and formalize the general mechanisms (e.g., random walks and feature diffusion) that give rise to community-or role-based structural embeddings. We theoretically prove that embedding methods based on these mechanisms result in either community-or role-based structural embeddings. These mechanisms are typically easy to identify and can help researchers quickly determine whether a method preserves community-or role-based embeddings. Furthermore, they also serve as a basis for developing new and improved methods for community-or role-based structural embeddings. Finally, we analyze and discuss applications and data characteristics where community-or role-based embeddings are most appropriate.  © 2020 ACM.",community-based embedding; graphlets; network representation learning; node embeddings; positions; proximity; proximity-based node embeddings; role discovery; Role-based structural embedding; roles; structural embeddings; structural node embeddings; structural similarity,Computer science; Data mining; Community OR; Data characteristics; Embedding method; In networks; Network embedding; Random Walk; Role-based; Structural similarity; Embeddings
Multi-User Mobile Sequential Recommendation for Route Optimization,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092312153&doi=10.1145%2f3360048&partnerID=40&md5=f24aa479f284fbffada9e1698d8dca3d,"We enhance the mobile sequential recommendation (MSR) model and address some critical issues in existing formulations by proposing three new forms of the MSR from a multi-user perspective. The multi-user MSR (MMSR) model searches optimal routes for multiple drivers at different locations while disallowing overlapping routes to be recommended. To enrich the properties of pick-up points in the problem formulation, we additionally consider the pick-up capacity as an important feature, leading to the following two modified forms of the MMSR: MMSR-m and MMSR-d. The MMSR-m sets a maximum pick-up capacity for all urban areas, while the MMSR-d allows the pick-up capacity to vary at different locations. We develop a parallel framework based on the simulated annealing to numerically solve the MMSR problem series. Also, a push-point method is introduced to improve our algorithms further for the MMSR-m and the MMSR-d, which can handle the route optimization in more practical ways. Our results on both real-world and synthetic data confirmed the superiority of our problem formulation and solutions under more demanding practical scenarios over several published benchmarks.  © 2020 ACM.",Mobile sequential recommendation; parallel computing; potential traveling distance; simulated annealing; trajectory data analysis,Simulated annealing; Critical issues; Important features; Optimal routes; Parallel framework; Pick up points; Problem formulation; Route optimization; Synthetic data; Pickups
Towards an Optimal Outdoor Advertising Placement: When a Budget Constraint Meets Moving Trajectories,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092327359&doi=10.1145%2f3350488&partnerID=40&md5=6155f314312e354d388f6f8aed58c7c9,"In this article, we propose and study the problem of trajectory-driven influential billboard placement: given a set of billboards U (each with a location and a cost), a database of trajectories T, and a budget L, we find a set of billboards within the budget to influence the largest number of trajectories. One core challenge is to identify and reduce the overlap of the influence from different billboards to the same trajectories, while keeping the budget constraint into consideration. We show that this problem is NP-hard and present an enumeration based algorithm with (1-1/e) approximation ratio. However, the enumeration would be very costly when |U| is large. By exploiting the locality property of billboards' influence, we propose a partition-based framework PartSel. PartSel partitions U into a set of small clusters, computes the locally influential billboards for each cluster, and merges them to generate the global solution. Since the local solutions can be obtained much more efficiently than the global one, PartSel would reduce the computation cost greatly; meanwhile it achieves a non-trivial approximation ratio guarantee. Then we propose a LazyProbe method to further prune billboards with low marginal influence, while achieving the same approximation ratio as PartSel. Next, we propose a branch-and-bound method to eliminate unnecessary enumerations in both PartSel and LazyProbe, as well as an aggregated index to speed up the computation of marginal influence. Experiments on real datasets verify the efficiency and effectiveness of our methods.  © 2020 ACM.",influence maximization; Outdoor advertising; trajectory,Approximation algorithms; Budget control; NP-hard; Trajectories; Approximation ratios; Budget constraint; Computation costs; Global solutions; Non-trivial; Outdoor advertisings; Real data sets; Small clusters; Branch and bound method
Adversarial Attacks on Graph Neural Networks: Perturbations and their Paterns,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092283312&doi=10.1145%2f3394520&partnerID=40&md5=97601604c2a39ab8e1b36eaae1e4618a,"Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, little is known about their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g., the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we present a study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain, we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given. For the first time, we successfully identify important patterns of adversarial attacks on graph neural networks (GNNs)-a first step towards being able to detect adversarial attacks on GNNs.  © 2020 ACM.",adversarial attacks; graph neural networks; poisoning attacks; Relational data,Deep learning; Graph structures; Graph theory; Learning systems; Turing machines; Attributed graphs; Classification models; Data characteristics; Graph neural networks; Incremental computation; Machine learning models; State of the art; Unsupervised approaches; Neural networks
A General Coreset-Based Approach to Diversity Maximization under Matroid Constraints,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092268275&doi=10.1145%2f3402448&partnerID=40&md5=288d303bb2919ba1e52be778099c9e0b,"Diversity maximization is a fundamental problem in web search and data mining. For a given dataset S of n elements, the problem requires to determine a subset of S containing kg n ""representatives""which maximize some diversity function expressed in terms of pairwise distances, where distance models dissimilarity. An important variant of the problem prescribes that the solution satisfy an additional orthogonal requirement, which can be specified as a matroid constraint (i.e., a feasible solution must be an independent set of size k of a given matroid). While unconstrained diversity maximization admits efficient coreset-based strategies for several diversity functions, known approaches dealing with the additional matroid constraint apply only to one diversity function (sum of distances), and are based on an expensive, inherently sequential, local search over the entire input dataset. We devise the first coreset-based algorithms for diversity maximization under matroid constraints for various diversity functions, together with efficient sequential, MapReduce, and Streaming implementations. Technically, our algorithms rely on the construction of a small coreset, that is, a subset of S containing a feasible solution which is no more than a factor 1-I away from the optimal solution for S. While our algorithms are fully general, for the partition and transversal matroids, if I is a constant in (0,1) and S has bounded doubling dimension, the coreset size is independent of n and it is small enough to afford the execution of a slow sequential algorithm to extract a final, accurate, solution in reasonable time. Extensive experiments show that our algorithms are accurate, fast, and scalable, and therefore they are capable of dealing with the large input instances typical of the big data scenario.  © 2020 ACM.",approximation algorithms; coresets; Diversity maximization; doubling spaces; MapReduce; matroids; streaming,Data mining; Large dataset; Diversity function; Doubling dimensions; Feasible solution; Independent set; Optimal solutions; Pairwise distances; Sequential algorithm; Sum of distances; Combinatorial mathematics
Efficient Mining of Outlying Sequence Patterns for Analyzing Outlierness of Sequence Data,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092310503&doi=10.1145%2f3399671&partnerID=40&md5=8173b5a923765ca9aa0cc6c7f34dc2b1,"Recently, a lot of research work has been proposed in different domains to detect outliers and analyze the outlierness of outliers for relational data. However, while sequence data is ubiquitous in real life, analyzing the outlierness for sequence data has not received enough attention. In this article, we study the problem of mining outlying sequence patterns in sequence data addressing the question: given a query sequence s in a sequence dataset D, the objective is to discover sequence patterns that will indicate the most unusualness (i.e., outlierness) of s compared against other sequences. Technically, we use the rank defined by the average probabilistic strength (aps) of a sequence pattern in a sequence to measure the outlierness of the sequence. Then a minimal sequence pattern where the query sequence is ranked the highest is defined as an outlying sequence pattern. To address the above problem, we present OSPMiner, a heuristic method that computes aps by incorporating several pruning techniques. Our empirical study using both real and synthetic data demonstrates that OSPMiner is effective and efficient.  © 2020 ACM.",average probabilistic strength; outlierness analysis; Outlying sequence pattern; sequence mining,Heuristic methods; Different domains; Empirical studies; Pruning techniques; Query sequence; Relational data; Sequence data; Sequence patterns; Synthetic data; Statistics
Pop Music Generation: From Melody to Multi-style Arrangement,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092267293&doi=10.1145%2f3374915&partnerID=40&md5=f5546916def1accc8012648469c23e0e,"Music plays an important role in our daily life. With the development of deep learning and modern generation techniques, researchers have done plenty of works on automatic music generation. However, due to the special requirements of both melody and arrangement, most of these methods have limitations when applying to multi-track music generation. Some critical factors related to the quality of music are not well addressed, such as chord progression, rhythm pattern, and musical style. In order to tackle the problems and ensure the harmony of multi-track music, in this article, we propose an end-to-end melody and arrangement generation framework to generate a melody track with several accompany tracks played by some different instruments. To be specific, we first develop a novel Chord based Rhythm and Melody Cross-Generation Model to generate melody with a chord progression. Then, we propose a Multi-Instrument Co-Arrangement Model based on multi-task learning for multi-track music arrangement. Furthermore, to control the musical style of arrangement, we design a Multi-Style Multi-Instrument Co-Arrangement Model to learn the musical style with adversarial training. Therefore, we can not only maintain the harmony of the generated music but also control the musical style for better utilization. Extensive experiments on a real-world dataset demonstrate the superiority and effectiveness of our proposed models.  © 2020 ACM.",Harmony evaluation; melody and arrangement generation; multi-task joint learning; Music generation; musical style,Deep learning; Multi-task learning; Arrangement model; Critical factors; Daily lives; End to end; Generation techniques; Multi tracks; Real-world; Learning systems
A Unified Framework for Sparse Online Learning,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092277250&doi=10.1145%2f3361559&partnerID=40&md5=ed181b17b323368d21ad2c243ca8903d,"The amount of data in our society has been exploding in the era of big data. This article aims to address several open challenges in big data stream classification. Many existing studies in data mining literature follow the batch learning setting, which suffers from low efficiency and poor scalability. To tackle these challenges, we investigate a unified online learning framework for the big data stream classification task. Different from the existing online data stream classification techniques, we propose a unified Sparse Online Classification (SOC) framework. Based on SOC, we derive a second-order online learning algorithm and a cost-sensitive sparse online learning algorithm, which could successfully handle online anomaly detection tasks with the extremely unbalanced class distribution. As the performance evaluation, we analyze the theoretical bounds of the proposed algorithms and conduct an extensive set of experiments. The encouraging experimental results demonstrate the efficacy of the proposed algorithms over the state-of-the-art techniques on multiple data stream classification tasks.  © 2020 ACM.",classification; cost-sensitive learning; Online learning; sparse learning,Anomaly detection; Big data; Classification (of information); Data mining; Data streams; E-learning; Class distributions; Data stream classifications; Multiple data streams; On-line classification; Online anomaly detection; Online learning algorithms; State-of-the-art techniques; Theoretical bounds; Learning algorithms
End-to-End Continual Rare-Class Recognition with Emerging Novel Subclasses,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092303888&doi=10.1145%2f3399660&partnerID=40&md5=bb15e16aa1cce149c7a5af1041b2bd66,"Given a labeled dataset that contains a rare (or minority) class containing of-interest instances, as well as a large class of instances that are not of interest, how can we learn to recognize future of-interest instances over a continuous stream? The setting is different from traditional classification in that instances from novel minority subclasses might continually emerge over time-and hence is often referred as continual, life-long, or open-world classification. We introduce RaRecognize, which (i) estimates a general decision boundary between the rare class and the majority class, (ii) learns to recognize the individual rare subclasses that exist within the training data, as well as (iii) flags instances from previously unseen rare subclasses as newly emerging (i.e., novel). The learner in (i) is general in the sense that by construction it is dissimilar to the specialized learners in (ii), thus distinguishes minority from the majority without overly tuning to what is only seen in the training data. Thanks to this generality, RaRecognize ignores all future instances that it labels as majority and recognizes the recurring as well as emerging rare subclasses only. This saves effort at test time as well as ensures that the model size grows moderately over time as it only maintains specialized minority learners. Overall, we build an end-to-end system which consists of (1) a representation learning component that transforms data instances into suitable vector inputs; (2) a continual classifier that labels incoming instances as majority (not of interest), rare recurrent, or rare emerging; and (3) a clustering component that groups the rare emerging instances into novel subclasses for expert vetting and model re-training. Through extensive experiments, we show that RaRecognize outperforms state-of-the art baselines on three real-world datasets that contain documents related to corporate-risk and (natural and man-made) disasters as rare classes.  © 2020 ACM.",classification with emerging classes; Continual learning; generalization to novel classes; open-world classification; rare class recognition,Large dataset; Decision boundary; End to end; End-to-end systems; Labeled dataset; Model size; Real-world datasets; State of the art; Training data; Learning systems
Introduction to the Special Issue on the Best Papers from KDD 2018,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092291489&doi=10.1145%2f3407901&partnerID=40&md5=a5d16cbf3a190740a0dedae8c2f9f41b,[No abstract available],,
Edge2vec: Edge-based Social Network Embedding,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088477910&doi=10.1145%2f3391298&partnerID=40&md5=fc86230d0db277d25063503062c2acc9,"Graph embedding, also known as network embedding and network representation learning, is a useful technique which helps researchers analyze information networks through embedding a network into a low-dimensional space. However, existing graph embedding methods are all node-based, which means they can just directly map the nodes of a network to low-dimensional vectors while the edges could only be mapped to vectors indirectly. One important reason is the computational cost, because the number of edges is always far greater than the number of nodes. In this article, considering an important property of social networks, i.e., the network is sparse, and hence the average degree of nodes is bounded, we propose an edge-based graph embedding (edge2vec) method to map the edges in social networks directly to low-dimensional vectors. Edge2vec takes both the local and the global structure information of edges into consideration to preserve structure information of embedded edges as much as possible. To achieve this goal, edge2vec first ingeniously combines the deep autoencoder and Skip-gram model through a well-designed deep neural network. The experimental results on different datasets show edge2vec benefits from the direct mapping in preserving the structure information of edges.  © 2020 ACM.",deep autoencoder; Edge2vec; Graph embedding; skip-gram,Deep neural networks; Embeddings; Graph theory; Information services; Neural networks; Computational costs; Global structure; Graph embeddings; Information networks; Low-dimensional spaces; Network embedding; Network representation; Structure information; Graph structures
Efficient Nonnegative Tensor Factorization via Saturating Coordinate Descent,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088454873&doi=10.1145%2f3385654&partnerID=40&md5=835b08437e920eb77c07a317cac45c05,"With the advancements in computing technology and web-based applications, data are increasingly generated in multi-dimensional form. These data are usually sparse due to the presence of a large number of users and fewer user interactions. To deal with this, the Nonnegative Tensor Factorization (NTF) based methods have been widely used. However existing factorization algorithms are not suitable to process in all three conditions of size, density, and rank of the tensor. Consequently, their applicability becomes limited. In this article, we propose a novel fast and efficient NTF algorithm using the element selection approach. We calculate the element importance using Lipschitz continuity and propose a saturation point-based element selection method that chooses a set of elements column-wise for updating to solve the optimization problem. Empirical analysis reveals that the proposed algorithm is scalable in terms of tensor size, density, and rank in comparison to the relevant state-of-the-art algorithms.  © 2020 ACM.",coordinate descent; element selection; Nonnegative tensor factorization; pattern mining; recommender systems; saturating coordinate descent,Factorization; Computing technology; Empirical analysis; Factorization algorithms; Lipschitz continuity; Nonnegative tensor factorizations; Optimization problems; State-of-the-art algorithms; Web-based applications; Tensors
Social Collaborative Mutual Learning for Item Recommendation,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088464556&doi=10.1145%2f3387162&partnerID=40&md5=7005578e45cd86ca9be08681f81eef75,"Recommender Systems (RSs) provide users with item choices based on their preferences reflected in past interactions and become important tools to alleviate the information overload problem for users. However, in real-world scenarios, the user-item interaction matrix is generally sparse, leading to the poor performance of recommendation methods. To cope with this problem, social information is introduced into these methods in several ways, such as regularization, ensemble, and sampling. However, these strategies to use social information have their limitations. The regularization and ensemble strategies may suffer from the over-smoothing problem, while the sampling-based strategy may be affected by the overfitting problem. To overcome the limitations of the previous efforts, a novel social recommendation model, namely, Social Collaborative Mutual Learning (SCML), is proposed in this article. SCML combines the item-based CF model with the social CF model by two well-designed mutual regularization strategies. The embedding-level mutual regularization forces the user representations in two models to be close, and the output-level mutual regularization matches the distributions of the predictions in two models. Extensive experiments on three public datasets show that SCML significantly outperforms the baseline methods and the proposed mutual regularization strategies can embrace the advantages of the item-based CF model and the social CF model to improve the recommendation performance.  © 2020 ACM.",collaborative filtering; mutual learning; Recommender systems; social network,Recommender systems; Social aspects; Ensemble strategies; Information overloads; Interaction matrices; Over fitting problem; Real-world scenario; Recommendation methods; Recommendation performance; Regularization strategies; Predictive analytics
Learning Bayesian Networks with the Saiyan Algorithm,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088441386&doi=10.1145%2f3385655&partnerID=40&md5=ebdbda7329ed5ac57accfdb940f7f221,"Some structure learning algorithms have proven to be effective in reconstructing hypothetical Bayesian Network graphs from synthetic data. However, in their mission to maximise a scoring function, many become conservative and minimise edges discovered. While simplicity is desired, the output is often a graph that consists of multiple independent subgraphs that do not enable full propagation of evidence. While this is not a problem in theory, it can be a problem in practice. This article examines a novel unconventional associational heuristic called Saiyan, which returns a directed acyclic graph that enables full propagation of evidence. Associational heuristics are not expected to perform well relative to sophisticated constraint-based and score-based learning approaches. Moreover, forcing the algorithm to connect all data variables implies that the forced edges will not be correct at the rate of those identified unrestrictedly. Still, synthetic and real-world experiments suggest that such a heuristic can be competitive relative to some of the well-established constraint-based, score-based and hybrid learning algorithms.  © 2020 ACM.",Bayesian networks; directed acyclic graphs; graphical models; structure learning,Bayesian networks; Directed graphs; Graph algorithms; Heuristic algorithms; Constraint-based; Directed acyclic graph (DAG); Hybrid learning algorithm; Learning approach; Learning Bayesian networks; Real world experiment; Scoring functions; Structure learning algorithm; Learning algorithms
Discovering Anomalies by Incorporating Feedback from an Expert,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088444938&doi=10.1145%2f3396608&partnerID=40&md5=a9ad73330d3c95fa7cfaca426b69366e,"Unsupervised anomaly detection algorithms search for outliers and then predict that these outliers are the anomalies. When deployed, however, these algorithms are often criticized for high false-positive and high false-negative rates. One main cause of poor performance is that not all outliers are anomalies and not all anomalies are outliers. In this article, we describe the Active Anomaly Discovery (AAD) algorithm, which incorporates feedback from an expert user that labels a queried data instance as an anomaly or nominal point. This feedback is intended to adjust the anomaly detector so that the outliers it discovers are more in tune with the expert user's semantic understanding of the anomalies. The AAD algorithm is based on a weighted ensemble of anomaly detectors. When it receives a label from the user, it adjusts the weights on each individual ensemble member such that the anomalies rank higher in terms of their anomaly score than the outliers. The AAD approach is designed to operate in an interactive data exploration loop. In each iteration of this loop, our algorithm first selects a data instance to present to the expert as a potential anomaly and then the expert labels the instance as an anomaly or as a nominal data point. When it receives the instance label, the algorithm updates its internal model and the loop continues until a budget of B queries is spent. The goal of our approach is to maximize the total number of true anomalies in the B instances presented to the expert. We show that the AAD method performs well and in some cases doubles the number of true anomalies found compared to previous methods. In addition we present approximations that make the AAD algorithm much more computationally efficient while maintaining a desirable level of performance.  © 2020 ACM.",Active learning; anomaly detection; optimization; user feedback,Approximation algorithms; Budget control; Iterative methods; Semantics; Statistics; Computationally efficient; Ensemble members; False negative rate; Interactive data exploration; Internal modeling; Poor performance; Semantic understanding; Unsupervised anomaly detection; Anomaly detection
Internal Evaluation of Unsupervised Outlier Detection,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088453243&doi=10.1145%2f3394053&partnerID=40&md5=49f21febae8dbaa08ad3ce1f08997aac,"Although there is a large and growing literature that tackles the unsupervised outlier detection problem, the unsupervised evaluation of outlier detection results is still virtually untouched in the literature. The so-called internal evaluation, based solely on the data and the assessed solutions themselves, is required if one wants to statistically validate (in absolute terms) or just compare (in relative terms) the solutions provided by different algorithms or by different parameterizations of a given algorithm in the absence of labeled data. However, in contrast to unsupervised cluster analysis, where indexes for internal evaluation and validation of clustering solutions have been conceived and shown to be very useful, in the outlier detection domain, this problem has been notably overlooked. Here we discuss this problem and provide a solution for the internal evaluation of outlier detection results. Specifically, we describe an index called Internal, Relative Evaluation of Outlier Solutions (IREOS) that can evaluate and compare different candidate outlier detection solutions. Initially, the index is designed to evaluate binary solutions only, referred to as top-n outlier detection results. We then extend IREOS to the general case of non-binary solutions, consisting of outlier detection scorings. We also statistically adjust IREOS for chance and extensively evaluate it in several experiments involving different collections of synthetic and real datasets.  © 2020 ACM.",Outlier detection; unsupervised evaluation; validation,Cluster analysis; Statistics; Binary solutions; Clustering solutions; Non-binary; Real data sets; Unsupervised cluster; Anomaly detection
Sparse Graph Connectivity for Image Segmentation,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088481831&doi=10.1145%2f3397188&partnerID=40&md5=6df882cf3519cd85d5513e71a2ebc2cb,"It has been demonstrated that the segmentation performance is highly dependent on both subspace preservation and graph connectivity. In the literature, the full connectivity method linearly represents each data point (e.g., a pixel in one image) by all data points for achieving subspace preservation, while the sparse connectivity method was designed to linearly represent each data point by a set of data points for achieving graph connectivity. However, previous methods only focused on either subspace preservation or graph connectivity. In this article, we propose a Sparse Graph Connectivity (SGC) method for image segmentation to automatically learn the affinity matrix from the low-dimensional space of original data, which aims at simultaneously achieving subspace preservation and graph connectivity. To do this, the proposed SGC simultaneously learns a self-representation affinity matrix for subspace preservation and a sparse affinity matrix for graph connectivity, from the intrinsic low-dimensional feature space of high-dimensional original data. Meanwhile, the self-representation affinity matrix is pushed to be similar to the sparse affinity as well as be the final segmentation results. Experimental result on synthetic and real-image datasets showed that our SGC method achieved the best segmentation performance, compared to state-of-the-art segmentation methods.  © 2020 ACM.",clustering; Image segmentation; similarity measurement; sparse learning,Graph theory; Matrix algebra; Full connectivities; Graph connectivity; High-dimensional; Low-dimensional spaces; Segmentation methods; Segmentation performance; Segmentation results; State of the art; Image segmentation
Incomplete Network Alignment: Problem Definitions and Fast Solutions,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088448863&doi=10.1145%2f3384203&partnerID=40&md5=9cd681e557ae2f2bd56a698aee04c471,"Networks are prevalent in many areas and are often collected from multiple sources. However, due to the veracity characteristics, more often than not, networks are incomplete. Network alignment and network completion have become two fundamental cornerstones behind a wealth of high-impact graph mining applications. The state-of-the-art have been addressing these two tasks in parallel. That is, most of the existing network alignment methods have implicitly assumed that the topology of the input networks for alignment are perfectly known a priori, whereas the existing network completion methods admit either a single network (i.e., matrix completion) or multiple aligned networks (e.g., tensor completion). In this article, we argue that network alignment and completion are inherently complementary with each other, and hence propose to jointly address them so that the two tasks can mutually benefit from each other. We formulate the problem from the optimization perspective, and propose an effective algorithm (iNeAt) to solve it. The proposed method offers two distinctive advantages. First (Alignment accuracy), our method benefits from the higher-quality input networks while mitigates the effect of the incorrectly inferred links introduced by the completion task itself. Second (Alignment efficiency), thanks to the low-rank structure of the complete networks and the alignment matrix, the alignment process can be significantly accelerated. We perform extensive experiments which show that (1) the network completion can significantly improve the alignment accuracy, i.e., up to 30% over the baseline methods; (2) the network alignment can in turn help recover more missing edges than the baseline methods; and (3) our method achieves a good balance between the running time and the accuracy, and scales with a provable linear complexity in both time and space.  © 2020 ACM.",Incomplete network alignment; low rank; network completion,Data mining; Alignment accuracy; Complete networks; Completion methods; Effective algorithms; Incomplete networks; Network alignments; Problem definition; Tensor completion; Computer science
The Gene of Scientific Success,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088433549&doi=10.1145%2f3385530&partnerID=40&md5=09f0e51e6a599ea8e5ddef101780c782,"This article elaborates how to identify and evaluate causal factors to improve scientific impact. Currently, analyzing scientific impact can be beneficial to various academic activities including funding application, mentor recommendation, discovering potential cooperators, and the like. It is universally acknowledged that high-impact scholars often have more opportunities to receive awards as an encouragement for their hard work. Therefore, scholars spend great efforts in making scientific achievements and improving scientific impact during their academic life. However, what are the determinate factors that control scholars' academic success? The answer to this question can help scholars conduct their research more efficiently. Under this consideration, our article presents and analyzes the causal factors that are crucial for scholars' academic success. We first propose five major factors including article-centered factors, author-centered factors, venue-centered factors, institution-centered factors, and temporal factors. Then, we apply recent advanced machine learning algorithms and jackknife method to assess the importance of each causal factor. Our empirical results show that author-centered and article-centered factors have the highest relevancy to scholars' future success in the computer science area. Additionally, we discover an interesting phenomenon that the h-index of scholars within the same institution or university are actually very close to each other.  © 2020 ACM.",academic networks; feature selection; machine learning; Scientific impact,Learning algorithms; Academic activities; H indices; Hard work; High impact; Jack-knife method; Major factors; Scientific achievements; Machine learning
Citywide Traffic Flow Prediction Based on Multiple Gated Spatio-temporal Convolutional Neural Networks,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088454825&doi=10.1145%2f3385414&partnerID=40&md5=9d4ad42e42910d13511513b3e426e702,"Traffic flow prediction is crucial for public safety and traffic management, and remains a big challenge because of many complicated factors, e.g., multiple spatio-temporal dependencies, holidays, and weather. Some work leveraged 2D convolutional neural networks (CNNs) and long short-term memory networks (LSTMs) to explore spatial relations and temporal relations, respectively, which outperformed the classical approaches. However, it is hard for these work to model spatio-temporal relations jointly. To tackle this, some studies utilized LSTMs to connect high-level layers of CNNs, but left the spatio-temporal correlations not fully exploited in low-level layers. In this work, we propose novel spatio-temporal CNNs to extract spatio-temporal features simultaneously from low-level to high-level layers, and propose a novel gated scheme to control the spatio-temporal features that should be propagated through the hierarchy of layers. Based on these, we propose an end-to-end framework, multiple gated spatio-temporal CNNs (MGSTC), for citywide traffic flow prediction. MGSTC can explore multiple spatio-temporal dependencies through multiple gated spatio-temporal CNN branches, and combine the spatio-temporal features with external factors dynamically. Extensive experiments on two real traffic datasets demonstrates that MGSTC outperforms other state-of-the-art baselines.  © 2020 ACM.",CNNs; spatio-temporal analysis; traffic flow prediction; Traffic prediction,Convolution; Forecasting; Traffic control; Classical approach; Spatio temporal features; Spatio-temporal dependencies; Spatio-temporal relations; Spatiotemporal correlation; Temporal relation; Traffic flow prediction; Traffic management; Convolutional neural networks
Fully Dynamic Approximate k-Core Decomposition in Hypergraphs,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088451713&doi=10.1145%2f3385416&partnerID=40&md5=47a17170dedc6a4099e70f302ba0000d,"In this article, we design algorithms to maintain approximate core values in dynamic hypergraphs. This notion has been well studied for normal graphs in both static and dynamic setting. We generalize the problem to hypergraphs when edges can be inserted or deleted by an adversary. We consider two dynamic scenarios. In the first case, there are only insertions; and in the second case, there can be both insertions and deletions. In either case, the update time is poly-logarithmic in the number of nodes, with the insertion-only case boasting a better approximation ratio. We also perform extensive experiments on large real-world datasets, which demonstrate the accuracy and efficiency of our algorithms.  © 2020 ACM.",Core values; dynamic network; hypergraphs,Large dataset; Approximation ratios; Core values; Dynamic scenarios; Dynamic settings; Insertions and deletions; Normal graphs; Only insertion; Real-world datasets; Graph theory
Neural Serendipity Recommendation: Exploring the Balance between Accuracy and Novelty with Sparse Explicit Feedback,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088461239&doi=10.1145%2f3396607&partnerID=40&md5=e1efc03b268275de6396e06abba40380,"Recommender systems have been playing an important role in providing personalized information to users. However, there is always a trade-off between accuracy and novelty in recommender systems. Usually, many users are suffering from redundant or inaccurate recommendation results. To this end, in this article, we put efforts into exploring the hidden knowledge of observed ratings to alleviate this recommendation dilemma. Specifically, we utilize some basic concepts to define a concept, Serendipity, which is characterized by high-satisfaction and low-initial-interest. Based on this concept, we propose a two-phase recommendation problem which aims to strike a balance between accuracy and novelty achieved by serendipity prediction and personalized recommendation. Along this line, a Neural Serendipity Recommendation (NSR) method is first developed by combining Muti-Layer Percetron and Matrix Factorization for serendipity prediction. Then, a weighted candidate filtering method is designed for personalized recommendation. Finally, extensive experiments on real-world data demonstrate that NSR can achieve a superior serendipity by a 12% improvement in average while maintaining stable accuracy compared with state-of-the-art methods.  © 2020 ACM.",matrix factorization; muti-layer percetron; recommender system; Serendipity,Economic and social effects; Factorization; Basic concepts; Explicit feedback; Filtering method; Hidden knowledge; Matrix factorizations; Personalized information; Personalized recommendation; State-of-the-art methods; Recommender systems
Self-weighted Multi-view Fuzzy Clustering,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087714997&doi=10.1145%2f3396238&partnerID=40&md5=e951cdeb93e6d8b0ad37a075aca22a58,"Since the data in each view may contain distinct information different from other views as well as has common information for all views in multi-view learning, many multi-view clustering methods have been designed to use these information (including the distinct information for each view and the common information for all views) to improve the clustering performance. However, previous multi-view clustering methods cannot effectively detect these information so that difficultly outputting reliable clustering models. In this article, we propose a fuzzy, sparse, and robust multi-view clustering method to consider all kinds of relations among the data (such as view importance, view stability, and view diversity), which can effectively extract both distinct information and common information as well as balance these two kinds of information. Moreover, we devise an alternating optimization algorithm to solve the resulting objective function as well as prove that our proposed algorithm achieves fast convergence. It is noteworthy that existing multi-view clustering methods only consider a part of the relations, and thus are a special case of our proposed framework. Experimental results on synthetic datasets and real datasets show that our proposed method outperforms the state-of-the-art clustering methods in terms of evaluation metrics of clustering such as clustering accuracy, normalized mutual information, purity, and adjusted rand index.  © 2020 ACM.",fuzzy clustering; Multi-view clustering; sparse learning,Computer science; Data mining; Adjusted rand index; Alternating optimizations; Clustering accuracy; Multi-view clustering; Multi-view learning; Normalized mutual information; Objective functions; Synthetic datasets; Cluster analysis
Framework for inferring following strategies from time series of movement data,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085263134&doi=10.1145%2f3385730&partnerID=40&md5=f3478811ffb094f6b73a6ef88a99702c,"How do groups of individuals achieve consensus in movement decisions? Do individuals follow their friends, the one predetermined leader, or whomever just happens to be nearby? To address these questions computationally, we formalize Coordination Strategy Inference Problem. In this setting, a group of multiple individuals moves in a coordinated manner toward a target path. Each individual uses a specific strategy to follow others (e.g., nearest neighbors, pre-defined leaders, and preferred friends). Given a set of time series that includes coordinated movement and a set of candidate strategies as inputs, we provide the first methodology (to the best of our knowledge) to infer whether each individual uses local-agreement system or dictatorship-like strategy to achieve movement coordination at the group level. We evaluate and demonstrate the performance of the proposed framework by predicting directions of movement of an individual in a group in both simulated datasets as well as in two real-world datasets: a school of fish and a troop of baboons. Moreover, since there is no prior methodology for inferring individual-level strategies, we compare our framework with the state-of-the-art approach for the task of classification of group-level-coordination models. Results show that our approach is highly accurate in inferring correct strategies in simulated datasets even in complicated mixed strategy settings, which no existing method can infer. In the task of classification of group-level-coordination models, our framework performs better than the state-of-the-art approach in all datasets. Animal data experiments show that fish, as expected, follow their neighbors, while baboons have a preference to follow specific individuals. Our methodology generalizes to arbitrary time series data of real numbers, beyond movement data. © 2020 ACM.",Coordination; Data science; Leadership; Model selection; Time series,Fish; Time series; Coordinated movement; Coordination model; Coordination strategy; Individual levels; Movement coordination; Real-world datasets; Simulated datasets; State-of-the-art approach; Classification (of information)
A deep multi-task contextual attention framework for multi-modal affect analysis,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085258273&doi=10.1145%2f3380744&partnerID=40&md5=37a2efd63838af41d42d4e68a7efacda,"Multi-modal affect analysis (e.g., sentiment and emotion analysis) is an interdisciplinary study and has been an emerging and prominent field in Natural Language Processing and Computer Vision. The effective fusion of multiple modalities (e.g., text, acoustic, or visual frames) is a non-trivial task, as these modalities, often, carry distinct and diverse information, and do not contribute equally. The issue further escalates when these data contain noise. In this article, we study the concept of multi-task learning for multi-modal affect analysis and explore a contextual inter-modal attention framework that aims to leverage the association among the neighboring utterances and their multi-modal information. In general, sentiments and emotions have inter-dependence on each other (e.g., anger → negative or happy → positive). In our current work, we exploit the relatedness among the participating tasks in the multi-task framework. We define three different multi-task setups, each having two tasks, i.e., sentiment 8 emotion classification, sentiment classification 8 sentiment intensity prediction, and emotion classificati on 8 emotion intensity prediction. Our evaluation of the proposed system on the CMU-Multi-modal Opinion Sentiment and Emotion Intensity benchmark dataset suggests that, in comparison with the single-task learning framework, our multi-task framework yields better performance for the inter-related participating tasks. Further, comparative studies show that our proposed approach attains state-of-the-art performance for most of the cases. © 2020 ACM.",Emotion analysis; Emotion intensity prediction; Inter-modal attention; Multi-modal Analysis; Multi-task learning; Sentiment analysis; Sentiment intensity prediction,Benchmarking; Learning algorithms; Learning systems; Natural language processing systems; Emotion classification; Intensity prediction; Inter-disciplinary studies; Multi-modal information; NAtural language processing; Sentiment classification; Single task learning; State-of-the-art performance; Modal analysis
Efficient ridesharing framework for ride-matching via heterogeneous network embedding,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085254198&doi=10.1145%2f3373839&partnerID=40&md5=60bca65ce5af7bbfe66123a90c2363fb,"Ridesharing has attracted increasing attention in recent years, and combines the flexibility and speed of private cars with the reduced cost of fixed-line systems to benefit alleviating traffic pressure. A major issue in ridesharing is the accurate assignment of passengers to drivers, and how to maximize the number of rides shared between people being assigned to different drivers has become an increasingly popular research topic. There are two major challenges facing ride-matching: scalability and sparsity. Here, we show that network embedding drives the optimal matches between drivers and riders. Contrary to existing approaches that merely depend on the proximity between passengers and drivers, we employ a heterogeneous network to learn the latent semantics from different choices in two types of ridesharing, and extract features in terms of user trajectories and sentiment. A novel framework for ridesharing, RShareForm, which encodes not only the objects but also a variety of semantic relationships between them, is proposed. This article extends the existing skip-gram model to incorporate meta-paths over a proposed heterogeneous network. It allows diverse features to be used to search for similar participants and then ranks them to improve the quality of ride-matching. Extensive experiments on a large-scale dataset from DiDi in Chengdu, China show that by leveraging heterogeneous network embedding with meta paths, RShareForm can significantly improve the accuracy of identifying the participants for ridesharing over existing methods, including both meta-path guided similarity search methods and variants of embedding methods. © 2020 ACM.",Heterogeneous network; Meta-path; Network embedding; Ridesharing matching; Skip-gram,Embeddings; Large dataset; Semantics; Diverse features; Embedding method; Large-scale dataset; Latent semantics; Network embedding; Semantic relationships; Similarity search; Traffic pressure; Heterogeneous networks
Better classifier calibration for small datasets,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085262660&doi=10.1145%2f3385656&partnerID=40&md5=84aa401797aa3746806ba700d5b58537,"Classifier calibration does not always go hand in hand with the classifier's ability to separate the classes. There are applications where good classifier calibration, i.e., the ability to produce accurate probability estimates, is more important than class separation. When the amount of data for training is limited, the traditional approach to improve calibration starts to crumble. In this article, we show how generating more data for calibration is able to improve calibration algorithm performance in many cases where a classifier is not naturally producing well-calibrated outputs and the traditional approach fails. The proposed approach adds computational cost but considering that the main use case is with small datasets this extra computational cost stays insignificant and is comparable to other methods in prediction time. From the tested classifiers, the largest improvement was detected with the random forest and naive Bayes classifiers. Therefore, the proposed approach can be recommended at least for those classifiers when the amount of data available for training is limited and good calibration is essential. © 2020 ACM.",Calibration; Overfitting; Small datasets,Calibration; Decision trees; Calibration algorithm; Class separation; Computational costs; Naive Bayes classifiers; Prediction time; Probability estimate; Small data set; Traditional approaches; Classification (of information)
Story forest: Extracting events and telling stories from breaking news,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085248099&doi=10.1145%2f3377939&partnerID=40&md5=04ca74fc7d8fa69bc057c4d88fb8afa9,"Extracting events accurately from vast news corpora and organize events logically is critical for news apps and search engines, which aim to organize news information collected from the Internet and present it to users in the most sensible forms. Intuitively speaking, an event is a group of news documents that report the same news incident possibly in different ways. In this article, we describe our experience of implementing a news content organization system at Tencent to discover events from vast streams of breaking news and to evolve news story structures in an online fashion. Our real-world system faces unique challenges in contrast to previous studies on topic detection and tracking (TDT) and event timeline or graph generation, in that we (1) need to accurately and quickly extract distinguishable events from massive streams of long text documents, and (2) must develop the structures of event stories in an online manner, in order to guarantee a consistent user viewing experience. In solving these challenges, we propose Story Forest, a set of online schemes that automatically clusters streaming documents into events, while connecting related events in growing trees to tell evolving stories. A core novelty of our Story Forest system is EventX, a semi-supervised scheme to extract events from massive Internet news corpora. EventX relies on a two-layered, graph-based clustering procedure to group documents into fine-grained events. We conducted extensive evaluations based on (1) 60 GB of real-world Chinese news data, (2) a large Chinese Internet news dataset that contains 11,748 news articles with truth event labels, and (3) the 20 News Groups English dataset, through detailed pilot user experience studies. The results demonstrate the superior capabilities of Story Forest to accurately identify events and organize news text into a logical structure that is appealing to human readers. © 2020 ACM.",Community detection; Document clustering; EventX; News articles organization; Story forest,Forestry; Graph structures; Graphic methods; Large dataset; User experience; Chinese internets; Graph generation; Graph-based clustering; Logical structure; News information; Organization system; Real-world system; Topic detection and tracking; Search engines
Continuous influence maximization,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085262769&doi=10.1145%2f3380928&partnerID=40&md5=f867ede4c8d6bbe760812bb04db29c30,"Imagine we are introducing a new product through a social network, where we know for each user in the network the function of purchase probability with respect to discount. Then, what discounts should we offer to those social network users so that, under a predefined budget, the adoption of the product is maximized in expectation? Although influence maximization has been extensively explored, this appealing practical problem still cannot be answered by the existing influence maximization methods. In this article, we tackle the problem systematically. We formulate the general continuous influence maximization problem, investigate the essential properties, and develop a general coordinate descent algorithmic framework as well as the engineering techniques for practical implementation. Our investigation does not assume any specific influence model and thus is general and principled. At the same time, using the most popularly adopted triggering model as a concrete example, we demonstrate that more efficient methods are feasible under specific influence models. Our extensive empirical study on four benchmark real-world networks with synthesized purchase probability curves clearly illustrates that continuous influence maximization can improve influence spread significantly with very moderate extra running time comparing to the classical influence maximization methods. © 2020 ACM.",Budget allocation; Influence maximization; Viral marketing,Computer science; Data mining; Algorithmic framework; Empirical studies; Engineering techniques; General coordinates; Influence maximizations; Practical problems; Probability curves; Real-world networks; Budget control
Mining career paths from large resume databases,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085262978&doi=10.1145%2f3379984&partnerID=40&md5=652351c467e5be6bd445029adbf26d67,"The emergence of online professional platforms, such as LinkedIn and Indeed, has led to unprecedented volumes of rich resume data that have revolutionized the study of careers. One of the most prevalent problems in this space is the extraction of prototype career paths from a workforce. Previous research has consistently relied on a two-step approach to tackle this problem. The first step computes the pairwise distances between all the career sequences in the database. The second step uses the distance matrix to create clusters, with each cluster representing a different prototype path. As we demonstrate in this work, this approach faces two significant challenges when applied on large resume databases. First, the overwhelming diversity of job titles in the modern workforce prevents the accurate evaluation of distance between career sequences. Second, the clustering step of the standard approach leads to highly heterogeneous clusters, due to its inability to handle categorical sequences and sensitivity to outliers. This leads to non-representative centroids and spurious prototype paths that do not accurately represent the actual groups in the workforce. Our work addresses these two challenges and has practical implications for the numerous researchers and practitioners working on the analysis of career data across domains. © 2020 ACM.",Career data; Career paths; Clustering; Data mining,Employment; Personnel; Career paths; Categorical sequence; Distance matrices; Heterogeneous clusters; LinkedIn; Pairwise distances; Two-step approach; Database systems
Enhanced data mining technique to measure satisfaction degree of social media users of xeljanz drug,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085256048&doi=10.1145%2f3389433&partnerID=40&md5=372020f08c81ffbdda80d2c1d02a202b,"In the recent times, social media has become important in the field of health care as a major resource of valuable health information. Social media can provide massive amounts of data in real-time through user interaction, and this data can be analysed to reflect the harms and benefits of treatment by using the personal health experiences of users to improve health outcomes. In this study, we propose an enhanced data mining framework for analysing user opinions on Twitter and on a health-care forum. The proposed framework measures the degree of satisfaction of consumers regarding the drug Xeljanz, which is used to treat rheumatoid arthritis. The proposed framework is based on seven steps distributed in two phases. The first phase involves aggregating data related to the drug Xeljanz. This data is pre-processed to produce a list of words with a term frequency-inverse document frequency score. The word list is then classified into the following three categories: positive, negative and neutral. The second phase involves modelling social media posts using network analysis, identifying sub-graphs, calculating average opinions and detecting influential users. The results showed 77.3% user satisfaction with Xeljanz. Positive opinions were especially pronounced among users who switched to Xeljanz based on advice from a physician. Negative opinions of Xeljanz typically pertained to the high cost of the drug. © 2020 ACM.",Health care; Rheumatoid arthritis; Social media; User opinion; Xeljanz,Diseases; Health care; Social networking (online); Text processing; User experience; Data mining frameworks; Degree of satisfaction; Health informations; Influential users; Rheumatoid arthritis; Satisfaction degrees; Term frequency-inverse document frequencies; User satisfaction; Data mining
Robust drift characterization from event streams of business processes,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085250852&doi=10.1145%2f3375398&partnerID=40&md5=5b9a722617213631e079c41c9498cf49,"Process workers may vary the normal execution of a business process to adjust to changes in their operational environment, e.g., changes in workload, season, or regulations. Changes may be simple, such as skipping an individual activity, or complex, such as replacing an entire procedure with another. Over time, these changes may negatively affect process performance; hence, it is important to identify and understand them early on. As such, a number of techniques have been developed to detect process drifts, i.e., statistically significant changes in process behavior, from process event logs (offline) or event streams (online). However, detecting a drift without characterizing it, i.e., without providing explanations on its nature, is not enough to help analysts understand and rectify root causes for process performance issues. Existing approaches for drift characterization are limited to simple changes that affect individual activities. This article contributes an efficient, accurate, and noise-tolerant automated method for characterizing complex drifts affecting entire process fragments. The method, which works both offline and online, relies on two cornerstone techniques, one to automatically discover process trees from event streams (logs) and the other to transform process trees using a minimum number of change operations. The operations identified are then translated into natural language statements to explain the change behind a drift. The method has been extensively evaluated on artificial and real-life datasets, and against a state-of-the-art baseline method. The results from one of the real-life datasets have also been validated with a process stakeholder. © 2020 ACM.",Business process; Business process management; Concept drift; Drift characterization; Process drift; Process mining,Forestry; Automated methods; Change operations; Drift characterization; Natural languages; Operational environments; Process Fragments; Process performance; Real life datasets; Voltage measurement
Linearization of dependency and sampling for participation-based betweenness centrality in very large b-hypergraphs,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085257888&doi=10.1145%2f3375399&partnerID=40&md5=28068a62ced179610b4daad08f462c15,"A B-hypergraph consisting of nodes and directed hyperedges is a generalization of the directed graph. A directed hyperedge in the B-hypergraph represents a relation from a set of source nodes to a single destination node. We suggest one possible definition of betweenness centrality (BC) in B-hypergraphs, called Participation-based BC (PBC). A PBC score of a node is computed based on the number of the shortest paths in which the node participates. This score can be expressed in terms of dependency on the set of its outgoing hyperedges. In this article, we focus on developing efficient computation algorithms for PBC. We first present an algorithm called ePBC for computing exact PBC scores of nodes, which has a cubic-time complexity. This algorithm, however, can be used for only small-sized B-hypergraphs because of its cubic-time complexity, so we propose linearized PBC (ℓPBC) that is an approximation method of ePBC. ℓPBC that comes with a guaranteed upper bound on its error, uses a linearization of dependency on a set of hyperedges. ℓPBC improves the computing time of ePBC by an order of magnitude (i.e., it requires a quadratic time) while maintaining a high accuracy. ℓPBC works well on small to medium-sized B-hypergraphs, but is not scalable enough for a very large B-hypergraph with more than a million hyperedges. To cope with such a very large B-hypergraph, we propose a very fast heuristic sampling-based method called sampling-based ℓPBC (sℓPBC). We show through extensive experiments that ℓPBC and sℓPBC can efficiently estimate PBC scores in various real-world B-hypergraphs with a reasonably good precision@k. The experimental results show that sℓPBC works efficiently even for a very large B-hypergraph. © 2020 ACM.",B-hypergraph; Betweenness centrality; Directed hypergraph; Inclusion-exclusion principle; Rademacher complexity,Approximation algorithms; Computational complexity; Directed graphs; Heuristic methods; Linearization; Approximation methods; Betweenness centrality; Computing time; Efficient computation; Heuristic sampling; Sampling-based; Single destination node; Time complexity; Computational efficiency
MP2SDA: Multi-party parallelized sparse discriminant learning,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085256987&doi=10.1145%2f3374919&partnerID=40&md5=5e2027e0a1f22a412c610b86f9bd1b9e,"Sparse Discriminant Analysis (SDA) has been widely used to improve the performance of classical Fisher's Linear Discriminant Analysis in supervised metric learning, feature selection, and classification. With the increasing needs of distributed data collection, storage, and processing, enabling the Sparse Discriminant Learning to embrace the multi-party distributed computing environments becomes an emerging research topic. This article proposes a novel multi-party SDA algorithm, which can learn SDA models effectively without sharing any raw data and basic statistics among machines. The proposed algorithm (1) leverages the direct estimation of SDA to derive a distributed loss function for the discriminant learning, (2) parameterizes the distributed loss function with local/global estimates through bootstrapping, and (3) approximates a global estimation of linear discriminant projection vector by optimizing the ""distributed bootstrapping loss function"" with gossip-based stochastic gradient descent. Experimental results on both synthetic and real-world benchmark datasets show that our algorithm can compete with the aggregated SDA with similar performance, and significantly outperforms the most recent distributed SDA in terms of accuracy and F1-score. © 2020 ACM.",Distributed; Multi-party; Parallelized; Sparse discriminant analysis,Benchmarking; Digital storage; Discriminant analysis; Gradient methods; Stochastic systems; Benchmark datasets; Distributed computing environment; Distributed loss; Fisher's linear discriminant analysis; Global estimation; Linear discriminants; Projection vectors; Stochastic gradient descent; Data Sharing
Network embedding for community detection in attributed networks,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085251333&doi=10.1145%2f3385415&partnerID=40&md5=6ee1ecbd4d2bc27d8fca19cd04cc3a4d,"Community detection aims to partition network nodes into a set of clusters, such that nodes are more densely connected to each other within the same cluster than other clusters. For attributed networks, apart from the denseness requirement of topology structure, the attributes of nodes in the same community should also be homogeneous. Network embedding has been proved extremely useful in a variety of tasks, such as node classification, link prediction, and graph visualization, but few works dedicated to unsupervised embedding of node features specified for clustering task, which is vital for community detection and graph clustering. By post-processing with clustering algorithms like k-means, most existing network embedding methods can be applied to clustering tasks. However, the learned embeddings are not designed for clustering task, they only learn topological and attributed information of networks, and no clustering-oriented information is explored. In this article, we propose an algorithm named Network Embedding for node Clustering (NEC) to learn network embedding for node clustering in attributed graphs. Specifically, the presented work introduces a framework that simultaneously learns graph structure-based representations and clustering-oriented representations together. The framework consists of the following three modules: graph convolutional autoencoder module, soft modularity maximization module, and self-clustering module. Graph convolutional autoencoder module learns node embeddings based on topological structure and node attributes. We introduce soft modularity, which can be easily optimized using gradient descent algorithms, to exploit the community structure of networks. By integrating clustering loss and embedding loss, NEC can jointly optimize node cluster labels assignment and learn representations that keep local structure of network. This model can be effectively optimized using stochastic gradient algorithm. Empirical experiments on real-world networks and synthetic networks validate the feasibility and effectiveness of our algorithm on community detection task compared with network embedding based methods and traditional community detection methods. © 2020 ACM.",Attributed network; Community detection; Network embedding; Representation learning,Convolution; Embeddings; Gradient methods; Graph algorithms; Graph structures; Graph theory; Graphic methods; Learning systems; Optimization; Population dynamics; Stochastic models; Stochastic systems; Community detection; Community structures; Empirical experiments; Gradient descent algorithms; Graph visualization; Real-world networks; Stochastic gradient algorithms; Topological structure; K-means clustering
Data sharing via differentially private coupled matrix factorization,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085260963&doi=10.1145%2f3372408&partnerID=40&md5=1b0d57ba0374f1c5a88e9cd42708aa99,"We address the privacy-preserving data-sharing problem in a distributed multiparty setting. In this setting, each data site owns a distinct part of a dataset and the aim is to estimate the parameters of a statistical model conditioned on the complete data without any site revealing any information about the individuals in their own parts. The sites want to maximize the utility of the collective data analysis while providing privacy guarantees for their own portion of the data as well as for each participating individual. Our first contribution is to classify these different privacy requirements as (i) site-level and (ii) user-level differential privacy and present formal privacy guarantees for these two cases under the model of differential privacy. To satisfy a stronger form of differential privacy, we use a variant of differential privacy which is local differential privacy where the sensitive data is perturbed with a randomized response mechanism prior to the estimation. In this study, we assume that the data instances that are partitioned between several parties are arranged as matrices. A natural statistical model for this distributed scenario is coupled matrix factorization. We present two generic frameworks for privatizing Bayesian inference for coupled matrix factorization models that are able to guarantee proposed differential privacy notions based on the privacy requirements of the model. To privatize Bayesian inference, we first exploit the connection between differential privacy and sampling from a Bayesian posterior via stochastic gradient Langevin dynamics and then derive an efficient coupled matrix factorization method. In the local privacy context, we propose two models that have an additional privatization mechanism to achieve a stronger measure of privacy and introduce a Gibbs sampling based algorithm. We demonstrate that the proposed methods are able to provide good prediction accuracy on synthetic and real datasets while adhering to the introduced privacy constraints. © 2020 ACM.",Collective matrix factorization; Differential privacy; Distributed data; Local differential privacy; Markov Chain Monte Carlo (MCMC); Stochastic gradient Langevin dynamics (SGLD),Bayesian networks; Data privacy; Data Sharing; Factorization; Inference engines; Privatization; Stochastic systems; Bayesian inference; Differential privacies; Prediction accuracy; Privacy constraints; Privacy requirements; Randomized response; Statistical modeling; Stochastic gradient; Matrix algebra
Probabilistic topic modeling for comparative analysis of document collections,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081627935&doi=10.1145%2f3369873&partnerID=40&md5=599234eaec3612d000ce02d226bacee0,"Probabilistic topic models, which can discover hidden patterns in documents, have been extensively studied. However, rather than learning from a single document collection, numerous real-world applications demand a comprehensive understanding of the relationships among various document sets. To address such needs, this article proposes a new model that can identify the common and discriminative aspects of multiple datasets. Specifically, our proposed method is a Bayesian approach that represents each document as a combination of common topics (shared across all document sets) and distinctive topics (distributions over words that are exclusive to a particular dataset). Through extensive experiments, we demonstrate the effectiveness of our method compared with state-of-the-artmodels. The proposedmodel can be useful for ""comparative thinking"" analysis in real-world document collections. © 2020 Association for Computing Machinery.",Probabilistic topic modeling; text mining,Bayesian networks; Statistics; Bayesian approaches; Comparative analysis; Document collection; Document sets; Hidden patterns; Multiple data sets; Probabilistic topic models; Real-world; Text mining
CenEEGs: Valid EEG selection for classification,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081637184&doi=10.1145%2f3371153&partnerID=40&md5=afbcaf8fbde1a1b78100ab337687c5b9,"This article explores valid brain electroencephalography (EEG) selection for EEG classification with different classifiers, which has been rarely addressed in previous studies and is mostly ignored by existing EEG processing methods and applications. Importantly, traditional selection methods are not able to select valid EEG signals for different classifiers. This article focuses on a source control-based valid EEG selection to reduce the impact of invalid EEG signals and aims to improve EEG-based classification performance for different classifiers. We propose a novel centroid-based EEG selection approach named CenEEGs, which uses a scale-and-shift-invariance similarity metric to measure similarities of EEG signals and then applies a globally optimal centroid strategy to select valid EEG signals with respect to a similarity threshold. A detailed comparison with several state-of-the-art time series selection methods by using standard criteria on 8 EEG datasets demonstrates the efficacy and superiority of CenEEGs for different classifiers. © 2020 Association for Computing Machinery.",centroid searching; classification; EEG similarity; Electroencephalography (EEG) selection,Classification (of information); Electroencephalography; Electrophysiology; centroid searching; Classification performance; EEG classification; Processing method; Selection methods; Shift invariance; Similarity metrics; Similarity threshold; Biomedical signal processing
New algorithms of feature selection and big data assignment for CBR system integrated by Bayesian network,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081585763&doi=10.1145%2f3373086&partnerID=40&md5=1f47557572c43954a0f9d54f46b43dab,"Under big data, the integrated system of case-based reasoning and Bayesian network has exhibited great advantage in implementing the intelligence of engineering application in many domains. To further improve the performance of the hybrid system, this article proposes Probability Change Measurement of Solution Parameters (PCMSP)-Half-Division-Cross (HDC) method, which includes two algorithms, namely PCMSP andHDC algorithm. PCMSP algorithm can select principal problem features according to their effects upon all solution features measured by calculating theweighted relative probability (RP) change of all solution features caused by each problem feature. PCMSP algorithm can perfectly work under big data no matter how complex the data types are and how huge the data size is. HDC algorithm is used to assign the computation task of big data to enhance the efficiency of the integrated system. HDC algorithm assigns big data by grouping all the problem parameters into many small sub-groups and then distributing the data which covers the same sub-group of problem parameters to a slave node. HDC algorithm can guarantee enough efficiency of the integrated system under big data no matter how large the number of problem parameters is. Finally, lots of experiments are executed to validate the proposed method. © 2020 Association for Computing Machinery.",big data; CBR; Feature selection; integrated system,Bayesian networks; Big data; Case based reasoning; Computational efficiency; Efficiency; Feature extraction; Hybrid systems; Integrated control; All solutions; Computation tasks; Data assignments; Engineering applications; Integrated systems; Problem parameters; Relative probability; Solution parameters; Parameter estimation
Ar2Net: An attentive neural approach for business location selection with satellite data and urban data,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079824499&doi=10.1145%2f3372406&partnerID=40&md5=42a2f39291aff75705652ea2f4254d93,"Business location selection is crucial to the success of businesses. Traditional approaches like manual survey investigate multiple factors, such as foot traffic, neighborhood structure, and available workforce, which are typically hard to measure. In this article, we propose to explore both satellite data (e.g., satellite images and nighttime light data) and urban data for business location selection tasks of various businesses. We extract discriminative features from the two kinds of data and perform empirical analysis to evaluate the correlation between extracted features and the business popularity of locations. A novel neural network approach named R2Net is proposed to learn deep interactions among features and predict the business popularity of locations. The proposed approach is trained with a regression-and-ranking combined loss function to preserve accurate popularity estimation and the ranking order of locations simultaneously. To support the location selection for multiple businesses, we propose an approach named AR2Net with three attention modules, which enable the approach to focus on different latent features according to business types. Comprehensive experiments on a real-world dataset demonstrate that the satellite features are effective and our models outperform the state-of-the-art methods in terms of four metrics. © 2020 Association for Computing Machinery.",Business location selection; Nighttime light; Satellite data; Satellite images,Neural networks; Satellites; Discriminative features; Location selection; Neighborhood structure; Night-time lights; Satellite data; Satellite images; State-of-the-art methods; Traditional approaches; Location
Multiple set matching with bloom matrix and bloom vector,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079795962&doi=10.1145%2f3372409&partnerID=40&md5=a4e51a5d93b66823c333f0cbab90b1af,"Bloom Filter is a space-efficient probabilistic data structure for checking the membership of elements in a set. Given multiple sets, a standard Bloom Filter is not sufficient when looking for the items to which an element or a set of input elements belong. An example case is searching for documents with keywords in a large text corpus, which is essentially a multiple set matching problem where the input is single or multiple keywords, and the result is a set of possible candidate documents. This article solves the multiple set matching problem by proposing two efficient Bloom Multifilters called Bloom Matrix and Bloom Vector, which generalize the standard Bloom Filter. Both structures are space-efficient and answer queries with a set of identifiers for multiple set matching problems. The space efficiency can be optimized according to the distribution of labels among multiple sets: Uniform and Zipf. Bloom Vector efficiently exploits the Zipf distribution of data for further space reduction. Indeed, both structures are much more space-efficient compared with the state-ofthe-art, Bloofi. The results also highlight that a Lookup operation on Bloom Matrix is significantly faster than on Bloom Vector and Bloofi. © 2020 Association for Computing Machinery.",Big data; Bloom filter; Bloomier filter; Multiple sets; Uniform distribution; Zipf distribution,Big data; Data structures; Matrix algebra; Query processing; Vectors; Bloom filters; Bloomier filter; Multiple set; Uniform distribution; Zipf distribution; Vector spaces
"Fast, accurate and provable triangle counting in fully dynamic graph streams",2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079803687&doi=10.1145%2f3375392&partnerID=40&md5=e803490f301a910f0ef07bdc6e6efd22,"Given a stream of edge additions and deletions, how can we estimate the count of triangles in it? If we can store only a subset of the edges, how can we obtain unbiased estimates with small variances? Counting triangles (i.e., cliques of size three) in a graph is a classical problem with applications in a wide range of research areas, including social network analysis, data mining, and databases. Recently, streaming algorithms for triangle counting have been extensively studied since they can naturally be used for large dynamic graphs. However, existing algorithms cannot handle edge deletions or suffer from low accuracy. Can we handle edge deletions while achieving high accuracy? We propose ThinkD, which accurately estimates the counts of global triangles (i.e., all triangles) and local triangles associated with each node in a fully dynamic graph stream with additions and deletions of edges. Compared to its best competitors, ThinkD is (a) Accurate: up to 4.3× more accurate within the same memory budget, (b) Fast: up to 2.2× faster for the same accuracy requirements, and (c) Theoretically sound: always maintaining estimates with zero bias (i.e., the difference between the true triangle count and the expected value of its estimate) and small variance. As an application, we use ThinkD to detect suddenly emerging dense subgraphs, and we show its advantages over state-of-the-art methods. © 2020 Association for Computing Machinery.",Edge deletions; Local triangles; Triangle counting,Budget control; Data mining; Classical problems; Dense sub-graphs; Edge deletions; Local triangles; State-of-the-art methods; Streaming algorithm; Triangle counting; Unbiased estimates; Graph algorithms
Community detection by motif-aware label propagation,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079743311&doi=10.1145%2f3378537&partnerID=40&md5=f58c44e9ac2d6422f7fbe0c73452dc85,"Community detection (or graph clustering) is crucial for unraveling the structural properties of complex networks. As an important technique in community detection, label propagation has shown the advantage of finding a good community structure with nearly linear time complexity. However, despite the progress that has been made, there are still several important issues that have not been properly addressed. First, the label propagation typically proceeds over the lower order structure of the network and only the direct one-hop connections between nodes are taken into consideration. Unfortunately, the higher order structure that may encode design principle of the network and be crucial for community detection is neglected under this regime. Second, the stability of the identified community structure may also be seriously affected by the inherent randomness in the label propagation process. To tackle the above issues, this article proposes a Motif-Aware Weighted Label Propagation method for community detection. We focus on triangles within the network, but our technique extends to other kinds of motifs as well. Specifically, the motif-based higher order structure mining is conducted to capture structural characteristics of the network. First, the motif of interest (locally meaningful pattern) is identified, and then, the motif-based hypergraph can be constructed to encode the higher order connections. To further utilize the structural information of the network, a re-weighted network is designed, which unifies both the higher order structure and the original lower order structure. Accordingly, a novel voting strategy termed NaS (considering both Number and Strength of connections) is proposed to update node labels during the label propagation process. In this way, the random label selection can be effectively eliminated, yielding more stable community structures. Experimental results on multiple real-world datasets have shown the superiority of the proposed method. © 2020 Association for Computing Machinery.",Community detection; Higher order structure; Label propagation; Motifs,Encoding (symbols); Population dynamics; Sodium compounds; Sulfur compounds; Community detection; Higher-order structure; Label propagation; Linear time complexity; Motifs; Structural characteristics; Structural information; Structural properties of complex networks; Complex networks
Budget-constrained real-time bidding optimization: Multiple predictors make it better,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079736961&doi=10.1145%2f3375393&partnerID=40&md5=d7fdf8d2511a31de956bfebb07af46ce,"In this article, we pursue a better solution for the promising problem, i.e., the bidding strategy design, in the real-time bidding (RTB) advertising (AD) environment. Under the budget constraint, the design of an optimal strategy for bidding on each incoming impression opportunity targets at acquiring as many clicks as possible during an AD campaign. State-of-the-art bidding algorithms rely on a single predictor, the clickthrough rate predictor, to calculate the bidding value for each impression. This provides reasonable performance if the predictor has appropriate accuracy in predicting the probability of user clicking. However, the classical methods usually fail to capture optimal results since the predictor accuracy is limited. We improve the situation by accomplishing an additional winning price predictor in the bidding process. In this article, an algorithm combining powers of multiple prediction models is developed. It emerges from an analogy to the online stochastic knapsack problem, and the efficiency of the algorithm is also theoretically analyzed. Experiments conducted on real world RTB datasets show that the proposed solution performs better with regard to both number of clicks achieved and effective cost per click in many different settings of budget constraints. © 2020 Association for Computing Machinery.",Bidding strategy design; Bidding with click; Demand-side platform; Real-time bidding; Winning price predictors,Combinatorial optimization; Constrained optimization; Stochastic systems; Bidding strategy; Bidding with click; Demand-side; Real time; Winning price predictors; Budget control
Bradykinesia recognition in Parkinson's disease via single RGB video,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079808823&doi=10.1145%2f3369438&partnerID=40&md5=cbe90e59c4ff58f462bfeb71adf34ddd,"Parkinson's disease is a progressive nervous system disorder afflicting millions of patients. Among its motor symptoms, bradykinesia is one of the cardinal manifestations. Experienced doctors are required for the clinical diagnosis of bradykinesia, but sometimes they also miss subtle changes, especially in early stages of such disease. Therefore, developing auxiliary diagnostic methods that can automatically detect bradykinesia has received more and more attention. In this article, we employ a two-stage framework for bradykinesia recognition based on the video of patient movement. First, convolution neural networks are trained to localize keypoints in each video frame. These time-varying coordinates form motion trajectories that represent the whole movement. From the trajectory, we then propose novel measurements, namely stability, completeness, and self-similarity, to quantify different motor behaviors. We also propose a periodic motion model called PMNet. An encoder-decoder structure is applied to learn a low dimensional representation of a motion process. The compressed motion process and quantified motor behaviors are combined as inputs to a fully-connected neural network. Different from the traditional means, our solution extends the application scenario outside the hospital and can be easily transplanted to conduct similar tasks. A commonly used clinical assessment is served as a case study. Experimental results based on real-world data validate the effectiveness of our approach for bradykinesia recognition. © 2020 Association for Computing Machinery.",Bradykinesia; Computer vision; Parkinson's disease; RGB video; Time sequence analysis,Computer vision; Neurodegenerative diseases; Bradykinesia; Clinical assessments; Convolution neural network; Fully connected neural network; Low-dimensional representation; Parkinson's disease; RGB video; Time sequence analysis; Diagnosis
Real-time transportation prediction correction using reconstruction error in deep learning,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079830512&doi=10.1145%2f3369871&partnerID=40&md5=7fc5073fd72de3ac4f04f715b3842800,"In online complex systems such as transportation system, an important work is real-time traffic prediction. Due to the data shift, data model inconsistency, and sudden change of traffic patterns (like transportation accident), the prediction result derived from an offline-built model would be unreliable. Retraining the model is usually not time affordable for online prediction, especially when the prediction model is very complex and costs a lot of training time (for example, deep neural networks). A real-time prediction correction strategy would be of great value under this situation. Traditionally, the prediction correction usually relies on the prediction error in several previous time intervals. They assume that the error pattern is similar in the current time interval, so that it is time-delayed to some extent. In this article, we propose the prediction correction strategy using the reconstruction error in the deep neural network. The reconstruction error can reflect the model's ability on feature representation and then determine the fitness of an input data to the model. We first build the relationship between reconstruction error and prediction error. From the perspective of the prediction interval, we demonstrate that the reconstruction error is in positive relation with the prediction interval. Thus the prediction result is more reliable when the reconstruction error is smaller. Then we propose two mechanisms of real-time prediction correction using the reconstruction error. The data driven prediction correction approach selects several training instances with similar reconstruction errors to the current instance and using their average prediction error in correcting the prediction result. The model-driven approach builds several component deep neural networks in training. The component training set for each network is selected according to the reconstruction error of training instances. For a predicting instance, it first computes the reconstruction error of the sample in each component network and then averages the results by the reconstruction error and prediction interval. The model-driven approach is actually a reconstruction error-based deep neural network ensemble approach. Finally, a series of experiments demonstrated that reconstruction error based prediction correction approaches are effective in several prediction problems in transportation including traffic flow prediction on road, traffic flow prediction in entrance and exit station and travel time prediction. Besides the high overall accuracy, our approach can also provide many observations of using the reconstruction error in transportation prediction. © 2020 Association for Computing Machinery.",Deep learning; Prediction correction; Reconstruction error; Traffic flow prediction; Travel time prediction,Complex networks; Deep learning; Deep neural networks; Errors; Forecasting; Online systems; Real time systems; Time varying control systems; Travel time; Average prediction error; Feature representation; Neural network ensembles; Prediction correction; Real-time transportation; Reconstruction error; Traffic flow prediction; Travel time prediction; Street traffic control
Ranking from crowdsourced pairwise comparisons via smoothed Riemannian optimization,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079753166&doi=10.1145%2f3372407&partnerID=40&md5=84e87680889f9611bd57552f789fe87a,"Social Internet of Things has recently become a promising paradigm for augmenting the capability of humans and devices connected in the networks to provide services. In social Internet of Things network, crowdsourcing that collects the intelligence of the human crowd has served as a powerful tool for data acquisition and distributed computing. To support critical applications (e.g., a recommendation system and assessing the inequality of urban perception), in this article, we shall focus on the collaborative ranking problems for user preference prediction from crowdsourced pairwise comparisons. Based on the Bradley-Terry-Luce (BTL) model, a maximum likelihood estimation (MLE) is proposed via low-rank approach in order to estimate the underlying weight/score matrix, thereby predicting the ranking list for each user. A novel regularized formulation with the smoothed surrogate of elementwise infinity norm is proposed in order to address the unique challenge of the coupled the non-smooth elementwise infinity norm constraint and non-convex low-rank constraint in the MLE problem. We solve the resulting smoothed rank-constrained optimization problem via developing the Riemannian trust-region algorithm on quotient manifolds of fixed-rank matrices, which enjoys the superlinear convergence rate. The admirable performance and algorithmic advantages of the proposed method over the state-of-the-art algorithms are demonstrated via numerical results. Moreover, the proposed method outperforms state-of-the-art algorithms on large collaborative filtering datasets in both success rate of inferring preference and normalized discounted cumulative gain. © 2020 Association for Computing Machinery.",Crowdsourced data; Low-rank optimization; Pairwise comparison; Ranking; Smoothed matrix manifold optimization; Social Internet of Things,Collaborative filtering; Crowdsourcing; Data acquisition; Internet of things; Large dataset; Matrix algebra; Maximum likelihood estimation; Numerical methods; Social computing; Constrained optimi-zation problems; Crowdsourced data; Matrix manifolds; Pair-wise comparison; Ranking; Riemannian optimizations; State-of-the-art algorithms; Superlinear convergence rate; Constrained optimization
Multi-task information bottleneck co-clustering for unsupervised cross-view human action categorization,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079785213&doi=10.1145%2f3375394&partnerID=40&md5=1e8451f0532159743877f9de9b02a1f8,"The widespread adoption of low-cost cameras generates massive amounts of videos recorded from different viewpoints every day. To cope with this vast amount of unlabeled and heterogeneous data, a new multi-task information bottleneck co-clustering (MIBC) approach is proposed to automatically categorize human actions in collections of unlabeled cross-view videos. Our motivation is that, if a learning action category from each view is seen as a single task, it is reasonable to assume that the tasks of learning action patterns from the videos recorded by multiple cameras are dependent and inter-related, since the actions of the same subjects synchronously recorded from different camera viewpoints are complementary to each other. MIBC aims to transfer the shared view knowledge across multiple tasks (i.e., camera viewpoints) to boost the performance of each task. Specifically, MIBC involves the following two parts: (1) extracting action categories for each task by independently maintaining its own relevant information, and (2) allowing the feature representations of all tasks to be compressed into a common feature space, which is utilized to capture the relatedness of multiple tasks and transfer the shared knowledge across different camera viewpoints. These two parts of MIBC work simultaneously and can be solved in a novel co-clustering mechanism. Our experimental evaluation on several cross-view action collections shows that the MIBC algorithm outperforms the existing state-of-the-art baselines. © 2020 Association for Computing Machinery.",Cross-view action categorization; Information bottleneck; Multi-task clustering; View knowledge transfer,Knowledge management; Cross-view action categorization; Experimental evaluation; Feature representation; Heterogeneous data; Information bottleneck; Knowledge transfer; Learning actions; Multiple cameras; Cameras
Generalizing long short-term memory network for deep learning from generic data,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079801443&doi=10.1145%2f3366022&partnerID=40&md5=afd601d07f4d4b6e0ce1e0943b046ace,"Long Short-Term Memory (LSTM) network, a popular deep-learning model, is particularly useful for data with temporal correlation, such as texts, sequences, or time series data, thanks to its well-sought after recurrent network structures designed to capture temporal correlation. In this article, we propose to generalize LSTM to generic machine-learning tasks where data used for training do not have explicit temporal or sequential correlation. Our theme is to explore feature correlation in the original data and convert each instance into a synthetic sentence format by using a two-gram probabilistic language model. More specifically, for each instance represented in the original feature space, our conversion first seeks to horizontally align original features into a sequentially correlated feature vector, resembling to the letter coherence within a word. In addition, a vertical alignment is also carried out to create multiple time points and simulate word sequential order in a sentence (i.e., word correlation). The two dimensional horizontal-and-vertical alignments not only ensure feature correlations are maximally utilized, but also preserve the original feature values in the new representation. As a result, LSTM model can be utilized to achieve good classification accuracy, even if the underlying data do not have temporal or sequential dependency. Experiments on 20 generic datasets show that applying LSTM to generic data can improve the classification accuracy, compared to conventional machine-learning methods. This research opens a new opportunity for LSTM deep learning to be broadly applied to generic machine-learning tasks. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Classification; Deep learning; Feature learning; Long short-term memory,Brain; Classification (of information); Data handling; Deep learning; Learning systems; Vector spaces; Classification accuracy; Conventional machines; Feature learning; Probabilistic language; Sequential correlations; Sequential dependencies; Sequential ordering; Temporal correlations; Long short-term memory
Inferring lifetime status of point-of-interest: A multitask multiclass approach,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079217766&doi=10.1145%2f3369799&partnerID=40&md5=96673c250cfe0a6b83784834bfcec296,"A Point-of-Interest (POI) refers to a specific location that people may find useful or interesting. In modern cities, a large number of POIs emerge, grow, stabilize for a period, then finally disappear. The stages (e.g., emerge and grow) in this process are called lifetime statuses of a POI. While a large body of research has been devoted to identifying and recommending POIs, there are few studies on inferring the lifetime status of POIs. Indeed, the predictive analytics of POI lifetime status can be valuable for various tasks, such as urban planning, business site selection, and real estate appraisal. In this article, we propose a multitask learning approach, named inferring POI lifetime status, to inferring the POI lifetime status with multifaceted data sources. Specifically, we first define three types of POI lifetime status, i.e., booming, decaying, and stable. Then,we formulate a serial classification problem to predict the sequential/successive lifetime statuses of POIs over time. Leveraging geographical data and human mobility data, we examine and integrate three aspects of features related to the prosperity of POIs, i.e., region popularity, region demands, and peer competitiveness. Next, as the booming/decaying POIs are relatively rare in our data, we perform stable class decomposition to alleviate the imbalance between stable POIs and booming/decaying POIs. Finally, we develop a POI lifetime status classifier by exploiting the multitask learning framework as well as the multiclass kernel-based vector machines. We perform extensive experiments using large-scale and real-world datasets of New York City. The experimental results validate the effectiveness of our approach to automatically inferring POI lifetime status. © 2020 Association for Computing Machinery.",Lifetime status; Multiclass classification; Multitask learning; Point-of-Interest; Urban computing,Large dataset; Multi-task learning; Predictive analytics; Site selection; Urban growth; Class decomposition; Geographical data; Lifetime status; Multi-class classification; Point of interest; Real estate appraisals; Real-world datasets; Urban computing; Learning systems
A unified multi-view clustering algorithm using multi-objective optimization coupled with generative model,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079226022&doi=10.1145%2f3365673&partnerID=40&md5=d153dea64847c8a6a85705000c957e4d,"There is a large body of works on multi-view clustering that exploit multiple representations (or views) of the same input data for better convergence. These multiple views can come from multiple modalities (image, audio, text) or different feature subsets. Obtaining one consensus partitioning after considering different views is usually a non-trivial task. Recently, multi-objective based multi-view clustering methods have suppressed the performance of single objective based multi-view clustering techniques. One key problem is that it is difficult to select a single solution from a set of alternative partitionings generated by multi-objective techniques on the final Pareto optimal front. In this article, we propose a novel multi-objective based multi-view clustering framework that overcomes the problem of selecting a single solution in multi-objective based techniques. In particular, our proposed framework has three major components as follows: (i) multi-view based multi-objective algorithm, Multiview-AMOSA, for initial clustering of data points; (ii) a generative model for generating a combined solution having probabilistic labels; and (iii) K-means algorithm for obtaining the final labels. As the first component, we have adopted a recently developed multi-view based multi-objective clustering algorithm to generate different possible consensus partitionings of a given dataset taking into account different views. A generative model is coupled with the first component to generate a single consensus partitioning after considering multiple solutions. It exploits the latent subsets of the non-dominated solutions obtained from the multi-objective clustering algorithm and combines them to produce a single probabilistic labeled solution. Finally, a simple clustering algorithm, namely K-means, is applied on the generated probabilistic labels to obtain the final cluster labels. Experimental validation of our proposed framework is carried out over several benchmark datasets belonging to three different domains; UCI datasets, multi-view datasets, search result clustering datasets, and patient stratification datasets. Experimental results show that our proposed framework achieves an improvement of around 2%-4% over different evaluation metrics in all the four domains in comparison to state-of-the art methods. © 2020 Association for Computing Machinery.",Generative model; Multi-objective clustering; Multi-view clustering; Search result clustering,Cluster analysis; Multiobjective optimization; Pareto principle; Experimental validations; Generative model; Multi objective algorithm; Multi-objective clustering; Multi-view clustering; Multiple representation; Search result clustering; State-of-the-art methods; K-means clustering
Adaptive local linear discriminant analysis,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079245700&doi=10.1145%2f3369870&partnerID=40&md5=94e107c432a015bece82d337bd7f8854,"Dimensionality reduction plays a significant role in high-dimensional data processing, and Linear Discriminant Analysis (LDA) is a widely used supervised dimensionality reduction approach. However, a major drawback of LDA is that it is incapable of extracting the local structure information, which is crucial for handling multimodal data. In this article, we propose a novel supervised dimensionality reduction method named Adaptive Local Linear Discriminant Analysis (ALLDA), which adaptively learns a k-nearest neighbors graph from data themselves to extract the local connectivity of data. Furthermore, the original high-dimensional data usually contains noisy and redundant features, which has a negative impact on the evaluation of neighborships and degrades the subsequent classification performance. To address this issue, our method learns the similarity matrix and updates the subspace simultaneously so that the neighborships can be evaluated in the optimal subspaces where the noises have been removed. Through the optimal graph embedding, the underlying sub-manifolds of data in intra-class can be extracted precisely. Meanwhile, an efficient iterative optimization algorithm is proposed to solve the minimization problem. Promising experimental results on synthetic and real-world datasets are provided to evaluate the effectiveness of proposed method. © 2020 Association for Computing Machinery.",Linear discriminant analysis; Local connectivity; Optimal graph embedding; Supervised dimensionality reduction,Clustering algorithms; Data mining; Data reduction; Discriminant analysis; Embeddings; Image recognition; Iterative methods; Nearest neighbor search; Classification performance; Dimensionality reduction method; High dimensional data; Iterative optimization algorithms; Linear discriminant analysis; Local connectivity; Minimization problems; Optimal graphs; Dimensionality reduction
"Core decomposition in multilayer networks: Theory, algorithms, and applications",2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079231249&doi=10.1145%2f3369872&partnerID=40&md5=56ec0f47094099dd8075d4687e134367,"Multilayer networks are a powerful paradigm to model complex systems, where multiple relations occur between the same entities. Despite the keen interest in a variety of tasks, algorithms, and analyses in this type of network, the problem of extracting dense subgraphs has remained largely unexplored so far. As a first step in this direction, in this work, we study the problem of core decomposition of a multilayer network. Unlike the single-layer counterpart in which cores are all nested into one another and can be computed in linear time, the multilayer context is much more challenging as no total order exists among multilayer cores; rather, they form a lattice whose size is exponential in the number of layers. In this setting, we devise three algorithms, which differ in the way they visit the core lattice and in their pruning techniques.We assess time and space efficiency of the three algorithms on a large variety of real-world multilayer networks. We then move a step forward and study the problem of extracting the inner-most (also known as maximal) cores, i.e., the cores that are not dominated by any other core in terms of their core index in all the layers. inner-most cores are typically orders of magnitude less than all the cores. Motivated by this, we devise an algorithm that effectively exploits the maximality property and extracts inner-most cores directly, without first computing a complete decomposition. This allows for a consistent speed up over a naive method that simply filters out non-inner-most ones from all the cores. Finally, we showcase the multilayer core-decomposition tool in a variety of scenarios and problems. We start by considering the problem of densest-subgraph extraction in multilayer networks. We introduce a definition of multilayer densest subgraph that tradesoff between high density and number of layers in which the high density holds, and exploit multilayer core decomposition to approximate this problem with quality guarantees. As further applications, we show how to utilize multilayer core decomposition to speed-up the extraction of frequent cross-graph quasi-cliques and to generalize the community-search problem to the multilayer setting. © 2020 Copyright held by the owner/author(s).",Cliques and quasi-cliques; Community search; Core decomposition; Dense-subgraph extraction; Graph mining; Multilayer networks,Computation theory; Extraction; Filtration; Cliques and quasi-cliques; Community search; Dense subgraph; Graph mining; Multi-layer network; Multilayers
CFOF: A concentration free measure for anomaly detection,2020,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079214068&doi=10.1145%2f3362158&partnerID=40&md5=20ca63282150ac7fda5e4865acdc3b55,"We present a novel notion of outlier, called the Concentration Free Outlier Factor, or CFOF. As a main contribution, we formalize the notion of concentration of outlier scores and theoretically prove that CFOF does not concentrate in the Euclidean space for any arbitrary large dimensionality. To the best of our knowledge, there are no other proposals of data analysis measures related to the Euclidean distance for which it has been provided theoretical evidence that they are immune to the concentration effect. We determine the closed form of the distribution of CFOF scores in arbitrarily large dimensionalities and show that the CFOF score of a point depends on its squared norm standard score and on the kurtosis of the data distribution, thus providing a clear and statistically founded characterization of this notion. Moreover, we leverage this closed form to provide evidence that the definition does not suffer of the hubness problem affecting other measures in high dimensions. We prove that the number of CFOF outliers coming from each cluster is proportional to cluster size and kurtosis, a property that we call semi-locality.We leverage theoretical findings to shed lights on properties of well-known outlier scores. Indeed, we determine that semi-locality characterizes existing reverse nearest neighbor-based outlier definitions, thus clarifying the exact nature of their observed local behavior. We also formally prove that classical distance-based and density-based outliers concentrate both for bounded and unbounded sample sizes and for fixed and variable values of the neighborhood parameter. We introduce the fast-CFOF algorithm for detecting outliers in large high-dimensional dataset. The algorithm has linear cost, supports multi-resolution analysis, and is embarrassingly parallel. Experiments highlight that the technique is able to efficiently process huge datasets and to deal even with large values of the neighborhood parameter, to avoid concentration, and to obtain excellent accuracy. © 2020 Association for Computing Machinery.",Concentration of distances; Curse of dimensionality; High-dimensional data; Hubness; Huge datasets; Kurtosis; Outlier detection,Clustering algorithms; Higher order statistics; Large dataset; Nearest neighbor search; Statistics; Curse of dimensionality; High dimensional data; Hubness; Huge datasets; Kurtosis; Anomaly detection
Recurrent meta-structure for robust similarity measure in heterogeneous information networks,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077803121&doi=10.1145%2f3364226&partnerID=40&md5=406f6647d4646291e3b8fda02e81e8d2,"Similarity measure is one of the fundamental task in heterogeneous information network (HIN) analysis. It has been applied to many areas, such as product recommendation, clustering, and Web search. Most of the existing metrics can provide personalized services for users by taking a meta-path or meta-structure as input. However, these metrics may highly depend on the user-specified meta-path or meta-structure. In addition, users must know how to select an appropriate meta-path or meta-structure. In this article, we propose a novel similarity measure in HINs, called Recurrent Meta-Structure (RecurMS)-based Similarity (RMSS). The RecurMS as a schematic structure in HINs provides a unified framework for integrating all of the meta-paths and meta-structures, and can be constructed automatically by means of repetitively traversing the network schema. In order to formalize the semantics, the RecurMS is decomposed into several recurrent meta-paths and recurrent meta-trees, and we then define the commuting matrices of the recurrent meta-paths and meta-trees. All of these commuting matrices are combined together according to different weights. We propose two kinds of weighting strategies to determine the weights. The first is called the local weighting strategy that depends on the sparsity of the commuting matrices, and the second is called the global weighting strategy that depends on the strength of the commuting matrices. As a result, RMSS is defined by means of the weighted summation of the commuting matrices. Note that RMSS can also provide personalized services for users by means of the weights of the recurrent meta-paths and meta-trees. Experimental evaluations show that the proposed RMSS is robust and outperforms the existing metrics in terms of ranking and clustering task. © 2019 Association for Computing Machinery.",Heterogeneous information network; Meta-path; Meta-structure; Similarity,Semantics; Technology transfer; Experimental evaluation; Heterogeneous information; Meta structures; Meta-path; Personalized service; Product recommendation; Similarity; Weighting strategies; Information services
Treatment effect estimation via differentiated confounder balancing and regression,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077000699&doi=10.1145%2f3365677&partnerID=40&md5=6fb918cd98f2005659a2d3272f7fe9da,"Treatment effect plays an important role on decision making in many fields, such as social marketing, healthcare, and public policy. The key challenge on estimating treatment effect in the wild observational studies is to handle confounding bias induced by imbalance of the confounder distributions between treated and control units. Traditional methods remove confounding bias by re-weighting units with supposedly accurate propensity score estimation under the unconfoundedness assumption. Controlling high-dimensional variables may make the unconfoundedness assumption more plausible, but poses new challenge on accurate propensity score estimation. One strand of recent literature seeks to directly optimize weights to balance confounder distributions, bypassing propensity score estimation. But existing balancing methods fail to do selection and differentiation among the pool of a large number of potential confounders, leading to possible underperformance inmany high-dimensional settings. In this article, we propose a data-driven Differentiated Confounder Balancing (DCB) algorithm to jointly select confounders, differentiate weights of confounders and balance confounder distributions for treatment effect estimation in the wild high-dimensional settings. Besides, under some settings with heavy confounding bias, in order to further reduce the bias and variance of estimated treatment effect, we propose a Regression Adjusted Differentiated Confounder Balancing (RADCB) algorithm based on our DCB algorithm by incorporating outcome regression adjustment. The synergistic learning algorithmswe proposed are more capable of reducing the confounding bias in many observational studies. To validate the effectiveness of our DCB and RA-DCB algorithms, we conduct extensive experiments on both synthetic and real-world datasets. The experimental results clearly demonstrate that our algorithms outperform the state-of-the-art methods. By incorporating regression adjustment, our RA-DCB algorithm achieves more precise estimation on treatment effect than DCB algorithm, especially under the settings with heavy confounding bias. Moreover, we show that the top features ranked by our algorithm generate accurate prediction of online advertising effect. © 2019 Association for Computing Machinery.",Confounding bias; Differentiated confounder balancing; Regression adjustment; Treatment effect estimation,Decision making; Marketing; Accurate prediction; Confounder; Confounding bias; Observational study; Real-world datasets; Regression adjustment; State-of-the-art methods; Treatment effects; Regression analysis
Fast parallel algorithms for counting and listing triangles in big graphs,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076955004&doi=10.1145%2f3365676&partnerID=40&md5=3fde57ba009719c8d74a17834be1f9b8,"Big graphs (networks) arising in numerous application areas pose significant challenges for graph analysts as these graphs grow to billions of nodes and edges and are prohibitively large to fit in the main memory. Finding the number of triangles in a graph is an important problem in the mining and analysis of graphs. In this article, we present two efficient MPI-based distributed memory parallel algorithms for counting triangles in big graphs. The first algorithm employs overlapping partitioning and efficient load balancing schemes to provide a very fast parallel algorithm. The algorithm scales well to networks with billions of nodes and can compute the exact number of triangles in a network with 10 billion edges in 16 minutes. The second algorithm divides the network into non-overlapping partitions leading to a space-efficient algorithm. Our results on both artificial and real-world networks demonstrate a significant space saving with this algorithm. We also present a novel approach that reduces communication cost drastically leading the algorithm to both a space- and runtime-efficient algorithm. Further, we demonstrate how our algorithms can be used to list all triangles in a graph and compute clustering coefficients of nodes. Our algorithm can also be adapted to a parallel approximation algorithm using an edge sparsification method. © 2019 Association for Computing Machinery.",Clustering-coefficient; Graph mining; Massive networks; Parallel algorithms; Social networks; Triangle-counting,Approximation algorithms; Graph theory; Graphic methods; Large scale systems; Parallel algorithms; Social networking (online); Clustering coefficient; Fast parallel algorithms; Graph mining; Load-balancing schemes; Massive networks; Parallel approximation algorithms; Space efficient algorithms; Triangle-counting; Clustering algorithms
Evolutionary classifier and cluster selection approach for ensemble classification,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076969655&doi=10.1145%2f3366633&partnerID=40&md5=0ea75d77859270c2815d3015a6b5a261,"Ensemble classifiers improve the classification performance by combining several classifiers using a suitable fusion methodology. Many ensemble classifier generation methods have been developed that allowed the training of multiple classifiers on a single dataset. As such random subspace is a common methodology utilized by many state-of-the-art ensemble classifiers that generate random subsamples from the input data and train classifiers on different subsamples. Real-world datasets have randomness and noise in them, therefore not all randomly generated samples are suitable for training. In this article, we propose a novel particle swarm optimization-based approach to optimize the random subspace to generate an ensemble classifier. We first generate a random subspace by incrementally clustering input data and then optimize all generated data clusters. On all optimized data clusters, a set of classifiers is trained and added to the pool. The pool of classifiers is then optimized and an optimized ensemble classifier is generated. The proposed approach is tested on 12 benchmark datasets from the UCI repository and results are compared with current state-of-the-art ensemble classifier approaches. A statistical significance test is also conducted and an analysis is presented. © 2019 Association for Computing Machinery.",Clustering; Multiple classifier systems; Particle swarm optimization,Input output programs; Particle swarm optimization (PSO); Classification performance; Clustering; Ensemble classification; Ensemble classifiers; Multiple classifier systems; Multiple classifiers; Real-world datasets; Statistical significance test; Classification (of information)
A review on OLAP technologies applied to information networks,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077011016&doi=10.1145%2f3370912&partnerID=40&md5=2450d8bd8fe7c264df387aafc2a14dd8,"Many real systems produce network data or highly interconnected data, which can be called information networks. These information networks form a critical component in modern information infrastructure, constituting a large graph data volume. The analysis of information network data covers several technological areas, among them OLAP technologies. OLAP is a technology that enables multi-dimensional and multi-level analysis on a large volume of data, providing aggregated data visualizations with different perspectives. This article presents a literature review on the main applications of OLAP technology in the analysis of information network data. To achieve such goal, it shows a systematic review to list the works that apply OLAP technologies in graph data. It defines seven comparison criteria (Materialization, Network, Selection, Aggregation, Model, OLAP Operations, Analytics) to qualify the works found based on their functionalities. The works are analyzed according to each criterion and discussed to identify trends and challenges in the application of OLAP in the information network. © 2019 Association for Computing Machinery.",Cube; Data warehousing; Graph analytics; Graph-database; Information network; OLAP,Data warehouses; Graph Databases; Information analysis; Comparison criterion; Critical component; Cube; Graph analytics; Information infrastructures; Information networks; Multi-level analysis; OLAP; Information services
A new smooth approximation to the zero one loss with a probabilistic interpretation,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076977839&doi=10.1145%2f3365672&partnerID=40&md5=c2622998c592bba249f9c8bf1c820870,"We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. Our approach is based on using the posterior mean of a novel generalized Beta-Bernoulli formulation. This leads to a generalized logistic function that approximates the zero one loss, but retains a probabilistic formulation conferring a number of useful properties. The approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction. We present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima. Our experiments indicate that optimization quality is improved when learning metaparameters are themselves optimized using a validation set. Our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard UC Irvine and libSVM evaluation datasets to product review predictions and a visual information extraction task. We observe that the approach is as follows: (1) more robust to outliers compared to the logistic and hinge losses; (2) outperforms comparable logistic and max margin models on larger scale benchmark problems; (3) when combined with Gaussian-Laplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than Support Vector Machine classifiers; and (4) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance. © 2019 Association for Computing Machinery.",Classification; Logistic regression; Machine learning; Probabilistic models; Supervised learning,Benchmarking; Classification (of information); Forecasting; Gradient methods; Information retrieval; Learning systems; Machine learning; Optimization; Regression analysis; Supervised learning; Support vector machines; Kernel logistic regression; Logistic regressions; Optimization quality; Probabilistic formulation; Probabilistic interpretation; Probabilistic models; Structured prediction; Support vector machine classifiers; Data reduction
Local overlapping community detection,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076946333&doi=10.1145%2f3361739&partnerID=40&md5=3ae1127779c856ed8294fb26b04ceed3,"Local community detection refers to finding the community that contains the given node based on local information, which becomes very meaningful when global information about the network is unavailable or expensive to acquire. Most studies on local community detection focus on finding non-overlapping communities. However, many real-world networks contain overlapping communities like social networks. Given an overlapping node that belongs to multiple communities, the problem is to find communities to which it belongs according to local information. We propose a framework for local overlapping community detection. The framework has three steps. First, find nodes in multiple communities to which the given node belongs. Second, select representative nodes from nodes obtained above, which tends to be in different communities. Third, discover the communities to which these representative nodes belong. In addition, to demonstrate the effectiveness of the framework, we implement six versions of this framework. Experimental results demonstrate that the six implementation versions outperform the other algorithms. © 2019 Association for Computing Machinery.",Community detection; Local community detection; Local overlapping community detection; Social network,Computer science; Data mining; Social networking (online); Community detection; Global informations; Implementation versions; Local community; Local information; Local overlapping communities; Overlapping communities; Real-world networks; Population dynamics
A pipeline computing method of SpTV for three-order tensors on CPU and GPU,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075598110&doi=10.1145%2f3363575&partnerID=40&md5=d0e999a9ecb5ea1d2907d4ef7f848279,"Tensors have drawn a growing attention in many applications, such as physics, engineering science, social networks, recommended systems. Tensor decomposition is the key to explore the inherent intrinsic data relationship of tensor. There are many sparse tensor and vector multiplications (SpTV) in tensor decomposition. We analyze a variety of storage formats of sparse tensors and develop a piecewise compression strategy to improve the storage efficiency of large sparse tensors. This compression strategy can avoid storing a large number of empty slices and empty fibers in sparse tensors, and thus the storage space is significantly reduced. A parallel algorithm for the SpTV based on the high-order compressed format based on slices is designed to greatly improve its computing performance on graphics processing unit. Each tensor is cut into multiple slices to form a series of sparse matrix and vector multiplications, which form the pipelined parallelism. The transmission time of the slices can be hidden through pipelined parallel to further optimize the performance of the SpTV. © 2019 Association for Computing Machinery.",Pipeline; Slices; Tensor; Tensor and vector multiplication,Computer graphics; Computer graphics equipment; Digital storage; Graphics processing unit; Pipelines; Program processors; Compression strategies; Computing performance; Engineering science; Pipelined parallelism; Recommended systems; Slices; Tensor decomposition; Vector multiplication; Tensors
In Search of a Stochastic Model for the E-News Reader,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075604966&doi=10.1145%2f3362695&partnerID=40&md5=c1341cbafe144b60a2c86b40de4ad758,"E-news readers have increasingly at their disposal a broad set of news articles to read. Online newspaper sites use recommender systems to predict and to offer relevant articles to their users. Typically, these recommender systems do not leverage users' reading behavior. If we know how the topics-reads change in a reading session, we may lead to fine-tuned recommendations, for example, after reading a certain number of sports items, it may be counter-productive to keep recommending other sports news. The motivation for this article is the assumption that understanding user behavior when reading successive online news articles can help in developing better recommender systems. We propose five categories of stochastic models to describe this behavior depending on how the previous reading history affects the future choices of topics.We instantiated these five classes with many different stochastic processes covering short-term memory, revealed-preference, cumulative advantage, and geometric sojourn models. Our empirical study is based on large datasets of E-news from two online newspapers. We collected data from more than 13 million users who generated more than 23 million reading sessions, each one composed by the successive clicks of the users on the posted news. We reduce each user session to the sequence of reading news topics. The models were fitted and compared using the Akaike Information Criterion and the Brier Score. We found that the best models are those in which the user moves through topics influenced only by their most recent readings. Our models were also better to predict the next reading than the recommender systems currently used in these journals showing that our models can improve user satisfaction. © 2019 Association for Computing Machinery.",Modeling user behavior; Online newspapers; Stochastic models,Behavioral research; Large dataset; Newsprint; Online systems; Random processes; Recommender systems; Sports; Stochastic systems; Technology transfer; Websites; Akaike information criterion; Empirical studies; Large datasets; Online newspaper; Revealed preference; Short term memory; User behaviors; User satisfaction; Stochastic models
Attention models in graphs: A survey,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075546468&doi=10.1145%2f3363574&partnerID=40&md5=32e6b2123cefb1b732575756c29d2eae,"Graph-structured data arise naturally in many different application domains. By representing data as graphs, we can capture entities (i.e., nodes) as well as their relationships (i.e., edges) with each other. Many useful insights can be derived from graph-structured data as demonstrated by an ever-growing body of work focused on graph mining. However, in the real-world, graphs can be both large-with many complex patterns-and noisy, which can pose a problem for effective graph mining. An effective way to deal with this issue is to incorporate ""attention"" into graph mining solutions. An attention mechanism allows a method to focus on task-relevant parts of the graph, helping it to make better decisions. In this work, we conduct a comprehensive and focused survey of the literature on the emerging field of graph attention models. We introduce three intuitive taxonomies to group existing work. These are based on problem setting (type of input and output), the type of attention mechanism used, and the task (e.g., graph classification, link prediction). We motivate our taxonomies through detailed examples and use each to survey competing approaches from a unique standpoint. Finally, we highlight several challenges in the area and discuss promising directions for future work. © 2019 Association for Computing Machinery.",Attention mechanism; Deep learning; Graph attention; Graph attention survey,Data mining; Deep learning; Graphic methods; Taxonomies; Attention mechanisms; Attention model; Complex pattern; Graph attention; Graph classification; Graph structured data; Input and outputs; Link prediction; Surveys
Mining rank data,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075549841&doi=10.1145%2f3363572&partnerID=40&md5=8eed02f55b6c7ac6a97fa5779d1a9ccb,"The problem of frequent pattern mining has been studied quite extensively for various types of data, including sets, sequences, and graphs. Somewhat surprisingly, another important type of data, namely rank data, has received very little attention in data mining so far. In this article, we therefore address the problem of mining rank data, that is, data in the form of rankings (total orders) of an underlying set of items. More specifically, two types of patterns are considered, namely frequent rankings and dependencies between such rankings in the form of association rules. Algorithms for mining frequent rankings and frequent closed rankings are proposed and tested experimentally, using both synthetic and real data. © 2019 Association for Computing Machinery.",Association rules; Data mining; Frequent pattern mining; Rank data,Association rules; Frequent pattern mining; Rank data; Synthetic and real data; Total order; Data mining
CT LIS: Learning influences and susceptibilities through temporal behaviors,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075570952&doi=10.1145%2f3363570&partnerID=40&md5=f7ddfaca730f4a09a6b9d015414af8c3,"How to quantify influences between users, seeing that social network users influence each other in their temporal behaviors? Previous work has directly defined an independent model parameter to capture the interpersonal influence between each pair of users. To do so, these models need a parameter for each pair of users, which results in high-dimensional models becoming easily trapped into the overfitting problem. However, such models do not consider how influences depend on each other if influences are sent from the same user or if influences are received by the same user. Therefore, we propose a model that defines parameters for every user with a latent influence vector and a susceptibility vector, opposite to define influences on user pairs. Such low-dimensional representations naturally cause the interpersonal influences involving the same user to be coupled with each other, thus reducing the model's complexity. Additionally, the model can easily consider the temporal information and sentimental polarities of users' messages. Finally, we conduct extensive experiments on two real-world Microblog datasets, showing that our model with such representations achieves best performance on three prediction tasks, compared to the state-of-the-art and pair-wise baselines. © 2019 Copyright held by the owner/author(s).",Influence; Susceptibility; Time series; User behaviors,Magnetic susceptibility; Time series; High-dimensional models; Independent model; Influence; Interpersonal influences; Low-dimensional representation; Over fitting problem; Temporal information; User behaviors; Behavioral research
High-utility itemset mining with effective pruning strategies,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075181464&doi=10.1145%2f3363571&partnerID=40&md5=5c5bdb261be668b4189cc084a3683694,"High-utility itemset mining is a popular data mining problem that considers utility factors, such as quantity and unit profit of items besides frequency measure from the transactional database. It helps to find the most valuable and profitable products/items that are difficult to track by using only the frequent itemsets. An item might have a high-profit value which is rare in the transactional database and has a tremendous importance. While there are many existing algorithms to find high-utility itemsets (HUIs) that generate comparatively large candidate sets, our main focus is on significantly reducing the computation time with the introduction of new pruning strategies. The designed pruning strategies help to reduce the visitation of unnecessary nodes in the search space, which reduces the time required by the algorithm. In this article, two new stricter upper bounds are designed to reduce the computation time by refraining from visiting unnecessary nodes of an itemset. Thus, the search space of the potential HUIs can be greatly reduced, and the mining procedure of the execution time can be improved. The proposed strategies can also significantly minimize the transaction database generated on each node. Experimental results showed that the designed algorithm with two pruning strategies outperform the state-of-the-art algorithms for mining the required HUIs in terms of runtime and number of revised candidates. The memory usage of the designed algorithm also outperforms the state-of-theart approach. Moreover, amulti-thread concept is also discussed to further handle the problem of big datasets. © 2019 Association for Computing Machinery.",High-utility itemset; HUIM; Multiple threads; Pruning strategy,Database systems; Profitability; High utility itemset; High utility itemset minings; High utility itemsets; HUIM; Multiple threads; Pruning strategy; State-of-the-art algorithms; Transactional database; Data mining
Multi-label punitive kNN with self-adjusting memory for drifting data streams,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075637250&doi=10.1145%2f3363573&partnerID=40&md5=9502c9c75a5d3db29aa3131e597959ec,"In multi-label learning, datamay simultaneously belong to more than one class. When multi-label data arrives as a stream, the challenges associated with multi-label learning are joined by those of data stream mining, including the need for algorithms that are fast and flexible, able to match both the speed and evolving nature of the stream. This article presents a punitive k nearest neighbors algorithm with a self-adjusting memory (MLSAMPkNN) for multi-label, drifting data streams. The memory adjusts in size to contain only the current concept and a novel punitive system identifies and penalizes errant data examples early, removing them from the window. By retaining and using only data that are both current and beneficial, MLSAMPkNN is able to adapt quickly and efficiently to changes within the data stream while still maintaining a low computational complexity. Additionally, the punitive removal mechanism offers increased robustness to various data-level difficulties present in data streams, such as class imbalance and noise. The experimental study compares the proposal to 24 algorithms using 30 real-world and 15 artificial multi-label data streams on six multi-label metrics, evaluation time, and memory consumption. The superior performance of the proposed method is validated through non-parametric statistical analysis, proving both high accuracy and low time complexity. MLSAMPkNN is a versatile classifier, capable of returning excellent performance in diverse stream scenarios. © 2019 Copyright held by the owner/author(s).",Concept drift; Data stream; Multi-label classification; Nearest neighbor,Classification (of information); Learning systems; Concept drifts; Data stream; K-nearest neighbors; Low computational complexity; Multi label classification; Multi-label learning; Nearest neighbors; Non-parametric statistical analysis; Nearest neighbor search
Aspect aware learning for aspect category sentiment analysis,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073750560&doi=10.1145%2f3350487&partnerID=40&md5=c7884470656a6ccbb18767e37127c105,"Aspect category sentiment analysis (ACSA) is an underexploited subtask in aspect level sentiment analysis. It aims to identify the sentiment of predefined aspect categories. The main challenge in ACSA comes from the fact that the aspect category may not occur in the sentence in most of the cases. For example, the review ""they have delicious sandwiches"" positively talks about the aspect category ""food"" in an implicit manner. In this article, we propose a novel aspect aware learning (AAL) framework for ACSA tasks. Our key idea is to exploit the interaction between the aspect category and the contents under the guidance of both sentiment polarity and predefined categories. To this end, we design a two-way memory network for integrating AAL into the framework of sentiment classification.We further present two algorithms to incorporate the potential impacts of aspect categories. One is to capture the correlations between aspect terms and the aspect category like ""sandwiches"" and ""food."" The other is to recognize the aspect category for sentiment representations like ""food"" for ""delicious."" We conduct extensive experiments on four SemEval datasets. The results reveal the essential role of AAL in ACSA by achieving the state-of-the-art performance. © 2019 Association for Computing Machinery.",Aspect aware learning; Aspect category sentiment analysis; Memory network,Computer science; Data mining; Aspect aware learning; Memory network; Potential impacts; State-of-the-art performance; Subtask; Two ways; Sentiment analysis
Interactive recommendation with user-specific deep reinforcement learning,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073759780&doi=10.1145%2f3359554&partnerID=40&md5=05e0da30ee946e3b2db67789b3c4fb68,"In this article, we study a multi-step interactive recommendation problem for explicit-feedback recommender systems. Different from the existing works, we propose a novel user-specific deep reinforcement learning approach to the problem. Specifically, we first formulate the problem of interactive recommendation for each target user as a Markov decision process (MDP). We then derive a multi-MDP reinforcement learning task for all involved users. To model the possible relationships (including similarities and differences) between different users' MDPs, we construct user-specific latent states by using matrix factorization. After that, we propose a user-specific deepQ-learning (UDQN) method to estimate optimal policies based on the constructed user-specific latent states. Furthermore, we propose Biased UDQN (BUDQN) to explicitly model user-specific information by employing an additional bias parameter when estimating the Q-values for different users. Finally, we validate the effectiveness of our approach by comprehensive experimental results and analysis. © 2019 Association for Computing Machinery.",Deep Q-learning; Deep reinforcement learning; Interactive recommendation,Deep learning; Factorization; Learning algorithms; Machine learning; Markov processes; Explicit feedback; Interactive recommendation; Markov Decision Processes; Matrix factorizations; Optimal policies; Q-learning; Reinforcement learning approach; Specific information; Reinforcement learning
A unified framework with multi-source data for predicting passenger demands of ride services,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073750864&doi=10.1145%2f3355563&partnerID=40&md5=f83b8e665d18ce5c677795414bb30ce6,"Ride-hailing applications have been offering convenient ride services for people in need. However, such applications still suffer from the issue of supply-demand disequilibrium, which is a typical problem for traditional taxi services. With effective predictions on passenger demands, we can alleviate the disequilibrium by predispatching, dynamic pricing or avoiding dispatching cars to zero-demand areas. Existing studies of demand predictions mainly utilize limited data sources, trajectory data, or orders of ride services or both of them, which also lacks a multi-perspective consideration. In this article, we present a unified framework with a new combined model and a road-network-based spatial partition to leverage multi-source data and model the passenger demands from temporal, spatial, and zero-demand-area perspectives. In addition, our framework realizes offline training and online predicting, which can satisfy the real-time requirement more easily. We analyze and evaluate the performance of our combined model using the actual operational data from UCAR. The experimental results indicate that our model outperforms baselines on both Mean Absolute Error and Root Mean Square Error on average. © 2019 Association for Computing Machinery.",Demand prediction; Ride service; Ride-hailing application; Spatio-temporal data,Mean square error; Motor transportation; Taxicabs; Demand prediction; Mean absolute error; Multi-perspective; Off-line training; Real time requirement; Ride service; Root mean square errors; Spatio-temporal data; Forecasting
Probabilistic mixture model for mapping the underground pipes,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073227129&doi=10.1145%2f3344721&partnerID=40&md5=abb565eeac7d10b637809472a9d2eb86,"Buried pipes beneath our city are blood vessels that feed human civilization through the supply of water, gas, electricity, and so on, and mapping the buried pipes has long been addressed as an issue. In this article, a suitable coordinate of the detected area is established, the noisy Ground Penetrating Radar (GPR) and Global Positioning System (GPS) data are analyzed and normalized, and the pipeline is described mathematically. Based on these, the Probabilistic Mixture Model is proposed to map the buried pipes, which takes discrete noisy GPR and GPS data as the input and the accurate pipe locations and directions as the output. The proposed model consists of the Preprocessing, the Pipe Fitting algorithm, the Classification Fitting Expectation Maximization (CFEM) algorithm, and the Angle-limited Hough (Al-Hough) transform. The direction information of the detecting point is added into the measuring of the distance from the point to nearby pipelines, to handle some areas where the pipes are intersected or difficult to classify. The Expectation Maximization (EM) algorithm is upgraded to CFEM algorithm that is able to classify detecting points into different classes, and connect and fit multiple points in each class to get accurate pipeline locations and directions, and the Al-Hough transform provides reliable initializations for CFEM, to some extent, ensuring the convergence of the proposed model. The experimental results on the simulated and real-world datasets demonstrate the effectiveness of the proposed model. © 2019 Association for Computing Machinery.",,Blood vessels; Classification (of information); Geological surveys; Geophysical prospecting; Global positioning system; Ground penetrating radar systems; Hough transforms; Mapping; Maximum principle; Mixtures; Pipelines; Expectation - maximizations; Expectation-maximization algorithms; Fitting algorithms; Ground penetrating radar (GPR); Human civilization; Probabilistic mixture models; Real-world datasets; Underground pipes; Pipe fittings
Fine-grained air qality inference with remote sensing data and ubiquitous urban data,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073114020&doi=10.1145%2f3340847&partnerID=40&md5=a65af8373be61687a0ea8f92dfef2aab,"Air quality has gained much attention in recent years and is of great importance to protecting people's health. Due to the influence of multiple factors, the limited air quality monitoring stations deployed in cities are unable to provide fne-grained air quality information. One cost-effective way is to infer air quality with records from existing monitoring stations. However, the severe data sparsity problem (e.g., only 0.2% data are known) leads to the failure of most inference methods. We observe that remote sensing data are of high quality and have a strong correlation with the air quality. Therefore, we propose to integrate remote sensing data and ubiquitous urban data for the air quality inference. But there are two main challenges, i.e., data heterogeneity and incompleteness of the remote sensing data. To address the challenges, we propose a two-stage approach. In the frst stage, we infer and predict air quality conditions of some places leveraging the remote sensing data and meteorological data with two proposed ANN-based methods, respectively. This stage signifcantly alleviates the data sparsity problem. In the second stage, the records and estimated air quality data are put in a tensor. A tensor decomposition method is applied to complete the tensor. The features extracted from urban data are classifed into the spatial features (i.e., road features and POI features) and the temporal features (i.e., meteorological features) as the constraints to further address the data sparsity problem. In addition, an iterative training framework is proposed to improve the inference performance. Experiments on a real-world dataset show that our approach outperforms state-of-the-art methods, such as U-Air. © 2019 Association for Computing Machinery.",Air quality inference; AOT; Neural network; Remote sensing; Tensor decomposition,Air quality; Cost effectiveness; Iterative methods; Meteorology; Neural networks; Tensors; Air quality monitoring stations; Data sparsity problems; Meteorological data; Monitoring stations; Quality information; Remote sensing data; State-of-the-art methods; Tensor decomposition; Remote sensing
Hybrid crowd-machine wrapper inference,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073095149&doi=10.1145%2f3344720&partnerID=40&md5=80a397d9ba6ed84b0ad765661b99b861,"Wrapper inference deals in generating programs to extract data from Web pages. Several supervised and unsupervised wrapper inference approaches have been proposed in the literature. On one hand, unsupervised approaches produce erratic wrappers: whenever the sources do not satisfy underlying assumptions of the inference algorithm, their accuracy is compromised. On the other hand, supervised approaches produce accurate wrappers, but since they need training data, their scalability is limited. The recent advent of crowdsourcing platforms has opened new opportunities for supervised approaches, as they make possible the production of large amounts of training data with the support of workers recruited online. Nevertheless, involving human workers has monetary costs. We present an original hybrid crowd-machine wrapper inference system that offers the benefts of both approaches exploiting the cooperation of crowd workers and unsupervised algorithms. Based on a principled probabilistic model that estimates the quality of wrappers, humans workers are recruited only when unsupervised wrapper induction algorithms are not able to produce sufciently accurate solutions. © 2019 Association for Computing Machinery.",Crowdsourcing; Data extraction; Wrapper inference,Crowdsourcing; Data mining; Websites; Crowdsourcing platforms; Data extraction; Inference algorithm; Probabilistic modeling; Unsupervised algorithms; Unsupervised approaches; Wrapper induction; Wrapper inference; Inference engines
Krylov subspace approximation for local community detection in large networks,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073113192&doi=10.1145%2f3340708&partnerID=40&md5=8a90b1c6025c14655daafa59cb1f2c10,"Community detection is an important information mining task to uncover modular structures in large networks. For increasingly common large network datasets, global community detection is prohibitively expensive, and attention has shifted to methods that mine local communities, i.e., identifying all latent members of a particular community from a few labeled seed members. To address such semi-supervised mining task, we systematically develop a local spectral (LOSP) subspace-based community detection method, called LOSP. We defne a family of LOSP subspaces based on Krylov subspaces, and seek a sparse indicator for the target community via an 1 norm minimization over the Krylov subspace. Variants of LOSP depend on type of random walks with different diffusion speeds, type of random walks, dimension of the LOSP subspace, and step of diffusions. The effectiveness of the proposed LOSP approach is theoretically analyzed based on Rayleigh quotients, and it is experimentally verifed on a wide variety of real-world networks across social, production, and biological domains, as well as on an extensive set of synthetic LFR benchmark datasets. © 2019 Association for Computing Machinery.",Krylov subspace; Local community detection; Rayleigh quotient; Sparse linear coding; Spectral clustering,Clustering algorithms; Large dataset; Random processes; Krylov sub spaces; Linear coding; Local community; Rayleigh quotients; Spectral clustering; Population dynamics
Self-Adaptive particle swarm optimization for large-scale feature selection in classification,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073105965&doi=10.1145%2f3340848&partnerID=40&md5=927b02425f95e992a4935d8955e4ddbb,"Many evolutionary computation (EC) methods have been used to solve feature selection problems and they perform well on most small-scale feature selection problems. However, as the dimensionality of feature selection problems increases, the solution space increases exponentially. Meanwhile, there are more irrelevant features than relevant features in datasets, which leads to many local optima in the huge solution space. Therefore, the existing EC methods still suffer from the problem of stagnation in local optima on large-scale feature selection problems. Furthermore, large-scale feature selection problems with different datasets may have different properties. Thus, it may be of low performance to solve different large-scale feature selection problems with an existing EC method that has only one candidate solution generation strategy (CSGS). In addition, it is time-consuming to fnd a suitable EC method and corresponding suitable parameter values for a given largescale feature selection problem if we want to solve it effectively and efciently. In this article, we propose a self-adaptive particle swarm optimization (SaPSO) algorithm for feature selection, particularly for largescale feature selection. First, an encoding scheme for the feature selection problem is employed in the SaPSO. Second, three important issues related to self-adaptive algorithms are investigated. After that, the SaPSO algorithm with a typical self-adaptive mechanism is proposed. The experimental results on 12 datasets show that the solution size obtained by the SaPSO algorithm is smaller than its EC counterparts on all datasets. The SaPSO algorithm performs better than its non-EC and EC counterparts in terms of classifcation accuracy not only on most training sets but also on most test sets. Furthermore, as the dimensionality of the feature selection problem increases, the advantages of SaPSO become more prominent. This highlights that the SaPSO algorithm is suitable for solving feature selection problems, particularly large-scale feature selection problems. © 2019 Association for Computing Machinery.",Classifcation; Feature selection; Large-scale; Particle swarm optimization; Self-adaptive,Adaptive algorithms; Large dataset; Particle swarm optimization (PSO); Adaptive particle swarm optimizations; Classifcation; Feature selection problem; Large-scale; Self adaptive algorithms; Self-adaptive; Self-adaptive mechanisms; Small-scale features; Feature extraction
Real-Time estimation of the urban air qality with mobile sensor system,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073123789&doi=10.1145%2f3356584&partnerID=40&md5=5598f3703695e0ac4d3695b10f23d5b8,"Recently, real-time air quality estimation has attracted more and more attention from all over the world, which is close to our daily life. With the prevalence of mobile sensors, there is an emerging way to monitor the air quality with mobile sensors on vehicles. Compared with traditional expensive monitor stations, mobile sensors are cheaper and more abundant, but observations from these sensors have unstable spatial and temporal distributions, which results in the existing model could not work very well on this type of data. In this article, taking advantage of air quality data from mobile sensors, we propose an real-time urban air quality estimation method based on the Gaussian Process Regression for air pollution of the unmonitored areas, pivoting on the diffusion effect and the accumulation effect of air pollution. In order to meet the real-time demands, we propose a two-layer ensemble learning framework and a self-adaptivity mechanism to improve computational efciency and adaptivity. We evaluate our model with real data from mobile sensor system located in Beijing, China. And the experiments show that our proposed model is superior to the state-of-the-art spatial regression methods in both precision and time performances. © 2019 Association for Computing Machinery.",Air quality real-time estimation; Ensemble learning; Gaussian process regression; Mobile sensors,Gaussian distribution; Gaussian noise (electronic); Regression analysis; Accumulation effects; Ensemble learning; Gaussian process regression; Mobile sensor systems; Mobile sensors; Quality estimation; Real-time estimation; Spatial and temporal distribution; Air quality
Density-friendly graph decomposition,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073118702&doi=10.1145%2f3344210&partnerID=40&md5=f9eb552a2e8f4393882d8cdea5ab6a3b,"Decomposing a graph into a hierarchical structure via k-core analysis is a standard operation in any modern graph-mining toolkit. k-core decomposition is a simple and efcient method that allows to analyze a graph beyond its mere degree distribution. More specifcally, it is used to identify areas in the graph of increasing centrality and connectedness, and it allows to reveal the structural organization of the graph. Despite the fact that k-core analysis relies on vertex degrees, k-cores do not satisfy a certain, rather natural, density property. Simply put, the most central k-core is not necessarily the densest subgraph. This inconsistency between k-cores and graph density provides the basis of our study. We start by defning what it means for a subgraph to be locally dense, and we show that our defnition entails a nested chain decomposition of the graph, similar to the one given by k-cores, but in this case the components are arranged in order of increasing density. We show that such a locally dense decomposition for a graph G = (V, E) can be computed in polynomial time. The running time of the exact decomposition algorithm is O(|V |2|E|) but is signifcantly faster in practice. In addition, we develop a linear-time algorithm that provides a factor-2 approximation to the optimal locally dense decomposition. Furthermore, we show that the k-core decomposition is also a factor-2 approximation, however, as demonstrated by our experimental evaluation, in practice k-cores have different structure than locally dense subgraphs, and as predicted by the theory, k-cores are not always well-aligned with graph density. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fractional programming; Locally-dense graph decomposition; Max-flow,Approximation algorithms; Clustering algorithms; Core analysis; Mathematical programming; Petroleum reservoir evaluation; Polynomial approximation; Dense graphs; Experimental evaluation; Fractional programming; Graph decompositions; Hierarchical structures; Linear-time algorithms; Max-flow; Structural organization; Flow graphs
Computing top-k closeness centrality faster in unweighted graphs,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073096024&doi=10.1145%2f3344719&partnerID=40&md5=29e8a5f9414a3692a47badafde28be12,"Given a connected graph G = (V, E), whereV denotes the set of nodes and E the set of edges of the graph, the length (that is, the number of edges) of the shortest path between two nodes v and w is denoted by d(v,w). The closeness centrality of a vertex v is then defned as w Vn-d1(v,w), where n = |V |. This measure is widely used in the analysis of real-world complex networks, and the problem of selecting the k most central vertices has been deeply analyzed in the last decade. However, this problem is computationally not easy, especially for large networks: in the frst part of the article, we prove that it is not solvable in time O(|E|2-) on directed graphs, for any constant > 0, under reasonable complexity assumptions. Furthermore, we propose a new algorithm for selecting the k most central nodes in a graph: we experimentally show that this algorithm improves signifcantly both the textbook algorithm, which is based on computing the distance between all pairs of vertices, and the state of the art. For example, we are able to compute the top k nodes in few dozens of seconds in real-world networks with millions of nodes and edges. Finally, as a case study, we compute the 10 most central actors in the Internet Movie Database (IMDB) collaboration network, where two actors are linked if they played together in a movie, and in the Wikipedia citation network, which contains a directed edge from a page p to a page q if p contains a link to q. © 2019 Association for Computing Machinery.",Centrality; Closeness; Complex networks,Directed graphs; Centrality; Citation networks; Closeness; Closeness centralities; Collaboration network; Complexity assumptions; Internet movie database; Real-world networks; Complex networks
Bayesian model selection approach to multiple change-points detection with non-local prior distributions,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073122477&doi=10.1145%2f3340804&partnerID=40&md5=29d57b2bb01c54fffe67f3370c3d7906,"We propose a Bayesian model selection (BMS) boundary detection procedure using non-local prior distributions for a sequence of data with multiple systematic mean changes. By using the non-local priors in the BMS framework, the BMS method can effectively suppress the non-boundary spike points with large instantaneous changes. Further, we speedup the algorithm by reducing the multiple change points to a series of single change point detection problems. We establish the consistency of the estimated number and locations of the change points under various prior distributions. From both theoretical and numerical perspectives, we show that the non-local inverse moment prior leads to the fastest convergence rate in identifying the true change points on the boundaries. Extensive simulation studies are conducted to compare the BMS with existing methods, and our method is illustrated with application to the magnetic resonance imaging guided radiation therapy data. © 2019 Association for Computing Machinery.",Bayes factor; Bayesian model selection; Inverse moment prior; Local prior; Marginal likelihood; Moment prior; Parallel computing,Bayesian networks; Magnetic resonance imaging; Parallel processing systems; Bayes factor; Bayesian model selection; Inverse moment prior; Local prior; Marginal likelihood; Inverse problems
Clustering Users by Their Mobility Behavioral Patterns,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075557883&doi=10.1145%2f3322126&partnerID=40&md5=8a062fc3ff95b1fd21fc242feca88e2b,"The immense stream of data from mobile devices during recent years enables one to learn more about human behavior and provide mobile phone users with personalized services. In this work, we identify clusters of users who share similar mobility behavioral patterns. We analyze trajectories of semantic locations to find users who have similar mobility ""lifestyle,"" even when they live in different areas. For this task, we propose a new grouping scheme that is called Lifestyle-Based Clustering (LBC).We represent the mobilitymovement of each user by a Markov model and calculate the Jensen Shannon distances among pairs of users. The pairwise distances are represented by a similarity matrix, which is used for the clustering. To validate the unsupervised clustering task, we develop an entropy-based clustering measure, namely, an index that measures the homogeneity of mobility patterns within clusters of users. The analysis is validated on a real-world dataset that contains location-movements of 50,000 cellular phone users that were analyzed over a two-month period. © 2019 Association for Computing Machinery. All rights reserved.",clustering evaluation; Clustering trajectories,Cellular telephones; Markov processes; Mobile telecommunication systems; Semantics; Behavioral patterns; Clustering evaluation; Mobile-phone users; Pairwise distances; Personalized service; Semantic locations; Similarity matrix; Unsupervised clustering; Behavioral research
Bursty Event Detection in Twitter Streams,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075553563&doi=10.1145%2f3332185&partnerID=40&md5=0cb6011138e63a1a59a95461d7177d47,"Social media, in recent years, have become an invaluable source of information for both public and private organizations to enhance the comprehension of people interests and the onset of new events. Twitter, especially, allows a fast spread of news and events happening real time that can contribute to situation awareness during emergency situations, but also to understand trending topics of a period. The article proposes an online algorithm that incrementally groups tweet streams into clusters. The approach summarizes the examined tweets into the cluster centroid by maintaining a number of textual and temporal features that allow the method to effectively discover groups of interest on particular themes. Experiments on messages posted by users addressing different issues, and a comparison with state-of-The-Art approaches show that the method is capable to detect discussions regarding topics of interest, but also to distinguish bursty events revealed by a sudden spreading of attention on messages published by users. © 2019 Association for Computing Machinery. All rights reserved.",bursty event; event detection; online clustering; Twitter,Computer science; Data mining; bursty event; Emergency situation; Event detection; Online-clustering; Private organizations; Situation awareness; State-of-the-art approach; Twitter; Social networking (online)
Maximally Correlated Principal Component Analysis Based on Deep Parameterization Learning,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075570591&doi=10.1145%2f3332183&partnerID=40&md5=60665e2b181ef476671e4125f331c7eb,"Dimensionality reduction is widely used to deal with high-dimensional data. As a famous dimensionality reduction method, principal component analysis (PCA) aiming at finding the low dimension feature of original data has made great successes, and many improved PCA algorithms have been proposed. However, most algorithms based on PCA only consider the linear correlation of data features. In this article, we propose a novel dimensionality reduction model called maximally correlated PCA based on deep parameterization learning (MCPCADP), which takes nonlinear correlation into account in the deep parameterization framework for the purpose of dimensionality reduction. The new model explores nonlinear correlation by maximizing Ky-Fan norm of the covariance matrix of nonlinearly mapped data features. A new BP algorithm for model optimization is derived. In order to assess the proposed method, we conduct experiments on both a synthetic database and several real-world databases. The experimental results demonstrate that the proposed algorithm is comparable to several widely used algorithms. © 2019 Association for Computing Machinery. All rights reserved.",back propagation; classification; deep parameterization learning; Maximally correlated principal component analysis,Backpropagation; Classification (of information); Clustering algorithms; Covariance matrix; Data reduction; Deep learning; Parameterization; deep parameterization learning; Dimensionality reduction; Dimensionality reduction method; Dimensionality-reduction models; High dimensional data; Model optimization; Non-linear correlations; Real-world database; Principal component analysis
On the Impact of Voice Encoding and Transmission on the Predictions of Speaker Warmth and Attractiveness,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075551593&doi=10.1145%2f3332146&partnerID=40&md5=490cbaf875ad890ff288fa42990c5ab5,"Modern human-computer interaction systems may not only be based on interpreting natural language but also on detecting speaker interpersonal characteristics in order to determine dialog strategies. This may be of high interest in different fields such as telephone marketing or automatic voice-based interactive services. However, when such systems encounter signals transmitted over a communication network instead of clean speech, e.g., in call centers, the speaker characterization accuracy might be impaired by the degradations caused in the speech signal by the encoding and communication processes. This article addresses a binary classification of high versus low warm-attractive speakers over different channel and encoding conditions. The ground truth is derived from ratings given to clean speech extracted from an extensive subjective test. Our results show that, under the considered conditions, the AMR-WB+ codec permits good levels of classification accuracy, comparable to the classification with clean, non-degraded speech. This is especially notable for the case of a Random Forest-based classifier, which presents the best performance among the set of evaluated algorithms. The impact of different packet loss rates has been examined, whereas jitter effects have been found to be negligible. © 2019 Association for Computing Machinery. All rights reserved.",predictive modeling; Speaker characteristics; speech processing; transmission channels,Decision trees; Encoding (symbols); Human computer interaction; Signal encoding; Speech communication; Speech processing; Binary classification; Classification accuracy; Communication process; Human-computer interaction system; Interactive services; Predictive modeling; Speaker characteristics; Transmission channels; Speech transmission
A Distance Measure for the Analysis of Polar Opinion Dynamics in Social Networks,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075581957&doi=10.1145%2f3332168&partnerID=40&md5=5d9d49a1e6ef22b0df21f4cd0aaf78f3,"Analysis of opinion dynamics in social networks plays an important role in today's life. For predicting users' political preference, it is particularly important to be able to analyze the dynamics of competing polar opinions, such as pro-Democrat vs. pro-Republican. While observing the evolution of polar opinions in a social network over time, can we tell when the network evolved abnormally? Furthermore, can we predict how the opinions of the users will change in the future? To answer such questions, it is insufficient to study individual user behavior, since opinions can spread beyond users' ego-networks. Instead, we need to consider the opinion dynamics of all users simultaneously and capture the connection between the individuals' behavior and the global evolution pattern of the social network. In this work, we introduce the Social Network Distance (SND)-A distance measure that quantifies the likelihood of evolution of one snapshot of a social network into another snapshot under a chosen model of polar opinion dynamics. SND has a rich semantics of a transportation problem, yet, is computable in time linear in the number of users and, as such, is applicable to large-scale online social networks. In our experiments with synthetic and Twitter data, we demonstrate the utility of our distance measure for anomalous event detection. It achieves a true positive rate of 0.83, twice as high as that of alternatives. The same predictions presented in precision-recall space show that SND retains perfect precision for recall up to 0.2. Its precision then decreases while maintaining more than 2-fold improvement over alternatives for recall up to 0.95. When used for opinion prediction in Twitter data, SND's accuracy is 75.6%, which is 7.5% higher than that of the next best method. © 2019 Association for Computing Machinery. All rights reserved.",anomaly detection; competing opinions; distance measure; earth mover's distance; minimum-cost network flow; model-driven analysis; opinion dynamics; opinion prediction; polar opinions; polarization; Social network; time-series; transportation problem; wasserstein metric,Anomaly detection; Behavioral research; Clustering algorithms; Dynamics; Forecasting; Polarization; Semantics; Time series; Time series analysis; competing opinions; Distance measure; Earth Mover's distance; Minimum cost network flow; Model-driven; Opinion dynamics; polar opinions; Transportation problem; Wasserstein metric; Social networking (online)
Heterogeneous-length text topic modeling for reader-Aware multi-document summarization,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075582088&doi=10.1145%2f3333030&partnerID=40&md5=bf29c18c65fdd919ea61067bcd191f17,"More and more user comments like Tweets are available, which often contain user concerns. In order to meet the demands of users, a good summary generating from multiple documents should consider reader interests as reflected in reader comments. In this article, we focus on how to generate a summary from multi-document documents by considering reader comments, named as reader-Aware multi-document summarization (RA-MDS).We present an innovative topic-based method for RA-MDA, which exploits latent topics to obtain the most salient and lessen redundancy summary from multiple documents. Since finding latent topics for RA-MDS is a crucial step, we also present a Heterogeneous-length Text Topic Modeling (HTTM) to extract topics from the corpus that includes both news reports and user comments, denoted as heterogeneous-length texts. In this case, the latent topics extract by HTTM cover not only important aspects of the event, but also aspects that attract reader interests. Comparisons on summary benchmark datasets also confirm that the proposed RA-MDS method is effective in improving the quality of extracted summaries. In addition, experimental results demonstrate that the proposed topic modeling method outperforms existing topic modeling algorithms. © 2019 Association for Computing Machinery. All rights reserved.",heterogeneous-length text; LDA; multi-document summarization; Topic modeling,Benchmarking; Benchmark datasets; heterogeneous-length text; Multi-document; Multi-document summarization; Multiple documents; News reports; Topic Modeling; Topic modeling algorithms; Data mining
Housing demand estimation based on express delivery data,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075545691&doi=10.1145%2f3332522&partnerID=40&md5=63a5e70a0856eaab1687eda218926f24,"Housing demand estimation is an important topic in the field of economic research. It is beneficial and helpful for various applications including real estate market regulation and urban planning, and therefore is crucial for both real estate investors and government administrators. Meanwhile, given the rapid development of the express industry, abundant useful information is embedded in express delivery records, which is helpful for researchers in profiling urban life patterns. The express delivery behaviors of the residents in a residential community can reflect the housing demand to some extent. Although housing demand has been analyzed in previous studies, its estimation has not been very good, and the subject remains under explored. To this end, in this article, we propose a systematic housing demand estimation method based on express delivery data. First, the express delivery records are aggregated on the community scale with the use of clustering methods, and themissing values in the records are completed. Then, various features are extracted from a less sparse dataset considering both the probability of residential mobility and the attractiveness of residential communities. In addition, given that the correlations between different districts can influence the performances of the inference model, the commonalities and differences of different districts are considered. After obtaining the features and correlations between different districts being obtained, the housing demand is estimated by using a multi-Task learning method based on neural networks. The experimental results for real-world data show that the proposed model is effective at estimating the housing demand at the residential community level. © 2019 Association for Computing Machinery.",Express delivery data; Housing demand; Multi-Task learning,Learning systems; Clustering methods; Delivery behaviors; Express delivery data; Express industries; Multitask learning; Real estate market; Residential communities; Residential mobility; Housing
Continuous-Time Relationship Prediction in Dynamic Heterogeneous Information Networks,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075556556&doi=10.1145%2f3333028&partnerID=40&md5=17c1c6e0fea9adf203d5c4018f4c4408,"Online social networks, World Wide Web, media, and technological networks, and other types of so-called information networks are ubiquitous nowadays. These information networks are inherently heterogeneous and dynamic. They are heterogeneous as they consist of multi-Typed objects and relations, and they are dynamic as they are constantly evolving over time. One of the challenging issues in such heterogeneous and dynamic environments is to forecast those relationships in the network that will appear in the future. In this article, we try to solve the problem of continuous-Time relationship prediction in dynamic and heterogeneous information networks. This implies predicting the time it takes for a relationship to appear in the future, given its features that have been extracted by considering both heterogeneity and temporal dynamics of the underlying network. To this end, we first introduce a feature extraction framework that combines the power of meta-path-based modeling and recurrent neural networks to effectively extract features suitable for relationship prediction regarding heterogeneity and dynamicity of the networks. Next, we propose a supervised non-parametric approach, called Non-Parametric Generalized Linear Model (Np-Glm), which infers the hidden underlying probability distribution of the relationship building time given its features. We then present a learning algorithm to train Np-Glm and an inference method to answer time-related queries. Extensive experiments conducted on synthetic data and three real-world datasets, namely Delicious, MovieLens, and DBLP, demonstrate the effectiveness of Np-Glm in solving continuous-Time relationship prediction problem vis-vis competitive baselines. © 2019 Association for Computing Machinery. All rights reserved.",autoencoder; heterogeneous network; Link prediction; nonparametric modeling; recurrent neural network; social network analysis,Continuous time systems; Forecasting; Heterogeneous networks; Inference engines; Information services; Learning systems; Probability distributions; Query processing; Social networking (online); Auto encoders; Generalized linear model; Heterogeneous information; Link prediction; Non-parametric model; Nonparametric approaches; On-line social networks; Technological networks; Recurrent neural networks
Identifying complements and substitutes of products: A neural network framework based on product embedding,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069475426&doi=10.1145%2f3320277&partnerID=40&md5=273ecbae1ed327b2f58a151ebf1bfd3b,"Complements and substitutes are two typical product relationships that deserve consideration in online product recommendation. One of the key objectives of recommender systems is to promote cross-selling, which heavily relies on recommending the appropriate type of products in specific scenarios. Research on consumer behavior has shown that consumers usually prefer substitutes in the browsing stage whereas complements in the purchasing stage. Thus, it is of great importance to identify the complementary and substitutable relationships between products. In this article, we design a neural network based framework that integrates the textual content and non-textual information of online reviews to mine product relationships. For the textual content, we utilize methods such as LDA topic modeling to represent products in a succinct form called “embedding.” To capture the semantics of complementary and substitutable relationships, we design a modeling process that transfers the product embeddings into semantic features and incorporates additional non-textual factors of product reviews. Extensive experiments are conducted to verify the effectiveness of the proposed product relationship mining model. The advantages and robustness of our model are discussed from various perspectives. © 2019 Association for Computing Machinery.",Complements; Online reviews; Product embedding; Product recommendation; Product relationship; Substitutes,Consumer behavior; Embeddings; Semantics; Complements; Online reviews; Product embedding; Product recommendation; Product relationship; Substitutes; Product design
A survey of parallel sequential pattern mining,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069544254&doi=10.1145%2f3314107&partnerID=40&md5=6fb0928f0951b1c9e6a8d5cc1e7ce8de,"With the growing popularity of shared resources, large volumes of complex data of different types are collected automatically. Traditional data mining algorithms generally have problems and challenges including huge memory cost, low processing speed, and inadequate hard disk space. As a fundamental task of data mining, sequential pattern mining (SPM) is used in a wide variety of real-life applications. However, it is more complex and challenging than other pattern mining tasks, i.e., frequent itemset mining and association rule mining, and also suffers from the above challenges when handling the large-scale data. To solve these problems, mining sequential patterns in a parallel or distributed computing environment has emerged as an important issue with many applications. In this article, an in-depth survey of the current status of parallel SPM (PSPM) is investigated and provided, including detailed categorization of traditional serial SPM approaches, and state-of-the art PSPM. We review the related work of PSPM in details including partition-based algorithms for PSPM, apriori-based PSPM, pattern-growth-based PSPM, and hybrid algorithms for PSPM, and provide deep description (i.e., characteristics, advantages, disadvantages, and summarization) of these parallel approaches of PSPM. Some advanced topics for PSPM, including parallel quantitative/weighted/utility SPM, PSPM from uncertain data and stream data, hardware acceleration for PSPM, are further reviewed in details. Besides, we review and provide some well-known open-source software of PSPM. Finally, we summarize some challenges and opportunities of PSPM in the big data era. © 2019 Association for Computing Machinery.",Big data; Data mining; Data science; Parallelism; Sequential pattern,Big data; Computer hardware description languages; Data Science; Distributed computer systems; Open source software; Open systems; Surveys; Distributed computing environment; Frequent itemset mining; Mining sequential patterns; Parallelism; Partition-based algorithms; Problems and challenges; Sequential patterns; Sequential-pattern mining; Data mining
Road network construction with complex intersections based on sparsely sampled private car trajectory data,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069457123&doi=10.1145%2f3326060&partnerID=40&md5=4f2b1d02339be5208f38bf4724e306f3,"A road network is a critical aspect of both urban planning and route recommendation. This article proposes an efficient approach to build a fine-grained road network based on sparsely sampled private car trajectory data under complex urban environment. In order to resolve difficulties introduced by low sampling rate trajectory data, we concentrate sample points around intersections by utilizing the turning characteristics from the large-scale trajectory data to ensure the accuracy of the detection of intersections and road segments. In front of complex road networks including many complex intersections, such as the overpasses and underpasses, we first layer intersections into major and minor one, and then propose a simplified representation of intersections and corresponding computable model based on the features of roads, which can significantly improve the accuracy of detected road networks, especially for the complex intersections. In order to construct fine-grained road networks, we distinguish various types of intersections using direction information and detected turning limit. To the best of our knowledge, our road network building method is the first time to give fine-grained road networks based on low-sampling rate private car trajectory data, especially able to infer the location of complex intersections and its connections to other intersections. Last but not the least, we propose an effective parameter selection process for the Density-Based Spatial Clustering of Applications with Noise based clustering algorithm, which is used to implement the reliable intersection detection. Extensive evaluations are conducted based on a real-world trajectory dataset from 1,345 private cars in Futian district, Shenzhen city of China. The results demonstrate the effectiveness of the proposed method. The constructed road network matches close to the one from a public editing map OpenStreetMap, especially the location of the road intersections and road segments, which achieves 92.2% intersections within 20m and 91.6% road segments within 8m. © 2019 Association for Computing Machinery.",Private cars; Road networks; Trajectory data,Clustering algorithms; Complex networks; Motor transportation; Roads and streets; Trajectories; Complex road networks; Complex urban environments; Density based spatial clustering of applications with noise; Effective parameters; Private cars; Real-world trajectories; Road network; Trajectory data; Traffic control
Active two phase collaborative representation classifier,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068461758&doi=10.1145%2f3326919&partnerID=40&md5=c38a732cd1fc285e349c6d334053c64b,"The Sparse Representation Classifier, the Collaborative Representation Classifier (CRC), and the Two Phase Test Sample Sparse Representation (TPTSSR) classifier were introduced in recent times. All these frameworks are supervised and passive in the sense that they cannot benefit from unlabeled data samples. In this paper, inspired by active learning paradigms, we introduce an active CRC that can be used by these frameworks. More precisely, we are interested in the TPTSSR framework due to its good performance and its reasonable computational cost. Our proposed Active Two Phase Collaborative Representation Classifier (ATPCRC) starts by predicting the label of the available unlabeled samples. At testing stage, two coding processes are carried out separately on the set of originally labeled samples and the whole set (original and predicted label). The two types of class-wise reconstruction errors are blended in order to decide the class of any test image. Experiments conducted on four public image datasets show that the proposed ATPCRC can outperform the classic TPTSSR as well as many state-of-the-art methods that exploit label and unlabeled data samples. © 2019 Association for Computing Machinery.",Active learning; Image categorization; Semi-supervised learning; Two phase collaborative representation classifier,Machine learning; Active Learning; Collaborative representations; Computational costs; Image Categorization; Reconstruction error; Semi- supervised learning; Sparse representation; State-of-the-art methods; Supervised learning
Balancing prediction errors for robust sentiment classification,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069519643&doi=10.1145%2f3328795&partnerID=40&md5=8a24a1d6b4b77fa055d69796ac354f48,"Sentiment classification is a popular text mining task in which textual content (e.g., a message) is assigned a polarity label (typically positive or negative) reflecting the sentiment expressed in it. Sentiment classification is used widely in applications like customer feedback analysis where robustness and correctness of results are critical. In this article, we highlight that prediction accuracy alone is not sufficient for assessing the performance of a sentiment classifier; it is also important that the classifier is not biased toward positive or negative polarity, thus distorting the distribution of positive and negative messages in the predictions. We propose a measure, called Polarity Bias Rate, for quantifying this bias in a sentiment classifier. Second, we present two methods for removing this bias in the predictions of unsupervised and supervised sentiment classifiers. Our first method, called Bias-Aware Thresholding (BAT), shifts the decision boundary to control the bias in the predictions. Motivated from cost-sensitive learning, BAT is easily applicable to both lexicon-based unsupervised and supervised classifiers. Our second method, called Balanced Logistic Regression (BLR) introduces a bias-remover constraint into the standard logistic regression model. BLR is an automatic bias-free supervised sentiment classifier. We evaluate our methods extensively on seven real-world datasets. The experiments involve two lexicon-based and two supervised sentiment classifiers and include evaluation on multiple train-test data sizes. The results show that bias is controlled effectively in predictions. Furthermore, prediction accuracy is also increased in many cases, thus enhancing the robustness of sentiment classification. © 2019 Association for Computing Machinery.",Bias-aware sentiment analysis; Fairness in learning; Lexicon-based methods; Sentiment analysis; Supervised methods,Regression analysis; Sentiment analysis; Cost-sensitive learning; Fairness in learning; Lexicon-based; Logistic Regression modeling; Logistic regressions; Sentiment classification; Supervised classifiers; Supervised methods; Forecasting
Tensorizing restricted Boltzmann machine,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069498242&doi=10.1145%2f3321517&partnerID=40&md5=8e5a4f8883a0f756bb6484836120b24e,"Restricted Boltzmann machine (RBM) is a famous model for feature extraction and can be used as an initializer for neural networks. When applying the classic RBM to multidimensional data such as 2D/3D tensors, one needs to vectorize such as high-order data. Vectorizing will result in dimensional disaster and valuable spatial information loss. As RBM is a model with fully connected layers, it requires a large amount of memory. Therefore, it is difficult to use RBM with high-order data on low-end devices. In this article, to utilize classic RBM on tensorial data directly, we propose a new tensorial RBM model parameterized by the tensor train format (TTRBM). In this model, both visible and hidden variables are in tensorial form, which are connected by a parameter matrix in tensor train format. The biggest advantage of the proposed model is that TTRBM can obtain comparable performance compared with the classic RBM with much fewer model parameters and faster training process. To demonstrate the advantages of TTRBM, we conduct three real-world applications, face reconstruction, handwritten digit recognition, and image super-resolution in the experiments. © 2019 Association for Computing Machinery.",Restricted boltzmann machine; Tensor; Tensor train format,Character recognition; Dimensional disaster; Face reconstruction; Handwritten digit recognition; Image super resolutions; Multidimensional data; Restricted boltzmann machine; Spatial informations; Tensor trains; Tensors
Time-sync video tag extraction using semantic association graph,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068445496&doi=10.1145%2f3332932&partnerID=40&md5=526038d95722cac2c366e898d680aded,"Time-sync comments (TSCs) reveal a new way of extracting the online video tags. However, such TSCs have lots of noises due to users’ diverse comments, introducing great challenges for accurate and fast video tag extractions. In this article, we propose an unsupervised video tag extraction algorithm named Semantic Weight-Inverse Document Frequency (SW-IDF). Specifically, we first generate corresponding semantic association graph (SAG) using semantic similarities and timestamps of the TSCs. Second, we propose two graph cluster algorithms, i.e., dialogue-based algorithm and topic center-based algorithm, to deal with the videos with different density of comments. Third, we design a graph iteration algorithm to assign the weight to each comment based on the degrees of the clustered subgraphs, which can differentiate the meaningful comments from the noises. Finally, we gain the weight of each word by combining Semantic Weight (SW) and Inverse Document Frequency (IDF). In this way, the video tags are extracted automatically in an unsupervised way. Extensive experiments have shown that SW-IDF (dialogue-based algorithm) achieves 0.4210 F1-score and 0.4932 MAP (Mean Average Precision) in high-density comments, 0.4267 F1-score and 0.3623 MAP in low-density comments; while SW-IDF (topic center-based algorithm) achieves 0.4444 F1-score and 0.5122 MAP in high-density comments, 0.4207 F1-score and 0.3522 MAP in low-density comments. It has a better performance than the state-of-the-art unsupervised algorithms in both F1-score and MAP. © 2019 Association for Computing Machinery.",Extraction,Extraction; Iterative methods; Semantics; Cluster algorithms; Different densities; Inverse Document Frequency; Iteration algorithms; Semantic associations; Semantic similarity; State of the art; Unsupervised algorithms; Clustering algorithms
Robust regression via heuristic corruption thresholding and its adaptive estimation variation,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069477872&doi=10.1145%2f3314105&partnerID=40&md5=fa754def2cf0220eb418678973c25915,"The presence of data noise and corruptions has recently invoked increasing attention on robust least-squares regression (RLSR), which addresses this fundamental problem that learns reliable regression coefficients when response variables can be arbitrarily corrupted. Until now, the following important challenges could not be handled concurrently: (1) rigorous recovery guarantee of regression coefficients, (2) difficulty in estimating the corruption ratio parameter, and (3) scaling to massive datasets. This article proposes a novel Robust regression algorithm via Heuristic Corruption Thresholding (RHCT) that concurrently addresses all the above challenges. Specifically, the algorithm alternately optimizes the regression coefficients and estimates the optimal uncorrupted set via heuristic thresholding without a pre-defined corruption ratio parameter until its convergence. Moreover, to improve the efficiency of corruption estimation in large-scale data, a Robust regression algorithm via Adaptive Corruption Thresholding (RACT) is proposed to determine the size of the uncorrupted set in a novel adaptive search method without iterating data samples exhaustively. In addition, we prove that our algorithms benefit from strong guarantees analogous to those of state-of-the-art methods in terms of convergence rates and recovery guarantees. Extensive experiments demonstrate that the effectiveness of our new methods is superior to that of existing methods in the recovery of both regression coefficients and uncorrupted sets, with very competitive efficiency. © 2019 Association for Computing Machinery.",Adaptive search; Adversarial data corruption; Discrete optimization; Hard thresholding; Robust regression,Crime; Efficiency; Optimization; Recovery; Regression analysis; Adaptive search; Data corruption; Discrete optimization; Hard thresholding; Robust regressions; Parameter estimation
Feature selection via transferring knowledge across different classes,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074814907&doi=10.1145%2f3314202&partnerID=40&md5=2a9a29e4acbaca38ac05740038e52652,"The problem of feature selection has attracted considerable research interest in recent years. Supervised information is capable of significantly improving the quality of selected features. However, existing supervised feature selection methods all require that classes in the labeled data (source domain) and unlabeled data (target domain) to be identical, which may be too restrictive in many cases. In this article, we consider a more challenging cross-class setting where the classes in these two domains are related but different, which has rarely been studied before. We propose a cross-class knowledge transfer feature selection framework which transfers the cross-class knowledge from the source domain to guide target domain feature selection. Specifically, high-level descriptions, i.e., attributes, are used as the bridge for knowledge transfer. To further improve the quality of the selected features, our framework jointly considers the tasks of cross-class knowledge transfer and feature selection. Experimental results on four benchmark datasets demonstrate the superiority of the proposed method. © 2019 Association for Computing Machinery.",Dimension reduction; Feature selection; Supervision transfer,Knowledge management; Benchmark datasets; Dimension reduction; Feature selection methods; High level description; Knowledge transfer; Research interests; Selection framework; Supervision transfer; Feature extraction
Leveraging label-specific discriminant mapping features for multi-label learning,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065715619&doi=10.1145%2f3319911&partnerID=40&md5=30846f2e5fddf57451872f727a293375,"As an important machine learning task, multi-label learning deals with the problem where each sample instance (feature vector) is associated with multiple labels simultaneously. Most existing approaches focus on manipulating the label space, such as exploiting correlations between labels and reducing label space dimension, with identical feature space in the process of classification. One potential drawback of this traditional strategy is that each label might have its own specific characteristics and using identical features for all label cannot lead to optimized performance. In this article, we propose an effective algorithm named LSDM, i.e., leveraging label-specific discriminant mapping features for multi-label learning, to overcome the drawback. LSDM sets diverse ratio parameter values to conduct cluster analysis on the positive and negative instances of identical label. It reconstructs label-specific feature space which includes distance information and spatial topology information. Our experimental results show that combining these two parts of information in the new feature representation can better exploit the clustering results in the learning process. Due to the problem of diverse combinations for identical label, we employ simplified linear discriminant analysis to efficiently excavate optimal one for each label and perform classification by querying the corresponding results. Comparison with the state-of-the-art algorithms on a total of 20 benchmark datasets clearly manifests the competitiveness of LSDM. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Label specific features; Machine learning; Multi-label learning,Cluster analysis; Clustering algorithms; Discriminant analysis; Learning systems; Machine learning; Mapping; Distance information; Effective algorithms; Feature representation; Linear discriminant analysis; Multi-label learning; Negative instances; Optimized performance; State-of-the-art algorithms; Learning algorithms
Near-optimal and practical algorithms for graph scan statistics with connectivity constraints,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065725151&doi=10.1145%2f3309712&partnerID=40&md5=2b8eafa1491a68393513d17c3d5a9b73,"One fundamental task in network analysis is detecting “hotspots” or “anomalies” in the network; that is, detecting subgraphs where there is significantly more activity than one would expect given historical data or some baseline process. Scan statistics is one popular approach used for anomalous subgraph detection. This methodology involves maximizing a score function over all connected subgraphs, which is a challenging computational problem. A number of heuristics have been proposed for these problems, but they do not provide any quality guarantees. Here, we propose a framework for designing algorithms for optimizing a large class of scan statistics for networks, subject to connectivity constraints. Our algorithms run in time that scales linearly on the size of the graph and depends on a parameter we call the “effective solution size,” while providing rigorous approximation guarantees. In contrast, most prior methods have super-linear running times in terms of graph size. Extensive empirical evidence demonstrates the effectiveness and efficiency of our proposed algorithms in comparison with state-of-the-art methods. Our approach improves on the performance relative to all prior methods, giving up to over 25% increase in the score. Further, our algorithms scale to networks with up to a million nodes, which is 1-2 orders of magnitude larger than all prior applications. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Anomalous subgraph detection; Graph anomaly detection; Parameterized complexity; Scan statistics,Approximation algorithms; Optimization; Computational problem; Connectivity constraints; Effectiveness and efficiencies; In-network analysis; Parameterized complexity; Scan statistics; State-of-the-art methods; Subgraphs; Anomaly detection
Translations diversification for expert finding: A novel clustering-based approach,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069446127&doi=10.1145%2f3320489&partnerID=40&md5=129f60f8bb9085b04da5fda4c70833f8,"Expert finding is the task of retrieving and ranking knowledgeable people in the subject of user’s query. It is a well-studied problem that has attracted the attention of many researchers. The most important challenge in expert finding is to determine the similarity between query words and documents authored by candidate experts. One of the most important challenges in Information Retrieval (IR) community is the issue of vocabulary gap between queries and documents. In this study, a translation model based on words clustering in two query and co-occurrence spaces is proposed to overcome this problem. First, the words that are semantically close, are clustered in a query space and then each cluster in this space are clustered again in a co-occurrence space. Representatives of each cluster in the co-occurrence space are considered as a diverse subset of the parent cluster. By this method, the query translations are expected to be diversified in the query space. Next, a probabilistic model, that is based on the belonging degree of word to cluster and similarity of cluster to query in the query space, is used to consider the problem of vocabulary gap. Finally, the corresponding translations to each query are used in conjunction with a combination model for expert finding. Experiments on Stack Overflow dataset show the effectiveness of the proposed method for expert finding. © 2019 Association for Computing Machinery.",Data clustering; Expert finding; Stack overflow; Translation diversification; Translation model,Combination models; Data clustering; Expert finding; Novel clustering; Probabilistic modeling; Query translations; Stack overflow; Translation models; Clustering algorithms
Probabilistic feature selection and classification vector machine,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065804414&doi=10.1145%2f3309541&partnerID=40&md5=75d11a99da49b6a50803559c6d7de3d0,"Sparse Bayesian learning is a state-of-the-art supervised learning algorithm that can choose a subset of relevant samples from the input data and make reliable probabilistic predictions. However, in the presence of high-dimensional data with irrelevant features, traditional sparse Bayesian classifiers suffer from performance degradation and low efficiency due to the incapability of eliminating irrelevant features. To tackle this problem, we propose a novel sparse Bayesian embedded feature selection algorithm that adopts truncated Gaussian distributions as both sample and feature priors. The proposed algorithm, called probabilistic feature selection and classification vector machine (PFCVMLP) is able to simultaneously select relevant features and samples for classification tasks. In order to derive the analytical solutions, Laplace approximation is applied to compute approximate posteriors and marginal likelihoods. Finally, parameters and hyperparameters are optimized by the type-II maximum likelihood method. Experiments on three datasets validate the performance of PFCVMLP along two dimensions: classification performance and effectiveness for feature selection. Finally, we analyze the generalization performance and derive a generalization error bound for PFCVMLP. By tightening the bound, the importance of feature selection is demonstrated. © 2019 Association for Computing Machinery.",EEG emotion recognition; Feature selection; Probabilistic classification model; Sparse Bayesian learning; Supervised learning,Clustering algorithms; Data mining; Feature extraction; Learning algorithms; Machine learning; Maximum likelihood; Supervised learning; Embedded feature selections; Emotion recognition; Feature selection and classification; Generalization error bounds; Probabilistic classification models; Sparse Bayesian classifiers; Sparse Bayesian learning; Type ii maximum likelihoods; Classification (of information)
Leveraging kernel-incorporated matrix factorization for APP recommendation,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068962124&doi=10.1145%2f3320482&partnerID=40&md5=5c2fda34920241f09bfe849b110f35ad,"The ever-increasing number of smartphone applications (apps) available on different app markets poses a challenge for personalized app recommendation. Conventional collaborative filtering-based recommendation methods suffer from sparse and binary user-app implicit feedback, which results in poor performance in discriminating user-app preferences. In this article, we first propose two kernel incorporated probabilistic matrix factorization models, which introduce app-categorical information to constrain the user and app latent features to be similar to their neighbors in the latent space. The two models are solved by Stochastic Gradient Descent with a user-oriented negative sampling scheme. To further improve the recommendation performance, we construct pseudo user-app ratings based on user-app usage information, and propose a novel kernelized non-negative matrix factorization by incorporating non-negative constraints on latent factors to predict user-app preferences. This model also leverages user–user and app–app similarities with regard to app-categorical information to mine the latent geometric structure in the pseudo-rating space. Adopting the Karush–Kuhn–Tucker conditions, a Multiplicative Updating Rules based optimization is proposed for model learning, and the convergence is proved by introducing an auxiliary function. The experimental results on a real user-app installation usage dataset show the comparable performance of our models with the state-of-the-art baselines in terms of two ranking-oriented evaluation metrics. © 2019 Association for Computing Machinery.",App recommendation; Kernel function; Non-negative matrix factorization; Probabilistic matrix factorization,Collaborative filtering; Gradient methods; Matrix algebra; Stochastic models; Stochastic systems; Kernel function; Matrix factorizations; Nonnegative matrix factorization; Probabilistic matrix factorizations; Recommendation methods; Recommendation performance; Smart-phone applications; Stochastic gradient descent; Factorization
Multi-task crowdsourcing via an optimization framework,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069464520&doi=10.1145%2f3310227&partnerID=40&md5=0d10672cbb30105ebf00dbab0e865596,"The unprecedented amounts of data have catalyzed the trend of combining human insights with machine learning techniques, which facilitate the use of crowdsourcing to enlist label information both effectively and efficiently. One crucial challenge in crowdsourcing is the diverse worker quality, which determines the accuracy of the label information provided by such workers. Motivated by the observations that same set of tasks are typically labeled by the same set of workers, we studied their behaviors across multiple related tasks and proposed an optimization framework for learning from task and worker dual heterogeneity. The proposed method uses a weight tensor to represent the workers’ behaviors across multiple tasks, and seeks to find the optimal solution of the tensor by exploiting its structured information. Then, we propose an iterative algorithm to solve the optimization problem and analyze its computational complexity. To infer the true label of an example, we construct a worker ensemble based on the estimated tensor, whose decisions will be weighted using a set of entropy weight. We also prove that the gradient of the most time-consuming updating block is separable with respect to the workers, which leads to a randomized algorithm with faster speed. Moreover, we extend the learning framework to accommodate to the multi-class setting. Finally, we test the performance of our framework on several datasets, and demonstrate its superiority over state-of-the-art techniques. © 2019 Association for Computing Machinery.",Crowdsourcing; Entropy ensemble; Multi-task learning; Optimization; Tensor representation,Crowdsourcing; Entropy; Iterative methods; Optimization; Tensors; Machine learning techniques; Multitask learning; Optimization framework; Optimization problems; Randomized Algorithms; State-of-the-art techniques; Structured information; Tensor representation; Learning systems
Variant grassmann manifolds: A representation augmentation method for action recognition,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065721247&doi=10.1145%2f3314203&partnerID=40&md5=a75f4a5cc41c8298d761417f1b78f63a,"In classification tasks, classifiers trained with finite examples might generalize poorly to new data with unknown variance. For this issue, data augmentation is a successful solution where numerous artificial examples are added to training sets. In this article, we focus on the data augmentation for improving the accuracy of action recognition, where action videos are modeled by linear dynamical systems and approximately represented as linear subspaces. These subspace representations lie in a non-Euclidean space, named Grassmann manifold, containing points as orthonormal matrixes. It is our concern that poor generalization may result from the variance of manifolds when data come from different sources or classes. Thus, we introduce infinitely many variant Grassmann manifolds (VGM) subject to a known distribution, then represent each action video as different Grassmann points leading to augmented representations. Furthermore, a prior based on the stability of subspace bases is introduced, so the manifold distribution can be adaptively determined, balancing discrimination and representation. Experimental results of multi-class and multi-source classification show that VGM softmax classifiers achieve lower test error rates compared to methods with a single manifold. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Action recognition; Data augmentation; Grassmann manifold; Representation learning,Dynamical systems; Action recognition; Augmentation methods; Data augmentation; Grassmann manifold; Linear dynamical systems; Non-Euclidean spaces; Representation learning; Subspace representation; Linear control systems
Fast approximate score computation on large-scale distributed data for learning multinomial Bayesian networks,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063238795&doi=10.1145%2f3301304&partnerID=40&md5=1131689c01f154a89503223f2b0c13ed,"In this article, we focus on the problem of learning a Bayesian network over distributed data stored in a commodity cluster. Specifically, we address the challenge of computing the scoring function over distributed data in an efficient and scalable manner, which is a fundamental task during learning.While exact score computation can be done using the MapReduce-style computation, our goal is to compute approximate scores much faster with probabilistic error bounds and in a scalable manner. We propose a novel approach, which is designed to achieve the following: (a) decentralized score computation using the principle of gossiping; (b) lower resource consumption via a probabilistic approach for maintaining scores using the properties of a Markov chain; and (c) effective distribution of tasks during score computation (on large datasets) by synergistically combining well-known hashing techniques.We conduct theoretical analysis of our approach in terms of convergence speed of the statistics required for score computation, and memory and network bandwidth consumption.We also discuss how our approach is capable of efficiently recomputing scores when new data are available.We conducted a comprehensive evaluation of our approach and compared with the MapReducestyle computation using datasets of different characteristics on a 16-node cluster. When theMapReduce-style computation provided exact statistics for score computation, it was nearly 10 times slower than our approach. Although it ran faster on randomly sampled datasets than on the entire datasets, it performed worse than our approach in terms of accuracy. Our approach achieved high accuracy (below 6% average relative error) in estimating the statistics for approximate score computation on all the tested datasets. In conclusion, it provides a feasible tradeoff between computation time and accuracy for fast approximate score computation on large-scale distributed data. © 2019 Association for Computing Machinery.",Approximate score computation; Bayesian networks; Distributed data; Gossip algorithms; Structure learning,Bayesian networks; Computational efficiency; Error analysis; Error statistics; Large dataset; Markov processes; Petroleum reservoir evaluation; Probability distributions; Average relative error; Comprehensive evaluation; Distributed data; Effective distribution; Gossip algorithms; Probabilistic approaches; Probabilistic error bounds; Structure-learning; Distributed computer systems
Performance bounds of decentralized search in expert networks forquery answering,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063237276&doi=10.1145%2f3300230&partnerID=40&md5=d08ac6bd72577b590216bb2168a3f2de,"Expert networks are formed by a group of expert-professionals with different specialties to collaboratively resolve specific queries posted to the network. In such networks, when a query reaches an expert who does not have sufficient expertise, this query needs to be routed to other experts for further processing until it is completely solved; therefore, query answering efficiency is sensitive to the underlying query routing mechanism being used. Among all possible query routing mechanisms, decentralized search, operating purely on each expert's local information without any knowledge of network global structure, represents the most basic and scalable routing mechanism, which is applicable to any network scenarios even in dynamic networks. However, there is still a lack of fundamental understanding of the efficiency of decentralized search in expert networks. In this regard, we investigate decentralized search by quantifying its performance under a variety of network settings. Our key findings reveal the existence of network conditions, under which decentralized search can achieve significantly short query routing paths (i.e., between O(logn) and O(log2 n) hops, n: total number of experts in the network). Based on such theoretical foundation, we further study how the unique properties of decentralized search in expert networks are related to the anecdotal small-world phenomenon. In addition, we demonstrate that decentralized search is robust against estimation errors introduced by misinterpreting the required expertise levels. The developed performance bounds, confirmed by real datasets, are able to assist in predicting network performance and designing complex expert networks. © 2019 Association for Computing Machinery.",Decentralized search; Expert networks; Performance bounds; Query answering; Theory,Efficiency; Decentralized searches; Expert networks; Performance bounds; Query answering; Theory; Network routing
DWE-Med: Dynamic word embeddings for medical domain,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063199923&doi=10.1145%2f3310254&partnerID=40&md5=99ec1f8dc24d9229cb99735823c9455b,"Recent advances in unsupervised language processing methods have created an opportunity to exploit massive text corpora for developing high-quality vector space representation (also known as word embeddings) of words. Towards this direction, practitioners have developed and applied several data driven embedding models with quite good rate of success. However, a drawback of these models lies in their premise of static context; wherein, the meaning of a word is assumed to remain the same over the period of time. This is limiting because it is known that the semantic meaning of a concept evolves over time. While such semantic drifts are routinely observed in almost all the domains; their effect is acute in domain such as biomedicine, where the semantic meaning of a concept changes relatively fast. To address this, in this study, we aim to learn temporally aware vector representation of medical concepts from the timestamped text data, and in doing so provide a systematic approach to formalize the problem. More specifically, a dynamic word embedding based model that jointly learns the temporal characteristics of medical concepts and performs across time-alignment is proposed. Apart from capturing the evolutionary characteristics in an optimal manner, the model also factors in the implicit medical properties useful for a variety of bio-medical applications. Empirical studies conducted on two important bio-medical use cases validates the effectiveness of the proposed approach and suggests that the model not only learns quality embeddings but also facilitates intuitive trajectory visualizations. © 2019 Copyright held by the owner/author(s).",Biomedical domain; Temporal dynamics; Word embeddings,Medical applications; Semantics; Vector spaces; Biomedical applications; Biomedical domain; Empirical studies; Language processing; Medical concepts; Temporal characteristics; Temporal dynamics; Vector representations; Embeddings
Outcome-oriented predictive process monitoring: Review and benchmark,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063188465&doi=10.1145%2f3301300&partnerID=40&md5=cb3ef2e0b1dc7ceb84aafc83ae77f9e9,"Predictive business process monitoring refers to the act of making predictions about the future state of ongoing cases of a business process, based on their incomplete execution traces and logs of historical (completed) traces. Motivated by the increasingly pervasive availability of fine-grained event data about business process executions, the problem of predictive process monitoring has received substantial attention in the past years. In particular, a considerable number of methods have been put forward to address the problem of outcomeoriented predictive process monitoring, which refers to classifying each ongoing case of a process according to a given set of possible categorical outcomes-e.g., Will the customer complain or not? Will an order be delivered, canceled, or withdrawn? Unfortunately, different authors have used different datasets, experimental settings, evaluation measures, and baselines to assess their proposals, resulting in poor comparability and an unclear picture of the relative merits and applicability of different methods. To address this gap, this article presents a systematic review and taxonomy of outcome-oriented predictive process monitoring methods, and a comparative experimental evaluation of eleven representative methods using a benchmark covering 24 predictive process monitoring tasks based on nine real-life event logs. © 2019 Copyright held by the owner/author(s).",Business process; Predictive monitoring; Sequence classification,Process control; Process monitoring; Business Process; Business process execution; Business process monitoring; Evaluation measures; Experimental evaluation; Monitoring methods; Predictive monitoring; Sequence classification; Monitoring
Rumor blocking through online link deletion on social networks,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063226682&doi=10.1145%2f3301302&partnerID=40&md5=aea787586767eb3929b708265da5e78b,"In recent years, social networks have become important platforms for people to disseminate information. However, we need to take effective measures such as blocking a set of links to control the negative rumors spreading over the network. In this article, we propose a Rumor Spread Minimization (RSM) problem, i.e., we remove an edge set from network such that the rumor spread is minimized.We first prove the objective function of RSM problem is not submodular. Then, we propose both submodular lower-bound and upper-bound of the objective function. Next, we develop a heuristic algorithm to approximate the objective function. Furthermore, we reformulate our objective function as the DS function (the Difference of Submodular functions). Finally, we conduct experiments on real-world datasets to evaluate our proposed method. The experiment results show that the upper and lower bounds are very close, which indicates the good quality of them. And, the proposed method outperforms the comparison methods. © 2019 Association for Computing Machinery.",Approximation algorithm; Nonsubmodularity; Rumor blocking; Social network,Approximation algorithms; Heuristic algorithms; Information dissemination; Comparison methods; Effective measures; Nonsubmodularity; Objective functions; Real-world datasets; Rumor blocking; Submodular functions; Upper and lower bounds; Social networking (online)
Taxonomy and evaluation for microblog popularity prediction,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063196848&doi=10.1145%2f3301303&partnerID=40&md5=28a57e34892cef82afd14109ea45fa15,"As social networks become amajor source of information, predicting the outcome of information diffusion has appeared intriguing to both researchers and practitioners. By organizing and categorizing the joint efforts of numerous studies on popularity prediction, this article presents a hierarchical taxonomy and helps to establish a systematic overview of popularity prediction methods for microblog. Specifically, we uncover three lines of thoughts: the feature-based approach, time-series modelling, and the collaborative filtering approach and analyse them, respectively. Furthermore, we also categorize prediction methods based on their underlying rationale: whether they attempt to model the motivation of users or monitor the early responses. Finally, we put these prediction methods to test by performing experiments on real-life data collected from popular social networks Twitter and Weibo. We compare the methods in terms of accuracy, efficiency, timeliness, robustness, and bias. As far as we are concerned, there is no precedented survey aimed at microblog popularity prediction at the time of submission. By establishing a taxonomy and evaluation for the first time, we hope to provide an in-depth review of state-of-the-art prediction methods and point out directions for further research. Our evaluations show that time-series modelling has the advantage of high accuracy and the ability to improve over time. The feature-based methods using only temporal features performs nearly as well as using all possible features, producing average results. This suggests that temporal features do have strong predictive power and that power is better exploited with time-series models. On the other hand, this implies that we know little about the future popularity of an item before it is posted, which may be the focus of further research. © 2019 Association for Computing Machinery.",Evaluation; Popularity prediction; Social network; Taxonomy,Collaborative filtering; Social networking (online); Taxonomies; Time series; Evaluation; Feature based approaches; Feature-based method; Hierarchical taxonomy; Information diffusion; Popularity predictions; Time series models; Time-series modelling; Forecasting
Information diffusion prediction with network regularized role-based user representation learning,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065783057&doi=10.1145%2f3314106&partnerID=40&md5=10ecbd421416f95a58b64861934781c4,"In this article, we aim at developing a user representation learning model to solve the information diffusion prediction problem in social media. The main idea is to project the diffusion users into a continuous latent space as the role-based (sender and receiver) representations, which capture unique diffusion characteristics of users. The model learns the role-based representations based on a cascade modeling objective that aims at maximizing the likelihood of observed cascades, and employs the matrix factorization objective of reconstructing structural proximities as a regularization on representations. By jointly embedding the information of cascades and network, the learned representations are robust on different diffusion data. We evaluate the proposed model on three real-world datasets. The experimental results demonstrate the better performance of the proposed model than state-of-the-art diffusion embedding and network embedding models and other popular graph-based methods. © 2019 Association for Computing Machinery.",Diffusion role; Information diffusion; Network regularization; Representation learning,Factorization; Graphic methods; Diffusion characteristics; Graph-based methods; Information diffusion; Matrix factorizations; Prediction problem; Real-world datasets; Representation learning; Sender and receivers; Embeddings
Chameleon 2: An improved graph-based clustering algorithm,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061216552&doi=10.1145%2f3299876&partnerID=40&md5=9e6f3a37822358b81f5712a1566720d9,"Traditional clustering algorithms fail to produce human-like results when confronted with data of variable density, complex distributions, or in the presence of noise. We propose an improved graph-based clustering algorithm called Chameleon 2, which overcomes several drawbacks of state-of-the-art clustering approaches. We modified the internal cluster quality measure and added an extra step to ensure algorithm robustness. Our results reveal a significant positive impact on the clustering quality measured by Normalized Mutual Information on 32 artificial datasets used in the clustering literature. This significant improvement is also confirmed on real-world datasets. The performance of clustering algorithms such as DBSCAN is extremely parameter sensitive, and exhaustive manual parameter tuning is necessary to obtain a meaningful result. All hierarchical clustering methods are very sensitive to cutoff selection, and a human expert is often required to find the true cutoff for each clustering result. We present an automated cutoff selection method that enables the Chameleon 2 algorithm to generate high-quality clustering in autonomous mode. © 2019 Copyright held by the owner/author(s).",Cluster analysis; clustering; graph clustering; pattern recognition,Cluster analysis; Graphic methods; Pattern recognition; clustering; Clustering approach; Graph clustering; Graph-based clustering; Hierarchical clustering methods; Normalized mutual information; Real-world datasets; Traditional clustering; Clustering algorithms
Characterizing directed and undirected networks via multidimensional walks with jumps,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061204329&doi=10.1145%2f3299877&partnerID=40&md5=f0a5b147c1b0218537f85060507b26e2,"Estimating distributions of node characteristics (labels) such as number of connections or citizenship of users in a social network via edge and node sampling is a vital part of the study of complex networks. Due to its low cost, sampling via a random walk (RW) has been proposed as an attractive solution to this task. Most RW methods assume either that the network is undirected or that walkers can traverse edges regardless of their direction. Some RW methods have been designed for directed networks where edges coming into a node are not directly observable. In this work, we propose Directed Unbiased Frontier Sampling (DUFS), a sampling method based on a large number of coordinated walkers, each starting from a node chosen uniformly at random. It applies to directed networks with invisible incoming edges because it constructs, in real time, an undirected graph consistent with the walkers trajectories, and its use of random jumps to prevent walkers from being trapped. DUFS generalizes previous RW methods and is suited for undirected networks and to directed networks regardless of in-edge visibility. We also propose an improved estimator of node label distribution that combines information from initial walker locations with subsequent RW observations. We evaluate DUFS, compare it to other RWmethods, investigate the impact of its parameters on estimation accuracy and provide practical guidelines for choosing them. In estimating out-degree distributions, DUFS yields significantly better estimates of the head of the distribution than other methods, while matching or exceeding estimation accuracy of the tail. Last, we show that DUFS outperforms uniform sampling when estimating distributions of node labels of the top 10% largest degree nodes, even when sampling a node uniformly has the same cost as RW steps. © 2019 Association for Computing Machinery.",Complex networks; directed networks; graph sampling; random walks,Directed graphs; Random processes; Attractive solutions; Degree distributions; Directed network; Graph samplings; Label distribution; Practical guidelines; Random Walk; Undirected network; Complex networks
Robust spectral ensemble clustering via rank minimization,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061203496&doi=10.1145%2f3278606&partnerID=40&md5=a74e7537f1f373ace417588c85c81b26,"Ensemble Clustering (EC) is an important topic for data cluster analysis. It targets to integrate multiple Basic Partitions (BPs) of a particular dataset into a consensus partition. Among previous works, one promising and effective way is to transform EC as a graph partitioning problem on the co-association matrix, which is a pair-wise similarity matrix summarized by all the BPs in essence. However, most existing EC methods directly utilize the co-association matrix, yet without considering various noises (e.g., the disagreement between different BPs and the outliers) that may exist in it. These noises can impair the cluster structure of a co-association matrix, and thus mislead the final graph partitioning process. To address this challenge, we propose a novel Robust Spectral Ensemble Clustering (RSEC) algorithm in this article. Specifically, we learn low-rank representation (LRR) for the co-association matrix to uncover its cluster structure and handle the noises, and meanwhile, we perform spectral clustering with the learned representation to seek for a consensus partition. These two steps are jointly proceeded within a unified optimization framework. In particular, during the optimizing process, we leverage consensus partition to iteratively enhance the blockdiagonal structure of LRR, in order to assist the graph partitioning. To solve RSEC, we first formulate it by using nuclear norm as a convex proxy to the rank function. Then, motivated by the recent advances in non-convex rank minimization, we further develop a non-convex model for RSEC and provide it a solution by the majorization-minimization Augmented Lagrange Multiplier algorithm. Experiments on 18 real-world datasets demonstrate the effectiveness of our algorithm compared with state-of-the-art methods. Moreover, several impact factors on the clustering performance of our approach are also explored extensively. © 2019 Association for Computing Machinery.",co-association matrix; Ensemble clustering; low-rank representation; non-convex relaxation; spectral clustering,Cluster analysis; Graph theory; Iterative methods; Lagrange multipliers; Optimization; Relaxation processes; Co-association matrix; Convex relaxation; Ensemble clustering; Low-rank representations; Spectral clustering; Clustering algorithms
The relationship between online social network ties and user attributes,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065756064&doi=10.1145%2f3314204&partnerID=40&md5=ced0f944c6feb21c1a5b64ecfb8f1bb7,"The distance between users has an effect on the formation of social network ties, but it is not the only or even the main factor. Knowing all the features that influence such ties is very important for many related domains such as location-based recommender systems and community and event detection systems for online social networks (OSNs). In recent years, researchers have analyzed the role of user geo-location in OSNs. Researchers have also attempted to determine the probability of friendships being established based on distance, where friendship is not only a function of distance. However, some important features of OSNs remain unknown. In order to comprehensively understand the OSN phenomenon, we also need to analyze users' attributes. Basically, an OSN functions according to four main user properties: user geo-location, user weight, number of user interactions, and user lifespan. The research presented here sought to determine whether the user mobility pattern can be used to predict users' interaction behavior. It also investigated whether, in addition to distance, the number of friends (known as user weight) interferes in social network tie formation. To this end, we analyzed the above-stated features in three large-scale OSNs. We found that regardless of a high degree freedom in user mobility, the fraction of the number of outside activities over the inside activity is a significant fraction that helps us to address the user interaction behavior. To the best of our knowledge, research has not been conducted elsewhere on this issue. We also present a high-resolution formula in order to improve the friendship probability function. © 2019 Association for Computing Machinery.",Distance; IO-fraction; Online social network; Tie formation; User weight,Behavioral research; Location; Online systems; Distance; IO-fraction; On-line social networks; Tie formation; User weight; Social networking (online)
Large scale online multiple kernel regression with application to time-series prediction,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061190440&doi=10.1145%2f3299875&partnerID=40&md5=7cfe4c7cb479527f3328399205400a75,"Kernel-based regression represents an important family of learning techniques for solving challenging regression tasks with non-linear patterns. Despite being studied extensively, most of the existing work suffers from two major drawbacks as follows: (i) they are often designed for solving regression tasks in a batch learning setting, making them not only computationally inefficient and but also poorly scalable in real-world applications where data arrives sequentially; and (ii) they usually assume that a fixed kernel function is given prior to the learning task, which could result in poor performance if the chosen kernel is inappropriate. To overcome these drawbacks, this work presents a novel scheme of Online Multiple Kernel Regression (OMKR), which sequentially learns the kernel-based regressor in an online and scalable fashion, and dynamically explore a pool of multiple diverse kernels to avoid suffering from a single fixed poor kernel so as to remedy the drawback of manual/heuristic kernel selection. The OMKR problem is more challenging than regular kernelbased regression tasks since we have to on-the-fly determine both the optimal kernel-based regressor for each individual kernel and the best combination of the multiple kernel regressors. We propose a family of OMKR algorithms for regression and discuss their application to time series prediction tasks including application to AR, ARMA, and ARIMA time series.We develop novel approaches to make OMKR scalable for large datasets, to counter the problems arising from an unbounded number of support vectors.We also explore the effect of kernel combination at prediction level and at the representation level. Finally, we conduct extensive experiments to evaluate the empirical performance on both real-world regression and times series prediction tasks. © 2019 Association for Computing Machinery.",large-scale kernel learning; multiple kernel regression; Online learning; time-series prediction,Forecasting; Large dataset; Time series; Empirical performance; Kernel based regression; Kernel learning; Kernel-based regressions; Learning techniques; Multiple kernels; Online learning; Time series prediction; Regression analysis
Differentiating regularization weights - A simple mechanism to alleviate cold start in recommender systems,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061175486&doi=10.1145%2f3285954&partnerID=40&md5=dec5c0196b3c3d28e5ee1f7f8c775e82,"Matrix factorization (MF) and its extended methodologies have been studied extensively in the community of recommender systems in the last decade. Essentially, MF attempts to search for low-ranked matrices that can (1) best approximate the known rating scores, and (2) maintain low Frobenius norm for the low-ranked matrices to prevent overfitting. Since the two objectives conflict with each other, the common practice is to assign the relative importance weights as the hyper-parameters to these objectives. The two low-ranked matrices returned by MF are often interpreted as the latent factors of a user and the latent factors of an item thatwould affect the rating of the user on the item.As a result, it is typical that, in the loss function,we assign a regularization weight λp on the norms of the latent factors for all users, and another regularization weight λq on the norms of the latent factors for all the items.We argue that such amethodology probably over-simplifies the scenario. Alternatively, we probably should assign lower constraints to the latent factors associated with the items or users that reveal more information, and set higher constraints to the others. In this article, we systematically study this topic. We found that such a simple technique can improve the prediction results of theMF-based approaches based on several public datasets. Specifically, we applied the proposed methodology on three baseline models - SVD, SVD++, and the NMF models. We found that this technique improves the prediction accuracy for all these baseline models. Perhaps more importantly, this technique better predicts the ratings on the long-tail items, i.e., the items that were rated/viewed/purchased by few users. This suggests that this approach may partially remedy the cold-start issue. The proposed method is very general and can be easily applied on various recommendation models, such as FactorizationMachines, Field-aware Factorization Machines, Factorizing Personalized Markov Chains, Prod2Vec, Behavior2Vec, and so on.We release the code for reproducibility.We implemented a Python package that integrates the proposed regularization technique with the SVD, SVD++, and the NMF model. © 2019 Copyright held by the owner/author(s).",cold start; collaborative filtering; long tail; matrix factorization; Recommender systems; SVD; SVD++,Collaborative filtering; Factorization; Markov processes; Recommender systems; Singular value decomposition; Cold start; Factorization machines; Hyper-parameter; Importance weights; Long tail; Matrix factorizations; Prediction accuracy; Regularization technique; Matrix algebra
Sequential feature explanations for anomaly detection,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061200950&doi=10.1145%2f3230666&partnerID=40&md5=381a65fb57cdf6c57c86bf0850c44ac9,"In many applications, an anomaly detection system presents the most anomalous data instance to a human analyst, who then must determine whether the instance is truly of interest (e.g., a threat in a security setting). Unfortunately, most anomaly detectors provide no explanation about why an instance was considered anomalous, leaving the analyst with no guidance about where to begin the investigation. To address this issue, we study the problems of computing and evaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE of an anomaly is a sequence of features, which are presented to the analyst one at a time (in order) until the information contained in the highlighted features is enough for the analyst to make a confident judgement about the anomaly. Since analyst effort is related to the amount of information that they consider in an investigation, an explanation's quality is related to the number of features that must be revealed to attain confidence. In this article, we first formulate the problem of optimizing SFEs for a particular density-based anomaly detector. We then present both greedy algorithms and an optimal algorithm, based on branch-and-bound search, for optimizing SFEs. Finally, we provide a large scale quantitative evaluation of these algorithms using a novel framework for evaluating explanations. The results show that our algorithms are quite effective and that our best greedy algorithm is competitive with optimal solutions. © 2019 Association for Computing Machinery.",Anomaly detection; anomaly explanation; anomaly interpretation; explanation evaluation,Feature extraction; Amount of information; Anomaly detection systems; anomaly explanation; anomaly interpretation; Branch and bound search; explanation evaluation; Optimal algorithm; Quantitative evaluation; Anomaly detection
Detecting and assessing anomalous evolutionary behaviors of nodes in evolving social networks,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061178718&doi=10.1145%2f3299886&partnerID=40&md5=04379cd3ac0bac3c16ff506fecdf1cd9,"Based on the performance of entire social networks, anomaly analysis for evolving social networks generally ignores the otherness of the evolutionary behaviors of different nodes, such that it is difficult to precisely identify the anomalous evolutionary behaviors of nodes (AEBN). Assuming that a node's evolutionary behavior that generates and removes edges normally follows stable evolutionary mechanisms, this study focuses on detecting and assessing AEBN, whose evolutionary mechanisms deviate from their past mechanisms, and proposes a link prediction detection (LPD) method and a matrix perturbation assessment (MPA) method. LPD describes a node's evolutionary behavior by fitting its evolutionary mechanism, and designs indexes for edge generation and removal to evaluate the extent to which the evolutionary mechanism of a node's evolutionary behavior can be fitted by a link prediction algorithm. Furthermore, it detects AEBN by quantifying the differences among behavior vectors that characterize the node's evolutionary behaviors in different periods. In addition, MPA considers AEBN as a perturbation of the social network structure, and quantifies the effect of AEBN on the social network structure based on matrix perturbation analysis. Extensive experiments on eight disparate real-world networks demonstrate that analyzing AEBN from the perspective of evolutionary mechanisms is important and beneficial. © 2019 Association for Computing Machinery.",Anomalous evolutionary behaviors of nodes; assessment; detection; perturbation analysis; social network,Data mining; Error detection; Social networking (online); Anomalous evolutionary behaviors of nodes; Anomaly analysis; assessment; Evolutionary mechanisms; Matrix perturbation; Perturbation Analysis; Real-world networks; Social network structures; Computer science
Text mining for evaluating authors' birth and death years,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061199352&doi=10.1145%2f3281631&partnerID=40&md5=399cecc4d561609f53e1eccfe5cf7469,"This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allowthe temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of ""Iron-clad,"" Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results. © 2019 Copyright is held by the owner/author(s).",Hebrew-Aramaic documents; key-phrases; knowledge discovery; Temporal-data; text and data mining; time analysis; undated documents,Classification (of information); Hebrew-Aramaic documents; Key-phrase; Temporal Data; Time analysis; undated documents; Data mining
Tensor completion algorithms in big data analytics,2019,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059210061&doi=10.1145%2f3278607&partnerID=40&md5=ec6aed8389c3aadd238c5bc297b332b9,"Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in areas like data mining, computer vision, signal processing, and neuroscience. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. We characterize these advances from the following four perspectives: General tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume), and dynamic tensor completion algorithms (velocity). Further, we identify several tensor completion applications on real-world data-driven problems and present some common experimental frameworks popularized in the literature along with several available software repositories. Our goal is to summarize these popular methods and introduce them to researchers and practitioners for promoting future research and applications. We conclude with a discussion of key challenges and promising research directions in this community for future exploration. © 2019 Association for Computing Machinery.",big data analytics; dynamic data analysis; multilinear data analysis; Tensor; tensor completion; tensor decomposition; tensor factorization,Advanced Analytics; Application programs; Big data; Data Analytics; Data handling; Data mining; Signal processing; Auxiliary information; Data-driven problems; Dynamic data analysis; Research and application; Software repositories; Tensor completion; Tensor decomposition; Tensor factorization; Tensors
PSP-AMS: Progressive mining of sequential patterns across multiple streams,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059557976&doi=10.1145%2f3281632&partnerID=40&md5=79673f82fbdf01b934c95bd7faf1881f,"Sequential pattern mining is used to find frequent data sequences over time. When sequential patterns are generated, the newly arriving patterns may not be identified as frequent sequential patterns due to the existence of old data and sequences. Progressive sequential pattern mining aims to find the most up-to-date sequential patterns given that obsolete items will be deleted from the sequences. When sequences come with multiple data streams, it is difficult to maintain and update the current sequential patterns. Even worse, when we consider the sequences across multiple streams, previous methods cannot efficiently compute the frequent sequential patterns. In this work, we propose an efficient algorithm PSP-AMS to address this problem. PSP-AMS uses a novel data structure PSP-MS-tree to insert new items, update current items, and delete obsolete items. By maintaining a PSP-MS-tree, PSP-AMS efficiently finds the frequent sequential patterns across multiple streams. The experimental results show that PSP-AMS significantly outperforms previous algorithms for mining of progressive sequential patterns across multiple streams on synthetic data as well as real data. © 2018 Association for Computing Machinery.",Across data streams; Across-streams sequential patterns; Multiple data streams; Progressive mining; Sequential patterns,Data mining; Forestry; Data stream; Frequent sequential patterns; Multiple data streams; Multiple streams; Progressive minings; Sequential patterns; Sequential-pattern mining; Synthetic data; Trees (mathematics)
Algorithms for online influencer marketing,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059567773&doi=10.1145%2f3274670&partnerID=40&md5=18f78fac6acb323969086601bf0c777a,"Influence maximization is the problem of finding influential users, or nodes, in a graph so as to maximize the spread of information. It has many applications in advertising and marketing on social networks. In this article, we study a highly generic version of influence maximization, one of optimizing influence campaigns by sequentially selecting “spread seeds” from a set of influencers, a small subset of the node population, under the hypothesis that, in a given campaign, previously activated nodes remain persistently active. This problem is in particular relevant for an important form of online marketing, known as influencer marketing, in which the marketers target a sub-population of influential people, instead of the entire base of potential buyers. Importantly, we make no assumptions on the underlying diffusion model, and we work in a setting where neither a diffusion network nor historical activation data are available. We call this problem online influencer marketing with persistence (in short, OIMP). We first discuss motivating scenarios and present our general approach. We introduce an estimator on the influencers' remaining potential - the expected number of nodes that can still be reached from a given influencer - and justify its strength to rapidly estimate the desired value, relying on real data gathered from Twitter. We then describe a novel algorithm, GT-UCB, relying on probabilistic upper confidence bounds on the remaining potential. We show that our approach leads to high-quality spreads on both simulated and real datasets. Importantly, it is orders of magnitude faster than state-of-the-art influence maximization methods, making it possible to deal with large-scale online scenarios. © Copyright held by the owner/author(s).",Influence maximization; Influencer marketing; Information diffusion; Multi-armed bandits; Online learning; Online social networks,Marketing; Influence maximizations; Information diffusion; Multi armed bandit; On-line social networks; Online learning; Social networking (online)
Digger: Detect similar groups in heterogeneous social networks,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059551271&doi=10.1145%2f3267106&partnerID=40&md5=e5aef960b7d7c83f8513b27db57aa9e3,"People participate in multiple online social networks, e.g., Facebook, Twitter, and Linkedin, and these social networks with heterogeneous social content and user relationship are named as heterogeneous social networks. Group structure widely exists in heterogeneous social networks, which reveals the evolution of human cooperation. Detecting similar groups in heterogeneous networks has a great significance for many applications, such as recommendation system and spammer detection, using the wealth of group information. Although promising, this novel problem encounters a variety of technical challenges, including incomplete data, high time complexity, and ground truth. To address the research gap and technical challenges, we take advantage of a ratio-cut optimization function to model this novel problem by the linear mixed-effects method and graph spectral theory. Based on this model, we propose an efficient algorithm called Digger to detect the similar groups in the large graphs. Digger consists of three steps, including measuring user similarity, construct a matching graph, and detecting similar groups. We adopt several strategies to lower the computational cost and detail the basis of labeling the ground truth. We evaluate the effectiveness and efficiency of our algorithm on five different types of online social networks. The extensive experiments show that our method achieves 0.693, 0.783, and 0.735 in precision, recall, and F1-measure, which significantly surpass the state-of-arts by 24.4%, 15.3%, and 20.7%, respectively. The results demonstrate that our proposal can detect similar groups in heterogeneous networks effectively. © 2018 Association for Computing Machinery.",Detecting similar groups; Graph spectral; Heterogeneous networks; Linear mixed-effects,Heterogeneous networks; Computational costs; Detecting similar groups; Effectiveness and efficiencies; Graph spectral; Mixed effects; On-line social networks; Optimization function; Technical challenges; Social networking (online)
Event analytics via discriminant tensor factorization,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061214884&doi=10.1145%2f3184455&partnerID=40&md5=d43480620694415556805cda63d5adcd,"Analyzing the impact of disastrous events has been central to understanding and responding to crises. Traditionally, the assessment of disaster impact has primarily relied on the manual collection and analysis of surveys and questionnaires as well as the review of authority reports. This can be costly and time-consuming, whereas a timely assessment of an event's impact is critical for crisis management and humanitarian operations. In this work, we formulate the impact discovery as the problem to identify the shared and discriminative subspace via tensor factorization due to the multi-dimensional nature of mobility data. Existing work in mining the shared and discriminative subspaces typically requires the predefined number of either type of them. In the context of event impact discovery, this could be impractical, especially for those unprecedented events. To overcome this, we propose a new framework, called ""PairFac,"" that jointly factorizes the multidimensional data to discover the latent mobility pattern along with its associated discriminative weight. This framework does not require splitting the shared and discriminative subspaces in advance and at the same time automatically captures the persistent and changing patterns from multi-dimensional behavioral data. Our work has important applications in crisis management and urban planning, which provides a timely assessment of impacts of major events in the urban environment. © 2018 Association for Computing Machinery.",data mining; event analytics; tensor factorization; Urban computing,Data mining; Factorization; Surveys; Crisis management; Discriminative weight; event analytics; Humanitarian operations; Multidimensional data; Tensor factorization; Urban computing; Urban environments; Tensors
A general embedding framework for heterogeneous information learning in large-scale networks,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054874749&doi=10.1145%2f3241063&partnerID=40&md5=1a6a3bb0c9f91b0561f3379274cc20f1,"Network analysis has been widely applied in many real-world tasks, such as gene analysis and targeted marketing. To extract effective features for these analysis tasks, network embedding automatically learns a low-dimensional vector representation for each node, such that the meaningful topological proximity is well preserved. While the embedding algorithms on pure topological structure have attracted considerable attention, in practice, nodes are often abundantly accompanied with other types of meaningful information, such as node attributes, second-order proximity, and link directionality. A general framework for incorporating the heterogeneous information into network embedding could be potentially helpful in learning better vector representations. However, it remains a challenging task to jointly embed the geometrical structure and a distinct type of information due to the heterogeneity. In addition, the real-world networks often contain a large number of nodes, which put demands on the scalability of the embedding algorithms. To bridge the gap, in this article, we propose a general embedding framework named Heterogeneous Information Learning in Large-scale networks (HILL) to accelerate the joint learning. It enables the simultaneous node proximity assessing process to be done in a distributed manner by decomposing the complex modeling and optimization into many simple and independent sub-problems. We validate the significant correlation between the heterogeneous information and topological structure, and illustrate the generalizability of HILL by applying it to perform attributed network embedding and second-order proximity learning. A variation is proposed for link directionality modeling. Experimental results on real-world networks demonstrate the effectiveness and efficiency of HILL. © 2018 Association for Computing Machinery.",Data mining; Distributed processing; Heterogeneity; Network embedding,Data mining; Encoding (symbols); Distributed processing; Effectiveness and efficiencies; Geometrical structure; Heterogeneity; Heterogeneous information; Network embedding; Topological structure; Vector representations; Topology
Stability and Robustness in Influence Maximization,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053475175&doi=10.1145%2f3233227&partnerID=40&md5=c68cd7293028a3e8651971bf98b205dc,"In the well-studied Influence Maximization problem, the goal is to identify a set of k nodes in a social network whose joint influence on the network is maximized. A large body of recent work has justified research on Influence Maximization models and algorithms with their potential to create societal or economic value. However, in order to live up to this potential, the algorithms must be robust to large amounts of noise, for they require quantitative estimates of the influence, which individuals exert on each other; ground truth for such quantities is inaccessible, and even decent estimates are very difficult to obtain. We begin to address this concern formally. First, we exhibit simple inputs on which even very small estimation errors may mislead every algorithm into highly suboptimal solutions. Motivated by this observation, we propose the Perturbation Interval model as a framework to characterize the stability of Influence Maximization against noise in the inferred diffusion network. Analyzing the susceptibility of specific instances to estimation errors leads to a clean algorithmic question, which we term the Influence Difference Maximization problem. However, the objective function of Influence Difference Maximization is NP-hard to approximate within a factor of O(n1−ϵ ) for any ϵ > 0. Given the infeasibility of diagnosing instability algorithmically, we focus on finding influential users robustly across multiple diffusion settings. We define a Robust Influence Maximization framework wherein an algorithm is presented with a set of influence functions. The algorithm's goal is to identify a set of k nodes who are simultaneously influential for all influence functions, compared to the (function-specific) optimum solutions. We show strong approximation hardness results for this problem unless the algorithm gets to select at least a logarithmic factor more seeds than the optimum solution. However, when enough extra seeds may be selected, we show that techniques of Krause et al. can be used to approximate the optimum robust influence to within a factor of 1 − 1/e. We evaluate this bicriteria approximation algorithm against natural heuristics on several real-world datasets. Our experiments indicate that the worst-case hardness does not necessarily translate into bad performance on real-world datasets; all algorithms perform fairly well. © 2018 ACM.",Influence maximization; Information diffusion; Robust optimization; Social networks; Stability analysis; Submodular optimization,Hardness; Method of moments; Optimization; Social networking (online); Stability; Influence maximizations; Information diffusion; Robust optimization; Stability analysis; Submodular optimizations; Approximation algorithms
Modeling Alzheimer's disease progression with fused laplacian sparse group lasso,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053481283&doi=10.1145%2f3230668&partnerID=40&md5=78bfa8bb2110ccf9d73ca8d2ac0aa5df,"Alzheimer's disease (AD), the most common type of dementia, not only imposes a huge financial burden on the health care system, but also a psychological and emotional burden on patients and their families. There is thus an urgent need to infer trajectories of cognitive performance over time and identify biomarkers predictive of the progression. In this article, we propose the multi-task learning with fused Laplacian sparse group lasso model, which can identify biomarkers closely related to cognitive measures due to its sparsity-inducing property, and model the disease progression with a general weighted (undirected) dependency graphs among the tasks. An efficient alternative directions method of multipliers based optimization algorithm is derived to solve the proposed non-smooth objective formulation. The effectiveness of the proposed model is demonstrated by its superior prediction performance over multiple state-of-the-art methods and accurate identification of compact sets of cognition-relevant imaging biomarkers that are consistent with prior medical studies. © 2018 ACM.",ADMM; Alzheimer's disease; Disease progression; Graph laplacian; Multi-task learning,Biomarkers; Laplace transforms; Learning systems; Medical imaging; ADMM; Alzheimer's disease; Disease progression; Graph Laplacian; Multitask learning; Neurodegenerative diseases
Enumerating trillion subgraphs on distributed systems,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054820617&doi=10.1145%2f3237191&partnerID=40&md5=ea228ae2772aa99739caca3a4a3621c5,"How can we find patterns from an enormous graph with billions of vertices and edges? The subgraph enumeration, which is to find patterns from a graph, is an important task for graph data analysis with many applications, including analyzing the social network evolution, measuring the significance of motifs in biological networks, observing the dynamics of Internet, and so on. Especially, the triangle enumeration, a special case of the subgraph enumeration, where the pattern is a triangle, has many applications such as identifying suspicious users in social networks, detecting web spams, and finding communities. However, recent networks are so large that most of the previous algorithms fail to process them. Recently, several MapReduce algorithms have been proposed to address such large networks; however, they suffer from the massive shuffled data resulting in a very long processing time. In this article, we propose scalable methods for enumerating trillion subgraphs on distributed systems. We first propose PTE (Pre-partitioned Triangle Enumeration), a new distributed algorithm for enumerating triangles in enormous graphs by resolving the structural inefficiency of the previous MapReduce algorithms. PTE enumerates trillions of triangles in a billion scale graph by decreasing three factors: the amount of shuffled data, total work, and network read. We also propose PSE (Pre-partitioned Subgraph Enumeration), a generalized version of PTE for enumerating subgraphs that match an arbitrary query graph. Experimental results show that PTE provides 79 times faster performance than recent distributed algorithms on real-world graphs, and succeeds in enumerating more than 3 trillion triangles on the ClueWeb12 graph with 6.3 billion vertices and 72 billion edges. Furthermore, PSE successfully enumerates 265 trillion clique subgraphs with 4 vertices from a subdomain hyperlink network, showing 47 times faster performance than the state of the art distributed subgraph enumeration algorithm. © 2018 Association for Computing Machinery.",Big data; Distributed algorithm; Graph algorithm; Network analysis; Scalable algorithm; Subgraph enumeration; Triangle enumeration,Bioinformatics; Distributed computer systems; Graph theory; Hypertext systems; MapReduce; Parallel algorithms; Scalability; Social networking (online); Web services; Distributed systems; Graph algorithms; Graph data; Map-reduce; Performance; Scalable algorithms; Social network evolution; Subgraph enumerations; Subgraphs; Triangle enumeration; Biology
Coupled clustering ensemble by exploring data interdependence,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053459431&doi=10.1145%2f3230967&partnerID=40&md5=c76111f27d19eac4cc8cf0fe737f786f,"Clustering ensembles combine multiple partitions of data into a single clustering solution. It is an effective technique for improving the quality of clustering results. Current clustering ensemble algorithms are usually built on the pairwise agreements between clusterings that focus on the similarity via consensus functions, between data objects that induce similarity measures from partitions and re-cluster objects, and between clusters that collapse groups of clusters into meta-clusters. In most of those models, there is a strong assumption on IIDness (i.e., independent and identical distribution), which states that base clusterings perform independently of one another and all objects are also independent. In the real world, however, objects are generally likely related to each other through features that are either explicit or even implicit. There is also latent but definite relationship among intermediate base clusterings because they are derived from the same set of data. All these demand a further investigation of clustering ensembles that explores the interdependence characteristics of data. To solve this problem, a new coupled clustering ensemble (CCE) framework that works on the interdependence nature of objects and intermediate base clusterings is proposed in this article. The main idea is to model the coupling relationship between objects by aggregating the similarity of base clusterings, and the interactive relationship among objects by addressing their neighborhood domains. Once these interdependence relationships are discovered, they will act as critical supplements to clustering ensembles. We verified our proposed framework by using three types of consensus function: clustering-based, object-based, and cluster-based. Substantial experiments on multiple synthetic and real-life benchmark datasets indicate that CCE can effectively capture the implicit interdependence relationships among base clusterings and among objects with higher clustering accuracy, stability, and robustness compared to 14 state-of-the-art techniques, supported by statistical analysis. In addition, we show that the final clustering quality is dependent on the data characteristics (e.g., quality and consistency) of base clusterings in terms of sensitivity analysis. Finally, the applications in document clustering, as well as on the datasets with much larger size and dimensionality, further demonstrate the effectiveness, efficiency, and scalability of our proposed models. © 2018 ACM.",Base clustering; Behavior interior dimensions; Clustering ensemble; Coupling; Interdependence; Object,Clustering algorithms; Couplings; Quality control; Sensitivity analysis; Base clustering; Behavior interior dimensions; Clustering Ensemble; Interdependence; Object; Cluster analysis
FrauDetector+: An incremental graph-mining approach for efficient fraudulent phone call detection,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053541568&doi=10.1145%2f3234943&partnerID=40&md5=2295c9f96b134ef1a5ed003e310174da,"In recent years, telecommunication fraud has become more rampant internationally with the development of modern technology and global communication. Because of rapid growth in the volume of call logs, the task of fraudulent phone call detection is confronted with big data issues in real-world implementations. Although our previous work, FrauDetector, addressed this problem and achieved some promising results, it can be further enhanced because it focuses only on fraud detection accuracy, whereas the efficiency and scalability are not top priorities. Other known approaches for fraudulent call number detection suffer from long training times or cannot accurately detect fraudulent phone calls in real time. However, the learning process of FrauDetector is too time-consuming to support real-world application. Although we have attempted to accelerate the the learning process of FrauDetector by parallelization, the parallelized learning process, namely PFrauDetector, still cannot afford the computing cost. In this article, we propose a highly efficient incremental graph-mining-based fraudulent phone call detection approach, namely FrauDetector+, which can automati-8 cally label fraudulent phone numbers with a “fraud” tag a crucial prerequisite for distinguishing fraudulent phone call numbers from nonfraudulent ones. FrauDetector+ initially generates smaller, more manageable subnetworks from original graph and performs a parallelized weighted HITS algorithm for a significant speed increase in the graph learning module. It adopts a novel aggregation approach to generate a trust (or experience) value for each phone number (or user) based on their respective local values. After the initial procedure, we can incrementally update the trust (or experience) value for each phone number (or user) while a new fraud phone number is identified. An efficient fraud-centric hash structure is constructed to support fast real-time detection of fraudulent phone numbers in the detection module. We conduct a comprehensive experimental study based on real datasets collected through an antifraud mobile application called Whoscall. The results demonstrate a significantly improved efficiency of our approach compared with FrauDetector as well as superior performance against other major classifier-based methods. © 2018 ACM 1556-4681/2018/08-ART68 $15.00",Fraudulent phone call detection; Incremental learning; Parallelized weighted HITS algorithm; Telecommunication fraud; Trust value mining,Big data; Crime; Efficiency; Information retrieval; Learning algorithms; Learning systems; HITS algorithms; Incremental learning; Phone calls; Telecommunication fraud; Trust values; Telephone sets
Semi-supervised learning meets factorization: Learning to recommend with chain graph model,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054872710&doi=10.1145%2f3264745&partnerID=40&md5=08684740ceecfbb4235a729a15a62292,"Recently, latent factor model (LFM) has been drawing much attention in recommender systems due to its good performance and scalability. However, existing LFMs predictmissing values in a user-item rating matrix only based on the known ones, and thus the sparsity of the rating matrix always limits their performance. Meanwhile, semi-supervised learning (SSL) provides an effective way to alleviate the label (i.e., rating) sparsity problem by performing label propagation, which ismainly based on the smoothness insight on affinity graphs. However, graph-based SSL suffers serious scalability and graph unreliable problems when directly being applied to do recommendation. In this article, we propose a novel probabilistic chain graph model (CGM) to marry SSL with LFM. The proposed CGM is a combination of Bayesian network and Markov random field. The Bayesian network is used to model the rating generation and regression procedures, and the Markov random field is used to model the confidence-aware smoothness constraint between the generated ratings. Experimental results show that our proposed CGM significantly outperforms the state-of-the-art approaches in terms of four evaluation metrics, and with a larger performance margin when data sparsity increases. © 2018 Association for Computing Machinery.",Chain graph model; Data sparsity; Latent factor model; Semi-supervised learning,Bayesian networks; Chains; Graph theory; Graphic methods; Markov processes; Matrix algebra; Scalability; Chain graph; Data sparsity; Latent factor models; Performance and scalabilities; Semi- supervised learning; Semi-supervised learning (SSL); Smoothness constraints; State-of-the-art approach; Supervised learning
Protecting privacy in trajectories with a user-centric approach,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053478463&doi=10.1145%2f3233185&partnerID=40&md5=18698c1edd4a49178759987d9ad1a674,"The increased use of location-aware devices, such as smartphones, generates a large amount of trajectory data. These data can be useful in several domains, like marketing, path modeling, localization of an epidemic focus, and so on. Nevertheless, since trajectory information contains personal mobility data, improper use or publication of trajectory data can threaten users' privacy. It may reveal sensitive details like habits of behavior, religious beliefs, and sexual preferences. Therefore, many users might be unwilling to share their trajectory data without a previous anonymization process. Currently, several proposals to address this problem can be found in the literature. These solutions focus on anonymizing data before its publication, i.e., when they are already stored in the server database. Nevertheless, we argue that this approach gives the user no control about the information she shares. For this reason, we propose anonymizing data in the users' mobile devices, before they are sent to a third party. This article extends our previous work which was, to the best of our knowledge, the first one to anonymize data at the client side, allowing users to select the amount and accuracy of shared data. In this article, we describe an improved version of the protocol, and we include the implementation together with an analysis of the results obtained after the simulation with real trajectory data. © 2018 ACM.",Privacy; Trajectory anonymization; User-centric protocol,Computer science; Data mining; Data privacy; Anonymization; Location-aware; Personal mobility; Real trajectories; Server database; Trajectory data; Trajectory information; User-centric; Trajectories
Entity-based query recommendation for long-tail queries,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052569193&doi=10.1145%2f3233186&partnerID=40&md5=bbb5f3eca8d94ec9a5a9a4a5e9d677b6,"Query recommendation, which suggests related queries to search engine users, has attracted a lot of attention in recent years. Most of the existing solutions, which perform analysis of users' search history (or query logs), are often insufficient for long-tail queries that rarely appear in query logs. To handle such queries, we study the use of entities found in queries to provide recommendations. Specifically, we extract entities from a query, and use these entities to explore new ones by consulting an information source. The discovered entities are then used to suggest new queries to the user. In this article, we examine two information sources: (1) a knowledge base (or KB), such as YAGO and Freebase; and (2) a click log, which contains the URLs accessed by a query user. We study how to use these sources to find new entities useful for query recommendation. We further study a hybrid framework that integrates different query recommendation methods effectively. As shown in the experiments, our proposed approaches provide better recommendations than existing solutions for long-tail queries. In addition, our query recommendation process takes less than 100ms to complete. Thus, our solution is suitable for providing online query recommendation services for search engines. © 2018 ACM.",Entity; Knowledge base; Query recommendation,Knowledge based systems; Search engines; Entity; Hybrid framework; Information sources; Knowledge base; Long tail; Query logs; Query recommendations; Search history; Information retrieval
Exploiting user posts for web document summarization,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052604688&doi=10.1145%2f3186566&partnerID=40&md5=fe0600de0f08d9d4493a239a4e43b464,"Relevant user posts such as comments or tweets of a Web document provide additional valuable information to enrich the content of this document. When creating user posts, readers tend to borrow salient words or phrases in sentences. This can be considered as word variation. This article proposes a framework that models the word variation aspect to enhance the quality of Web document summarization. Technically, the framework consists of two steps: scoring and selection. In the first step, the social information of a Web document such as user posts is exploited to model intra-relations and inter-relations in lexical and semantic levels. These relations are denoted by a mutual reinforcement similarity graph used to score each sentence and user post. After scoring, summaries are extracted by using a ranking approach or concept-based method formulated in the form of Integer Linear Programming. To confirm the efficiency of our framework, sentence and story highlight extraction tasks were taken as a case study on three datasets in two languages, English and Vietnamese. Experimental results show that: (i) the framework can improve ROUGE-scores compared to state-of-the-art baselines of social context summarization and (ii) the combination of the two relations benefits the sentence extraction of single Web documents. © 2018 ACM.",Data mining; ILP; Information retrieval; Ranking; Social context summarization; Summarization,Extraction; Information retrieval; Integer programming; Semantics; Integer Linear Programming; Mutual reinforcement; Ranking; Ranking approach; Sentence extraction; Social context; Social information; Summarization; Data mining
Online active learning with expert advice,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060576139&doi=10.1145%2f3201604&partnerID=40&md5=bc1bdd1b2cca9d3ecdaf3aa7949b9abd,"In literature, learning with expert advice methods usually assume that a learner always obtain the true label of every incoming training instance at the end of each trial. However, in many real-world applications, acquiring the true labels of all instances can be both costly and time consuming, especially for large-scale problems. For example, in the social media, data stream usually comes in a high speed and volume, and it is nearly impossible and highly costly to label all of the instances. In this article, we address this problem with active learning with expert advice, where the ground truth of an instance is disclosed only when it is requested by the proposed active query strategies. Our goal is to minimize the number of requests while training an online learning model without sacrificing the performance. To address this challenge, we propose a framework of active forecasters, which attempts to extend two fully supervised forecasters, Exponentially Weighted Average Forecaster and Greedy Forecaster, to tackle the task of online active learning (OAL) with expert advice. Specifically, we proposed two OAL with expert advice algorithms, named Active ExponentiallyWeighted Average Forecaster (AEWAF) and active greedy forecaster (AGF), by considering the difference of expert advices. To further improve the robustness of the proposed AEWAF and AGF algorithms in the noisy scenarios (where noisy experts exist), we also proposed two robust active learning with expert advice algorithms, named Robust Active Exponentially Weighted Average Forecaster and Robust Active Greedy Forecaster. We validate the efficacy of the proposed algorithms by an extensive set of experiments in both normal scenarios (where all of experts are comparably reliable) and noisy scenarios. © 2018 ACM.",Active learning; Data streaming; Expert advice; Online learning,Artificial intelligence; Statistical methods; Active Learning; Data streaming; Expert advice; Large-scale problem; Online learning; Query strategies; Social media; Weighted averages; E-learning
Behavior2Vec: Generating distributed representations of users' behaviors on products for recommender systems,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052600597&doi=10.1145%2f3184454&partnerID=40&md5=07ec5b7ef55462c3f4c6629fb2017eeb,"Most studies on recommender systems target at increasing the click through rate, and hope that the number of orders will increase as well. We argue that clicking and purchasing an item are different behaviors. Thus, we should probably apply different strategies for different objectives, e.g., increase the click through rate, or increase the order rate. In this article, we propose to generate the distributed representations of users' viewing and purchasing behaviors on an e-commerce website. By leveraging on the cosine distance between the distributed representations of the behaviors on items under different contexts, we can predict a user's next clicking or purchasing item more precisely, compared to several baseline methods. Perhaps more importantly, we found that the distributed representations may help discover interesting analogies among the products. We may utilize such analogies to explain how two products are related, and eventually apply different recommendation strategies under different scenarios. We developed the Behavior2Vec library for demonstration. The library can be accessed at https://github.com/ncu-dart/behavior2vec/. © 2018 ACM.",Behavior embedding; Collaborative filtering; Distributed representation; Learning representations; Matrix factorization; Word2Vec,Collaborative filtering; Factorization; Recommender systems; Sales; Behavior embedding; Distributed representation; Learning representations; Matrix factorizations; Word2Vec; Electronic commerce
Spatio-temporal routine mining on mobile phone data,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060625934&doi=10.1145%2f3201577&partnerID=40&md5=efa448a1be22870deeccc81c9105800c,"Mining human behaviors has always been an important subarea of Data Mining.While it provides empirical evidences to psychological/behavioral studies, it also builds the foundation of various big-data systems, which rely heavily on the prediction of human behaviors. In recent years, the ubiquitous spreading of mobile phones and the massive amount of spatio-temporal data collected from them make it possible to keep track of the daily commute behaviors of mobile subscribers and further conduct routine mining on them. In this article, we propose to model mobile subscribers' daily commute behaviors by three levels: location trajectory, oneday pattern, and routine pattern. We develop the model Spatio-Temporal Routine Mining Model (STRMM) to characterize the generative process between these three levels. From daily trajectories, the STRMM model unsupervisedly extracts spatio-temporal routine patterns that contain two aspects of information: (1) How people's typical commute patterns are. (2) How much their commute behaviors vary from day to day. Compared to traditional methods, STRMM takes into account the different degrees of behavioral uncertainty in different timespans of a day, yielding more realistic and intuitive results. To learnmodel parameters, we adopt Stochastic Expectation Maximization algorithm. Experiments are conducted on two real world datasets, and the empirical results show that the STRMM model can effectively discover hidden routine patterns of human commute behaviors and yields higher accuracy results in trajectory prediction task. © 2018 ACM.",Mobile phone data; Routine mining; Spatio-temporal pattern,Cellular telephones; Data mining; Image segmentation; Maximum principle; Stochastic systems; Trajectories; Generative process; Mobile phone datum; Mobile subscribers; Real-world datasets; Spatio-temporal data; Spatiotemporal patterns; Stochastic expectation maximization; Trajectory prediction; Behavioral research
Coordination event detection and initiator identification in time series data,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060597224&doi=10.1145%2f3201406&partnerID=40&md5=ac873101003def3c4b02b374a0582198,"Behavior initiation is a form of leadership and is an important aspect of social organization that affects the processes of group formation, dynamics, and decision-making in human societies and other social animal species. In this work, we formalize the Coordination Initiator Inference Problem and propose a simple yet powerful framework for extracting periods of coordinated activity and determining individuals who initiated this coordination, based solely on the activity of individuals within a group during those periods. The proposed approach, given arbitrary individual time series, automatically (1) identifies times of coordinated group activity, (2) determines the identities of initiators of those activities, and (3) classifies the likely mechanism by which the group coordination occurred, all of which are novel computational tasks.We demonstrate our framework on both simulated and real-world data: trajectories tracking of animals as well as stock market data. Our method is competitive with existing global leadership inference methods but provides the first approaches for local leadership and coordination mechanism classification. Our results are consistent with ground-truthed biological data and the framework finds many known events in financial data which are not otherwise reflected in the aggregate NASDAQ index. Our method is easily generalizable to any coordinated time series data from interacting entities. © 2018 ACM.",Coordination; Influence; Leadership; Time series,Animals; Decision making; Coordinated activity; Coordination; Coordination mechanisms; Group coordination; Influence; Interacting entities; Leadership; Social organizations; Time series
ClassiNet - Predicting missing features for short-text classification,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060568307&doi=10.1145%2f3201578&partnerID=40&md5=081b67acce1b0db0d8ec66069345151e,"                             Short and sparse texts such as tweets, search engine snippets, product reviews, and chat messages are abundant on the Web. Classifying such short-texts into a pre-defined set of categories is a common problem that arises in various contexts, such as sentiment classification, spam detection, and information recommendation. The fundamental problem in short-text classification is feature sparseness - the lack of feature overlap between a trained model and a test instance to be classified. We propose ClassiNet - a network of classifiers trained for predicting missing features in a given instance, to overcome the feature sparseness problem. Using a set of unlabeled training instances, we first learn binary classifiers as feature predictors for predicting whether a particular feature occurs in a given instance. Next, each feature predictor is represented as a vertex ν                             i                              in the ClassiNet, where a one-to-one correspondence exists between feature predictors and vertices. The weight of the directed edge e                             ij                              connecting a vertex ν                             i                              to a vertex ν                             j                              represents the conditional probability that given ν                             i                              exists in an instance, ν                             j                              also exists in the same instance. We show that ClassiNets generalize word co-occurrence graphs by considering implicit co-occurrences between features. We extract numerous features from the trained ClassiNet to overcome feature sparseness. In particular, for a given instance x, we find similar features from ClassiNet that did not appear in x, and append those features in the representation of x. Moreover, we propose a method based on graph propagation to find features that are indirectly related to a given short-text.We evaluate ClassiNets on several benchmark datasets for short-text classification. Our experimental results show that by using ClassiNet, we can statistically significantly improve the accuracy in short-text classification tasks, without having to use any external resources such as thesauri for finding related features.                          © 2018 Association for Computing Machinery. All rights reserved.",Classifier networks; Feature sparseness; Short-texts; Text classification,Forecasting; Search engines; Text processing; Conditional probabilities; Feature sparseness; Information recommendation; Sentiment classification; Short text classifications; Short texts; Text classification; Word co-occurrence; Classification (of information)
SemRe-Rank: Improving automatic term extraction by incorporating semantic relatedness with personalised page rank,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060572872&doi=10.1145%2f3201408&partnerID=40&md5=f259c5fe1b2b16ee74dd7b63f3e897a2,"Automatic Term Extraction (ATE) dealswith the extraction of terminology from a domain specific corpus, and has long been an established research area in data and knowledge acquisition. ATE remains a challenging task as it is known that there is no existing ATE methods that can consistently outperform others in any domain. This work adopts a refreshed perspective to this problem: instead of searching for such a 'one-size-fit-all' solution that may never exist, we propose to develop generic methods to 'enhance' existing ATE methods. We introduce SemRe-Rank, the first method based on this principle, to incorporate semantic relatedness-an often overlooked venue-into an existing ATE method to further improve its performance. SemRe-Rank incorporates word embeddings into a personalised PageRank process to compute 'semantic importance' scores for candidate terms from a graph of semantically related words (nodes), which are then used to revise the scores of candidate terms computed by a base ATE algorithm. Extensively evaluated with 13 state-of-the-art base ATE methods on four datasets of diverse nature, it is shown to have achieved widespread improvement over all base methods and across all datasets, with up to 15 percentage points when measured by the Precision in the top ranked K candidate terms (the average for a set of K's), or up to 28 percentage points in F1 measured at a K that equals to the expected real terms in the candidates (F1 in short). Compared to an alternative approach built on the well-known TextRank algorithm, SemRe-Rank can potentially outperform by up to 8 points in Precision at top K, or up to 17 points in F1. © 2018 ACM.",ATE; ATR; Automatic term extraction; Automatic term recognition; Information extraction; Information retrieval; Personalised pagerank; Semantic relatedness; Termhood; Text mining; Word embedding,Character recognition; Embeddings; Information retrieval; Natural language processing systems; Semantics; Text processing; Automatic term extraction; Automatic term recognition; PageRank; Semantic relatedness; Termhood; Text mining; Word embedding; Data mining
iGRM: Improved grey relational model and its ensembles for occupancy sensing in internet of things applications,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052593749&doi=10.1145%2f3186268&partnerID=40&md5=1195187f69e4f4d3ba2192aebcf8e214,"Occupancy detection is one of the many applications of Building Automation Systems (BAS) or Heating, Ventilation, and Air Conditioning (HVAC) control systems, especially, with the rising demand of Internet of Things (IoT) services. This article describes the fusion of data collected from sensors by exploiting their potential to sense occupancy in a room. For this purpose, a sensor test bed is deployed that includes four sensors measuring temperature, relative humidity, distance from the first obstacle, and light along with a Arduino micro-controller to validate our model. In addition, this article proposes three algorithms for efficient fusion of the sensor data that is inspired by the Grey theory. An improved Grey Relational Model (iGRM) is proposed, which acts as the base classifier for the other two algorithms, namely, Grey Relational Model with Bagging (iGRM-BG) and Grey Relational Model with Boosting (iGRM-BT). Furthermore, all three algorithms use a sliding window concept, where only the samples inside the window participate in model training. Also, we have considered varying number of window size for optimal comparison. The algorithms were tested against the experimental data collected through a test bed as well as on a publicly available large dataset, where both the ensemble models, iGRM-BG and iGRM-BT, are seen to enhance the performance of iGRM. The results reveal exceptionally high performances with accuracies above 95% (iGRM) and up to 100% (iGRM-BT) for the experimental dataset and above 98.24% (iGRM) and up to 99.49% (iGRM-BG) using the publicly available dataset. Among the three proposed models, iGRM-BG was observed to outperform both iGRM and iGRM-BT owing to its advantage of being an ensemble model and its robustness against over-fitting. © 2018 ACM.",Bagging; Boosting; Window,Air conditioning; Automation; Equipment testing; Humidity control; Intelligent buildings; Internet of things; Statistical tests; Windows; Bagging; Boosting; Building automation systems; Grey relational model; Internet of Things (IOT); Internet of things applications; Measuring temperature; Occupancy detections; Sensor data fusion
Consensus guided multi-view clustering,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052596766&doi=10.1145%2f3182384&partnerID=40&md5=c5809c7c4ada9b4109ae69914334ec1f,"In recent decades, tremendous emerging techniques thrive the artificial intelligence field due to the increasing collected data captured from multiple sensors. These multi-view data provide more rich information than traditional single-view data. Fusing heterogeneous information for certain tasks is a core part of multi-view learning, especially for multi-view clustering. Although numerous multi-view clustering algorithms have been proposed, most scholars focus on finding the common space of different views, but unfortunately ignore the benefits from partition level by ensemble clustering. For ensemble clustering, however, there is no interaction between individual partitions from each view and the final consensus one. To fill the gap, we propose a Consensus Guided Multi-View Clustering (CMVC) framework, which incorporates the generation of basic partitions from each view and fusion of consensus clustering in an interactive way, i.e., the consensus clustering guides the generation of basic partitions, and high quality basic partitions positively contribute to the consensus clustering as well. We design a non-trivial optimization solution to formulate CMVC into two iterative k-means clusterings by an approximate calculation. In addition, the generalization of CMVC provides a rich feasibility for different scenarios, and the extension of CMVC with incomplete multi-view clustering further validates the effectiveness for real-world applications. Extensive experiments demonstrate the advantages of CMVC over other widely used multi-view clustering methods in terms of cluster validity, and the robustness of CMVC to some important parameters and incomplete multi-view data. © 2018 ACM.",Ensemble clustering; Multi-view clustering; Utility function,Cluster analysis; Iterative methods; Approximate calculations; Consensus clustering; Ensemble clustering; Heterogeneous information; Multi-view clustering; Multi-view learning; Optimization solution; Utility functions; Clustering algorithms
Discovering mobile application usage patterns from a large-scale dataset,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053438606&doi=10.1145%2f3209669&partnerID=40&md5=82b8c80464c21ec9869f02eca9c65f5a,"The discovering of patterns regarding how, when, and where users interact with mobile applications reveals important insights for mobile service providers. In this work, we exploit for the first time a real and largescale dataset representing the records of mobile application usage of 5,342 users during 2014. The data was collected by a software agent, installed at the users' smartphones, which monitors detailed usage of applications. First, we look for general patterns of how users access some of the most popular mobile applications in terms of frequency, duration, diversity, and data traffic. Next, we mine the dataset looking for temporal patterns in terms of when and how often accesses occur. Finally, we exploit the location of each access to detect users' points of interest and location-based communities. Based on the results, we derive a model to generate synthetic datasets of mobile application usage and evaluate solutions to predict the next application to be launched.We also discuss a series of implications of the findings regarding telecommunication services, mobile advertisements, and smart cities. This is the first time this dataset is used, and we also make it publicly available for other researchers. © 2018 ACM.",Mobile data; Pattern mining; Synthetic data generation,Large dataset; Mobile computing; Software agents; Telecommunication services; Large-scale dataset; Mobile advertisement; Mobile applications; Mobile data; Mobile service providers; Pattern mining; Synthetic data generations; Synthetic datasets; Application programs
Comparison of ontology alignment systems across single matching task via the McNemar's test,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052596763&doi=10.1145%2f3193573&partnerID=40&md5=db7dc6331d34d60a24d249998d6442a3,"Ontology alignment is widely used to find the correspondences between different ontologies in diverse fields. After discovering the alignments, several performance scores are available to evaluate them. The scores typically require the identified alignment and a reference containing the underlying actual correspondences of the given ontologies. The current trend in the alignment evaluation is to put forward a new score (e.g., precision, weighted precision, semantic precision, etc.) and to compare various alignments by juxtaposing the obtained scores. However, it is substantially provocative to select one measure among others for comparison. On top of that, claiming if one system has a better performance than one another cannot be substantiated solely by comparing two scalars. In this article, we propose the statistical procedures that enable us to theoretically favor one system over one another. The McNemar's test is the statistical means by which the comparison of two ontology alignment systems over one matching task is drawn. The test applies to a 2 × 2 contingency table, which can be constructed in two different ways based on the alignments, each of which has their own merits/pitfalls. The ways of the contingency table construction and various apposite statistics from the McNemar's test are elaborated in minute detail. In the case of having more than two alignment systems for comparison, the family wise error rate is expected to happen. Thus, the ways of preventing such an error are also discussed. A directed graph visualizes the outcome of the McNemar's test in the presence of multiple alignment systems. From this graph, it is readily understood if one system is better than one another or if their differences are imperceptible. The proposed statistical methodologies are applied to the systems participated in the OAEI 2016 anatomy track, and also compares several well-known similarity metrics for the same matching problem. © 2018 ACM.",Anatomy; Family-wise error rate; McNemar's test; OAEI; Ontology alignment,Directed graphs; Errors; Semantics; Statistical methods; Testing; Anatomy; Error rate; McNemar's tests; OAEI; Ontology alignment; Ontology
Cluster's quality evaluation and selective clustering ensemble,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060655139&doi=10.1145%2f3211872&partnerID=40&md5=1c2194cad934b8f6b98986b5085afb29,"Clustering ensemble has drawn much attention in recent years due to its ability to generate a high quality and robust partition result. Weighted clustering ensemble and selective clustering ensemble are two general ways to further improve the performance of a clustering ensemble method. Existing weighted clustering ensemble methods assign the same weight to each cluster in a partition of the ensemble. Since the qualities of the clusters in a partition are different, the clusters should be weighted differently. To address this issue, this article proposes a newmeasure to calculate the similarity between a cluster and a partition. Theoretically, this measure is effective in handling two problems in measuring the quality of a cluster, which are defined as the symmetric problem and the context meaning problem. In addition, some properties of the proposed measure are analyzed. This measure can be easily expanded to a clustering performance measure that calculates the similarity between two partitions. As a result of this measure, we propose a novel selective clustering ensemble framework, which considers the differences between the objective of the ensemble selection stage and the object of the ensemble integration stage in the selective clustering ensemble. To verify the performance of the new measure, we compare the performance of the measure with the two existing measures in weighting clusters. The experiments show that the proposed measure is more effective. To verify the performance of the novel framework, four existing state-of-the-art selective clustering ensemble frameworks are employed as references. The experiments show that the proposed framework is statistically better than the others on 17 UCI benchmark datasets, 8 document datasets, and the Olivetti Face Database. © 2018 ACM.",Cluster quality; Clustering ensemble; Selective clustering ensemble; Weighted clustering ensemble,Data mining; Benchmark datasets; Cluster qualities; Clustering Ensemble; Document datasets; Ensemble integration; Ensemble selections; Performance measure; Quality evaluation; Computer science
Time series classification with HIVE-COTE: The hierarchical vote collective of transformation-based ensembles,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060599747&doi=10.1145%2f3182382&partnerID=40&md5=6f51d237c775a0a0dd5e2b07740afeea,"A recent experimental evaluation assessed 19 time series classification (TSC) algorithms and found that one was significantly more accurate than all others: the Flat Collective of Transformation-based Ensembles (Flat- COTE). Flat-COTE is an ensemble that combines 35 classifiers over four data representations. However, while comprehensive, the evaluation did not consider deep learning approaches. Convolutional neural networks (CNN) have seen a surge in popularity and are now state of the art in many fields and raises the question of whether CNNs could be equally transformative for TSC. We implement a benchmark CNN for TSC using a common structure and use results from a TSC-specific CNN from the literature. We compare both to Flat-COTE and find that the collective is significantly more accurate than both CNNs. These results are impressive, but Flat-COTE is not without deficiencies.We significantly improve the collective by proposing a new hierarchical structure with probabilistic voting, defining and including two novel ensemble classifiers built in existing feature spaces, and adding further modules to represent two additional transformation domains. The resulting classifier, the Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE), encapsulates classifiers built on five data representations. We demonstrate that HIVE-COTE is significantly more accurate than Flat-COTE (and all other TSC algorithms that we are aware of) over 100 resamples of 85 TSC problems and is the new state of the art for TSC. Further analysis is included through the introduction and evaluation of 3 new case studies and extensive experimentation on 1,000 simulated datasets of 5 different types. 2018 Copyright is held by the owner/author(s). © 2018 Association for Computing Machinery. All rights reserved.",Deep learning; Heterogeneous ensembles; Meta ensembles; Time series classification,Deep learning; Metadata; Neural networks; Time series; Convolutional neural network; Experimental evaluation; Heterogeneous ensembles; Hierarchical structures; Meta ensembles; Time series classifications; Transformation based; Transformation domain; Classification (of information)
Mining graphlet counts in online social networks,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052597871&doi=10.1145%2f3182392&partnerID=40&md5=55ce0fcdfb2f0041f7d0c9b5aeb1a260,"Counting subgraphs is a fundamental analysis task for online social networks (OSNs). Given the sheer size and restricted access of OSN, efficient computation of subgraph counts is highly challenging. Although a number of algorithms have been proposed to estimate the relative counts of subgraphs in OSNs with restricted access, there are only few works which try to solve a more general problem, i.e., counting subgraph frequencies. In this article, we propose an efficient random walk-based framework to estimate the subgraph counts. Our framework generates samples by leveraging consecutive steps of the random walk as well as by observing neighbors of visited nodes. Using the importance sampling technique, we derive unbiased estimators of the subgraph counts. To make better use of the degree information of visited nodes, we also design improved estimators, which increases the accuracy of the estimation with no additional cost. We conduct extensive experimental evaluation on real-world OSNs to confirm our theoretical claims. The experiment results show that our estimators are unbiased, accurate, efficient, and better than the state-of-the-art algorithms. For the Weibo graph with more than 58 million nodes, our method produces estimate of triangle count with an error less than 5% using only 20,000 sampled nodes. Detailed comparison with the state-of-the-art methods demonstrates that our algorithm is 2-10 times more accurate. © 2018 ACM.",Graphlet count; Markov chain Monte Carlo; Online social networks; Random walk,Importance sampling; Markov processes; Experimental evaluation; Graphlet count; Markov Chain Monte-Carlo; On-line social networks; Online social networks (OSNs); Random Walk; State-of-the-art algorithms; State-of-the-art methods; Social networking (online)
Employing semantic context for sparse information extraction assessment,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060574568&doi=10.1145%2f3201407&partnerID=40&md5=712d1b11193fca2bd6c68ed9b98f3ac3,"A huge amount of texts available on the World Wide Web presents an unprecedented opportunity for information extraction (IE). One important assumption in IE is that frequent extractions are more likely to be correct. Sparse IE is hence a challenging task because no matter how big a corpus is, there are extractions supported by only a small amount of evidence in the corpus. However, there is limited research on sparse IE, especially in the assessment of the validity of sparse IEs. Motivated by this, we introduce a lightweight, explicit semantic approach for assessing sparse IE.1 We first use a large semantic network consisting of millions of concepts, entities, and attributes to explicitly model the context of any semantic relationship. Second, we learn from three semantic contexts using different base classifiers to select an optimal classification model for assessing sparse extractions. Finally, experiments show that as compared with several state-of-the-art approaches, our approach can significantly improve the F-score in the assessment of sparse extractions while maintaining the efficiency. © 2018 Association for Computing Machinery. All rights reserved.",Classification; IsA relationship; Semantic network; Sparse information extraction,Artificial intelligence; Classification (of information); Data mining; Information retrieval; Semantic Web; Base classifiers; Explicit semantics; IsA relationship; Optimal classification; Semantic context; Semantic network; Semantic relationships; State-of-the-art approach; Semantics
Representation learning for classification in heterogeneous graphs with application to social networks,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060659164&doi=10.1145%2f3201603&partnerID=40&md5=a47c434d20033caee096893e4abf7aa9,"We address the task of node classification in heterogeneous networks, where the nodes are of different types, each type having its own set of labels, and the relations between nodes may also be of different types. A typical example is provided by social networks where node types may for example be users, content, or films, and relations friendship, like, authorship. Learning and performing inference on such heterogeneous networks is a recent task requiring new models and algorithms. We propose a model, Labeling Heterogeneous Network (LaHNet), a transductive approach to classification that learns to project the different types of nodes into a common latent space. This embedding is learned so as to reflect different characteristics of the problem such as the correlation between node labels, as well as the graph topology. The application focus is on social graphs, but the algorithm is general and can be used for other domains. The model is evaluated on five datasets representative of different instances of social data. © 2018 ACM.",Node classification; Relational data; Representation learning,Heterogeneous networks; Inference engines; Graph topology; Heterogeneous graph; Models and algorithms; Node types; Relational data; Representation learning; Social datum; Social graphs; Topology
Social network monitoring for bursty cascade detection,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052583377&doi=10.1145%2f3178048&partnerID=40&md5=47184204b77cfc49f8bc3f18e5a2d793,"Social network services have become important and efficient platforms for users to share all kinds of information. The capability to monitor user-generated information and detect bursts from information diffusions in these social networks brings value to a wide range of real-life applications, such as viral marketing. However, in reality, as a third party, there is always a cost for gathering information from each user or so-called social network sensor. The question then arises how to select a budgeted set of social network sensors to form the data stream for burst detection without compromising the detection performance. In this article, we present a general sensor selection solution for different burst detection approaches. We formulate this problem as a constraint satisfaction problem that has high computational complexity. To reduce the computational cost, we first reduce most of the constraints by making use of the fact that bursty cascades are rare among the whole population. We then transform the problem into an Linear Programming (LP) problem. Furthermore, we use the sub-gradient method instead of the standard simplex method or interior-point method to solve the LP problem, which makes it possible for our solution to scale up to large social networks. Evaluating our solution on millions of real information cascades, we demonstrate both the effectiveness and efficiency of our approach. © 2018 ACM.",Linear programming; Social network sensors; Sub-gradient method,Budget control; Constraint satisfaction problems; Constraint theory; Gradient methods; Linear programming; Mathematical transformations; Detection performance; Effectiveness and efficiencies; Information diffusion; Network sensors; Real-life applications; Social network services; Standard simplex method; Sub-gradient methods; Problem solving
Evasion-robust classification on binary domains,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051469625&doi=10.1145%2f3186282&partnerID=40&md5=fe79c68d786c45a07e3ea577ef4ef1b9,"The success of classification learning has led to numerous attempts to apply it in adversarial settings such as spam and malware detection. The core challenge in this class of applications is that adversaries are not static, but make a deliberate effort to evade the classifiers. We investigate both the problem of modeling the objectives of such adversaries, as well as the algorithmic problem of accounting for rational, objective-driven adversaries. We first present a general approach based on mixed-integer linear programming (MILP) with constraint generation. This approach is the first to compute an optimal solution to adversarial loss minimization for two general classes of adversarial evasion models in the context of binary feature spaces. To further improve scalability and significantly generalize the scope of the MILP-based method, we propose a principled iterative retraining framework, which can be used with arbitrary classifiers and essentially arbitrary attack models. We show that the retraining approach, when it converges, minimizes an upper bound on adversarial loss. Extensive experiments demonstrate that the mixed-integer programming approach significantly outperforms several state-of-the-art adversarial learning alternatives. Moreover, the retraining framework performs nearly as well, but scales significantly better. Finally, we show that our approach is robust to misspecifications of the adversarial model. © 2018 ACM.",Adversarial classification; Adversarial examples; Classifier evasion; Mixed-integer linear programming; Robust learning,Iterative methods; Malware; Adversarial classifications; Adversarial examples; Classification learning; Constraint generation; Mixed integer linear programming; Mixed integer linear programming (MILP); Mixed integer programming; Robust learning; Integer programming
GTΔ: Detecting temporal changes in group stochastic processes,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052576732&doi=10.1145%2f3183346&partnerID=40&md5=a38c7deced25e4e967a371fa27f89f36,"Given a portfolio of stocks or a series of frames in a video how do we detect significant changes in a group of values for real-time applications? In this article, we formalize the problem of sequentially detecting temporal changes in a group of stochastic processes. As a solution to this particular problem, we propose the group temporal change (GTΔ) algorithm, a simple yet effective technique for the sequential detection of significant changes in a variety of statistical properties of a group over time. Due to the flexible framework of the GTΔ algorithm, a domain expert is able to select one or more statistical properties that they are interested in monitoring. The usefulness of our proposed algorithm is also demonstrated against state-of-the-art techniques on synthetically generated data as well as on two real-world applications; a portfolio of healthcare stocks over a 20 year period and a video monitoring the activity of our Sun. © 2018 ACM.",Anomaly detection; Group change detection; Time series analysis,Financial markets; Random processes; Stochastic systems; Anomaly detection; Change detection; Flexible framework; Real-time application; Sequential detection; State-of-the-art techniques; Statistical properties; Video monitoring; Time series analysis
Designing size consistent statistics for accurate anomaly detection in dynamic networks,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052550665&doi=10.1145%2f3185059&partnerID=40&md5=09b89a17d5a9412dc4f7a66dcf2d060a,"An important task in network analysis is the detection of anomalous events in a network time series. These events could merely be times of interest in the network timeline or they could be examples of malicious activity or network malfunction. Hypothesis testing using network statistics to summarize the behavior of the network provides a robust framework for the anomaly detection decision process. Unfortunately, choosing network statistics that are dependent on confounding factors like the total number of nodes or edges can lead to incorrect conclusions (e.g., false positives and false negatives). In this article, we describe the challenges that face anomaly detection in dynamic network streams regarding confounding factors. We also provide two solutions to avoiding error due to confounding factors: the first is a randomization testing method that controls for confounding factors, and the second is a set of size-consistent network statistics that avoid confounding due to the most common factors, edge count and node count. © 2018 ACM.",Data mining; Graphs; Statistics,Data mining; Statistics; Testing; Time series analysis; Anomaly detection; Consistent network; Decision process; Graphs; Hypothesis testing; In-network analysis; Malicious activities; Network statistics; Error statistics
Exploring multiobjective optimization for multiview clustering,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052564927&doi=10.1145%2f3182181&partnerID=40&md5=f29530e43737787ba4cfb286d3877823,"We present a new multiview clustering approach based on multiobjective optimization. In contrast to existing clustering algorithms based on multiobjective optimization, it is generally applicable to data represented by two or more views and does not require specifying the number of clusters a priori. The approach builds upon the search capability of a multiobjective simulated annealing based technique, AMOSA, as the underlying optimization technique. In the first version of the proposed approach, an internal cluster validity index is used to assess the quality of different partitionings obtained using different views. A new way of checking the compatibility of these different partitionings is also proposed and this is used as another objective function. A new encoding strategy and some new mutation operators are introduced. Finally, a new way of computing a consensus partitioning from multiple individual partitions obtained on multiple views is proposed. As a baseline and for comparison, two multiobjective based ensemble clustering techniques are proposed to combine the outputs of different simple clustering approaches. The efficacy of the proposed clustering methods is shown for partitioning several real-world datasets having multiple views. To show the practical usefulness of the method, we present results on web-search result clustering, where the task is to find a suitable partitioning of web snippets. © 2018 ACM.",Multiobjective optimization; Multiview classification; Search result clustering; Simulated annealing,Cluster analysis; Multiobjective optimization; Simulated annealing; Cluster validity indices; Clustering approach; Multi-view clustering; Multi-views; Objective functions; Optimization techniques; Real-world datasets; Search result clustering; Clustering algorithms
Generating realistic synthetic population datasets,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052552643&doi=10.1145%2f3182383&partnerID=40&md5=4532400826cd4e90c3e2c800bee38b11,"Modern studies of societal phenomena rely on the availability of large datasets capturing attributes and activities of synthetic, city-level, populations. For instance, in epidemiology, synthetic population datasets are necessary to study disease propagation and intervention measures before implementation. In social science, synthetic population datasets are needed to understand how policy decisions might affect preferences and behaviors of individuals. In public health, synthetic population datasets are necessary to capture diagnostic and procedural characteristics of patient records without violating confidentialities of individuals. To generate such datasets over a large set of categorical variables, we propose the use of the maximum entropy principle to formalize a generative model such that in a statistically well-founded way we can optimally utilize given prior information about the data, and are unbiased otherwise. An efficient inference algorithm is designed to estimate the maximum entropy model, and we demonstrate how our approach is adept at estimating underlying data distributions. We evaluate this approach against both simulated data and US census datasets, and demonstrate its feasibility using an epidemic simulation application. © 2018 ACM.",Maximum entropy models; Multivariate categorical data; Probabilistic modeling; Synthetic population,Diagnosis; Entropy; Inference engines; Categorical data; Categorical variables; Intervention measures; Maximum entropy modeling; Maximum entropy models; Maximum entropy principle; Probabilistic modeling; Synthetic populations; Population statistics
Motif counting beyond five nodes,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052573954&doi=10.1145%2f3186586&partnerID=40&md5=9c36ec309ef95ce4d087f0441d0db8f7,"Counting graphlets is a well-studied problem in graph mining and social network analysis. Recently, several papers explored very simple and natural algorithms based on Monte Carlo sampling of Markov Chains (MC), and reported encouraging results. We show, perhaps surprisingly, that such algorithms are outperformed by color coding (CC) [2], a sophisticated algorithmic technique that we extend to the case of graphlet sampling and for which we prove strong statistical guarantees. Our computational experiments on graphs with millions of nodes show CC to be more accurate than MC; furthermore, we formally show that the mixing time of the MC approach is too high in general, even when the input graph has high conductance. All this comes at a price however. While MC is very efficient in terms of space, CC's memory requirements become demanding when the size of the input graph and that of the graphlets grow. And yet, our experiments show that CC can push the limits of the state-of-the-art, both in terms of the size of the input graph and of that of the graphlets. © 2018 ACM.",Color coding; Graph mining; Motif counting; Subgraph counting,Markov processes; Monte Carlo methods; Algorithmic techniques; Color coding; Computational experiment; Graph mining; Monte Carlo sampling; Motif counting; Statistical guarantee; Subgraphs; Sampling
A session-Based approach to fast-But-Approximate interactive data cube exploration,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042527551&doi=10.1145%2f3070648&partnerID=40&md5=89446ed84e8869bd4c466ead85441ba9,"With the proliferation of large datasets, sampling has become pervasive in data analysis. Sampling has numerous benefits—from reducing the computation time and cost to increasing the scope of interactive analysis. A popular task in data science, well-suited toward sampling, is the computation of fast-but-approximate aggregations over sampled data. Aggregation is a foundational block of data analysis, with data cube being its primary construct. We observe that such aggregation queries are typically issued in an ad-hoc, interactive setting. In contrast to one-off queries, a typical query session consists of a series of quick queries, interspersed with the user inspecting the results and formulating the next query. The similarity between session queries opens up opportunities for reusing computation of not just query results, but also error estimates. Error estimates need to be provided alongside sampled results for the results to be meaningful. We propose Sesame, a rewrite and caching framework that accelerates the entire interactive session of aggregation queries over sampled data. We focus on two unique and computationally expensive aspects of this use case: query speculation in the presence of sampling, and error computation, and provide novel strategies for result and error reuse. We demonstrate that our approach outperforms conventional sampled aggregation techniques by at least an order of magnitude, without modifying the underlying database. © 2018 ACM.",Aggregation; Error reuse; Faceted exploration; Interactive visualization; Session,Agglomeration; Data handling; Information analysis; Query processing; Visualization; Aggregation queries; Aggregation techniques; Computation time; Error computation; Interactive analysis; Interactive session; Interactive visualizations; Session; Errors
Interactive discovery of coordinated relationship chains with maximum entropy models,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042529214&doi=10.1145%2f3047017&partnerID=40&md5=5fdab5c6ee9528a89e74b62346dd615b,"Modern visual analytic tools promote human-in-the-loop analysis but are limited in their ability to direct the user toward interesting and promising directions of study. This problem is especially acute when the analysis task is exploratory in nature, e.g., the discovery of potentially coordinated relationships in massive text datasets. Such tasks are very common in domains like intelligence analysis and security forensics where the goal is to uncover surprising coalitions bridging multiple types of relations. We introduce new maximum entropy models to discover surprising chains of relationships leveraging count data about entity occurrences in documents. These models are embedded in a visual analytic system called MERCER (Maximum Entropy Relational Chain ExploRer) that treats relationship bundles as first class objects and directs the user toward promising lines of inquiry. We demonstrate how user input can judiciously direct analysis toward valid conclusions, whereas a purely algorithmic approach could be led astray. Experimental results on both synthetic and real datasets from the intelligence community are presented. © 2018 ACM.",Interactive visual data exploration; Maximum entropy models; Multi-relational pattern mining,Chains; Visualization; Algorithmic approach; Intelligence analysis; Intelligence communities; Maximum entropy models; Pattern mining; Types of relations; Visual analytic tools; Visual data exploration; Entropy
Partial sum minimization of singular values representation on grassmann manifolds,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042529162&doi=10.1145%2f3092690&partnerID=40&md5=bad3f9988e524274f76e5343d82e12ab,"Clustering is one of the fundamental topics in data mining and pattern recognition. As a prospective clustering method, the subspace clustering has made considerable progress in recent researches, e.g., sparse subspace clustering (SSC) and low rank representation (LRR). However, most existing subspace clustering algorithms are designed for vectorial data from linear spaces, thus not suitable for high-dimensional data with intrinsic non-linear manifold structure. For high-dimensional or manifold data, few research pays attention to clustering problems. The purpose of clustering on manifolds tends to cluster manifold-valued data into several groups according to the mainfold-based similarity metric. This article proposes an extended LRR model for manifold-valued Grassmann data that incorporates prior knowledge by minimizing partial sum of singular values instead of the nuclear norm, namely Partial Sum minimization of Singular Values Representation (GPSSVR). The new model not only enforces the global structure of data in low rank, but also retains important information by minimizing only smaller singular values. To further maintain the local structures among Grassmann points, we also integrate the Laplacian penalty with GPSSVR. The proposed model and algorithms are assessed on a public human face dataset, some widely used human action video datasets and a real scenery dataset. The experimental results show that the proposed methods obviously outperform other state-of-the-art methods. © 2018 ACM.",,Data mining; Pattern recognition; Clustering problems; High dimensional data; Low-rank representations; Manifold-valued datum; Model and algorithms; Nonlinear manifolds; State-of-the-art methods; Sub-Space Clustering; Clustering algorithms
Visual analysis of brain networks using sparse regression models,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042544077&doi=10.1145%2f3023363&partnerID=40&md5=ba55af77fe1ce2544914b618a1742a2e,"Studies of the human brain network are becoming increasingly popular in the fields of neuroscience, computer science, and neurology. Despite this rapidly growing line of research, gaps remain on the intersection of data analytics, interactive visual representation, and the human intelligence—all needed to advance our understanding of human brain networks. This article tackles this challenge by exploring the design space of visual analytics. We propose an integrated framework to orchestrate computational models with comprehensive data visualizations on the human brain network. The framework targets two fundamental tasks: the visual exploration of multi-label brain networks and the visual comparison among brain networks across different subject groups. During the first task, we propose a novel interactive user interface to visualize sets of labeled brain networks; in our second task, we introduce sparse regression models to select discriminative features from the brain network to facilitate the comparison. Through user studies and quantitative experiments, both methods are shown to greatly improve the visual comparison performance. Finally, real-world case studies with domain experts demonstrate the utility and effectiveness of our framework to analyze reconstructions of human brain connectivity maps. The perceptually optimized visualization design and the feature selection model calibration are shown to be the key to our significant findings. © 2018 ACM.",Brain network; Connectome; Feature selection; Visual analysis,Brain; Neurology; Regression analysis; User interfaces; Visualization; Brain networks; Connectome; Discriminative features; Interactive user interfaces; Quantitative experiments; Visual analysis; Visual representations; Visualization designs; Feature extraction
VisIRR: A visual analytics system for information retrieval and recommendation for large-Scale document data,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042531976&doi=10.1145%2f3070616&partnerID=40&md5=69f7c99ca3547fa289a69ac6016e3f71,"In this article, we present an interactive visual information retrieval and recommendation system, called VisIRR, for large-scale document discovery. VisIRR effectively combines the paradigms of (1) a passive pull through query processes for retrieval and (2) an active push that recommends items of potential interest to users based on their preferences. Equipped with an efficient dynamic query interface against a large-scale corpus, VisIRR organizes the retrieved documents into high-level topics and visualizes them in a 2D space, representing the relationships among the topics along with their keyword summary. In addition, based on interactive personalized preference feedback with regard to documents, VisIRR provides document recommendations from the entire corpus, which are beyond the retrieved sets. Such recommended documents are visualized in the same space as the retrieved documents, so that users can seamlessly analyze both existing and newly recommended ones. This article presents novel computational methods, which make these integrated representations and fast interactions possible for a large-scale document corpus. We illustrate how the system works by providing detailed usage scenarios. Additionally, we present preliminary user study results for evaluating the effectiveness of the system. © 2018 ACM.",Clustering; Dimension reduction; Information retrieval; Recommendation; Topic modeling,Information retrieval; Recommender systems; Clustering; Dimension reduction; Document recommendation; Integrated representations; Recommendation; Topic Modeling; Visual analytics systems; Visual information retrieval; Search engines
Memory-Efficient and accurate sampling for counting local triangles in graph streams: From simple to multigraphs,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042533513&doi=10.1145%2f3022186&partnerID=40&md5=3575a0181b888b4afe6c90ece89998c5,"How can we estimate local triangle counts accurately in a graph stream without storing the whole graph? How to handle duplicated edges in local triangle counting for graph stream? Local triangle counting, which computes the number of triangles attached to each node in a graph, is a very important problem with wide applications in social network analysis, anomaly detection, web mining, and the like. In this article, we propose algorithms for local triangle counting in a graph stream based on edge sampling: Mascot for a simple graph, and MultiBMascot and MultiWMascot for a multigraph. To develop Mascot, we first present two naive local triangle counting algorithms in a graph stream, called Mascot-C and Mascot-A. Mascot-C is based on constant edge sampling, and Mascot-A improves its accuracy by utilizing more memory spaces. Mascot achieves both accuracy and memory-efficiency of the two algorithms by unconditional triangle counting for a new edge, regardless of whether it is sampled or not. Extending the idea to a multigraph, we develop two algorithms MultiBMascot and MultiWMascot. MultiBMascot enables local triangle counting on the corresponding simple graph of a streamed multigraph without explicit graph conversion; MultiWMascot considers repeated occurrences of an edge as its weight and counts each triangle as the product of its three edge weights. In contrast to the existing algorithm that requires prior knowledge on the target graph and appropriately set parameters, our proposed algorithms require only one parameter of edge sampling probability. Through extensive experiments, we show that for the same number of edges sampled, Mascot provides the best accuracy compared to the existing algorithm as well as Mascot-C and Mascot-A. We also demonstrate that MultiBMascot on a multigraph is comparable to Mascot-C on the counterpart simple graph, and MultiWMascot becomes more accurate for higher degree nodes. Thanks to Mascot, we also discover interesting anomalous patterns in real graphs, including core-peripheries in the web, a bimodal call pattern in a phone call history, and intensive collaboration in DBLP. © 2018 ACM.",Anomaly detection; Edge sampling; Graph stream mining; Pharses: local triangle counting,Directed graphs; Anomaly detection; Core peripheries; Explicit graphs; Memory efficiency; Memory efficient; Number of triangles; Pharses: local triangle counting; Stream mining; Graph theory
ATR-Vis: Visual and interactive information retrieval for parliamentary discussions in twitter,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042525214&doi=10.1145%2f3047010&partnerID=40&md5=918ab7ee7f6ff0b5ff1c52429b5da7f5,"The worldwide adoption of Twitter turned it into one of the most popular platforms for content analysis as it serves as a gauge of the public’s feeling and opinion on a variety of topics. This is particularly true of political discussions and lawmakers’ actions and initiatives. Yet, one common but unrealistic assumption is that the data of interest for analysis is readily available in a comprehensive and accurate form. Data need to be retrieved, but due to the brevity and noisy nature of Twitter content, it is difficult to formulate user queries that match relevant posts that use different terminology without introducing a considerable volume of unwanted content. This problem is aggravated when the analysis must contemplate multiple and related topics of interest, for which comments are being concurrently posted. This article presents Active Tweet Retrieval Visualization (ATR-Vis), a user-driven visual approach for the retrieval of Twitter content applicable to this scenario. The method proposes a set of active retrieval strategies to involve an analyst in such a way that a major improvement in retrieval coverage and precision is attained with minimal user effort. ATR-Vis enables non-technical users to benefit from the aforementioned active learning strategies by providing visual aids to facilitate the requested supervision. This supports the exploration of the space of potentially relevant tweets, and affords a better understanding of the retrieval results. We evaluate our approach in scenarios in which the task is to retrieve tweets related to multiple parliamentary debates within a specific time span. We collected two Twitter datasets, one associated with debates in the Canadian House of Commons during a particular week in May 2014, and another associated with debates in the Brazilian Federal Senate during a selected week in May 2015. The two use cases illustrate the effectiveness of ATR-Vis for the retrieval of relevant tweets, while quantitative results show that our approach achieves high retrieval quality with a modest amount of supervision. Finally, we evaluated our tool with three external users who perform searching in social media as part of their professional work. © 2018 ACM.",Active learning; Information retrieval; Visual analytics,Artificial intelligence; Data mining; Information retrieval; Visualization; Active Learning; Active learning strategies; Canadian House of Commons; Interactive information retrieval; Non-technical users; Quantitative result; Retrieval strategies; Visual analytics; Social networking (online)
Editorial: TKDD special issue on interactive data exploration and analytics,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042544411&doi=10.1145%2f3181707&partnerID=40&md5=9d001aa33743d70f5cd50fca43c77c5a,[No abstract available],,
Data stream evolution diagnosis using recursive wavelet density estimators,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042542931&doi=10.1145%2f3106369&partnerID=40&md5=5c979b55737ef9f5e8bfca6ea26d7cf1,"Data streams are a new class of data that is becoming pervasively important in a wide range of applications, ranging from sensor networks, environmental monitoring to finance. In this article, we propose a novel framework for the online diagnosis of evolution of multidimensional streaming data that incorporates Recursive Wavelet Density Estimators into the context of Velocity Density Estimation. In the proposed framework changes in streaming data are characterized by the use of local and global evolution coefficients. In addition, we propose for the analysis of changes in the correlation structure of the data a recursive implementation of the Pearson correlation coefficient using exponential discounting. Two visualization tools, namely temporal and spatial velocity profiles, are extended in the context of the proposed framework. These are the three main advantages of the proposed method over previous approaches: (1) the memory storage required is minimal and independent of any window size; (2) it has a significantly lower computational complexity; and (3) it makes possible the fast diagnosis of data evolution at all dimensions and at relevant combinations of dimensions with only one pass of the data. With the help of the four examples, we show the framework’s relevance in a change detection context and its potential capability for real world applications. © 2018 Copyright is held by the owner/author(s).",Data stream evolution diagnosis; Data streams mining; Incremental statistics; Velocity density estimation,Correlation methods; Sensor networks; Correlation structure; Data stream; Data streams mining; Environmental Monitoring; Pearson correlation coefficients; Potential capability; Temporal and spatial; Velocity density estimation; Digital storage
Mining redescriptions with siren,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042549456&doi=10.1145%2f3007212&partnerID=40&md5=9b209bac11550301818faa03318a0d73,"In many areas of science, scientists need to find distinct common characterizations of the same objects and, vice versa, to identify sets of objects that admit multiple shared descriptions. For example, in biology, an important task is to identify the bioclimatic constraints that allow some species to survive, that is, to describe geographical regions both in terms of the fauna that inhabits them and of their bioclimatic conditions. In data analysis, the task of automatically generating such alternative characterizations is called redescription mining. If a domain expert wants to use redescription mining in his research, merely being able to find redescriptions is not enough. He must also be able to understand the redescriptions found, adjust them to better match his domain knowledge, test alternative hypotheses with them, and guide the mining process toward results he considers interesting. To facilitate these goals, we introduce Siren, an interactive tool for mining and visualizing redescriptions. Siren allows to obtain redescriptions in an anytime fashion through efficient, distributed mining, to examine the results in various linked visualizations, to interact with the results either directly or via the visualizations, and to guide the mining algorithm toward specific redescriptions. In this article, we explain the features of Siren and why they are useful for redescription mining. We also propose two novel redescription mining algorithms that improve the generalizability of the results compared to the existing ones. © 2018 ACM.",Interactive data mining; Redescription mining; Visual data mining,Geographical regions; Signaling; Sirens; Visualization; Alternative hypothesis; Domain experts; Domain knowledge; Interactive data mining; Interactive tool; Mining algorithms; Mining process; Visual data mining; Data mining
GrammarViz 3.0: Interactive discovery of variable-Length time series patterns,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042545495&doi=10.1145%2f3051126&partnerID=40&md5=403dfbc879a087cfddd7028c28a3406f,"The problems of recurrent and anomalous pattern discovery in time series, e.g., motifs and discords, respectively, have received a lot of attention from researchers in the past decade. However, since the pattern search space is usually intractable, most existing detection algorithms require that the patterns have discriminative characteristics and have its length known in advance and provided as input, which is an unreasonable requirement for many real-world problems. In addition, patterns of similar structure, but of different lengths May co-exist in a time series. Addressing these issues, we have developed algorithms for variable-length time series pattern discovery that are based on symbolic discretization and grammar inference—two techniques whose combination enables the structured reduction of the search space and discovery of the candidate patterns in linear time. In this work, we present GrammarViz 3.0—a software package that provides implementations of proposed algorithms and graphical user interface for interactive variable-length time series pattern discovery. The current version of the software provides an alternative grammar inference algorithm that improves the time series motif discovery workflow, and introduces an experimental procedure for automated discretization parameter selection that builds upon the minimum cardinality maximum cover principle and aids the time series recurrent and anomalous pattern discovery. © 2018 ACM.",Interactive data mining,Data mining; Graphical user interfaces; Inference engines; User interfaces; Candidate patterns; Detection algorithm; Discretization parameters; Experimental procedure; Interactive data mining; Real-world problem; Time series patterns; Time-series motifs; Time series
A viewable indexing structure for the interactive exploration of dynamic and large image collections,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042523112&doi=10.1145%2f3047011&partnerID=40&md5=c165eaf1f739055533ac7f23454e07e2,"Thanks to the capturing devices cost reduction and the advent of social networks, the size of image collections is becoming extremely huge. Many works in the literature have addressed the indexing of large image collections for search purposes. However, there is a lack of support for exploratory data mining. One May want to wander around the images and experience serendipity in the exploration process. Thus, effective paradigms not only for organising, but also visualising these image collections become necessary. In this article, we present a study to jointly index and visualise large image collections. The work focuses on satisfying three constraints. First, large image collections, up to million of images, shall be handled. Second, dynamic collections, such as ever-growing collections, shall be processed in an incremental way, without reprocessing the whole collection at each modification. Finally, an intuitive and interactive exploration system shall be provided to the user to allow him to easily mine image collections. To this end, a data partitioning algorithm has been modified and proximity graphs have been used to fit the visualisation purpose. A custom web platform has been implemented to visualise the hierarchical and graph-based hybrid structure. The results of a user evaluation we have conducted show that the exploration of the collections is intuitive and smooth thanks to the proposed structure. Furthermore, the scalability of the proposed indexing method is proved using large public image collections. © 2018 ACM.",Dynamic collections; Incremental construction; Interactive visualisation; Large image collections,Cost reduction; Data mining; Graphic methods; Visualization; Data partitioning algorithms; Exploration process; Exploratory data mining; Incremental construction; Indexing structures; Interactive exploration; Interactive visualisation; Large images; Indexing (of information)
Learning to infer competitive relationships in heterogeneous networks,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042522739&doi=10.1145%2f3051127&partnerID=40&md5=d6ce219d3c77660bb29637ae622ad3a9,"Detecting and monitoring competitors is fundamental to a company to stay ahead in the global market. Existing studies mainly focus on mining competitive relationships within a single data source, while competing information is usually distributed in multiple networks. How to discover the underlying patterns and utilize the heterogeneous knowledge to avoid biased aspects in this issue is a challenging problem. In this article, we study the problem of mining competitive relationships by learning across heterogeneous networks. We use Twitter and patent records as our data sources and statistically study the patterns behind the competitive relationships. We find that the two networks exhibit different but complementary patterns of competitions. Overall, we find that similar entities tend to be competitors, with a probability of 4 times higher than chance. On the other hand, in social network, we also find a 10 minutes phenomenon: when two entities are mentioned by the same user within 10 minutes, the likelihood of them being competitors is 25 times higher than chance. Based on the discovered patterns, we propose a novel Topical Factor Graph Model. Generally, our model defines a latent topic layer to bridge the Twitter network and patent network. It then employs a semi-supervised learning algorithm to classify the relationships between entities (e.g., companies or products). We test the proposed model on two real data sets and the experimental results validate the effectiveness of our model, with an average of +46% improvement over alternative methods. Besides, we further demonstrate the competitive relationships inferred by our proposed model can be applied in the job-hopping prediction problem by achieving an average of +10.7% improvement. © 2018 ACM.",Competitive relationship; Heterogeneous network; Social network,Heterogeneous networks; International trade; Patents and inventions; Social networking (online); Supervised learning; Competitive relationship; Heterogeneous Knowledge; Multiple networks; Patent network; Prediction problem; Real data sets; Relationships between entities; Twitter networks; Learning algorithms
ABRA: Approximating betweenness centrality in static and dynamic graphs with rademacher averages,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051454831&doi=10.1145%2f3208351&partnerID=40&md5=124f0381332743dd0625a7fd2b5bb068,"We present ABRA, a suite of algorithms to compute and maintain probabilistically guaranteed high-quality approximations of the betweenness centrality of all nodes (or edges) on both static and fully dynamic graphs. Our algorithms use progressive random sampling and their analysis rely on Rademacher averages and pseudodimension, fundamental concepts from statistical learning theory. To our knowledge, ABRA is the first application of these concepts to the field of graph analysis. Our experimental results show that ABRA is much faster than exact methods, and vastly outperforms, in both runtime number of samples, and accuracy, state-of-The-Art algorithms with the same quality guarantees. © 2018 ACM.",Centrality measures; Pseudodimension; Statistical learning theory; Uniform bounds,Sampling; Betweenness centrality; Centrality measures; Fundamental concepts; Number of samples; Pseudodimension; State-of-the-art algorithms; Statistical learning theory; Uniform bounds; Approximation algorithms
ProgressER: Adaptive progressive approach to relational entity resolution,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047009428&doi=10.1145%2f3154410&partnerID=40&md5=9350de121f8844875d5df805714feb1f,"Entity resolution (ER) is the process of identifying which entities in a dataset refer to the same real-world object. In relational ER, the dataset consists of multiple entity-sets and relationships among them. Such relationships cause the resolution of some entities to influence the resolution of other entities. For instance, consider a relational dataset that consists of a set of research paper entities and a set of venue entities. In such a dataset, deciding that two research papers are the same may trigger the fact that their venues are also the same. This article proposes a progressive approach to relational ER, named ProgressER, that aims to produce the highest quality result given a constraint on the resolution budget, specified by the user. Such a progressive approach is useful for many emerging analytical applications that require low latency response (and thus cannot tolerate delays caused by cleaning the entire dataset) and/or in situations where the underlying resources are constrained or costly to use. To maximize the quality of the result, ProgressER follows an adaptive strategy that periodically monitors and reassesses the resolution progress to determine which parts of the dataset should be resolved next and how they should be resolved. More specifically, ProgressER divides the input budget into several resolution windows and analyzes the resolution progress at the beginning of each window to generate a resolution plan for the current window. A resolution plan specifies which blocks of entities and which entity pairs within blocks need to be resolved during the plan execution phase of that window. In addition, ProgressER specifies, for each identified pair of entities, the order in which the similarity functions should be applied on the pair. Such an order plays a significant role in reducing the overall cost because applying the first few functions in this order might be sufficient to resolve the pair. The empirical evaluation of ProgressER demonstrates its significant advantage in terms of progressiveness over the traditional ER techniques for the given problem settings. © 2018 ACM.",Collective entity resolution; Data cleaning; Entity resolution; Progressive computation; Relational entity resolution; Resolution plan; Resolution workflow,Computer science; Data mining; Adaptive strategy; Analytical applications; Data cleaning; Empirical evaluations; Entity resolutions; Real-world objects; Research papers; Similarity functions; Budget control
G-RoI: Automatic region-of-interest detection driven by geotagged social media data,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042539261&doi=10.1145%2f3154411&partnerID=40&md5=bdd98f13b9c1675e0b882405058c6cea,"Geotagged data gathered from social media can be used to discover interesting locations visited by users called Places-of-Interest (PoIs). Since a PoI is generally identified by the geographical coordinates of a single point, it is hard to match it with user trajectories. Therefore, it is useful to define an area, called Region-of-Interest (RoI), to represent the boundaries of the PoI’s area. RoI mining techniques are aimed at discovering ROIs from PoIs and other data. Existing RoI mining techniques are based on three main approaches: predefined shapes, density-based clustering, and grid-based aggregation. This article proposes G-RoI, a novel RoI mining technique that exploits the indications contained in geotagged social media items to discover RoIs with a high accuracy. Experiments performed over a set of PoIs in Rome and Paris using social media geotagged data, demonstrate that G-RoI in most cases achieves better results than existing techniques. In particular, the mean F1 score is 0.34 higher than that obtained with the well-known DBSCAN algorithm in Rome RoIs and 0.23 higher in Paris RoIs. © 2018 ACM.",Geotagged social media; Places-of-Interest; Regions-of-interest; RoI mining; Social network analysis,Social networking (online); Density-based Clustering; Geographical coordinates; Mining techniques; Places-of-Interest; Region of interest; Regions of interest; Social media; Social media datum; Image segmentation
Function-on-function regression with mode-sparsity regularization,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047017452&doi=10.1145%2f3178113&partnerID=40&md5=5dae7dcf3e9af932c5e0ead4f45c986c,"Functional data is ubiquitous in many domains, such as healthcare, social media, manufacturing process, sensor networks, and so on. The goal of function-on-function regression is to build a mapping from functional predictors to functional response. In this article, we propose a novel function-on-function regression model based on mode-sparsity regularization. The main idea is to represent the regression coefficient function between predictor and response as the double expansion of basis functions, and then use a mode-sparsity regularization to automatically filter out irrelevant basis functions for both predictors and responses. The proposed approach is further extended to the tensor version to accommodate multiple functional predictors. While allowing the dimensionality of the regression weight matrix or tensor to be relatively large, the mode-sparsity regularized model facilitates the multi-way shrinking of basis functions for each mode. The proposed mode-sparsity regularization covers a wide spectrum of sparse models for function-on-function regression. The resulting optimization problem is challenging due to the non-smooth property of the mode-sparsity regularization. We develop an efficient algorithm to solve the problem, which works in an iterative update fashion, and converges to the global optimum. Furthermore, we analyze the generalization performance of the proposed method and derive an upper bound for the consistency between the recovered function and the underlying true function. The effectiveness of the proposed approach is verified on benchmark functional datasets in various domains. © 2018 ACM.",Function-on-function regression; Mode-sparsity regularization,Functions; Iterative methods; Sensor networks; Tensors; Function regression; Functional response; Generalization performance; Manufacturing process; Optimization problems; Regression coefficient; Regression weights; Sparsity regularizations; Regression analysis
Mining event-oriented topics in microblog stream with unsupervised multi-view hierarchical embedding,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046998483&doi=10.1145%2f3173044&partnerID=40&md5=742ad12b922d5451aacecf39dcf7a6f0,"This article presents an unsupervised multi-view hierarchical embedding (UMHE) framework to sufficiently reveal the intrinsic topical knowledge in social events. Event-oriented topics are highly related to such events as it can provide explicit descriptions of what have happened in social community. In many real-world cases, however, it is difficult to include all attributes of microblogs, more often, textual aspects only are available. Traditional topic modelling methods have failed to generate event-oriented topics with the textual aspects, since the inherent relations between topics are often overlooked in these methods. Meanwhile, the metrics in original word vocabulary space might not effectively capture semantic distances. Our UMHE framework overcomes the severe information deficiency and poor feature representation. The UMHE first develops a multi-view Bayesian rose tree to preliminarily generate prior knowledge for latent topics and their relations. With such prior knowledge, we design an unsupervised translation-based hierarchical embedding method to make a better representation of these latent topics. By applying self-adaptive spectral clustering on the embedding space and the original space concomitantly, we eventually extract event-oriented topics in word distributions to express social events. Our framework is purely data-driven and unsupervised, without any external knowledge. Experimental results on TREC Tweets2011 dataset and Sina Weibo dataset demonstrate that the UMHE framework can construct hierarchical structure with high fitness, but also yield topic embeddings with salient semantics; therefore, it can derive event-oriented topics with meaningful descriptions. © 2018 ACM.",Bayesian rose tree; Event-oriented topic; Multi-view hierarchical embedding; Unsupervised learning,Clustering algorithms; Semantics; Unsupervised learning; Event-oriented; External knowledge; Feature representation; Hierarchical structures; Multi-views; Rose tree; Social communities; Spectral clustering; Data mining
Emerging trends in personality identification using online social networks—A literature survey,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042551044&doi=10.1145%2f3070645&partnerID=40&md5=39f112f406aaa025007601c61c49f0c8,"Personality is a combination of all the attributes—behavioral, temperamental, emotional, and mental—that characterizes a unique individual. Ability to identify personalities of people has always been of great interest to the researchers due to its importance. It continues to find highly useful applications in many domains. Owing to the increasing popularity of online social networks, researchers have started looking into the possibility of predicting a user’s personality from his online social networking profile, which serves as a rich source of textual as well as non-textual content published by users. In the process of creating social networking profiles, users reveal a lot about themselves both in what they share and how they say it. Studies suggest that the online social networking websites are, in fact, a relevant and valid means of communicating personality. In this article, we review these various studies reported in literature toward identification of personality using online social networks. To the best of our knowledge, this is the first reported survey of its kind at the time of submission. We hope that our contribution, especially in summarizing the previous findings and in identifying the directions for future research in this area, would encourage researchers to do more work in this budding area. © 2018 ACM.",Facebook; Mining social network; Online social network; Personality; Personality prediction; Twitter,Online systems; Surveys; Websites; Facebook; On-line social networks; Personality; Personality predictions; Twitter; Social networking (online)
Collaborative filtering with topic and social latent factors incorporating implicit feedback,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042545167&doi=10.1145%2f3127873&partnerID=40&md5=913904a1be2b21cc59af63997699f1d5,"Recommender systems (RSs) provide an effective way of alleviating the information overload problem by selecting personalized items for different users. Latent factors-based collaborative filtering (CF) has become the popular approaches for RSs due to its accuracy and scalability. Recently, online social networks and user-generated content provide diverse sources for recommendation beyond ratings. Although social matrix factorization (Social MF) and topic matrix factorization (Topic MF) successfully exploit social relations and item reviews, respectively; both of them ignore some useful information. In this article, we investigate the effective data fusion by combining the aforementioned approaches. First, we propose a novel model MR3 to jointly model three sources of information (i.e., ratings, item reviews, and social relations) effectively for rating prediction by aligning the latent factors and hidden topics. Second, we incorporate the implicit feedback from ratings into the proposed model to enhance its capability and to demonstrate its flexibility. We achieve more accurate rating prediction on real-life datasets over various state-of-the-art methods. Furthermore, we measure the contribution from each of the three data sources and the impact of implicit feedback from ratings, followed by the sensitivity analysis of hyperparameters. Empirical studies demonstrate the effectiveness and efficacy of our proposed model and its extension. © 2018 ACM.",Collaborative filtering; Hidden topics; Implicit feedback; Latent social factors; Recommendation systems,Data fusion; Factorization; Matrix algebra; Recommender systems; Sensitivity analysis; Social networking (online); Hidden topics; Implicit feedback; Matrix factorizations; On-line social networks; Social factor; Sources of informations; State-of-the-art methods; User-generated content; Collaborative filtering
Systematic review of clustering high-Dimensional and large datasets,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042548793&doi=10.1145%2f3132088&partnerID=40&md5=ba7381544cbf0c1c54a09afa93ae4593,"Technological advancement has enabled us to store and process huge amount of data in relatively short spans of time. The nature of data is rapidly changing, particularly its dimensionality is more commonly multi- and high-dimensional. There is an immediate need to expand our focus to include analysis of high-dimensional and large datasets. Data analysis is becoming a mammoth task, due to incremental increase in data volume and complexity in terms of heterogony of data. It is due to this dynamic computing environment that the existing techniques either need to be modified or discarded to handle new data in multiple high-dimensions. Data clustering is a tool that is used in many disciplines, including data mining, so that meaningful knowledge can be extracted from seemingly unstructured data. The aim of this article is to understand the problem of clustering and various approaches addressing this problem. This article discusses the process of clustering from both microviews (data treating) and macroviews (overall clustering process). Different distance and similarity measures, which form the cornerstone of effective data clustering, are also identified. Further, an in-depth analysis of different clustering approaches focused on data mining, dealing with large-scale datasets is given. These approaches are comprehensively compared to bring out a clear differentiation among them. This article also surveys the problem of high-dimensional data and the existing approaches, that makes it more relevant. It also explores the latest trends in cluster analysis, and the real-life applications of this concept. This survey is exhaustive as it tries to cover all the aspects of clustering in the field of data mining. © 2018 ACM.",Cluster analysis; Clustering tendency; Data clustering applications; Data clustering process; Dimensionality reduction; Large scale data mining,Clustering algorithms; Data mining; Surveys; Clustering tendency; Computing environments; Data clustering; Dimensionality reduction; High dimensional data; Large scale data; Real-life applications; Technological advancement; Cluster analysis
Multi-view low-rank analysis with applications to outlier detection,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047017661&doi=10.1145%2f3168363&partnerID=40&md5=ea805fe4ac1468eac74cfaff99a5d371,"Detecting outliers or anomalies is a fundamental problem in various machine learning and data mining applications. Conventional outlier detection algorithms are mainly designed for single-view data. Nowadays, data can be easily collected from multiple views, and many learning tasks such as clustering and classification have benefited from multi-view data. However, outlier detection from multi-view data is still a very challenging problem, as the data in multiple views usually have more complicated distributions and exhibit inconsistent behaviors. To address this problem, we propose a multi-view low-rank analysis (MLRA) framework for outlier detection in this article. MLRA pursuits outliers from a new perspective, robust data representation. It contains two major components. First, the cross-view low-rank coding is performed to reveal the intrinsic structures of data. In particular, we formulate a regularized rank-minimization problem, which is solved by an efficient optimization algorithm. Second, the outliers are identified through an outlier score estimation procedure. Different from the existing multi-view outlier detection methods, MLRA is able to detect two different types of outliers from multiple views simultaneously. To this end, we design a criterion to estimate the outlier scores by analyzing the obtained representation coefficients. Moreover, we extend MLRA to tackle the multi-view group outlier detection problem. Extensive evaluations on seven UCI datasets, the MovieLens, the USPS-MNIST, and the WebKB datasets demon strate that our approach outperforms several state-of-the-art outlier detection methods. © 2018 ACM.",Low-rank matrix recovery; Multi-view learning; Outlier detection,Data handling; Data mining; Learning systems; Data mining applications; Estimation procedures; Intrinsic structures; Low-rank matrix recoveries; Multi-view learning; Optimization algorithms; Outlier Detection; Outlier detection algorithm; Statistics
GOOWE: Geometrically optimum and online-Weighted ensemble classifier for evolving data streams,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042528576&doi=10.1145%2f3139240&partnerID=40&md5=e2082a548315f9eb30159744c4dc4e40,"Designing adaptive classifiers for an evolving data stream is a challenging task due to the data size and its dynamically changing nature. Combining individual classifiers in an online setting, the ensemble approach, is a well-known solution. It is possible that a subset of classifiers in the ensemble outperforms others in a time-varying fashion. However, optimum weight assignment for component classifiers is a problem, which is not yet fully addressed in online evolving environments. We propose a novel data stream ensemble classifier, called Geometrically Optimum and Online-Weighted Ensemble (GOOWE), which assigns optimum weights to the component classifiers using a sliding window containing the most recent data instances. We map vote scores of individual classifiers and true class labels into a spatial environment. Based on the Euclidean distance between vote scores and ideal-points, and using the linear least squares (LSQ) solution, we present a novel, dynamic, and online weighting approach. While LSQ is used for batch mode ensemble classifiers, it is the first time that we adapt and use it for online environments by providing a spatial modeling of online ensembles. In order to show the robustness of the proposed algorithm, we use real-world datasets and synthetic data generators using the Massive Online Analysis (MOA) libraries. First, we analyze the impact of our weighting system on prediction accuracy through two scenarios. Second, we compare GOOWE with eight state-of-the-art ensemble classifiers in a comprehensive experimental environment. Our experiments show that GOOWE provides improved reactions to different types of concept drift compared to our baselines. The statistical tests indicate a significant improvement in accuracy, with conservative time and memory requirements. © 2018 ACM.",Concept drift; Dynamic weighting; Ensemble classifier; Evolving data stream; Geometry of voting; Least squares; Spatial modeling for online ensembles,Computer science; Data mining; Concept drifts; Ensemble classifiers; Evolving datum; Geometry of voting; Least Square; Spatial modeling; Classification (of information)
Large-Scale Bayesian probabilistic matrix factorization with memo-free distributed variational inference,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042541646&doi=10.1145%2f3161886&partnerID=40&md5=c40fe0de4435100a47b3d1e17e0a114e,"Bayesian Probabilistic Matrix Factorization (BPMF) is a powerful model in many dyadic data prediction problems, especially the applications of Recommender system. However, its poor scalability has limited its wide applications on massive data. Based on the conditional independence property of observed entries in BPMF model, we propose a novel distributed memo-free variational inference method for large-scale matrix factorization problems. Compared with the state-of-the-art methods, the proposed method is favored for several attractive properties. Specifically, it does not require tuning of learning rate carefully, shuffling the training set at each iteration, or storing massive redundant variables, and can introduce new agents into the computations on the fly. We conduct extensive experiments on both synthetic and real-world datasets. The experimental results show that our method can converge significantly faster with better prediction performance than alternative algorithms. © 2017 ACM.",BPMF; Distributed computation; Large-scale; Memo-free variational inference; Recommender system,Distributed computer systems; Factorization; Iterative methods; Recommender systems; BPMF; Conditional independences; Distributed computations; Large-scale; Probabilistic matrix factorizations; State-of-the-art methods; Variational inference; Variational inference methods; Matrix algebra
Will triadic closure strengthen ties in social networks?,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042544004&doi=10.1145%2f3154399&partnerID=40&md5=82421a1230fe366f115b7aaacf15d837,"The social triad—a group of three people—is one of the simplest and most fundamental social groups. Extensive network and social theories have been developed to understand its structure, such as triadic closure and social balance. Over the course of a triadic closure—the transition from two ties to three among three users, the strength dynamics of its social ties, however, are much less well understood. Using two dynamic networks from social media and mobile communication, we examine how the formation of the third tie in a triad affects the strength of the existing two ties. Surprisingly, we find that in about 80% social triads, the strength of the first two ties is weakened although averagely the tie strength in the two networks maintains an increasing or stable trend. We discover that (1) the decrease in tie strength among three males is more sharply than that among females, and (2) the tie strength between celebrities is more likely to be weakened as the closure of a triad than those between ordinary people. Furthermore, we formalize a triadic tie strength dynamics prediction problem to infer whether social ties of a triad will become weakened after its closure. We propose a TRIST method—a kernel density estimation (KDE)-based graphical model—to solve the problem by incorporating user demographics, temporal effects, and structural information. Extensive experiments demonstrate that TRIST offers a greater than 82% potential predictability for inferring triadic tie strength dynamics in both networks. The leveraging of the KDE and structural correlations enables TRIST to outperform baselines by up to 30% in terms of F1-score. © 2018 ACM.",Dynamics; Predictive model; Social triad; Tie strength,Social networking (online); Kernel Density Estimation; Mobile communications; Prediction problem; Predictive modeling; Social triad; Structural correlation; Structural information; Tie strengths; Dynamics
Twitter geolocation: A hybrid approach,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047017710&doi=10.1145%2f3178112&partnerID=40&md5=3e8e884f972a00774fff837e79b886b3,"Geotagging Twitter messages is an important tool for event detection and enrichment. Despite the availability of both social media content and user network information, these two features are generally utilized separately in the methodology. In this article, we create a hybrid method that uses Twitter content and network information jointly as model features. We use Gaussian mixture models to map the raw spatial distribution of the model features to a predicted field. This approach is scalable to large datasets and provides a natural representation of model confidence. Our method is tested against other approaches and we achieve greater prediction accuracy. The model also improves both precision and coverage. © 2018 Association for Computing Machinery. All rights reserved.",Comprehensive accuracy error; Gaussian mixture model; Geotag; Prediction region area; Simple accuracy error; Twitter,Communication channels (information theory); Gaussian distribution; Object recognition; Accuracy errors; Gaussian Mixture Model; Geo-tags; Modeling features; Natural representation; Network information; Prediction accuracy; Twitter; Social networking (online)
Prioritized relationship analysis in heterogeneous information networks,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042523949&doi=10.1145%2f3154401&partnerID=40&md5=8c1e1cc0e55c3b81abd60bc31280d908,"An increasing number of applications are modeled and analyzed in network form, where nodes represent entities of interest and edges represent interactions or relationships between entities. Commonly, such relationship analysis tools assume homogeneity in both node type and edge type. Recent research has sought to redress the assumption of homogeneity and focused on mining heterogeneous information networks (HINs) where both nodes and edges can be of different types. Building on such efforts, in this work, we articulate a novel approach for mining relationships across entities in such networks while accounting for user preference over relationship type and interestingness metric. We formalize the problem as a top-k lightest paths problem, contextualized in a real-world communication network, and seek to find the k most interesting path instances matching the preferred relationship type. Our solution, PROphetic HEuristic Algorithm for Path Searching (PRO-HEAPS), leverages a combination of novel graph preprocessing techniques, well-designed heuristics and the venerable A* search algorithm. We run our algorithm on real-world large-scale graphs and show that our algorithm significantly outperforms a wide variety of baseline approaches with speedups as large as 100X. To widen the range of applications, we also extend PRO-HEAPS to (i) support relationship analysis between two groups of entities and (ii) allow pattern path in the query to contain logical statements with operators AND, OR, NOT, and wild-card “.”. We run experiments using this generalized version of PRO-HEAPS and demonstrate that the advantage of PRO-HEAPS becomes even more pronounced for these general cases. Furthermore, we conduct a comprehensive analysis to study how the performance of PRO-HEAPS varies with respect to various attributes of the input HIN. We finally conduct a case study to demonstrate valuable applications of our algorithm. © 2018 ACM.",Graph algorithms; Heterogeneous information networks; Semantic relationship queries,Heuristic algorithms; Semantics; Comprehensive analysis; Graph algorithms; Heterogeneous information; Logical statements; Preprocessing techniques; Relationship analysis; Relationships between entities; Semantic relationships; Information services
De-anonymizing clustered social networks by percolation graph matching,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042531729&doi=10.1145%2f3127876&partnerID=40&md5=24b839b9db910cb29b094b29344f2125,"Online social networks offer the opportunity to collect a huge amount of valuable information about billions of users. The analysis of this data by service providers and unintended third parties are posing serious treats to user privacy. In particular, recent work has shown that users participating in more than one online social network can be identified based only on the structure of their links to other users. An effective tool to de-anonymize social network users is represented by graph matching algorithms. Indeed, by exploiting a sufficiently large set of seed nodes, a percolation process can correctly match almost all nodes across the different social networks. In this article, we show the crucial role of clustering, which is a relevant feature of social network graphs (and many other systems). Clustering has both the effect of making matching algorithms more prone to errors, and the potential to greatly reduce the number of seeds needed to trigger percolation. We show these facts by considering a fairly general class of random geometric graphs with variable clustering level. We assume that seeds can be identified in particular sub-regions of the network graph, while no a priori knowledge about the location of the other nodes is required. Under these conditions, we show how clever algorithms can achieve surprisingly good performance while limiting the number of matching errors. © 2018 ACM.",Bootstrap percolation; De-anonymization; Graph matching,Clustering algorithms; Online systems; Pattern matching; Solvents; Websites; Anonymization; Bootstrap percolations; Graph matchings; Graph-matching algorithms; Matching algorithm; On-line social networks; Percolation process; Random geometric graphs; Social networking (online)
Tied kronecker product graph models to capture variance in network populations,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047016729&doi=10.1145%2f3161885&partnerID=40&md5=ba0fe433af83ba616a1e8e656f2b7da2,"Much of the past work on mining and modeling networks has focused on understanding the observed properties of single example graphs. However, in many real-life applications it is important to characterize the structure of populations of graphs. In this work, we analyze the distributional properties of probabilistic generative graph models (PGGMs) for network populations. PGGMs are statistical methods that model the network distribution and match common characteristics of real-world networks. Specifically, we show that most PGGMs cannot reflect the natural variability in graph properties observed across multiple networks because their edge generation process assumes independence among edges. Then, we propose the mixed Kronecker Product Graph Model (mKPGM), a scalable generalization of KPGMs that uses tied parameters to increase the variability of the sampled networks, while preserving the edge probabilities in expectation. We compare mKPGM to several other graph models. The results show that learned mKPGMs accurately represent the characteristics of real-world networks, while also effectively capturing the natural variability in network structure. © 2018 ACM.",Graph generation models; Kronecker product graph models; Social network analysis,Computer science; Data mining; Social networking (online); Distributional property; Generation process; Graph generation; Kronecker product; Natural variability; Network distributions; Real-life applications; Real-world networks; Graph theory
"Fast, accurate, and flexible algorithms for dense subtensor mining",2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042545195&doi=10.1145%2f3154414&partnerID=40&md5=b8c39b138d12990a1072fd7c3945b76d,"Given a large-scale and high-order tensor, how can we detect dense subtensors in it? Can we spot them in near-linear time but with quality guarantees? Extensive previous work has shown that dense subtensors, as well as dense subgraphs, indicate anomalous or fraudulent behavior (e.g., lockstep behavior in social networks). However, available algorithms for detecting dense subtensors are not satisfactory in terms of speed, accuracy, and flexibility. In this work, we propose two algorithms, called M-Zoom and M-Biz, for fast and accurate dense-subtensor detection with various density measures. M-Zoom gives a lower bound on the density of detected subtensors, while M-Biz guarantees the local optimality of detected subtensors. M-Zoom and M-Biz can be combined, giving the following advantages: (1) Scalable: scale near-linearly with all aspects of tensors and are up to 114× faster than state-of-the-art methods with similar accuracy, (2) Provably accurate: provide a guarantee on the lowest density and local optimality of the subtensors they find, (3) Flexible: support multi-subtensor detection and size bounds as well as diverse density measures, and (4) Effective: successfully detected edit wars and bot activities in Wikipedia, and spotted network attacks from a TCP dump with near-perfect accuracy (AUC = 0.98). © 2018 ACM.",Anomaly detection; Dense subtensor; Fraud detection; Tensor,Computer science; Data mining; Anomaly detection; Dense sub-graphs; Dense subtensor; Fraud detection; High order tensors; Near-linear time; State-of-the-art methods; Various densities; Tensors
Joint representation learning for location-Based social networks with multi-Grained sequential contexts,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042536682&doi=10.1145%2f3127875&partnerID=40&md5=bbb48997730da0d2e7e00ae3ec898dc3,"This article studies the problem of learning effective representations for Location-Based Social Networks (LBSN), which is useful in many tasks such as location recommendation and link prediction. Existing network embedding methods mainly focus on capturing topology patterns reflected in social connections, while check-in sequences, the most important data type in LBSNs, are not directly modeled by these models. In this article, we propose a representation learning method for LBSNs called as JRLM++, which models check-in sequences together with social connections. To capture sequential relatedness, JRLM++ characterizes two levels of sequential contexts, namely fine-grained and coarse-grained contexts. We present a learning algorithm tailored to the hierarchical architecture of the proposed model. We conduct extensive experiments on two important applications using real-world datasets. The experimental results demonstrate the superiority of our model. The proposed model can generate effective representations for both users and locations in the same embedding space, which can be further utilized to improve multiple LBSN tasks. © 2018 ACM.",Check-in sequences; Contextual information; Distributed representation; Location recommendation; Social link prediction,Location; Check-in; Contextual information; Distributed representation; Hierarchical architectures; Link prediction; Location-based social networks; Real-world datasets; Social connection; Learning algorithms
Continuous-time user modeling in presence of badges: A probabilistic approach,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047018055&doi=10.1145%2f3162050&partnerID=40&md5=de2eef8b88f59962f46adf9624016e5e,"User modeling plays an important role in delivering customized web services to the users and improving their engagement. However, most user models in the literature do not explicitly consider the temporal behavior of users. More recently, continuous-time user modeling has gained considerable attention and many user behavior models have been proposed based on temporal point processes. However, typical point process-based models often considered the impact of peer influence and content on the user participation and neglected other factors. Gamification elements are among those factors that are neglected, while they have a strong impact on user participation in online services. In this article, we propose interdependent multi-dimensional temporal point processes that capture the impact of badges on user participation besides the peer influence and content factors. We extend the proposed processes to model user actions over the community-based question and answering websites, and propose an inference algorithm based on Variational-Expectation Maximization that can efficiently learn the model parameters. Extensive experiments on both synthetic and real data gathered from Stack Overflow show that our inference algorithm learns the parameters efficiently and the proposed method can better predict the user behavior compared to the alternatives. © 2018 ACM.",Badge; Gamification; Stack overflow; Temporal point process; User modeling; User profiling; Variational EM,Continuous time systems; Inference engines; Maximum principle; Parameter estimation; Web services; Websites; Badge; Gamification; Point process; Stack overflow; User Modeling; User profiling; Variational EM; Behavioral research
Community detection using diffusion information,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042550958&doi=10.1145%2f3110215&partnerID=40&md5=152071eacc9d5b7e6ac10380d9e8a2ec,"Community detection in social networks has become a popular topic of research during the last decade. There exist a variety of algorithms for modularizing the network graph into different communities. However, they mostly assume that partial or complete information of the network graphs are available that is not feasible in many cases. In this article, we focus on detecting communities by exploiting their diffusion information. To this end, we utilize the Conditional Random Fields (CRF) to discover the community structures. The proposed method, community diffusion (CoDi), does not require any prior knowledge about the network structure or specific properties of communities. Furthermore, in contrast to the structure-based community detection methods, this method is able to identify the hidden communities. The experimental results indicate considerable improvements in detecting communities based on accuracy, scalability, and real cascade information measures. © 2018 ACM.",Community detection; Conditional random field; Information diffusion; Social influence; Social networks,Image segmentation; Random processes; Social networking (online); Community detection; Community structures; Complete information; Conditional random field; Information diffusion; Information measures; Social influence; Specific properties; Population dynamics
Local Spectral Clustering for Overlapping Community Detection,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040812697&doi=10.1145%2f3106370&partnerID=40&md5=5b2e9923eec5a4cc2782189e691fb700,"Large graphs arise in a number of contexts and understanding their structure and extracting information from them is an important research area. Early algorithms for mining communities have focused on global graph structure, and often run in time proportional to the size of the entire graph. As we explore networks with millions of vertices and find communities of size in the hundreds, it becomes important to shift our attention from macroscopic structure to microscopic structure in large networks. A growing body of work has been adopting local expansion methods in order to identify communities from a few exemplary seed members. In this article, we propose a novel approach for finding overlapping communities called Lemon (Local Expansion via Minimum One Norm). Provided with a few known seeds, the algorithm finds the community by performing a local spectral diffusion. The core idea of Lemon is to use short random walks to approximate an invariant subspace near a seed set, which we refer to as local spectra. Local spectra can be viewed as the low-dimensional embedding that captures the nodes’ closeness in the local network structure. We show that Lemon’s performance in detecting communities is competitive with state-of-the-art methods. Moreover, the running time scales with the size of the community rather than that of the entire graph. The algorithm is easy to implement and is highly parallelizable. We further provide theoretical analysis of the local spectral properties, bounding the measure of tightness of extracted community using the eigenvalues of graph Laplacian. We thoroughly evaluate our approach using both synthetic and real-world datasets across different domains, and analyze the empirical variations when applying our method to inherently different networks in practice. In addition, the heuristics on how the seed set quality and quantity would affect the performance are provided. © 2018 ACM 1556-4681/2018/01-ART17 $15.00",Community detection; Graph diffusion; Local spectral clustering; Random walk; Seed set expansion,Citrus fruits; Clustering algorithms; Eigenvalues and eigenfunctions; Random processes; Community detection; Low dimensional embedding; Overlapping communities; Overlapping community detections; Random Walk; Seed set; Spectral clustering; State-of-the-art methods; Population dynamics
CommunityDiff: Visualizing Community Clustering Algorithms,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040782034&doi=10.1145%2f3047009&partnerID=40&md5=dc3ba85194bad6daa31e8f9b135705c9,"Community detection is an oft-used analytical function of network analysis but can be a black art to apply in practice. Grouping of related nodes is important for identifying patterns in network datasets but also notoriously sensitive to input data and algorithm selection. This is further complicated by the fact that, depending on domain and use case, the ground truth knowledge of the end-user can vary from none to complete. In this work, we present CommunityDiff, an interactive visualization system that combines visualization and active learning (AL) to support the end-user’s analytical process. As the end-user interacts with the system, a continuous refinement process updates both the community labels and visualizations. CommunityDiff features a mechanism for visualizing ensemble spaces, weighted combinations of algorithm output, that can identify patterns, commonalities, and differences among multiple community detection algorithms. Among other features, CommunityDiff introduces an AL mechanism that visually indicates uncertainty about community labels to focus end-user attention and supporting end-user control that ranges from explicitly indicating the number of expected communities to merging and splitting communities. Based on this end-user input, CommunityDiff dynamically recalculates communities. We demonstrate the viability of our through a study of speed of end-user convergence on satisfactory community labels. As part of building CommunityDiff, we describe a design process that can be adapted to other Interactive Machine Learning applications. 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Community detection; Interactive machine learning; Visualization,Artificial intelligence; Arts computing; Flow visualization; Learning systems; Population dynamics; Visualization; Algorithm selection; Analytical functions; Analytical process; Community detection; Community detection algorithms; Interactive machine learning; Interactive visualization systems; Merging and splitting; Clustering algorithms
Profit Maximization with Sufficient Customer Satisfactions,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040807397&doi=10.1145%2f3110216&partnerID=40&md5=0ea625a31f3cc9e05b35e4ae0566de8f,"In many commercial campaigns, we observe that there exists a tradeoff between the number of customers satisfied by the company and the profit gained. Merely satisfying as many customers as possible or maximizing the profit is not desirable. To this end, in this article, we propose a new problem called k-Satisfiability Assignment for Maximizing the Profit (k-SAMP), where k is a user parameter and a non-negative integer. Given a set P of products and a set O of customers, k-SAMP is to find an assignment between P and O such that at least k customers are satisfied in the assignment and the profit incurred by this assignment is maximized. Although we find that this problem is closely related to two classic computer science problems, namely maximum weight matching and maximum matching, the techniques developed for these classic problems cannot be adapted to our k-SAMP problem. In this work, we design a novel algorithm called Adjust for the k-SAMP problem. Given an assignment A, Adjust iteratively increases the profit of A by adjusting some appropriate matches in A while keeping at least k customers satisfied in A. We prove that Adjust returns a global optimum. Extensive experiments were conducted that verified the efficiency of Adjust. © 2018 ACM 1556-4681/2018/01-ART19 $15.00",Assignment; Profit maximization; Users’ satisfactions,Customer satisfaction; Iterative methods; Sales; Assignment; Global optimum; K-satisfiability; Maximum matchings; Maximum weight matching; Nonnegative integers; Novel algorithm; Profit maximization; Profitability
Enhancing Reputation via Price Discounts in E-Commerce Systems: A Data-Driven Approach,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040782755&doi=10.1145%2f3154417&partnerID=40&md5=129fad3b0e7e78ba387935ad4b2b8423,"Reputation systems have become an indispensable component of modern E-commerce systems, as they help buyers make informed decisions in choosing trustworthy sellers. To attract buyers and increase the transaction volume, sellers need to earn reasonably high reputation scores. This process usually takes a substantial amount of time. To accelerate this process, sellers can provide price discounts to attract users, but the underlying difficulty is that sellers have no prior knowledge on buyers’ preferences over price discounts. In this article, we develop an online algorithm to infer the optimal discount rate from data. We first formulate an optimization framework to select the optimal discount rate given buyers’ discount preferences, which is a tradeoff between the short-term profit and the ramp-up time (for reputation). We then derive the closed-form optimal discount rate, which gives us key insights in applying a stochastic bandits framework to infer the optimal discount rate from the transaction data with regret upper bounds. We show that the computational complexity of evaluating the performance metrics is infeasibly high, and therefore, we develop efficient randomized algorithms with guaranteed performance to approximate them. Finally, we conduct experiments on a dataset crawled from eBay. Experimental results show that our framework can trade 60% of the short-term profit for reducing the ramp-up time by 40%. This reduction in the ramp-up time can increase the long-term profit of a seller by at least 20%. © 2018 ACM",E-commerce; Price discounts; Randomized algorithms; Reputation systems; Stochastic bandits,Commerce; Costs; Electronic commerce; Optimization; Profitability; Sales; Stochastic systems; Data-driven approach; Guaranteed performance; Optimization framework; Performance metrics; Price discount; Randomized Algorithms; Reputation systems; Stochastic bandits; Computational efficiency
Discovering Communities and Anomalies in Attributed Graphs: Interactive Visual Exploration and Summarization,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040834843&doi=10.1145%2f3139241&partnerID=40&md5=8fa53dbb71252c378816bdd9450b6558,"Given a network with node attributes, how can we identify communities and spot anomalies? How can we characterize, describe, or summarize the network in a succinct way? Community extraction requires a measure of quality for connected subgraphs (e.g., social circles). Existing subgraph measures, however, either consider only the connectedness of nodes inside the community and ignore the cross-edges at the boundary (e.g., density) or only quantify the structure of the community and ignore the node attributes (e.g., conductance). In this work, we focus on node-attributed networks and introduce: (1) a new measure of subgraph quality for attributed communities called normality, (2) a community extraction algorithm that uses normality to extract communities and a few characterizing attributes per community, and (3) a summarization and interactive visualization approach for attributed graph exploration. More specifically, (1) we first introduce a new measure to quantify the normality of an attributed subgraph. Our normality measure carefully utilizes structure and attributes together to quantify both the internal consistency and external separability. We then formulate an objective function to automatically infer a few attributes (called the “focus”) and respective attribute weights, so as to maximize the normality score of a given subgraph. Most notably, unlike many other approaches, our measure allows for many cross-edges as long as they can be “exonerated;” i.e., either (i) are expected under a null graph model, and/or (ii) their boundary nodes do not exhibit the focus attributes. Next, (2) we propose AMEN (for Attributed Mining of Entity Networks), an algorithm that simultaneously discovers the communities and their respective focus in a given graph, with a goal to maximize the total normality. Communities for which a focus that yields high normality cannot be found are considered low quality or anomalous. Last, (3) we formulate a summarization task with a multi-criteria objective, which selects a subset of the communities that (i) cover the entire graph well, are (ii) high quality and (iii) diverse in their focus attributes. We further design an interactive visualization interface that presents the communities to a user in an interpretable, user-friendly fashion. The user can explore all the communities, analyze various algorithm-generated summaries, as well as devise their own summaries interactively to characterize the network in a succinct way. As the experiments on real-world attributed graphs show, our proposed approaches effectively find anomalous communities and outperform several existing measures and methods, such as conductance, density, OddBall, and SODA. We also conduct extensive user studies to measure the capability and efficiency that our approach provides to the users toward network summarization, exploration, and sensemaking. © 2018 ACM 1556-4681/2018/01-ART24 $15.00",,Extraction; Visualization; Attribute weight; Attributed graphs; Community extractions; Connected subgraphs; Interactive visualizations; Internal consistency; Objective functions; Visual exploration; Graph theory
Mining Overlapping Communities and Inner Role Assignments through Bayesian Mixed-Membership Models of Networks with Context-Dependent Interactions,2018,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040814319&doi=10.1145%2f3106368&partnerID=40&md5=8f5fbb46a8b464e977d408ad369e094b,"Community discovery and role assignment have been recently integrated into an unsupervised approach for the exploratory analysis of overlapping communities and inner roles in networks. However, the formation of ties in these prototypical research efforts is not truly realistic, since it does not account for a fundamental aspect of link establishment in real-world networks, i.e., the explicative reasons that cause interactions among nodes. Such reasons can be interpreted as generic requirements of nodes, that are met by other nodes and essentially pertain both to the nodes themselves and to their interaction contexts (i.e., the respective communities and roles). In this article, we present two new model-based machine-learning approaches, wherein community discovery and role assignment are seamlessly integrated and simultaneously performed through approximate posterior inference in Bayesian mixed-membership models of directed networks. The devised models account for the explicative reasons governing link establishment in terms of node-specific and contextual latent interaction factors. The former are inherently characteristic of nodes, while the latter are characterizations of nodes in the context of the individual communities and roles. The generative process of both models assigns nodes to communities with respective roles and connects them through directed links, which are probabilistically governed by their node-specific and contextual interaction factors. The difference between the proposed models lies in the exploitation of the contextual interaction factors. More precisely, in one model, the contextual interaction factors have the same impact on link generation. In the other model, the contextual interaction factors are weighted by the extent of involvement of the linked nodes in the respective communities and roles. We develop MCMC algorithms implementing approximate posterior inference and parameter estimation within our models. Finally, we conduct an intensive comparative experimentation, which demonstrates their superiority in community compactness and link prediction on various real-world and synthetic networks. © 2018 ACM 1556-4681/2018/01-ART18 $15.00",Bayesian probabilistic network analysis; Link prediction; Overlapping community detection; Role assignment,Inference engines; Learning systems; Bayesian probabilistic network; Community discoveries; Link prediction; Machine learning approaches; Overlapping communities; Overlapping community detections; Role assignment; Unsupervised approaches; Bayesian networks
Real-time large-scale map matching using mobile phone data,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026648089&doi=10.1145%2f3046945&partnerID=40&md5=13a39ca60b3101ba94685b0a6f7509dd,"With the wide spread use of mobile phones, cellular mobile big data is becoming an important resource that provides a wealth of information with almost no cost. However, the data generally suffers from relatively high spatial granularity, limiting the scope of its application. In this article, we consider, for the first time, the utility of actual mobile big data for map matching allowing for “microscopic” level traffic analysis. The state-of-the-art in map matching generally targets GPS data, which provides far denser sampling and higher location resolution than the mobile data. Our approach extends the typical Hidden-Markov model used in map matching to accommodate for highly sparse location trajectories, exploit the large mobile data volume to learn the model parameters, and exploit the sparsity of the data to provide for real-time Viterbi processing. We study an actual, anonymised mobile trajectories data set of the city of Dakar, Senegal, spanning a year, and generate a corresponding road-level traffic density, at an hourly granularity, for each mobile trajectory. We observed a relatively high correlation between the generated traffic intensities and corresponding values obtained by the gravity and equilibrium models typically used in mobility analysis, indicating the utility of the approach as an alternative means for traffic analysis. © 2017 ACM",Adaptive HMM; Cellular duration records; Fine-grained spatial tracking; Low cost; Mobile big data,Cellular telephones; Global positioning system; Hidden Markov models; Markov processes; Mobile phones; Telephone sets; Trajectories; Adaptive HMM; Cellular duration records; Low costs; Mobile phone datum; Spatial granularity; Spatial tracking; Trajectories datum; Wealth of information; Big data
Assessing Human Error Against a Benchmark of Perfection,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027381205&doi=10.1145%2f3046947&partnerID=40&md5=e4f4b34e1f43ca3b49cdafd351107d49,"An increasing number of domains are providing us with detailed trace data on human decisions in settings where we can evaluate the quality of these decisions via an algorithm. Motivated by this development, an emerging line of work has begun to consider whether we can characterize and predict the kinds of decisions where people are likely to make errors. To investigate what a general framework for human error prediction might look like, we focus on a model system with a rich history in the behavioral sciences: the decisions made by chess players as they select moves in a game. We carry out our analysis at a large scale, employing datasets with several million recorded games, and using chess tablebases to acquire a form of ground truth for a subset of chess positions that have been completely solved by computers but remain challenging for even the best players in the world. We organize our analysis around three categories of features that we argue are present in most settings where the analysis of human error is applicable: the skill of the decision-maker, the time available to make the decision, and the inherent difficulty of the decision. We identify rich structure in all three of these categories of features, and find strong evidence that in our domain, features describing the inherent difficulty of an instance are significantly more powerful than features based on skill or time. © 2017 ACM.",Blunder prediction; Human decision-making,Computer games; Decision making; Errors; Forecasting; Quality control; Behavioral science; Decision makers; Human decision making; Human decisions; Human error prediction; Human errors; Rich structure; Three categories; Behavioral research
COMENGO: A dynamic model for social group evolution,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027339693&doi=10.1145%2f3059214&partnerID=40&md5=38a34b860ef7d892c520a732560721b6,"How do social groups, such as Facebook groups and Wechat groups, dynamically evolve over time? How do people join the social groups, uniformly or with burst? What is the pattern of people quitting from groups? Is there a simple universal model to depict the come-and-go patterns of various groups? In this article, we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users. We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR. For example, we observe both diffusion and nondiffusion mechanism in the group joining process, and power-law decay in group quitting process, rather than exponential decay as expected in SIR model. Therefore, we propose a new model COMENGO, a concise yet flexible dynamic model for group evolution. Our model has the following advantages: (a) Unification power: it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation. (b) Succinctness and interpretability: it contains only six parameters with clear physical meanings. (c) Accuracy: it can capture various kinds of group evolution patterns preciously, and the goodness of fit increases by 58% over baseline. (d) Usefulness: it can be used in multiple application scenarios, such as forecasting and pattern discovery. Furthermore, our model can provide insights about different evolution patterns of social groups, and we also find that group structure and its evolution has notable relations with temporal patterns of group evolution. © 2017 ACM.",Dynamic model; Group evolution; Temporal patterns,Decay (organic); Dynamics; Joining; Different evolutions; Evolution patterns; Exponential decays; Flexible dynamics; Group evolution; Multiple applications; Temporal evolution; Temporal pattern; Dynamic models
Rumor gauge: Predicting the veracity of rumors on twitter,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026522971&doi=10.1145%2f3070644&partnerID=40&md5=e05d9489c526632c1b414af2f0a497cd,"The spread of malicious or accidental misinformation in social media, especially in time-sensitive situations, such as real-world emergencies, can have harmful effects on individuals and society. In this work, we developed models for automated verification of rumors (unverified information) that propagate through Twitter. To predict the veracity of rumors, we identified salient features of rumors by examining three aspects of information spread: linguistic style used to express rumors, characteristics of people involved in propagating information, and network propagation dynamics. The predicted veracity of a time series of these features extracted from a rumor (a collection of tweets) is generated using Hidden Markov Models. The verification algorithm was trained and tested on 209 rumors representing 938,806 tweets collected from real-world events, including the 2013 Boston Marathon bombings, the 2014 Ferguson unrest, and the 2014 Ebola epidemic, and many other rumors about various real-world events reported on popular websites that document public rumors. The algorithm was able to correctly predict the veracity of 75% of the rumors faster than any other public source, including journalists and law enforcement officials. The ability to track rumors and predict their outcomes may have practical applications for news consumers, financial markets, journalists, and emergency services, and more generally to help minimize the impact of false information on Twitter. © 2017 ACM",Fake news; Propagation; Rumor; Twitter; Veracity prediction,Electronic trading; Emergency services; Hidden Markov models; Markov processes; Social networking (online); Wave propagation; Automated verification; Fake news; Linguistic styles; Propagation dynamics; Rumor; Salient features; Twitter; Verification algorithms; Forecasting
Modeling temporal activity to detect anomalous behavior in social media,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026497936&doi=10.1145%2f3064884&partnerID=40&md5=f84bce78128713711ad2475a3fa698d5,"Social media has become a popular and important tool for human communication. However, due to this popularity, spam and the distribution of malicious content by computer-controlled users, known as bots, has become a widespread problem. At the same time, when users use social media, they generate valuable data that can be used to understand the patterns of human communication. In this article, we focus on the following important question: Can we identify and use patterns of human communication to decide whether a human or a bot controls a user? The first contribution of this article is showing that the distribution of inter-arrival times (IATs) between postings is characterized by following four patterns: (i) heavy-tails, (ii) periodic-spikes, (iii) correlation between consecutive values, and (iv) bimodallity. As our second contribution, we propose a mathematical model named Act-M (Activity Model). We show that Act-M can accurately fit the distribution of IATs from social media users. Finally, we use Act-M to develop a method that detects if users are bots based only on the timing of their postings. We validate Act-M using data from over 55 million postings from four social media services: Reddit, Twitter, Stack-Overflow, and Hacker-News. Our experiments show that Act-M provides a more accurate fit to the data than existing models for human dynamics. Additionally, when detecting bots, Act-M provided a precision higher than 93% and 77% with a sensitivity of 70% for the Twitter and Reddit datasets, respectively. © 2017 ACM",Anomaly detection; Communication dynamics; Inter-arrival times; Social media,Botnet; Personal computing; Activity modeling; Anomalous behavior; Anomaly detection; Human communications; Human dynamics; Inter-arrival time; Social media; Social media services; Social networking (online)
Query-Driven Learning for Predictive Analytics of Data Subspace Cardinality,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023170374&doi=10.1145%2f3059177&partnerID=40&md5=cd7a1512add57c86d0bb5fd87108bcd8,"Fundamental to many predictive analytics tasks is the ability to estimate the cardinality (number of data items) of multi-dimensional data subspaces, defined by query selections over datasets. This is crucial for data analysts dealing with, e.g., interactive data subspace explorations, data subspace visualizations, and in query processing optimization. However, in many modern data systems, predictive analytics may be (i) too costly money-wise, e.g., in clouds, (ii) unreliable, e.g., in modern Big Data query engines, where accurate statistics are difficult to obtain/maintain, or (iii) infeasible, e.g., for privacy issues. We contribute a novel, query-driven, function estimation model of analyst-defined data subspace cardinality. The proposed estimation model is highly accurate in terms of prediction and accommodating the well-known selection queries: multi-dimensional range and distance-nearest neighbors (radius) queries. Our function estimation model: (i) quantizes the vectorial query space, by learning the analysts' access patterns over a data space, (ii) associates query vectors with their corresponding cardinalities of the analyst-defined data subspaces, (iii) abstracts and employs query vectorial similarity to predict the cardinality of an unseen/unexplored data subspace, and (iv) identifies and adapts to possible changes of the query subspaces based on the theory of optimal stopping. The proposed model is decentralized, facilitating the scaling-out of such predictive analytics queries. The research significance of the model lies in that (i) it is an attractive solution when data-driven statistical techniques are undesirable or infeasible, (ii) it offers a scale-out, decentralized training solution, (iii) it is applicable to different selection query types, and (iv) it offers a performance that is superior to that of data-driven approaches.",Analytics selection queries; Data subspace exploration; Optimal stopping theory; Predictive analytics; Predictive learning; Vector regression quantization,Big data; Education; Predictive analytics; Query processing; Vector spaces; Vectors; Analytics selection queries; Data-driven approach; Multidimensional data; Optimal stopping theories; Predictive learning; Query processing optimization; Research significances; Statistical techniques; Search engines
Cross-dependency inference in multi-layered networks: A collaborative filtering perspective,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023174347&doi=10.1145%2f3056562&partnerID=40&md5=8229768fda72552d13242dedf1e7bc4c,"The increasingly connected world has catalyzed the fusion of networks from different domains, which facilitates the emergence of a new network model-multi-layered networks. Examples of such kind of network systems include critical infrastructure networks, biological systems, organization-level collaborations, crossplatform e-commerce, and so forth. One crucial structure that distances multi-layered network from other network models is its cross-layer dependency, which describes the associations between the nodes from different layers. Needless to say, the cross-layer dependency in the network plays an essential role in many data mining applications like system robustness analysis and complex network control. However, it remains a daunting task to know the exact dependency relationships due to noise, limited accessibility, and so forth. In this article, we tackle the cross-layer dependency inference problem by modeling it as a collective collaborative filtering problem. Based on this idea, we propose an effective algorithm FASCINATE that can reveal unobserved dependencies with linear complexity. Moreover, we derive FASCINATE-ZERO, an online variant of FASCINATE that can respond to a newly added node timely by checking its neighborhood dependencies. We perform extensive evaluations on real datasets to substantiate the superiority of our proposed approaches.",Cross-layer dependency; Graph mining; Multi-layered network,Collaborative filtering; Complex networks; Distributed computer systems; Signal filtering and prediction; Cross layer; Data mining applications; Dependency relationship; Different domains; Effective algorithms; Graph mining; Inference problem; Multi-layered; Network layers
Discovering conditional matching rules,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023170564&doi=10.1145%2f3070647&partnerID=40&md5=acb87a699051f1bd7dd3c215872f2291,"Matching dependencies (MDS) have recently been proposed to make data dependencies tolerant to various information representations, and found useful in data quality applications such as record matching. Instead of the strict equality function used in traditional dependency syntax (e.g., functional dependencies), MDS specify constraints based on similarity and identification.However, in practice, MDS may still be too strict and applicable only in a subset of tuples in a relation. Thereby, we study the conditional matching dependencies (CMDS), which bindmatching dependencies only in a certain part of a table, i.e., MDS conditionally applicable in a subset of tuples. Compared to MDS, CMDS have more expressive power that enables them to satisfy wider application needs. In this article, we study several important theoretical and practical issues of CMDS, including irreducible CMDS with respect to the implication, discovery of CMDS from data, reliable CMDS agreed most by a relation, approximate CMDS almost satisfied in a relation, and finally applications of CMDS in record matching and missing value repairing. Through an extensive experimental evaluation in real data sets, we demonstrate the efficiency of proposed CMDS discovery algorithms and effectiveness of CMDS in real applications.",Conditional matching dependency; Data repair; Record matching,Computer science; Data mining; Data repairs; Discovery algorithm; Experimental evaluation; Functional dependency; Information representation; Matching dependencies; Real applications; Record matching; Repair
Large-Scale Online Feature Selection for Ultra-High Dimensional Sparse Data,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023166211&doi=10.1145%2f3070646&partnerID=40&md5=0054199fdc62a420262ffd8b9fb905ac,"Feature selection (FS) is an important technique in machine learning and data mining, especially for largescale high-dimensional data. Most existing studies have been restricted to batch learning, which is often inefficient and poorly scalable when handling big data in real world. As real data may arrive sequentially and continuously, batch learning has to retrain the model for the new coming data, which is very computationally intensive. Online feature selection (OFS) is a promising new paradigm that is more efficient and scalable than batch learning algorithms. However, existing online algorithms usually fall short in their inferior efficacy. In this article, we present a novel second-order OFS algorithm that is simple yet effective, very fast and extremely scalable to deal with large-scale ultra-high dimensional sparse data streams. The basic idea is to exploit the second-order information to choose the subset of important features with high confidence weights. Unlike existing OFS methods that often suffer from extra high computational cost, we devise a novel algorithm with a MaxHeap-based approach, which is not only more effective than the existing firstorder algorithms, but also significantly more efficient and scalable. Our extensive experiments validated that the proposed technique achieves highly competitive accuracy as compared with state-of-The-Art batch FS methods, meanwhile it consumes significantly less computational cost that is orders of magnitude lower. Impressively, on a billion-scale synthetic dataset (1-billion dimensions, 1-billion non-zero features, and 1- million samples), the proposed algorithm takes less than 3 minutes to run on a single PC.",Feature selection; Second-order online learning; Sparsity; Ultra-high dimensionality,Big data; Computational efficiency; Data handling; Data mining; Education; Learning algorithms; Computational costs; First-order algorithms; High dimensional data; Online feature selection; Online learning; Orders of magnitude; Sparsity; Ultra-high; Feature extraction
TRIÈ ST: Counting local and global triangles in fully dynamic streams with fixed memory size,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023202876&doi=10.1145%2f3059194&partnerID=40&md5=5ac7257cd1e017afc303e24e44e0816d,"We present TRÌEST, a suite of one-pass streaming algorithms to compute unbiased, low-variance, highquality approximations of the global and local (i.e., incident to each vertex) number of triangles in a fully dynamic graph represented as an adversarial stream of edge insertions and deletions. Our algorithms use reservoir sampling and its variants to exploit the user-specified memory space at all times. This is in contrast with previous approaches, which require hard-To-choose parameters (e.g., a fixed sampling probability) and offer no guarantees on the amount of memory they use. We analyze the variance of the estimations and show novel concentration bounds for these quantities. Our experimental results on very large graphs demonstrate that TRÌEST outperforms state-of-The-Art approaches in accuracy and exhibits a small update time.",Cycle counting; Reservoir sampling; Subgraph counting,Approximation algorithms; Concentration bounds; Cycle counting; Insertions and deletions; Number of triangles; Reservoir samplings; State-of-the-art approach; Streaming algorithm; Subgraphs; Graph theory
Introduction to special issue on the best papers from KDD 2016,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023189913&doi=10.1145%2f3092689&partnerID=40&md5=bcb3b749e72ba104657bdabaae909ce3,"This issue contains the best papers from the ACM KDD Conference 2016. As is customary at KDD, special issue papers are invited only from the research track. The top-ranked papers from the KDD 2016 conference are included in this issue. This issue contains a total of six articles, which are from different areas of data mining. A brief description of these articles is also provided in this article.",Data mining,Data mining; Paper
Ranking causal anomalies for system fault diagnosis via temporal and dynamical analysis on vanishing correlations,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023164652&doi=10.1145%2f3046946&partnerID=40&md5=16fa5b311ffed10fd17aa018837a06ac,"Detecting system anomalies is an important problem in many fields such as security, fault management, and industrial optimization. Recently, invariant network has shown to be powerful in characterizing complex system behaviours. In the invariant network, a node represents a system component and an edge indicates a stable, significant interaction between two components. Structures and evolutions of the invariance network, in particular the vanishing correlations, can shed important light on locating causal anomalies and performing diagnosis. However, existing approaches to detect causal anomalies with the invariant network often use the percentage of vanishing correlations to rank possible casual components, which have several limitations: (1) fault propagation in the network is ignored, (2) the root casual anomalies may not always be the nodes with a high percentage of vanishing correlations, (3) temporal patterns of vanishing correlations are not exploited for robust detection, and (4) prior knowledge on anomalous nodes are not exploited for (semi-)supervised detection. To address these limitations, in this article we propose a network diffusion based framework to identify significant causal anomalies and rank them. Our approach can effectivelymodel fault propagation over the entire invariant network and can perform joint inference on both the structural and the time-evolving broken invariance patterns. As a result, it can locate high-confidence anomalies that are truly responsible for the vanishing correlations and can compensate for unstructuredmeasurement noise in the system. Moreover, when the prior knowledge on the anomalous status of some nodes are available at certain time points, our approach is able to leverage them to further enhance the anomaly inference accuracy. When the prior knowledge is noisy, our approach also automatically learns reliable information and reduces impacts from noises. By performing extensive experiments on synthetic datasets, bank information system datasets, and coal plant cyber-physical system datasets, we demonstrate the effectiveness of our approach.",Causal anomalies ranking; Label propagation; Nonnegative matrix factorization,Complex networks; Embedded systems; Factorization; Failure analysis; Fault detection; Knowledge management; Matrix algebra; Causal anomalies ranking; Dynamical analysis; Industrial optimization; Label propagation; Network diffusions; Nonnegative matrix factorization; Synthetic datasets; System components; Cyber Physical System
Mining community structures in multidimensional networks,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023182610&doi=10.1145%2f3080574&partnerID=40&md5=61216408a8d39e8ff9fd11e3c667655a,"We investigate the problem of community detection in multidimensional networks, that is, networks where entities engage in various interaction types (dimensions) simultaneously. While some approaches have been proposed to identify community structures in multidimensional networks, there are a number of problems still to solve. In fact, the majority of the proposed approaches suffer from one or even more of the following limitations: (1) difficulty detecting communities in networks characterized by the presence of many irrelevant dimensions, (2) lack of systematic procedures to explicitly identify the relevant dimensions of each community, and (3) dependence on a set of user-supplied parameters, including the number of communities, that require a proper tuning. Most of the existing approaches are inadequate for dealing with these three issues in a unified framework. In this paper, we develop a novel approach that is capable of addressing the aforementioned limitations in a single framework. The proposed approach allows automated identification of communities and their sub-dimensional spaces using a novel objective function and a constrained label propagation-based optimization strategy. By leveraging the relevance of dimensions at the node level, the strategy aims to maximize the number of relevant within-community links while keeping track of the most relevant dimensions. A notable feature of the proposed approach is that it is able to automatically identify low dimensional community structures embedded in a high dimensional space. Experiments on synthetic and real multidimensional networks illustrate the suitability of the new method.",Community detection; Data mining; Social networks,Constrained optimization; Data mining; Population dynamics; Social networking (online); Automated identification; Community detection; Community structures; High dimensional spaces; Mining communities; Multi-dimensional networks; Objective functions; Optimization strategy; Social sciences
Graph-based fraud detection in the face of camouflage,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023201416&doi=10.1145%2f3056563&partnerID=40&md5=b5833998a5c21e89bdad9827d099221f,"Given a bipartite graph of users and the products that they review, or followers and followees, how can we detect fake reviews or follows? Existing fraud detection methods (spectral, etc.) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph. Fraudsters can evade these methods using camouflage, by adding reviews or follows with honest targets so that they look ""normal."" Even worse, some fraudsters use hijacked accounts from honest users, and then the camouflage is indeed organic. Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts. We propose FRAUDAR, an algorithm that (a) is camouflage resistant, (b) provides upper bounds on the effectiveness of fraudsters, and (c) is effective in real-world data. Experimental results under various attacks show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud. Additionally, in real-world experiments with a Twitter follower-followee graph of 1.47 billion edges, FRAUDAR successfully detected a subgraph of more than 4,000 detected accounts, of which a majority had tweets showing that they used follower-buying services.",Fraud detection; Link analysis; Spam detection,Crime; Bipartite graphs; Dense sub-graphs; Fraud detection; Link analysis; Real world experiment; Spam detection; Upper Bound; Various attacks; Graph theory
Partitioned similarity search with cache-conscious data traversal,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018498968&doi=10.1145%2f3014060&partnerID=40&md5=9b9ca5ea618ede82076b4178eab162d5,"All pairs similarity search (APSS) is used in many web search and data mining applications. Previous work has used techniques such as comparison filtering, inverted indexing, and parallel accumulation of partial results. However, shuffling intermediate results can incur significant communication overhead as data scales up. This paper studies a scalable two-phase approach called Partition-based Similarity Search (PSS). The first phase is to partition the data and group vectors that are potentially similar. The second phase is to run a set of tasks where each task compares a partition of vectors with other candidate partitions. Due to data sparsity and the presence of memory hierarchy, accessing feature vectors during the partition comparison phase incurs significant overhead. This paper introduces a cache-conscious design for data layout and traversal to reduce access time through size-controlled data splitting and vector coalescing, and it provides an analysis to guide the choice of optimization parameters. The evaluation results show that for the tested datasets, the proposed approach can lead to an early elimination of unnecessary I/O and data communication while sustaining parallel efficiency with one order of magnitude of performance improvement and it can also be integrated with LSH for approximated APSS. © 2017 ACM.","All-pairs similarity search; Data traversal; Design; Experimentation; H.3.3 [information storage and retrieval]: information search and retrieval - clustering, search process; H.3.4 [information storage and retrieval]: systems and software - distributed systems; Memory hierarchy; Partitioning; Performance",Computer architecture; Data mining; Design; Digital storage; Distributed computer systems; Flocculation; Information retrieval; Memory architecture; Vectors; Data traversal; Experimentation; Memory hierarchy; Partitioning; Performance; Search process; Similarity search; Systems and software; Search engines
A randomized rounding algorithm for sparse PCA,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018505521&doi=10.1145%2f3046948&partnerID=40&md5=4718222c61c897ff1621986fd3f406c7,"We present and analyze a simple, two-step algorithm to approximate the optimal solution of the sparse PCA problem. In the proposed approach, we first solve an ℓ1-penalized version of the NP-hard sparse PCA optimization problem and then we use a randomized rounding strategy to sparsify the resulting dense solution. Our main theoretical result guarantees an additive error approximation and provides a tradeoff between sparsity and accuracy. Extensive experimental evaluation indicates that the proposed approach is competitive in practice, even compared to state-of-the-art toolboxes such as Spasm. © 2017 ACM.",Rounding randomized algorithm; Sparce pca,Computer science; Data mining; Experimental evaluation; Optimal solutions; Optimization problems; Randomized Algorithms; Randomized rounding; Sparce pca; State of the art; Two-step algorithms; Optimization
Moving destination prediction using sparse dataset: A mobility gradient descent approach,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018464122&doi=10.1145%2f3051128&partnerID=40&md5=69382e602b35076324cb899d7bb1d708,"Moving destination prediction offers an important category of location-based applications and provides essential intelligence to business and governments. In existing studies, a common approach to destination prediction is to match the given query trajectory with massive recorded trajectories by similarity calculation. Unfortunately, due to privacy concerns, budget constraints, and many other factors, in most circumstances, we can only obtain a sparse trajectory dataset. In sparse dataset, the available moving trajectories are far from enough to cover all possible query trajectories; thus the predictability of the matching-based approach will decrease remarkably. Toward destination prediction with sparse dataset, instead of searching similar trajectories over the sparse records, we alternatively examine the changes of distances from sampling locations to final destination on query trajectory. The underlying idea is intuitive: It is directly motivated by travel purpose, people always get closer to the final destination during the movement. By borrowing the conception of gradient descent in optimization theory, we propose a novel moving destination prediction approach, namely MGDPre. Building upon the mobility gradient descent, MGDPre only investigates the behavior characteristics of query trajectory itself without matching historical trajectories, and thus is applicable for sparse dataset. We evaluate our approach based on extensive experiments, using GPS trajectories generated by a sample of taxis over a 10-day period in Shenzhen city, China. The results demonstrate that the effectiveness, efficiency, and scalability of our approach outperform state-of-the-art baseline methods. © 2017 ACM.",Gradient descent; Markov transition model; Moving destination prediction; Space division; Sparse dataset,Budget control; Forecasting; Optimization; Site selection; Behavior characteristic; Gradient descent; Location-based applications; Optimization theory; Similarity calculation; Space division; Sparse dataset; Transition model; Trajectories
Spatial prediction for multivariate non-Gaussian data,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017157997&doi=10.1145%2f3022669&partnerID=40&md5=57c835553c4f9c9687a4ccd83722a150,"With the ever increasing volume of geo-referenced datasets, there is a real need for better statistical estimation and prediction techniques for spatial analysis. Most existing approaches focus on predicting multivariate Gaussian spatial processes, but as the data may consist of non-Gaussian (or mixed type) variables, this creates two challenges: (1) how to accurately capture the dependencies among different data types, both Gaussian and non-Gaussian; and (2) how to efficiently predict multivariate non-Gaussian spatial processes. In this article, we propose a generic approach for predicting multiple response variables of mixed types. The proposed approach accurately captures cross-spatial dependencies among response variables and reduces the computational burden by projecting the spatial process to a lower dimensional space with knot-based techniques. Efficient approximations are provided to estimate posterior marginals of latent variables for the predictive process, and extensive experimental evaluations based on both simulation and real-life datasets are provided to demonstrate the effectiveness and efficiency of this new approach. © 2017 ACM.",Approximate Bayesian inference; Computational statistics; Gaussian and non-Gaussian processes; Geostatistics; Laplace approximation; Predictive process model,Bayesian networks; Forecasting; Gaussian noise (electronic); Inference engines; Approximate Bayesian inference; Computational statistics; Geo-statistics; Laplace approximation; Non-Gaussian process; Predictive process; Gaussian distribution
Finding dynamic dense subgraphs,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017119240&doi=10.1145%2f3046791&partnerID=40&md5=61dc31113516fad997ed1650d9b1ae8d,"Online social networks are often defined by considering interactions of entities at an aggregate level. For example, a call graph is formed among individuals who have called each other at least once; or at least κ times. Similarly, in social-media platforms, we consider implicit social networks among users who have interacted in some way, e.g., have made a conversation, have commented to the content of each other, and so on. Such definitions have been used widely in the literature and they have offered significant insights regarding the structure of social networks. However, it is obvious that they suffer from a severe limitation: They neglect the precise time that interactions among the network entities occur. In this article, we consider interaction networks, where the data description contains not only information about the underlying topology of the social network, but also the exact time instances that network entities interact. In an interaction network, an edge is associated with a timestamp, and multiple edges may occur for the same pair of entities. Consequently, interaction networks offer a more fine-grained representation, which can be leveraged to reveal otherwise hidden dynamic phenomena. In the setting of interaction networks, we study the problem of discovering dynamic dense subgraphs whose edges occur in short time intervals. We view such subgraphs as fingerprints of dynamic activity occurring within network communities. Such communities represent groups of individuals who interact with each other in specific time instances, for example, a group of employees who work on a project and whose interaction intensifies before certain project milestones. We prove that the problem we define is NPhard, and we provide efficient algorithms by adapting techniques for finding dense subgraphs. We also show how to speed-up the proposed methods by exploiting concavity properties of our objective function and by the means of fractional programming. We perform extensive evaluation of the proposed methods on synthetic and real datasets, which demonstrates the validity of our approach and shows that our algorithms can be used to obtain high-quality results. © 2017 ACM 1556-4681/2017/03-ART27 15.00.",Community Discovery; Dense Subgraphs; Dynamic Graphs; Graph Mining; Interaction Networks; Social-Network Analysis; Time-Evolving Networks,Mathematical programming; Project management; Quality control; Websites; Community discoveries; Dense sub-graphs; Dynamic graph; Evolving networks; Graph mining; Interaction networks; Social networking (online)
Scalable and efficient flow-based community detection for large-scale graph analysis,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014800177&doi=10.1145%2f2992785&partnerID=40&md5=d5c43828db64e3ef29acec16df9c1045,"Community detection is an increasingly popular approach to uncover important structures in large networks. Flow-based community detection methods rely on communication patterns of the network rather than structural properties to determine communities. The Infomap algorithm in particular optimizes a novel objective function called the map equation and has been shown to outperform other approaches in third-party benchmarks. However, Infomap and its variants are inherently sequential, limiting their use for large-scale graphs. In this article, we propose a novel algorithm to optimize the map equation called RelaxMap. RelaxMap provides two important improvements over Infomap: parallelization, so that the map equation can be optimized over much larger graphs, and prioritization, so that the most important work occurs first, iterations take less time, and the algorithm converges faster. We implement these techniques using OpenMP on shared-memory multicore systems, and evaluate our approach on a variety of graphs from standard graph clustering benchmarks as well as real graph datasets. Our evaluation shows that both techniques are effective: RelaxMap achieves 70% parallel efficiency on eight cores, and prioritization improves algorithm performance by an additional 20-50% on average, depending on the graph properties. Additionally, RelaxMap converges in the similar number of iterations and provides solutions of equivalent quality as the serial Infomap implementation. © 2017 ACM.",Algorithms; Community detection; Design; F.1.2 [modes of computation]: parallelism and concurrency; Graph analysis; I.5.3 [clustering]: algorithms; Parallelization; Performance; Prioritization,Algorithms; Application programming interfaces (API); Design; Flow graphs; Population dynamics; Community detection; Graph analysis; Parallelism and concurrencies; Parallelizations; Performance; Prioritization; Clustering algorithms
Recommendations based on comprehensively exploiting the latent factors hidden in items' ratings and content,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017098994&doi=10.1145%2f3003728&partnerID=40&md5=8d50c9b515178c5453ef40b08dabec94,"To improve the performance of recommender systems in a practical manner, several hybrid approaches have been developed by considering item ratings and content information simultaneously. However, most of these hybrid approaches make recommendations basedon aggregating different recommendation techniques using various strategies, rather than considering joint modeling of the item's ratings and content, and thus fail to detect many latent factors that could potentially improve the performance of the recommender systems. For this reason, these approaches continue to suffer from data sparsity and do not work well for recommending items to individual users. A few studies try to describe a user's preference by detecting items' latent features from content-description texts as compensation for the sparse ratings. Unfortunately, most of these methods are still generally unable to accomplish recommendation tasks well for two reasons: (1) they learn latent factors from text descriptions or user-item ratings independently, rather than combining them together; and (2) influences of latent factors hidden in texts and ratings are not fully explored. In this study, we propose a probabilistic approach that we denote as latent random walk (LRW) based on the combination of an integrated latent topic model and random walk (RW) with the restart method, which can be used to rank items according to expected user preferences by detecting both their explicit and implicit correlative information, in order to recommend top-ranked items to potentially interested users. As presented in this article, the goal of this work is to comprehensively discover latent factors hidden in items' ratings and content in order to alleviate the data sparsity problem and to improve the performance of recommender systems. The proposed topic model provides a generative probabilistic framework that discovers users' implicit preferences and items' latent features simultaneously by exploiting both ratings and item content information. On the basis of this probabilistic framework, RW can predict a user's preference for unrated items by discovering global latent relations. In order to show the efficiency of the proposed approach, we test LRW and other stateof-the-art methods on three real-world datasets, namely, CAMRa2011, Yahoo!, and APP. The experiments indicate that our approach outperforms all comparative methods and, in addition, that it is less sensitive to the data sparsity problem, thus demonstrating the robustness of LRW for recommendation tasks. © 2017 ACM.",Correlative information discovering; Latent factors detection; Probabilistic topic model; Random walk with restart; Recommender system,Random processes; Correlative information discovering; Latent factor; Performance of recommender systems; Probabilistic approaches; Probabilistic topic models; Random walk with restart; Recommendation techniques; State-of-the-art methods; Recommender systems
An influence propagation view of PageRank,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017097944&doi=10.1145%2f3046941&partnerID=40&md5=34e95f185785dc5dfb6e79c7e01533e8,"For a long time, PageRank has been widely used for authority computation and has been adopted as a solid baseline for evaluating social influence related applications. However, when measuring the authority of network nodes, the traditional PageRank method does not take the nodes' prior knowledge into consideration. Also, the connection between PageRank and social influence modeling methods is not clearly established. To that end, this article provides a focused study on understanding PageRank as well as the relationship between PageRank and social influence analysis. Along this line, we first propose a linear social influence model and reveal that this model generalizes the PageRank-based authority computation by introducing some constraints. Then, we show that the authority computation by PageRank can be enhanced if exploiting more reasonable constraints (e.g., from prior knowledge). Next, to deal with the computational challenge of linear model with general constraints, we provide an upper bound for identifying nodes with top authorities. Moreover, we extend the proposed linear model for better measuring the authority of the given node sets, and we also demonstrate the way to quickly identify the top authoritative node sets. Finally, extensive experimental evaluations on four real-world networks validate the effectiveness of the proposed linear model with respect to different constraint settings. The results show that the methods with more reasonable constraints can lead to better ranking and recommendation performance. Meanwhile, the upper bounds formed by PageRank values could be used to quickly locate the nodes and node sets with the highest authorities. © 2017 ACM.",Authority; PageRank; Priors; Social influence propagation; Upper bounds,Computer science; Data mining; Authority; PageRank; Priors; Social influence; Upper Bound; Economic and social effects
Graph manipulations for fast centrality computation,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017142518&doi=10.1145%2f3022668&partnerID=40&md5=b4e4475ba9caeb7780341548bd94d715,"The betweenness and closeness metrics are widely used metrics in many network analysis applications. Yet, they are expensive to compute. For that reason, making the betweenness and closeness centrality computations faster is an important and well-studied problem. In this work, we propose the framework BADIOS that manipulates the graph by compressing it and splitting into pieces so that the centrality computation can be handled independently for each piece. Experimental results show that the proposed techniques can be a great arsenal to reduce the centrality computation time for various types and sizes of networks. In particular, it reduces the betweenness centrality computation time of a 4.6 million edges graph from more than 5 days to less than 16 hours. For the same graph, the closeness computation time is decreased from more than 3 days to 6 hours (12.7x speedup). © 2017 ACM.",Betweenness centrality; Closeness centrality; Shortest path,Computer science; Data mining; Betweenness; Betweenness centrality; Closeness centralities; Computation time; Graph manipulation; Shortest path; Computational efficiency
Combining structured node content and topology information for networked graph clustering,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017122894&doi=10.1145%2f2996197&partnerID=40&md5=63472217b6802dd166e8190e351e6b79,"Graphs are popularly used to represent objects with shared dependency relationships. To date, all existing graph clustering algorithms consider each node as a single attribute or a set of independent attributes, without realizing that content inside each node may also have complex structures. In this article, we formulate a new networked graph clustering task where a network contains a set of inter-connected (or networked) super-nodes, each of which is a single-attribute graph. The new super-node representation is applicable to many real-world applications, such as a citation network where each node denotes a paper whose content can be described as a graph, and citation relationships between papers form a networked graph (i.e., a supergraph). Networked graph clustering aims to find similar node groups, each of which contains nodes with similar content and structure information. The main challenge is to properly calculate the similarity between super-nodes for clustering. To solve the problem, we propose to characterize node similarity by integrating structure and content information of each super-node. To measure node content similarity, we use cosine distance by considering overlapped attributes between two super-nodes. To measure structure similarity, we propose an Attributed Random Walk Kernel (ARWK) to calculate the similarity between super-nodes. Detailed node content analysis is also included to build relationships between super-nodes with shared internal structure information, so the structure similarity can be calculated in a precise way. By integrating the structure similarity and content similarity as one matrix, the spectral clustering is used to achieve networked graph clustering. Our method enjoys sound theoretical properties, including bounded similarities and better structure similarity assessment than traditional graph clustering methods. Experiments on real-world applications demonstrate that our method significantly outperforms baseline approaches. © 2017 ACM.",Clustering; Kernel; Networked graphs; Super-nodes; Topology,Clustering algorithms; Topology; Clustering; Content and structure; Graph clustering algorithms; Kernel; Networked graphs; Structure similarity; Supernodes; Topology information; Graph theory
Robust graph regularized nonnegative matrix factorization for clustering,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014764596&doi=10.1145%2f3003730&partnerID=40&md5=b1d7a58a889121ca7d6495e6da5b1eb6,"Matrix factorization is often used for data representation in many data mining and machine-learning problems. In particular, for a dataset without any negative entries, nonnegative matrix factorization (NMF) is often used to find a low-rank approximation by the product of two nonnegative matrices. With reduced dimensions, these matrices can be effectively used for many applications such as clustering. The existing methods of NMF are often afflicted with their sensitivity to outliers and noise in the data. To mitigate this drawback, in this paper, we consider integrating NMF into a robust principal component model, and design a robust formulation that effectively captures noise and outliers in the approximation while incorporating essential nonlinear structures. A set of comprehensive empirical evaluations in clustering applications demonstrates that the proposed method has strong robustness to gross errors and superior performance to current state-of-the-art methods. © 2017 ACM.",Clustering; Manifold; Nonnegative factorization; Robust principal component analysis,Approximation theory; Data mining; Factorization; Learning systems; Principal component analysis; Statistics; Clustering; Machine learning problem; Manifold; Non-negative factorization; Nonnegative matrix factorization; Robust principal component analysis; Robust principal components; State-of-the-art methods; Matrix algebra
Learning multiple diagnosis codes for ICU patients with local disease correlation mining,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014767985&doi=10.1145%2f3003729&partnerID=40&md5=18f9753d42a7431e6990873a26632d91,"In the era of big data, a mechanism that can automatically annotate disease codes to patients' records in the medical information system is in demand. The purpose of this work is to propose a framework that automatically annotates the disease labels of multi-source patient data in Intensive Care Units (ICUs). We extract features from two main sources, medical charts and notes. The Bag-of-Words model is used to encode the features. Unlike most of the existing multi-label learning algorithms that globally consider correlations between diseases, our model learns disease correlation locally in the patient data. To achieve this, we derive a local disease correlation representation to enrich the discriminant power of each patient data. This representation is embedded into a unified multi-label learning framework. We develop an alternating algorithm to iteratively optimize the objective function. Extensive experiments have been conducted on a real-world ICU database. We have compared our algorithm with representative multi-label learning algorithms. Evaluation results have shown that our proposed method has state-of-the-art performance in the annotation of multiple diagnostic codes for ICU patients. This study suggests that problems in the automated diagnosis code annotation can be reliably addressed by using a multi-label learning model that exploits disease correlation. The findings of this study will greatly benefit health care and management in ICU considering that the automated diagnosis code annotation can significantly improve the quality and management of health care for both patients and caregivers. © 2017 ACM.",Diagnosis code annotation; ICU data mining; Local correlation exploiting; MIMIC II database; Multi-label learning; Pattern discovery,Big data; Codes (symbols); Data mining; Diagnosis; Health care; Hospital data processing; Information retrieval; Intensive care units; Iterative methods; Learning systems; Medical information systems; Code annotation; Local correlations; Mimic-ii; Multi-label learning; Pattern discovery; Learning algorithms
Modeling buying motives for personalized product bundle recommendation,2017,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014778832&doi=10.1145%2f3022185&partnerID=40&md5=1b909e73605b61825fc1170becffae43,"Product bundling is a marketing strategy that offers several products/items for sale as one bundle. While the bundling strategy has been widely used, less efforts have been made to understand how items should be bundled with respect to consumers' preferences and buying motives for product bundles. This article investigates the relationships between the items that are bought together within a product bundle. To that end, each purchased product bundle is formulated as a bundle graph with items as nodes and the associations between pairs of items in the bundle as edges. The relationships between items can be analyzed by the formation of edges in bundle graphs, which can be attributed to the associations of feature aspects. Then, a probabilistic model BPM (Bundle Purchases with Motives) is proposed to capture the composition of each bundle graph, with two latent factors node-type and edge-type introduced to describe the feature aspects and relationships respectively. Furthermore, based on the preferences inferred from the model, an approach for recommending items to form product bundles is developed by estimating the probability that a consumer would buy an associative item together with the item already bought in the shopping cart. Finally, experimental results on real-world transaction data collected from well-known shopping sites show the effectiveness advantages of the proposed approach over other baseline methods. Moreover, the experiments also show that the proposed model can explain consumers' buying motives for product bundles in terms of different node-types and edge-types. © 2017 ACM.",Algorithms; Buying motives; Design; Experimentation; H.2.8 [database management]: database applications - data mining; H.3.3 [information storage and retrieval]: information filtering - recommendation; Probabilistic graphical model; Product bundle; Recommendation,Algorithms; Behavioral research; Data mining; Design; Digital storage; Graph theory; Information filtering; Information management; Marketing; Buying motives; Experimentation; H.2.8 [database management]: database applications - data minings; Information storage and retrieval; Probabilistic graphical models; Product bundle; Recommendation; Product design
World knowledge as indirect supervision for document clustering,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008881028&doi=10.1145%2f2953881&partnerID=40&md5=b9da262ac2fe28a83f57e7c83c679a23,"One of the key obstacles in making learning protocols realistic in applications is the need to supervise them, a costly process that often requires hiring domain experts. We consider the framework to use the world knowledge as indirect supervision. World knowledge is general-purpose knowledge, which is not designed for any specific domain. Then, the key challenges are how to adapt the world knowledge to domains and how to represent it for learning. In this article, we provide an example of using world knowledge for domain-dependent document clustering. We provide three ways to specify the world knowledge to domains by resolving the ambiguity of the entities and their types, and represent the data with world knowledge as a heterogeneous information network. Then, we propose a clustering algorithm that can cluster multiple types and incorporate the sub-type information as constraints. In the experiments, we use two existing knowledge bases as our sources of world knowledge. One is Freebase, which is collaboratively collected knowledge about entities and their organizations. The other is YAGO2, a knowledge base automatically extracted from Wikipedia and maps knowledge to the linguistic knowledge base, WordNet. Experimental results on two text benchmark datasets (20newsgroups and RCV1) show that incorporating world knowledge as indirect supervision can significantly outperform the state-of-the-art clustering algorithms as well as clustering algorithms enhanced with world knowledge features. A preliminary version of this work appeared in the proceedings of KDD 2015 [Wang et al. 2015a]. This journal version has made several major improvements. First, we have proposed a new and general learning framework for machine learning with world knowledge as indirect supervision, where document clustering is a special case in the original paper. Second, in order to make our unsupervised semantic parsing method more understandable, we add several real cases from the original sentences to the resulting logic forms with all the necessary information. Third, we add details of the three semantic filtering methods and conduct deep analysis of the three semantic filters, by using case studies to show why the conceptualization-based semantic filter can produce more accurate indirect supervision. Finally, in addition to the experiment on 20 newsgroup data and Freebase, we have extended the experiments on clustering results by using all the combinations of text (20 newsgroup, MCAT, CCAT, ECAT) and world knowledge sources (Freebase, YAGO2). © 2016 ACM.",Document clustering; Heterogeneous information network; Knowledge base; Knowledge graph; World knowledge,Cluster analysis; Information retrieval; Information services; Knowledge based systems; Learning systems; Semantics; Syntactics; Document Clustering; Heterogeneous information; Knowledge base; Knowledge graphs; World knowledge; Clustering algorithms
Unsupervised head-modifier detection in search queries,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008462046&doi=10.1145%2f2988235&partnerID=40&md5=18e322fa13e3881b798c0565757dbaee,"Interpreting the user intent in search queries is a key task in query understanding. Query intent classification has been widely studied. In this article, we go one step further to understand the query from the view of head-modifier analysis. For example, given the query ""popular iphone 5 smart cover,"" instead of using coarse-grained semantic classes (e.g., find electronic product), we interpret that ""smart cover"" is the head or the intent of the query and ""iphone 5"" is its modifier. Query head-modifier detection can help search engines to obtain particularly relevant content, which is also important for applications such as ads matching and query recommendation. We introduce an unsupervised semantic approach for query head-modifier detection. First, we mine a large number of instance level head-modifier pairs from search log. Then, we develop a conceptualization mechanism to generalize the instance level pairs to concept level. Finally, we derive weighted concept patterns that are concise, accurate, and have strong generalization power in head-modifier detection. The developed mechanism has been used in production for search relevance and ads matching. We use extensive experiment results to demonstrate the effectiveness of our approach. © 2016 ACM.",Concept pattern; Head and modifier; Information systems → Web log analysis; Knowledge modeling; Query intent; Query intent,Semantics; Smartphones; Concept pattern; Head and modifier; Knowledge model; Query intent; Web log analysis; Search engines
Scalable and accurate online feature selection for big data,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006986745&doi=10.1145%2f2976744&partnerID=40&md5=2d5c6da2839385d5207a50e9a526370a,"Feature selection is important in many big data applications. Two critical challenges closely associate with big data. First, in many big data applications, the dimensionality is extremely high, in millions, and keeps growing. Second, big data applications call for highly scalable feature selection algorithms in an online manner such that each feature can be processed in a sequential scan. We present SAOLA, a Scalable and Accurate OnLine Approach for feature selection in this paper. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel pairwise comparison techniques and maintains a parsimonious model over time in an online manner. Furthermore, to deal with upcoming features that arrive by groups, we extend the SAOLA algorithm, and then propose a new group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the levels of both groups and individual features simultaneously. An empirical study using a series of benchmark real datasets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on datasets of extremely high dimensionality and have superior performance over the state-of-the-art feature selection methods. © 2016 ACM.",,Benchmarking; Feature extraction; Big data applications; Feature selection methods; Individual features; Online feature selection; Pair-wise comparison technique; Pairwise correlation; Parsimonious modeling; Scalable feature selection; Big data
Adaptive cluster tendency visualization and anomaly detection for streaming data,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007009774&doi=10.1145%2f2997656&partnerID=40&md5=1bdfc06221097bc09c223241cf2f33d4,"The growth in pervasive network infrastructure called the Internet of Things (IoT) enables a wide range of physical objects and environments to be monitored in fine spatial and temporal detail. The detailed, dynamic data that are collected in large quantities from sensor devices provide the basis for a variety of applications. Automatic interpretation of these evolving large data is required for timely detection of interesting events. This article develops and exemplifies two new relatives of the visual assessment of tendency (VAT) and improved visual assessment of tendency (iVAT) models, which uses cluster heat maps to visualize structure in static datasets. One new model is initialized with a static VAT/iVAT image, and then incrementally (hence inc-VAT/inc-iVAT) updates the current minimal spanning tree (MST) used by VAT with an efficient edge insertion scheme. Similarly, dec-VAT/dec-iVAT efficiently removes a node from the current VAT MST. A sequence of inc-iVAT/dec-iVAT images can be used for (visual) anomaly detection in evolving data streams and for sliding window based cluster assessment for time series data. The method is illustrated with four real datasets (three of them being smart city IoT data). The evaluation demonstrates the algorithms' ability to successfully isolate anomalies and visualize changing cluster structure in the streaming data. © 2016 ACM.",,Internet of things; Adaptive clusters; Anomaly detection; Cluster structure; Internet of thing (IOT); Minimal spanning tree; Pervasive networks; Sliding window-based; Visual assessments; Data visualization
Comparing clustering with pairwise and relative constraints: A unified framework,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007004401&doi=10.1145%2f2996467&partnerID=40&md5=ea2bd1a260ee8eb7769889baf6ac6008,"Clustering can be improved with the help of side information about the similarity relationships among instances. Such information has been commonly represented by two types of constraints: pairwise constraints and relative constraints, regarding similarities about instance pairs and triplets, respectively. Prior work has mostly considered these two types of constraints separately and developed individual algorithms to learn from each type. In practice, however, it is critical to understand/compare the usefulness of the two types of constraints as well as the cost of acquiring them, which has not been studied before. This paper provides an extensive comparison of clustering with these two types of constraints. Specifically, we compare their impacts both on human users that provide such constraints and on the learning system that incorporates such constraints into clustering. In addition, to ensure that the comparison of clustering is performed on equal ground (without the potential bias introduced by different learning algorithms), we propose a probabilistic semi-supervised clustering framework that can learn from either type of constraints. Our experiments demonstrate that the proposed semi-supervised clustering framework is highly effective at utilizing both types of constraints to aid clustering. Our user study provides valuable insights regarding the impact of the constraints on human users, and our experiments on clustering with the human-labeled constraints reveal that relative constraint is often more efficient at improving clustering. © 2016 ACM.",,Clustering algorithms; Human users; Pairwise constraints; Semi-supervised Clustering; Side information; Unified framework; User study; Learning algorithms
Mining for topics to suggest knowledge model extensions,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007004857&doi=10.1145%2f2997657&partnerID=40&md5=c19ab39bd6d12dd66908611020fb2cfa,"Electronic concept maps, interlinked with other concept maps and multimedia resources, can provide rich knowledge models to capture and share human knowledge. This article presents and evaluates methods to support experts as they extend existing knowledge models, by suggesting new context-relevant topics mined from Web search engines. The task of generating topics to support knowledge model extension raises two research questions: first, how to extract topic descriptors and discriminators from concept maps; and second, how to use these topic descriptors and discriminators to identify candidate topics on the Web with the right balance of novelty and relevance. To address these questions, this article first develops the theoretical framework required for a ""topic suggester"" to aid information search in the context of a knowledge model under construction. It then presents and evaluates algorithms based on this framework and applied in EXTENDER, an implemented tool for topic suggestion. EXTENDER has been developed and tested within CmapTools, a widely used system for supporting knowledge modeling using concept maps. However, the generality of the algorithms makes them applicable to a broad class of knowledge modeling systems, and to Web search in general. ©2016 ACM.",Concept mapping; Intelligent suggesters; Knowledge construction; Knowledge discovery; Web mining,Information retrieval; Multimedia systems; Search engines; Websites; Concept mapping; Intelligent suggesters; Knowledge construction; Knowledge modeling systems; Multimedia resources; Supporting knowledge; Theoretical framework; Web Mining; Data mining
Assignment problems of different-sized inputs in MapReduce,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006998093&doi=10.1145%2f2987376&partnerID=40&md5=91ae2909e9ab08a9938567261589f60e,"A MapReduce algorithm can be described by a mapping schema, which assigns inputs to a set of reducers, such that for each required output there exists a reducer that receives all the inputs participating in the computation of this output. Reducers have a capacity that limits the sets of inputs they can be assigned. However, individual inputs may vary in terms of size. We consider, for the first time, mapping schemas where input sizes are part of the considerations and restrictions. One of the significant parameters to optimize in any MapReduce job is communication cost between the map and reduce phases. The communication cost can be optimized by minimizing the number of copies of inputs sent to the reducers. The communication cost is closely related to the number of reducers of constrained capacity that are used to accommodate appropriately the inputs, so that the requirement of how the inputs must meet in a reducer is satisfied. In this work, we consider a family of problems where it is required that each input meets with each other input in at least one reducer. We also consider a slightly different family of problems in which each input of a list, X, is required to meet each input of another list, Y, in at least one reducer. We prove that finding an optimal mapping schema for these families of problems is NP-hard, and present a bin-packing-based approximation algorithm for finding a near optimal mapping schema. © 2016 ACM.",,Approximation algorithms; Combinatorial optimization; Costs; Mapping; Assignment problems; Bin packing; Communication cost; Input size; Map-reduce; Minimizing the number of; Near-optimal; Optimal mapping; Optimization
Lifecycle modeling for buzz temporal pattern discovery,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007000512&doi=10.1145%2f2994605&partnerID=40&md5=c413aecaca29c966d9dfc9f89c0923b0,"In social media analysis, one critical task is detecting a burst of topics or buzz, which is reflected by extremely frequent mentions of certain keywords in a short-time interval. Detecting buzz not only provides useful insights into the information propagation mechanism, but also plays an essential role in preventing malicious rumors. However, buzz modeling is a challenging task becauseabuzz time-series often exhibits sudden spikes and heavy tails, wherein most existing time-series models fail. In this article, we propose novel buzz modeling approaches that capture the rise and fade temporal patterns via Product Lifecycle (PLC) model, a classical concept in economics. More specifically, we propose to model multiple peaks in buzz time-series with PLC mixture or PLC group mixture and develop a probabilistic graphical model (K-Mixture of Product Lifecycle (K-MPLC) to automatically discover inherent lifecycle patterns within a collection of buzzes. Furthermore, we effectively utilize the model parameters of PLC mixture or PLC group mixture for burst prediction. Our experimental results show that our proposed methods significantly outperform existing leading approaches on buzz clustering and buzz-type prediction. © 2016 ACM.",,Information dissemination; Mixtures; Speech recognition; Time series; Information propagation; Life cycle model; Probabilistic graphical models; Product-life-cycle; Short time intervals; Social media analysis; Temporal pattern discovery; Time series models; Life cycle
A novel bipartite graph based competitiveness degree analysis from query logs,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006985095&doi=10.1145%2f2996196&partnerID=40&md5=3b83d9e3ae0cec01a48873cf1de6149b,"Competitiveness degree analysis is a focal point of business strategy and competitive intelligence, aimed to help managers closely monitor to what extent their rivals are competing with them. This article proposes a novel method, namely BCQ, to measure the competitiveness degree between peers from query logs as an important form of user generated contents, which reflects the ""wisdom of crowds"" from the search engine users' perspective. In doing so, a bipartite graph model is developed to capture the competitive relationships through conjoint attributes hidden in query logs, where the notion of competitiveness degree for entity pairs is introduced, and then used to identify the competitive paths mapped in the bipartite graph. Subsequently, extensive experiments are conducted to demonstrate the effectiveness of BCQ to quantify the competitiveness degrees. Experimental results reveal that BCQ can well support competitors ranking, which is helpful for devising competitive strategies and pursuing market performance. In addition, efficiency experiments on synthetic data show a good scalability of BCQ on large scale of query logs. © 2016 ACM.",,Competitive intelligence; Graph theory; Graphic methods; Information retrieval; Search engines; Bipartite graphs; Business strategy; Competitive strategy; Focal points; Market performance; Synthetic data; User-generated content; Wisdom of crowds; Competition
Structural analysis of user choices for mobile app recommendation,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997219096&doi=10.1145%2f2983533&partnerID=40&md5=c7397caf8628fa6adc5289e931f707e1,"Advances in smartphone technology have promoted the rapid development of mobile apps. However, the availability of a huge number of mobile apps in application stores has imposed the challenge of finding the right apps to meet the user needs. Indeed, there is a critical demand for personalized app recommendations. Along this line, there are opportunities and challenges posed by two unique characteristics of mobile apps. First, app markets have organized apps in a hierarchical taxonomy. Second, apps with similar functionalities are competing with each other. Although there are a variety of approaches for mobile app recommendations, these approaches do not have a focus on dealing with these opportunities and challenges. To this end, in this article, we provide a systematic study for addressing these challenges. Specifically, we develop a structural user choice model (SUCM) to learn fine-grained user preferences by exploiting the hierarchical taxonomy of apps as well as the competitive relationships among apps. Moreover, we design an efficient learning algorithm to estimate the parameters for the SUCM model. Finally, we perform extensive experiments on a large app adoption dataset collected from Google Play. The results show that SUCM consistently outperforms state-of-the-art Top-N recommendation methods by a significant margin. © 2016 ACM.",Hierarchy structure; Mobile apps; Recommender systems; Structural choices,Computer science; Data mining; Recommender systems; Application stores; Efficient learning; Hierarchical taxonomy; Hierarchy structure; Mobile apps; Recommendation methods; Structural choices; Systematic study; Taxonomies
Partitioning networks with node attributes by compressing information flow,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997417436&doi=10.1145%2f2968451&partnerID=40&md5=e6d1d303439a2e243332af97973ae431,"Real-world networks are often organized as modules or communities of similar nodes that serve as functional units. These networks are also rich in content, with nodes having distinguished features or attributes. In order to discover a network's modular structure, it is necessary to take into account not only its links but also node attributes. We describe an information-theoretic method that identifies modules by compressing descriptions of information flow on a network. Our formulation introduces node content into the description of information flow, which we then minimize to discover groups of nodes with similar attributes that also tend to trap the flow of information. The method is conceptually simple and does not require ad-hoc parameters to specify the number of modules or to control the relative contribution of links and node attributes to network structure. We apply the proposed method to partition real-world networks with known community structure. We demonstrate that adding node attributes helps recover the underlying community structure in content-rich networks more effectively than using links alone. In addition, we show that our method is faster and more accurate than alternative state-of-the-art algorithms. © 2016 ACM.",Community detection; Graph partitioning; Information theory; Rich networks,Computer networks; Information theory; Community detection; Community structures; Graph Partitioning; Information-theoretic methods; Modular structures; Partitioning networks; Real-world networks; Relative contribution; Graph theory
Exploiting viral marketing for location promotion in location-based social networks,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997418638&doi=10.1145%2f3001938&partnerID=40&md5=20380e96c4421972cd24f62654509edd,"With the explosion of smartphones and social network services, location-based social networks (LBSNs) are increasingly seen as tools for businesses (e.g., restaurants and hotels) to promote their products and services. In this article, we investigate the key techniques that can help businesses promote their locations by advertising wisely through the underlying LBSNs. In order to maximize the benefit of location promotion, we formalize it as an influence maximization problem in an LBSN, i.e., given a target location and an LBSN, a set of k users (called seeds) should be advertised initially such that they can successfully propagate and attract many other users to visit the target location. Existing studies have proposed different ways to calculate the information propagation probability, that is, how likely it is that a user may influence another, in the setting of a static social network. However, it is more challenging to derive the propagation probability in an LBSN since it is heavily affected by the target location and the user mobility, both of which are dynamic and query dependent. This article proposes two user mobility models, namely the Gaussian-based and distance-based mobility models, to capture the check-in behavior of individual LBSN users, based on which location-aware propagation probabilities can be derived. Extensive experiments based on two real LBSN datasets have demonstrated the superior effectiveness of our proposals compared with existing static models of propagation probabilities to truly reflect the information propagation in LBSNs. © 2016 ACM.",Check-in behavior; Influence maximization; Location-based social network; Propagation probability,Information dissemination; Location; Marketing; Probability; Check-in; Influence maximizations; Information propagation; Location-based social networks; Products and services; Propagation probabilities; Social network services; Target location; Location based services
Distributed algorithms for computing very large thresholded covariance matrices,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997109020&doi=10.1145%2f2935750&partnerID=40&md5=f4c04dbf592ebc5ec6a5890592a1d676,"Computation of covariance matrices from observed data is an important problem, as such matrices are used in applications such as principal component analysis (PCA), linear discriminant analysis (LDA), and increasingly in the learning and application of probabilistic graphical models. However, computing an empirical covariance matrix is not always an easy problem. There are two key difficulties associated with computing such a matrix from a very high-dimensional dataset. The first problem is over-fitting. For a p-dimensional covariance matrix, there are p(p - 1)/2 unique, off-diagonal entries in the empirical covariance matrix Ŝ; for large p (say, p > 105), the size n of the dataset is often much smaller than the number of covariances to compute. Over-fitting is a concern in any situation in which the number of parameters learned can greatly exceed the size of the dataset. Thus, there are strong theoretical reasons to expect that for high-dimensional data - even Gaussian data - the empirical covariance matrix is not a good estimate for the true covariance matrix underlying the generative process. The second problem is computational. Computing a covariance matrix takes O(np2) time. For large p (greater than 10,000) and n much greater than p, this is debilitating. In this article, we consider how both of these difficulties can be handled simultaneously. Specifically, a key regularization technique for high-dimensional covariance estimation is thresholding, in which the smallest or least significant entries in the covariance matrix are simply dropped and replaced with the value 0. This suggests an obvious way to address the computational difficulty as well: First, compute the identities of the K entries in the covariance matrix that are actually important in the sense that they will not be removed during thresholding, and then in a second step, compute the values of those entries. This can be done in O(Kn) time. If K ≪ p2 and the identities of the important entries can be computed in reasonable time, then this is a big win. The key technical contribution of this article is the design and implementation of two different distributed algorithms for approximating the identities of the important entries quickly, using sampling. We have implemented these methods and tested them using an 800-core compute cluster. Experiments have been run using real datasets having millions of data points and up to 40,000 dimensions. These experiments show that the proposed methods are both accurate and efficient. © 2016 ACM.",Algorithms; Covariance matrices; Design; Distributed algorithm; H.2.8 [database management]: database applications; Performance; Sampling; Text processing; Thresholding,Algorithms; Clustering algorithms; Design; Discriminant analysis; Importance sampling; Matrix algebra; Parallel algorithms; Principal component analysis; Sampling; Text processing; Covariance matrices; Database applications; Design and implementations; High-dimensional dataset; Linear discriminant analysis; Performance; Probabilistic graphical models; Thresholding; Covariance matrix
Permanence and community structure in complex networks,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997207484&doi=10.1145%2f2953883&partnerID=40&md5=a852bb66321189d413f05d42dbe4a84f,"The goal of community detection algorithms is to identify densely connected units within large networks. An implicit assumption is that all the constituent nodes belong equally to their associated community. However, some nodes are more important in the community than others. To date, efforts have been primarily made to identify communities as a whole, rather than understanding to what extent an individual node belongs to its community. Therefore, most metrics for evaluating communities, for example modularity, are global. These metrics produce a score for each community, not for each individual node. In this article, we argue that the belongingness of nodes in a community is not uniform. We quantify the degree of belongingness of a vertex within a community by a new vertex-based metric called permanence. The central idea of permanence is based on the observation that the strength of membership of a vertex to a community depends upon two factors (i) the extent of connections of the vertex within its community versus outside its community, and (ii) how tightly the vertex is connected internally. We present the formulation of permanence based on these two quantities. We demonstrate that compared to other existing metrics (such as modularity, conductance, and cut-ratio), the change in permanence is more commensurate to the level of perturbation in ground-truth communities. We discuss how permanence can help us understand and utilize the structure and evolution of communities by demonstrating that it can be used to - (i) measure the persistence of a vertex in a community, (ii) design strategies to strengthen the community structure, (iii) explore the core-periphery structure within a community, and (iv) select suitable initiators for message spreading. We further show that permanence is an excellent metric for identifying communities. We demonstrate that the process of maximizing permanence (abbreviated as MaxPerm) produces meaningful communities that concur with the ground-truth community structure of the networks more accurately than eight other popular community detection algorithms. Finally, we provide mathematical proofs to demonstrate the correctness of finding communities by maximizing permanence. In particular, we show that the communities obtained by this method are (i) less affected by the changes in vertex ordering, and (ii) more resilient to resolution limit, degeneracy of solutions, and asymptotic growth of values. © 2016 ACM.",Community discovery; Community evaluation metric; Modularity; Permanence,Complex networks; Population dynamics; Associated communities; Community detection algorithms; Community discoveries; Community evaluations; Community structures; Mathematical proof; Modularity; Permanence; Signal detection
Modeling of Geographic Dependencies for Real Estate Ranking,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985011703&doi=10.1145%2f2934692&partnerID=40&md5=f0179fe8b78a18446aae8ded807c0510,"It is traditionally a challenge for home buyers to understand, compare, and contrast the investment value of real estate. Although a number of appraisal methods have been developed to value real properties, the performances of these methods have been limited by traditional data sources for real estate appraisal. With the development of new ways of collecting estate-related mobile data, there is a potential to leverage geographic dependencies of real estate for enhancing real estate appraisal. Indeed, the geographic dependencies of the investment value of an estate can be from the characteristics of its own neighborhood (individual), the values of its nearby estates (peer), and the prosperity of the affiliated latent business area (zone). To this end, in this paper, we propose a geographic method, named ClusRanking, for real estate appraisal by leveraging the mutual enforcement of ranking and clustering power. ClusRanking is able to exploit geographic individual, peer, and zone dependencies in a probabilistic ranking model. Specifically, we first extract the geographic utility of estates from geography data, estimate the neighborhood popularity of estates by mining taxicab trajectory data, and model the influence of latent business areas. Also, we fuse these three influential factors and predict real estate investment value. Moreover, we simultaneously consider individual, peer and zone dependencies, and derive an estate-specific ranking likelihood as the objective function. Furthermore, we propose an improved method named CR-ClusRanking by incorporating checkin information as a regularization term which reduces the performance volatility of real estate ranking system. Finally, we conduct a comprehensive evaluation with the real estate-related data of Beijing, and the experimental results demonstrate the effectiveness of our proposed methods. © 2016 ACM.",Clustering; Geographic dependencies; Ranking; Real estate,Taxicabs; Clustering; Comprehensive evaluation; Geographic dependencies; Probabilistic ranking; Ranking; Real estate; Real estate appraisals; Real estate investment; Investments
Greedily improving our own closeness centrality in a network,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979902939&doi=10.1145%2f2953882&partnerID=40&md5=8fb362f1b29ba9167c51d031383fd48a,"The closeness centrality is a well-known measure of importance of a vertex within a given complex network. Having high closeness centrality can have positive impact on the vertex itself: hence, in this paper we consider the optimization problem of determining how much a vertex can increase its centrality by creating a limited amount of new edges incident to it. We will consider both the undirected and the directed graph cases. In both cases, we first prove that the optimization problem does not admit a polynomial-time approximation scheme (unless P = NP), and then propose a greedy approximation algorithm (with an almost tight approximation ratio), whose performance is then tested on synthetic graphs and real-world networks. © 2016 ACM.",Approximation algorithms; Graph augmentation; Greedy algorithm; Large networks,Complex networks; Directed graphs; Optimization; Polynomial approximation; Approximation ratios; Closeness centralities; Graph augmentation; Greedy algorithms; Greedy approximation algorithms; Large networks; Optimization problems; Polynomial time approximation schemes; Approximation algorithms
Leveraging neighbor attributes for classification in sparsely labeled networks,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979920482&doi=10.1145%2f2898358&partnerID=40&md5=9088acbf35561ba22b4ab6442bb4efc4,"Many analysis tasks involve linked nodes, such as people connected by friendship links. Research on linkbased classification (LBC) has studied how to leverage these connections to improve classification accuracy. Most such prior research has assumed the provision of a densely labeled training network. Instead, this article studies the common and challenging case when LBC must use a single sparsely labeled network for both learning and inference, a case where existing methods often yield poor accuracy. To address this challenge, we introduce a novel method that enables prediction via ""neighbor attributes,"" which were briefly considered by early LBC work but then abandoned due to perceived problems. We then explain, using both extensive experiments and loss decomposition analysis, how using neighbor attributes often significantly improves accuracy. We further show that using appropriate semi-supervised learning (SSL) is essential to obtaining the best accuracy in this domain and that the gains of neighbor attributes remain across a range of SSL choices and data conditions. Finally, given the challenges of label sparsity for LBC and the impact of neighbor attributes, we show that multiple previous studies must be re-considered, including studies regarding the best model features, the impact of noisy attributes, and strategies for active learning. © 2016 ACM.",Collective classification; Collective inference; Link-based classification; Statistical relational learning,Artificial intelligence; Classification accuracy; Collective classifications; Collective inference; Decomposition analysis; Link-based; Semi-supervised learning (SSL); Statistical relational learning; Training network; Supervised learning
Sampling for Nyström extension-based spectral clustering: Incremental perspective and novel analysis,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979887695&doi=10.1145%2f2934693&partnerID=40&md5=8b0440476bf917dd6d9b37f4b7da022e,"Sampling is the key aspect for Nyström extension based spectral clustering. Traditional sampling schemes select the set of landmark points on a whole and focus on how to lower the matrix approximation error. However, the matrix approximation error does not have direct impact on the clustering performance. In this article, we propose a sampling framework from an incremental perspective, i.e., the landmark points are selected one by one, and each next point to be sampled is determined by previously selected landmark points. Incremental sampling builds explicit relationships among landmark points; thus, they work together well and provide a theoretical guarantee on the clustering performance. We provide two novel analysis methods and propose two schemes for selecting-the-next-one of the framework. The first scheme is based on clusterability analysis, which provides a better guarantee on clustering performance than schemes based on matrix approximation error analysis. The second scheme is based on loss analysis, which provides maximized predictive ability of the landmark points on the (implicit) labels of the unsampled points. Experimental results on a wide range of benchmark datasets demonstrate the superiorities of our proposed incremental sampling schemes over existing sampling schemes.",Clusterability analysis; Incremental sampling; Loss analysis; Nyström extension; Spectral clustering,Errors; Sampling; Benchmark datasets; Clusterability analysis; Incremental samplings; Loss analysis; Matrix approximation; Predictive abilities; Spectral clustering; Theoretical guarantees; Clustering algorithms
Catching synchronized behaviors in large networks: A graph mining approach,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978971731&doi=10.1145%2f2746403&partnerID=40&md5=6fa13a2478cec60ed03a452e0380f5ea,"Given a directed graph of millions of nodes, how can we automatically spot anomalous, suspicious nodes judging only from their connectivity patterns? Suspicious graph patterns show up in many applications, from Twitter users who buy fake followers, manipulating the social network, to botnet members performing distributed denial of service attacks, disturbing the network traffic graph. We propose a fast and effective method, CATCHSYNC, which exploits two of the tell-tale signs left in graphs by fraudsters: (a) synchronized behavior: suspicious nodes have extremely similar behavior patterns because they are often required to perform some task together (such as follow the same user); and (b) rare behavior: their connectivity patterns are very different from the majority. We introduce novel measures to quantify both concepts (""synchronicity"" and ""normality"") and we propose a parameter-free algorithm that works on the resulting synchronicitynormality plots. Thanks to careful design, CATCHSYNC has the following desirable properties: (a) it is scalable to large datasets, being linear in the graph size; (b) it is parameter free; and (c) it is side-information-oblivious: it can operate using only the topology, without needing labeled data, nor timing information, and the like., while still capable of using side information if available. We applied CATCHSYNC on three large, real datasets, 1-billion-edge Twitter social graph, 3-billion-edge, and 12-billion-edge Tencent Weibo social graphs, and several synthetic ones; CATCHSYNC consistently outperforms existing competitors, both in detection accuracy by 36% on Twitter and 20% on Tencent Weibo, as well as in speed. © 2016 ACM.",Anomaly detection; Connectivity pattern; Graph mining; Suspicious behavior,Computer crime; Denial-of-service attack; Directed graphs; Malware; Network security; Pattern recognition; Social networking (online); Topology; Anomaly detection; Behavior patterns; Connectivity pattern; Detection accuracy; Distributed denial of service attack; Graph mining; Suspicious behavior; Timing information; Graph theory
Latent time-series motifs,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979944329&doi=10.1145%2f2940329&partnerID=40&md5=01ea40ab3ad86473f45838a893f867c8,"Motifs are the most repetitive/frequent patterns of a time-series. The discovery of motifs is crucial for practitioners in order to understand and interpret the phenomena occurring in sequential data. Currently, motifs are searched among series sub-sequences, aiming at selecting the most frequently occurring ones. Search-based methods, which try out series sub-sequence as motif candidates, are currently believed to be the best methods in finding the most frequent patterns. However, this paper proposes an entirely new perspective infinding motifs. We demonstrate that searching is non-optimal since the domain of motifs is restricted, and instead we propose a principled optimization approach able to find optimal motifs. We treat the occurrence frequency as a function and time-series motifs as its parameters, therefore we learn the optimal motifs that maximize the frequency function. In contrast to searching, our method is able to discover the most repetitive patterns (hence optimal), even in cases where they do not explicitly occur as sub-sequences. Experiments on several real-life time-series datasets show that the motifs found by our method are highly more frequent than the ones found through searching, for exactly the same distance threshold. © 2016 ACM.",Motifs; Repeated patterns; Time series,Computer science; Data mining; Life-times; Motifs; Optimization approach; Repeated patterns; Repetitive pattern; Search-based; Sequential data; Time-series motifs; Time series
Shop-type recommendation leveraging the data from social media and location-based services,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979895464&doi=10.1145%2f2930671&partnerID=40&md5=7798578363547eeabf681fde56a33245,"It is an important yet challenging task for investors to determine the most suitable type of shop (e.g., restaurant, fashion) fora newly opened store. Traditional ways are predominantlyfield surveys and empirical estimation, which are not effective as they lack shop-related data. As social media and location-based services (LBS) are becoming more and more pervasive, user-generated data from these platforms are providing rich information not only about individual consumption experiences, but also about shop attributes. In this paper, we investigate the recommendation of shop types for a given location, by leveraging heterogeneous data that are mainly historical user preferences and location context from social media and LBS. Our goal is to select the most suitable shop type, seeking to maximize the number of customers served from a candidate set of types. We propose a novel bias learning matrix factorization method with feature fusion for shop popularity prediction. Features are defined and extracted from two perspectives: location, where features are closely related to location characteristics, and commercial, where features are about the relationships between shops in the neighborhood. Experimental results show that the proposed method outperforms state-of-theart solutions. © 2016 ACM.",Location-based services; Matrix factorization; Shop-type recommendation; Social media,Factorization; Location; Matrix algebra; Social networking (online); Telecommunication services; Empirical estimations; Heterogeneous data; Location contexts; Matrix factorizations; Number of customers served; Popularity predictions; Shop-type recommendation; Social media; Location based services
Scalable clustering by iterative partitioning and point attractor representation,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979901404&doi=10.1145%2f2934688&partnerID=40&md5=4de3dd32af047dd45f71338d95ddcff2,"Clustering very large datasets while preserving cluster quality remains a challenging data-mining task to date. In this paper, we propose an effective scalable clustering algorithm for large datasets that builds upon the concept of synchronization. Inherited from the powerful concept of synchronization, the proposed algorithm, CIPA (Clustering by Iterative Partitioning and Point Attractor Representations), is capable of handling very large datasets by iteratively partitioning them into thousands of subsets and clustering each subset separately. Using dynamic clustering by synchronization, each subset is then represented by a set of point attractors and outliers. Finally, CIPA identifies the cluster structure of the original dataset by clustering the newly generated dataset consisting of points attractors and outliers from all subsets. We demonstrate that our new scalable clustering approach has several attractive benefits: (a) CIPA faithfully captures the cluster structure of the original data by performing clustering on each separate data iteratively instead of using any sampling or statistical summarization technique. (b) It allows clustering very large datasets efficiently with high cluster quality. (c) CIPA is parallelizable and also suitable for distributed data. Extensive experiments demonstrate the effectiveness and efficiency of our approach. © 2016 ACM.",High-performance algorithm; Scalable clustering; Synchronization,Data mining; Dynamical systems; Iterative methods; Sampling; Set theory; Statistics; Synchronization; Cluster qualities; Cluster structure; Data mining tasks; Dynamic clustering; Effectiveness and efficiencies; High performance algorithms; Iterative Partitioning; Scalable clustering; Clustering algorithms
The convergence behavior of naive Bayes on large sparse datasets,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979882471&doi=10.1145%2f2948068&partnerID=40&md5=f9916d216fcca579092dc7c43d3e703b,"Large and sparse datasets with a lot of missing values are common in the big data era, such as user behaviors over a large number of items. Classification in such datasets is an important topic for machine learning and data mining. Practically, naive Bayes is still a popular classification algorithm for large sparse datasets, as its time and space complexity scales linearly with the size of non-missing values. However, several important questions about the behavior of naive Bayes are yet to be answered. For example, how different mechanisms of data missing, data sparsity, and the number of attributes systematically affect the learning curves and convergence? In this paper, we address several common data missing mechanisms and propose novel data generation methods based on these mechanisms. We generate large and sparse data systematically, and study the entire AUC (Area Under ROC Curve) learning curve and convergence behavior of naive Bayes. We not only have several important experiment observations, but also provide detailed theoretic studies. Finally, we summarize our empirical and theoretic results as an intuitive decision flowchart and a useful guideline for classifying large sparse datasets in practice. © 2016 ACM.",Convergence behavior; Large sparse data,Artificial intelligence; Behavioral research; Big data; Classification (of information); Classifiers; Learning systems; Area under roc curve (AUC); Classification algorithm; Convergence behaviors; Data generation; Different mechanisms; Learning curves; Sparse data; Time and space complexity; Data mining
Convex sparse PCA for unsupervised feature learning,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979924773&doi=10.1145%2f2910585&partnerID=40&md5=c90d703cf22ffcd0ef633ff4e7b05a18,"Principal component analysis (PCA) has been widely applied to dimensionality reduction and data preprocessing for different applications in engineering, biology, social science, and the like. Classical PCA and its variants seek for linear projections of the original variables to obtain the low-dimensional feature representations with maximal variance. One limitation is that it is difficult to interpret the results of PCA. Besides, the classical PCA is vulnerable to certain noisy data. In this paper, we propose a Convex Sparse Principal Component Analysis (CSPCA) algorithm and apply it to feature learning. First, we show that PCA can be formulated as a low-rank regression optimization problem. Based on the discussion, the l2,1-norm minimization is incorporated into the objective function to make the regression coefficients sparse, thereby robust to the outliers. Also, based on the sparse model used in CSPCA, an optimal weight is assigned to each of the original feature, which in turn provides the output with good interpretability. With the output of our CSPCA, we can effectively analyze the importance of each feature under the PCA criteria. Our new objective function is convex, and we propose an iterative algorithm to optimize it. We apply the CSPCA algorithm to feature selection and conduct extensive experiments on seven benchmark datasets. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art unsupervised feature selection algorithms. © 2016 ACM.",Convex PCA; Feature analysis; Principal component analysis (PCA); Sparse PCA,Algorithms; Feature extraction; Iterative methods; Optimization; Convex PCA; Dimensionality reduction; Feature analysis; Feature representation; Sparse PCA; Sparse principal component analysis; Unsupervised feature learning; Unsupervised feature selection; Principal component analysis
Fast sampling for time-varying Determinantal Point Processes,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979882467&doi=10.1145%2f2943785&partnerID=40&md5=a2306af151d1e9f5cac7ce00b218f814,"Determinantal Point Processes (DPPs) are stochastic models which assign each subset of a base dataset with a probability proportional to the subset's degree of diversity. It has been shown that DPPs are particularly appropriate in data subset selection and summarization (e.g., news display, video summarizations). DPPs prefer diverse subsets while other conventional models cannot offer. However, DPPs inference algorithms have a polynomial time complexity which makes it difficult to handle large and time-varying datasets, especially when real-time processing is required. To address this limitation, we developed a fast sampling algorithm for DPPs which takes advantage of the nature of some time-varying data (e.g., news corpora updating, communication network evolving), where the data changes between time stamps are relatively small. The proposed algorithm is built upon the simplification of marginal density functions over successive time stamps and the sequential Monte Carlo (SMC) sampling technique. Evaluations on both a real-world news dataset and the Enron Corpus confirm the efficiency of the proposed algorithm. © 2016 ACM.",Fast sampling; Sequential Monte Carlo; Time-varying Determinantal Point Processes (TV-DPPs),Algorithms; Complex networks; Inference engines; Monte Carlo methods; Polynomial approximation; Sampling; Set theory; Stochastic models; Stochastic systems; Degree of diversity; Fast sampling; Point process; Polynomial time complexity; Probability proportional; Realtime processing; Sequential Monte Carlo; Video summarization; Time varying networks
Listwise learning to rank from crowds,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979939279&doi=10.1145%2f2910586&partnerID=40&md5=251a3cfbb43b0721d029be17bccbb884,"Learning to rank has received great attention in recent years as it plays a crucial role in many applications such as information retrieval and data mining. The existing concept of learning to rank assumes that each training instance is associated with a reliable label. However, in practice, this assumption does not necessarily hold true as it may be infeasible or remarkably expensive to obtain reliable labels for many learning to rank applications. Therefore, a feasible approach is to collect labels from crowds and then learn a ranking function from crowdsourcing labels. This study explores the listwise learning to rank with crowdsourcing labels obtained from multiple annotators, who may be unreliable. A new probabilistic ranking model is first proposed by combining two existing models. Subsequently, a ranking function is trained by proposing a maximum likelihood learning approach, which estimates ground-truth labels and annotator expertise, and trains the ranking function iteratively. In practical crowdsourcing machine learning, valuable side information (e.g., professional grades) about involved annotators is normally attainable. Therefore, this study also investigates learning to rank from crowd labels when side information on the expertise of involved annotators is available. In particular, three basic types of side information are investigated, and corresponding learning algorithms are consequently introduced. Further, the top-k learning to rank from crowdsourcing labels are explored to deal with long training ranking lists. The proposed algorithms are tested on both synthetic and real-world data. Results reveal that the maximum likelihood estimation approach significantly outperforms the average approach and existing crowdsourcing regression methods. The performances of the proposed algorithms are comparable to those of the learning model in consideration reliable labels. The results of the investigation further indicate that side information is helpful in inferring both ranking functions and expertise degrees of annotators. © 2016 ACM.",Crowdsourcing; Listwise learning to rank; Multiple annotators; Probabilistic ranking model; Side information,Artificial intelligence; Crowdsourcing; Data mining; Information retrieval; Iterative methods; Learning systems; Maximum likelihood; Maximum likelihood estimation; Regression analysis; Learning models; Learning to rank; Maximum likelihood learning; Multiple annotators; Probabilistic ranking; Ranking functions; Regression method; Side information; Learning algorithms
Spatial-proximity optimization for rapid task group deployment,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976370794&doi=10.1145%2f2818714&partnerID=40&md5=ec3617db082595e7cfab04a90443110b,"Spatial proximity is one of the most important factors for the quick deployment of the task groups in various time-sensitive missions. This article proposes a new spatial query, Spatio-Social Team Query (SSTQ), that forms a strong task group by considering (1) the group's spatial distance (i.e., transportation time), (2) skills of the candidate group members, and (3) social rapport among the candidates. Efficient processing of SSTQ is very challenging, because the aforementioned spatial, skill, and social factors need to be carefully examined. In this article, therefore, we first formulate two subproblems of SSTQ, namely Hop-Constrained Team Problem (HCTP) and Connection-Oriented Team Query (COTQ). HCTP is a decision problem that considers only social and skill dimensions. We prove that HCTP is NP-Complete. Moreover, based on the hardness of HCTP, we prove that SSTQ is NP-Hard and inapproximable within any factor. On the other hand, COTQ is a special case of SSTQ that relaxes the social constraint. We prove that COTQ is NP-Hard and propose an approximation algorithm for COTQ, namelyCOTprox. Furthermore, based on the observations on COTprox, we devise an approximation algorithm, SSTprox, with a guaranteed error bound for SSTQ. Finally, to efficiently obtain the optimal solution to SSTQ for small instances, we design two efficient algorithms, SpatialFirst and SkillFirst, with different scenarios in mind. These two algorithms incorporate various effective ordering and pruning techniques to reduce the search space for answering SSTQ. Experimental results on real datasets indicate that the proposed algorithms can efficiently answer SSTQ under various parameter settings. © 2016 ACM.",Query processing; Social network; Spatial database,Algorithms; Approximation algorithms; Query languages; Query processing; Social networking (online); Connection oriented; Guaranteed error bounds; Parameter setting; Pruning techniques; Social constraints; Spatial database; Task group deployments; Transportation time; Optimization
Inferring dynamic diffusion networks in online media,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976370577&doi=10.1145%2f2882968&partnerID=40&md5=2b171eb4d07737a46fb293bf487cb476,"Online media play an important role in information societies by providing a convenient infrastructure for different processes. Information diffusion that is a fundamental process taking place on social and information networks has been investigated in many studies. Research on information diffusion in these networks faces two main challenges: (1) In most cases, diffusion takes place on an underlying network, which is latent and its structure is unknown. (2) This latent network is not fixed and changes over time. In this article, we investigate the diffusion network extraction (DNE) problem when the underlying network is dynamic and latent. We model the diffusion behavior (existence probability) of each edge as a stochastic process and utilize the Hidden Markov Model (HMM) to discover the most probable diffusion links according to the current observation of the diffusion process, which is the infection time of nodes and the past diffusion behavior of links. We evaluate the performance of our Dynamic Diffusion Network Extraction (DDNE) method, on both synthetic and real datasets. Experimental results show that the performance of the proposed method is independent of the cascade transmission model and outperforms the state of art method in terms of F-measure. © 2016 ACM.",Dynamic network; Hidden Markov model; Information diffusion; Online media,Extraction; Information services; Markov processes; Random processes; Stochastic models; Stochastic systems; Dynamic network; Existence probabilities; Information diffusion; Information networks; Information society; Online media; State-of-art methods; Underlying networks; Hidden Markov models
Product selection problem: Improve market share by learning consumer behavior,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978160645&doi=10.1145%2f2753764&partnerID=40&md5=7f3a073326582160dee8a183ccd51cc7,"It is often crucial for manufacturers to decide what products to produce so that they can increase their market share in an increasingly fierce market. To decide which products to produce, manufacturers need to analyze the consumers' requirements and how consumers make their purchase decisions so that the new products will be competitive in the market. In this paper, we first present a general distance-based product adoption model to capture consumers' purchase behavior. Using this model, various distance metrics can be used to describe different real life purchase behavior. We then provide a learning algorithm to decide which set of distance metrics one should use when we are given some accessible historical purchase data. Based on the product adoption model, we formalize the k most marketable products (or k-MMP) selection problem and formally prove that the problem is NP-hard. To tackle this problem, we propose an efficient greedy-based approximation algorithm with a provable solution guarantee. Using submodularity analysis, we prove that our approximation algorithm can achieve at least 63%of the optimal solution. We apply our algorithm on both synthetic datasets and real-world datasets (TripAdvisor.com), and show that our algorithm can easily achieve five or more orders of speedup over the exhaustive search and achieve about 96%of the optimal solution on average. Our experiments also demonstrate the robustness of our distance metric learning method, and illustrate how one can adopt it to improve the accuracy of product selection. © 2016 ACM.",Approximation algorithm; Consumer behavior; Model learning; Product selection; Submodular set function,Algorithms; Approximation algorithms; Commerce; Competition; Learning algorithms; Manufacture; Optimal systems; Optimization; Sales; Distance Metric Learning; Model learning; Product selection; Purchase decision; Real-world datasets; Selection problems; Submodular set functions; Synthetic datasets; Consumer behavior
Eigen-optimization on large graphs by edge manipulation,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976349037&doi=10.1145%2f2903148&partnerID=40&md5=f6162bb5aef173072ecb693e638913eb,"Large graphs are prevalent in many applications and enable a variety of information dissemination processes, e.g., meme, virus, and influence propagation. How can we optimize the underlying graph structure to affect the outcome of such dissemination processes in a desired way (e.g., stop a virus propagation, facilitate the propagation of a piece of good idea, etc)? Existing research suggests that the leading eigenvalue of the underlying graph is the key metric in determining the so-called epidemic threshold for a variety of dissemination models. In this paper, we study the problem of how to optimally place a set of edges (e.g., edge deletion and edge addition) to optimize the leading eigenvalue of the underlying graph, so that we can guide the dissemination process in a desired way. We propose effective, scalable algorithms for edge deletion and edge addition, respectively. In addition, we reveal the intrinsic relationship between edge deletion and node deletion problems. Experimental results validate the effectiveness and efficiency of the proposed algorithms. © 2016 ACM.",Edge manipulation; Graph mining; Immunization; Scalability,Immunization; Information dissemination; Scalability; Viruses; Edge manipulation; Effectiveness and efficiencies; Epidemic threshold; Graph mining; Node deletion; Scalable algorithms; Underlying graphs; Virus propagation; Eigenvalues and eigenfunctions
Heterogeneous translated hashing: A scalable solution towards multi-modal similarity search,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995581954&doi=10.1145%2f2744204&partnerID=40&md5=c8cdf077f1707b20394f8bd0033f054a,"Multi-modal similarity search has attracted considerable attention to meet the need of information retrieval across different types of media. To enable efficient multi-modal similarity search in large-scale databases recently, researchers start to study multi-modal hashing. Most of the existing methods are applied to search across multi-views among which explicit correspondence is provided. Given a multi-modal similarity search task, we observe that abundant multi-view data can be found on the Web which can serve as an auxiliary bridge. In this paper, we propose a Heterogeneous Translated Hashing (HTH) method with such auxiliary bridge incorporated not only to improve current multi-view search but also to enable similarity search across heterogeneous media which have no direct correspondence. HTH provides more flexible and discriminative ability by embedding heterogeneous media into different Hamming spaces, compared to almost all existing methods thatmap heterogeneous data in a commonHamming space.We formulate a joint optimization model to learn hash functions embedding heterogeneous media into different Hamming spaces, and a translator aligning different Hamming spaces. The extensive experiments on two real-world datasets, one publicly available dataset of Flickr, and the other MIRFLICKR-Yahoo Answers dataset, highlight the effectiveness and efficiency of our algorithm. © 2016 ACM.",Hash function learning; Heterogeneous translated hashing; Scalability; Similarity search,Optimization; Scalability; Discriminative ability; Effectiveness and efficiencies; Function learning; Heterogeneous media; Heterogeneous translated hashing; Large-scale database; Real-world datasets; Similarity search; Hash functions
Biomedical ontology quality assurance using a big data approach,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973497133&doi=10.1145%2f2768830&partnerID=40&md5=3a27e486dd1fcf83d2d4efe2511fbfd5,"This article presents recent progresses made in using scalable cloud computing environment, Hadoop and MapReduce, to perform ontology quality assurance (OQA), and points to areas of future opportunity. The standard sequential approach used for implementing OQA methods can take weeks if not months for exhaustive analyses for large biomedical ontological systems. With OQA methods newly implemented using massively parallel algorithms in the MapReduce framework, several orders of magnitude in speed-up can be achieved (e.g., from three months to three hours). Such dramatically reduced time makes it feasible not only to perform exhaustive structural analysis of large ontological hierarchies, but also to systematically track structural changes between versions for evolutional analysis. As an exemplar, progress is reported in using MapReduce to perform evolutional analysis and visualization on the Systemized Nomenclature of Medicine - Clinical Terms (SNOMED CT), a prominent clinical terminology system. Future opportunities in three areas are described: one is to extend the scope of MapReduce-based approach to existing OQA methods, especially for automated exhaustive structural analysis. The second is to apply our proposed MapReduce Pipeline for Lattice-based Evaluation (MaPLE) approach, demonstrated as an exemplar method for SNOMED CT, to other biomedical ontologies. The third area is to develop interfaces for reviewing results obtained by OQA methods and for visualizing ontological alignment and evolution, which can also take advantage of cloud computing technology to systematically pre-compute computationally intensive jobs in order to increase performance during user interactions with the visualization interface. Advances in these directions are expected to better support the ontological engineering lifecycle. © 2016 ACM.",Hadoop; Lattice; SNOMED CT; Terminology quality assurance,Cloud computing; Distributed computer systems; Multiprocessing systems; Ontology; Quality assurance; Structural analysis; Terminology; Visualization; Biomedical ontologies; Cloud computing environments; Cloud computing technologies; Hadoop; Lattice; Ontological engineering; SNOMED-CT; Systemized nomenclature of medicines; Big data
Unsupervised rare pattern mining: A survey,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973469334&doi=10.1145%2f2898359&partnerID=40&md5=1a62234da655ae752a25af7d0ae53ef9,"Association rule mining was first introduced to examine patterns among frequent items. The original motivation for seeking these rules arose from need to examine customer purchasing behaviour in supermarket transaction data. It seeks to identify combinations of items or itemsets, whose presence in a transaction affects the likelihood of the presence of another specific item or itemsets. In recent years, there has been an increasing demand for rare association rule mining. Detecting rare patterns in data is a vital task, with numerous high-impact applications including medical, finance, and security. This survey aims to provide a general, comprehensive, and structured overview of the state-of-the-art methods for rare pattern mining. We investigate the problems in finding rare rules using traditional association rule mining. As rare association rule mining has not been well explored, there is still specific groundwork that needs to be established. We will discuss some of the major issues in rare association rule mining and also look at current algorithms. As a contribution, we give a general framework for categorizing algorithms: Apriori and Tree based. We highlight the differences between these methods. Finally, we present several real-world application using rare pattern mining in diverse domains. We conclude our survey with a discussion on open and practical challenges in the field. © 2016 ACM.",Association rule mining; Infrequent patterns; Rare rules,Data mining; Surveys; Diverse domains; High impact; Infrequent patterns; Pattern mining; Rare association rules; Rare rules; State-of-the-art methods; Transaction data; Association rules
CGC: A flexible and robust approach to integrating co-regularized multi-domain graph for clustering,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973444495&doi=10.1145%2f2903147&partnerID=40&md5=f95938966dff91c6e248be929793dcb3,"Multi-view graph clustering aims to enhance clustering performance by integrating heterogeneous information collected in different domains. Each domain provides a different view of the data instances. Leveraging cross-domain information has been demonstrated an effective way to achieve better clustering results. Despite the previous success, existing multi-view graph clustering methods usually assume that different views are available for the same set of instances. Thus, instances in different domains can be treated as having strict one-to-one relationship. In many real-life applications, however, data instances in one domain may correspond to multiple instances in another domain. Moreover, relationships between instances in different domains may be associated with weights based on prior (partial) knowledge. In this article, we propose a flexible and robust framework, Co-regularized Graph Clustering (CGC), based on non-negative matrix factorization (NMF), to tackle these challenges. CGC has several advantages over the existing methods. First, it supports many-to-many cross-domain instance relationship. Second, it incorporates weight on cross-domain relationship. Third, it allows partial cross-domain mapping so that graphs in different domains may have different sizes. Finally, it provides users with the extent to which the cross-domain instance relationship violates the in-domain clustering structure, and thus enables users to re-evaluate the consistency of the relationship. We develop an efficient optimization method that guarantees to find the global optimal solution with a given confidence requirement. The proposed method can automatically identify noisy domains and assign smaller weights to them. This helps to obtain optimal graph partition for the focused domain. Extensive experimental results on UCI benchmark datasets, newsgroup datasets, and biological interaction networks demonstrate the effectiveness of our approach. © 2016 ACM.",Co-regularization; Graph clustering; Nonnegative matrix factorization,Clustering algorithms; Factorization; Biological interactions; Global optimal solutions; Graph clustering; Heterogeneous information; Multiple instances; Nonnegative matrix factorization; Optimization method; Real-life applications; Matrix algebra
Kernelized information-theoretic metric learning for cancer diagnosis using high-dimensional molecular profiling data,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973505603&doi=10.1145%2f2789212&partnerID=40&md5=6e06d93971c0c58e812856fea383d688,"With the advancement of genome-wide monitoring technologies, molecular expression data have become widely used for diagnosing cancer through tumor or blood samples. When mining molecular signature data, the process of comparing samples through an adaptive distance function is fundamental but difficult, as such datasets are normally heterogeneous and high dimensional. In this article, we present kernelized information-theoretic metric learning (KITML) algorithms that optimize a distance function to tackle the cancer diagnosis problem and scale to high dimensionality. By learning a nonlinear transformation in the input space implicitly through kernelization, KITML permits efficient optimization, low storage, and improved learning of distance metric. We propose two novel applications of KITML for diagnosing cancer using high-dimensional molecular profiling data: (1) for sample-level cancer diagnosis, the learned metric is used to improve the performance of k-nearest neighbor classification; and (2) for estimating the severity level or stage of a group of samples, we propose a novel set-based ranking approach to extend KITML. For the sample-level cancer classification task, we have evaluated on 14 cancer gene microarray datasets and compared with eight other state-of-the-art approaches. The results show that our approach achieves the best overall performance for the task of molecular-expression-driven cancer sample diagnosis. For the group-level cancer stage estimation, we test the proposed set-KITML approach using three multi-stage cancer microarray datasets, and correctly estimated the stages of sample groups for all three studies. © 2016 ACM.",Cancer diagnosis; High-dimensional data; Metric learning,Classification (of information); Clustering algorithms; Computer aided diagnosis; Data reduction; Diagnosis; Digital storage; Information theory; Mathematical transformations; Nearest neighbor search; Cancer classification; Cancer diagnosis; High dimensional data; K-nearest neighbor classification; Metric learning; Monitoring technologies; Non-linear transformations; State-of-the-art approach; Diseases
"Mining dual networks: Models, algorithms, and applications",2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973482184&doi=10.1145%2f2785970&partnerID=40&md5=b4818045535d397de50f755766a75e12,"Finding the densest subgraph in a single graph is a fundamental problem that has been extensively studied. In many emerging applications, there exist dual networks. For example, in genetics, it is important to use protein interactions to interpret genetic interactions. In this application, one network represents physical interactions among nodes, for example, protein-protein interactions, and another network represents conceptual interactions, for example, genetic interactions. Edges in the conceptual network are usually derived based on certain correlation measure or statistical test measuring the strength of the interaction. Two nodes with strong conceptual interaction may not have direct physical interaction. In this article, we propose the novel dual-network model and investigate the problem of finding the densest connected subgraph (DCS), which has the largest density in the conceptual network and is also connected in the physical network. Density in the conceptual network represents the average strength of the measured interacting signals among the set of nodes. Connectivity in the physical network shows how they interact physically. Such pattern cannot beidentified using the existing algorithms for a single network. Weshow that even though finding the densest subgraph in a single network is polynomial time solvable, the DCS problem is NP-hard. We develop a two-step approach to solve the DCS problem. In the first step, we effectively prune the dual networks, while guarantee that the optimal solution is contained in the remaining networks. For the second step, we develop two efficient greedy methods based on different search strategies to find the DCS. Different variations of the DCS problem are also studied. We perform extensive experiments on a variety of real and synthetic dual networks to evaluate the effectiveness and efficiency of the developed methods. © 2016 ACM.",Densest connected subgraphs (DCSs); Dual networks,Algorithms; Polynomial approximation; Proteins; Connected subgraphs; Correlation measures; Effectiveness and efficiencies; Emerging applications; Genetic interaction; Interacting signals; Physical interactions; Protein-protein interactions; Problem solving
Co-clustering structural temporal data with applications to semiconductor manufacturing,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973484230&doi=10.1145%2f2875427&partnerID=40&md5=efe4fc34b826628133e5bfd26de1bc6a,"Recent years have witnessed data explosion in semiconductor manufacturing due to advances in instrumentation and storage techniques. The large amount of data associated with process variables monitored over time form a rich reservoir of information, which can be used for a variety of purposes, such as anomaly detection, quality control, and fault diagnostics. In particular, following the same recipe for a certain Integrated Circuit device, multiple tools and chambers can be deployed for the production of this device, during which multiple time series can be collected, such as temperature, impedance, gas flow, electric bias, etc. These time series naturally fit into a two-dimensional array (matrix), i.e., each element in this array corresponds to a time series for one process variable from one chamber. To leverage the rich structural information in such temporal data, in this article, we propose a novel framework named C-Struts to simultaneously cluster on the two dimensions of this array. In this framework, we interpret the structural information as a set of constraints on the cluster membership, introduce an auxiliary probability distribution accordingly, and design an iterative algorithm to assign each time series to a certain cluster on each dimension. Furthermore, we establish the equivalence between C-Struts and a generic optimization problem, which is able to accommodate various distance functions. Extensive experiments on synthetic, benchmark, as well as manufacturing datasets demonstrate the effectiveness of the proposed method. © 2016 ACM.",Co-clustering; Semiconductor; Structural; Temporal,Algorithms; Digital storage; Electric tools; Flow of gases; Functions; Iterative methods; Manufacture; Optimization; Probability distributions; Semiconductor device manufacture; Semiconductor materials; Struts; Time series; Co-clustering; Integrated circuit devices; Multiple time series; Semiconductor manufacturing; Structural; Structural information; Temporal; Two-dimensional arrays; Cobalt compounds
Jointly modeling label and feature heterogeneity in medical informatics,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973444975&doi=10.1145%2f2768831&partnerID=40&md5=c0579a78d1de502b94438fa1ee02211f,"Multiple types of heterogeneity including label heterogeneity and feature heterogeneity often co-exist in many real-world data mining applications, such as diabetes treatment classification, gene functionality prediction, and brain image analysis. To effectively leverage such heterogeneity, in this article, we propose a novel graph-based model for Learning with both Label and Feature heterogeneity, namely L2F. It models the label correlation by requiring that any two label-specific classifiers behave similarly on the same views if the associated labels are similar, and imposes the view consistency by requiring that view-based classifiers generate similar predictions on the same examples. The objective function for L2F is jointly convex. To solve the optimization problem, we propose an iterative algorithm, which is guaranteed to converge to the global optimum. One appealing feature of L2F is that it is capable of handling data with missing views and labels. Furthermore, we analyze its generalization performance based on Rademacher complexity, which sheds light on the benefits of jointly modeling the label and feature heterogeneity. Experimental results on various biomedical datasets show the effectiveness of the proposed approach. © 2016 ACM.",Heterogeneous learning; Medical informatics; Multi-label learning; Multi-view learning,Brain mapping; Data mining; Graphic methods; Iterative methods; Data mining applications; Generalization performance; Heterogeneous learning; Medical informatics; Multi-label learning; Multi-view learning; Optimization problems; Rademacher complexity; Data handling
"Featuring, detecting, and visualizing human sentiment in Chinese micro-blog",2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973494717&doi=10.1145%2f2821513&partnerID=40&md5=b89bc9429d37f501b0be40a3921df748,"Micro-blog has been increasingly used for the public to express their opinions, and for organizations to detect public sentiment about social events or public policies. In this article, we examine and identify the key problems of this field, focusing particularly on the characteristics of innovative words, multi-media elements, and hierarchical structure of Chinese ""Weibo."" Based on the analysis, we propose a novel approach and develop associated theoretical and technological methods toaddress these problems. These include a new sentiment word mining method based on three wording metrics and point-wise information, a rule set model for analyzing sentiment features of different linguistic components, and the corresponding methodology for calculating sentiment on multi-granularity considering emoticon elements as auxiliary affective factors. We evaluate our new word discovery and sentiment detection methods on a real-life Chinese micro-blog dataset. Initial results show that our new diction can improve sentiment detection, and they demonstrate that our multi-level rule set methodis more effective, with the average accuracy being 10.2% and 1.5% higher than two existing methods for Chinese micro-blog sentiment analysis. In addition, we exploit visualization techniques to study the relationships between online sentiment and real life. The visualization of detected sentiment can help depict temporal patterns and spatial discrepancy. © 2016 ACM.",Rule set-based model; Sentiment detection; Sentiment lexicon expansion; Visualization,Flow visualization; Mining; Sentiment analysis; Visualization; Hierarchical structures; Public sentiments; Rule set; Sentiment detections; Sentiment features; Sentiment lexicons; Technological methods; Visualization technique; Blogs
Guest editorial: Special issue on connected health at big data era (BigChat): A TKDD special issue,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973443999&doi=10.1145%2f2912122&partnerID=40&md5=98e27c53b94d09c080d3a84d6fc805b3,[No abstract available],,
Less is more: Building selective anomaly ensembles,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973441855&doi=10.1145%2f2890508&partnerID=40&md5=5a9a769b1b1da39d0edf3d196836cc5a,"Ensemble learning for anomaly detection has been barely studied, due to difficulty in acquiring ground truth and the lack of inherent objective functions. In contrast, ensemble approaches for classification and clustering have been studied and effectively used for long. Our work taps into this gap and builds a new ensemble approach for anomaly detection, with application to event detection in temporal graphs as well as outlier detection in no-graph settings. It handles and combines multiple heterogeneous detectors to yield improved and robust performance. Importantly, trusting results from all the constituent detectors may deteriorate the overall performance of the ensemble, as some detectors could provide inaccurate results depending on the type of data in hand and the underlying assumptions of a detector. This suggests that combining the detectors selectively is key to building effective anomaly ensembles - hence ""less is more"". In this paper we propose a novel ensemble approach called SELECT for anomaly detection, which automatically and systematically selects the results from constituent detectors to combine in a fully unsupervised fashion. We apply our method to event detection in temporal graphs and outlier detection in multi-dimensional point data (no-graph), where SELECT successfully utilizes five base detectors and seven consensus methods under a unified ensemble framework. We provide extensive quantitative evaluation of our approach for event detection on five real-world datasets (four with ground truth events), including Enron email communications, Reality Mining SMS and phone call records, New York Times news corpus, and World Cup 2014 Twitter news feed. We also provide results for outlier detection on seven real-world multi-dimensional point datasets from UCI Machine Learning Repository. Thanks to its selection mechanism, SELECT yields superior performance compared to the individual detectors alone, the full ensemble (naively combining all results), an existing diversity-based ensemble, and an existing weighted ensemble approach. © 2016 ACM.",Anomaly ensembles; Anomaly mining; Dynamic graphs; Ensemble methods; Event detection; Rank aggregation; Unsupervised learning,Artificial intelligence; Data handling; Evolutionary algorithms; Learning systems; Signal detection; Unsupervised learning; Anomaly ensembles; Anomaly minings; Dynamic graph; Ensemble methods; Event detection; Rank aggregation; Statistics
Do anesthesiologists know what they are doing? Mining a surgical time-series database to correlate expert assessment with outcomes,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964598959&doi=10.1145%2f2822897&partnerID=40&md5=5ab60cd4037b93332ec7ee5ee107dcdf,"Anesthesiologists are taught to carefully manage patient vital signs during surgery. Unfortunately, there is little empirical evidence that vital sign management, as currently practiced, is correlated with patient outcomes. We seek to validate or repudiate current clinical practice and determine whether or not clinician evaluation of surgical vital signs correlate with outcomes. Using a database of over 90,000 cases, we attempt to determine whether those cases that anesthesiologists would subjectively decide are ""low quality"" are more likely to result in negative outcomes. The problem reduces to one of multi-dimensional time-series classification. Our approach is to have a set of expert anesthesiologists independently label a small number of training cases, from which we build classifiers and label all 90,000 cases. We then use the labeling to search for correlation with outcomes and compare the prevalence of important 30-day outcomes between providers. To mimic the providers' quality labels, we consider several standard classification methods, such as dynamic time warping in conjunction with a kNN classifier, as well as complexity invariant distance, and a regression based upon the feature extraction methods outlined by Mao et al. 2012 (using features such as time-series mean, standard deviation, skew, etc.). We also propose a new feature selection mechanism that learns a hidden Markov model to segment the time series; the fraction of time that each series spends in each state is used to label the series using a regression-based classifier. In the end, we obtain strong, empirical evidence that current best practice is correlated with reduced negative patient outcomes. We also learn that all of the experts were able to significantly separate cases by outcome, with higher prevalence of negative 30-day outcomes in the cases labeled as ""low quality"" for almost all of the outcomes investigated. © 2016 ACM.",Hidden Markov model (HMM); Ordinal regression; Vital signs,Anesthesiology; Feature extraction; Markov processes; Regression analysis; Surgery; Time series; Classification methods; Dynamic time warping; Feature extraction methods; Ordinal regression; Selection mechanism; Time series classifications; Time Series Database; Vital sign; Hidden Markov models
DELTACON: Principled massive-graph similarity function with attribution,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964791120&doi=10.1145%2f2824443&partnerID=40&md5=a033ce16d3706cd4e45a45ffea364426,"How much has a network changed since yesterday? How different is the wiring of Bob's brain (a left-handed male) and Alice's brain (a right-handed female), and how is it different? Graph similarity with given node correspondence, i.e., the detection of changes in the connectivity of graphs, arises in numerous settings. In this work, we formally state the axioms and desired properties of the graph similarity functions, and evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs. We propose DELTACON, a principled, intuitive, and scalable algorithm that assesses the similarity between two graphs on the same nodes (e.g., employees of a company, customers of a mobile carrier). In conjunction, we propose DELTACON-ATTR, a related approach that enables attribution of change or dissimilarity to responsible nodes and edges. Experiments on various synthetic and real graphs showcase the advantages of our method over existing similarity measures. Finally, we employ DELTACON and DELTACON-ATTR on real applications: (a) we classify people to groups of high and low creativity based on their brain connectivity graphs, (b) do temporal anomaly detection in the who-emails-whom Enron graph and find the top culprits for the changes in the temporal corporate email graph, and (c) recover pairs of test-retest large brain scans (∼17M edges, up to 90M edges) for 21 subjects. © 2016 ACM.",Anomaly detection; Culprit nodes and edges; Edge attribution; Graph classification; Graph comparison; Graph similarity; Network monitoring; Node attribution,Electronic mail; Graphic methods; Signal detection; Anomaly detection; Culprit nodes and edges; Edge attribution; Graph classification; Graph comparison; Graph similarity; Network Monitoring; Node attribution; Graph theory
Multimodal Data Mining in a Multimedia Database Based on Structured Max Margin Learning,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968865990&doi=10.1145%2f2742549&partnerID=40&md5=4e6bb712b12f7a460da5dbf6da396b3c,"Mining knowledge from a multimedia database has received increasing attentions recently since huge repositories are made available by the development of the Internet. In this article, we exploit the relations among different modalities in a multimedia database and present a framework for general multimodal data mining problem where image annotation and image retrieval are considered as the special cases. Specifically, the multimodal data mining problem can be formulated as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables. In addition, in order to reduce the demanding computation, we propose a new max margin structure learning approach called Enhanced Max Margin Learning (EMML) framework, which is much more efficient with a much faster convergence rate than the existing max margin learning methods, as verified through empirical evaluations. Furthermore, we apply EMML framework to develop an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale. The EMML framework allows an efficient multimodal data mining query in a very large scale multimedia database, and excels many existing multimodal data mining methods in the literature that do not scale up at all. The performance comparison with a state-of-the-art multimodal data mining method is reported for the real-world image databases. © 2016 ACM.",image annotation; image retrieval; maxmargin; Multimodal datamining,Database systems; Image analysis; Image retrieval; Query processing; Empirical evaluations; Image annotation; Interdependent output variables; maxmargin; Multi-modal; Performance comparison; Query response time; Structured prediction; Data mining
Mining User Development Signals for Online Community Churner Detection,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073102603&doi=10.1145%2f2798730&partnerID=40&md5=1f0acf2d31bf4b8808abe7dac18c3aaa,"Churners are users who stop using a given service after previously signing up. In the domain of telecommunications and video games, churners represent a loss of revenue as a user leaving indicates that they will no longer pay for the service. In the context of online community platforms (e.g., community message boards, social networking sites, question-answering systems, etc.), the churning of a user can represent different kinds of loss: of social capital, of expertise, or of a vibrant individual who is a mediator for interaction and communication. Detecting which users are likely to churn from online communities, therefore, enables community managers to offer incentives to entice those users back; as retention is less expensive than re-signing users up. In this article, we tackle the task of detecting churners on four online community platforms by mining user development signals. These signals explain how users have evolved along different dimensions (i.e., social and lexical) relative to their prior behaviour and the community in which they have interacted. We present a linear model, based upon elastic-net regularisation, that uses extracted features from the signals to detect churners. Our evaluation of this model against several state of the art baselines, including our own prior work, empirically demonstrates the superior performance that this approach achieves for several experimental settings. This article presents a novel approach to churn prediction that takes a different route from existing approaches that are based on measuring static social network properties of users (e.g., centrality, in-degree, etc.).  © 2016 ACM.",Churn prediction; lexical terms; lifecycle mining; online communities; social dynamics,Online systems; Churn predictions; Lexical term; Lifecycle mining; Linear modeling; Message boards; On-line communities; Question answering systems; Social capitals; Social dynamics; Video-games; Social networking (online)
Adaptive model rules from high-speed data streams,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055849147&doi=10.1145%2f2829955&partnerID=40&md5=e3152c9963fb51b9a2c7e204f27eedfa,"Decision rules are one of the most expressive and interpretable models for machine learning. In this article, we present Adaptive Model Rules (AMRules), the first stream rule learning algorithm for regression problems. In AMRules, the antecedent of a rule is a conjunction of conditions on the attribute values, and the consequent is a linear combination of the attributes. In order to maintain a regression model compatible with the most recent state of the process generating data, each rule uses a Page-Hinkley test to detect changes in this process and react to changes by pruning the rule set. Online learning might be strongly affected by outliers. AMRules is also equipped with outliers detection mechanisms to avoid model adaption using anomalous examples. In the experimental section, we report the results of AMRules on benchmark regression problems, and compare the performance of our system with other streaming regression algorithms. © 2016 ACM",And Phrases: Data streams; Regression; Rule learning,Benchmarking; Learning systems; Regression analysis; Statistics; Data stream; Experimental section; Linear combinations; Outliers detection; Regression; Regression algorithms; Rule learning; Rule learning algorithms; Learning algorithms
An efficient algorithm for weak hierarchical lasso,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048351879&doi=10.1145%2f2791295&partnerID=40&md5=39cbaaf2f4b441e8b9a5c52ff2860eba,"Linear regression is a widely used tool in data mining and machine learning. In many applications, fitting a regression model with only linear effects may not be sufficient for predictive or explanatory purposes. One strategy that has recently received increasing attention in statistics is to include feature interactions to capture the nonlinearity in the regression model. Such model has been applied successfully in many biomedical applications. One major challenge in the use of such model is that the data dimensionality is significantly higher than the original data, resulting in the small sample size large dimension problem. Recently, weak hierarchical Lasso, a sparse interaction regression model, is proposed that produces a sparse and hierarchical structured estimator by exploiting the Lasso penalty and a set of hierarchical constraints. However, the hierarchical constraints make it a non-convex problem and the existing method finds the solution to its convex relaxation, which needs additional conditions to guarantee the hierarchical structure. In this article, we propose to directly solve the non-convex weak hierarchical Lasso by making use of the General Iterative Shrinkage and Thresholding (GIST) optimization framework, which has been shown to be efficient for solving non-convex sparse formulations. The key step in GIST is to compute a sequence of proximal operators. One of our key technical contributions is to show that the proximal operator associated with the non-convex weak hierarchical Lasso admits a closed-form solution. However, a naive approach for solving each subproblem of the proximal operator leads to a quadratic time complexity, which is not desirable for large-size problems. We have conducted extensive experiments on both synthetic and real datasets. Results show that our proposed algorithm is much more efficient and effective than its convex relaxation. To this end, we further develop an efficient algorithm for computing the subproblems with a linearithmic time complexity. In addition, we extend the technique to perform the optimization-based hierarchical testing of pairwise interactions for binary classification problems, which is essentially the proximal operator associated with weak hierarchical Lasso. Simulation studies show that the non-convex hierarchical testing framework outperforms the convex relaxation when a hierarchical structure exists between main effects and interactions. © 2016 ACM.",And Phrases: Sparse learning; Non-convex; Proximal operator; Weak hierarchical Lasso,Data mining; Iterative methods; Learning systems; Medical applications; Relaxation processes; And Phrases: Sparse learning; Binary classification problems; Biomedical applications; Hierarchical structures; Non-convex; Optimization framework; Proximal operator; Weak hierarchical Lasso; Regression analysis
Mining product adopter information from online reviews for improving product recommendation,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964682683&doi=10.1145%2f2842629&partnerID=40&md5=fe8f1d47e2d00bac683a33c27045f7cd,"We present in this article an automated framework that extracts product adopter information from online reviews and incorporates the extracted information into feature-based matrix factorization formore effective product recommendation. In specific, we propose a bootstrapping approach for the extraction of product adopters from review text and categorize them into a number of different demographic categories. The aggregated demographic information of many product adopters can be used to characterize both products and users in the form of distributions over different demographic categories. We further propose a graphbased method to iteratively update user- and product-related distributions more reliably in a heterogeneous user-product graph and incorporate them as features into the matrix factorization approach for product recommendation. Our experimental results on a large dataset crawled from JINGDONG, the largest B2C e-commerce website in China, show that our proposed framework outperforms a number of competitive baselines for product recommendation.",Matrix factorisation; Online review; Product adopter; Product recommendation,Factorization; Iterative methods; Population statistics; Demographic information; Graph-based methods; Heterogeneous users; Matrix factorisation; Matrix factorizations; Online reviews; Product adopter; Product recommendation; Matrix algebra
Synchronization-core-based discovery of processes with decomposable cyclic dependencies,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015994879&doi=10.1145%2f2845086&partnerID=40&md5=5ac9ae8f18de6a26f895a524d864e42f,"Traditional process discovery techniques mine process models based upon event traces giving little consideration to workflow relevant data recorded in event logs. The neglect of such information usually leads to incorrect discovered models, especially when activities have decomposable cyclic dependencies. To address this problem, the recorded workflow relevant data and decision tree learning technique are utilized to classify cases into case clusters. Each case cluster contains causality and concurrency activity dependencies only. Then, a set of activity ordering relations are derived based on case clusters. And a synchronization-core-based process model is discovered from the ordering relations and composite cases. Finally, the discovered model is transformed to a BPMN model. The proposed approach is validated with a medical treatment process and an open event log. Meanwhile, a prototype system is presented. c 2016 ACM",And Phrases: Process mining; BPMN; Decision tree; Process discovery,Decision trees; BPMN; Cyclic dependencies; Decision tree learning; Medical treatment; Process Discovery; Process mining; Process Modeling; Prototype system; Trees (mathematics)
Mining influencers using information flows in social streams,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991681135&doi=10.1145%2f2815625&partnerID=40&md5=7bf8989bd9af7b243c9468b0b86430ce,"The problem of discovering information flow trends in social networks has become increasingly relevant due to the increasing amount of content in online social networks, and its relevance as a tool for research into the content trends analysis in the network. An important part of this analysis is to determine the key patterns of flow in the underlying network. Almost all the work in this area has focused on fixed models of the network structure, and edge-based transmission between nodes. In this article, we propose a fully content-centered model of flow analysis in networks, in which the analysis is based on actual content transmissions in the underlying social stream, rather than a static model of transmission on the edges. First, we introduce the problem of influence analysis in the context of information flow in networks. We then propose a novel algorithm InFlowMine to discover the information flow patterns in the network and demonstrate the effectiveness of the discovered information flows using an influence mining application. This application illustrates the flexibility and effectiveness of our information flow model to find topic- or network-specific influencers, or their combinations. We empirically show that our information flow mining approach is effective and efficient than the existing methods on a number of different measures. © 2016 ACM",Influencer analysis; Information flows; Network analysis,Electric network analysis; Social networking (online); Content transmission; Influence analysis; Influencer analysis; Information flow model; Information flows; Network structures; On-line social networks; Underlying networks; Information use
Collective graph identification,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046531955&doi=10.1145%2f2818378&partnerID=40&md5=de3afb3b82c5cfc35819e5b6a42e32b8,"Data describing networks—such as communication networks, transaction networks, disease transmission networks, collaboration networks, etc.—are becoming increasingly available. While observational data can be useful, it often only hints at the actual underlying process that governs interactions and attributes. For example, an email communication network provides insight into its users and their relationships, but is not the same as the “real” underlying social network. In this article, we introduce the problem of graph identification, i.e., discovering the latent graph structure underlying an observed network. We cast the problem as a probabilistic inference task, in which we must infer the nodes, edges, and node labels of a hidden graph, based on evidence. This entails solving several canonical problems in network analysis: entity resolution (determining when two observations correspond to the same entity), link prediction (inferring the existence of links), and node labeling (inferring hidden attributes). While each of these subproblems has been well studied in isolation, here we consider them as a single, collective task. We present a simple, yet novel, approach to address all three subproblems simultaneously. Our approach, which we refer to as C3, consists of a collection of Coupled Collective Classifiers that are applied iteratively to propagate inferred information among the subproblems. We consider variants of C3 using different learning and inference techniques and empirically demonstrate that C3 is superior, both in terms of predictive accuracy and running time, to state-of-the-art probabilistic approaches on four real problems. © 2016 ACM",And Phrases: Entity resolution; Collective classification; Link prediction; Semi-supervised learning,Classification (of information); Iterative methods; Supervised learning; Collaboration network; Collective classifications; Entity resolutions; Inference techniques; Link prediction; Probabilistic approaches; Probabilistic inference; Semi- supervised learning; Problem solving
Toward generalizing the unification with statistical outliers: The gradient outlier factor measure,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994569955&doi=10.1145%2f2829956&partnerID=40&md5=b9cb301c10fc2fedd60f9cf4332c9dfb,"In this work, we introduce a novel definition of outlier, namely the Gradient Outlier Factor (or GOF), with the aim to provide a definition that unifies with the statistical one on some standard distributions but has a different behavior in the presence of mixture distributions. Intuitively, the GOF score measures the probability to stay in the neighborhood of a certain object. It is directly proportional to the density and inversely proportional to the variation of the density. We derive formal properties under which the GOF definition unifies the statistical outlier definition and show that the unification holds for some standard distributions, while the GOF is able to capture tails in the presence of different distributions even if their densities sensibly differ. Moreover, we provide a probabilistic interpretation of the GOF score, by means of the notion of density of the data density. Experimental results confirm that there are scenarios in which the novel definition can be profitably employed. To the best of our knowledge, except for distance-based outlier, no other data mining outlier definition has a so clearly established relationship with statistical outliers. © 2016 ACM.",,Data mining; Data density; Different distributions; Distance-based; Formal properties; Mixture distributions; Probabilistic interpretation; Standard distributions; Statistics
Put three and three together: Triangle-driven community detection,2016,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028414408&doi=10.1145%2f2775108&partnerID=40&md5=0aeb2efacdf96bbb9287dc22a8b8533a,"Community detection has arisen as one of the most relevant topics in the field of graph data mining due to its applications in many fields such as biology, social networks, or network traffic analysis. Although the existing metrics used to quantify the quality of a community work well in general, under some circumstances, they fail at correctly capturing such notion. The main reason is that these metrics consider the internal community edges as a set, but ignore how these actually connect the vertices of the community. We propose the Weighted Community Clustering (WCC), which is a new community metric that takes the triangle instead of the edge as the minimal structural motif indicating the presence of a strong relation in a graph. We theoretically analyse WCC in depth and formally prove, by means of a set of properties, that the maximization of WCC guarantees communities with cohesion and structure. In addition, we propose Scalable Community Detection (SCD), a community detection algorithm based on WCC, which is designed to be fast and scalable on SMP machines, showing experimentally that WCC correctly captures the concept of community in social networks using real datasets. Finally, using ground-truth data, we show that SCD provides better quality than the best disjoint community detection algorithms of the state of the art while performing faster. © 2016 ACM.",Community detection; Parallel algorithm; Scalable algorithm; Social networks; Triangles,Data mining; Parallel algorithms; Signal detection; Social networking (online); Community detection; Community detection algorithms; Ground truth data; ITS applications; Scalable algorithms; State of the art; Structural motifs; Triangles; Population dynamics
Congestion-aware Spatio-Temporal Graph Convolutional Network-based A∗Search Algorithm for Fastest Route Search,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196742273&doi=10.1145%2f3657640&partnerID=40&md5=f1266edb2592e169cdc1db26544db460,"The fastest route search, which is to find a path with the shortest travel time when the user initiates a query, has become one of the most important services in many map applications. To enhance the user experience of travel, it is necessary to achieve accurate and real-time route search. However, traffic conditions are changing dynamically, and the frequent occurrence of traffic congestion may greatly increase travel time. Thus, it is challenging to achieve the above goal. To deal with it, we present a congestion-aware spatio-temporal graph convolutional network-based A∗search algorithm for the task of fastest route search. We first identify a sequence of consecutive congested traffic conditions as a traffic congestion event. Then, we propose a spatio-temporal graph convolutional network that jointly models the congestion events and changing travel time to capture their complex spatio-temporal correlations, which can predict the future travel-time information of each road segment as the basis of route planning. Further, we design a path-aided neural network to achieve effective origin-destination (OD) shortest travel-time estimation by encoding the complex relationships between OD pairs and their corresponding fastest paths. Finally, the cost function in the A∗algorithm is set by fusing the output results of the two components, which is used to guide the route search. Our experimental results on the two real-world datasets show the superior performance of the proposed method. © 2024 Copyright held by the owner/author(s).",A; search algorithm; spatio-temporal mining; traffic congestion; travel-time estimation,Complex networks; Convolution; Cost functions; Graph theory; Motor transportation; Travel time; A; Congestion-aware; Convolutional networks; Fastest route; Network-based; Search Algorithms; Shortest travel time; Spatio-temporal graphs; Spatio-temporal minings; Travel time estimation; Traffic congestion
A Fully test-time training framework for semi-supervised node classification on out-of-distribution graphs,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192555526&doi=10.1145%2f3649507&partnerID=40&md5=d67ecf20858f06fa1872b7c3772d4b97,"Graph neural networks (GNNs) have shown great potential in representation learning for various graph tasks. However, the distribution shift between the training and test sets poses a challenge to the efficiency of GNNs. To address this challenge, HomoTTT proposes a fully test-time training framework for GNNs to enhance the model's generalization capabilities for node classification tasks. Specifically, our proposed HomoTTT designs a homophily-based and parameter-free graph contrastive learning task with adaptive augmentation to guide the model's adaptation during the test-time training, allowing the model to adapt for specific target data. In the inference stage, HomoTTT proposes to integrate the original GNN model and the adapted model after TTT using a homophily-based model selection method, which prevents potential performance degradation caused by unconstrained model adaptation. Extensive experimental results on six benchmark datasets demonstrate the effectiveness of our proposed framework. Additionally, the exploratory study further validates the rationality of the homophily-based graph contrastive learning task with adaptive augmentation and the homophily-based model selection designed in HomoTTT.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive learning; graph neural network; Out-of-distribution,Graph theory; Learning systems; Contrastive learning; Graph neural networks; Homophily; Learning tasks; Model Adaptation; Out-of-distribution; Semi-supervised; Test time; Training framework; Training sets; Graph neural networks
Enhancing unsupervised outlier model selection: A study on ireos algorithms,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196842042&doi=10.1145%2f3653719&partnerID=40&md5=750ba059cfbf6601957a307c415cf609,"Outlier detection stands as a critical cornerstone in the field of data mining, with a wide range of applications spanning from fraud detection to network security. However, real-world scenarios often lack labeled data for training, necessitating unsupervised outlier detection methods. This study centers on Unsupervised Outlier Model Selection (UOMS), with a specific focus on the family of Internal, Relative Evaluation of Outlier Solutions (IREOS) algorithms. IREOS measures outlier candidate separability by evaluating multiple maximum-margin classifiers and, while effective, it is constrained by its high computational demands. We investigate the impact of several different separation methods in UOMS in terms of ranking quality and runtime. Surprisingly, our findings indicate that different separability measures have minimal impact on IREOS' effectiveness. However, using linear separation methods within IREOS significantly reduces its computation time. These insights hold significance for real-world applications where efficient outlier detection is critical. In the context of this work, we provide the code for the IREOS algorithm and our separability techniques.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",anomaly detection; model selection; Outlier detection; unsupervised evaluation,Data mining; Network security; Statistics; Anomaly detection; Fraud detection; Labeled data; Model Selection; Networks security; Outlier Detection; Real-world scenario; Separation methods; Solution algorithms; Unsupervised evaluation; Anomaly detection
LMACL: Improving Graph Collaborative Filtering with Learnable Model Augmentation Contrastive Learning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196748529&doi=10.1145%2f3657302&partnerID=40&md5=ca3bfd2e6c2fbf615e4636c4a6ea9576,"Graph collaborative filtering (GCF) has achieved exciting recommendation performance with its ability to aggregate high-order graph structure information. Recently, contrastive learning (CL) has been incorporated into GCF to alleviate data sparsity and noise issues. However, most of the existing methods employ random or manual augmentation to produce contrastive views that may destroy the original topology and amplify the noisy effects. We argue that such augmentation is insufficient to produce the optimal contrastive view, leading to suboptimal recommendation results. In this article, we proposed a Learnable Model Augmentation Contrastive Learning (LMACL) framework for recommendation, which effectively combines graph-level and node-level collaborative relations to enhance the expressiveness of collaborative filtering (CF) paradigm. Specifically, we first use the graph convolution network (GCN) as a backbone encoder to incorporate multi-hop neighbors into graph-level original node representations by leveraging the high-order connectivity in user-item interaction graphs. At the same time, we treat the multi-head graph attention network (GAT) as an augmentation view generator to adaptively generate high-quality node-level augmented views. Finally, joint learning endows the end-to-end training fashion. In this case, the mutual supervision and collaborative cooperation of GCN and GAT achieves learnable model augmentation. Extensive experiments on several benchmark datasets demonstrate that LMACL provides a significant improvement over the strongest baseline in terms of Recall and NDCG by 2.5%-3.8% and 1.6%-4.0%, respectively. Our model implementation code is available at https://github.com/LiuHsinx/LMACL. © 2024 Copyright held by the owner/author(s).",collaborative filtering; contrastive learning; graph neural network; Recommender systems,Collaborative filtering; Graph theory; Learning systems; Recommender systems; Contrastive learning; Data noise; Data sparsity; Graph neural networks; Graph structures; High-order; Higher-order; Order graph; Recommendation performance; Structure information; Graph neural networks
Towards robust rumor detection with graph contrastive and curriculum learning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196848370&doi=10.1145%2f3653023&partnerID=40&md5=af0ed5f1244d3fce6d81ebad139160ce,"Establishing a robust rumor detection model is vital in safeguarding the veracity of information on social media platforms. However, existing approaches to stopping rumor from spreading rely on abundant and clean training data, which is rarely available in real-world scenarios. In this work, we aim to develop a trustworthy rumor detection model that can handle inadequate and noisy labeled data. Our work addresses robust rumor detection, including classic and early detection, as well as five types of robustness issues: noisy and incomplete propagation, label scarcity and noise, and user disappearance. We propose a novel method, Robustness-Enhanced Rumor Detection (RERD), which mainly leverages the information propagation graphs of source tweets, along with user profiles and retweeting knowledge, for model learning. The novelty of RERD is four-fold. First, we jointly exploit the propagation structures of non-text and text retweets to learn the representation of a source tweet. Second, we simultaneously utilize the top-down and bottom-up information flows with relational propagations for graph representation learning. Third, to have effective early and robust detection, we implement contrastive learning on graphs with early and complete views of information propagation so that small snapshots can foresee their future shapes. Last, we use curriculum pseudo-labeling to mitigate the impact of label scarcity and noisy labels, and to correct representations learned from corrupted data. Experimental results on three benchmark datasets demonstrate that RERD consistently outperforms competitors in classic, early, and robust rumor detection scenarios. To the best of our knowledge, we are the first to simultaneously cope with early and five robust detections of rumors.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",curriculum labeling; graph contrastive learning; graph neural networks; propagation graphs; Rumor detection,Backpropagation; Graph neural networks; Information dissemination; Knowledge management; Curriculum labeling; Detection models; Graph contrastive learning; Graph neural networks; Information propagation; Labelings; Propagation graph; Robust detection; Rumor detection; Social media platforms; Curricula
Mixed Graph Contrastive Network for Semi-supervised Node Classification,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196798417&doi=10.1145%2f3641549&partnerID=40&md5=211afe697af11d0081c32257eafba01e,"Graph Neural Networks (GNNs) have achieved promising performance in semi-supervised node classification in recent years. However, the problem of insufficient supervision, together with representation collapse, largely limits the performance of the GNNs in this field. To alleviate the collapse of node representations in semi-supervised scenario, we propose a novel graph contrastive learning method, termed Mixed Graph Contrastive Network (MGCN). In our method, we improve the discriminative capability of the latent embeddings by an interpolation-based augmentation strategy and a correlation reduction mechanism. Specifically, we first conduct the interpolation-based augmentation in the latent space and then force the prediction model to change linearly between samples. Second, we enable the learned network to tell apart samples across two interpolation-perturbed views through forcing the correlation matrix across views to approximate an identity matrix. By combining the two settings, we extract rich supervision information from both the abundant unlabeled nodes and the rare yet valuable labeled nodes for discriminative representation learning. Extensive experimental results on six datasets demonstrate the effectiveness and the generality of MGCN compared to the existing state-of-the-art methods. The code of MGCN is available at https://github.com/xihongyang1999/MGCN on Github.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive learning; graph neural network; Semi-supervised classification,Data mining; Graph neural networks; Graph theory; Learning systems; Supervised learning; Contrastive learning; Embeddings; Graph neural networks; Learning methods; Mixed graph; Performance; Prediction modelling; Reduction mechanisms; Semi-supervised; Semisupervised classification (SSC); Interpolation
ID-SR: Privacy-Preserving Social Recommendation Based on Infinite Divisibility for Trustworthy AI,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196795547&doi=10.1145%2f3639412&partnerID=40&md5=bf5afba311c6b4d378b976c7ccc038c2,"Recommendation systems powered by artificial intelligence (AI) are widely used to improve user experience. However, AI inevitably raises privacy leakage and other security issues due to the utilization of extensive user data. Addressing these challenges can protect users' personal information, benefit service providers, and foster service ecosystems. Presently, numerous techniques based on differential privacy have been proposed to solve this problem. However, existing solutions encounter issues such as inadequate data utilization and a tenuous trade-off between privacy protection and recommendation effectiveness. To enhance recommendation accuracy and protect users' private data, we propose ID-SR, a novel privacy-preserving social recommendation scheme for trustworthy AI based on the infinite divisibility of Laplace distribution. We first introduce a novel recommendation method adopted in ID-SR, which is established based on matrix factorization with a newly designed social regularization term for improving recommendation effectiveness. We then propose a differential privacy-preserving scheme tailored to the above method that leverages the Laplace distribution's characteristics to safeguard user data. Theoretical analysis and experimentation evaluation on two publicly available datasets demonstrate that our scheme achieves a superior balance between privacy protection and recommendation effectiveness, ultimately delivering an enhanced user experience.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",differential privacy; Laplace mechanism; matrix factorization; Social recommendation; trustworthy artificial intelligence,Economic and social effects; Laplace transforms; Matrix algebra; Matrix factorization; Privacy-preserving techniques; Differential privacies; Infinite divisibility; Laplace mechanism; Matrix factorizations; Privacy preserving; Privacy protection; Social recommendation; Trustworthy artificial intelligence; User data; Users' experiences; Artificial intelligence
Automatically inspecting thousands of static bug warnings with large language model: How far are we?,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190890871&doi=10.1145%2f3653718&partnerID=40&md5=7f8eb0d5e71dab2e4063132b0745b3f7,"Static analysis tools for capturing bugs and vulnerabilities in software programs are widely employed in practice, as they have the unique advantages of high coverage and independence from the execution environment. However, existing tools for analyzing large codebases often produce a great deal of false warnings over genuine bug reports. As a result, developers are required to manually inspect and confirm each warning, a challenging, time-consuming, and automation-essential task.This article advocates a fast, general, and easily extensible approach called Llm4sa that automatically inspects a sheer volume of static warnings by harnessing (some of) the powers of Large Language Models (LLMs). Our key insight is that LLMs have advanced program understanding capabilities, enabling them to effectively act as human experts in conducting manual inspections on bug warnings with their relevant code snippets. In this spirit, we propose a static analysis to effectively extract the relevant code snippets via program dependence traversal guided by the bug warning reports themselves. Then, by formulating customized questions that are enriched with domain knowledge and representative cases to query LLMs, Llm4sa can remove a great deal of false warnings and facilitate bug discovery significantly. Our experiments demonstrate that Llm4sa is practical in automatically inspecting thousands of static warnings from Juliet benchmark programs and 11 real-world C/C++ projects, showcasing a high precision (81.13%) and a recall rate (94.64%) for a total of 9,547 bug warnings. Our research introduces new opportunities and methodologies for using the LLMs to reduce human labor costs, improve the precision of static analyzers, and ensure software trustworthiness  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AI for program analysis; false alarms; Large language model; static analysis; static bug warning,C++ (programming language); Codes (symbols); Computational linguistics; Computer software; Inspection; Program debugging; Wages; AI for program analyse; Analysis tools; Bug reports; Execution environments; Falsealarms; Language model; Large language model; Program analysis; Software project; Static bug warning; Static analysis
Computing Random Forest-distances in the presence of missing data,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196740786&doi=10.1145%2f3656345&partnerID=40&md5=dc13cc824d6e390a1f20b9aa1fe3d287,"In this article, we study the problem of computing Random Forest-distances in the presence of missing data. We present a general framework which avoids pre-imputation and uses in an agnostic way the information contained in the input points. We centre our investigation on RatioRF, an RF-based distance recently introduced in the context of clustering and shown to outperform most known RF-based distance measures. We also show that the same framework can be applied to several other state-of-the-art RF-based measures and provide their extensions to the missing data case. We provide significant empirical evidence of the effectiveness of the proposed framework, showing extensive experiments with RatioRF on 15 datasets. Finally, we also positively compare our method with many alternative literature distances, which can be computed with missing values. © 2024 Copyright held by the owner/author(s).",missing data; Random forest distances; RatioRF measure,Clusterings; Distance measure; Missing data; Missing values; Random forest distance; Random forests; Ratiorf measure; State of the art; Data mining
Properties of Fairness Measures in the Context of Varying Class Imbalance and Protected Group Ratios,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196798423&doi=10.1145%2f3654659&partnerID=40&md5=2860f3294b71812c145c41d5a7ca149f,"Society is increasingly relying on predictive models in fields like criminal justice, credit risk management, and hiring. To prevent such automated systems from discriminating against people belonging to certain groups, fairness measures have become a crucial component in socially relevant applications of machine learning. However, existing fairness measures have been designed to assess the bias between predictions for protected groups without considering the imbalance in the classes of the target variable. Current research on the potential effect of class imbalance on fairness focuses on practical applications rather than dataset-independent measure properties. In this article, we study the general properties of fairness measures for changing class and protected group proportions. For this purpose, we analyze the probability mass functions of six of the most popular group fairness measures. We also measure how the probability of achieving perfect fairness changes for varying class imbalance ratios. Moreover, we relate the dataset-independent properties of fairness measures described in this work to classifier fairness in real-life tasks. Our results show that measures such as Equal Opportunity and Positive Predictive Parity are more sensitive to changes in class imbalance than Accuracy Equality. These findings can help guide researchers and practitioners in choosing the most appropriate fairness measures for their classification problems.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",class imbalance; Group fairness; protected group imbalance,Automation; Risk assessment; Risk management; Automated systems; Class imbalance; Credit risk management; Criminal justice; Fairness measures; Group fairness; In-field; Predictive models; Property; Protected group imbalance; Classification (of information)
Dual-Side Adversarial Learning Based Fair Recommendation for Sensitive Attribute Filtering,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196804331&doi=10.1145%2f3648683&partnerID=40&md5=58aaf83e1a4f89504428de3fd99beb30,"With the development of recommendation algorithms, researchers are paying increasing attention to fairness issues such as user discrimination in recommendations. To address these issues, existing works often filter users' sensitive information that may cause discrimination during the process of learning user representations. However, these approaches overlook the latent relationship between items' content attributes and users' sensitive information. In this article, we propose DALFRec, a fairness-aware recommendation algorithm based on user-side and item-side adversarial learning to mitigate the effects of sensitive information on both sides of the recommendation process. First, we conduct a statistical analysis to demonstrate the latent relationship between items' information and users' sensitive attributes. Then, we design a dual-side adversarial learning network that simultaneously filters out users' sensitive information on the user and item side. Additionally, we propose a new evaluation strategy that leverages the latent relationship between items' content attributes and users' sensitive attributes to better assess the algorithm's ability to reduce discrimination. Our experiments on three real datasets demonstrate the superiority of our proposed algorithm over state-of-the-art methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial learning; fairness; Recommender system,Recommender systems; Adversarial learning; Evaluation strategies; Fairness; Learning network; Process of learning; Real data sets; Recommendation algorithms; Sensitive attribute; Sensitive informations; State-of-the-art methods; Learning systems
Voxel-wise medical image generalization for eliminating distribution shift,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196769348&doi=10.1145%2f3643034&partnerID=40&md5=5befd29514999920417a444bda82a27c,"Currently, the medical field is witnessing an increase in the use of machine learning techniques. Supervised learning methods adopted in classification, prediction, and segmentation tasks for medical images always experience decreased performance when the training and testing datasets do not follow the independent and identically distributed assumption. These distribution shift situations seriously influence machine learning applications' robustness, fairness, and trustworthiness in the medical domain. Hence, in this article, we adopt the CycleGAN (generative adversarial network) method to cycle train the computed tomography data from different scanners/manufacturers. It aims to eliminate the distribution shift from diverse data terminals based on our previous work [14]. However, due to the model collapse problem and generative mechanisms of the GAN-based model, the images we generated contained serious artifacts. To remove the boundary marks and artifacts, we adopt score-based diffusion generative models to refine the images voxel-wisely. This innovative combination of two generative models enhances the quality of data providers while maintaining significant features. Meanwhile, we use five paired patients' medical images to deal with the evaluation experiments with structural similarity index measure metrics and the segmentation model's performance comparison. We conclude that CycleGAN can be utilized as an efficient data augmentation technique rather than a distribution-shift-eliminating method. In contrast, the denoising diffusion the denoising diffusion model is more suitable for dealing with the distribution shift problem aroused by the different terminal modules. The limitation of generative methods applied in medical images is the difficulty in obtaining large and diverse datasets that accurately capture the complexity of biological structure and variability. In our following research, we plan to assess the initial and generated datasets to explore more possibilities to overcome the above limitation. We will also incorporate the generative methods into the federated learning architecture, which can maintain their advantages and resolve the distribution shift issue on a larger scale.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CycleGAN; Data fairness; machine learning; model robustness; score-based generative model,Classification (of information); Computerized tomography; Diffusion; Generative adversarial networks; Image segmentation; Medical imaging; Supervised learning; Cyclegan; Data fairness; De-noising; Generalisation; Generative methods; Generative model; Machine-learning; Medical fields; Model robustness; Score-based generative model; Large datasets
Attacking Social Media via Behavior Poisoning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194009856&doi=10.1145%2f3654673&partnerID=40&md5=498aa8ebea0f36e56712e221fe8883d4,"Since social media such as Facebook and X (formerly known as Twitter) have permeated various aspects of daily life, people have strong incentives to influence information dissemination on these platforms and differentiate their content from the fierce competition. Existing dissemination strategies typically employ marketing techniques, such as seeking publicity through renowned actors or targeted advertising placements. Despite their various forms, most simply spread information to strengthen user impressions without conducting formal analyses of specific influence enhancement. And coupled with high costs, most fall short of expectations. To this end, we ingeniously formulate the task of social media dissemination as poisoning attacks, which influence specified content's dissemination among target users by intervening in some users' social media behaviors (including retweeting, following, and profile modifying). Correspondingly, we propose a novel poisoning attack, Influence-based Social Media Attack (ISMA) to generate discrete poisoning behaviors, which is difficult to achieve with existing attacks. In ISMA, we first contribute an efficient influence evaluator to quantify the spread influence of poisoning behaviors. Based on the estimated influence, we then present an imperceptible hierarchical selector and a profile modification method ProMix to select influential behaviors to poison. Notably, our attack is driven by custom attack objectives, which allows one to flexibly design different optimization goals to change the information flow, which could solve the blindness of existing influence maximization methods. Besides, behaviors such as retweeting are gentle and simple to implement. These properties make our attack more cost-effective and practical. Extensive experiments on two large-scale real-world datasets demonstrate the superiority of our method as it significantly outperforms baselines, and additionally, the proposed evaluator's analysis of user influence provides new insights for influence maximization on social media.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",poisoning attacks; social activity; Social media,Cost effectiveness; Large datasets; Marketing; Social networking (online); User profile; Daily lives; Facebook; Formal analysis; Influence maximizations; Marketing techniques; Poisoning attacks; Poisoning behavior; Social activities; Social media; Targeted advertising; Information dissemination
FairGAT: Fairness-Aware Graph Attention Networks,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196840181&doi=10.1145%2f3645096&partnerID=40&md5=5d2021dd38d83e1d02acb7d4b74bcd57,"Graphs can facilitate modeling various complex systems such as gene networks and power grids as well as analyzing the underlying relations within them. Learning over graphs has recently attracted increasing attention, particularly graph neural network (GNN)-based solutions, among which graph attention networks (GATs) have become one of the most widely utilized neural network structures for graph-based tasks. Although it is shown that the use of graph structures in learning results in the amplification of algorithmic bias, the influence of the attention design in GATs on algorithmic bias has not been investigated. Motivated by this, the present study first carries out a theoretical analysis in order to demonstrate the sources of algorithmic bias in GAT-based learning for node classification. Then, a novel algorithm, FairGAT, which leverages a fairness-aware attention design, is developed based on the theoretical findings. Experimental results on real-world networks demonstrate that FairGAT improves group fairness measures while also providing comparable utility to the fairness-aware baselines for node classification and link prediction.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention-based node representations; fair attention; fair node representation learning; fairness-aware machine learning; Learning over graphs,Graph algorithms; Graph structures; Graphic methods; Machine learning; Algorithmics; Attention-based node representation; Fair attention; Fair node representation learning; Fairness-aware machine learning; Gene networks; Graph neural networks; Learning over graph; Machine-learning; Power grids; Graph neural networks
Sa2e-ad: A stacked attention autoencoder for anomaly detection in multivariate time series,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196814181&doi=10.1145%2f3653677&partnerID=40&md5=1ef84a8bc3e5b65d22b188dffa952bfa,"Anomaly detection for multivariate time series is an essential task in the modern industrial field. Although several methods have been developed for anomaly detection, they usually fail to effectively exploit the metrical-temporal correlation and the other dependencies among multiple variables. To address this problem, we propose a stacked attention autoencoder for anomaly detection in multivariate time series (SA2E-AD); it focuses on fully utilizing the metrical and temporal relationships among multivariate time series. We design a multiattention block, alternately containing the temporal attention and metrical attention components in a hierarchical structure to better reconstruct normal time series, which is helpful in distinguishing the anomalies from the normal time series. Meanwhile, a two-stage training strategy is designed to further separate the anomalies from the normal data. Experiments on three publicly available datasets show that SA2E-AD outperforms the advanced baseline methods in detection performance and demonstrate the effectiveness of each part of the process in our method.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Anomaly detection; deep learning; dilated convolutional neural network; multivariate time series; unsupervised learning,Convolutional neural networks; Deep learning; Learning systems; Time series; Unsupervised learning; Anomaly detection; Auto encoders; Convolutional neural network; Deep learning; Dilated convolutional neural network; Industrial fields; It focus; Multivariate time series; Temporal correlations; Times series; Anomaly detection
FETILDA: Evaluation Framework for Effective Representations of Long Financial Documents,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196716111&doi=10.1145%2f3657299&partnerID=40&md5=b4f63877185c6400c09e1d6069e0fb21,"In the financial sphere, there is a wealth of accumulated unstructured financial data, such as the textual disclosure documents that companies submit on a regular basis to regulatory agencies, such as the Securities and Exchange Commission. These documents are typically very long and tend to contain valuable soft information about a company's performance that is not present in quantitative predictors. It is therefore of great interest to learn predictive models from these long textual documents, especially for forecasting numerical key performance indicators. In recent years, there has been great progress in natural language processing via pre-trained language models (LMs) learned from large corpora of textual data. This prompts the important question of whether they can be used effectively to produce representations for long documents, as well as how we can evaluate the effectiveness of representations produced by various LMs. Our work focuses on answering this critical question, namely, the evaluation of the efficacy of various LMs in extracting useful soft information from long textual documents for prediction tasks. In this article, we propose and implement a deep learning evaluation framework that utilizes a sequential chunking approach combined with an attention mechanism. We perform an extensive set of experiments on a collection of 10-K reports submitted annually by U.S. banks, and another dataset of reports submitted by U.S. companies, to investigate thoroughly the performance of different types of language models. Overall, our framework using LMs outperforms strong baseline methods for textual modeling as well as for numerical regression. Our work provides better insights into how utilizing pre-trained domain-specific and fine-tuned long-input LMs for representing long documents can improve the quality of representation of textual data and, therefore, help in improving predictive analyses. © 2024 Copyright held by the owner/author(s).",10-K reports; financial documents; language models; long text documents; Text regression,Benchmarking; Computational linguistics; Deep learning; Finance; Natural language processing systems; 10-K report; Evaluation framework; Financial document; Language model; Long text document; Soft information; Text document; Text regression; Textual data; Textual documents; Numerical methods
Distributed Pseudo-Likelihood Method for Community Detection in Large-Scale Networks,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196755289&doi=10.1145%2f3657300&partnerID=40&md5=c2c1c8208c435bb4f4b2589030f16f9e,"This paper proposes a distributed pseudo-likelihood method (DPL) to conveniently identify the community structure of large-scale networks. Specifically, we first propose a block-wise splitting method to divide large-scale network data into several subnetworks and distribute them among multiple workers. For simplicity, we assume the classical stochastic block model. Then, the DPL algorithm is iteratively implemented for the distributed optimization of the sum of the local pseudo-likelihood functions. At each iteration, the worker updates its local community labels and communicates with the master. The master then broadcasts the combined estimator to each worker for the new iterative steps. Based on the distributed system, DPL significantly reduces the computational complexity of the traditional pseudo-likelihood method using a single machine. Furthermore, to ensure statistical accuracy, we theoretically discuss the requirements of the worker sample size. Moreover, we extend the DPL method to estimate degree-corrected stochastic block models. The superior performance of the proposed distributed algorithm is demonstrated through extensive numerical studies and real data analysis. © 2024 Copyright held by the owner/author(s).",Community detection; computational efficiency; distributed algorithm; large-scale networks; pseudo-likelihood,Iterative methods; Population dynamics; Stochastic models; Stochastic systems; Community detection; Community structures; Large-scale network; Likelihood methods; Network data; Pseudo-likelihood; Splitting method; Stochastic block models; Subnetworks; Workers'; Computational efficiency
Toward few-label vertical federated learning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196826010&doi=10.1145%2f3656344&partnerID=40&md5=8460363d3e21017ef37e67cf5d6940a6,"Federated Learning (FL) provides a novel paradigm for privacy-preserving machine learning, enabling multiple clients to collaborate on model training without sharing private data. To handle multi-source heterogeneous data, Vertical Federated Learning (VFL) has been extensively investigated. However, in the context of VFL, the label information tends to be kept in one authoritative client and is very limited. This poses two challenges for model training in the VFL scenario. On the one hand, a small number of labels cannot guarantee to train a well VFL model with informative network parameters, resulting in unclear boundaries for classification decisions. On the other hand, the large amount of unlabeled data is dominant and should not be discounted, and it is worthwhile to focus on how to leverage them to improve representation modeling capabilities. To address the preceding two challenges, we first introduce supervised contrastive loss to enhance the intra-class aggregation and inter-class estrangement, which is to deeply explore label information and improve the effectiveness of downstream classification tasks. Then, for unlabeled data, we introduce a pseudo-label-guided consistency mechanism to induce the classification results coherent across clients, which allows the representations learned by local networks to absorb the knowledge from other clients, and alleviates the disagreement between different clients for classification tasks. We conduct sufficient experiments on four commonly used datasets, and the experimental results demonstrate that our method is superior to the state-of-the-art methods, especially in the low-label rate scenario, and the improvement becomes more significant.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive learning; semi-supervised learning; Vertical federated learning,Classification (of information); Learning systems; Classification tasks; Contrastive learning; Label information; Machine-learning; Model training; Multiple clients; Privacy preserving; Semi-supervised learning; Unlabeled data; Vertical federated learning; Privacy-preserving techniques
Tomgpt: Reliable text-only training approach for cost-effective multi-modal large language model,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196745641&doi=10.1145%2f3654674&partnerID=40&md5=a511becd5e4b3fa1df31fa5dae8e81eb,"Multi-modal large language models (MLLMs), such as GPT-4, exhibit great comprehension capabilities on human instruction, as well as zero-shot ability on new downstream multi-modal tasks. To integrate the different modalities within a unified embedding space, previous MLLMs attempted to conduct visual instruction tuning with massive and high-quality image-Text pair data, which requires substantial costs in data collection and training resources. In this article, we propose TOMGPT (Text-Only training Multi-modal GPT), a cost-effective MLLM tuned solely on easily accessible text data with much fewer resources. Along with pre-Trained visual-linguistic coupled modality space (e.g., CLIP and ALIGN model), a text-only training strategy is devised to further project the aligned multi-modal latent space to that of LLM, endowing the LLM with visual comprehension capabilities in an efficient manner. Instead of enormous image-Text training data required by previous MLLMs, we find that TOMGPT can be well-Tuned with fewer yet diverse GPT-generated free-form text data, as we establish the semantic connection between LLM and pre-Trained vision-language model. A quantitative evaluation is conducted on both MME and LVLM, which are recently released and extensively utilized MLLM benchmarks. The experiments reveal that TOMGPT achieved reliable performance compared to numerous models trained on a large amount of image-Text pair data. Case studies are also presented, demonstrating TOMGPT's broad understanding and dialogue capabilities across diverse image categories.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",large language model; Multi-modal; text-only training,Computational linguistics; Cost effectiveness; Zero-shot learning; Cost effective; Down-stream; Embeddings; High quality images; Image texts; Language model; Large language model; Multi-modal; Text data; Text-only training; Semantics
A Survey of Trustworthy Representation Learning Across Domains,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196755929&doi=10.1145%2f3657301&partnerID=40&md5=934d9bd609ef9a362cac41e8b7e78a66,"As AI systems have obtained significant performance to be deployed widely in our daily lives and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts of AI systems, and representation learning is the fundamental technology in machine learning. How to make representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four concepts, i.e., robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions. © 2024 Copyright held by the owner/author(s).",cross-domain learning; explainability; fairness; privacy; representation learning; robustness; Trustworthy,Learning systems; Robustness (control systems); AI systems; Cross-domain learning; Explainability; Fairness; Machine-learning; Performance; Privacy; Representation learning; Robustness; Trustworthy; Machine learning
BapFL: You can Backdoor Personalized Federated Learning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196865951&doi=10.1145%2f3649316&partnerID=40&md5=77f581cffcf5a160a3089fa88218de74,"In federated learning (FL), malicious clients could manipulate the predictions of the trained model through backdoor attacks, posing a significant threat to the security of FL systems. Existing research primarily focuses on backdoor attacks and defenses within the generic federated learning scenario, where all clients collaborate to train a single global model. A recent study conducted by Qin et al. [24] marks the initial exploration of backdoor attacks within the personalized federated learning (pFL) scenario, where each client constructs a personalized model based on its local data. Notably, the study demonstrates that pFL methods with parameter decoupling can significantly enhance robustness against backdoor attacks. However, in this article, we whistleblow that pFL methods with parameter decoupling are still vulnerable to backdoor attacks. The resistance of pFL methods with parameter decoupling is attributed to the heterogeneous classifiers between malicious clients and benign counterparts. We analyze two direct causes of the heterogeneous classifiers: (1) data heterogeneity inherently exists among clients and (2) poisoning by malicious clients further exacerbates the data heterogeneity. To address these issues, we propose a two-pronged attack method, BapFL, which comprises two simple yet effective strategies: (1) poisoning only the feature encoder while keeping the classifier fixed and (2) diversifying the classifier through noise introduction to simulate that of the benign clients. Extensive experiments on three benchmark datasets under varying conditions demonstrate the effectiveness of our proposed attack. Additionally, we evaluate the effectiveness of six widely used defense methods and find that BapFL still poses a significant threat even in the presence of the best defense, Multi-Krum. We hope to inspire further research on attack and defense strategies in pFL scenarios. The code is available at: https://github.com/BapFL/code  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",backdoor attack; model security; Personalized federated learning,Classification (of information); Network security; Backdoor attack; Backdoors; Data heterogeneity; Decouplings; Federated learning system; Heterogeneous classifiers; Learning methods; Learning scenarios; Model security; Personalized federated learning; Learning systems
Fair feature selection: A causal perspective,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196808295&doi=10.1145%2f3643890&partnerID=40&md5=53e9de699e89d222ba2fddb52559883e,"Fair feature selection for classification decision tasks has recently garnered significant attention from researchers. However, existing fair feature selection algorithms fall short of providing a full explanation of the causal relationship between features and sensitive attributes, potentially impacting the accuracy of fair feature identification. To address this issue, we propose a fair causal feature selection algorithm, called FairCFS. Specifically, FairCFS constructs a localized causal graph that identifies the Markov blankets of class and sensitive variables, to block the transmission of sensitive information for selecting fair causal features. Extensive experiments on seven public real-world datasets validate that FairCFS has accuracy comparable to eight state-of-the-art feature selection algorithms while presenting more superior fairness.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Causal fairness; fair feature selection; markov blanket,Causal fairness; Causal relationships; Classification decision; Decision task; Fair feature selection; Feature attributes; Feature selection algorithm; Features selection; Markov Blankets; Sensitive attribute; Feature Selection
FulBM: Fast Fully Batch Maintenance for Landmark-based 3-hop Cover Labeling,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192355195&doi=10.1145%2f3650035&partnerID=40&md5=ac4817008b1f633d56b4483cab4a01f1,"Landmark-based 3-hop cover labeling is a category of approaches for shortest distance/path queries on large-scale complex networks. It pre-computes an index offline to accelerate the online distance/path query. Most real-world graphs undergo rapid changes in topology, which makes index maintenance on dynamic graphs necessary. So far, the majority of index maintenance methods can handle only one edge update (either an addition or deletion) each time. To keep up with frequently changing graphs, we research the fully batch maintenance problem for the 3-hop cover labeling, and proposed the method called FulBM. FulBM is composed of two algorithms: InsBM and DelBM, which are designed to handle batch edge insertions and deletions, respectively. This separation is motivated by the insight that batch maintenance for edge insertions are much more time-efficient and the fact that most edge updates in the real world are incremental. Both InsBM and DelBM are equipped with well-designed pruning strategies to minimize the number of vertex accesses. We have conducted comprehensive experiments on both synthetic and real-world graphs to verify the efficiency of FulBM and its variants for weighted graphs. The results show that our methods achieve 5.5× to 228× speedup compared with the state-of-the-art method. © 2024 Copyright held by the owner/author(s).",3-hop cover labeling; dynamic graph; index maintenance,Complex networks; Graphic methods; Maintenance; 3-hop cover labeling; Batch maintenance; Dynamic graph; Index maintenance; Labelings; Large-scales; Offline; Path queries; Real-world graphs; Shortest distance paths; Topology
DeepMeshCity: A Deep Learning Model for Urban Grid Prediction,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192385211&doi=10.1145%2f3652859&partnerID=40&md5=4a43bf5ac9a71282fb5180477c68635d,"Urban grid prediction can be applied to many classic spatial-temporal prediction tasks such as air quality prediction, crowd density prediction, and traffic flow prediction, which is of great importance to smart city building. In light of its practical values, many methods have been developed for it and have achieved promising results. Despite their successes, two main challenges remain open: (a) how to well capture the global dependencies and (b) how to effectively model the multi-scale spatial-temporal correlations? To address these two challenges, we propose a novel method—DeepMeshCity, with a carefully-designed Self-Attention Citywide Grid Learner (SA-CGL) block comprising of a self-attention unit and a Citywide Grid Learner (CGL) unit. The self-attention block aims to capture the global spatial dependencies, and the CGL unit is responsible for learning the spatial-temporal correlations. In particular, a multi-scale memory unit is proposed to traverse all stacked SA-CGL blocks along a zigzag path to capture the multi-scale spatial-temporal correlations. In addition, we propose to initialize the single-scale memory units and the multi-scale memory units by using the corresponding ones in the previous fragment stack, so as to speed up the model training. We evaluate the performance of our proposed model by comparing with several state-of-the-art methods on four real-world datasets for two urban grid prediction applications. The experimental results verify the superiority of DeepMeshCity over the existing ones. The code is available at https://github.com/ILoveStudying/DeepMeshCity. © 2024 Copyright held by the owner/author(s).",crowd/traffic flow prediction; Spatial-temporal prediction; urban computing,Air quality; Deep learning; Learning systems; Crowd/traffic flow prediction; Learning models; Memory units; Multi-scales; Spatial temporals; Spatial-temporal correlation; Spatial-temporal prediction; Temporal prediction; Traffic flow prediction; Urban computing; Forecasting
Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192372850&doi=10.1145%2f3648684&partnerID=40&md5=287ac9f512112e68c4d3dc7a2b954b03,"The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner’s objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it’s crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this article, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term fairness constraints into a strongly adapted loss regret framework. Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive fairness-aware online meta-learning algorithm, referred to as FairSAOML. This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy. The problem is framed as a bi-level convex-concave optimization, considering both the model’s primal and dual parameters, which pertain to its accuracy and fairness attributes, respectively. Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of fairness constraints. Our experimental evaluation of various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods. © 2024 Copyright held by the owner/author(s).",adaption; changing environments; Fairness; online meta-learning,E-learning; Learning systems; Population statistics; Adaption; Changing environment; Dynamic environments; Fairness; Fairness constraints; Learning frameworks; Life long learning; Metalearning; Online learning; Online meta-learning; Learning algorithms
NOODLE: Joint Cross-View Discrepancy Discovery and High-Order Correlation Detection for Multi-View Subspace Clustering,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192375288&doi=10.1145%2f3653305&partnerID=40&md5=fc7416a2195b31c2524974d75b78faab,"Benefiting from the effective exploration of the valuable topological pair-wise relationship of data points across multiple views, multi-view subspace clustering (MVSC) has received increasing attention in recent years. However, we observe that existing MVSC approaches still suffer from two limitations that need to be further improved to enhance the clustering effectiveness. Firstly, previous MVSC approaches mainly prioritize extracting multi-view consistency, often neglecting the cross-view discrepancy that may arise from noise, outliers, and view-inherent properties. Secondly, existing techniques are constrained by their reliance on pair-wise sample correlation and pair-wise view correlation, failing to capture the high-order correlations that are enclosed within multiple views. To address these issues, we propose a novel MVSC framework via joiNt crOss-view discrepancy discOvery anD high-order correLation dEtection (NOODLE), seeking an informative target subspace representation compatible across multiple features to facilitate the downstream clustering task. Specifically, we first exploit the self-representation mechanism to learn multiple view-specific affinity matrices, which are further decomposed into cohesive factors and incongruous factors to fit the multiview consistency and discrepancy, respectively. Additionally, an explicit cross-view sparse regularization is applied to incoherent parts, ensuring the consistency and discrepancy to be precisely separated from the initial subspace representations. Meanwhile, the multiple cohesive parts are stacked into a three-dimensional tensor associated with a tensor-Singular Value Decomposition (t-SVD) based weighted tensor nuclear norm constraint, enabling effective detection of the high-order correlations implicit in multi-view data. Our proposed method outperforms state-of-the-art methods for multi-view clustering on six benchmark datasets, demonstrating its effectiveness. © 2024 Copyright held by the owner/author(s).",cross-view discrepancy; high-order correlation; low-rank tensor representation; Multi-view clustering,Clustering algorithms; Singular value decomposition; Correlation detection; Cross-view discrepancy; High order correlation; Higher order correlation; Low-rank tensor representation; Multi-view clustering; Multi-views; Multiple views; Subspace clustering; Tensor representation; Tensors
Representative and Back-In-Time Sampling from Real-world Hypergraphs,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192369645&doi=10.1145%2f3653306&partnerID=40&md5=7697bdcad6dc1e08ea6f893a57489dac,"Graphs are widely used for representing pairwise interactions in complex systems. Since such real-world graphs are large and often evergrowing, sampling subgraphs is useful for various purposes, including simulation, visualization, stream processing, representation learning, and crawling. However, many complex systems consist of group interactions (e.g., collaborations of researchers and discussions on online Q&A platforms) and thus are represented more naturally and accurately by hypergraphs than by ordinary graphs. Motivated by the prevalence of large-scale hypergraphs, we study the problem of sampling from real-world hypergraphs, aiming at answering (Q1) how can we measure the goodness of sub-hypergraphs, and (Q2) how can we efficiently find a “good” sub-hypergraph. Regarding Q1, we distinguish between two goals: (a) representative sampling, which aims at capturing the characteristics of the input hypergraph, and (b) back-in-time sampling, which aims at closely approximating a past snapshot of the input time-evolving hypergraph. To evaluate the similarity of the sampled sub-hypergraph to the target (i.e., the input hypergraph or its past snapshot), we consider 10 graph-level, hyperedge-level, and node-level statistics. Regarding Q2, we first conduct a thorough analysis of various intuitive approaches using 11 real-world hypergraphs. Then, based on this analysis, we propose MiDaS and MiDaS-B, designed for representative sampling and back-in-time sampling, respectively. Regarding representative sampling, we demonstrate through extensive experiments that MiDaS, which employs a sampling bias toward high-degree nodes in hyperedge selection, is (a) Representative: finding overall the most representative samples among 15 considered approaches, (b) Fast: several orders of magnitude faster than the strongest competitors, and (c) Automatic: automatically tuning the degree of sampling bias. Regarding back-in-time sampling, we demonstrate that MiDaS-B inherits the strengths of MiDaS despite an additional challenge—the unavailability of the target (i.e., past snapshot). It effectively handles this challenge by focusing on replicating universal evolutionary patterns, rather than directly replicating the target. © 2024 Copyright held by the owner/author(s).",higher-order network; Hypergraph; sampling; structural property,Complex networks; Graph theory; Large scale systems; Higher order networks; Hyper graph; Hyperedges; Pairwise interaction; Real-world; Real-world graphs; Representative sampling; Sampling bias; Stream processing; Subgraphs; Online systems
Dual Homogeneity Hypergraph Motifs with Cross-view Contrastive Learning for Multiple Social Recommendations,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192357808&doi=10.1145%2f3653976&partnerID=40&md5=4d2a5b3b1c7e17e0c6c205bee2dcf4b0,"Social relations are often used as auxiliary information to address data sparsity and cold-start issues in social recommendations. In the real world, social relations among users are complex and diverse. Widely used graph neural networks (GNNs) can only model pairwise node relationships and are not conducive to exploring higher-order connectivity, while hypergraph provides a natural way to model high-order relations between nodes. However, recent studies show that social recommendations still face the following challenges: 1) a majority of social recommendations ignore the impact of multifaceted social relationships on user preferences; 2) the item homogeneity is often neglected, mainly referring to items with similar static attributes have similar attractiveness when exposed to users that indicating hidden links between items; and 3) directly combining the representations learned from different independent views cannot fully exploit the potential connections between different views. To address these challenges, in this article, we propose a novel method DH-HGCN++ for multiple social recommendations. Specifically, dual homogeneity (i.e., social homogeneity and item homogeneity) is introduced to mine the impact of diverse social relations on user preferences and enrich item representations. Hypergraph convolution networks with motifs are further exploited to model the high-order relations between nodes. Finally, cross-view contrastive learning is proposed as an auxiliary task to jointly optimize the DH-HGCN++. Real-world datasets are used to validate the effectiveness of the proposed model, where we use sentiment analysis to extract comment relations and employ the k-means clustering algorithm to construct the item-item correlation graph. Experiment results demonstrate that our proposed method consistently outperforms the state-of-the-art baselines on Top-N recommendations. © 2024 Copyright held by the owner/author(s).",contrastive learning; homogeneity; hypergraph convolution network; Multiple social recommendation,Convolutional neural networks; Data mining; Graph neural networks; Graph theory; K-means clustering; Sentiment analysis; Contrastive learning; High-order; Higher-order; Homogeneity; Hyper graph; Hypergraph convolution network; Multiple social recommendation; Ordering relations; Social relations; User's preferences; Convolution
Hierarchical Convolutional Neural Network with Knowledge Complementation for Long-Tailed Classification,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192367460&doi=10.1145%2f3653717&partnerID=40&md5=8f854a866c7eaa0b7cc4b0a839dbb8c2,"Existing methods based on transfer learning leverage auxiliary information to help tail generalization and improve the performance of the tail classes. However, they cannot fully exploit the relationships between auxiliary information and tail classes and bring irrelevant knowledge to the tail classes. To solve this problem, we propose a hierarchical CNN with knowledge complementation, which regards hierarchical relationships as auxiliary information and transfers relevant knowledge to tail classes. First, we integrate semantics and clustering relationships as hierarchical knowledge into the CNN to guide feature learning. Then, we design a complementary strategy to jointly exploit the two types of knowledge, where semantic knowledge acts as a prior dependence and clustering knowledge reduces the negative information caused by excessive semantic dependence (i.e., semantic gaps). In this way, the CNN facilitates the utilization of the two complementary hierarchical relationships and transfers useful knowledge to tail data to improve long-tailed classification accuracy. Experimental results on public benchmarks show that the proposed model outperforms existing methods. In particular, our model improves accuracy by 3.46% compared with the second-best method on the long-tailed tieredImageNet dataset. © 2024 Copyright held by the owner/author(s).",deep learning; hierarchical relationship; knowledge transfer; Long-tailed classification,Convolutional neural networks; Deep learning; Knowledge management; Transfer learning; Auxiliary information; Clusterings; Complementation; Convolutional neural network; Deep learning; Generalisation; Hierarchical relationship; Knowledge transfer; Long-tailed classification; Transfer learning; Semantics
Semi-Supervised Multi-View Clustering based on NMF with Fusion Regularization,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192360599&doi=10.1145%2f3653022&partnerID=40&md5=df8552d396409bccd27b0fb27cd15b24,"Multi-view clustering has attracted significant attention and application. Nonnegative matrix factorization is one popular feature of learning technology in pattern recognition. In recent years, many semi-supervised nonnegative matrix factorization algorithms were proposed by considering label information, which has achieved outstanding performance for multi-view clustering. However, most of these existing methods have either failed to consider discriminative information effectively or included too much hyper-parameters. Addressing these issues, a semi-supervised multi-view nonnegative matrix factorization with a novel fusion regularization (FRSMNMF) is developed in this article. In this work, we uniformly constrain alignment of multiple views and discriminative information among clusters with designed fusion regularization. Meanwhile, to align the multiple views effectively, two kinds of compensating matrices are used to normalize the feature scales of different views. Additionally, we preserve the geometry structure information of labeled and unlabeled samples by introducing the graph regularization simultaneously. Due to the proposed methods, two effective optimization strategies based on multiplicative update rules are designed. Experiments implemented on six real-world datasets have demonstrated the effectiveness of our FRSMNMF comparing with several state-ofthe-art unsupervised and semi-supervised approaches. © 2024 Copyright held by the owner/author(s).",fusion regularization; Multi-view; scale normalization; semi-supervised clustering,Clustering algorithms; Matrix factorization; Pattern recognition; Fusion regularization; Learning technology; Multi-view clustering; Multi-views; Multiple views; Nonnegative matrix factorization; Regularisation; Scale normalization; Semi-supervised; Semi-supervised Clustering; Matrix algebra
Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192456361&doi=10.1145%2f3649506&partnerID=40&md5=ae40a404dc79ce4408fa4bf9a22c3c3a,"This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pretraining data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at https://github.com/Mooler0410/LLMsPracticalGuide. An LLMs evolutionary tree, editable yet regularly updated, can be found at llmtree.ai. © 2024 Copyright held by the owner/author(s).",ChatGPT; Large language models; neural language processing; practical guide,Computational linguistics; ChatGPT; Down-stream; End-users; Language model; Language processing; Large language model; Natural languages; Neural language processing; Power; Practical guide; Natural language processing systems
A Dual Perspective Framework of Knowledge-correlation for Cross-domain Recommendation,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192390797&doi=10.1145%2f3652520&partnerID=40&md5=5cf2a9e9cbf87f56e6397b4cdd5d2815,"Recommender System provides users with online services in a personalized way. The performance of traditional recommender systems may deteriorate because of problems such as cold-start and data sparsity. Cross-domain Recommendation System utilizes the richer information from auxiliary domains to guide the task in the target domain. However, direct knowledge transfer may lead to a negative impact due to data heterogeneity and feature mismatch between domains. In this article, we innovatively explore the cross-domain correlation from the perspectives of content semanticity and structural connectivity to fully exploit the information of Knowledge Graph. First, we adopt domain adaptation that automatically extracts transferable features to capture cross-domain semantic relations. Second, we devise a knowledge-aware graph neural network to explicitly model the high-order connectivity across domains. Third, we develop feature fusion strategies to combine the advantages of semantic and structural information. By simulating the cold-start scenario on two real-world datasets, the experimental results show that our proposed method has superior performance in accuracy and diversity compared with the SOTA methods. It demonstrates that our method can accurately predict users’ expressed preferences while exploring their potential diverse interests. © 2024 Copyright held by the owner/author(s).",cold-start; Cross-domain recommendation; domain adaptation; graph neural network; knowledge graph,Graph neural networks; Knowledge graph; Knowledge management; Semantics; Cold-start; Cross-domain; Cross-domain recommendations; Data sparsity; Domain adaptation; Graph neural networks; Knowledge graphs; On-line service; Performance; Target domain; Recommender systems
Multi-source and Multi-modal Deep Network Embedding for Cross-network Node Classification,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192362516&doi=10.1145%2f3653304&partnerID=40&md5=c0304dc03b577b744a4b33af2e4f1b58,"In recent years, to address the issue of networked data sparsity in node classification tasks, cross-network node classification (CNNC) leverages the richer information from a source network to enhance the performance of node classification in the target network, which typically has sparser information. However, in real-world applications, labeled nodes may be collected from multiple sources with multiple modalities (e.g., text, vision, and video). Naive application of single-source and single-modal CNNC methods may result in sub-optimal solutions. To this end, in this article, we propose a model called Multi-source and Multi-modal Cross-network Deep Network Embedding (M2CDNE) for cross-network node classification. In M2CDNE, we propose a deep multi-modal network embedding approach that combines the extracted deep multi-modal features to make the node vector representations network invariant. In addition, we apply dynamic adversarial adaptation to assess the significance of marginal and conditional probability distributions between each source and target network to make node vector representations label discriminative. Furthermore, we devise to classify nodes in the target network through the related source classifier and aggregate different predictions utilizing respective network weights, corresponding to the discrepancy between each source and target network. Extensive experiments performed on real-world datasets demonstrate that the proposed M2CDNE significantly outperforms the state-of-the-art approaches. © 2024 Copyright held by the owner/author(s).",multi-modal fusion; multi-source transfer learning; Network embedding; node classification,Classification (of information); Probability distributions; Cross networks; Multi-modal; Multi-modal fusion; Multi-source transfer learning; Multi-Sources; Network embedding; Network node; Node classification; Source Transfer; Transfer learning; Network embeddings
MoMENt: Marked Point Processes with Memory-Enhanced Neural Networks for User Activity Modeling,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192363158&doi=10.1145%2f3649504&partnerID=40&md5=441c7c812ff3debf18fd537c6b3252c7,"Marked temporal point process models (MTPPs) aim to model event sequences and event markers (associated features) in continuous time. These models have been applied to various application domains where capturing event dynamics in continuous time is beneficial, such as education systems, social networks, and recommender systems. However, current MTPPs suffer from two major limitations, i.e., inefficient representation of event dynamic’s influence on marker distribution and losing fine-grained representation of historical marker distributions in the modeling. Motivated by these limitations, we propose a novel model called Marked Point Processes with Memory-Enhanced Neural Networks (MoMENt) that can capture the bidirectional interrelations between markers and event dynamics while providing fine-grained marker representations. Specifically, MoMENt is constructed of two concurrent networks: Recurrent Activity Updater (RAU) to capture model event dynamics and Memory-Enhanced Marker Updater (MEMU) to represent markers. Both RAU and MEMU components are designed to update each other at every step to model the bidirectional influence of markers and event dynamics. To obtain a fine-grained representation of maker distributions, MEMU is devised with external memories that model detailed marker-level features with latent component vectors. Our extensive experiments on six real-world user interaction datasets demonstrate that MoMENt can accurately represent users’ activity dynamics, boosting time, type, and marker predictions, as well as recommendation performance up to 76.5%, 65.6%, 77.2%, and 57.7%, respectively, compared to baseline approaches. Furthermore, our case studies show the effectiveness of MoMENt in providing meaningful and fine-grained interpretations of user-system relations over time, e.g., how user choices influence their future preferences in the recommendation domain. © 2024 Copyright held by the owner/author(s).",Hawkes process; Point process; sequential model; user activity modeling,Dynamics; Recurrent neural networks; User profile; Activity modeling; Event dynamics; Fine grained; Hawkes process; Marked point process; Point process; Sequential modeling; Updater; User activity; User activity modeling; Continuous time systems
Learning to Generate Temporal Origin-destination Flow Based on Urban Regional Features and Traffic Information,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192374606&doi=10.1145%2f3649141&partnerID=40&md5=75e88e9d4fdffc018e91443639bd34f3,"Origin-destination (OD) flow contains population mobility information between every two regions in the city, which is of great value in urban planning and transportation management. Nevertheless, the collection of OD flow data is extremely difficult due to the hindrance of privacy issues and collection costs. Significant efforts have been made to generate OD flow based on urban regional features, e.g., demographics, land use, and so on, since spatial heterogeneity of urban function is the primary cause that drives people to move from one place to another. On the other hand, people travel through various routes between OD, which will have effects on urban traffic, e.g., road travel speed and time. These effects of OD flows reveal the fine-grained spatiotemporal patterns of population mobility. Few works have explored the effectiveness of incorporating urban traffic information into OD generation. To bridge this gap, we propose to generate real-world daily temporal OD flows enhanced by urban traffic information in this paper. Our model consists of two modules: Urban2OD and OD2Traffic. In the Urban2OD module, we devise a spatiotemporal graph neural network to model the complex dependencies between daily temporal OD flows and regional features. In the OD2Traffic module, we introduce an attention-based neural network to predict urban traffic based on OD flow from the Urban2OD module. Then, by utilizing gradient backpropagation, these two modules are able to enhance each other to generate high-quality OD flow data. Extensive experiments conducted on real-world datasets demonstrate the superiority of our proposed model over the state of the art. © 2024 Copyright held by the owner/author(s).",origin-destination; spatiotemporal graph learning; traffic flow; Urban mobility,Backpropagation; Digital storage; Flow graphs; Geographic information systems; Graph neural networks; Motor transportation; Urban transportation; Flow based; Origin destination; Origin-destination flows; Regional feature; Spatio-temporal graphs; Spatiotemporal graph learning; Traffic flow; Traffic information; Urban mobility; Urban traffic; Land use
SsAG: Summarization and Sparsification of Attributed Graphs,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192349641&doi=10.1145%2f3651619&partnerID=40&md5=1d13870ddda2414909201bd2d9a75657,"Graph summarization has become integral for managing and analyzing large-scale graphs in diverse real-world applications, including social networks, biological networks, and communication networks. Existing methods for graph summarization often face challenges, being either computationally expensive, limiting their applicability to large graphs, or lacking the incorporation of node attributes. In response, we introduce SsAG, an efficient and scalable lossy graph summarization method designed to preserve the essential structure of the original graph. SsAG computes a sparse representation (summary) of the input graph, accommodating graphs with node attributes. The summary is structured as a graph on supernodes (subsets of vertices of G), where weighted superedges connect pairs of supernodes. The methodology focuses on constructing a summary graph with k supernodes, aiming to minimize the reconstruction error (the difference between the original graph and the graph reconstructed from the summary) while maximizing homogeneity with respect to the node attributes. The construction process involves iteratively merging pairs of nodes. To enhance computational efficiency, we derive a closed-form expression for efficiently computing the reconstruction error (RE) after merging a pair, enabling constant-time approximation of this score. We assign a weight to each supernode, quantifying their contribution to the score of pairs, and utilize a weighted sampling strategy to select the best pair for merging. Notably, a logarithmic-sized sample achieves a summary comparable in quality based on various measures. Additionally, we propose a sparsification step for the constructed summary, aiming to reduce storage costs to a specified target size with a marginal increase in RE. Empirical evaluations across diverse real-world graphs demonstrate that SsAG exhibits superior speed, being up to 17× faster, while generating summaries of comparable quality. This work represents a significant advancement in the field, addressing computational challenges and showcasing the effectiveness of SsAG in graph summarization. © 2024 Copyright held by the owner/author(s).",attributed graphs; graph sparsification; Graph summarization,Computational efficiency; Graph theory; Graphic methods; Iterative methods; Attributed graphs; Biological networks; Graph sparsification; Graph summarization; Large-scales; Node attribute; Real-world; Reconstruction error; Sparsification; Supernode; Merging
Fairness-Aware Graph Neural Networks: A Survey,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192382350&doi=10.1145%2f3649142&partnerID=40&md5=013ffad22f839b52023a6aa35ff426c1,"Graph Neural Networks (GNNs) have become increasingly important due to their representational power and state-of-the-art predictive performance on many fundamental learning tasks. Despite this success, GNNs suffer from fairness issues that arise as a result of the underlying graph data and the fundamental aggregation mechanism that lies at the heart of the large class of GNN models. In this article, we examine and categorize fairness techniques for improving the fairness of GNNs. We categorize these techniques by whether they focus on improving fairness in the pre-processing, in-processing (during training), or post-processing phases. We discuss how such techniques can be used together whenever appropriate and highlight the advantages and intuition as well. We also introduce an intuitive taxonomy for fairness evaluation metrics, including graph-level fairness, neighborhood-level fairness, embedding-level fairness, and prediction-level fairness metrics. In addition, graph datasets that are useful for benchmarking the fairness of GNN models are summarized succinctly. Finally, we highlight key open problems and challenges that remain to be addressed. © 2024 Copyright held by the owner/author(s).",Bias; Fairness; Graph Neural Networks,Bias; Fairness; Graph data; Graph neural networks; Learning tasks; Neural network model; Power; Predictive performance; State of the art; Underlying graphs; Graph neural networks
Adaptive Content-Aware Influence Maximization via Online Learning to Rank,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192380050&doi=10.1145%2f3651987&partnerID=40&md5=a15ba4fec9491ca6cb496612daa6f13a,"How can we adapt the composition of a post over a series of rounds to make it more appealing in a social network? Techniques that progressively learn how to make a fixed post more influential over rounds have been studied in the context of the Influence Maximization (IM) problem, which seeks a set of seed users that maximize a post’s influence. However, there is no work on progressively learning how a post’s features affect its influence. In this article, we propose and study the problem of Adaptive Content-Aware Influence Maximization (ACAIM), which calls to find k features to form a post in each round so as to maximize the cumulative influence of those posts over all rounds. We solve ACAIM by applying, for the first time, an Online Learning to Rank (OLR) framework for IM purposes. We introduce the CATRID propagation model, which expresses how posts disseminate in a social network using click probabilities and post visibility criteria and develop a simulator that runs CATRID via a training-testing scheme based on real posts of the VK social network, so as to realistically represent the learning environment. We deploy three learners that solve ACAIM in an online (real-time) manner. We experimentally prove the practical suitability of our solutions via exhaustive experiments on multiple brands (operating as different case studies) and several VK datasets; the best learner is evaluated on 45 separate case studies yielding convincing results. © 2024 Copyright held by the owner/author(s).",content recommendation; Influence maximization; online learning; simulation; social networks,E-learning; Learning systems; Social networking (online); Adaptive content; Content recommendations; Content-aware; Cumulative influence; Influence maximizations; Learn+; Maximization problem; Online learning; Simulation; Social network; Computer aided instruction
Multi-Scenario and Multi-Task Aware Feature Interaction for Recommendation System,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192382519&doi=10.1145%2f3651312&partnerID=40&md5=228044c83a7bb970857011f4d4b7c782,"Multi-scenario and multi-task recommendation can use various feedback behaviors of users in different scenarios to learn users’ preferences and then make recommendations, which has attracted attention. However, the existing work ignores feature interactions and the fact that a pair of feature interactions will have differing levels of importance under different scenario-task pairs, leading to sub-optimal user preference learning. In this article, we propose a Multi-scenario and Multi-task aware Feature Interaction model, dubbed MMFI, to explicitly model feature interactions and learn the importance of feature interaction pairs in different scenarios and tasks. Specifically, MMFI first incorporates a pairwise feature interaction unit and a scenario-task interaction unit to effectively capture the interaction of feature pairs and scenario-task pairs. Then MMFI designs a scenario-task aware attention layer for learning the importance of feature interactions from coarse-grained to fine-grained, improving the model’s performance on various scenario-task pairs. More specifically, this attention layer consists of three modules: a fully shared bottom module, a partially shared middle module, and a specific output module. Finally, MMFI adapts two sparsity-aware functions to remove some useless feature interactions. Extensive experiments on two public datasets demonstrate the superiority of the proposed method over the existing multi-task recommendation, multi-scenario recommendation, and multi-scenario & multi-task recommendation models. © 2024 Copyright held by the owner/author(s).",interaction; Recommendation,Data mining; Feature interactions; Feedback behaviours; Interaction; Interaction units; Learn+; Multi scenarios; Multi tasks; Recommendation; Task-aware; User's preferences; Recommender systems
On Breaking Truss-based and Core-based Communities,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192362305&doi=10.1145%2f3644077&partnerID=40&md5=d40ee5cf0476b34f9053e0daa042472c,"We introduce the general problem of identifying a smallest edge subset of a given graph whose deletion makes the graph community-free. We consider this problem under two community notions that have attracted significant attention: k-truss and k-core. We also introduce a problem variant where the identified subset contains edges incident to a given set of nodes and ensures that these nodes are not contained in any community: k-truss or k-core, in our case. These problems are directly applicable in social networks: The identified edges can be hidden by users or sanitized from the output graph; or in communication networks: the identified edges correspond to vital network connections. We present a series of theoretical and practical results. On the theoretical side, we show through non-trivial reductions that the problems we introduce are NP-hard and, in fact, hard to approximate. For the k-truss-based problems, we also show exact exponential-time algorithms, as well as a non-trivial lower bound on the size of an optimal solution. On the practical side, we develop a series of heuristics that are sped up by efficient data structures that we propose for updating the truss or core decomposition under edge deletions. In addition, we develop an algorithm to compute the lower bound. Extensive experiments on 11 real-world and synthetic graphs show that our heuristics are effective, outperforming natural baselines, and also efficient (up to two orders of magnitude faster than a natural baseline), thanks to our data structures. Furthermore, we present a case study on a co-authorship network and experiments showing that the removal of edges identified by our heuristics does not substantially affect the clustering structure of the input graph. © 2024 Copyright held by the owner/author(s).",community detection; Graph algorithms; k-core; k-truss,Trusses; Breakings; Communications networks; Community detection; Graph algorithms; K-cores; K-truss; Low bound; Network connection; Non-trivial; Truss core; Data structures
Do We Really Need Imputation in AutoML Predictive Modeling?,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192390411&doi=10.1145%2f3643643&partnerID=40&md5=cd3c0ffd09bad89c6adb4baf8bc58f45,"Numerous real-world data contain missing values, while in contrast, most Machine Learning (ML) algorithms assume complete datasets. For this reason, several imputation algorithms have been proposed to predict and fill in the missing values. Given the advances in predictive modeling algorithms tuned in an Automated Machine Learning context (AutoML) setting, a question that naturally arises is to what extent sophisticated imputation algorithms (e.g., Neural Network based) are really needed, or we can obtain a descent performance using simple methods like Mean/Mode (MM). In this article, we experimentally compare six state-of-the-art representatives of different imputation algorithmic families from an AutoML predictive modeling perspective, including a feature selection step and combined algorithm and hyper-parameter selection. We used a commercial AutoML tool for our experiments, in which we included the selected imputation methods. Experiments ran on 25 binary classification real-world incomplete datasets with missing values and 10 binary classification complete datasets in which synthetic missing values are introduced according to different missingness mechanisms, at varying missing frequencies. The main conclusion drawn from our experiments is that the best method on average is the Denoise AutoEncoder on real-world datasets and the MissForest in simulated datasets, followed closely by MM. In addition, binary indicator variables encoding missingness patterns actually improve predictive performance, on average. Last, although there are cases where Neural-Network-based imputation significantly improves predictive performance, this comes at a great computational cost and requires measuring all feature values to impute new samples. © 2024 Copyright held by the owner/author(s).",automl; imputation; machine learning; Missing values; optimization,Classification (of information); Learning algorithms; Automated machines; Automl; Imputation; Imputation algorithm; Learning context; Machine-learning; Missing values; Optimisations; Predictive models; Real-world; Machine learning
Node Embedding Preserving Graph Summarization,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192379138&doi=10.1145%2f3649505&partnerID=40&md5=e02060de901b78599dc4063e9a1a75a5,"Graph summarization is a useful tool for analyzing large-scale graphs. Some works tried to preserve original node embeddings encoding rich structural information of nodes on the summary graph. However, their algorithms are designed heuristically and not theoretically guaranteed. In this article, we theoretically study the problem of preserving node embeddings on summary graph. We prove that three matrix-factorization-based node embedding methods of the original graph can be approximated by that of the summary graph, and we propose a novel graph summarization method, named HCSumm, based on this analysis. Extensive experiments are performed on real-world datasets to evaluate the effectiveness of our proposed method. The experimental results show that our method outperforms the state-of-the-art methods in preserving node embeddings. © 2024 Copyright held by the owner/author(s).",Graph summarization; hierarchical clustering; node embedding,Graph embeddings; Graph theory; Embedding method; Embeddings; Encodings; Graph summarization; Hier-archical clustering; Hierarchical Clustering; Large-scales; Matrix factorizations; Node embedding; Structural information; Factorization
Citation Forecasting with Multi-Context Attention-Aided Dependency Modeling,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192378048&doi=10.1145%2f3649140&partnerID=40&md5=54b7ee84e452afccddfe18a5be5cb8b4,"Forecasting citations of scientific patents and publications is a crucial task for understanding the evolution and development of technological domains and for foresight into emerging technologies. By construing citations as a time series, the task can be cast into the domain of temporal point processes. Most existing work on forecasting with temporal point processes, both conventional and neural network-based, only performs single-step forecasting. In citation forecasting, however, the more salient goal is n-step forecasting: predicting the arrival of the next n citations. In this article, we propose Dynamic Multi-Context Attention Networks (DMA-Nets), a novel deep learning sequence-to-sequence (Seq2Seq) model with a novel hierarchical dynamic attention mechanism for long-term citation forecasting. Extensive experiments on two real-world datasets demonstrate that the proposed model learns better representations of conditional dependencies over historical sequences compared to state-of-the-art counterparts and thus achieves significant performance for citation predictions. © 2024 Copyright held by the owner/author(s).",Citation analysis; deep learning; recurrent neural networks,Learning systems; Recurrent neural networks; Citation analysis; Deep learning; Dependency model; Emerging technologies; Learning sequences; Network-based; Neural-networks; Point process; Single-step; Times series; Forecasting
Building Shortcuts between Distant Nodes with Biaffine Mapping for Graph Convolutional Networks,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192396033&doi=10.1145%2f3650113&partnerID=40&md5=593469a0b3f87f8028530ff22fddfc54,"Multiple recent studies show a paradox in graph convolutional networks (GCNs)—that is, shallow architectures limit the capability of learning information from high-order neighbors, whereas deep architectures suffer from over-smoothing or over-squashing. To enjoy the simplicity of shallow architectures and overcome their limits of neighborhood extension, in this work we introduce a biaffine technique to improve the expressiveness of GCNs with a shallow architecture. The core design of our method is to learn direct dependency on long-distance neighbors for nodes, with which only 1-hop message passing is capable of capturing rich information for node representation. Besides, we propose a multi-view contrastive learning method to exploit the representations learned from long-distance dependencies. Extensive experiments on nine graph benchmark datasets suggest that the shallow biaffine graph convolutional networks (BAGCN) significantly outperform state-of-the-art GCNs (with deep or shallow architectures) on semi-supervised node classification. We further verify the effectiveness of biaffine design in node representation learning and the performance consistency on different sizes of training data. © 2024 Copyright held by the owner/author(s).",biaffine mapping; Graph convolutional networks; long-distance dependency,Classification (of information); Convolution; Data mining; Graph neural networks; Graph theory; Learning systems; Message passing; Network architecture; Biaffine mapping; Convolutional networks; Core design; Deep architectures; Graph convolutional network; High-order; Higher-order; Learn+; Long-distance dependencies; Neighbourhood; Mapping
nSimplex Zen: A Novel Dimensionality Reduction for Euclidean and Hilbert Spaces,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192365227&doi=10.1145%2f3647642&partnerID=40&md5=0ac96408b7f7e0d3b2ffe8fd4ece99bf,"Dimensionality reduction techniques map values from a high dimensional space to one with a lower dimension. The result is a space which requires less physical memory and has a faster distance calculation. These techniques are widely used where required properties of the reduced-dimension space give an acceptable accuracy with respect to the original space. Many such transforms have been described. They have been classified in two main groups: linear and topological. Linear methods such as Principal Component Analysis (PCA) and Random Projection (RP) define matrix-based transforms into a lower dimension of Euclidean space. Topological methods such as Multidimensional Scaling (MDS) attempt to preserve higher-level aspects such as the nearest-neighbour relation, and some may be applied to non-Euclidean spaces. Here, we introduce nSimplex Zen, a novel topological method of reducing dimensionality. Like MDS, it relies only upon pairwise distances measured in the original space. The use of distances, rather than coordinates, allows the technique to be applied to both Euclidean and other Hilbert spaces, including those governed by Cosine, Jensen–Shannon and Quadratic Form distances. We show that in almost all cases, due to geometric properties of high-dimensional spaces, our new technique gives better properties than others, especially with reduction to very low dimensions. © 2024 Copyright held by the owner/author(s).",Dimensionality reduction; Hilbert space; metric embedding; metric spaces; n-point property,Hilbert spaces; Nearest neighbor search; Number theory; Topology; Vector spaces; Dimensionality reduction; Dimensionality reduction techniques; Euclidean; High dimensional spaces; Metric embeddings; Metric spaces; Multi-dimensional scaling; N-point property; Property; Topological methods; Principal component analysis
Social Behavior Analysis in Exclusive Enterprise Social Networks by FastHAND,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192347911&doi=10.1145%2f3646552&partnerID=40&md5=ebb53292d6b78d23e1859fa811db9fd4,"There is an emerging trend in the Chinese automobile industries that automakers are introducing exclusive enterprise social networks (EESNs) to expand sales and provide after-sale services. The traditional online social networks (OSNs) and enterprise social networks (ESNs), such as X (formerly known as Twitter) and Yammer, are ingeniously designed to facilitate unregulated communications among equal individuals. However, users in EESNs are naturally social stratified, consisting of both enterprise staffs and customers. In addition, the motivation to operate EESNs can be quite complicated, including providing customer services and facilitating communication among enterprise staffs. As a result, the social behaviors in EESNs can be quite different from those in OSNs and ESNs. In this work, we aim to analyze the social behaviors in EESNs. We consider the Chinese car manufacturer NIO as a typical example of EESNs and provide the following contributions. First, we formulate the social behavior analysis in EESNs as a link prediction problem in heterogeneous social networks. Second, to analyze this link prediction problem, we derive plentiful user features and build multiple meta-path graphs for EESNs. Third, we develop a novel Fast (H)eterogeneous graph (A)ttention (N)etwork algorithm for (D)irected graphs (FastHAND) to predict directed social links among users in EESNs. This algorithm introduces feature group attention at the node-level and uses an edge sampling algorithm over directed meta-path graphs to reduce the computation cost. By conducting various experiments on the NIO community data, we demonstrate the predictive power of our proposed FastHAND method. The experimental results also verify our intuitions about social affinity propagation in EESNs. © 2024 Copyright held by the owner/author(s).",directed graphs; graph attention neural network; graph spectral sparsification; Heterogeneous social network; link prediction,Automotive industry; Forecasting; Graphic methods; Sales; Social networking (online); Behavior analysis; Graph attention neural network; Graph spectral sparsification; Heterogeneous social network; Link prediction; Neural-networks; Path graphs; Prediction problem; Social behaviour; Sparsification; Directed graphs
Intricate Spatiotemporal Dependency Learning for Temporal Knowledge Graph Reasoning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192378804&doi=10.1145%2f3648366&partnerID=40&md5=a0df98edaac7a891c89f21816cf0b44c,"Knowledge Graph (KG) reasoning has been an interesting topic in recent decades. Most current researches focus on predicting the missing facts for incomplete KG. Nevertheless, Temporal KG (TKG) reasoning, which is to forecast future facts, still faces with a dilemma due to the complex interactions between entities over time. This article proposes a novel intricate Spatiotemporal Dependency learning Network (STDN) based on Graph Convolutional Network (GCN) to capture the underlying correlations of an entity at different timestamps. Specifically, we first learn an adaptive adjacency matrix to depict the direct dependencies from the temporally adjacent facts of an entity, obtaining its previous context embedding. Then, a Spatiotemporal feature Encoding GCN (STE-GCN) is proposed to capture the latent spatiotemporal dependencies of the entity, getting the spatiotemporal embedding. Finally, a time gate unit is used to integrate the previous context embedding and the spatiotemporal embedding at the current timestamp to update the entity evolutional embedding for predicting future facts. STDN could generate the more expressive embeddings for capturing the intricate spatiotemporal dependencies in TKG. Extensive experiments on WIKI, ICEWS14, and ICEWS18 datasets prove our STDN has the advantage over state-of-the-art baselines for the temporal reasoning task. © 2024 Copyright held by the owner/author(s).",adaptive adjacency matrix; graph convolutional network; spatiotemporal dependency; temporal dependency; Temporal knowledge graph,Convolution; Data mining; Forecasting; Graph embeddings; Adaptive adjacency matrix; Adjacency matrix; Convolutional networks; Embeddings; Graph convolutional network; Knowledge graphs; Spatio-temporal dependencies; Temporal dependency; Temporal knowledge; Temporal knowledge graph; Knowledge graph
DP-GCN: Node Classification by Connectivity and Local Topology Structure on Real-World Network,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192370996&doi=10.1145%2f3649460&partnerID=40&md5=8ea997305d85ff09cd90e45d6f0fbdee,"Node classification is to predict the class label of a node by analyzing its properties and interactions in a network. We note that many existing solutions for graph-based node classification only consider node connectivity but not the node’s local topology structure. However, nodes residing in different parts of a real-world network may share similar local topology structures. For example, local topology structures in a payment network may reveal sellers’ business roles (e.g., supplier or retailer). To model both connectivity and local topology structure for better node classification performance, we present DP-GCN, a dual-path graph convolution network. DP-GCN consists of three main modules: (i) a C-GCN module to capture the connectivity relationships between nodes, (ii) a T-GCN module to capture the topology structure similarity among nodes, and (iii) a multi-head self-attention module to align both properties. We evaluate DP-GCN on seven benchmark datasets against diverse baselines to demonstrate its effectiveness. We also provide a case study of running DP-GCN on three large-scale payment networks from PayPal, a leading payment service provider, for risky seller detection. Experimental results show DP-GCN’s effectiveness and practicability in large-scale settings. PayPal’s internal testing also shows DP-GCN’s effectiveness in defending against real risks from transaction networks. © 2024 Copyright held by the owner/author(s).",large-scale graph; network topology structure; Node classification,Data mining; Graph neural networks; Graph theory; Class labels; Graph-based; Large-scale graph; Large-scales; Local topology; Network topology structure; Node classification; Property; Real-world networks; Topology structure; Graphic methods
ProtoMGAE: Prototype-Aware Masked Graph Auto-Encoder for Graph Representation Learning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192480402&doi=10.1145%2f3649143&partnerID=40&md5=e7fc79f04d315f6f5a2c5ec3a8d75a72,"Graph self-supervised representation learning has gained considerable attention and demonstrated remarkable efficacy in extracting meaningful representations from graphs, particularly in the absence of labeled data. Two representative methods in this domain are graph auto-encoding and graph contrastive learning. However, the former methods primarily focus on global structures, potentially overlooking some fine-grained information during reconstruction. The latter methods emphasize node similarity across correlated views in the embedding space, potentially neglecting the inherent global graph information in the original input space. Moreover, handling incomplete graphs in real-world scenarios, where original features are unavailable for certain nodes, poses challenges for both types of methods. To alleviate these limitations, we integrate masked graph auto-encoding and prototype-aware graph contrastive learning into a unified model to learn node representations in graphs. In our method, we begin by masking a portion of node features and utilize a specific decoding strategy to reconstruct the masked information. This process facilitates the recovery of graphs from a global or macro level and enables handling incomplete graphs easily. Moreover, we treat the masked graph and the original one as a pair of contrasting views, enforcing the alignment and uniformity between their corresponding node representations at a local or micro level. Last, to capture cluster structures from a meso level and learn more discriminative representations, we introduce a prototype-aware clustering consistency loss that is jointly optimized with the preceding two complementary objectives. Extensive experiments conducted on several datasets demonstrate that the proposed method achieves significantly better or competitive performance on downstream tasks, especially for graph clustering, compared with the state-of-the-art methods, showcasing its superiority in enhancing graph representation learning. © 2024 Copyright held by the owner/author(s).",graph auto-encoder; graph contrastive learning; masking; Self-supervised representation learning,Encoding (symbols); Graph theory; Graphic methods; Learning systems; Auto encoders; Encodings; Global structure; Graph auto-encoder; Graph contrastive learning; Graph representation; Labeled data; Learn+; Masking; Self-supervised representation learning; Signal encoding
Scalable and Inductive Semi-supervised Classifier with Sample Weighting Based on Graph Topology,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189665127&doi=10.1145%2f3643645&partnerID=40&md5=88ba710ceb86dfd8fdbb022f6d553c01,"Recently, graph-based semi-supervised learning (GSSL) has garnered significant interest in the realms of machine learning and pattern recognition. Although some of the proposed methods have made some progress, there are still some shortcomings that need to be overcome. There are three main limitations. First, the graphs used in these approaches are usually predefined regardless of the task at hand. Second, due to the use of graphs, almost all approaches are unable to process and consider data with a very large number of unlabeled samples. Thirdly, the imbalance of the topology of the samples is very often not taken into account. In particular, processing large datasets with GSSL might pose challenges in terms of computational resource feasibility. In this article, we present a scalable and inductive GSSL method. We broaden the scope of the graph topology imbalance paradigm to extensive databases. Second, we employ the calculated weights of the labeled sample for the label-matching term in the global objective function. This leads to a unified, scalable, semi-supervised learning model that allows simultaneous labeling of unlabeled data, projection of the feature space onto the labeling space, along with the graph matrix of anchors. In the proposed scheme, the integration of labels and features from anchors is applied for the adaptive construction of the anchor graph. Experimental results were performed on four large databases: NORB, RCV1, Covtype, and MNIST. These experiments demonstrate that the proposed method exhibits superior performance when compared to existing scalable semi-supervised learning models. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph structured data; inductive semi-supervised models; reduced flexible manifold embedding; Scalable graph-based semi-supervised learning; topology imbalance,Anchors; Embeddings; Graph algorithms; Large datasets; Pattern recognition; Semi-supervised learning; Embeddings; Graph structured data; Graph topology; Graph-based; Inductive semi-supervised model; Reduced flexible manifold embedding; Scalable graph-based semi-supervised learning; Semi-supervised; Semi-supervised learning; Topology imbalance; Graphic methods
Enhancing Out-of-distribution Generalization on Graphs via Causal Attention Learning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189348084&doi=10.1145%2f3644392&partnerID=40&md5=5f199d62a28d5651f47444deb750feac,"In graph classification, attention- and pooling-based graph neural networks (GNNs) predominate to extract salient features from the input graph and support the prediction. They mostly follow the paradigm of “learning to attend,” which maximizes the mutual information between the attended graph and the ground-truth label. However, this paradigm causes GNN classifiers to indiscriminately absorb all statistical correlations between input features and labels in the training data without distinguishing the causal and noncausal effects of features. Rather than emphasizing causal features, the attended graphs tend to rely on noncausal features as shortcuts to predictions. These shortcut features may easily change outside the training distribution, thereby leading to poor generalization for GNN classifiers. In this article, we take a causal view on GNN modeling. Under our causal assumption, the shortcut feature serves as a confounder between the causal feature and prediction. It misleads the classifier into learning spurious correlations that facilitate prediction in in-distribution (ID) test evaluation while causing significant performance drop in out-of-distribution (OOD) test data. To address this issue, we employ the backdoor adjustment from causal theory—combining each causal feature with various shortcut features, to identify causal patterns and mitigate the confounding effect. Specifically, we employ attention modules to estimate the causal and shortcut features of the input graph. Then, a memory bank collects the estimated shortcut features, enhancing the diversity of shortcut features for combination. Simultaneously, we apply the prototype strategy to improve the consistency of intra-class causal features. We term our method as CAL+, which can promote stable relationships between causal estimation and prediction, regardless of distribution changes. Extensive experiments on synthetic and real-world OOD benchmarks demonstrate our method’s effectiveness in improving OOD generalization. Our codes are released at https://github.com/shuyao-wang/CAL-plus. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention mechanism; Graph learning; out-of-distribution generalization,Classification (of information); Forecasting; Attention mechanisms; Generalisation; Graph classification; Graph learning; Graph neural networks; Input graphs; Mutual informations; Neural networks classifiers; Out-of-distribution generalization; Salient features; Graph neural networks
X-FSPMiner: A Novel Algorithm for Frequent Similar Pattern Mining,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189363435&doi=10.1145%2f3643820&partnerID=40&md5=35ab386341190a0ee67670e9ae6f47c7,"Frequent similar pattern mining (FSP mining) allows for finding frequent patterns hidden from the classical approach. However, the use of similarity functions implies more computational effort, necessitating the development of more efficient algorithms for FSP mining. This work aims to improve the efficiency of mining all FSPs when using Boolean and non-increasing monotonic similarity functions. A data structure to condense an object description collection, named FV-Tree, and an algorithm for mining all FSPs from the FV-Tree, named X-FSPMiner, are proposed. The experimental results reveal that the novel algorithm X-FSPMiner vastly outperforms the state-of-the-art algorithms for mining all FSPs using Boolean and non-increasing monotonic similarity functions. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Data mining; frequent patterns; mixed data; similarity functions,Computational efficiency; Trees (mathematics); Classical approach; Computational effort; Frequent pattern; Mixed data; Monotonics; Novel algorithm; Object description; Similar pattern mining; Similarity functions; State-of-the-art algorithms; Data mining
A Taxonomy for Learning with Perturbation and Algorithms,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189319655&doi=10.1145%2f3644391&partnerID=40&md5=3cee56679e98a448fc9ad1b51c677604,"Weighting strategy prevails in machine learning. For example, a common approach in robust machine learning is to exert low weights on samples which are likely to be noisy or quite hard. This study summarizes another less-explored strategy, namely, perturbation. Various incarnations of perturbation have been utilized but it has not been explicitly revealed. Learning with perturbation is called perturbation learning and a systematic taxonomy is constructed for it in this study. In our taxonomy, learning with perturbation is divided on the basis of the perturbation targets, directions, inference manners, and granularity levels. Many existing learning algorithms including some classical ones can be understood with the constructed taxonomy. Alternatively, these algorithms share the same component, namely, perturbation in their procedures. Furthermore, a family of new learning algorithms can be obtained by varying existing learning algorithms with our taxonomy. Specifically, three concrete new learning algorithms are proposed for robust machine learning. Extensive experiments on image classification and text sentiment analysis verify the effectiveness of the three new algorithms. Learning with perturbation can also be used in other various learning scenarios, such as imbalanced learning, clustering, regression, and so on. © 2024 Copyright held by the owner/author(s).",learning taxonomy; perturbation; robust machine learning; Sample weighting,Learning algorithms; Machine learning; Sentiment analysis; Granularity levels; Learning taxonomy; Machine-learning; Perturbation; Perturbation learning; Robust machine learning; Sample's weighting; Target direction; Taxonomy learning; Weighting strategies; Taxonomies
Math Word Problem Generation via Disentangled Memory Retrieval,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189440136&doi=10.1145%2f3639569&partnerID=40&md5=67b6ff04a6815d316f5a5637b57a8da7,"The task of math word problem (MWP) generation, which generates an MWP given an equation and relevant topic words, has increasingly attracted researchers' attention. In this work, we introduce a simple memory retrieval module to search related training MWPs, which are used to augment the generation. To retrieve more relevant training data, we also propose a disentangled memory retrieval module based on the simple memory retrieval module. To this end, we first disentangle the training MWPs into logical description and scenario description and then record them in respective memory modules. Later, we use the given equation and topic words as queries to retrieve relevant logical descriptions and scenario descriptions from the corresponding memory modules, respectively. The retrieved results are then used to complement the process of the MWP generation. Extensive experiments and ablation studies verify the superior performance of our method and the effectiveness of each proposed module. The code is available at https://github.com/mwp-g/MWPG-DMR.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",math word problem; Memory; retrieval; text generation.,Memory architecture; Math word problem; Memory modules; Memory retrieval; Retrieval; Scenario description; Simple++; Text generation.; Text generations; Topic words; Word problem; Information retrieval
Generation-based Multi-view Contrast for Self-supervised Graph Representation Learning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189451786&doi=10.1145%2f3645095&partnerID=40&md5=18a8906d4406dc60d92390016df6d348,"Graph contrastive learning has made remarkable achievements in the self-supervised representation learning of graph-structured data. By employing perturbation function (i.e., perturbation on the nodes or edges of graph), most graph contrastive learning methods construct contrastive samples on the original graph. However, the perturbation-based data augmentation methods randomly change the inherent information (e.g., attributes or structures) of the graph. Therefore, after nodes embedding on the perturbed graph, we cannot guarantee the validity of the contrastive samples as well as the learned performance of graph contrastive learning. To this end, in this article, we propose a novel generation-based multi-view contrastive learning framework (GMVC) for self-supervised graph representation learning, which generates the contrastive samples based on our generator rather than perturbation function. Specifically, after nodes embedding on the original graph we first employ random walk in the neighborhood to develop multiple relevant node sequences for each anchor node. We then utilize the transformer to generate the representations of relevant contrastive samples of anchor node based on the features and structures of the sampled node sequences. Finally, by maximizing the consistency between the anchor view and the generated views, we force the model to effectively encode graph information into nodes embeddings. We perform extensive experiments of node classification and link prediction tasks on eight benchmark datasets, which verify the effectiveness of our generation-based multi-view graph contrastive learning method.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive learning; multi-view generation; representation learning,Graph embeddings; Graph theory; Graphic methods; Anchor nodes; Contrastive learning; Embeddings; Graph representation; Learning methods; Multi-view generation; Multi-views; Perturbation functions; Representation learning; View generation; Classification (of information)
Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189668716&doi=10.1145%2f3643821&partnerID=40&md5=c00bdcbadcc141eb9bdadcd58198c665,"With increasing frequency of high-profile privacy breaches in various online platforms, users are becoming more concerned about their privacy. And recommender system is the core component of online platforms for providing personalized service, consequently, its privacy preservation has attracted great attention. As the gold standard of privacy protection, differential privacy has been widely adopted to preserve privacy in recommender systems. However, existing differentially private recommender systems only consider static and independent interactions, so they cannot apply to sequential recommendation where behaviors are dynamic and dependent. Meanwhile, little attention has been paid on the privacy risk of sensitive user features, most of them only protect user feedbacks. In this work, we propose a novel DIfferentially Private Sequential recommendation framework with a noisy Graph Neural Network approach (denoted as DIPSGNN) to address these limitations. To the best of our knowledge, we are the first to achieve differential privacy in sequential recommendation with dependent interactions. Specifically, in DIPSGNN, we first leverage piecewise mechanism to protect sensitive user features. Then, we innovatively add calibrated noise into aggregation step of graph neural network based on aggregation perturbation mechanism. And, this noisy graph neural network can protect sequentially dependent interactions and capture user preferences simultaneously. Extensive experiments demonstrate the superiority of our method over state-of-the-art differentially private recommender systems in terms of better balance between privacy and accuracy. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Differential privacy; graph neural networks; sequential recommendation,Graph neural networks; User profile; Core components; Differential privacies; Gold standards; Graph neural networks; Online platforms; Personalized service; Privacy breaches; Privacy preservation; Sequential recommendation; User feature; Recommender systems
Local Community Detection in Multiple Private Networks,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189455699&doi=10.1145%2f3644078&partnerID=40&md5=b6b6a1e1113b18d6a65e3dc69acfa0b4,"Individuals are often involved in multiple online social networks. Considering that owners of these networks are unwilling to share their networks, some global algorithms combine information from multiple networks to detect all communities in multiple networks without sharing their edges. When data owners are only interested in the community containing a given node, it is unnecessary and computationally expensive for multiple networks to interact with each other to mine all communities. Moreover, data owners who are specifically looking for a community typically prefer to provide less data than the global algorithms require. Therefore, we propose the Local Collaborative Community Detection problem (LCCD). It exploits information from multiple networks to jointly detect the local community containing a given node without directly sharing edges between networks. To address the LCCD problem, we present a method developed from M method, called colM, to detect the local community in multiple networks. This method adopts secure multiparty computation protocols to protect each network's private information. Our experiments were conducted on real-world and synthetic datasets. Experimental results show that colM method could effectively identify community structures and outperform comparison algorithms.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Community detection; Local collaborative community detection; Local community detection; Multi-network local modularity,Network security; Population dynamics; Collaborative community; Community detection; Local collaborative community detection; Local community; Local community detection; Local modularity; Multi-network local modularity; Multiple networks; Social networking (online)
Mining Top-k High On-shelf Utility Itemsets Using Novel Threshold Raising Strategies,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189435187&doi=10.1145%2f3645115&partnerID=40&md5=5dfe61a7b5d9de9e4e83c8a959f33849,"High utility itemsets (HUIs) mining is an emerging area of data mining which discovers sets of items generating a high profit from transactional datasets. In recent years, several algorithms have been proposed for this task. However, most of them do not consider the on-shelf time period of items and negative utility of items. High on-shelf utility itemset (HOUIs) mining is more difficult than traditional HUIs mining because it deals with on-shelf-based time period and negative utility of items. Moreover, most algorithms need minimum utility threshold (min_util) to find rules. However, specifying the appropriate min_util threshold is a difficult problem for users. A smaller min_util threshold may generate too many rules and a higher one may generate a few rules, which can degrade performance. To address these issues, a novel top-k HOUIs mining algorithm named TKOS (Top-K high On-Shelf utility itemsets miner) is proposed which considers on-shelf time period and negative utility. TKOS presents a novel branch and bound-based strategy to raise the internal min_util threshold efficiently. It also presents two pruning strategies to speed up the mining process. In order to reduce the dataset scanning cost, we utilize transaction merging and dataset projection techniques. Extensive experiments have been conducted on real and synthetic datasets having various characteristics. Experimental results show that the proposed algorithm outperforms the state-of-the-art algorithms. The proposed algorithm is up to 42 times faster and uses up-to 19 times less memory compared to the state-of-the-art KOSHU. Moreover, the proposed algorithm has excellent scalability in terms of time periods and the number of transactions.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",branch & bound; High on-shelf utility itemsets; negative utility; on-shelf time periods; top-k itemsets; utility mining,Branch & bound; High on-shelf utility itemset; High utility itemset minings; Itemset; Negative utility; On-shelf time period; Time-periods; Top-k itemset; Utility mining; Data mining
Conditional Generative Adversarial Network for Early Classification of Longitudinal Datasets Using an Imputation Approach,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189457072&doi=10.1145%2f3644821&partnerID=40&md5=2dab585eaf879933c24625e12d73992c,"Early classification of longitudinal data remains an active area of research today. The complexity of these datasets and the high rates of missing data caused by irregular sampling present data-level challenges for the Early Longitudinal Data Classification (ELDC) problem. Coupled with the algorithmic challenge of optimising the opposing objectives of early classification (i.e., earliness and accuracy), ELDC becomes a non-trivial task. Inspired by the generative power and utility of the Generative Adversarial Network (GAN), we propose a novel context-conditional, longitudinal early classifier GAN (LEC-GAN). This model utilises informative missingness, static features and earlier observations to improve the ELDC objective. It achieves this by incorporating ELDC as an auxiliary task within an imputation optimization process. Our experiments on several datasets demonstrate that LEC-GAN outperforms all relevant baselines in terms of F1 scores while increasing the earliness of prediction.  © 2024 Copyright held by the owner/author(s).",adversarial networks; longitudinal data classification; Longitudinal datasets,Classification (of information); Large datasets; Active area; Adversarial networks; Data classification; Data level; High rate; Irregular sampling; Longitudinal data; Longitudinal data classification; Longitudinal dataset; Missing data; Generative adversarial networks
On the Value of Head Labels in Multi-Label Text Classification,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189290860&doi=10.1145%2f3643853&partnerID=40&md5=bc8f2562c8bf1db0f0f5bb74fbe05beb,"A formidable challenge in the multi-label text classification (MLTC) context is that the labels often exhibit a long-tailed distribution, which typically prevents deep MLTC models from obtaining satisfactory performance. To alleviate this problem, most existing solutions attempt to improve tail performance by means of sampling or introducing extra knowledge. Data-rich labels, though more trustworthy, have not received the attention they deserve. In this work, we propose a multiple-stage training framework to exploit both model- and feature-level knowledge from the head labels, to improve both the representation and generalization ability of MLTC models. Moreover, we theoretically prove the superiority of our framework design over other alternatives. Comprehensive experiments on widely used MLTC datasets clearly demonstrate that the proposed framework achieves highly superior results to state-of-the-art methods, highlighting the value of head labels in MLTC. © 2024 Copyright held by the owner/author(s).",long-tail; Multi-label text classification; self-supervised learning,Supervised learning; Text processing; Feature level; Generalization ability; Long tail; Long-tailed distributions; Multi-label text classification; Multiple stages; Performance; Self-supervised learning; Text classification models; Training framework; Classification (of information)
Incorporating Multi-Level Sampling with Adaptive Aggregation for Inductive Knowledge Graph Completion,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189319455&doi=10.1145%2f3644822&partnerID=40&md5=ff566c55e418840bf0cebab4e4809606,"In recent years, Graph Neural Networks (GNNs) have achieved unprecedented success in handling graph-structured data, thereby driving the development of numerous GNN-oriented techniques for inductive knowledge graph completion (KGC). A key limitation of existing methods, however, is their dependence on pre-defined aggregation functions, which lack the adaptability to diverse data, resulting in suboptimal performance on established benchmarks. Another challenge arises from the exponential increase in irrelated entities as the reasoning path lengthens, introducing unwarranted noise and consequently diminishing the model’s generalization capabilities. To surmount these obstacles, we design an innovative framework that synergizes Multi-Level Sampling with an Adaptive Aggregation mechanism (MLSAA). Distinctively, our model couples GNNs with enhanced set transformers, enabling dynamic selection of the most appropriate aggregation function tailored to specific datasets and tasks. This adaptability significantly boosts both the model’s flexibility and its expressive capacity. Additionally, we unveil a unique sampling strategy designed to selectively filter irrelevant entities, while retaining potentially beneficial targets throughout the reasoning process. We undertake an exhaustive evaluation of our novel inductive KGC method across three pivotal benchmark datasets and the experimental results corroborate the efficacy of MLSAA. © 2024 Copyright held by the owner/author(s).",adaptive aggregation; Inductive knowledge graph completion; multi-level sampling,Benchmarking; Data mining; Graph neural networks; Adaptive aggregation; Aggregation functions; Aggregation mechanism; Graph neural networks; Graph structured data; Inductive knowledge graph completion; Knowledge graphs; Multi-level sampling; Multilevels; Network oriented; Knowledge graph
Multi-Instance Learning with One Side Label Noise,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189316061&doi=10.1145%2f3644076&partnerID=40&md5=e70d5404b04d5106bf881d228a732924,"Multi-instance Learning (MIL) is a popular learning paradigm arising from many real applications. It assigns a label to a set of instances, which is called a bag, and the bag’s label is determined by the instances within it. A bag is positive if and only if it has at least one positive instance. Since labeling bags is more complicated than labeling each instance, we will often face the mislabeling problem in MIL. Furthermore, it is more common that a negative bag has been mislabeled to a positive one, since one mislabeled instance will lead to the change of the whole bag label. This is an important problem that originated from real applications, e.g., web mining and image classification, but little research has concentrated on it as far as we know. In this article, we focus on this MIL problem with one side label noise that the negative bags are mislabeled as positive ones. To address this challenging problem, we propose, to the best our our knowledge, a novel multi-instance learning method with one side label noise. We design a new double weighting approach under traditional framework to characterize the “faithfulness” of each instance and each bag in learning the classifier. Briefly, on the instance level, we employ a sparse weighting method to select the key instances, and the MIL problem with one size label noise is converted to a mislabeled supervised learning scenario. On the bag level, the weights of bags, together with the selected key instances, will be utilized to identify the real positive bags. In addition, we have solved our proposed model by an alternative iteration method with proved convergence behavior. Empirical studies on various datasets have validated the effectiveness of our method. © 2024 Association for Computing Machinery. All rights reserved.",Multi-instance learning; one side label noise; sparse; weighting,Data mining; Learning systems; Labelings; Learning paradigms; Learning problem; Multi-instance learning; One side label noise; Positive instances; Real applications; Sparse; Web Mining; Weighting; Iterative methods
Asymmetric Learning for Graph Neural Network based Link Prediction,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189489854&doi=10.1145%2f3640347&partnerID=40&md5=d0eba5ff0a81a6da38d272dab11491ea,"Link prediction is a fundamental problem in many graph-based applications, such as protein-protein interaction prediction. Recently, graph neural network (GNN) has been widely used for link prediction. However, existing GNN-based link prediction (GNN-LP) methods suffer from scalability problem during training for large-scale graphs, which has received little attention from researchers. In this paper, we first analyze the computation complexity of existing GNN-LP methods, revealing that one reason for the scalability problem stems from their symmetric learning strategy in applying the same class of GNN models to learn representation for both head nodes and tail nodes. We then propose a novel method, called asymmetric learning (AML), for GNN-LP. More specifically, AML applies a GNN model to learn head node representation while applying a multi-layer perceptron (MLP) model to learn tail node representation. To the best of our knowledge, AML is the first GNN-LP method to adopt an asymmetric learning strategy for node representation learning. Furthermore, we design a novel model architecture and apply a row-wise mini-batch sampling strategy to ensure promising model accuracy and training efficiency for AML. Experiments on three real large-scale datasets show that AML is 1.7× 1/47.3× faster in training than baselines with a symmetric learning strategy while having almost no accuracy loss.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",large-scale graphs; link prediction; neural networks,Forecasting; Graph neural networks; Graphic methods; Large datasets; Learning systems; Scalability; Asymmetric learning; Graph neural networks; Large-scale graph; Large-scales; Learn+; Learning strategy; Link prediction; Network LP; Network-based; Neural-networks; Proteins
Correlation-aware Graph Data Augmentation with Implicit and Explicit Neighbors,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189693691&doi=10.1145%2f3638057&partnerID=40&md5=610acd088ae93a8aee28df8d21bc31a5,"In recent years, there has been a significant surge in commercial demand for citation graph-based tasks, such as patent analysis, social network analysis, and recommendation systems. Graph Neural Networks (GNNs) are widely used for these tasks due to their remarkable performance in capturing topological graph information. However, GNNs’ output results are highly dependent on the composition of local neighbors within the topological structure. To address this issue, we identify two types of neighbors in a citation graph: explicit neighbors based on the topological structure and implicit neighbors based on node features. Our primary motivation is to clearly define and visualize these neighbors, emphasizing their importance in enhancing graph neural network performance. We propose a Correlation-aware Network (CNet) to re-organize the citation graph and learn more valuable informative representations by leveraging these implicit and explicit neighbors. Our approach aims to improve graph data augmentation and classification performance, with the majority of our focus on stating the importance of using these neighbors, while also introducing a new graph data augmentation method. We compare CNet with state-of-the-art (SOTA) GNNs and other graph data augmentation approaches acting on GNNs. Extensive experiments demonstrate that CNet effectively extracts more valuable informative representations from the citation graph, significantly outperforming baselines. The code is available on public GitHub.1 © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",citation graphs; explicit neighbors; graph data augmentation; Graph neural networks; implicit,Graphic methods; Topology; Citation graphs; Commercial demand; Data augmentation; Explicit neighbor; Graph data; Graph data augmentation; Graph neural networks; Graph-based; Implicit; Topological structure; Graph neural networks
Networked Time-series Prediction with Incomplete Data via Generative Adversarial Network,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189323688&doi=10.1145%2f3643822&partnerID=40&md5=49137b33764b5bf54fc3d91986e3d633,"A networked time series (NETS) is a family of time series on a given graph, one for each node. It has a wide range of applications from intelligent transportation to environment monitoring to smart grid management. An important task in such applications is to predict the future values of a NETS based on its historical values and the underlying graph. Most existing methods require complete data for training. However, in real-world scenarios, it is not uncommon to have missing data due to sensor malfunction, incomplete sensing coverage, and so on. In this article, we study the problem of NETS prediction with incomplete data. We propose networked time series Imputation Generative Adversarial Network (NETS-ImpGAN), a novel deep learning framework that can be trained on incomplete data with missing values in both history and future. Furthermore, we propose Graph Temporal Attention Networks, which incorporate the attention mechanism to capture both inter-time series and temporal correlations. We conduct extensive experiments on four real-world datasets under different missing patterns and missing rates. The experimental results show that NETS-ImpGAN outperforms existing methods, reducing the Mean Absolute Error by up to 25%. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",imputation; incomplete data; Networked time series; prediction,Deep learning; Generative adversarial networks; Time series; Environment monitoring; Grid management; Imputation; Incomplete data; Intelligent transportation; Networked time series; Smart grid; Time series prediction; Times series; Underlying graphs; Forecasting
TaSPM: Targeted Sequential Pattern Mining,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189313779&doi=10.1145%2f3639827&partnerID=40&md5=f4d967bfeeecd080e378de62b983a4fc,"Sequential pattern mining (SPM) is an important technique in the field of pattern mining, which has many applications in reality. Although many efficient SPM algorithms have been proposed, there are few studies that can focus on targeted tasks. Targeted querying of the concerned sequential patterns can not only reduce the number of patterns generated, but also increase the efficiency of users in performing related analysis. The current algorithms available for targeted sequence querying are based on specific scenarios and can not be extended to other applications. In this article, we formulate the problem of targeted sequential pattern mining and propose a generic algorithm, namely TaSPM. What is more, to improve the efficiency of TaSPM on large-scale datasets and multiple-item-based sequence datasets, we propose several pruning strategies to reduce meaningless operations in the mining process. Totally four pruning strategies are designed in TaSPM, and hence TaSPM can terminate unnecessary pattern extensions quickly and achieve better performance. Finally, we conducted extensive experiments on different datasets to compare the baseline SPM algorithm with TaSPM. Experiments show that the novel targeted mining algorithm TaSPM can achieve faster running time and less memory consumption. © 2024 Copyright held by the owner/author(s).",Data mining; sequential pattern; target sequence; targeted querying,Computer programming; Data mining; Database systems; Large datasets; 'current; Generic algorithm; Large-scale datasets; Pattern mining; Pruning strategy; Sequential pattern mining algorithm; Sequential patterns; Sequential-pattern mining; Target sequences; Targeted querying; Efficiency
Multi-Task Learning with Sequential Dependence Toward Industrial Applications: A Systematic Formulation,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189456508&doi=10.1145%2f3640468&partnerID=40&md5=3fa7b967036e89cbb20c96c6935bdff1,"Multi-task learning (MTL) is widely used in the online recommendation and financial services for multi-step conversion estimation, but current works often overlook the sequential dependence among tasks. In particular, sequential dependence multi-task learning (SDMTL) faces challenges in dealing with complex task correlations and extracting valuable information in real-world scenarios, leading to negative transfer and a deterioration in the performance. Herein, a systematic learning paradigm of the SDMTL problem is established for the first time, which applies to more general multi-step conversion scenarios with longer conversion paths or various task dependence relationships. Meanwhile, an SDMTL architecture, named Task-Aware Feature Extraction (TAFE), is designed to enable the dynamic task representation learning from a sample-wise view. TAFE selectively reconstructs the implicit shared information corresponding to each sample case and performs the explicit task-specific extraction under dependence constraints, which can avoid the negative transfer, resulting in more effective information sharing and joint representation learning. Extensive experiment results demonstrate the effectiveness and applicability of the proposed theoretical and implementation frameworks. Furthermore, the online evaluations at MYbank showed that TAFE had an average increase of 9.22% and 3.76% in various scenarios on the post-view click-through & conversion rate (CTCVR) estimation task. Currently, TAFE is deployed in an online platform to provide various traffic services.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",industrial applications; Multi-task learning; negative transfer; sequential dependency,Deterioration; Learning systems; 'current; Complex task; Features extraction; Financial service; Multisteps; Multitask learning; Negative transfer; Real-world scenario; Sequential dependencies; Task-aware; Extraction
Domain Generalization in Time Series Forecasting,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189676067&doi=10.1145%2f3643035&partnerID=40&md5=1cd603ee52caaafea77e4e071aa6bd50,"Domain generalization aims to design models that can effectively generalize to unseen target domains by learning from observed source domains. Domain generalization poses a significant challenge for time series data, due to varying data distributions and temporal dependencies. Existing approaches to domain generalization are not designed for time series data, which often results in suboptimal or unstable performance when confronted with diverse temporal patterns and complex data characteristics. We propose a novel approach to tackle the problem of domain generalization in time series forecasting. We focus on a scenario where time series domains share certain common attributes and exhibit no abrupt distribution shifts. Our method revolves around the incorporation of a key regularization term into an existing time series forecasting model: domain discrepancy regularization. In this way, we aim to enforce consistent performance across different domains that exhibit distinct patterns. We calibrate the regularization term by investigating the performance within individual domains and propose the domain discrepancy regularization with domain difficulty awareness. We demonstrate the effectiveness of our method on multiple datasets, including synthetic and real-world time series datasets from diverse domains such as retail, transportation, and finance. Our method is compared against traditional methods, deep learning models, and domain generalization approaches to provide comprehensive insights into its performance. In these experiments, our method showcases superior performance, surpassing both the base model and competing domain generalization models across all datasets. Furthermore, our method is highly general and can be applied to various time series models. © 2024 Copyright held by the owner/author(s).",domain generalization; regularization; Time series forecasting,Deep learning; Forecasting; Learning systems; Data distribution; Design models; Domain generalization; Generalisation; Performance; Regularisation; Regularization terms; Target domain; Time series forecasting; Time-series data; Time series
A Survey on AutoML Methods and Systems for Clustering,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189684769&doi=10.1145%2f3643564&partnerID=40&md5=e868e2f02fe3e76dd4770da8b8f9d3ba,"Automated Machine Learning (AutoML) aims to identify the best-performing machine learning algorithm along with its input parameters for a given dataset and a specific machine learning task. This is a challenging problem, as the process of finding the best model and tuning it for a particular problem at hand is both time-consuming for a data scientist and computationally expensive. In this survey, we focus on unsupervised learning, and we turn our attention on AutoML methods for clustering. We present a systematic review that includes many recent research works for automated clustering. Furthermore, we provide a taxonomy for the classification of existing works, and we perform a qualitative comparison. As a result, this survey provides a comprehensive overview of the field of AutoML for clustering. Moreover, we identify open challenges for future research in this field. © 2024 Copyright held by the owner/author(s).",algorithm selection; automated machine learning; clustering; hyperparameter tuning; meta-learning; Unsupervised learning,Automation; Clustering algorithms; Learning algorithms; Algorithm selection; Automated machine learning; Automated machines; Clusterings; Hyper-parameter; Hyperparameter tuning; Machine learning methods; Machine learning systems; Machine-learning; Metalearning; Unsupervised learning
Supervised Clustering of Persian Handwritten Images Using Regularization and Dimension Reduction Methods,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189689380&doi=10.1145%2f3638060&partnerID=40&md5=db89c1b4d4876f161cac439066217a96,"Clustering, as a fundamental exploratory data technique, not only is used to discover patterns and structures in complex datasets but also is utilized to group variables in high-dimensional data analysis. Dimension reduction through clustering helps identify important variables and reduce data dimensions without losing significant information. High-dimensional image datasets, such as Persian handwritten images, have numerous pixels, making statistical inference difficult. Such high-dimensionality property pose challenges for analysis and processing, requiring specialized techniques like clustering to extract information. Incorporating response variable information enhances clustering analysis, transforming it into a supervised method. This article evaluates a supervised clustering approach using Ridge and Lasso penalties, comparing them in analyzing a real dataset while identifying important variables. We demonstrate that despite choosing a small number of variables as important variables, Lasso penalty performs relatively well in predicting the labels of new observations for this multi-class dataset. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dimension reduction; high-dimensional data; Lasso; Persian handwritten images; regularization methods; Supervised clustering,Data mining; Data reduction; Reduction; Regression analysis; Clusterings; Dimension reduction; Handwritten images; High dimensional data; Lasso; Persian handwritten image; Persians; Regularisation; Regularization methods; Supervised clustering; Clustering algorithms
FiFrauD: Unsupervised Financial Fraud Detection in Dynamic Graph Streams,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189683155&doi=10.1145%2f3641857&partnerID=40&md5=91524998416f4503e1d77b2cbc8f58ee,"Given a stream of financial transactions between traders in an e-market, how can we accurately detect fraudulent traders and suspicious behaviors in real time? Despite the efforts made in detecting these fraudsters, this field still faces serious challenges, including the ineffectiveness of existing methods for the complex and streaming environment of e-markets. As a result, it is still difficult to quickly and accurately detect suspected traders and behavior patterns in real-time transactions, and it is still considered an open problem. To solve this problem and alleviate the existing challenges, in this article, we propose FiFrauD, which is an unsupervised, scalable approach that depicts the behavior of manipulators in a transaction stream. In this approach, real-time transactions between traders are converted into a stream of graphs and, instead of using supervised and semi-supervised learning methods, fraudulent traders are detected precisely by exploiting density signals in graphs. Specifically, we reveal the traits of fraudulent traders in the market and propose a novel metric from this perspective, i.e., graph topology, time, and behavior. Then, we search for suspicious blocks by greedily optimizing the proposed metric. Theoretical analysis demonstrates upper bounds for FiFrauD’s effectiveness in catching suspicious trades. Extensive experiments on five real-world datasets with both actual and synthetic labels demonstrate that FiFrauD achieves significant accuracy improvements compared with state-of-the-art fraud detection methods. Also, it can find various suspicious behavior patterns in a linear runtime and provide interpretable results. Furthermore, FiFrauD is resistant to the camouflage tactics used by fraudulent traders. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Dynamic graph; fraud detection; market manipulation detection; stream learning; unsupervised learning,Crime; Learning systems; Topology; Unsupervised learning; Behaviour patterns; Dynamic graph; E-Market; E-markets; Fraud detection; Market manipulation; Market manipulation detection; Real-time transactions; Stream learning; Suspicious behaviours; Commerce
Package Arrival Time Prediction via Knowledge Distillation Graph Neural Network,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189658387&doi=10.1145%2f3643033&partnerID=40&md5=8b661c758ff6e4ea7103f21386d743f5,"Accurately estimating packages’ arrival time in e-commerce can enhance users’ shopping experience and improve the placement rate of products. This problem is often formalized as an Origin-Destination (OD)-based ETA (i.e., estimated time of arrival) prediction task, where the delivery time is estimated mainly based on sender and receiver addresses and other context information. One inherent challenge of the OD-based ETA problem is that the delivery time highly depends on the actual delivery trajectory which is unknown at the time of prediction. In this article, we tackle this challenge by effectively exploiting historical delivery trajectories. We propose a novel Knowledge Distillation Graph neural network-based package ETA prediction (KDG-ETA) model, which uses knowledge distillation in the training phase to distill the knowledge of historical trajectories into OD pair embeddings. In KDG-ETA, a multi-level trajectory graph representation model is proposed to fully exploit trajectory information at the node-level, edge-level, and path-level. Then, the OD representations embedded with trajectory knowledge are combined with context embeddings from feature extraction module for delivery time prediction using an adaptive attention module. KDG-ETA consistently outperforms existing state-of-the-art OD-based ETA prediction methods on three real-world Alibaba datasets, reducing the Mean Absolute Error (MAE) by 3.0%–39.1% as demonstrated in our extensive empirical evaluation. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph neural network; knowledge distillation; Package arrival time prediction; trajectory data mining,Data mining; Forecasting; Graph embeddings; Graph neural networks; Graph theory; Trajectories; Arrival time; Arrival time predictions; Delivery time; Embeddings; Estimated time of arrivals; Graph neural networks; Knowledge distillation; Origin destination; Package arrival time prediction; Trajectory data minings; Distillation
X-distribution: Retraceable Power-law Exponent of Complex Networks,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189485060&doi=10.1145%2f3639413&partnerID=40&md5=5ab3bc430d0b9c6417f73e3647acf1b3,"Network modeling has been explored extensively by means of theoretical analysis as well as numerical simulations for Network Reconstruction (NR). The network reconstruction problem requires the estimation of the power-law exponent (Λ) of a given input network. Thus, the effectiveness of the NR solution depends on the accuracy of the calculation of Λ. In this article, we re-examine the degree distribution-based estimation of Λ, which is not very accurate due to approximations. We propose X-distribution, which is more accurate than degree distribution. Various state-of-the-art network models, including CPM, NRM, RefOrCite2, BA, CDPAM, and DMS, are considered for simulation purposes, and simulated results support the proposed claim. Further, we apply X-distribution over several real-world networks to calculate their power-law exponents, which differ from those calculated using respective degree distributions. It is observed that X-distributions exhibit more linearity (straight line) on the log-log scale than degree distributions. Thus, X-distribution is more suitable for the evaluation of power-law exponent using linear fitting (on the log-log scale). The MATLAB implementation of power-law exponent (Λ) calculation using X-distribution for different network models and the real-world datasets used in our experiments are available at https://github.com/Aikta-Arya/X-distribution-Retraceable-Power-Law-Exponent-of-Complex-Networks.git.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Degree distribution; network modeling; network reconstruction; power law; scale-free networks; X-distribution,MATLAB; Degree distributions; Log log scale; Network models; Network reconstruction; Power-law; Power-law exponents; Reconstruction problems; Scale free networks; State of the art; X-distribution; Complex networks
CoBjeason: Reasoning Covered Object in Image by Multi-Agent Collaboration Based on Informed Knowledge Graph,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189666946&doi=10.1145%2f3643565&partnerID=40&md5=b830a854a9ee91d0541c3b4f08ca060d,"Object detection is a widely studied problem in existing works. However, in this paper, we turn to a more challenging problem of “Covered Object Reasoning”, aimed at reasoning the category label of target object in the given image particularly when it has been totally covered (or invisible). To resolve this problem, we propose CoBjeason to seize the opportunity when visual reasoning meets the knowledge graph, where “empirical cognition” on common visual contexts have been incorporated as knowledge graph to conduct reinforced multi-hop reasoning via two collaborative agents. Such two agents, for one thing, stand at the covered object (or unknown entity) to observe the surrounding visual cues in the given image and gradually select entities and relations from the global gallery-level knowledge graph which contains entity-pairs frequently occurring across the entire image-collection, so as to infer the main structure of image-level knowledge graph forward expanded from the unknown entity. In turn, for another, based on the reasoned image-level knowledge graph, the semantic context among entities will be aggregated backward into unknown entity to select an appropriate entity from the global gallery-level knowledge graph as the reasoning result. Moreover, such two agents will collaborate with each other, securing that the above Forward & Backward Reasoning will step towards the same destination of the higher performance on covered object reasoning. To our best knowledge, this is the first work on Covered Object Reasoning with Knowledge Graphs and reinforced Multi-Agent collaboration. Particularly, our study on Covered Object Reasoning and the proposed model CoBjeason could offer novel insights into more basic Computer Vision (CV) tasks, such as Semantic Segmentation with better understanding on the current scene when some objects are blurred or covered, Visual Question Answering with enhancement on the inference in more complicated visual context when some objects are covered or invisible, and Image Caption Generation with the augmentation on the richness of visual context for images containing partially visible objects. The improvement on the above basic CV tasks can further refine more complicated ones involved with nuanced visual interpretation like Autonomous Driving, where the recognition and reasoning on partially visible or covered object are critical. According to the experimental results, our proposed CoBjeason can achieve the best overall ranking performance on covered object reasoning compared with other models, meanwhile enjoying the advantage of lower “exploration cost”, with the insensitivity against the long-tail covered objects and the acceptable time complexity. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Covered object reasoning; multi-agent reinforcement learning; multi-hop knowledge graph reasoning; visual reasoning,Image enhancement; Multi agent systems; Object detection; Reinforcement; Reinforcement learning; Semantic Segmentation; Semantic Web; Semantics; Agent collaboration; Covered object reasoning; Knowledge graphs; Multi agent; Multi-agent reinforcement learning; Multi-hop knowledge graph reasoning; Multi-hops; Unknown entities; Visual context; Visual reasoning; Knowledge graph
Attacking Click-through Rate Predictors via Generating Realistic Fake Samples,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189694073&doi=10.1145%2f3643685&partnerID=40&md5=8bdc99fdc543bbce7b306aa765f5e0cb,"How to construct imperceptible (realistic) fake samples is critical in adversarial attacks. Due to the sample feature diversity of a recommender system (containing both discrete and continuous features), traditional gradient-based adversarial attack methods may fail to construct realistic fake samples. Meanwhile, most recommendation models adopt click-through rate (CTR) predictors, which usually utilize black-box deep models with discrete features as input. Thus, how to efficiently construct realistic fake samples for black-box recommender systems is still full of challenges. In this article, we propose a hierarchical adversarial attack method against black-box CTR models via generating realistic fake samples, named CTRAttack. To better train the generation network, the weights of its embedding layer are shared with those of the substitute model, with both the similarity loss and classification loss used to update the generation network. To ensure that the discrete features of the generated fake samples are all real, we first adopt the similarity loss to ensure that the distribution of the generated perturbed samples is sufficiently close to the distribution of the real features, and then the nearest neighbor algorithm is used to retrieve the most appropriate features for non-existent discrete features from the candidate instance set. Extensive experiments demonstrate that CTRAttack can not only effectively attack the black-box recommender systems but also improve the robustness of these models while maintaining prediction accuracy. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial attack; deep learning; Recommender system; robustness,Deep learning; Fake detection; Adversarial attack; Attack methods; Black boxes; Clickthrough rates (CTR); Continuous features; Deep learning; Gradient based; Rate models; Robustness; Sample features; Recommender systems
EffCause: Discover Dynamic Causal Relationships Efficiently from Time-Series,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189490871&doi=10.1145%2f3640818&partnerID=40&md5=60eb19d541bf9350dee75e1fcda8a3f9,"Since the proposal of Granger causality, many researchers have followed the idea and developed extensions to the original algorithm. The classic Granger causality test aims to detect the existence of the static causal relationship. Notably, a fundamental assumption underlying most previous studies is the stationarity of causality, which requires the causality between variables to keep stable. However, this study argues that it is easy to break in real-world scenarios. Fortunately, our paper presents an essential observation: if we consider a sufficiently short window when discovering the rapidly changing causalities, they will keep approximately static and thus can be detected using the static way correctly. In light of this, we develop EffCause, bringing dynamics into classic Granger causality. Specifically, to efficiently examine the causalities on different sliding window lengths, we design two optimization schemes in EffCause and demonstrate the advantage of EffCause through extensive experiments on both simulated and real-world datasets. The results validate that EffCause achieves state-of-the-art accuracy in continuous causal discovery tasks while achieving faster computation. Case studies from cloud system failure analysis and traffic flow monitoring show that EffCause effectively helps us understand real-world time-series data and solve practical problems. © 2024 Copyright held by the owner/author(s).",causal discovery; dynamic causal relationship estimation; Granger causality; optimization,Statistical tests; Time series; Causal discovery; Causal relationships; Dynamic causal relationship estimation; Granger Causality; Granger causality test; Optimisations; Original algorithms; Real-world scenario; Stationarity; Times series; Time series analysis
Prerequisite-Enhanced Category-Aware Graph Neural Networks for Course Recommendation,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189677906&doi=10.1145%2f3643644&partnerID=40&md5=2377f684b104e35fbce40212eacbd964,"The rapid development of Massive Open Online Courses (MOOCs) platforms has created an urgent need for an efficient personalized course recommender system that can assist learners of all backgrounds and levels of knowledge in selecting appropriate courses. Currently, most existing methods utilize a sequential recommendation paradigm that captures the user’s learning interests from their learning history, typically through recurrent or graph neural networks. However, fewer studies have explored how to incorporate principles of human learning at both the course and category levels to enhance course recommendations. In this article, we aim at addressing this gap by introducing a novel model, named Prerequisite-Enhanced Catory-Aware Graph Neural Network (PCGNN), for course recommendation. Specifically, we first construct a course prerequisite graph that reflects the human learning principles and further pre-train the course prerequisite relationships as the base embeddings for courses and categories. Then, to capture the user’s complex learning patterns, we build an item graph and a category graph from the user’s historical learning records, respectively: (1) the item graph reflects the course-level local learning transition patterns and (2) the category graph provides insight into the user’s long-term learning interest. Correspondingly, we propose a user interest encoder that employs a gated graph neural network to learn the course-level user interest embedding and design a category transition pattern encoder that utilizes GRU to yield the category-level user interest embedding. Finally, the two fine-grained user interest embeddings are fused to achieve precise course prediction. Extensive experiments on two real-world datasets demonstrate the effectiveness of PCGNN compared with other state-of-the-art methods. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Course recommendation; graph neural network; MOOCs; prerequisite relationship,Embeddings; Graph neural networks; Recurrent neural networks; Signal encoding; Complex learning; Course recommendation; Embeddings; Graph neural networks; Human learning; Massive open online course; Personalized course; Prerequisite relationship; Transition patterns; Users' interests; Curricula
Graph Time-series Modeling in Deep Learning: A Survey,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189437086&doi=10.1145%2f3638534&partnerID=40&md5=0fa780056b55012513f2321bbcc8cd4a,"Time-series and graphs have been extensively studied for their ubiquitous existence in numerous domains. Both topics have been separately explored in the field of deep learning. For time-series modeling, recurrent neural networks or convolutional neural networks model the relations between values across timesteps, while for graph modeling, graph neural networks model the inter-relations between nodes. Recent research in deep learning requires simultaneous modeling for time-series and graphs when both representations are present. For example, both types of modeling are necessary for time-series classification, regression, and anomaly detection in graphs. This article aims to provide a comprehensive summary of these models, which we call graph time-series models. To the best of our knowledge, this is the first survey article that provides a picture of related models from the perspective of deep graph time-series modeling to address a range of time-series tasks, including regression, classification, and anomaly detection. Graph time-series models are split into two categories: (a) graph recurrent/convolutional neural networks and (b) graph attention neural networks. Under each category, we further categorize models based on their properties. Additionally, we compare representative models and discuss how distinctive model characteristics are utilized with respect to various model components and data challenges. Pointers to commonly used datasets and code are included to facilitate access for further research. In the end, we discuss potential directions for future research.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",evolving graphs; neural networks; spatial-temporal networks; time-series modeling,Anomaly detection; Graph neural networks; Graph theory; Graphic methods; Learning systems; Neural network models; Recurrent neural networks; Anomaly detection; Convolutional neural network; Evolving graphs; Neural network model; Neural-networks; Spatial temporals; Spatial-temporal network; Temporal networks; Times series; Times series models; Time series
Rationalizing Graph Neural Networks with Data Augmentation,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185726255&doi=10.1145%2f3638781&partnerID=40&md5=f268572cb5ef28d3b585db1634650f73,"Graph rationales are representative subgraph structures that best explain and support the graph neural network (GNN) predictions. Graph rationalization involves the joint identification of these subgraphs during GNN training, resulting in improved interpretability and generalization. GNN is widely used for node-level tasks such as paper classification and graph-level tasks such as molecular property prediction. However, on both levels, little attention has been given to GNN rationalization and the lack of training examples makes it difficult to identify the optimal graph rationales. In this work, we address the problem by proposing a unified data augmentation framework with two novel operations on environment subgraphs to rationalize GNN prediction. We define the environment subgraph as the remaining subgraph after rationale identification and separation. The framework efficiently performs rationale-environment separation in the representation space for a node's neighborhood graph or a graph's complete structure to avoid the high complexity of explicit graph decoding and encoding. We conduct experiments on 17 datasets spanning node classification, graph classification, and graph regression. Results demonstrate that our framework is effective and efficient in rationalizing and enhancing GNNs for different levels of tasks on graphs. © 2024 Copyright held by the owner/author(s)",data augmentation; Graph neural network; graph property prediction; node classification; rationalization,Data mining; Forecasting; Graph neural networks; Graph theory; Data augmentation; Graph neural networks; Graph properties; Graph property prediction; Joint identifications; Neural network predictions; Node classification; Property predictions; Rationalisation; Subgraphs; Classification (of information)
Traceable Group-Wise Self-Optimizing Feature Transformation Learning: A Dual Optimization Perspective,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185810108&doi=10.1145%2f3638059&partnerID=40&md5=c148d2caa409d6e09b1440c41b771c90,"Feature transformation aims to reconstruct an effective representation space by mathematically refining the existing features. It serves as a pivotal approach to combat the curse of dimensionality, enhance model generalization, mitigate data sparsity, and extend the applicability of classical models. Existing research predominantly focuses on domain knowledge-based feature engineering or learning latent representations. However, these methods, while insightful, lack full automation and fail to yield a traceable and optimal representation space. An indispensable question arises: Can we concurrently address these limitations when reconstructing a feature space for a machine learning task? Our initial work took a pioneering step towards this challenge by introducing a novel self-optimizing framework. This framework leverages the power of three cascading reinforced agents to automatically select candidate features and operations for generating improved feature transformation combinations. Despite the impressive strides made, there was room for enhancing its effectiveness and generalization capability. In this extended journal version, we advance our initial work from two distinct yet interconnected perspectives: 1) We propose a refinement of the original framework, which integrates a graph-based state representation method to capture the feature interactions more effectively and develop different Q-learning strategies to alleviate Q-value overestimation further. 2) We utilize a new optimization technique (actor-critic) to train the entire self-optimizing framework in order to accelerate the model convergence and improve the feature transformation performance. Finally, to validate the improved effectiveness and generalization capability of our framework, we perform extensive experiments and conduct comprehensive analyses. These provide empirical evidence of the strides made in this journal version over the initial work, solidifying our framework's standing as a substantial contribution to the field of automated feature transformation. To improve the reproducibility, we have released the associated code and data by the Github link https://github.com/coco11563/TKDD2023_code. Copyright © 2024 held by the owner/author(s)",automated feature engineering; Feature transformation; multi-agent reinforcement learning,Automation; Graphic methods; Learning systems; Multi agent systems; Reinforcement learning; Automated feature engineering; Automated features; Curse of dimensionality; Dual optimizations; Feature engineerings; Feature transformations; Generalization capability; Multi-agent reinforcement learning; Representation space; Self-optimizing; Domain Knowledge
PU-Detector: A PU Learning-based Framework for Real Money Trading Detection in MMORPG,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185714284&doi=10.1145%2f3638561&partnerID=40&md5=758e40947318f5e082fc051e6f60b654,"Massive multiplayer online role-playing games (MMORPG) have been becoming one of the most popular and exciting online games. In recent years, a cheating phenomenon called real money trading (RMT) has arisen and damaged the fantasy world in many ways. RMT is the sale of in-game items, currency, or even characters to earn real money, breaking the balance of the game economy ecosystem and damaging the game experience. Therefore, some studies have emerged to address the problem of RMT detection. However, they cannot well handle the label uncertainty problem in practice, where there are only labeled RMT samples (positive samples) and unlabeled samples, which could either be RMT samples or normal transactions (negative samples). Meanwhile, the trading relationship between RMTers is modeled in a simple way, leading to some normal transactions being falsely classified as RMT. In this article, we propose PU-Detector, a novel framework based on PU learning (learning from positive and unlabeled data) for RMT detection, considering the fact that there are only labeled RMT samples and other unlabeled transactions. We first automatically estimate the likelihood of one transaction being RMT by developing an improved PU learning method and proposing an assessment rule. Sequentially, we use the estimated likelihood as edge weight to construct a trading graph to learn trader representation. Then, with the trader representations and basic trading features, we detect RMT samples by the improved PU learning method. PU-Detector is evaluated on a large-scale real world dataset consisting of 33, 809, 956 transaction logs generated by 43, 217 unique players. Compared with other approaches, it achieves the state-of-the-art performance and demonstrates its advantages in detecting underlying RMT samples. Copyright © 2024 held by the owner/author(s)",community detection; graph representation learning; online games; positive and unlabeled learning; Real money trading detection,Commerce; E-learning; Large datasets; Social networking (online); Breakings; Community detection; Graph representation; Graph representation learning; Learning methods; Massive multiplayer online role playing games; On-line games; Positive and unlabeled learning; Real money trading; Real money trading detection; Learning systems
A Semantics-enhanced Topic Modelling Technique: Semantic-LDA,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185721645&doi=10.1145%2f3639409&partnerID=40&md5=ed2cf6b45ba832ac7e8afbc96bf5f1c2,"Topic modelling is a beneficial technique used to discover latent topics in text collections. But to correctly understand the text content and generate a meaningful topic list, semantics are important. By ignoring semantics, that is, not attempting to grasp the meaning of the words, most of the existing topic modelling approaches can generate some meaningless topic words. Even existing semantic-based approaches usually interpret the meanings of words without considering the context and related words. In this article, we introduce a semantic-based topic model called semantic-LDA that captures the semantics of words in a text collection using concepts from an external ontology. A new method is introduced to identify and quantify the concept-word relationships based on matching words from the input text collection with concepts from an ontology without using pre-calculated values from the ontology that quantify the relationships between the words and concepts. These pre-calculated values may not reflect the actual relationships between words and concepts for the input collection, because they are derived from datasets used to build the ontology rather than from the input collection itself. Instead, quantifying the relationship based on the word distribution in the input collection is more realistic and beneficial in the semantic capture process. Furthermore, an ambiguity handling mechanism is introduced to interpret the unmatched words, that is, words for which there are no matching concepts in the ontology. Thus, this article makes a significant contribution by introducing a semantic-based topic model that calculates the word-concept relationships directly from the input text collection. The proposed semantic-based topic model and an enhanced version with the disambiguation mechanism were evaluated against a set of state-of-the-art systems, and our approaches outperformed the baseline systems in both topic quality and information filtering evaluations. © 2024 Copyright held by the owner/author(s)",concepts; disambiguation; semantics; Topic modelling,Information filtering; Ontology; Quality control; Calculated values; Concept; Disambiguation; Matchings; Modeling approach; Modelling techniques; Ontology's; Text collection; Text content; Topic Modeling; Semantics
Measuring and Mitigating Gender Bias in Legal Contextualized Language Models,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185776879&doi=10.1145%2f3628602&partnerID=40&md5=9d4cf966986ebfbd58ad36a7617a066f,"Transformer-based contextualized language models constitute the state-of-the-art in several natural language processing (NLP) tasks and applications. Despite their utility, contextualized models can contain human-like social biases, as their training corpora generally consist of human-generated text. Evaluating and removing social biases in NLP models has been a major research endeavor. In parallel, NLP approaches in the legal domain, namely, legal NLP or computational law, have also been increasing. Eliminating unwanted bias in legal NLP is crucial, since the law has the utmost importance and effect on people. In this work, we focus on the gender bias encoded in BERT-based models. We propose a new template-based bias measurement method with a new bias evaluation corpus using crime words from the FBI database. This method quantifies the gender bias present in BERT-based models for legal applications. Furthermore, we propose a new fine-tuning-based debiasing method using the European Court of Human Rights (ECtHR) corpus to debias legal pre-trained models. We test the debiased models' language understanding performance on the LexGLUE benchmark to confirm that the underlying semantic vector space is not perturbed during the debiasing process. Finally, we propose a bias penalty for the performance scores to emphasize the effect of gender bias on model performance. © 2024 Copyright held by the owner/author(s)",BERT; contextualized models; gender bias; Legal NLP; LegalBERT,Benchmarking; Computational linguistics; Natural language processing systems; Semantics; BERT; Contextualized model; De-biasing; Gender bias; Language model; Language processing; Legal natural language processing; LegalBERT; Natural languages; Performance; Vector spaces
Information-aware Multi-view Outlier Detection,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185714568&doi=10.1145%2f3638354&partnerID=40&md5=0e99d86c59b6c604c75563c4d55a6c14,"With the development of multi-view learning, multi-view outlier detection has received increasing attention in recent years. However, the current research still faces two challenges: (1) The current research lacks theoretical analysis tools for multi-view outliers. (2) Most current multi-view outlier detection algorithms are based on shallow structural assumptions of the data, such as cluster assumptions and subspace assumptions, thus they are not suitable for more complex data distributions. In addressing these two issues, this article proposes three occurrence mechanisms of multi-view outlier, which serve as foundational theoretical analysis tools for multi-view outliers. Utilizing proposed mechanisms, we analyze the impact of multi-view outliers and the information structure of multi-view data and validate our findings through experiments. Finally, we propose a novel algorithm referred to as Information-Aware Multi-View Outlier Detection (IAMOD). In contrast to other methods, IAMOD focuses on the information structure of multi-view data without relying on shallow structural assumptions. By learning a compact representation of the sample that is semantically rich and non-redundant, IAMOD can accurately identify multi-view outliers by comparing the consistency of the representations' neighbors and views. Extensive experimental results demonstrate that our approach outperforms several state-of-the-art multi-view outlier detection methods. Copyright © 2024 held by the owner/author(s)",information theory; multi-view learning; Outlier detection,Anomaly detection; Clustering algorithms; Statistics; 'current; Analysis tools; Information structures; Information-aware; Multi-view datum; Multi-view learning; Multi-views; Outlier Detection; Outlier detection algorithm; Structural assumption; Information theory
Concept Drift Adaptation by Exploiting Drift Type,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185782653&doi=10.1145%2f3638777&partnerID=40&md5=1cee63209d2a464e3fca3efdbb6cad5d,"Concept drift is a phenomenon where the distribution of data streams changes over time. When this happens, model predictions become less accurate. Hence, models built in the past need to be re-learned for the current data. Two design questions need to be addressed in designing a strategy to re-learn models: which type of concept drift has occurred, and how to utilize the drift type to improve re-learning performance. Existing drift detection methods are often good at determining when drift has occurred. However, few retrieve information about how the drift came to be present in the stream. Hence, determining the impact of the type of drift on adaptation is difficult. Filling this gap, we designed a framework based on a lazy strategy called Type-Driven Lazy Drift Adaptor (Type-LDA). Type-LDA first retrieves information about both how and when a drift has occurred, then it uses this information to re-learn the new model. To identify the type of drift, a drift type identifier is pre-trained on synthetic data of known drift types. Furthermore, a drift point locator locates the optimal point of drift via a sharing loss. Hence, Type-LDA can select the optimal point, according to the drift type, to re-learn the new model. Experiments validate Type-LDA on both synthetic data and real-world data, and the results show that accurately identifying drift type can improve adaptation accuracy. Copyright © 2024 held by the owner/author(s)",Concept drift; data streams; drift adaptation; drift detection,Change-over time; Concept drifts; Current data; Data stream; Drift adaptation; Drift detection; Learn+; Model prediction; Optimal points; Synthetic data
Distributional Learning for Network Alignment with Global Constraints,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185723000&doi=10.1145%2f3638056&partnerID=40&md5=6967f32a5a13c3607779e365d912ebea,"Network alignment, pairing corresponding nodes across the source and target networks, plays an important role in many data mining tasks. Extensive studies focus on learning node embeddings across different networks in a unified space. However, these methods have not taken the large structural discrepancy between aligned nodes into account and, thus, are largely confined by the deterministic representations of nodes. In this work, we propose a novel network alignment framework highlighted by distributional learning and globally optimal alignment. By modeling the uncertainty of each node by Gaussian distribution, our framework builds similarity matrices on the Wasserstein distance between distributions and applies Sinkhorn operation, which learns the globally optimal mapping in an end-to-end fashion. We show that each integrated part of the framework contributes to the overall performance. Under a variety of experimental settings, our alignment framework shows superior accuracy and efficiency to the state-of-the-art. Copyright © 2024 held by the owner/author(s)",distributional learning; Network alignment,Data mining; Uncertainty analysis; Data mining tasks; Deterministics; Distributional learning; Embeddings; Global constraints; Network alignments; Optimal alignments; Similarity matrix; Uncertainty; Wasserstein distance; Alignment
Robust Graph Meta-Learning for Weakly Supervised Few-Shot Node Classification,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185723757&doi=10.1145%2f3630260&partnerID=40&md5=3a2adde91c82c220f0ca0f982c4c5364,"Graph machine learning (Graph ML) models typically require abundant labeled instances to provide sufficient supervision signals, which is commonly infeasible in real-world scenarios since labeled data for newly emerged concepts (e.g., new categorizations of nodes) on graphs is rather limited. To efficiently learn with a small amount of data on graphs, meta-learning has been investigated in Graph ML. By transferring the knowledge learned from previous experiences to new tasks, graph meta-learning approaches have demonstrated promising performance on few-shot graph learning problems. However, most existing efforts predominately assume that all the data from the seen classes is gold labeled, yet those methods may lose their efficacy when the seen data is weakly labeled with severe label noise. As such, we aim to investigate a novel problem of weakly supervised graph meta-learning for improving the model robustness in terms of knowledge transfer. To achieve this goal, we propose Meta-GIN (Meta Graph Interpolation Network), a new graph meta-learning framework. Based on a new robustness-enhanced episodic training paradigm, Meta-GIN is meta-learned to interpolate node representations from weakly labeled data and extracts highly transferable meta-knowledge, which enables the model to quickly adapt to unseen tasks with few labeled instances. Extensive experiments demonstrate the superiority of Meta-GIN over existing graph meta-learning studies on the task of weakly supervised few-shot node classification. Copyright © 2024 held by the owner/author(s)",few-shot learning; Graph neural networks; noisy labels; weak supervision,Data mining; Graph theory; Knowledge management; Learning systems; Few-shot learning; Graph machine; Graph neural networks; Labeled data; Machine learning models; Meta-graph; Metalearning; Noisy labels; Robust graphs; Weak supervision; Graph neural networks
Diverse Structure-Aware Relation Representation in Cross-Lingual Entity Alignment,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185704998&doi=10.1145%2f3638778&partnerID=40&md5=71fc18c35ba3cd5e831fd4aa47f31bc3,"Cross-lingual entity alignment (CLEA) aims to find equivalent entity pairs between knowledge graphs (KGs) in different languages. It is an important way to connect heterogeneous KGs and facilitate knowledge completion. Existing methods have found that incorporating relations into entities can effectively improve KG representation and benefit entity alignment, and these methods learn relation representation depending on entities, which cannot capture the diverse structures of relations. However, multiple relations in KG form diverse structures, such as adjacency structure and ring structure. This diversity of relation structures makes the relation representation challenging. Therefore, we propose to construct the weighted line graphs to model the diverse structures of relations and learn relation representation independently from entities. Especially, owing to the diversity of adjacency structures and ring structures, we propose to construct adjacency line graph and ring line graph, respectively, to model the structures of relations and to further improve entity representation. In addition, to alleviate the hubness problem in alignment, we introduce the optimal transport into alignment and compute the distance matrix in a different way. From a global perspective, we calculate the optimal 1-to-1 alignment bi-directionally to improve the alignment accuracy. Experimental results on two benchmark datasets show that our proposed method significantly outperforms state-of-the-art CLEA methods in both supervised and unsupervised manners. Copyright © 2024 held by the owner/author(s)",Cross-lingual entity alignment; knowledge graph; line graph; optimal transport,Graph theory; Structural optimization; Cross-lingual; Cross-lingual entity alignment; Graph representation; Heterogeneous Knowledge; Knowledge graphs; Learn+; Linegraph; Optimal transport; Rings structure; Structure-aware; Knowledge graph
HITS-based Propagation Paradigm for Graph Neural Networks,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185819935&doi=10.1145%2f3638779&partnerID=40&md5=d8a351aa0e9ec481ed6545d603bdeb54,"In this article, we present a new propagation paradigm based on the principle of Hyperlink-Induced Topic Search (HITS) algorithm. The HITS algorithm utilizes the concept of a “self-reinforcing” relationship of authority-hub. Using HITS, the centrality of nodes is determined via repeated updates of authority-hub scores that converge to a stationary distribution. Unlike PageRank-based propagation methods, which rely solely on the idea of authorities (in-links), HITS considers the relevance of both authorities (in-links) and hubs (out-links), thereby allowing for a more informative graph learning process. To segregate node prediction and propagation, we use a Multilayer Perceptron in combination with a HITS-based propagation approach and propose two models: HITS-GNN and HITS-GNN+. We provided additional validation of our models' efficacy by performing an ablation study to assess the performance of authority-hub in independent models. Moreover, the effect of the main hyper-parameters and normalization is also analyzed to uncover how these techniques influence the performance of our models. Extensive experimental results indicate that the proposed approach significantly improves baseline methods on the graph (citation network) benchmark datasets by a decent margin for semi-supervised node classification, which can aid in predicting the categories (labels) of scientific articles not exclusively based on their content but also based on the type of articles they cite. Copyright © 2024 held by the owner/author(s)",citation networks; Graph Neural Networks (GNNs); Hyperlink-Induced Topic Search (HITS); node classification; semi-supervised learning,Backpropagation; Graph neural networks; Graph theory; Hypertext systems; Information retrieval; Citation networks; Graph neural network; Graph neural networks; Hyperlink-induced topic search; Hyperlinks; Node classification; Performance; Search Algorithms; Search-based; Semi-supervised learning; Classification (of information)
ArieL: Adversarial Graph Contrastive Learning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185728149&doi=10.1145%2f3638054&partnerID=40&md5=db3a573d7f95889a7409c547ed5c19d6,"Contrastive learning is an effective unsupervised method in graph representation learning. The key component of contrastive learning lies in the construction of positive and negative samples. Previous methods usually utilize the proximity of nodes in the graph as the principle. Recently, the data-augmentation-based contrastive learning method has advanced to show great power in the visual domain, and some works have extended this method from images to graphs. However, unlike the data augmentation on images, the data augmentation on graphs is far less intuitive and it is much harder to provide high-quality contrastive samples, which leaves much space for improvement. In this work, by introducing an adversarial graph view for data augmentation, we propose a simple but effective method, Adversarial Graph Contrastive Learning (ArieL), to extract informative contrastive samples within reasonable constraints. We develop a new technique called information regularization for stable training and use subgraph sampling for scalability. We generalize our method from node-level contrastive learning to the graph level by treating each graph instance as a super-node. ArieL consistently outperforms the current graph contrastive learning methods for both node-level and graph-level classification tasks on real-world datasets. We further demonstrate that ArieL is more robust in the face of adversarial attacks. Copyright © 2024 held by the owner/author(s)",adversarial training; contrastive learning; Graph representation learning; mutual information,Data mining; Graph theory; Graphic methods; Image enhancement; Learning systems; Adversarial training; Contrastive learning; Data augmentation; Graph representation; Graph representation learning; Learning methods; Mutual informations; Negative samples; Power; Unsupervised method; Classification (of information)
Personalized Federated Learning with Layer-Wise Feature Transformation via Meta-Learning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185716169&doi=10.1145%2f3638252&partnerID=40&md5=1a86f72b39a0cefb1a9be1f2bc10aaf6,"Federated learning enables multiple clients to collaboratively learn machine learning models in a privacy-preserving manner. However, in real-world scenarios, a key challenge encountered in federated learning is the statistical heterogeneity among clients. Existing work mainly focused on a single global model shared across the clients, making it hard to generalize well to all clients due to the large discrepancy in the data distributions. To address this challenge, we propose pFedLT, a novel approach that can adapt the single global model to different data distributions. Specifically, we propose to perform a pluggable layer-wise transformation during the local update phase based on scaling and shifting operations. In particular, these operations are learned with a meta-learning strategy. By doing so, pFedLT can capture the diversity of data distribution among clients, therefore, can generalize well even when the data distributions among clients exhibit high statistical heterogeneity. We conduct extensive experiments on synthetic and real-world datasets (MNIST, Fashion_MNIST, CIFAR-10, and Office+Caltech10) under different Non-IID settings. Experimental results demonstrate that pFedLT significantly improves the model accuracy by up to 11.67% and reduces the communication costs compared with state-of-the-art approaches. Copyright © 2024 held by the owner/author(s)",edge computing; federated learning; Meta-learning; personalized learning,Learning systems; Privacy-preserving techniques; Data distribution; Edge computing; Feature transformations; Federated learning; Global models; Layer-wise; Metalearning; Multiple clients; Personalized learning; Statistical heterogeneities; Edge computing
"A Lightweight, Effective, and Efficient Model for Label Aggregation in Crowdsourcing",2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185802822&doi=10.1145%2f3630102&partnerID=40&md5=6148bd5381f2ade58ac30e2140bf05ac,"Due to the presence of noise in crowdsourced labels, label aggregation (LA) has become a standard procedure for post-processing these labels. LA methods estimate true labels from crowdsourced labels by modeling worker quality. However, most existing LA methods are iterative in nature. They require multiple passes through all crowdsourced labels, jointly and iteratively updating true labels and worker qualities until a termination condition is met. As a result, these methods are burdened with high space and time complexities, which restrict their applicability in scenarios where scalability and online aggregation are essential. Furthermore, defining a suitable termination condition for iterative algorithms can be challenging. In this article, we view LA as a dynamic system and represent it as a Dynamic Bayesian Network. From this dynamic model, we derive two lightweight and scalable algorithms: LAonepass and LAtwopass. These algorithms can efficiently and effectively estimate worker qualities and true labels by traversing all labels at most twice, thereby eliminating the need for explicit termination conditions and multiple traversals over the crowdsourced labels. Due to their dynamic nature, the proposed algorithms are also capable of performing label aggregation online. We provide theoretical proof of the convergence property of the proposed algorithms and bound the error of the estimated worker qualities. Furthermore, we analyze the space and time complexities of our proposed algorithms, demonstrating their equivalence to those of majority voting. Through experiments conducted on 20 real-world datasets, we demonstrate that our proposed algorithms can effectively and efficiently aggregate labels in both offline and online settings, even though they traverse all labels at most twice. The code is on https://github.com/yyang318/LA_onepass. © 2024 Copyright held by the owner/author(s)",bayesian networks; Crowdsourcing; online truth inference; truth inference,Crowdsourcing; Iterative methods; Aggregation methods; Bayesia n networks; Label aggregation; Online truth inference; Post-processing; Space and time complexity; Standard procedures; Termination condition; Truth inference; Workers'; Bayesian networks
Exploring the Learning Difficulty of Data: Theory and Measure,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185712641&doi=10.1145%2f3636512&partnerID=40&md5=614b9c4beb2fe31f79ea335d248a187c,"“Easy/hard sample” is a popular parlance in machine learning. Learning difficulty of samples refers to how easy/hard a sample is during a learning procedure. An increasing need of measuring learning difficulty demonstrates its importance in machine learning (e.g., difficulty-based weighting learning strategies). Previous literature has proposed a number of learning difficulty measures. However, no comprehensive investigation for learning difficulty is available to date, resulting in that nearly all existing measures are heuristically defined without a rigorous theoretical foundation. This study attempts to conduct a pilot theoretical study for learning difficulty of samples. First, influential factors for learning difficulty are summarized. Under various situations conducted by summarized influential factors, correlations between learning difficulty and two vital criteria of machine learning, namely, generalization error and model complexity, are revealed. Second, a theoretical definition of learning difficulty is proposed on the basis of these two criteria. A practical measure of learning difficulty is proposed under the direction of the theoretical definition by importing the bias-variance trade-off theory. Subsequently, the rationality of theoretical definition and the practical measure are demonstrated, respectively, by analysis of several classical weighting methods and abundant experiments realized under all situations conducted by summarized influential factors. The mentioned weighting methods can be reasonably explained under the proposed theoretical definition and concerned propositions. The comparison in these experiments indicates that the proposed measure significantly outperforms the other measures throughout the experiments. Copyright © 2024 held by the owner/author(s)",bias-variance trade-off; generalization error; Learning difficulty; model complexity,Economic and social effects; Bias variance trade off; Data theories; Generalization Error; Influential factors; Learning difficulties; Learning procedures; Machine-learning; Modeling complexity; Practical measures; Weighting methods; Machine learning
diGRASS: Directed Graph Spectral Sparsification via Spectrum-Preserving Symmetrization,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185721167&doi=10.1145%2f3639568&partnerID=40&md5=b942060dce15f454a48b308d997774ab,"Recent spectral graph sparsification research aims to construct ultra-sparse subgraphs for preserving the original graph spectral (structural) properties, such as the first few Laplacian eigenvalues and eigenvectors, which has led to the development of a variety of nearly-linear time numerical and graph algorithms. However, there is very limited progress for spectral sparsification of directed graphs. In this work, we prove the existence of nearly-linear-sized spectral sparsifiers for directed graphs under certain conditions. Furthermore, we introduce a practically-efficient spectral algorithm (diGRASS) for sparsifying real-world, large-scale directed graphs leveraging spectral matrix perturbation analysis. The proposed method has been evaluated using a variety of directed graphs obtained from real-world applications, showing promising results for solving directed graph Laplacians, spectral partitioning of directed graphs, and approximately computing (personalized) PageRank vectors. © 2024 Copyright held by the owner/author(s)",directed graphs; laplacian solver; PageRank; spectral graph sparsification; Spectral graph theory,Computation theory; Eigenvalues and eigenfunctions; Graphic methods; Laplace transforms; Perturbation techniques; Graph sparsification; Laplacian solv; Laplacians; Page ranks; Real-world; Sparse subgraphs; Sparsification; Spectra's; Spectral graph sparsification; Spectral graph theory; Directed graphs
Cross-modal Multiple Granularity Interactive Fusion Network for Long Document Classification,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185729209&doi=10.1145%2f3631711&partnerID=40&md5=e61e2f7766022cf74cff1e9e4626e974,"Long Document Classification (LDC) has attracted great attention in Natural Language Processing and achieved considerable progress owing to the large-scale pre-trained language models. In spite of this, as a different problem from the traditional text classification, LDC is far from being settled. Long documents, such as news and articles, generally have more than thousands of words with complex structures. Moreover, compared with flat text, long documents usually contain multi-modal content of images, which provide rich information but not yet being utilized for classification. In this article, we propose a novel cross-modal method for long document classification, in which multiple granularity feature shifting networks are proposed to integrate the multi-scale text and visual features of long documents adaptively. Additionally, a multi-modal collaborative pooling block is proposed to eliminate redundant fine-grained text features and simultaneously reduce the computational complexity. To verify the effectiveness of the proposed model, we conduct experiments on the Food101 dataset and two constructed multi-modal long document datasets. The experimental results show that the proposed cross-modal method outperforms the single-modal text methods and defeats the state-of-the-art related multi-modal baselines. Copyright © 2024 held by the owner/author(s)",cross-modal multi-granularity interactive fusion; Long document classification; multi-modal collaborative pooling,Complex networks; Information retrieval systems; Natural language processing systems; Text processing; Cross-modal; Cross-modal multi-granularity interactive fusion; Document Classification; Long document classification; Modal method; Multi-granularity; Multi-modal; Multi-modal collaborative pooling; Natural languages; Text feature; Classification (of information)
Multiple-instance Learning from Triplet Comparison Bags,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185815285&doi=10.1145%2f3638776&partnerID=40&md5=7f5dab1c8375458c09f6776feadc1fb0,"Multiple-instance learning (MIL) solves the problem where training instances are grouped in bags, and a binary (positive or negative) label is provided for each bag. Most of the existing MIL studies need fully labeled bags for training an effective classifier, while it could be quite hard to collect such data in many real-world scenarios, due to the high cost of data labeling process. Fortunately, unlike fully labeled data, triplet comparison data can be collected in a more accurate and human-friendly way. Therefore, in this article, we for the first time investigate MIL from only triplet comparison bags, where a triplet (Xa, Xb, Xc ) contains the weak supervision information that bag Xa is more similar to Xb than to Xc. To solve this problem, we propose to train a bag-level classifier by the empirical risk minimization framework and theoretically provide a generalization error bound. We also show that a convex formulation can be obtained only when specific convex binary losses such as the square loss and the double hinge loss are used. Extensive experiments validate that our proposed method significantly outperforms other baselines. Copyright © 2024 held by the owner/author(s)",Empirical Risk Minimization; Multi-Instance Learning; Triplet Comparison,Data labelling; Empirical risk minimization; High costs; Human-friendly; Labeled data; Learning studies; Multi-instance learning; Multiple-instance learning; Real-world scenario; Triplet comparison; Learning systems
Image Hash Layer Triggered CNN Framework for Wafer Map Failure Pattern Retrieval and Classification,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185718317&doi=10.1145%2f3638053&partnerID=40&md5=f5805751ff20b365330f99a5325a15b0,"Recently, deep learning methods are often used in wafer map failure pattern classification. CNN requires less feature engineering but still needs preprocessing, e.g., denoising and resizing. Denoising is used to improve the quality of the input data, and resizing is used to transform the input into an identical size when the input data sizes are various. However, denoising and resizing may distort the original data information. Nevertheless, CNN-based applications are focusing on studying different feature map architectures and the input data manipulation is less attractive. In this study, we proposed an image hash layer triggered CNN framework for wafer map failure pattern retrieval and classification. The motivation and novelty are to design a CNN layer that can play as a resizing, information retrieval-preservation method in one step. The experiments proved that the proposed hash layer can retrieve the failure pattern information while maintaining the classification performance even though the input data size is decreased significantly. In the meantime, it can prevent overfitting, false negatives, and false positives, and save computing costs to a certain extent. Copyright © 2024 held by the owner/author(s)",CNN; failure pattern classification; feature extraction; image hash; image transformation,Classification (of information); Image classification; Input output programs; Learning systems; De-noising; Failure pattern classification; Failure patterns; Features extraction; Image hash; Image transformations; Input datas; Pattern retrieval; Patterns classification; Wafer maps; Deep learning
Learning Global and Multi-granularity Local Representation with MLP for Sequential Recommendation,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185802415&doi=10.1145%2f3638562&partnerID=40&md5=b066481b8e2ca586ee7f1cc203cd8a43,"Sequential recommendation aims to predict the next item of interest to users based on their historical behavior data. Usually, users' global and local preferences jointly affect the final recommendation result in different ways. Most existing works use transformers to globally model sequences, which makes them face the dilemma of quadratic computational complexity when dealing with long sequences. Moreover, the scope setting of the user's local preference is usually static and single, and cannot cover richer multi-level local semantics. To this end, we proposed a parallel architecture for capturing global representation and Multi-granularity Local dependencies with MLP for sequential Recommendation (MLM4Rec). For global representation, we utilize modified MLP-Mixer to capture global information of user sequences due to its simplicity and efficiency. For local representation, we incorporate convolution into MLP and propose a multi-granularity local awareness mechanism for capturing richer local semantic information. Moreover, we introduced a weight pooling method to adaptively fuse local-global representations instead of directly concatenation. Our model has the advantages of low complexity and high efficiency thanks to its simple MLP structure. Experimental results on three public datasets demonstrate the effectiveness of our proposed model. Our code is available here. Copyright © 2024 held by the owner/author(s)",global and local; MLP; multi-granularity; Sequential recommendation,Efficiency; Semantics; Global and local; Global informations; Global representation; Local semantics; Long sequences; MLP; Multi-granularity; Multilevels; Semantics Information; Sequential recommendation; Parallel architectures
Graph-based Text Classification by Contrastive Learning with Text-level Graph Augmentation,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185724829&doi=10.1145%2f3638353&partnerID=40&md5=d7425342afb484cc6f0a4a61e676a6fb,"Text Classification (TC) is a fundamental task in the information retrieval community. Nowadays, the mainstay TC methods are built on the deep neural networks, which can learn much more discriminative text features than the traditional shallow learning methods. Among existing deep TC methods, the ones based on Graph Neural Network (GNN) have attracted more attention due to the superior performance. Technically, the GNN-based TC methods mainly transform the full training dataset to a graph of texts; however, they often neglect the dependency between words, so as to miss potential semantic information of texts, which may be significant to exactly represent them. To solve the aforementioned problem, we generate graphs of words instead, so as to capture the dependency information of words. Specifically, each text is translated into a graph of words, where neighboring words are linked. We learn the node features of words by a GNN-like procedure and then aggregate them as the graph feature to represent the current text. To further improve the text representations, we suggest a contrastive learning regularization term. Specifically, we generate two augmented text graphs for each original text graph, we constrain the representations of the two augmented graphs from the same text close and the ones from different texts far away. We propose various techniques to generate the augmented graphs. Upon those ideas, we develop a novel deep TC model, namely Text-level Graph Networks with Contrastive Learning (TGNcl). We conduct a number of experiments to evaluate the proposed TGNcl model. The empirical results demonstrate that TGNcl can outperform the existing state-ofthe-art TC models. Copyright © 2024 held by the owner/author(s)",contrastive learning; graph augmentation; graph representation; label correlation; Multi-label classification,Classification (of information); Deep neural networks; Graphic methods; Learning systems; Semantics; Text processing; Contrastive learning; Graph augmentation; Graph networks; Graph neural networks; Graph representation; Label correlations; Level graphs; Multi-label classifications; Text classification; Text classification methods; Graph neural networks
Swarm Self-supervised Hypergraph Embedding for Recommendation,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185770062&doi=10.1145%2f3638058&partnerID=40&md5=fd914526faaaec3ff574652f64fc6fa5,"The information era brings both opportunities and challenges to information services. Confronting information overload, recommendation technology is dedicated to filtering personalized content to meet users' requirements. The extremely sparse interaction records and their imbalanced distribution become a big obstacle to building a high-quality recommendation model. In this article, we propose a swarm self-supervised hypergraph embedding (SHE) model to predict users' interests by hypergraph convolution and self-supervised discrimination. SHE builds a hypergraph with multiple interest clues to alleviate the interaction sparsity issue and performs interest propagation to embed CF signals in hybrid learning on the hypergraph. It follows an auxiliary local view by similar hypergraph construction and interest propagation to restrain unnecessary propagation between user swarms. Besides, interest contrast further inserts self-discrimination to deal with long-tail bias issue and enhance interest modeling, which aid recommendation by a multi-task learning optimization. Experiments on public datasets show that the proposed SHE outperforms the state-of-the-art models demonstrating the effectiveness of hypergraph-based interest propagation and swarm-aware interest contrast to enhance embedding for recommendation. Copyright © 2024 held by the owner/author(s)",contrastive learning; graph convolution; hypergraph; Recommendation; user interest,Convolution; Information filtering; Network embeddings; Swarm intelligence; User profile; Contrastive learning; Embeddings; Graph convolution; Hyper graph; Information eras; Information overloads; Personalized content; Recommendation; Recommendation technologies; Users' interests; Information services
NNC-GCN: Neighbours-to-Neighbours Contrastive Graph Convolutional Network for Semi-Supervised Classification,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185718040&doi=10.1145%2f3638780&partnerID=40&md5=03529cdca792d42274a43bcd6e144cbe,"Contrastive learning (CL) is a popular learning paradigm in deep learning, which uses contrastive principle to learn low-dimensional embeddings, and has been applied in Graph Neural Networks (GNNs) successfully. Existing works of contrastive multi-view GNNs usually focus on point-to-point contrastive learning strategies. However, they neglect the local information in neighbors, which brings isolated positive samples. The quality of selected positive samples is hard to evaluate, and these samples may lead to invalid contrastiveness. Therefore, we propose a simple and efficient neighbors-to-neighbors contrastive graph neural network (NNC-GCN), which constructs a consistent multi-view by using the topologies of original input graphs. Moreover, we raise a new learning problem of unlabeled data base on these constructed multi-view topologies and propose a loss function NNC-InfoNCE to guide its learning process. The NNC-InfoNCE is an improved version of InfoNCE, which can be adapted to neighborhood-level contrast learning. Specifically, the neighborhoods and the remaining nodes of the selected anchor are weighted and treated as positive and negative sample sets. The experimental results show that our method is effective on public benchmark datasets. © 2024 Copyright held by the owner/author(s)",contrastive learning; Graph neural networks; multi-view graph; multimedia; node classification,Data mining; Deep learning; Learning systems; Supervised learning; Topology; Contrastive learning; Convolutional networks; Graph neural networks; Learning paradigms; Multi-view graph; Multi-views; Multimedium; Neighbourhood; Node classification; Semisupervised classification (SSC); Graph neural networks
A Novel Neural Ensemble Architecture for On-the-fly Classification of Evolving Text Streams,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185706032&doi=10.1145%2f3639054&partnerID=40&md5=27cb39f18c548473fbf89403896a5cdd,"We study on-the-fly classification of evolving text streams in which the relation between the input data and target labels changes over time-i.e., “concept drift.” These variations decrease the model's performance, as predictions become less accurate over time and they necessitate a more adaptable system. While most studies focus on concept drift detection and handling with ensemble approaches, the application of neural models in this area is relatively less studied. We introduce Adaptive Neural Ensemble Network (AdaNEN), a novel ensemble-based neural approach, capable of handling concept drift in data streams. With our novel architecture, we address some of the problems neural models face when exploited for online adaptive learning environments. Most current studies address concept drift detection and handling in numerical streams, and the evolving text stream classification remains relatively unexplored. We hypothesize that the lack of public and large-scale experimental data could be one reason. To this end, we propose a method based on an existing approach for generating evolving text streams by introducing various types of concept drifts to real-world text datasets. We provide an extensive evaluation of our proposed approach using 12 state-of-the-art baselines and 13 datasets. We first evaluate concept drift handling capability of AdaNEN and the baseline models on evolving numerical streams; this aims to demonstrate the concept drift handling capabilities of our method on a general spectrum and motivate its use in evolving text streams. The models are then evaluated in evolving text stream classification. Our experimental results show that AdaNEN consistently outperforms the existing approaches in terms of predictive performance with conservative efficiency. © 2024 Copyright held by the owner/author(s)",concept drift; Data stream mining; ensemble methods; neural networks; text stream classification,Computer aided instruction; Data handling; Data mining; Network architecture; Text processing; Concept drifts; Data streams mining; Ensemble methods; Ensemble networks; Neural ensembles; Neural modelling; Neural-networks; Stream classification; Text stream classification; Text streams; Numerical methods
Utility-aware Privacy Perturbation for Training Data,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185713950&doi=10.1145%2f3639411&partnerID=40&md5=17edd95bdb69513d4ebe94f46ad9a70c,"Data perturbation under differential privacy constraint is an important approach of protecting data privacy. However, as the data dimensions increase, the privacy budget allocated to each dimension decreases and thus the amount of noise added increases, which eventually leads to lower data utility in training tasks. To protect the privacy of training data while enhancing data utility, we propose a Utility-aware training data Privacy Perturbation scheme based on attribute Partition and budget Allocation (UPPPA). UPPPA includes three procedures: the quantification of attribute privacy and attribute importance, attribute partition, and budget allocation. The quantification of attribute privacy and attribute importance based on information entropy and attribute correlation provide an arithmetic basis for attribute partition and budget allocation. During the attribute partition, all attributes of training data are classified into high and low classes to achieve privacy amplification and utility enhancement. During the budget allocation, a γ-privacy model is proposed to balance data privacy and data utility so as to provide privacy constraint and guide budget allocation. Three comprehensive sets of real-world data are applied to evaluate the performance of UPPPA. Experiments and privacy analysis show that our scheme can achieve the tradeoff between privacy and utility. © 2024 Copyright held by the owner/author(s)",Data perturbation; Data utility; Training data privacy,Budget control; Attribute importance; Budget allocation; Data dimensions; Data perturbation; Data utilities; Differential privacies; Partition allocation; Privacy constraints; Training data; Training data privacy; Data privacy
Enhancing Heterogeneous Knowledge Graph Completion with a Novel GAT-based Approach,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185814334&doi=10.1145%2f3639472&partnerID=40&md5=fcbccb24698b1a47827e7e3dc4d94a8d,"Knowledge graphs (KGs) play a vital role in enhancing search results and recommendation systems. With the rapid increase in the size of KGs, they are becoming inaccurate and incomplete. This problem can be solved by the KG completion methods, of which graph attention network (GAT)-based methods stand out because of their superior performance. However, existing GAT-based KG completion methods often suffer from overfitting issues when dealing with heterogeneous KGs, primarily due to the unbalanced number of samples. Additionally, these methods demonstrate poor performance in predicting the tail (head) entity that shares the same relation and head (tail) entity with others. To solve these problems, we propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH incorporates two separate attention network modules that work synergistically to predict the missing entities. We also introduce novel encoding and feature transformation approaches, enabling the robust performance of GATH in scenarios with imbalanced samples. Comprehensive experiments are conducted to evaluate GATH's performance. Compared with the existing state-of-the-art GAT-based model on Hits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on the FB15K-237 dataset and by 4.5% and 14.6% on the WN18RR dataset, respectively. © 2024 Copyright held by the owner/author(s)",attention mechanism; graph attention network; Knowledge graph completion,Attention mechanisms; Completion methods; Graph attention network; Heterogeneous Knowledge; Knowledge graph completion; Knowledge graphs; Network-based; Network-based approach; Overfitting; Performance; Knowledge graph
Pre-training Question Embeddings for Improving Knowledge Tracing with Self-supervised Bi-graph Co-contrastive Learning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185725634&doi=10.1145%2f3638055&partnerID=40&md5=356d0d707f9b5505e3dfab3e8e4463d6,"Learning high-quality vector representations (aka. embeddings) of educational questions lies at the core of knowledge tracing (KT), which defines a task of estimating students' knowledge states by predicting the probability that they correctly answer questions. Although existing KT efforts have leveraged question information to achieve remarkable improvements, most of them learn question embeddings by following the supervised learning paradigm. In this article, we propose a novel question embedding pre-training method for improving knowledge tracing with self-supervised Bi-graph Co-contrastive learning (BiCo). Technically, on the basis of self-supervised learning paradigm, we first select two similar but distinct views (i.e., representing objective and subjective semantic perspectives) as the semantic source of question embeddings. Then, we design a primary task (structure recovery) together with two auxiliary tasks (question difficulty recovery and contrastive learning) to further enhance the representativeness of questions. Finally, extensive experiments conducted on two real-world datasets show BiCo has a higher expressive power that enables KT methods to effectively predict students' performances. Copyright © 2024 held by the owner/author(s)",bi-graph; co-contrastive learning; Knowledge tracing; self-supervised learning,Embeddings; Knowledge management; Students; Supervised learning; Bi-graph; Co-contrastive learning; High quality; Knowledge tracings; Learning paradigms; Pre-training; Quality vectors; Question-embedding; Self-supervised learning; Vector representations; Semantics
Bayesian Graph Local Extrema Convolution with Long-tail Strategy for Misinformation Detection,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185811450&doi=10.1145%2f3639408&partnerID=40&md5=6d601aef29fc3a2ea5b3f730a50623b5,"It has become a cardinal task to identify fake information (misinformation) on social media, because it has significantly harmed the government and the public. There are many spam bots maliciously retweeting misinformation. This study proposes an efficient model for detecting misinformation with self-supervised contrastive learning. A Bayesian graph Local extrema Convolution (BLC) is first proposed to aggregate node features in the graph structure. The BLC approach considers unreliable relationships and uncertainties in the propagation structure, and the differences between nodes and neighboring nodes are emphasized in the attributes. Then, a new long-tail strategy for matching long-tail users with the global social network is advocated to avoid over-concentration on high-degree nodes in graph neural networks. Finally, the proposed model is experimentally evaluated with two public Twitter datasets and demonstrates that the proposed long-tail strategy significantly improves the effectiveness of existing graph-based methods in terms of detecting misinformation. The robustness of BLC has also been examined on three graph datasets and demonstrates that it consistently outperforms traditional algorithms when perturbed by 15% of a dataset. © 2024 Copyright held by the owner/author(s)",bot detection; graph neural network; Misinformation detection; social network,Backpropagation; Botnet; Fake detection; Graph neural networks; Graph structures; Graph theory; Graphic methods; Social networking (online); Bayesian; Bot detections; Graph neural networks; Graph structures; Local extremum; Long tail; Misinformation detection; Social media; Social network; Uncertainty; Convolution
Finding Subgraphs with Maximum Total Density and Limited Overlap in Weighted Hypergraphs,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185791831&doi=10.1145%2f3639410&partnerID=40&md5=269498e64de3f36e63b3858476844ea8,"Finding dense subgraphs in large (hyper)graphs is a key primitive in a variety of real-world application domains, encompassing social network analytics, event detection, biology, and finance. In most such applications, one typically aims at finding several (possibly overlapping) dense subgraphs, which might correspond to communities in social networks or interesting events. While a large amount of work is devoted to finding a single densest subgraph, perhaps surprisingly, the problem of finding several dense subgraphs in weighted hypergraphs with limited overlap has not been studied in a principled way, to the best of our knowledge. In this work, we define and study a natural generalization of the densest subgraph problem in weighted hypergraphs, where the main goal is to find at most k subgraphs with maximum total aggregate density, while satisfying an upper bound on the pairwise weighted Jaccard coefficient, i.e., the ratio of weights of intersection divided by weights of union on two nodes sets of the subgraphs. After showing that such a problem is NP-Hard, we devise an efficient algorithm that comes with provable guarantees in some cases of interest, as well as, an efficient practical heuristic. Our extensive evaluation on large real-world hypergraphs confirms the efficiency and effectiveness of our algorithms. © 2024 Copyright held by the owner/author(s)",Densest subgraphs; weighted hypergraphs,Applications domains; Dense sub-graphs; Events detection; Hyper graph; Large amounts; Natural generalization; Real-world; Subgraphs; Total density; Weighted hypergraphs
Totally-ordered Sequential Rules for Utility Maximization,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185717260&doi=10.1145%2f3628450&partnerID=40&md5=7007f51b17c1e86c9d90b05224384deb,"High-utility sequential pattern mining (HUSPM) is a significant and valuable activity in knowledge discovery and data analytics with many real-world applications. In some cases, HUSPM can not provide an excellent measure to predict what will happen. High-utility sequential rule mining (HUSRM) discovers high utility and high confidence sequential rules, so it can solve the issue in HUSPM. However, all existing HUSRM algorithms aim to find high-utility partially-ordered sequential rules (HUSRs), which are not consistent with reality and may generate fake HUSRs. Therefore, in this article, we formulate the problem of high-utility totally-ordered sequential rule mining and propose a novel algorithm, called TotalSR, which aims to identify all high-utility totally-ordered sequential rules (HTSRs). TotalSR introduces a left-first expansion strategy that can utilize the anti-monotonic property to use a confidence pruning strategy. TotalSR also designs a new utility upper bound: RSPEU, which is tighter than the existing upper bounds. TotalSR can drastically reduce the search space with the help of utility upper bounds pruning strategies, avoiding much more meaningless computation. To effectively compute the information, TotalSR proposes an auxiliary antecedent record table that can efficiently calculate the antecedent's support and a utility prefix sum list that can compute the upper bound in O(1) time for a sequence. Finally, there are numerous experimental results on both real and synthetic datasets demonstrating that TotalSR is more efficient than the existing algorithms. © 2024 Copyright held by the owner/author(s)",Data mining; knowledge discovery; sequence rule; totally-ordered; utility mining,Data Analytics; Discovery analytics; Pruning strategy; Rule mining; Sequence rule; Sequential rule; Sequential-pattern mining; Totally-ordered; Upper Bound; Utility maximizations; Utility mining; Data mining
Hi-PART: Going Beyond Graph Pooling with Hierarchical Partition Tree for Graph-Level Representation Learning,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185718694&doi=10.1145%2f3636429&partnerID=40&md5=910403fbed73f589f8e6867d6c850026,"Graph pooling refers to the operation that maps a set of node representations into a compact form for graph-level representation learning. However, existing graph pooling methods are limited by the power of the Weisfeiler-Lehman (WL) test in the performance of graph discrimination. In addition, these methods often suffer from hard adaptability to hyper-parameters and training instability. To address these issues, we propose Hi-PART, a simple yet effective graph neural network (GNN) framework with Hierarchical Partition Tree (HPT). In HPT, each layer is a partition of the graph with different levels of granularities that are going toward a finer grain from top to bottom. Such an exquisite structure allows us to quantify the graph structure information contained in HPT with the aid of structural information theory. Algorithmically, by employing GNNs to summarize node features into the graph feature based on HPT's hierarchical structure, Hi-PART is able to adequately leverage the graph structure information and provably goes beyond the power of the WL test. Due to the separation of HPT optimization from graph representation learning, Hi-PART involves the height of HPT as the only extra hyper-parameter and enjoys higher training stability. Empirical results on graph classification benchmarks validate the superior expressive power and generalization ability of Hi-PART compared with state-of-the-art graph pooling approaches. Copyright © 2024 held by the owner/author(s)",Graph representation learning; structural entropy,Graph neural networks; Graph structures; Graphic methods; Trees (mathematics); Graph neural networks; Graph representation; Graph representation learning; Graph structures; Hyper-parameter; Performance; Power; Simple++; Structural entropy; Structure information; Information theory
Trajectory-User Linking via Hierarchical Spatio-Temporal Attention Networks,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185704029&doi=10.1145%2f3635718&partnerID=40&md5=e99e9f0895f8fc6ae618f2de98e28e45,"Trajectory-User Linking (TUL) is crucial for human mobility modeling by linking different trajectories to users with the exploration of complex mobility patterns. Existing works mainly rely on the recurrent neural framework to encode the temporal dependencies in trajectories, have fall short in capturing spatial-temporal global context for TUL prediction. To fill this gap, this work presents a new hierarchical spatio-temporal attention neural network, called AttnTUL, to jointly encode the local trajectory transitional patterns and global spatial dependencies for TUL. Specifically, our first model component is built over the graph neural architecture to preserve the local and global context and enhance the representation paradigm of geographical regions and user trajectories. Additionally, a hierarchically structured attention network is designed to simultaneously encode the intra-trajectory and inter-trajectory dependencies, with the integration of the temporal attention mechanism and global elastic attentional encoder. Extensive experiments demonstrate the superiority of our AttnTUL method as compared to state-of-the-art baselines on various trajectory datasets. The source code of our model is available at https://github.com/Onedean/AttnTUL. Copyright © 2024 held by the owner/author(s)",attention neural networks; spatio-temporal data; trajectory representation learning; Trajectory-user linking,Encoding (symbols); Geographical regions; Attention neural network; Global context; Human mobility; Mobility modeling; Mobility pattern; Neural-networks; Spatio-temporal; Spatio-temporal data; Trajectory representation learning; Trajectory-user linking; Trajectories
Local Overlapping Spatial-aware Community Detection,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182600937&doi=10.1145%2f3634707&partnerID=40&md5=432f8e407a2af5908c535dac61a17886,"Local spatial-aware community detection refers to detecting a spatial-aware community for a given node using local information. A spatial-aware community means that nodes in the community are tightly connected in structure, and their locations are close to each other. Existing studies focus on detecting the local non-overlapping spatial-aware community, i.e., detecting a spatial-aware community containing the given node. However, many geosocial networks often contain overlapping spatial-aware communities. Therefore, we propose a local overlapping spatial-aware community detection (LOSCD) problem, which aims to detect all spatial-aware communities that contain a given node with local information. To address LOSCD problem, we design an algorithm based on Spatial Modularity and Edge Similarity, called SMES. SMES contains two processes: spatial expansion and structure detection. The spatial expansion process involves using spatial modularity to identify nodes that are spatially close, while the structural detection process employs edge similarity to identify nodes that are structurally close. Experimental results demonstrate that SMES outperforms comparison algorithms in terms of both structural and spatial cohesiveness.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGeosocial networks; local community detection; local overlapping spatial-aware community detection; local spatial-aware community detection,Expansion; Additional key word and phrasesgeosocial network; Community detection; Key words; Local community; Local community detection; Local information; Local overlapping spatial-aware community detection; Local spatial-aware community detection; Population dynamics
Causality-Based Fair Multiple Decision by Response Functions,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182600674&doi=10.1145%2f3632529&partnerID=40&md5=5c925cb651d89890491b1c02d315f30f,"A recent trend of fair machine learning is to build a decision model subjected to causality-based fairness requirements, which concern with the causality between sensitive attributes and decisions. Almost all (if not all) solutions focus on a single fair decision model and assume no hidden confounder to model causal effects in a too simplified way. However, multiple interdependent decision models are actually used and discrimination may transmit among them. The hidden confounder is another inescapable fact and causal effects cannot be computed from observational data in the unidentifiable situation. To address these problems, we propose a method called CMFL (Causality-based Multiple Fairness Learning). CMFL parameterizes the causal model by response-function variables, whose distributions capture the randomness of causal models. CMFL treats each classifier as a soft intervention to infer the post-intervention distribution, and combines the fairness constraints with the classification loss to train multiple decision classifiers. In this way, all classifiers can make approximately fair decisions. Experiments on synthetic and benchmark datasets confirm its effectiveness, the response-function variables can deal with the unidentifiable issue and hidden confounders.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFairness; causality; hidden confounders and unidentifiable issues; multiple decision learning; response-function variables,Additional key word and phrasesfairness; Causality; Confounder; Decision modeling; Function variables; Hidden confounder and unidentifiable issue; Key words; Multiple decision learning; Response functions; Response-function variable; Learning systems
Efficient Version Space Algorithms for Human-in-the-loop Model Development,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182606809&doi=10.1145%2f3637443&partnerID=40&md5=2ffa66e767ffd2a9bbd8f168aacd63a6,"When active learning (AL) is applied to help users develop a model on a large dataset through interactively presenting data instances for labeling, existing AL techniques often suffer from two main drawbacks: First, to reach high accuracy they may require the user to label hundreds of data instances, which is an onerous task for the user. Second, retrieving the next instance to label from a large dataset can be time-consuming, making it incompatible with the interactive nature of the human exploration process. To address these issues, we introduce a novel version-space-based active learner for kernel classifiers, which possesses strong theoretical guarantees on performance and efficient implementation in time and space. In addition, by leveraging additional insights obtained in the user labeling process, we can factorize the version space to perform active learning in a set of subspaces, which further reduces the user labeling effort. Evaluation results show that our algorithms significantly outperform state-of-the-art version space strategies, as well as a recent factorization-aware algorithm, for model development over large datasets.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesActive learning; factorization; hit-and-run; kernel classifiers; version space,Artificial intelligence; Factorization; Learning systems; Petroleum reservoir evaluation; Active Learning; Additional key word and phrasesactive learning; Hit-and-run; Kernel classifiers; Key words; Large datasets; Model development; Space algorithms; User labeling; Version space; Large dataset
StructCoder: Structure-Aware Transformer for Code Generation,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182592552&doi=10.1145%2f3636430&partnerID=40&md5=83d3c6b1e1501d6e397d1f403653db6e,"There has been a recent surge of interest in automating software engineering tasks using deep learning. This article addresses the problem of code generation, in which the goal is to generate target code given source code in a different language or a natural language description. Most state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model in which both the encoder and decoder are explicitly trained to recognize the syntax and dataflow in the source and target codes, respectively. We not only make the encoder structure aware by leveraging the source code's syntax tree and dataflow graph, but we also support the decoder in preserving the syntax and dataflow of the target code by introducing two novel auxiliary tasks: Abstract Syntax Tree (AST) path prediction and dataflow prediction. To the best of our knowledge, this is the first work to introduce a structure-aware Transformer decoder that models both syntax and dataflow to enhance the quality of generated code. The proposed StructCoder model achieves state-of-the-art performance on code translation and text-to-code generation tasks in the CodeXGLUE benchmark and improves over baselines of similar size on the APPS code generation benchmark. Our code is publicly available at https://github.com/reddy-lab-code-research/StructCoder/.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesDeep learning; code generation; language models; Transformer,Abstracting; Benchmarking; Data flow analysis; Decoding; Deep learning; Learning systems; Software engineering; Syntactics; Trees (mathematics); Additional key word and phrasesdeep learning; Codegeneration; Dataflow; Key words; Language model; Natural languages; Source codes; Structure-aware; Target codes; Transformer; Semantics
Privacy-Preserving Non-Negative Matrix Factorization with Outliers,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182595576&doi=10.1145%2f3632961&partnerID=40&md5=75f795d7381b40145f266d135a4a94d1,"Non-negative matrix factorization is a popular unsupervised machine learning algorithm for extracting meaningful features from inherently non-negative data. Such data often contain privacy-sensitive user information. Additionally, the dataset can contain outliers, which may lead to extracting sub-optimal features from the data. It is, therefore, necessary to address these two issues while analyzing privacy-sensitive data that may contain outliers. In this work, we develop a non-negative matrix factorization algorithm in the privacy-preserving framework that (i) considers the presence of outliers in the data, and (ii) can achieve results comparable to those of the non-private algorithm. We design our method in such a way that one has the control to select the degree of privacy grantee based on the required utility gap. We show the effectiveness of our proposed algorithm's performance on six real and diverse datasets. The experimental results show that our proposed method can achieve a performance that closely approximates the performance of the non-private algorithm under some parameter choices, while ensuring strict privacy guarantees.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDifferential privacy; facial features; Gaussian mechanism; non-negative matrix factorization; privacy accounting; Rényi differential privacy; topic modelling,Data mining; Learning algorithms; Machine learning; Matrix algebra; Privacy-preserving techniques; Sensitive data; Statistics; Additional key word and phrasesdifferential privacy; Differential privacies; Facial feature; Gaussian mechanism; Gaussians; Key words; Nonnegative matrix factorization; Privacy accounting; Renyi differential privacy; Topic Modeling; Non-negative matrix factorization
Parameter-Agnostic Deep Graph Clustering,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182603185&doi=10.1145%2f3633783&partnerID=40&md5=6fcd9bb1427b17a152c2e36d3c292428,"Deep graph clustering, efficiently dividing nodes into multiple disjoint clusters in an unsupervised manner, has become a crucial tool for analyzing ubiquitous graph data. Existing methods have acquired impressive clustering effects by optimizing the clustering network under the parametric condition - predefining the true number of clusters (Ktr). However, Ktr is inaccessible in pure unsupervised scenarios, in which existing methods are incapable of inferring the number of clusters (K), causing limited feasibility. This article proposes the first Parameter-Agnostic Deep Graph Clustering method (PADGC), which consists of two core modules: K-guidence clustering and topological-hierarchical inference, to infer K efficiently and gain impressive clustering predictions. Specifically, K-guidence clustering is employed to optimize the cluster assignments and discriminative embeddings in a mutual promotion manner under the latest updated K, even though K may deviate from Ktr. In turn, such optimized cluster assignments are utilized to explore more accurate K in the topological-hierarchical inference, which can split the dispersive clusters and merge the coupled ones. In this way, these two modules are complementarily optimized until generating the final convergent K and discriminative cluster assignments. Extensive experiments on several benchmarks, including graphs and images, can demonstrate the superiority of our method. The mean values of our inferred K, in 11 out of 12 datasets, deviates from Ktr by less than 1. Our method can also achieve competitive clustering effects with existing parametric deep graph clustering.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesParameter-agnostic graph clustering; deep graph clustering.; K-guidence clustering; topological-hierarchical inference,Additional key word and phrasesparameter-agnostic graph clustering; Cluster assignment; Clustering effect; Clusterings; Deep graph clustering.; Graph clustering; K-guidence clustering; Key words; Topological-hierarchical inference; Topology
LSAB: User Behavioral Pattern Modeling in Sequential Recommendation by Learning Self-Attention Bias,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182589871&doi=10.1145%2f3632625&partnerID=40&md5=536244ae748e3bde25ccf10798306e0f,"Since the weight of a self-attention model is not affected by the sequence interval, it can more accurately and completely describe the user interests, so it is widely used in processing sequential recommendation. However, the mainstream self-attention model focuses on the similarity between items when calculating the attention weight of user behavioral patterns but fails to reflect the impact of user sudden drift decisions on the model in time. In this article, we introduce a bias strategy in the self-attention module, referred to as Learning Self-Attention Bias (LSAB) to more accurately learn the fast-changing user behavioral patterns. The introduction of LSAB allows for the adjustment of bias resulting from self-attention weights, leading to enhanced prediction performance in sequential recommendation. In addition, this article designs four attention-weight bias types catering to diverse user behavior preferences. After testing on the benchmark datasets, each bias strategy in LSAB is useful for state-of-the-art and can improve the performance of the models by nearly 5% on average. The source code listing is publicly available at https://gitee.com/kyle-liao/lsab.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSequential recommendation; Behavioral pattern; Bias; Self-attention model,Benchmarking; Learning systems; Recommender systems; User profile; Additional key word and phrasessequential recommendation; Attention model; Behavioral patterns; Bias; Key words; Learn+; Prediction performance; Self-attention model; User behaviors; Users' interests; Behavioral research
Three-stage Transferable and Generative Crowdsourced Comment Integration Framework Based on Zero- and Few-shot Learning with Domain Distribution Alignment,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182595587&doi=10.1145%2f3636511&partnerID=40&md5=394143fb636362db0aa702adb3a97858,"Online shopping has become a crucial way to encourage daily consumption, where the User-generated, or crowdsourced product comments, can offer a broad range of feedback on e-commerce products. As a result, integrating critical opinions or major attitudes from the crowdsourced comments can provide valuable feedback for marketing strategy adjustment or product-quality monitoring. Unfortunately, the scarcity of annotated ground truth on the integrated comment, or the limited gold integration reference, has incurred the infeasibility of the regular supervised-learning-based comment integration. To resolve this problem, in this article, inspired by the principle of Transfer Learning, we propose a three-stage transferable and generative crowdsourced comment integration framework (TTGCIF) based on zero-and-few-shot learning with the support of domain distribution alignment. The proposed framework aims at generating abstractive integrated comment in target domain via the enhanced neural text generation model, by referring the available integration resource in related source domains, to avoid the exhausted effort on resource annotation devoted to the target domain. Specifically, at the first stage, to enhance the domain transferability, representations on the crowdsourced comments have been aligned up between the source and target domain, by minimizing the domain distribution discrepancy in the kernel space. At the second stage, Zero-shot comment integration mechanism has been adopted to deal with the dilemma that none of the gold integration reference may be available in target domain. In other words, taking the sample-level semantic prototype as input, the enhanced neural text generation model in TTGCIF is trained to learn data semantic association among different domains via semantic prototype transduction, so that the ""unlabeled""crowdsourced comments in target domain can be associated with existing integration references in related source domains. At the third stage, based on the parameters trained at the second stage, fast domain adaptation mechanism in a Few-shot manner has also been adopted by seeking most potential parameters along the gradient direction constrained by instances across multiple source domains. In this way, parameters in TTGCIF can be sensitive to any alteration on training data, ensuring that even if only few annotated resource in target domain are available for ""Fine-tune,""TTGCIF can still react promptly to achieve effective target domain adaptation. According to the experimental results, TTGCIF can achieve the best transferable product comment integration performance in target domain, with fast and stable domain adaption effect depending on no more than 10% annotated resource in target domain. More importantly, even if TTGCIF has not been fine-tuned on the target domain, yet by referring to the available integration resource in related source domains, the integrated comments generated by TTGCIF on the target domain are still superior to those generated by models already fine-tuned on the target domain.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and Phrasescrowdsourcing; comment integration; natural language generation; transfer learning,Alignment; Crowdsourcing; Electronic commerce; Gold; Integration; Learning systems; Natural language processing systems; Additional key word and phrasescrowdsourcing; Comment integration; Domain adaptation; Domain distribution; Integration frameworks; Key words; Natural language generation; Target domain; Text generations; Transfer learning; Semantics
DEWP: Deep Expansion Learning for Wind Power Forecasting,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182596967&doi=10.1145%2f3637552&partnerID=40&md5=0cb89ab567c94171317515d48fe79157,"Wind is one kind of high-efficient, environmentally-friendly, and cost-effective energy source. Wind power, as one of the largest renewable energy in the world, has been playing a more and more important role in supplying electricity. Though growing dramatically in recent years, the amount of generated wind power can be directly or latently affected by multiple uncertain factors, such as wind speed, wind direction, temperatures, and so on. More importantly, there exist very complicated dependencies of the generated power on the latent composition of these multiple time-evolving variables, which are always ignored by existing works and thus largely hinder the prediction performances. To this end, we propose DEWP, a novel Deep Expansion learning for Wind Power forecasting framework to carefully model the complicated dependencies with adequate expressiveness. DEWP starts with a stack-by-stack architecture, where each stack is composed of (i) a variable expansion block that makes use of convolutional layers to capture dependencies among multiple variables; (ii) a time expansion block that applies Fourier series and backcast/forecast mechanism to learn temporal dependencies in sequential patterns. These two tailored blocks expand raw inputs into different latent feature spaces which can model different levels of dependencies of time-evolving sequential data. Moreover, we propose an inference block corresponding for each stack, which applies multi-head self-attentions to acquire attentive features and maps expanded latent representations into generated wind power. In addition, to make DEWP more expressive in handling deep neural architectures, we adapt doubly residue learning to process stack-by-stack outputs. Accurate wind power forecasting (WPF) is then better achieved through fine-grained outputs by continuously removing stack residues and accumulating useful stack forecasts. Finally, we present extensive experiments in the real-world WPF application on two datasets from two different turbines, in order to demonstrate the effectiveness of our approach.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesWind power forecasting; deep learning; time series forecasting,Cost effectiveness; Deep learning; Weather forecasting; Wind power; Wind speed; Additional key word and phraseswind power forecasting; Cost effective; Deep learning; Effective energy; Energy source; High efficient; Key words; Power forecasting; Time series forecasting; Wind power forecasting; Expansion
Laplacian Change Point Detection for Single and Multi-view Dynamic Graphs,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182590585&doi=10.1145%2f3631609&partnerID=40&md5=6e87275ed452735076190cd50a96d672,"Dynamic graphs are rich data structures that are used to model complex relationships between entities over time. In particular, anomaly detection in temporal graphs is crucial for many real-world applications such as intrusion identification in network systems, detection of ecosystem disturbances, and detection of epidemic outbreaks. In this article, we focus on change point detection in dynamic graphs and address three main challenges associated with this problem: (i) how to compare graph snapshots across time, (ii) how to capture temporal dependencies, and (iii) how to combine different views of a temporal graph. To solve the above challenges, we first propose Laplacian Anomaly Detection (LAD) which uses the spectrum of graph Laplacian as the low dimensional embedding of the graph structure at each snapshot. LAD explicitly models short-term and long-term dependencies by applying two sliding windows. Next, we propose MultiLAD, a simple and effective generalization of LAD to multi-view graphs. MultiLAD provides the first change point detection method for multi-view dynamic graphs. It aggregates the singular values of the normalized graph Laplacian from different views through the scalar power mean operation. Through extensive synthetic experiments, we show that (i) LAD and MultiLAD are accurate and outperforms state-of-the-art baselines and their multi-view extensions by a large margin, (ii) MultiLAD's advantage over contenders significantly increases when additional views are available, and (iii) MultiLAD is highly robust to noise from individual views. In five real-world dynamic graphs, we demonstrate that LAD and MultiLAD identify significant events as top anomalies such as the implementation of government COVID-19 interventions which impacted the population mobility in multi-view traffic networks. © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSpectral methods; Graph algorithms; Graphs and networks; Machine learning,Graphic methods; Intrusion detection; Laplace transforms; Machine learning; Additional key word and phrasesspectral method; Anomaly detection; Change point detection; Dynamic graph; Graph algorithms; Graphs and networks; Key words; Laplacians; Machine-learning; Multi-views; Anomaly detection
Graph Domain Adaptation: A Generative View,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182599694&doi=10.1145%2f3631712&partnerID=40&md5=88aec586de652b22ed105443030ae941,"Recent years have witnessed tremendous interest in deep learning on graph-structured data. Due to the high cost of collecting labeled graph-structured data, domain adaptation is important to supervised graph learning tasks with limited samples. However, current graph domain adaptation methods are generally adopted from traditional domain adaptation tasks, and the properties of graph-structured data are not well utilized. For example, the observed social networks on different platforms are controlled not only by the different crowds or communities but also by domain-specific policies and background noise. Based on these properties in graph-structured data, we first assume that the graph-structured data generation process is controlled by three independent types of latent variables, i.e., the semantic latent variables, the domain latent variables, and the random latent variables. Based on this assumption, we propose a disentanglement-based unsupervised domain adaptation method for the graph-structured data, which applies variational graph auto-encoders to recover these latent variables and disentangles them via three supervised learning modules. Extensive experimental results on two real-world datasets in the graph classification task reveal that our method not only significantly outperforms the traditional domain adaptation methods and the disentangled-based domain adaptation methods but also outperforms the state-of-the-art graph domain adaptation algorithms. The code is available at https://github.com/rynewu224/GraphDA. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGraph Neural Network; Domain Adaptation; Graph Generative Models,Classification (of information); Deep learning; Graphic methods; Learning systems; Adaptation methods; Additional key word and phrasesgraph neural network; Domain adaptation; Generative model; Graph generative model; Graph structured data; Key words; Latent variable; Neural-networks; Property; Semantics
Enhanced Fuzzy Clustering for Incomplete Instance with Evidence Combination,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182586932&doi=10.1145%2f3638061&partnerID=40&md5=d1c6f4ebe8cba49494b361e4fd7d10e3,"Clustering incomplete instance is still a challenging task since missing values maybe make the cluster information ambiguous, leading to the uncertainty and imprecision in results. This article investigates an enhanced fuzzy clustering with evidence combination method based on Dempster-Shafer theory (DST) to address this problem. First, the dataset is divided into several subsets, and missing values are imputed by neighbors with different weights in each subset. It aims to model missing values locally to reduce the negative impact of the bad estimations. Second, an objective function of enhanced fuzzy clustering is designed and then optimized until the best membership and reliability matrices are found. Each subset has a membership matrix that contains all sub-instances' membership to different clusters. The fuzzy reliability matrix is employed to characterize the reliability of each subset on different clusters. Third, an adaptive evidence combination rule based on the DST is developed to combine the discounted subresults (memberships) with different reliability to make the final decision for each instance. The proposed method can characterize uncertainty and imprecision by assigning instances to specific clusters or meta-clusters composed of several specific clusters. Once an instance is assigned to a meta-cluster, the cluster information of this instance is (locally) imprecise. The effectiveness of proposed method is demonstrated on several real-world datasets by comparing with existing techniques.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesClustering; dempster-shafer theory; evidence combination; imprecision; incomplete instance; uncertainty,Fuzzy clustering; Probabilistic logics; Additional key word and phrasesclustering; Dempster-Shafer theory; Evidence combination; Imprecision; Incomplete instance; Key words; Membership matrix; Missing values; Reliability matrix; Uncertainty; Reliability
Learning Hierarchical Task Structures for Few-shot Graph Classification,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182589415&doi=10.1145%2f3635473&partnerID=40&md5=0b2e6f7b1fd95de5f0a943c8aed617ed,"The problem of few-shot graph classification targets at assigning class labels for graph samples, where only limited labeled graphs are provided for each class. To solve the problem brought by label scarcity, recent studies have proposed to adopt the prevalent few-shot learning framework to achieve fast adaptations to graph classes with limited labeled graphs. In particular, these studies typically propose to accumulate meta-knowledge across a large number of meta-training tasks, and then generalize such meta-knowledge to meta-test tasks sampled from a disjoint class set. Nevertheless, existing studies generally ignore the crucial task correlations among meta-training tasks and treat them independently. In fact, such task correlations can help promote the model generalization to meta-test tasks and result in better classification performance. On the other hand, it remains challenging to capture and utilize task correlations due to the complex components and interactions in meta-training tasks. To deal with this, we propose a novel few-shot graph classification framework FAITH to capture task correlations via learning a hierarchical task structure at different granularities. We further propose a task-specific classifier to incorporate the learned task correlations into the few-shot graph classification process. Moreover, we derive FAITH+, a variant of FAITH that can improve the sampling process for the hierarchical task structure. The extensive experiments on four prevalent graph datasets further demonstrate the superiority of FAITH and FAITH+ over other state-of-the-art baselines.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesFew-shot learning; data mining; graph classification; graph neural networks,Classification (of information); Graph neural networks; Additional key word and phrasesfew-shot learning; Class labels; Graph classification; Graph neural networks; Key words; Labeled graphs; Meta-knowledge; Meta-training; Task structure; Test tasks; Data mining
Sparse Grid Imputation Using Unpaired Imprecise Auxiliary Data: Theory and Application to PM2.5 Estimation,2024,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182597426&doi=10.1145%2f3634751&partnerID=40&md5=b8f5ef72cbeb7227b5752b9dbe27569f,"Sparse grid imputation (SGI) is a challenging problem, as its goal is to infer the values of the entire grid from a limited number of cells with values. Traditionally, the problem is solved using regression methods such as KNN and kriging, whereas in the real world, there is often extra information - usually imprecise - that can aid inference and yield better performance. In the SGI problem, in addition to the limited number of fixed grid cells with precise target domain values, there are contextual data and imprecise observations over the whole grid. To solve this problem, we propose a distribution estimation theory for the whole grid and realize the theory via the composition architecture of the Target-Embedding and the Contextual CycleGAN trained with contextual information and imprecise observations. Contextual CycleGAN is structured as two generator-discriminator pairs and uses different types of contextual loss to guide the training. We consider the real-world problem of fine-grained PM2.5 inference with realistic settings: a few (less than 1%) grid cells with precise PM2.5 data and all grid cells with contextual information concerning weather and imprecise observations from satellites and microsensors. The task is to infer reasonable values for all grid cells. As there is no ground truth for empty cells, out-of-sample mean squared error and Jensen-Shannon divergence measurements are used in the empirical study. The results show that Contextual CycleGAN supports the proposed theory and outperforms the methods used for comparison.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSparse grid imputation; contextual CycleGAN; distribution transformation; imprecise observations,Cytology; Kriging; Mean square error; Metadata; Regression analysis; Additional key word and phrasessparse grid imputation; Auxiliary data; Contextual cyclegan; Contextual information; Data theories; Distribution transformation; Grid cells; Imprecise observation; Key words; Sparse grid; Cells
Group-Aware Graph Neural Network for Nationwide City Air Quality Forecasting,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180298610&doi=10.1145%2f3631713&partnerID=40&md5=a81492c3506bc2b8ba2638dfcf89650d,"The problem of air pollution threatens public health. Air quality forecasting can provide the air quality index hours or even days later, which can help the public to prevent air pollution in advance. Previous works focus on citywide air quality forecasting and cannot solve nationwide city forecasting problems, whose difficulties lie in capturing the latent dependencies between geographically distant but highly correlated cities. In this article, we propose the group-aware graph neural network (GAGNN), a hierarchical model for nationwide city air quality forecasting. The model constructs a city graph and a city group graph to model the spatial and latent dependencies between cities, respectively. GAGNN introduces a differentiable grouping network to discover the latent dependencies among cities and generate city groups. Based on the generated city groups, a group correlation encoding module is introduced to learn the correlations between them, which can effectively capture the dependencies between city groups. After the graph construction, GAGNN implements message passing mechanism to model the dependencies between cities and city groups. The evaluation experiments on two real-world nationwide city air quality datasets, including the China dataset and the US dataset, indicate that our GAGNN outperforms existing forecasting models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAir quality forecasting; deep learning; graph neural network; urban computing,Air quality; Deep learning; Graph neural networks; Hierarchical systems; Message passing; Additional key word and phrasesair quality forecasting; Air quality forecasting; Air quality indices; City groups; Deep learning; Forecasting problems; Graph neural networks; Highly-correlated; Key words; Urban computing; Forecasting
Learning Entangled Interactions of Complex Causality via Self-Paced Contrastive Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182590766&doi=10.1145%2f3632406&partnerID=40&md5=ca20f28972cc5ddd0398199465e18ccf,"Learning causality from large-scale text corpora is an important task with numerous applications - for example, in finance, biology, medicine, and scientific discovery. Prior studies have focused mainly on simple causality, which only includes one cause-effect pair. However, causality is notoriously difficult to understand and analyze because of multiple cause spans and their entangled interactions. To detect complex causality, we propose a self-paced contrastive learning model, namely N2NCause, to learn entangled interactions between multiple spans. Specifically, N2NCause introduces data enhancement operations to convert implicit expressions into explicit expressions with the most rational causal connectives for the synthesis of positive samples and to invert the directed connection between a cause-effect pair for the synthesis of negative samples. To learn the semantic dependency and causal direction of positive and negative samples, self-paced contrastive learning is proposed to learn the entangled interactions among spans, including the interaction direction and interaction field. We evaluated the performance of N2NCause in three cause-effect detection tasks. The experimental results show that, with the least data annotation efforts, N2NCause demonstrates competitive performance in detecting simple cause-effect relations, and it is superior to existing solutions for the detection of complex causality.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEntangled interaction; causal directionality; causality detection; complex causality; self-paced contrastive learning,Additional key word and phrasesentangled interaction; Causal directionality; Causality detection; Cause-effect; Complex causality; Key words; Learn+; Negative samples; Self-paced contrastive learning; Simple++; Semantics
Modeling Interference for Individual Treatment Effect Estimation from Networked Observational Data,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182592422&doi=10.1145%2f3628449&partnerID=40&md5=07b438e8490aeffd39cde0bbd185aac3,"Estimating individual treatment effect (ITE) from observational data has attracted great interest in recent years, which plays a crucial role in decision-making across many high-impact domains such as economics, medicine, and e-commerce. Most existing studies of ITE estimation assume that different units at play are independent and do not influence each other. However, many social science experiments have shown that there often exist different levels of interactions between units in observational data, especially in a networked environment. As a result, the treatment assignment of one unit can affect the outcome of other units connected to it in the network, which is referred to as the interference or spillover effect. In this article, we study an important problem of ITE estimation from networked observational data by modeling the interference between different units and provide a principled framework to support such study. Methodologically, we propose a novel framework, SPNet, that first captures the influence of hidden confounders with the aid of graph convolutional network and then models the interference by introducing an environment summary variable and developing a masked attention mechanism. Experimental evaluations on several semi-synthetic datasets based on real-world networks corroborate the superiority of our proposed framework over state-of-the-art individual treatment effect estimation methods. Copyright © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCausal inference; ITE estimation; network interference,Additional key word and phrasescausal inference; Decisions makings; E- commerces; High impact; Individual treatment effect estimation; Key words; Network interferences; Observational data; Science experiments; Treatment effects; Decision making
EFMVFL: An Efficient and Flexible Multi-party Vertical Federated Learning without a Third Party,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182606741&doi=10.1145%2f3627993&partnerID=40&md5=b2a4da820a7fd42fc6669868840e197f,"Federated learning (FL) is a machine learning setting which allows multiple participants collaboratively to train a model under the orchestration of a server without disclosing their local data. Vertical federated learning (VFL) is a special structure in FL. It handles the situation where participants have the same ID space but different feature spaces. In order to guarantee the security and privacy of the local data of each participant, homomorphic encryption (HE) is often used to transmit intermediate parameters or data during the training process. In most VFL frameworks, a trusted third-party server is necessary because the plaintexts of the parameters need to be revealed for the computation. However, it is hard to find such a credible entity in the real world. Existing methods for solving this problem are either communication-intensive or unsuitable for multi-party scenarios. By combining secret sharing (SS) and HE, we propose a novel VFL framework without any trusted third parties called EFMVFL. It allows intermediate parameters to be transmitted among multiple parties without revealing the plaintexts. EFMVFL is applicable to generalized linear models (GLMs) and supports flexible expansion to multiple participants. Extensive experiments under Logistic Regression and Poisson Regression show that our framework is outstanding in communication (reduced by 3.2×- 6.8×) and efficiency (accelerated by 1.6×- 3.1×). Copyright © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFederated learning; generalized linear models; homomorphic encryption; privacy protection; secret sharing,Data privacy; Learning systems; Additional key word and phrasesfederated learning; Generalized linear model; Ho-momorphic encryptions; Homomorphic-encryptions; Key words; Learning frameworks; Local data; Privacy protection; Secret-sharing; Trusted third parties; Cryptography
Put Your Voice on Stage: Personalized Headline Generation for News Articles,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182595678&doi=10.1145%2f3629168&partnerID=40&md5=300b5efbaf9ca0cdbbbc0e165175cdc6,"In this article, we study the problem of personalized news headline generation, which aims to produce not only concise and fact-consistent titles for news articles but also decorate these titles as personalized irresistible reading invitations by incorporating readers' preferences. We propose an approach named PNG (Personalized News headline Generator) by utilizing distant supervision in readers' past click behaviors to resolve. First, user preference representations are learned through a knowledge-aware user encoder that comprehensively captures the genuine, sequential, and flash interests of users reflected in their historical clicked news. Then, a user-perturbed pointer-generator network is devised to accomplish the headline generation in which the learned user representations implicitly affect the word prediction. The proposed model is optimized by reinforcement learning solvers where indicators on factual, personalized, and linguistic aspects of the generated headline are regarded as rewards. Extensive experiments are conducted on the real-world dataset PENS,1 which is a large-scale benchmark collected from Microsoft News. Both the quantitative and qualitative results validate the effectiveness of our approach. Copyright © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNews headline generation; personalization; user modeling,Large dataset; User profile; Additional key word and phrasesnews headline generation; Headline generation; Key words; News articles; Personalizations; Personalized news; Preference representation; User Modelling; User's preferences; Word prediction; Reinforcement learning
History-enhanced and Uncertainty-aware Trajectory Recovery via Attentive Neural Network,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182591408&doi=10.1145%2f3615660&partnerID=40&md5=f3e9851de4270861cf0049c29dcc7ef8,"A considerable amount of mobility data has been accumulated due to the proliferation of location-based services. Nevertheless, compared with mobility data from transportation systems like the GPS module in taxis, this kind of data is commonly sparse in terms of individual trajectories in the sense that users do not access mobile services and contribute their data all the time. Consequently, the sparsity inevitably weakens the practical value of the data even if it has a high user penetration rate. To solve this problem, we propose a novel attentional neural network-based model, named AttnMove, to densify individual trajectories by recovering unobserved locations at a fine-grained spatial-temporal resolution. To tackle the challenges posed by sparsity, we design various intra- and inter- trajectory attention mechanisms to better model the mobility regularity of users and fully exploit the periodical pattern from long-term history. In addition, to guarantee the robustness of the generated trajectories to avoid harming downstream applications, we also exploit the Bayesian approximate neural network to estimate the uncertainty of each imputation. As a result, locations generated by the model with high uncertainty will be excluded. We evaluate our model on two real-world datasets, and extensive results demonstrate the performance gain compared with the state-of-the-art methods. In-depth analyses of each design of our model have been conducted to understand their contribution. We also show that, by providing high-quality mobility data, our model can benefit a variety of mobility-oriented downstream applications. Copyright © 2023 held by the owner/author(s).",Additional Key Words and PhrasesMobility; data augmentation; self-attention; spatial-temporal data mining,Data mining; Location; Location based services; Telecommunication services; Uncertainty analysis; Additional key word and phrasesmobility; Data augmentation; Downstream applications; Key words; Location-based services; Mobility datum; Neural-networks; Self-attention; Spatial-temporal data minings; Uncertainty; Trajectories
Fair and Private Data Preprocessing through Microaggregation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182594513&doi=10.1145%2f3617377&partnerID=40&md5=d1db937b7706cc467a92a1f58b9b4c97,"Privacy protection for personal data and fairness in automated decisions are fundamental requirements for responsible Machine Learning. Both may be enforced through data preprocessing and share a common target: data should remain useful for a task, while becoming uninformative of the sensitive information. The intrinsic connection between privacy and fairness implies that modifications performed to guarantee one of these goals, may have an effect on the other, e.g., hiding a sensitive attribute from a classification algorithm might prevent a biased decision rule having such attribute as a criterion. This work resides at the intersection of algorithmic fairness and privacy. We show how the two goals are compatible, and may be simultaneously achieved, with a small loss in predictive performance. Our results are competitive with both state-of-the-art fairness correcting algorithms and hybrid privacy-fairness methods. Experiments were performed on three widely used benchmark datasets: Adult Income, COMPAS, and German Credit. Copyright © 2023 held by the owner/author(s).",Additional Key Words and PhrasesResponsible machine learning; algorithmic fairness; ethical AI; fair classification; privacy preserving data mining,Data mining; Learning algorithms; Privacy-preserving techniques; Sensitive data; Additional key word and phrasesresponsible machine learning; Algorithmic fairness; Algorithmics; Data preprocessing; Ethical AI; Fair classification; Key words; Machine-learning; Privacy-preserving data mining; Private data; Machine learning
A Survey on Bid Optimization in Real-Time Bidding Display Advertising,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182591538&doi=10.1145%2f3628603&partnerID=40&md5=ee48b741e639b45c859c29f7beff3c23,"Real-Time Bidding (RTB) is one of the most important forms of online advertising, where an auction is hosted in real time to sell the individual ad impression. How to design an automated bidding strategy in response to the dynamic auction environment is crucial for improving user experience, protecting the interests of advertisers, and promoting the long-term development of the advertising platform. As an exciting topic in the real-world industry, it has attracted great research interest from several disciplines, most notably data science. There have been abundant studies on bidding strategy design which are based on the large volume of historical ad requests. Despite its popularity and significance, few works provide a summary for bid optimization. In this survey, we present the latest overview of the recent works to shed light on the optimization techniques where most of them are validated in practice. We first explore the optimization problem in different works, explaining how these different settings affect the bidding strategy designs. Then, some forms of bidding functions and specific optimization techniques are illustrated. Further, we specifically discuss a new trend about bidding in first-price auctions, which have gradually become popular in recent years. From this survey, both practitioners and researchers can gain insights of the challenges and future prospects of bid optimization in RTB. Copyright © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesComputational advertising; bidding strategy; real-time bidding,Additional key word and phrasescomputational advertizing; Advertizing; Bid optimizations; Bidding strategy; Display advertisings; Key words; Optimization techniques; Real- time; Real-time bidding; Strategy designs; Marketing
Adaptive Adversarial Contrastive Learning for Cross-Domain Recommendation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182605407&doi=10.1145%2f3630259&partnerID=40&md5=a1d8aec0e111906d25fbb5d12ddfdc54,"Graph-based cross-domain recommendations (CDRs) are useful for suggesting appropriate items because of their promising ability to extract features from user-item interactions and transfer knowledge across domains. Thus, the model can effectively alleviate cold start and data sparsity issues. Although the graph-based CDRs can capture valuable information, they still have some limitations. First, embeddings are highly vulnerable to noisy interactions, because the message aggregation in the graph convolutional network can further enlarge the impact. Second, because of the property of graph-structured data, the influence of high-degree nodes on representation learning is more than that of the long-tail items, and this can cause a poor recommendation performance. In this study, we devised a novel Adaptive Adversarial Contrastive Learning framework for graph-based Cross-Domain Recommendation (ACLCDR). The ACLCDR introduces reinforcement learning to generate adaptive augmented samples for contrastive learning tasks. Then, we leveraged a multitask training strategy to jointly optimize the model with auxiliary tasks. Finally, we verified the effectiveness of the ACLCDR through nine real-world cross-domain tasks adopted from Amazon and Douban. We observed that ACLCDR exceeded the best state-of-the-art baseline by 25%, 42.5%, 16.3%, and 23.8% in terms of HR@10 and NDCG@10 for the Music & Movie task from Amazon. Copyright © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSelf-supervised learning; adversarial learning; collaborative filtering; contrastive learning; cross-domain recommendation; reinforcement learning,Collaborative filtering; Graphic methods; Additional key word and phrasesself-supervised learning; Adversarial learning; Cold-start; Contrastive learning; Cross-domain recommendations; Data sparsity; Graph-based; Key words; Learning frameworks; Reinforcement learnings; Reinforcement learning
SILVAN: Estimating Betweenness Centralities with Progressive Sampling and Non-uniform Rademacher Bounds,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182595862&doi=10.1145%2f3628601&partnerID=40&md5=abb3d1f2867ece22096bd363f9c3926e,"""Sim Sala Bim!""- Silvan,https://en.wikipedia.org/wiki/Silvan_(illusionist)Betweenness centrality is a popular centrality measure with applications in several domains and whose exact computation is impractical for modern-sized networks. We present SILVAN, a novel, efficient algorithm to compute, with high probability, accurate estimates of the betweenness centrality of all nodes of a graph and a high-quality approximation of the top-k betweenness centralities. SILVAN follows a progressive sampling approach and builds on novel bounds based on Monte Carlo Empirical Rademacher Averages, a powerful and flexible tool from statistical learning theory. SILVAN relies on a novel estimation scheme providing non-uniform bounds on the deviation of the estimates of the betweenness centrality of all the nodes from their true values and a refined characterisation of the number of samples required to obtain a high-quality approximation. Our extensive experimental evaluation shows that SILVAN extracts high-quality approximations while outperforming, in terms of number of samples and accuracy, the state-of-the-art approximation algorithm with comparable quality guarantees. Copyright © 2023 held by the owner/author(s).",Additional Key Words and PhrasesBetweenness centrality; Rademacher averages; random sampling,Computation theory; Sampling; Additional key word and phrasesbetweenness centrality; Betweenness centrality; Centrality measures; High quality; Key words; Non-uniform; Number of samples; Progressive sampling; Rademacher average; Random sampling; Approximation algorithms
"From Asset Flow to Status, Action, and Intention Discovery: Early Malice Detection in Cryptocurrency",2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182608161&doi=10.1145%2f3626102&partnerID=40&md5=97d4effbd4db9fafba13c06d7fafb5db,"Cryptocurrency has been subject to illicit activities probably more often than traditional financial assets due to the pseudo-anonymous nature of its transacting entities. An ideal detection model is expected to achieve all three critical properties of early detection, good interpretability, and versatility for various illicit activities. However, existing solutions cannot meet all these requirements, as most of them heavily rely on deep learning without interpretability and are only available for retrospective analysis of a specific illicit type. To tackle all these challenges, we propose Intention Monitor for early malice detection in Bitcoin, where the on-chain record data for a certain address are much scarcer than other cryptocurrency platforms.We first define asset transfer paths with the Decision Tree based feature Selection and Complement to build different feature sets for different malice types. Then, the Status/Action Proposal module and the Intention-VAE module generate the status, action, intent-snippet, and hidden intent-snippet embedding. With all these modules, our model is highly interpretable and can detect various illegal activities. Moreover, well-designed loss functions further enhance the prediction speed and the model's interpretability. Extensive experiments on three real-world datasets demonstrate that our proposed algorithm outperforms the state-of-the-art methods. Furthermore, additional case studies justify that our model not only explains existing illicit patterns but also can find new suspicious characters. Copyright © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCybercrime; Bitcoin; cryptocurrency; early detection; intention discovery; malicious address,Bitcoin; Data mining; Deep learning; Action and intention; Additional key word and phrasescybercrime; Critical properties; Detection models; Early detection; Financial assets; Intention discovery; Interpretability; Key words; Malicious address; Decision trees
Co-Training-Teaching: A Robust Semi-Supervised Framework for Review-Aware Rating Regression,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177848639&doi=10.1145%2f3625391&partnerID=40&md5=162e50dc82f119f18e5ae3179f6563bf,"Review-aware Rating Regression (RaRR) suffers the severe challenge of extreme data sparsity as the multi-modality interactions of ratings accompanied by reviews are costly to obtain. Although some studies of semi-supervised rating regression are proposed to mitigate the impact of sparse data, they bear the risk of learning from noisy pseudo-labeled data. In this article, we propose a simple yet effective paradigm, called co-training-teaching (CoT2), for integrating the merits of both co-training and co-teaching toward robust semi-supervised RaRR. CoT2 employs two predictors trained with different feature sets of textual reviews, each of which functions as both ""labeler""and ""validator.""Specifically, one predictor (labeler) first labels unlabeled data for its peer predictor (validator); after that, the validator samples reliable instances from the noisy pseudo-labeled data it received and sends them back to the labeler for updating. By exchanging and validating pseudo-labeled instances, the two predictors are reinforced by each other in an iterative learning process. The final prediction is made by averaging the outputs of both the refined predictors. Extensive experiments show that our CoT2 considerably outperforms the state-of-the-art recommendation techniques in the RaRR task, especially when the training data is severely insufficient.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",co-teaching; co-training; Review-aware rating regression; semi-supervised learning,Regression analysis; Co-teaching; Co-training; Data sparsity; Labeled data; Multi-modality; Review-aware rating regression; Semi-supervised; Semi-supervised learning; Simple++; Sparse data; Iterative methods
Self-supervised Graph-level Representation Learning with Adversarial Contrastive Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177873134&doi=10.1145%2f3624018&partnerID=40&md5=4b1292cc70642c23fb21932bff880fef,"The recently developed unsupervised graph representation learning approaches apply contrastive learning into graph-structured data and achieve promising performance. However, these methods mainly focus on graph augmentation for positive samples, while the negative mining strategies for graph contrastive learning are less explored, leading to sub-optimal performance. To tackle this issue, we propose a Graph Adversarial Contrastive Learning (GraphACL) scheme that learns a bank of negative samples for effective self-supervised whole-graph representation learning. Our GraphACL consists of (i) a graph encoding branch that generates the representations of positive samples and (ii) an adversarial generation branch that produces a bank of negative samples. To generate more powerful hard negative samples, our method minimizes the contrastive loss during encoding updating while maximizing the contrastive loss adversarially over the negative samples for providing the challenging contrastive task. Moreover, the quality of representations produced by the adversarial generation branch is enhanced through the regularization of carefully designed bank divergence loss and bank orthogonality loss. We optimize the parameters of the graph encoding branch and adversarial generation branch alternately. Extensive experiments on 14 real-world benchmarks on both graph classification and transfer learning tasks demonstrate the effectiveness of the proposed approach over existing graph self-supervised representation learning methods.  © 2023 Copyright held by the owner/author(s).",contrastive learning; graph neural networks; Graph representation learning; self-supervised learning,Encoding (symbols); Graph structures; Learning systems; Signal encoding; Contrastive learning; Graph encoding; Graph neural networks; Graph representation; Graph representation learning; Graph structured data; Learning approach; Negative samples; Performance; Self-supervised learning; Graph neural networks
Resisting the Edge-Type Disturbance for Link Prediction in Heterogeneous Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177875870&doi=10.1145%2f3614099&partnerID=40&md5=f86fe3449329b222b9431feec97feae1,"The rapid development of heterogeneous networks has proposed new challenges to the long-standing link prediction problem. Existing models trained on the verified edge samples from different types usually learn type-specific knowledge, and their type-specific predictions may be contradictory for unverified edge samples with uncertain types. This challenge is termed edge-type disturbance in link prediction in heterogeneous networks. To address this challenge, we develop a disturbance-resilient prediction method (DRPM) comprising a structural characterizer, a type differentiator, and a resilient predictor. The structural characterizer is responsible for learning edge representations for link prediction. Concurrently, the type differentiator distinguishes type-specific edge representations to generate diverse type experts while maximizing their link prediction performances on specific types. Furthermore, the resilient predictor evaluates the reliability weights of different type experts to develop a resilient prediction mechanism to aggregate discriminable predictions. Extensive experiments conducted on various real-world datasets demonstrate the importance of the explainable introduction of the edge-type disturbance and the superiority of DRPM over state-of-the-art methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",edge-type disturbance; Heterogeneous network; link prediction,Forecasting; Differentiators; Edge-type disturbance; Learn+; Link prediction; Prediction mechanisms; Prediction methods; Prediction performance; Prediction problem; Real-world datasets; Specific knowledge; Heterogeneous networks
A Multisource Data Fusion-based Heterogeneous Graph Attention Network for Competitor Prediction,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177883219&doi=10.1145%2f3625101&partnerID=40&md5=2b13583be032bc0b1835fcbd4f016399,"Competitor identification is an essential component of corporate strategy. With the rapid development of artificial intelligence, various data-mining methodologies and frameworks have emerged to identify competitors. In general, the competitiveness among companies is determined by both market commonality and resource similarity. However, because resource information is more difficult to obtain than market information, existing studies primarily identify competitors via market commonality. To address this limitation, we introduce multisource company descriptions as well as heterogeneous business relationships, and we propose a novel method for simultaneously mining the market commonality and resource similarity. First, we use multisource company descriptions to represent companies and transform the heterogeneous business relationships into a heterogeneous business network. Then, we propose a novel multisource data fusion-based heterogeneous graph attention network (MHGAT) to learn the pairwise competitive relationships between companies. Specifically, a graph neural network-based model is proposed to learn the embeddings of companies by preserving their competition, and a multilevel attention framework is designed to integrate the embeddings from neighboring company level, heterogeneous relationship level, and multisource description level. Finally, experiments on a real-world dataset verify the effectiveness of our proposed MHGAT and demonstrate the usefulness of company descriptions and business relationships in competitor identification.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",business data; business intelligence; Competitor identification; graph neural network,Commerce; Data fusion; Data mining; Graph neural networks; Business data; Business relationships; Business-intelligence; Competitor identification; Embeddings; Graph neural networks; Heterogeneous graph; Learn+; Multi-Sources; Multisource data; Embeddings
Semi-Supervised Heterogeneous Graph Learning with Multi-Level Data Augmentation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177878965&doi=10.1145%2f3608953&partnerID=40&md5=caf03f07dfd00901ff136d1c3e9705a6,"In recent years, semi-supervised graph learning with data augmentation (DA) has been the most commonly used and best-performing method to improve model robustness in sparse scenarios with few labeled samples. However, most existing DA methods are based on the homogeneous graph, but none are specific for the heterogeneous graph. Differing from the homogeneous graph, DA in the heterogeneous graph faces greater challenges: heterogeneity of information requires DA strategies to effectively handle heterogeneous relations, which considers the information contribution of different types of neighbors and edges to the target nodes. Furthermore, over-squashing of information is caused by the negative curvature formed by the non-uniformity distribution and the strong clustering in a complex graph. To address these challenges, this article presents a novel method named HG-MDA (Semi-Supervised Heterogeneous Graph Learning with Multi-Level Data Augmentation). For the problem of heterogeneity of information in DA, node and topology augmentation strategies are proposed for the characteristics of the heterogeneous graph. Additionally, meta-relation-based attention is applied as one of the indexes for selecting augmented nodes and edges. For the problem of over-squashing of information, triangle-based edge adding and removing are designed to alleviate the negative curvature and bring the gain of topology. Finally, the loss function consists of the cross-entropy loss for labeled data and the consistency regularization for unlabeled data. To effectively fuse the prediction results of various DA strategies, sharpening is used. Existing experiments on public datasets (i.e., ACM, DBLP, and OGB) and the industry dataset MB show that HG-MDA outperforms current SOTA models. Additionally, HG-MDA is applied to user identification in internet finance scenarios, helping the business to add 30% key users, and increase loans and balances by 3.6%, 11.1%, and 9.8%.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",node augmentation; Semi-supervised learning; triangle augmentation,Data augmentation; Heterogeneous graph; Model robustness; Multilevels; Negative curvature; Node augmentation; Semi-supervised; Semi-supervised graphs; Semi-supervised learning; Triangle augmentation; Graph theory
Understanding Any Time Series Classifier with a Subsequence-based Explainer,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177879731&doi=10.1145%2f3624480&partnerID=40&md5=2dbb0c9760226866721e889e8199770d,"The growing availability of time series data has increased the usage of classifiers for this data type. Unfortunately, state-of-the-art time series classifiers are black-box models and, therefore, not usable in critical domains such as healthcare or finance, where explainability can be a crucial requirement. This paper presents a framework to explain the predictions of any black-box classifier for univariate and multivariate time series. The provided explanation is composed of three parts. First, a saliency map highlighting the most important parts of the time series for the classification. Second, an instance-based explanation exemplifies the black-box's decision by providing a set of prototypical and counterfactual time series. Third, a factual and counterfactual rule-based explanation, revealing the reasons for the classification through logical conditions based on subsequences that must, or must not, be contained in the time series. Experiments and benchmarks show that the proposed method provides faithful, meaningful, stable, and interpretable explanations.  © 2023 Copyright held by the owner/author(s).",Explainable AI; prototypes and counterfactuals; subsequence-based rules; time series classification,Classification (of information); Black boxes; Counterfactuals; Datatypes; Explainable AI; Prototype and counterfactual; State of the art; Subsequence-based rule; Time series classifications; Time-series data; Times series; Time series
Maximizing the Diversity of Exposure in Online Social Networks by Identifying Users with Increased Susceptibility to Persuasion,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177873223&doi=10.1145%2f3625826&partnerID=40&md5=85ba8a92c37235f89f5d86ea343542f2,"Individuals may have a range of opinions on controversial topics. However, the ease of making friendships in online social networks tends to create groups of like-minded individuals, who propagate messages that reinforce existing opinions and ignore messages expressing opposite opinions. This creates a situation where there is a decrease in the diversity of messages to which users are exposed (diversity of exposure). This means that users do not easily get the chance to be exposed to messages containing alternative viewpoints; it is even more unlikely that they forward such messages to their friends. Increasing the chance that such messages are propagated implies that an individuals' susceptibility to persuasion is increased, something that may ultimately increase the diversity of messages to which users are exposed. This article formulates a novel problem which aims to identify a small set of users for whom increasing susceptibility to persuasion maximizes the diversity of exposure of all users in the network. We study the properties of this problem and develop a method to find a solution with an approximation guarantee. For this, we first prove that the problem is neither submodular nor supermodular and then we develop submodular bounds for it. These bounds are used in the Sandwich framework to propose a method which approximates the solution using reverse sampling. The proposed method is validated using four real-world datasets. The obtained results demonstrate the superiority of the proposed method compared to baseline approaches.  © 2023 Copyright held by the owner/author(s).",Diversity of exposure; influence maximization; polarization; social bubbles; susceptibility to persuasion,Controversial topics; Diversity of exposure; Exposed to; Influence maximizations; Property; Real-world datasets; Social bubble; Submodular; Supermodular; Susceptibility to persuasion; Social networking (online)
Graph Mining for Cybersecurity: A Survey,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177860302&doi=10.1145%2f3610228&partnerID=40&md5=150ac016112ad79f0d8622e769e9d004,"The explosive growth of cyber attacks today, such as malware, spam, and intrusions, has caused severe consequences on society. Securing cyberspace has become a great concern for organizations and governments. Traditional machine learning based methods are extensively used in detecting cyber threats, but they hardly model the correlations between real-world cyber entities. In recent years, with the proliferation of graph mining techniques, many researchers have investigated these techniques for capturing correlations between cyber entities and achieving high performance. It is imperative to summarize existing graph-based cybersecurity solutions to provide a guide for future studies. Therefore, as a key contribution of this work, we provide a comprehensive review of graph mining for cybersecurity, including an overview of cybersecurity tasks, the typical graph mining techniques, and the general process of applying them to cybersecurity, as well as various solutions for different cybersecurity tasks. For each task, we probe into relevant methods and highlight the graph types, graph approaches, and task levels in their modeling. Furthermore, we collect open datasets and toolkits for graph-based cybersecurity. Finally, we present an outlook on the potential directions of this field for future research.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cyber attack; Cybersecurity; graph embedding; graph mining; graph neural network,Crime; Data mining; Graph embeddings; Graph neural networks; Graphic methods; Malware; Network security; Cyber security; Cyber-attacks; Cyberspaces; Explosive growth; Graph embeddings; Graph mining; Graph neural networks; Graph-based; Malwares; Mining techniques; Cybersecurity
Open-World Graph Active Learning for Node Classification,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177880850&doi=10.1145%2f3607144&partnerID=40&md5=58ca672234160440cd61c720ced4354a,"The great power of Graph Neural Networks (GNNs) relies on a large number of labeled training data, but obtaining the labels can be costly in many cases. Graph Active Learning (GAL) is proposed to reduce such annotation costs, but the existing methods mainly focus on improving labeling efficiency with fixed classes, and are limited to handle the emergence of novel classes. We term the problem as Open-World Graph Active Learning (OWGAL) and propose a framework of the same name. The key is to recognize novel-class as well as informative nodes in a unified framework. Instead of a fully connected neural network classifier, OWGAL employs prototype learning and label propagation to assign high uncertainty scores to the targeted nodes in the representation and topology space, respectively. Weighted sampling further suppresses the impact of unimportant classes by weighing both the node and class importance. Experimental results on four large-scale datasets demonstrate that our framework achieves a substantial improvement of 5.97% to 16.57% on Macro-F1 over state-of-the-art methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph active learning; open-world learning; prototype learning,Backpropagation; Data mining; Graph neural networks; Graph theory; Importance sampling; Active Learning; Graph active learning; Graph neural networks; Labeled training data; Labeling efficiencies; Open world; Open-world learning; Power; Prototype learning; Unified framework; Large dataset
FedEgo: Privacy-preserving Personalized Federated Graph Learning with Ego-graphs,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177861079&doi=10.1145%2f3624017&partnerID=40&md5=0b2d1b24dd2e8c12cd8145e3d7846e75,"As special information carriers containing both structure and feature information, graphs are widely used in graph mining, e.g., Graph Neural Networks (GNNs). However, graph data are stored separately in multiple distributed parties in some practical scenarios, which may not be directly shared due to conflicts of interest. Hence, federated graph neural networks are proposed to address such data silo issues while preserving each party's privacy (or client). Nevertheless, different graph data distributions of various parties, which is known as the statistical heterogeneity, may degrade the performance of naive federated learning algorithms like FedAvg. In this article, we propose FedEgo, a federated graph learning framework based on ego-graphs to tackle the challenges above, in which each client will train their local models while also contributing to the training of a global model. FedEgo applies GraphSAGE over ego-graphs to make full use of the structure information and utilizes Mixup for privacy concerns. To deal with the statistical heterogeneity, we integrate personalization into learning and propose an adaptive mixing coefficient strategy that enables clients to achieve their optimal personalization. Extensive experimental results and in-depth analysis demonstrate the effectiveness of FedEgo.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Ego-graphs; graph neural network; personalized federated learning,Data mining; Graph algorithms; Graph structures; Graphic methods; Learning algorithms; Privacy-preserving techniques; Ego-graph; Feature information; Graph data; Graph neural networks; Information carriers; Personalizations; Personalized federated learning; Privacy preserving; Statistical heterogeneities; Structure information; Graph neural networks
Attacking Shortest Paths by Cutting Edges,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177832266&doi=10.1145%2f3622941&partnerID=40&md5=c6d98deee1c9dda30478ec2cc4b67d5f,"Identifying shortest paths between nodes in a network is a common graph analysis problem that is important for many applications involving routing of resources. An adversary that can manipulate the graph structure could alter traffic patterns to gain some benefit (e.g., make more money by directing traffic to a toll road). This article presents the Force Path Cut problem, in which an adversary removes edges from a graph to make a particular path the shortest between its terminal nodes. We prove that the optimization version of this problem is APX-hard but introduce PATHATTACK, a polynomial-time approximation algorithm that guarantees a solution within a logarithmic factor of the optimal value. In addition, we introduce the Force Edge Cut and Force Node Cut problems, in which the adversary targets a particular edge or node, respectively, rather than an entire path. We derive a nonconvex optimization formulation for these problems and derive a heuristic algorithm that uses PATHATTACK as a subroutine. We demonstrate all of these algorithms on a diverse set of real and synthetic networks, illustrating where the proposed algorithms provide the greatest improvement over baseline methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adversarial graph analysis; APX-hardness; integer programming,Approximation algorithms; Graph theory; Heuristic algorithms; Polynomial approximation; Toll highways; Adversarial graph analyse; Analysis problems; APX-Hardness; Cutting edges; Graph analysis; Graph structures; Integer Program- ming; Routings; Short-path; Traffic pattern; Integer programming
Adversary for Social Good: Leveraging Adversarial Attacks to Protect Personal Attribute Privacy,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177884624&doi=10.1145%2f3614098&partnerID=40&md5=6293a7c439462d31c163dd36a97fbfed,"Social media has drastically reshaped the world that allows billions of people to engage in such interactive environments to conveniently create and share content with the public. Among them, text data (e.g., tweets, blogs) maintains the basic yet important social activities and generates a rich source of user-oriented information. While those explicit sensitive user data like credentials have been significantly protected by all means, personal private attribute (e.g., age, gender, location) disclosure due to inference attacks is somehow challenging to avoid, especially when powerful natural language processing (NLP) techniques have been effectively deployed to automate attribute inferences from implicit text data. This puts users' attribute privacy at risk. To address this challenge, in this article, we leverage the inherent vulnerability of machine learning to adversarial attacks, and design a novel text-space Adversarial attack for Social Good, called Adv4SG. In other words, we cast the problem of protecting personal attribute privacy as an adversarial attack formulation problem over the social media text data to defend against NLP-based attribute inference attacks. More specifically, Adv4SG proceeds with a sequence of word perturbations under given constraints such that the probed attribute cannot be identified correctly. Different from the prior works, we advance Adv4SG by considering social media property, and introducing cost-effective mechanisms to expedite attribute obfuscation over text data under the black-box setting. Extensive experiments on real-world social media datasets have demonstrated that our method can effectively degrade the inference accuracy with less computational cost over different attribute settings, which substantially helps mitigate the impacts of inference attacks and thus achieve high performance in user attribute privacy protection.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial text attack; attribute privacy; inference attack; Social media; text data,Data obfuscation; Natural language processing systems; Sensitive data; Social networking (online); Adversarial text attack; Attribute privacy; Inference attacks; Interactive Environments; Natural languages; Personal attributes; Social activities; Social media; Text data; User oriented; Cost effectiveness
Feature Selection for Efficient Local-to-global Bayesian Network Structure Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177880311&doi=10.1145%2f3624479&partnerID=40&md5=1f63b10f22128344696417b6f8570f1e,"Local-to-global learning approach plays an essential role in Bayesian network (BN) structure learning. Existing local-to-global learning algorithms first construct the skeleton of a DAG (directed acyclic graph) by learning the MB (Markov blanket) or PC (parents and children) of each variable in a dataset, then orient edges in the skeleton. However, existing MB or PC learning methods are often computationally expensive especially with a large-sized BN, resulting in inefficient local-to-global learning algorithms. To tackle the problem, in this article, we link feature selection with local BN structure learning and develop an efficient local-to-global learning approach using filtering feature selection. Specifically, we first analyze the rationale of the well-known Minimum-Redundancy and Maximum-Relevance (MRMR) feature selection approach for learning a PC set of a variable. Based on the analysis, we propose an efficient F2SL (feature selection-based structure learning) approach to local-to-global BN structure learning. The F2SL approach first employs the MRMR approach to learn the skeleton of a DAG, then orients edges in the skeleton. Employing independence tests or score functions for orienting edges, we instantiate the F2SL approach into two new algorithms, F2SL-c (using independence tests) and F2SL-s (using score functions). Compared to the state-of-the-art local-to-global BN learning algorithms, the experiments validated that the proposed algorithms in this article are more efficient and provide competitive structure learning quality than the compared algorithms.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bayesian network; feature selection; local-to-global structure learning; markov blanket,Barium compounds; Directed graphs; Feature Selection; Learning algorithms; Learning systems; Musculoskeletal system; Bayesia n networks; Bayesian network structure; Features selection; Global learning; Global structure; Learning approach; Local-to-global structure learning; Markov Blankets; Network structure learning; Structure-learning; Bayesian networks
Exploiting Conversation-Branch-Tweet HyperGraph Structure to Detect Misinformation on Social Media,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177816656&doi=10.1145%2f3610297&partnerID=40&md5=28bd54f136f1e0de5a6082d200903a26,"The spread of misinformation on social media is a serious issue that can have negative consequences for public health and political stability. While detecting and identifying misinformation can be challenging, many attempts have been made to address this problem. However, traditional models that focus on pairwise relationships on misinformation propagation paths may not be effective in capturing the underlying connections among multiple tweets. To address this limitation, the proposed ""Conversation-Branch-Tweet""hypergraph convolutional network (CBT-HGCN) uses a hypergraph to represent the internal structure and content of tweet data, with tweets and their replies viewed as nodes and hyperedges, respectively. The model first pre-processes the tweets of a conversation and then uses a pre-trained model as an encoder to extract node information. Finally, a hypergraph convolution network is used as an information fuser for classification. Experimental results on three benchmark datasets (Twitter15, Twitter16, and Pheme) show that the proposed model outperforms several strong baseline models and achieves state-of-the-art performance. This indicates that the CBT-HGCN approach is effective in detecting and identifying misinformation on social media by capturing the underlying connections among multiple tweets.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph neural network; hypergraph modeling; Misinformation detection; social media,Backpropagation; Benchmarking; Classification (of information); Convolution; Graph theory; Social networking (online); Convolutional networks; Graph neural networks; Hyper graph; Hypergraph model; Mis-information propagation; Misinformation detection; Political stability; Propagation paths; Social media; Traditional models; Graph neural networks
REST: Debiased Social Recommendation via Reconstructing Exposure Strategies,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177818155&doi=10.1145%2f3624986&partnerID=40&md5=b8a758d5264c40aa576f859a70261b24,"The recommendation system, relying on historical observational data to model the complex relationships among users and items, has achieved great success in real-world applications. Selection bias is one of the most important issues of the existing observational data-based approaches, which is actually caused by multiple types of unobserved exposure strategies (e.g., promotions and holiday effects). Though various methods have been proposed to address this problem, they are mainly relying on the implicit debiasing techniques but not explicitly modeling the unobserved exposure strategies. By explicitly Reconstructing Exposure STrategies (REST), we formalize the recommendation problem as the counterfactual reasoning and propose the debiased social recommendation method. In REST, we assume that the exposure of an item is controlled by the latent exposure strategies, the user, and the item. Based on the above generation process, we first provide the theoretical guarantee of our method via identification analysis. Second, we employ a variational auto-encoder to reconstruct the latent exposure strategies, with the help of the social networks and the items. Third, we devise a counterfactual reasoning based recommendation algorithm by leveraging the recovered exposure strategies. Experiments on four real-world datasets, including three published datasets and one private WeChat Official Account dataset, demonstrate significant improvements over several state-of-the-art methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",causal effect; Recommendation system; social recommendation system; variational auto-encoders,Data mining; Signal encoding; User profile; Auto encoders; Causal effect; Complex relationships; Counterfactuals; De-biasing; Observational data; Real-world; Selection bias; Social recommendation system; Variational auto-encoder; Recommender systems
Structure-Driven Representation Learning for Deep Clustering,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176791717&doi=10.1145%2f3623400&partnerID=40&md5=b967346f49bebe4731e801edeaf47009,"As an important branch of unsupervised learning methods, clustering makes a wide contribution in the area of data mining. It is well known that capturing the group-discriminative properties of each sample for clustering is crucial. Among them, deep clustering delivers promising results due to the strong representational power of neural networks. However, most of them adopt sample-level learning strategies, and the standalone data point barely captures its holistic cluster's context and may undergo sub-optimal cluster assignment. To tackle this issue, we propose a Structure-driven Representation Learning (SRL) method by introducing latent structure information into the representation learning process at both the local and global levels. Specifically, a local-structure-driven sample representation strategy is proposed to approximate the estimation of data distribution, which models the neighborhood distribution of samples with potential structure information and exploits statistical dependencies between them to improve cluster consistency. A global-structure-driven cluster representation strategy is designed, where the context of each cluster is sufficiently encoded according to its samples (exemplar-theory) and corresponding prototype (prototype-theory). In this case, each cluster can only be related to its most similar samples, and different clusters are separated as much as possible. These two models are seamlessly combined into a joint optimization problem, which can be efficiently solved. Experiments on six widely-used datasets demonstrate the superiority of SRL over state-of-the-art clustering methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",PhrasesDeep clustering; structure-driven representation learning,Cluster analysis; Deep learning; Learning systems; Unsupervised learning; Clusterings; Datapoints; Learning strategy; Neural-networks; Phrasesdeep clustering; Power; Property; Structure information; Structure-driven representation learning; Unsupervised learning method; Data mining
MEGA: Meta-Graph Augmented Pre-Training Model for Knowledge Graph Completion,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176404942&doi=10.1145%2f3617379&partnerID=40&md5=947a1ac6160da8cd2dab2f1b339dd5b4,"Nowadays, a large number of Knowledge Graph Completion (KGC) methods have been proposed by using embedding based manners, to overcome the incompleteness problem faced with knowledge graph (KG). One important recent innovation in Natural Language Processing (NLP) domain is the employ of deep neural models that make the most of pre-training, culminating in BERT, the most popular example of this line of approaches today. Recently, a series of new KGC methods introducing a pre-trained language model, such as KG-BERT, have been developed and released compelling performance. However, previous pre-training based KGC methods usually train the model by using simple training task and only utilize one-hop relational signals in KG, which leads that they cannot model high-order semantic contexts and multi-hop complex relatedness. To overcome this problem, this article presents a novel pre-training framework for KGC task, which especially consists of both one-hop relation level task (low-order) and multi-hop meta-graph level task (high-order). Hence, the proposed method can capture not only the elaborate sub-graph structure but also the subtle semantic information on the given KG. The empirical results show the efficiency of the proposed method on the widely used real-world datasets. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Knowledge graph completion; meta-graph; multi-task learning; pre-training model; semantic enhancement,Deep learning; Learning algorithms; Learning systems; Natural language processing systems; Semantics; Completion methods; High-order; Knowledge graph completion; Knowledge graphs; Meta-graph; Multitask learning; Pre-training; Pre-training model; Semantic enhancements; Training model; Knowledge graph
Discovering Interesting Patterns from Hypergraphs,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176737699&doi=10.1145%2f3622940&partnerID=40&md5=0f56c2f8d37ac252ea36abdd93c11854,"A hypergraph is a complex data structure capable of expressing associations among any number of data entities. Overcoming the limitations of traditional graphs, hypergraphs are useful to model real-life problems. Frequent pattern mining is one of the most popular problems in data mining with a lot of applications. To the best of our knowledge, there exists no flexible pattern mining framework for hypergraph databases decomposing associations among data entities. In this article, we propose a flexible and complete framework for mining frequent patterns from a collection of hypergraphs. To discover more interesting patterns beyond the traditional frequent patterns, we propose frameworks for weighted and uncertain hypergraph mining also. We develop three algorithms for mining frequent, weighted, and uncertain hypergraph patterns efficiently by introducing a canonical labeling technique for isomorphic hypergraphs. Extensive experiments have been conducted on real-life hypergraph databases to show both the effectiveness and efficiency of our proposed frameworks and algorithms.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Data mining; frequent pattern mining; graph mining; hypergraph; uncertain pattern mining; weighted pattern mining,Database systems; Graph theory; Complex data structures; Data entities; Frequent patterns minings; Graph mining; Hyper graph; Pattern mining; Uncertain pattern mining; Weighted pattern mining; Weighted patterns; Data mining
Modeling Users' Curiosity in Recommender Systems,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174875145&doi=10.1145%2f3617598&partnerID=40&md5=ae759f04234f46098d3772a08ffa2ed4,"Today's recommender systems are criticized for recommending items that are too obvious to arouse users' interests. Therefore, the research community has advocated some ""beyond accuracy""evaluation metrics such as novelty, diversity, and serendipity with the hope of promoting information discovery and sustaining users' interests over a long period of time. While bringing in new perspectives, most of these evaluation metrics have not considered individual users' differences in their capacity to experience those ""beyond accuracy""items. Open-minded users may embrace a wider range of recommendations than conservative users. In this article, we proposed to use curiosity traits to capture such individual users' differences. We developed a model to approximate an individual's curiosity distribution over different stimulus levels. We used an item's surprise level to estimate the stimulus level and whether such a level is in the range of the user's appetite for stimulus, called Comfort Zone. We then proposed a recommender system framework that considers both user preference and their Comfort Zone where the curiosity is maximally aroused. Our framework differs from a typical recommender system in that it leverages human's Comfort Zone for stimuli to promote engagement with the system. A series of evaluation experiments have been conducted to show that our framework is able to rank higher the items with not only high ratings but also high curiosity stimulation. The recommendation list generated by our algorithm has a higher potential of inspiring user curiosity compared to the state-of-the-art deep learning approaches. The personalization factor for assessing the surprise stimulus levels further helps the recommender model achieve smaller (better) inter-user similarity.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",curiosity; deep learning; Recommender systems; surprise,Deep learning; Learning systems; User profile; Accuracy evaluation; Comfort zone; Curiosity; Deep learning; Evaluation metrics; Information discovery; Research communities; Stimulus levels; Surprize; Users' interests; Recommender systems
A Clique-Querying Mining Framework for Discovering High Utility Co-Location Patterns without Generating Candidates,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176773397&doi=10.1145%2f3617378&partnerID=40&md5=a4492151704f4786c54c4e095eaaebb1,"Groups of spatial features whose instances frequently appear together in nearby areas are regarded as prevalent co-location patterns (PCPs). Traditional PCP mining ignores the significance of instances and features. However, in reality, these instances and features have different significance, the traditional PCPs may not sufficiently expose knowledge from spatial data. This study focuses on discovering high utility co-location patterns (HUCPs) in which each instance is assigned a utility to reflect its significance. To filter HUCPs, an adaptive utility participation index (UPI) is designed. Unfortunately, the UPI does not hold the downward closure property. The performance of mining HUCPs is very inefficient since unnecessary candidates cannot be early pruned. Thus, an efficient clique-querying mining framework is devised without generating candidates. This framework first divides neighboring instances into cliques, then compacts these cliques into a hash table structure. Next, the adaptive UPI of any patterns can be quickly calculated based on their participating instances that are obtained by executing a querying scheme on the hash table. Finally, HUCPs are filtered efficiently. The effectiveness and efficiency of the proposed method are proved in both theory and experiments to make a promise that the patterns mined are more meaningful and the mining performance is significantly improved compared to the previous methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",High utility co-location pattern; participating instance; querying scheme; spatial clique,Data structures; Co-location patterns; Hash table; High utility co-location pattern; Participating instance; Participation index; Pattern mining; Performance; Querying scheme; Spatial clique; Spatial features; Location
TDAN: Transferable Domain Adversarial Network for Link Prediction in Heterogeneous Social Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170401791&doi=10.1145%2f3610229&partnerID=40&md5=d94a8ec01967a284a9ace244eaf7648c,"Link prediction has received increased attention in social network analysis. One of the unique challenges in heterogeneous social networks is link prediction in new link types without verified link information, such as recommending products to new overseas groups. Existing link prediction models tend to learn type-specific knowledge on specific link types and predict missing or future links on the same link types. However, because of the uncertainty of new link types in the evolving process of social networks, it is difficult to collect sufficient verified link information in new link types. Therefore, we propose the Transferable Domain Adversarial Network (TDAN) based on transfer learning to handle the challenge. TDAN exploits transferable type-shared knowledge in historical link types to help predict the unobserved links in new link types. TDAN mainly comprises a structural encoder, a domain discriminator, and an optimization decoder. The structural encoder learns the link representations in a heterogeneous social network. Subsequently, to learn transferable type-shared knowledge, the domain discriminator distinguishes link representations into different link types while minimizing the differences between type-specific knowledge in adversarial training. Inspired by the denoising auto-encoder, the optimization decoder reconstructs the learned type-shared knowledge to eliminate the noise generated during the adversarial training. Extensive experiments on Facebook and YouTube show that TDAN can outperform the state-of-the-art models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",link prediction; Transfer learning; type-shared knowledge,Decoding; Forecasting; Signal encoding; Social networking (online); Adversarial networks; Learn+; Link informations; Link prediction; Optimisations; Prediction modelling; Social Network Analysis; Specific knowledge; Transfer learning; Type-shared knowledge; Knowledge management
A Dynamic Attributes-driven Graph Attention Network Modeling on Behavioral Finance for Stock Prediction,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176427121&doi=10.1145%2f3611311&partnerID=40&md5=086d21a803449672de64dee296018c27,"Stock prediction is a challenging task due to multiple influencing factors and complex market dependencies. Traditional solutions are based on a single type of information. With the success of multi-source information in different fields, the combination of different types of information such as numerical and textual information has become a promising option.Although multi-source information provides rich multi-view information, how to mine and construct structured relationships from them is a difficult problem. Specifically, most existing methods usually extract features from commonly used multi-source information as predictive information sources, without further pre-constructing stock relationship graphs with dependencies using broader information. More importantly, they typically treat each stock as an isolated forecasting, or employ stock market correlations based on a fixed predefined graph structure, but current methods are not sensitive enough to aggregate the attribute features extracted from multi-source information and stock relationship graph, to obtain the dynamic update of market relations and relationship strength. The stock market is highly temporally, and the attributes of nodes are affected by the time perception of other attributes, which is not fully considered.To address these problems, we propose a novel dynamic attributes-driven graph attention networks incorporating sentiment (DGATS) information, transaction data, and text data. Inspired by behavioral finance, we separately extract sentiment information as a factor of technical indicators, and further realize the early fusion of technical indicators and textual data through Kronecker product-based tensor fusion. In particular, by LSTM and temporal attention network, the short-term and long-term transition features are gradually grasped from the local composition of the fused stock trading sequence. Furthermore, real-time intra-market dependencies and key attributes information are captured with graph networks, enabling dynamic updates of relationships and relationship strengths in predefined graphs. Experiments on the real datasets show that the architecture can outperform the previous methods in prediction performance. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",behavioral finance; dynamic relationship; Knowledge graph; stock prediction,Commerce; Data mining; Electronic trading; Financial markets; Forecasting; Graph neural networks; Graph structures; Graphic methods; Long short-term memory; Behavioural finances; Dynamic attributes; Dynamic relationship; Dynamic update; Knowledge graphs; Multi-source informations; Network models; Relationship graphs; Stock predictions; Technical indicator; Knowledge graph
An Efficient Transfer Learning Method with Auxiliary Information,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176773606&doi=10.1145%2f3612930&partnerID=40&md5=6b22f3f32629f254d33fcdcdfeab430f,"Transfer learning (TL) is an information reuse learning tool, which can help us learn better classification effect than traditional single task learning, because transfer learning can share information within the task-to-task model. Most TL algorithms are studied in the field of data improvement, doing some data extraction and transformation. However, it ignores that existing the additional information to improve the model's accuracy, like Universum samples in the training data with privileged information. In this article, we focus on considering prior data to improve the TL algorithm, and the additional features also called privileged information are incorporated into the learning to improve the learning paradigm. In addition, we also carry out the Universum samples which do not belong to any indicated categories into the transfer learning paradigm to improve the utilization of prior knowledge. We propose a new TL Model (PU-TLSVM), in which each task with corresponding privileged features and Universum data is considered in the proposed model, so as to apply tasks with a priori data to the training stage. Then, we use Lagrange duality theorem to optimize our model to obtain the optimal discriminant for target task classification. Finally, we make a lot of predictions and tests to compare the actual effectiveness of the proposed method with the previous methods. The experiment results indicate that the proposed method is more effective and robust than other baselines.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",privileged learning; Transfer learning; Universum learning,Classification (of information); Information use; Learning algorithms; Learning systems; Auxiliary information; Information reuse; Learn+; Learning paradigms; Learning tool; Privileged learning; Single task learning; Transfer learning; Transfer learning methods; Universum learning; Metadata
Revisiting the Role of Heterophily in Graph Representation Learning: An Edge Classification Perspective,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176784454&doi=10.1145%2f3603378&partnerID=40&md5=cd70a55c7f89246232f0ab46fb13bea6,"Graph representation learning aims at integrating node contents with graph structure to learn nodes/graph representations. Nevertheless, it is found that many existing graph learning methods do not work well on data with high heterophily level that accounts for a large proportion of edges between different class labels. Recent efforts to this problem focus on improving the message passing mechanism. However, it remains unclear whether heterophily truly does harm to the performance of graph neural networks (GNNs). The key is to unfold the relationship between a node and its immediate neighbors, e.g., are they heterophilous or homophilious? From this perspective, here we study the role of heterophily in graph representation learning before/after the relationships between connected nodes are disclosed. In particular, we propose an end-to-end framework that both learns the type of edges (i.e., heterophilous/homophilious) and leverage edge type information to improve the expressiveness of graph neural networks. We implement this framework in two different ways. Specifically, to avoid messages passing through heterophilous edges, we can optimize the graph structure to be homophilious by dropping heterophilous edges identified by an edge classifier. Alternatively, it is possible to exploit the information about the presence of heterophilous neighbors for feature learning, so a hybrid message passing approach is devised to aggregate homophilious neighbors and diversify heterophilous neighbors based on edge classification. Extensive experiments demonstrate the remarkable performance improvement of GNNs with the proposed framework on multiple datasets across the full spectrum of homophily level.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",edge type; Graph neural networks; heterophily; hybrid message passing,Classification (of information); Graph theory; Graphic methods; Learning systems; Message passing; Edge classification; Edge type; Graph neural networks; Graph representation; Graph structures; Heterophily; Hybrid message passing; Learn+; Message-passing; Performance; Graph neural networks
Same or Different? Diff-Vectors for Authorship Analysis,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176808590&doi=10.1145%2f3609226&partnerID=40&md5=a3206bafe67090c872d1963b8722b9ae,"In this article, we investigate the effects on authorship identification tasks (including authorship verification, closed-set authorship attribution, and closed-set and open-set same-author verification) of a fundamental shift in how to conceive the vectorial representations of documents that are given as input to a supervised learner. In ""classic""authorship analysis, a feature vector represents a document, the value of a feature represents (an increasing function of) the relative frequency of the feature in the document, and the class label represents the author of the document. We instead investigate the situation in which a feature vector represents an unordered pair of documents, the value of a feature represents the absolute difference in the relative frequencies (or increasing functions thereof) of the feature in the two documents, and the class label indicates whether the two documents are from the same author or not. This latter (learner-independent) type of representation has been occasionally used before, but has never been studied systematically. We argue that it is advantageous, and that, in some cases (e.g., authorship verification), it provides a much larger quantity of information to the training process than the standard representation. The experiments that we carry out on several publicly available datasets (among which one that we here make available for the first time) show that feature vectors representing pairs of documents (that we here call Diff-Vectors) bring about systematic improvements in the effectiveness of authorship identification tasks, and especially so when training data are scarce (as it is often the case in real-life authorship identification scenarios). Our experiments tackle same-author verification, authorship verification, and closed-set authorship attribution; while DVs are naturally geared for solving the 1st, we also provide two novel methods for solving the 2nd and 3rd that use a solver for the 1st as a building block. The code to reproduce our experiments is open-source and available online.1  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",authorship analysis; Supervised learning; vector-based representations,Open systems; Supervised learning; Authorship analysis; Authorship attribution; Authorship identification; Authorship verification; Class labels; Closed set; Features vector; Increasing functions; Relative frequencies; Vector-based representations; Vectors
Self-Supervised Dynamic Graph Representation Learning via Temporal Subgraph Contrast,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176809558&doi=10.1145%2f3612931&partnerID=40&md5=dd238375a891b38d5c00e547dce741c4,"Self-supervised learning on graphs has recently drawn a lot of attention due to its independence from labels and its robustness in representation. Current studies on this topic mainly use static information such as graph structures but cannot well capture dynamic information such as timestamps of edges. Realistic graphs are often dynamic, which means the interaction between nodes occurs at a specific time. This article proposes a self-supervised dynamic graph representation learning framework DySubC, which defines a temporal subgraph contrastive learning task to simultaneously learn the structural and evolutional features of a dynamic graph. Specifically, a novel temporal subgraph sampling strategy is firstly proposed, which takes each node of the dynamic graph as the central node and uses both neighborhood structures and edge timestamps to sample the corresponding temporal subgraph. The subgraph representation function is then designed according to the influence of neighborhood nodes on the central node after encoding the nodes in each subgraph. Finally, the structural and temporal contrastive loss are defined to maximize the mutual information between node representation and temporal subgraph representation. Experiments on five real-world datasets demonstrate that (1) DySubC performs better than the related baselines including two graph contrastive learning models and five dynamic graph representation learning models, especially in the link prediction task, and (2) the use of temporal information cannot only sample more effective subgraphs, but also learn better representation by temporal contrastive loss.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dynamic graph representation learning; Self-supervised learning; temporal subgraph contrast,Graph theory; Graphic methods; Information use; Learning systems; Central nodes; Dynamic graph; Dynamic graph representation learning; Graph representation; Learn+; Learning models; Self-supervised learning; Subgraphs; Temporal subgraph contrast; Time-stamp; Supervised learning
Criterion-based Heterogeneous Collaborative Filtering for Multi-behavior Implicit Recommendation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176387767&doi=10.1145%2f3611310&partnerID=40&md5=a3b378039d43766ce99018d12dfa952d,"Recent years have witnessed the explosive growth of interaction behaviors in multimedia information systems, where multi-behavior recommender systems have received increasing attention by leveraging data from various auxiliary behaviors such as tip and collect. Among various multi-behavior recommendation methods, non-sampling methods have shown superiority over negative sampling methods. However, two observations are usually ignored in existing state-of-the-art non-sampling methods based on binary regression: (1) users have different preference strengths for different items, so they cannot be measured simply by binary implicit data; (2) the dependency across multiple behaviors varies for different users and items. To tackle the above issue, we propose a novel non-sampling learning framework named Criterion-guided Heterogeneous Collaborative Filtering (CHCF). CHCF introduces both upper and lower thresholds to indicate selection criteria, which will guide user preference learning. Besides, CHCF integrates criterion learning and user preference learning into a unified framework, which can be trained jointly for the interaction prediction of the target behavior. We further theoretically demonstrate that the optimization of Collaborative Metric Learning can be approximately achieved by the CHCF learning framework in a non-sampling form effectively. Extensive experiments on three real-world datasets show the effectiveness of CHCF in heterogeneous scenarios. © 2023 Copyright held by the owner/author(s).",Collaborative filtering; implicit feedback; multi-behavior recommendation; neural networks,Explosive growth; Implicit feedback; Interaction behavior; Learning frameworks; Multi-behavior recommendation; MultiMedia Information Systems; Neural-networks; Preference learning; Sampling method; User's preferences; Collaborative filtering
DeepCPR: Deep Path Reasoning Using Sequence of User-Preferred Attributes for Conversational Recommendation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176793355&doi=10.1145%2f3610775&partnerID=40&md5=b08e7b6396019292ef5e1f31d6482826,"Conversational recommender systems (CRS) have garnered significant attention in academia and industry because of their ability to capture user preferences via system questions and user responses. Typically, in a CRS, reinforcement learning (RL) is utilized to determine the optimal timing for requesting attribute information or suggesting items. However, existing methods consider user-preferred attributes independently and ignore that attributes may be of different importance to the same user, in the attribute and item selection phases, which limits the accuracy and interpretability of CRS. Inspired by this, we propose deep conversational path reasoning (DeepCPR), which involves constructing a reasoning path on a graph with a series of user-favored attributes. It utilizes the attention mechanism to thoroughly examine the connections between these attributes and provide improved explanations for which attributes to inquire about or which items to recommend. In DeepCPR, two deep-learning-based modules are proposed to realize attribute and item selection. In the first module, the sequence of attributes confirmed by the user in conversation is encoded with a gated graph neural network to obtain the user's long-term preference using a self-attention mechanism for the selection of candidate attributes. In the second module, a self-attention approach with more appropriate strategies is developed to dynamically select candidate items. In addition, to achieve fine-grained user preference modeling, a recurrent neural network is employed to aggregate the sequence of attributes that interact with the users. Numerous experimental evaluations conducted on four real CRS datasets show that the proposed method significantly outperforms existing advanced methods in terms of conversational recommendations.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Conversational recommender system; graph reasoning; reinforcement learning; self-attention mechanism,Graph neural networks; Graph theory; Recommender systems; Recurrent neural networks; User profile; Attention mechanisms; Attribute selection; Conversational recommendations; Conversational recommender systems; Graph reasoning; Item selection; Optimal timing; Reinforcement learnings; Self-attention mechanism; User's preferences; Reinforcement learning
Improving Node Classification Accuracy of GNN through Input and Output Intervention,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176763279&doi=10.1145%2f3610535&partnerID=40&md5=9132f976dd1c4c6af9826e4c6aa72a79,"Graph Neural Networks (GNNs) are a popular machine learning framework for solving various graph processing applications. This framework exploits both the graph topology and the feature vectors of the nodes. One of the important applications of GNN is in the semi-supervised node classification task. The accuracy of the node classification using GNN depends on (i) the number and (ii) the choice of the training nodes. In this article, we demonstrate that increasing the training nodes by selecting nodes from the same class that are spread out across non-contiguous subgraphs, can significantly improve the accuracy. We accomplish this by presenting a novel input intervention technique that can be used in conjunction with different GNN classification methods to increase the non-contiguous training nodes and, thereby, improve the accuracy. We also present an output intervention technique to identify misclassified nodes and relabel them with their potentially correct labels. We demonstrate on real-world networks that our proposed methods, both individually and collectively, significantly improve the accuracy in comparison to the baseline GNN algorithms. Both our methods are agnostic. Apart from the initial set of training nodes generated by the baseline GNN methods, our techniques do not need any other extra knowledge about the classes of the nodes. Thus, our methods are modular and can be used as pre-and post-processing steps with many of the currently available GNN methods to improve their accuracy.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",DeepWalk; GNN; K-means; K-NN; PaRWalk,Data mining; Graph theory; K-means clustering; Nearest neighbor search; Classification accuracy; Deepwalk; Graph neural networks; Input and outputs; K-means; K-NN; Learning frameworks; Machine-learning; Neural network method; Parwalk; Graph neural networks
Laplacian-based Cluster-Contractive t-SNE for High-Dimensional Data Visualization,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176935393&doi=10.1145%2f3612932&partnerID=40&md5=1bb26456ab098f71ba6d5d66fc5e704f,"Dimensionality reduction techniques aim at representing high-dimensional data in low-dimensional spaces to extract hidden and useful information or facilitate visual understanding and interpretation of the data. However, few of them take into consideration the potential cluster information contained implicitly in the high-dimensional data. In this article, we propose LaptSNE, a new graph-layout nonlinear dimensionality reduction method based on t-SNE, one of the best techniques for visualizing high-dimensional data as 2D scatter plots. Specifically, LaptSNE leverages the eigenvalue information of the graph Laplacian to shrink the potential clusters in the low-dimensional embedding when learning to preserve the local and global structure from high-dimensional space to low-dimensional space. It is nontrivial to solve the proposed model because the eigenvalues of normalized symmetric Laplacian are functions of the decision variable. We provide a majorization-minimization algorithm with convergence guarantee to solve the optimization problem of LaptSNE and show how to calculate the gradient analytically, which may be of broad interest when considering optimization with Laplacian-composited objective. We evaluate our method by a formal comparison with state-of-the-art methods on seven benchmark datasets, both visually and via established quantitative measurements. The results demonstrate the superiority of our method over baselines such as t-SNE and UMAP. We also provide out-of-sample extension, large-scale extension, and mini-batch extension for our LaptSNE to facilitate dimensionality reduction in various scenarios. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDimensionality reduction; data visualization; graph Laplacian; t-SNE,Clustering algorithms; Data mining; Eigenvalues and eigenfunctions; Laplace transforms; Reduction; Visualization; % reductions; Additional key word and phrasesdimensionality reduction; Eigen-value; Graph Laplacian; High dimensional data; High dimensional data visualization; Key words; Laplacians; Low-dimensional spaces; T-SNE; Data visualization
A Survey on Explainable Anomaly Detection,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176731972&doi=10.1145%2f3609333&partnerID=40&md5=ccfda044c2bd299abe66cd80c482d2aa,"In the past two decades, most research on anomaly detection has focused on improving the accuracy of the detection, while largely ignoring the explainability of the corresponding methods and thus leaving the explanation of outcomes to practitioners. As anomaly detection algorithms are increasingly used in safety-critical domains, providing explanations for the high-stakes decisions made in those domains has become an ethical and regulatory requirement. Therefore, this work provides a comprehensive and structured survey on state-of-the-art explainable anomaly detection techniques. We propose a taxonomy based on the main aspects that characterise each explainable anomaly detection technique, aiming to help practitioners and researchers find the explainable anomaly detection method that best suits their needs. © 2023 Copyright held by the owner/author(s).",anomaly detection; anomaly explanation; explainable artificial intelligence; explainable machine learning; interpretable anomaly detection; outlier detection; PhrasesExplainable anomaly detection,Machine learning; Safety engineering; Anomaly detection; Anomaly explanation; Explainable artificial intelligence; Explainable machine learning; Interpretable anomaly detection; Machine-learning; Outlier Detection; Phrasesexplainable anomaly detection; Anomaly detection
Distributed Cooperative Coevolution of Data Publishing Privacy and Transparency,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173622978&doi=10.1145%2f3613962&partnerID=40&md5=48b608381b28af5eea53337e2e0e3a78,"Data transparency is beneficial to data participants' awareness, users' fairness, and research work's reproducibility. However, when addressing transparency requirements, we cannot ignore data privacy. This article defines the multi-objective data publishing (MODP) problem, optimizing data privacy and transparency at the same time. Accordingly, we propose a distributed cooperative coevolutionary genetic algorithm (DCCGA) to optimize the MODP problem. In the population of DCCGA, each individual represents an anonymization solution to MODP. Three modules in DCCGA, i.e., grouping module, cooperative coevolutionary module, and evolving module, are proposed for distributed sub-population update and evaluation, improving DCCGA's optimization performance and parallel efficiency. Moreover, a matrix-based crossover operator and a matrix-based mutation operator are designed to exchange and adjust anonymization information in the individuals efficiently. Experimental results demonstrate that the proposed DCCGA outperforms the competitors with respect to solution accuracy, convergence speed, and scalability. Besides, we verify the effectiveness of all the proposed components in DCCGA.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cooperative coevolution; data privacy and transparency; genetic algorithm; Large-scale multi-objective optimization,Data privacy; Genetic algorithms; Matrix algebra; Multiobjective optimization; Anonymization; Cooperative co-evolution; Cooperative coevolutionary genetic algorithms; Data privacy and transparency; Data publishing; Large-scale multi-objective optimization; Large-scales; matrix; Multi objective; Multi-objectives optimization; Transparency
Adapting Knowledge Inference Algorithms to Measure Geometry Competencies through a Puzzle Game,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176742065&doi=10.1145%2f3614436&partnerID=40&md5=1ee9dcfde4f0c01d455b5feb609a6d8c,"The rapid technological evolution of the last years has motivated students to develop capabilities that will prepare them for an unknown future in the 21st century. In this context, many teachers intend to optimise the learning process, making it more dynamic and exciting through the introduction of gamification. Thus, this article focuses on a data-driven assessment of geometry competencies, which are essential for developing problem-solving and higher-order thinking skills. Our main goal is to adapt, evaluate and compare Bayesian Knowledge Tracing (BKT), Performance Factor Analysis (PFA), Elo, and Deep Knowledge Tracing (DKT) algorithms applied to the data of a geometry game named Shadowspect, in order to predict students' performance by means of several classifier metrics. We analysed two algorithmic configurations, with and without prioritisation of Knowledge Components (KCs) - the skills needed to complete a puzzle successfully, and we found Elo to be the algorithm with the best prediction power with the ability to model the real knowledge of students. However, the best results are achieved without KCs because it is a challenging task to differentiate between KCs effectively in game environments. Our results prove that the above-mentioned algorithms can be applied in formal education to improve teaching, learning, and organisational efficiency. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",capabilities; competencies; data mining; data-driven evaluation; PhrasesComputational social science,Education computing; Geometry; Inference engines; Students; Capability; Competency; Data driven; Data-driven evaluation; Inference algorithm; Knowledge components; Phrasescomputational social science; Puzzle games; Teachers'; Technological evolution; Data mining
Sequential and Graphical Cross-Domain Recommendations with a Multi-View Hierarchical Transfer Gate,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176761349&doi=10.1145%2f3604615&partnerID=40&md5=fc13cb384070c100724e3d74dba47006,"Cross-domain recommender systems could potentially improve the recommendation performance by means of transferring abundant knowledge from the auxiliary domain to the target domain. They could help address some key challenges in recommender systems, such as data sparsity and cold start. However, most existing cross-domain recommendation approaches represent the user preferences based on a single kind of user's feature or behavior and fail to explore the hidden interaction effects of different kinds of features or behaviors. In this article, we propose the Sequential and Graphical Cross-Domain Recommendations with a Multi-View Hierarchical Transfer Gate (SGCross) to transfer user representations from multiple perspectives. The SGCross model constructs a user profile by learning the personal preference from a personal view, the dynamic preference from a temporal view, as well as the collaborative preference from a collaborative view. Specifically, a Multi-view Hierarchical Gate (MHG) is designed to transfer the informative representations of user knowledge on different views from the auxiliary domain separately, aiming to enhance the user representations. Furthermore, a two-stage attentive fusion module is designed to integrate transferred information at two levels: the domain level and the view level. Extensive experiments on the Amazon dataset and the Douban dataset have demonstrated that SGCross effectively improves the accuracy of cross-domain recommendations and outperforms the state-of-the-art baseline models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cross-domain recommendation; deep learning; graph neural networks; recommender systems; transfer learning,Behavioral research; Deep learning; Graph neural networks; Hierarchical systems; User profile; Cross-domain; Cross-domain recommendations; Data sparsity; Deep learning; Graph neural networks; Multi-views; Recommendation performance; Target domain; Transfer gate; Transfer learning; Recommender systems
Multi-View Graph Convolutional Networks with Differentiable Node Selection,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176386299&doi=10.1145%2f3608954&partnerID=40&md5=3e6d74adf70631a3235811c7808ff0d6,"Multi-view data containing complementary and consensus information can facilitate representation learning by exploiting the intact integration of multi-view features. Because most objects in the real world often have underlying connections, organizing multi-view data as heterogeneous graphs is beneficial to extracting latent information among different objects. Due to the powerful capability to gather information of neighborhood nodes, in this article, we apply Graph Convolutional Network (GCN) to cope with heterogeneous graph data originating from multi-view data, which is still under-explored in the field of GCN. In order to improve the quality of network topology and alleviate the interference of noises yielded by graph fusion, some methods undertake sorting operations before the graph convolution procedure. These GCN-based methods generally sort and select the most confident neighborhood nodes for each vertex, such as picking the top-k nodes according to pre-defined confidence values. Nonetheless, this is problematic due to the non-differentiable sorting operators and inflexible graph embedding learning, which may result in blocked gradient computations and undesired performance. To cope with these issues, we propose a joint framework dubbed Multi-view Graph Convolutional Network with Differentiable Node Selection (MGCN-DNS), which is constituted of an adaptive graph fusion layer, a graph learning module, and a differentiable node selection schema. MGCN-DNS accepts multi-channel graph-structural data as inputs and aims to learn more robust graph fusion through a differentiable neural network. The effectiveness of the proposed method is verified by rigorous comparisons with considerable state-of-the-art approaches in terms of multi-view semi-supervised classification tasks, and the experimental results indicate that MGCN-DNS achieves pleasurable performance on several benchmark multi-view datasets. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",differentiable node selection; graph convolutional network; Multi-view learning; semi-supervised classification,Benchmarking; Classification (of information); Convolution; Data mining; Graph theory; Internet protocols; Multilayer neural networks; Network topology; Supervised learning; Convolutional networks; Differentiable node selection; Graph convolutional network; Heterogeneous graph; Multi-view datum; Multi-view learning; Multi-views; Neighbourhood; Node selection; Semisupervised classification (SSC); Sorting
Multi-Label Quantification,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176394873&doi=10.1145%2f3606264&partnerID=40&md5=36e6c94c6ebdafdc984a7022d167990c,"Quantification, variously called supervised prevalence estimation or learning to quantify, is the supervised learning task of generating predictors of the relative frequencies (a.k.a. prevalence values) of the classes of interest in unlabelled data samples. While many quantification methods have been proposed in the past for binary problems and, to a lesser extent, single-label multiclass problems, the multi-label setting (i.e., the scenario in which the classes of interest are not mutually exclusive) remains by and large unexplored. A straightforward solution to the multi-label quantification problem could simply consist of recasting the problem as a set of independent binary quantification problems. Such a solution is simple but naïve, since the independence assumption upon which it rests is, in most cases, not satisfied. In these cases, knowing the relative frequency of one class could be of help in determining the prevalence of other related classes. We propose the first truly multi-label quantification methods, i.e., methods for inferring estimators of class prevalence values that strive to leverage the stochastic dependencies among the classes of interest in order to predict their relative frequencies more accurately. We show empirical evidence that natively multi-label solutions outperform the naïve approaches by a large margin. The code to reproduce all our experiments is available online. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",class prior estimation; learning to quantify; multi-label learning; multi-label quantification; Quantification; supervised prevalence estimation,Data mining; Frequency estimation; Learning algorithms; Learning systems; Class prior estimation; Learning to quantify; Multi-label learning; Multi-label quantification; Multi-labels; Prevalence estimation; Quantification; Quantification methods; Relative frequencies; Supervised prevalence estimation; Stochastic systems
TTS-Norm: Forecasting Tensor Time Series via Multi-Way Normalization,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176806978&doi=10.1145%2f3605894&partnerID=40&md5=6d14e2dd288e9b1481c259ed631bff9a,"Tensor time series (TTS) data, a generalization of one-dimensional time series on a high-dimensional space, is ubiquitous in real-world applications. Compared to modeling time series or multivariate time series, which has received much attention and achieved tremendous progress in recent years, tensor time series has been paid less effort. However, properly coping with the TTS is a much more challenging task, due to its high-dimensional and complex inner structure. In this article, we start by revealing the structure of TTS data from afn statistical view of point. Then, in line with this analysis, we perform Tensor Time Series forecasting via a proposed Multi-way Normalization (TTS-Norm), which effectively disentangles multiple heterogeneous low-dimensional substructures from the original high-dimensional structure. Finally, we design a novel objective function for TTS forecasting, accounting for the numerical heterogeneity among different low-dimensional subspaces of TTS. Extensive experiments on two real-world datasets verify the superior performance of our proposed model.1 © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",neural network; normalization; representation learning; Tensor time series forecasting,Forecasting; Time series; Time series analysis; Generalisation; High dimensional spaces; Neural-networks; Normalisation; One-dimensional time series; Representation learning; Tensor time series forecasting; Time series forecasting; Time-series data; Times series; Tensors
Fairness in Recommender Systems: Evaluation Approaches and Assurance Strategies,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176423072&doi=10.1145%2f3604558&partnerID=40&md5=c24ce076933a4c57d9f00ae7dd044d23,"With the wide application of recommender systems, the potential impacts of recommender systems on customers, item providers and other parties have attracted increasing attention. Fairness, which is the quality of treating people equally, is also becoming important in recommender system evaluation and algorithm design. Therefore, in the past years, there has been a growing interest in fairness measurement and assurance in recommender systems. Although there are several reviews on related topics, such as fairness in machine learning and debias in recommender systems, they do not present a systematic view on fairness in recommender systems, which is context aware and has a multi-sided meaning. Therefore, in this review, the concept of fairness is discussed in detail in the various contexts of recommender systems. Specifically, a comprehensive framework to classify fairness metrics is proposed from four dimensions, i.e., Fairness for Whom, Demographic Unit, Time Frame, and Quantification Method. Then the strategies for eliminating unfairness in recommendations, fairness in different recommendation tasks and datasets are reviewed and summarized. Finally, the challenges and future work are discussed.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",fairness; Recommender system; survey,Quality control; Algorithm design; Context-Aware; Evaluation approach; Evaluation design; Fairness; Four dimensions; Machine-learning; Potential impacts; System algorithm; System evaluation; Recommender systems
Efficient Density-peaks Clustering Algorithms on Static and Dynamic Data in Euclidean Space,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176796219&doi=10.1145%2f3607873&partnerID=40&md5=ee5c0611f916f75aed810424df19ed9b,"Clustering multi-dimensional points is a fundamental task in many fields, and density-based clustering supports many applications because it can discover clusters of arbitrary shapes. This article addresses the problem of Density-Peaks Clustering (DPC) in Euclidean space. DPC already has many applications, but its straightforward implementation incurs O(n2) time, where n is the number of points, thereby does not scale to large datasets. To enable DPC on large datasets, we first propose empirically efficient exact DPC algorithm, Ex-DPC. Although this algorithm is much faster than the straightforward implementation, it still suffers from O(n2) time theoretically. We hence propose a new exact algorithm, Ex-DPC++, that runs in o(n2) time. We accelerate their efficiencies by leveraging multi-threading. Moreover, real-world datasets may have arbitrary updates (point insertions and deletions). It is hence important to support efficient cluster updates. To this end, we propose D-DPC for fully dynamic DPC. We conduct extensive experiments using real datasets, and our experimental results demonstrate that our algorithms are efficient and scalable.  © 2023 Copyright held by the owner/author(s).",Density-peaks clustering; multi-dimensional points; parallel algorithms,Clustering algorithms; Database systems; Geometry; Clusterings; Density-based Clustering; Density-peak clustering; Dynamic data; Euclidean spaces; Large datasets; Multi dimensional; Multi-dimensional point; Static datum; Statics and dynamics; Large dataset
HUSP-SP: Faster Utility Mining on Sequence Data,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176780188&doi=10.1145%2f3597935&partnerID=40&md5=40cf147b8d69a007f80be0753fa59baf,"High-utility sequential pattern mining (HUSPM) has emerged as an important topic due to its wide application and considerable popularity. However, due to the combinatorial explosion of the search space when the HUSPM problem encounters a low-utility threshold or large-scale data, it may be time-consuming and memory-costly to address the HUSPM problem. Several algorithms have been proposed for addressing this problem, but they still cost a lot in terms of running time and memory usage. In this article, to further solve this problem efficiently, we design a compact structure called sequence projection (seqPro) and propose an efficient algorithm, namely, discovering high-utility sequential patterns with the seqPro structure (HUSP-SP). HUSP-SP utilizes the compact seq-array to store the necessary information in a sequence database. The seqPro structure is designed to efficiently calculate candidate patterns' utilities and upper-bound values. Furthermore, a new upper bound on utility, namely, tighter reduced sequence utility and two pruning strategies in search space, are utilized to improve the mining performance of HUSP-SP. Experimental results on both synthetic and real-life datasets show that HUSP-SP can significantly outperform the state-of-the-art algorithms in terms of running time, memory usage, search space pruning efficiency, and scalability.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",seq-array; sequence data; upper bound; Utility mining,Database systems; Memory usage; Running time; Search spaces; Seq-array; Sequence data; Sequential pattern mining problem; Sequential patterns; Sequential-pattern mining; Upper Bound; Utility mining; Data mining
HGV4Risk: Hierarchical Global View-guided Sequence Representation Learning for Risk Prediction,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176726793&doi=10.1145%2f3605895&partnerID=40&md5=b6241fe8658ae732922c98cf78dee3c6,"Risk prediction, usually achieved by learning representations from patient's physiological sequence or user's behavioral sequence data, and has been widely applied in healthcare and finance. Despite that, some recent time-aware deep learning methods have led to superior performances in such sequence representation learning tasks, such improvement is limited due to a lack of guidance from hierarchical global view. To address this issue, we propose a novel end-to-end Hierarchical Global View-guided (HGV) sequence representation learning framework. Specifically, the Global Graph Embedding (GGE) module is proposed to learn sequential clip-aware representations from temporal correlation graph (TCG) at instance level. Furthermore, following the way of key-query attention, the harmonic β-attention (β-Attn) is also developed for making a global tradeoff between time-aware decay and observation significance at channel level adaptively. Moreover, the hierarchical representations at both instance level and channel level can be coordinated by the heterogeneous information aggregation under the guidance of global view. Experimental results on both healthcare risk prediction benchmark and SMEs credit overdue risk prediction task from the real-world industrial scenario in MYBank, Ant Group, have illustrated that the proposed model can achieve competitive prediction performance compared with other known baselines. The code has been released public available at: https://github.com/LiYouru0228/HGV.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; risk prediction; sequence representation learning,Behavioral research; Benchmarking; Deep learning; Health care; Learning systems; Risk assessment; Risk perception; Channel-level; Deep learning; End to end; Global view; Learning methods; Learning tasks; Performance; Risk predictions; Sequence data; Sequence representation learning; Forecasting
Multi-Label Feature Selection Via Adaptive Label Correlation Estimation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168795009&doi=10.1145%2f3604560&partnerID=40&md5=32421bfb089e1cf488aa5ff74c204cde,"In multi-label learning, each instance is associated with multiple labels simultaneously. Multi-label data often have noisy, irrelevant, and redundant features of high dimensionality. Multi-label feature selection has received considerable attention as an effective means for dealing with high-dimensional multi-label data. Many multi-label feature selection methods exploit label correlations to help select features. However, finding label correlations and selecting features in existing multi-label feature selection methods are often two separate processes, the existence of noises and outliers in training data makes the label correlations exploited from label space less reliable. Therefore, the learned label correlations may mislead the feature selection process and result in the selection of less informative features. This article proposes a novel algorithm named ROAD, i.e., multi-label featuRe selectiOn via ADaptive label correlation estimation. ROAD jointly performs adaptive label correlation exploration and feature selection with alternating optimization to obtain reliable estimation of label correlations, which can more effectively reveal the intrinsic manifold structure among labels and lead to the selection of a more proper feature subset. Comprehensive experiments on several frequently used datasets validate the superiority of ROAD against the state-of-the-art multi-label feature selection algorithms.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive label correlation estimation; Additional Key Words and PhrasesFeature selection; multi-label learning,Learning algorithms; Learning systems; Roads and streets; Adaptive label correlation estimation; Additional key word and phrasesfeature selection; Correlation estimation; Feature selection methods; Features selection; Key words; Label correlations; Multi-label learning; Multi-labels; Multiple labels; Feature Selection
Server-Client Collaborative Distillation for Federated Reinforcement Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176746496&doi=10.1145%2f3604939&partnerID=40&md5=f06a1c61e8eb5f14b540038ab9c38fb6,"Federated Learning (FL) learns a global model in a distributional manner, which does not require local clients to share private data. Such merit has drawn lots of attention in the interaction scenarios, where Federated Reinforcement Learning (FRL) emerges as a cross-field research direction focusing on the robust training of agents. Different from FL, the heterogeneity problem in FRL is more challenging because the data depends on the policy of agents and the environment dynamics. FRL learns to interact under the non-stationary environment feedback, while the typical FL methods aim at handling the constant data heterogeneity. In this article, we are among the first attempts to analyze the heterogeneity problem in FRL and propose an off-policy FRL framework. Specifically, a student-teacher-student model learning and fusion method, termed as Server-Client Collaborative Distillation (SCCD), is introduced. Unlike the traditional FL, we distill all local models on the server side for model fusion. To reduce the variance of the training, a local distillation is also conducted every time the agent receives the global model. Experimentally, we compare SCCD with a range of straightforward combinations between FL methods and RL. The results demonstrate that SCCD has a superior performance in four classical continuous control tasks with non-IID environments. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",collaborative learning; heterogeneous environment; PhrasesFederated learning,Data handling; Learning systems; Reinforcement learning; Collaborative learning; Field research; Global models; Heterogeneous environments; Learn+; Learning methods; Model fusion; Phrasesfederated learning; Private data; Reinforcement learnings; Distillation
CoupledGT: Coupled Geospatial-temporal Data Modeling for Air Quality Prediction,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168806394&doi=10.1145%2f3604616&partnerID=40&md5=7bf6e21958b1fd97d13f9e0c10aa6d16,"Air pollution seriously affects public health, while effective air quality prediction remains a challenging problem since the complex spatial-temporal couplings exist in multi-area monitoring data of the city. Current approaches rarely consider relative geographical locations when capturing spatial-temporal relations, instead the latent inter-dependencies (i.e., implicit spatial relations) of data as a replacement. However, such relations cannot necessarily reflect the diffusion of air pollutants in the real world, and genuine location-related information could be lost during the implicit relation learning process. In this article, we introduce a new concept, geospatial-temporal data, and propose a novel deep neural network architecture, CoupledGT, to learn the geospatial-temporal couplings within data for air quality prediction. Specifically, the asymmetric diffusion relation of air quality data between two areas is first explicitly represented by the newly developed planar Gaussian diffusion (PGD) equation. And then, a geospatial couplings diffuser (GCD) is designed to parameterize the PGD equation and learn multi-areas diffusion mutually affected geospatial couplings. Besides, the RNN is employed to capture temporal couplings of each area, and incorporated with GCD to learn both shared and unique characteristics of the geospatial-temporal data simultaneously, which empowers the generalization and efficiency of the model. Extensive experiments on two real-world datasets demonstrate our method is robust and outperforms existing baseline methods in air quality prediction tasks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGeospatial-temporal data; air quality prediction; coupled relational learning,Air quality; Couplings; Deep neural networks; Forecasting; Network architecture; Additional key word and phrasesgeospatial-temporal data; Air quality prediction; Coupled relational learning; Geo-spatial; Key words; Learn+; Relational learning; Spatial temporals; Temporal coupling; Temporal Data; Diffusion
Discrete Listwise Content-aware Recommendation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176764564&doi=10.1145%2f3609334&partnerID=40&md5=6b0eb8cc06e8ef3cc455d10fc17fa5de,"To perform online inference efficiently, hashing techniques, devoted to encoding model parameters as binary codes, play a key role in reducing the computational cost of content-aware recommendation (CAR), particularly on devices with limited computation resource. However, current hashing methods for CAR fail to align their learning objectives (e.g., squared loss) with the ranking-based metrics (e.g., Normalized Discounted Cumulative Gain (NDCG)), resulting in suboptimal recommendation accuracy. In this article, we propose a novel ranking-based CAR hashing method based on Factorization Machine (FM), called Discrete Listwise FM (DLFM), for fast and accurate recommendation. Concretely, our DLFM is to optimize NDCG in the Hamming space for preserving the listwise user-item relationships. We devise an efficient algorithm to resolve the challenging DLFM problem, which can directly learn binary parameters in a relaxed continuous solution space, without additional quantization. Particularly, our theoretical analysis shows that the optimal solution to the relaxed continuous optimization problem is approximately the same as that of the original discrete optimization problem. Through extensive experiments on two real-world datasets, we show that DLFM consistently outperforms state-of-the-art hashing-based recommendation techniques.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Learning to hash; NDCG,Data mining; Optimization; Computational costs; Content-aware; Encoding models; Factorization machines; Hashing method; Hashing techniques; Learning to hash; Modeling parameters; Normalized discounted cumulative gain; Online inferences; Parameter estimation
An Information Theory Based Method for Quantifying the Predictability of Human Mobility,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168802883&doi=10.1145%2f3597500&partnerID=40&md5=548eb4a0bd2c504217353730c26d426b,"Research on human mobility drives the development of economy and society. How to predict when and where one will go accurately is one of the core research questions. Existing work is mainly concerned with performance of mobility prediction models. Since accuracy of predict models does not indicate whether or not one's mobility is inherently easy to predict, there has not been a definite conclusion about that to what extent can our predictions of human mobility be accurate. To help solve this problem, we describe the formalized definition of predictability of human mobility, propose a model based on additive Markov chain to measure the probability of exploration, and further develop an information theory based method for quantifying the predictability considering exploration of human mobility. Then, we extend our method by using mutual information in order to measure the predictability considering external influencing factors, which has not been studied before. Experiments on simulation data and three real-world datasets show that our method yields a tighter upper bound on predictability of human mobility than previous work, and that predictability increased slightly when considering external factors such as weather and temperature.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHuman mobility; human behavior prediction; information entropy; predictability,Behavioral research; Digital storage; Forecasting; Markov processes; Additional key word and phraseshuman mobility; Behavior prediction; Economy and society; Human behavior prediction; Human behaviors; Human mobility; Information entropy; Key words; Predictability; Research questions; Information theory
Joint Inference of Diffusion and Structure in Partially Observed Social Networks Using Coupled Matrix Factorization,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167954836&doi=10.1145%2f3599237&partnerID=40&md5=5fe3eb844f86c857fa9109bad18960f1,"Access to complete data in large-scale networks is often infeasible. Therefore, the problem of missing data is a crucial and unavoidable issue in the analysis and modeling of real-world social networks. However, most of the research on different aspects of social networks does not consider this limitation. One effective way to solve this problem is to recover the missing data as a pre-processing step. In this paper, a model is learned from partially observed data to infer unobserved diffusion and structure networks. To jointly discover omitted diffusion activities and hidden network structures, we develop a probabilistic generative model called ""DiffStru.""The interrelations among links of nodes and cascade processes are utilized in the proposed method via learning coupled with low-dimensional latent factors. Besides inferring unseen data, latent factors such as community detection may also aid in network classification problems. We tested different missing data scenarios on simulated independent cascades over LFR networks and real datasets, including Twitter and Memetracker. Experiments on these synthetic and real-world datasets show that the proposed method successfully detects invisible social behaviors, predicts links, and identifies latent features.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesInformation diffusion; cascade completion; link prediction; matrix factorization; network structure; partially observed data; social network,Data mining; Diffusion; Matrix algebra; Social networking (online); Additional key word and phrasesinformation diffusion; Cascade completion; Key words; Link prediction; Matrix factorizations; Missing data; Network structures; Observed data; Partially observed data; Social network; Matrix factorization
A Survey on Influence Maximization: From an ML-Based Combinatorial Optimization,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168810021&doi=10.1145%2f3604559&partnerID=40&md5=7518eacf15b6c33f0f47ee441023465b,"Influence Maximization (IM) is a classical combinatorial optimization problem, which can be widely used in mobile networks, social computing, and recommendation systems. It aims at selecting a small number of users such that maximizing the influence spread across the online social network. Because of its potential commercial and academic value, there are a lot of researchers focusing on studying the IM problem from different perspectives. The main challenge comes from the NP-hardness of the IM problem and #P-hardness of estimating the influence spread, thus traditional algorithms for overcoming them can be categorized into two classes: heuristic algorithms and approximation algorithms. However, there is no theoretical guarantee for heuristic algorithms, and the theoretical design is close to the limit. Therefore, it is almost impossible to further optimize and improve their performance. With the rapid development of artificial intelligence, technologies based on Machine Learning (ML) have achieved remarkable achievements in many fields. In view of this, in recent years, a number of new methods have emerged to solve combinatorial optimization problems by using ML-based techniques. These methods have the advantages of fast solving speed and strong generalization ability to unknown graphs, which provide a brand-new direction for solving combinatorial optimization problems. Therefore, we abandon the traditional algorithms based on iterative search and review the recent development of ML-based methods, especially Deep Reinforcement Learning, to solve the IM problem and other variants in social networks. We focus on summarizing the relevant background knowledge, basic principles, common methods, and applied research. Finally, the challenges that need to be solved urgently in future IM research are pointed out.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesInfluence maximization; combinatorial optimization; deep reinforcement learning; graph embedding; machine learning; social networks,Approximation algorithms; Deep learning; Hardness; Heuristic algorithms; Iterative methods; Learning systems; Reinforcement learning; Social networking (online); Additional key word and phrasesinfluence maximization; Combinatorial optimization problems; Deep reinforcement learning; Graph embeddings; Influence maximizations; Key words; Machine-learning; Maximization problem; Reinforcement learnings; Social network; Combinatorial optimization
Multifaceted Relation-aware Meta-learning with Dual Customization for User Cold-start Recommendation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168807755&doi=10.1145%2f3597458&partnerID=40&md5=b4204407eb4fd6dabf6beac20b49f614,"User cold-start scenarios pose great challenges to recommendation systems in accurately capturing user preferences with sparse interaction records. Besides incorporating auxiliary information to enrich user/item representations, recent studies under the schema of meta-learning focus on quickly adapting personalized recommendation models based on cold-start users' scarce interactions. The majority of meta-learning based recommendation methods follow a bi-level optimization paradigm and learn globally shared initialization across all cold-start recommendation tasks. In addition, to further facilitate the ability of fast adaptation, existing methods have made efforts to tailor task-specific prior knowledge by identifying the individual characteristics of each task. However, we argue that multi-view commonalities between existing users and cold-start users are also essential for precisely distinguishing new tasks, but not comprehensively modeled in previous studies. In this article, we propose a multifaceted relation-aware meta-learning approach namely MeCM for user cold-start recommendation, which enhances task-adaptive initialization customization by extracting multiple views of task relevance. We design a dual customization framework consisting of two successive phases including cluster-level customization and task-level customization. Specifically, MeCM first extracts multifaceted semantic relations between tasks and refines task commonalities into task clusters maintained with memory networks (MNs). Globally learned fast weights corresponding to task clusters are queried to perform cluster-level customization. Then task-level customization is triggered based on contextual information of the target task via interaction-wise encoding. Extensive experiments on real-world datasets demonstrate the superior performance of our model over state-of-the-art meta-learning-based recommendation methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRecommendation systems; Dual initialization customization; memory networks; meta-learning; user cold-start,Learning systems; Semantics; Additional key word and phrasesrecommendation system; Cold-start; Cold-start Recommendations; Customisation; Dual initialization customization; Key words; Memory network; Metalearning; Recommendation methods; User cold-start; Recommender systems
Contrastive Learning Based Graph Convolution Network for Social Recommendation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164287599&doi=10.1145%2f3587268&partnerID=40&md5=3fcf94859994969d8afa6362ca59ed93,"Exploiting social networks is expected to enhance the performance of recommender systems when interaction information is sparse. Existing social recommendation models focus on modeling multi-graph structures and then aggregating the information from these multiple graphs to learn potential user preferences. However, these methods often employ complex models and redundant parameters to get a slight performance improvement. Contrastive learning has been widely researched as an effective paradigm in the area of recommendation. Most existing contrastive learning-based models usually focus on constructing multi-graph structures to perform graph augmentation for contrastive learning. However, the effect of graph augmentation on contrastive learning is inconclusive. In view of these challenges, in this work, we propose a contrastive learning based graph convolution network for social recommendation (CLSR), which integrates information from both the social graph and the interaction graph. First, we propose a fusion-simplified method to combine the social graph and the interaction graph. Technically, on the basis of exploring users' interests by interaction graph, we further exploit social connections to alleviate data sparsity. By combining the user embeddings learned through two graphs in a certain proportion, we can obtain user representation at a finer granularity. Meanwhile, we introduce a contrastive learning framework for multi-graph network modeling, where we explore the feasibility of constructing positive and negative samples of contrastive learning by conducting data augmentation on embedding representations. Extensive experiments verify the superiority of CLSR's contrastive learning framework and fusion-simplified method of integrating social relations.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Contrastive learning; embedding augmentation; graph convolution network; social recommendation,Convolution; Graph structures; Graphic methods; Recommender systems; User profile; Contrastive learning; Embedding augmentation; Embeddings; Graph augmentation; Graph convolution network; Graph structures; Interaction graphs; Simplified method; Social graphs; Social recommendation; Embeddings
Modeling Regime Shifts in Multiple Time Series,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164293261&doi=10.1145%2f3592857&partnerID=40&md5=96de5f358ccf9a44e29751a8b48d4588,"We investigate the problem of discovering and modeling regime shifts in an ecosystem comprising multiple time series known as co-evolving time series. Regime shifts refer to the changing behaviors exhibited by series at different time intervals. Learning these changing behaviors is a key step toward time series forecasting. While advances have been made, existing methods suffer from one or more of the following shortcomings: (1) failure to take relationships between time series into consideration for discovering regimes in multiple time series; (2) lack of an effective approach that models time-dependent behaviors exhibited by series; (3) difficulties in handling data discontinuities which may be informative. Most of the existing methods are unable to handle all of these three issues in a unified framework. This, therefore, motivates our effort to devise a principled approach for modeling interactions and time-dependency in co-evolving time series. Specifically, we model an ecosystem of multiple time series by summarizing the heavy ensemble of time series into a lighter and more meaningful structure called a mapping grid. By using the mapping grid, our model first learns time series behavioral dependencies through a dynamic network representation, then learns the regime transition mechanism via a full time-dependent Cox regression model. The originality of our approach lies in modeling interactions between time series in regime identification and in modeling time-dependent regime transition probabilities, usually assumed to be static in existing work.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Co-evolving time series; evolving networks; regime shift; survival analysis,Bioinformatics; Data handling; Data mining; Ecosystems; Mapping; Regression analysis; Time series analysis; Co-evolving time series; Evolving networks; Learn+; Model interaction; Modelling time; Multiple time series; Regime shift; Regime transition; Survival analysis; Times series; Time series
Adaptive Collaborative Soft Label Learning for Unsupervised Multi-View Feature Selection,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164302846&doi=10.1145%2f3591467&partnerID=40&md5=783e663d772ad911ff6729af08bc6145,"Unsupervised multi-view feature selection aims to select informative features with multi-view features and unsupervised learning. It is a challenging problem due to the absence of explicit semantic supervision. Recently, graph theory and hard pseudo-label learning have been adopted to solve multi-view feature selection problems under the unsupervised learning paradigm. However, graph-based methods are difficult to support large-scale real scenarios due to the high computational complexity of graph construction. Moreover, existing methods based on hard pseudo-label learning generally result in significant information loss. In this article, we propose an Adaptive Collaborative Soft Label Learning (ACSLL) model for unsupervised multi-view feature selection. In this model, collaborative soft label learning and multi-view feature selection are integrated into a unified framework. Specifically, we learn the pseudo soft labels from each view feature by a simple and efficient method and fuse them with an adaptive weighting strategy into a joint soft label matrix. This matrix is further used for guiding the feature selection process to identify valuable features. An effective optimization strategy guaranteed with proven convergence is derived to iteratively solve this problem. Experiments demonstrate the superiority of the proposed method in both feature selection accuracy and efficiency.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive learning; collaborative soft label; fuzziness; Unsupervised multi-view feature selection,Feature Selection; Graph theory; Graphic methods; Iterative methods; Learning systems; Semantics; Adaptive learning; Collaborative soft label; Explicit semantics; Feature selection problem; Features selection; Fuzziness; Learning paradigms; Multi-views; Soft labels; Unsupervised multi-view feature selection; Unsupervised learning
Multi-view Graph Representation Learning Beyond Homophily,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164300303&doi=10.1145%2f3592858&partnerID=40&md5=d187cb3b4a79f586896314748629649a,"Unsupervised graph representation learning (GRL) aims at distilling diverse graph information into task-agnostic embeddings without label supervision. Due to a lack of support from labels, recent representation learning methods usually adopt self-supervised learning, and embeddings are learned by solving a handcrafted auxiliary task (so-called pretext task). However, partially due to the irregular non-Euclidean data in graphs, the pretext tasks are generally designed under homophily assumptions and cornered in the low-frequency signals, which results in significant loss of other signals, especially high-frequency signals widespread in graphs with heterophily. Motivated by this limitation, we propose a multi-view perspective and the usage of diverse pretext tasks to capture different signals in graphs into embeddings. A novel framework, denoted as Multi-view Graph Encoder (MVGE), is proposed, and a set of key designs are identified. More specifically, a set of new pretext tasks are designed to encode different types of signals, and a straightforward operation is proposed to maintain both the commodity and personalization in both the attribute and the structural levels. Extensive experiments on synthetic and real-world network datasets show that the node representations learned with MVGE achieve significant performance improvements in three different downstream tasks, especially on graphs with heterophily.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",autoencoder; Graph representation learning; heterophily; homophily; multi-view,Graph embeddings; Auto encoders; Embeddings; Graph information; Graph representation; Graph representation learning; Heterophily; Homophily; Learning methods; Multi-views; Non-Euclidean; Graphic methods
Instrumental Variable-Driven Domain Generalization with Unobserved Confounders,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162809681&doi=10.1145%2f3595380&partnerID=40&md5=03a1845dff28b40abd42df93594f56c9,"Domain generalization (DG) aims to learn from multiple source domains a model that can generalize well on unseen target domains. Existing DG methods mainly learn the representations with invariant marginal distribution of the input features, however, the invariance of the conditional distribution of the labels given the input features is more essential for unknown domain prediction. Meanwhile, the existing of unobserved confounders which affect the input features and labels simultaneously cause spurious correlation and hinder the learning of the invariant relationship contained in the conditional distribution. Interestingly, with a causal view on the data generating process, we find that the input features of one domain are valid instrumental variables for other domains. Inspired by this finding, we propose an instrumental variable-driven DG method (IV-DG) by removing the bias of the unobserved confounders with two-stage learning. In the first stage, it learns the conditional distribution of the input features of one domain given input features of another domain. In the second stage, it estimates the relationship by predicting labels with the learned conditional distribution. Theoretical analyses and simulation experiments show that it accurately captures the invariant relationship. Extensive experiments on real-world datasets demonstrate that IV-DG method yields state-of-the-art results.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Causal learning; domain generalization; instrumental variable; unobserved confounder,Causal learning; Conditional distribution; Confounder; Domain generalization; Generalisation; Input features; Instrumental variables; Learn+; Multiple source; Unobserved confounder; Data mining
A Novel Classification Technique based on Formal Methods,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164284033&doi=10.1145%2f3592796&partnerID=40&md5=959fd7503e5eea597445052d9b430d38,"In last years, we are witnessing a growing interest in the application of supervised machine learning techniques in the most disparate fields. One winning factor of machine learning is represented by its ability to easily create models, as it does not require prior knowledge about the application domain. Complementary to machine learning are formal methods, that intrinsically offer safeness check and mechanism for reasoning on failures. Considering the weaknesses of machine learning, a new challenge could be represented by the use of formal methods. However, formal methods require the expertise of the domain, knowledge about modeling language with its semantic and mathematical rigour to specify properties. In this article, we propose a novel learning technique based on the adoption of formal methods for classification thanks to the automatic generation both of the formula and of the model. In this way the proposed method does not require any human intervention and thus it can be applied also to complex/large datasets. This leads to less effort both in using formal methods and in a better explainability and reasoning about the obtained results. Through a set of case studies from different real-world domains (i.e., driver detection, scada attack identification, arrhythmia characterization, mobile malware detection, and radiomics for lung cancer analysis), we demonstrate the usefulness of the proposed method, by showing that we are able to overcome the performances obtained from widespread classification algorithms.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",classification; formal methods; Model checking,Diseases; Formal methods; Learning algorithms; Learning systems; Malware; Modeling languages; Semantics; Supervised learning; Applications domains; Classification technique; Domain knowledge; Learning techniques; Machine learning techniques; Machine-learning; Models checking; Prior-knowledge; Property; Supervised machine learning; Model checking
Community-Based Influence Maximization Using Network Embedding in Dynamic Heterogeneous Social Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164292265&doi=10.1145%2f3594544&partnerID=40&md5=3f983616d7fcfd07515a544be86dd297,"Influence maximization (IM) is a very important issue in social network diffusion analysis. The topology of real social network is large-scale, dynamic, and heterogeneous. The heterogeneity, and continuous expansion and evolution of social network pose a challenge to find influential users. Existing IM algorithms usually assume that social networks are static or dynamic but homogeneous to simplify the complexity of the IM problem. We propose a community-based influence maximization algorithm using network embedding in dynamic heterogeneous social networks. We use DyHATR algorithm to obtain the propagation feature vectors of network nodes, and execute k-means cluster algorithm to transform the original network into a coarse granularity network (CGN). On CGN, we propose a community-based three-hop independent cascade model and construct the objective function of IM problem. We design a greedy heuristics algorithm to solve the IM problem with approximation guarantee and use community structure to quickly identify seed users and estimate their influence value. Experimental results on real social networks demonstrated that compared with existing IM algorithms, our proposed algorithm had better comprehensive performance with respect to the influence value, more less execution time and memory consumption, and better scalability.  © 2023 Copyright held by the owner/author(s).",community diffusion; feature learning; feature representation; Network embedding,Approximation algorithms; K-means clustering; Machine learning; Community diffusion; Community-based; Diffusion analysis; Feature learning; Feature representation; Influence maximizations; Maximization algorithm; Maximization problem; Network diffusions; Network embedding; Network embeddings
Conditional Independence Test Based on Residual Similarity,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164278017&doi=10.1145%2f3593810&partnerID=40&md5=246a08795d8e1b9efdcf0eeeb778561d,"Recently, many regression-based conditional independence (CI) test methods have been proposed to solve the problem of causal discovery. These methods provide alternatives to test CI of x,y given Z by first removing the information of the controlling set Z from x and y, and then testing the independence between the two residuals Rx,Z and Ry,Z. When the residuals are linearly uncorrelated, the independence test between them is nontrivial. With the ability to calculate inner product in high-dimensional space, kernel-based methods are usually used to achieve this goal, but they are considerably time-consuming. In this paper, we test the independence between two linear combinations under linear structural equation model. We show that the dependence between the two residuals can be captured by the difference between the similarity of Rx,Z and Ry,Z and that of Rx,Z and Rr (Rr is an independent copy of Ry,Z) in high-dimensional space. With this result, we provide a new way to test CI based on the similarity between residuals, which is called SCIT - the abbreviation of Similarity-based CI Testing. Furthermore, we develop two versions of the proposal, called Kernel-SCIT and Neural-SCIT, respectively. Kernel-SCIT calculates the similarity by using kernel functions, while Neural-SCIT approximates the upper bound of the similarity by using deep neural networks. In both algorithms, random permutation tests are performed to control Type I error rate. The proposed tests are evaluated on (conditional) independence test and causal discovery with both synthetic and real datasets. Experimental results show that Kernel-SCIT is simpler yet more efficient and effective than the typical existing kernel-based methods HSIC and KCIT in the cases of small sample size, and Neural-SCIT can significantly boost the performance of CI testing when sufficient samples are available. The source code is available at https://github.com/xyw5vplus1/SCIT.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Causal discovery; Conditional independence test; Residual similarity,Causal discovery; Conditional independence tests; Conditional independences; High dimensional spaces; Inner product; Kernel based methods; Linear combinations; Residual similarity; Test method; Deep neural networks
TechPat: Technical Phrase Extraction for Patent Mining,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168800790&doi=10.1145%2f3596603&partnerID=40&md5=a178e988fab6472316f0282b58ba6a73,"In recent years, due to the explosive growth of patent applications, patent mining has drawn extensive attention and interest. An important issue of patent mining is that of recognizing the technologies contained in patents, which serves as a fundamental preparation for deeper analysis. To this end, in this article, we make a focused study on constructing a technology portrait for each patent, i.e., to recognize technical phrases concerned in it, which can summarize and represent patents from a technical perspective. Along this line, a critical challenge is how to analyze the unique characteristics of technical phrases and illustrate them with definite descriptions. Therefore, we first generate the detailed descriptions about the technical phrases existing in extensive patents based on different criteria, including various previous works, practical experience, and statistical analyses. Then, considering the unique characteristics of technical phrases and the complex structure of patent documents, such as multi-aspect semantics and multi-level relevances, we further propose a novel unsupervised model, namely TechPat, which can not only automatically recognize technical phrases from massive patents but also avoid the need for expensive human labeling. After that, we evaluate the extraction results from various aspects. Specifically, we propose a novel evaluation metric called Information Retrieval Efficiency (IRE) to quantify the performance of extracted technical phrases from a new perspective. Extensive experiments on real-world patent data demonstrate that the TechPat model can effectively discriminate technical phrases in patents and greatly outperform existing methods. We further apply extracted technical phrases to two practical application tasks, namely patent search and patent classification, where the experimental results confirm the wide application prospects of technical phrases. Finally, we discuss the generalization ability of our proposed methods.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesTechnology portrait; multi-aspect; multi-level; patent mining; technical phrase extraction,Patents and inventions; Semantics; Additional key word and phrasestechnology portrait; Critical challenges; Explosive growth; Key words; Multi aspects; Multilevels; Patent applications; Patent mining; Phrase extraction; Technical phrase extraction; Extraction
A Hybrid Continuous-Time Dynamic Graph Representation Learning Model by Exploring Both Temporal and Repetitive Information,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168804193&doi=10.1145%2f3596447&partnerID=40&md5=f9f3f41c521df56417122907fc36893d,"Recently, dynamic graph representation learning has attracted more and more attention from both academic and industrial communities due to its capabilities of capturing different real-world phenomena. For a dynamic graph represented as a sequence of timestamped events, there are two kinds of evolutionary essences: temporal and repetitive information. At present, the temporal information of interactions (e.g., timestamps) have been deeply explored. However, as another vital nature of dynamic graphs, the repetitive information of interactions between two nodes is neglected, which may lead to inaccurate node representation. To address this issue, we propose a novel continuous-time dynamic graph representation learning model, which consists of a node-level-memory based module, a historical high-order neighborhood based vertical aggregation module and a repetitive-topological information based horizontal aggregation module. In particular, to characterize the evolving pattern of the repetitive information of interactions between a pair of nodes, we put forward a repetitive-interaction based attention mechanism to integrate the two key attributes (i.e., the content and the number of interactions) of repetitive interactions at different moments, based on the insight that the repetitive behaviors of nodes are widespread and essential. We conduct extensive experiments including future link prediction tasks (for transductive and inductive learning) and dynamic node classification task, and results on three real-life dynamic graph datasets demonstrate that the proposed method significantly outperforms state-of-the-art baselines, for both observed nodes and new ones.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGraph neural networks; continuous-time dynamic graph; representation learning,Classification (of information); Graph theory; Learning systems; Academic community; Additional key word and phrasesgraph neural network; Continuous-time dynamic graph; Continuous-time dynamics; Dynamic graph; Graph representation; Key words; Learning models; Neural-networks; Representation learning; Continuous time systems
Accurate Open-Set Recognition for Memory Workload,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168809981&doi=10.1145%2f3597027&partnerID=40&md5=2b66f2c3c23eda488a20a5859fdfa426,"How can we accurately identify new memory workloads while classifying known memory workloads? Verifying DRAM (Dynamic Random Access Memory) using various workloads is an important task to guarantee the quality of DRAM. A crucial component in the process is open-set recognition which aims to detect new workloads not seen in the training phase. Despite its importance, however, existing open-set recognition methods are unsatisfactory in terms of accuracy since they fail to exploit the characteristics of workload sequences.In this article, we propose Acorn, an accurate open-set recognition method capturing the characteristics of workload sequences. Acorn extracts two types of feature vectors to capture sequential patterns and spatial locality patterns in memory access. Acorn then uses the feature vectors to accurately classify a subsequence into one of the known classes or identify it as the unknown class. Experiments show that Acorn achieves state-of-the-art accuracy, giving up to 37% points higher unknown class detection accuracy while achieving comparable known class classification accuracy than existing methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesOpen-set recognition; DRAM; memory workload,Additional key word and phrasesopen-set recognition; Dynamic random access memory; Features vector; Key words; Memory workload; Recognition methods; Sequential patterns; Spatial locality; Training phasis; Unknown class; Dynamic random access storage
Towards a Better Tradeoff between Quality and Efficiency of Community Detection: An Inductive Embedding Method across Graphs,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162134375&doi=10.1145%2f3596605&partnerID=40&md5=73363bbf27c2276bbff9a550e51f195a,"Many network applications can be formulated as NP-hard combinatorial optimization problems of community detection (CD) that partitions nodes of a graph into several groups with dense linkage. Most existing CD methods are transductive, which independently optimized their models for each single graph, and can only ensure either high quality or efficiency of CD by respectively using advanced machine learning techniques or fast heuristic approximation. In this study, we consider the CD task and aims to alleviate its NP-hard challenge. Motivated by the efficient inductive inference of graph neural networks (GNNs), we explore the possibility to achieve a better tradeoff between the quality and efficiency of CD via an inductive embedding scheme across multiple graphs of a system and propose a novel inductive community detection (ICD) method. Concretely, ICD first conducts the offline training of an adversarial dual GNN structure on historical graphs to capture key properties of a system. The trained model is then directly generalized to new graphs of the same system for online CD without additional optimization, where a better tradeoff between quality and efficiency can be achieved. Compared with existing inductive approaches, we develop a novel feature extraction module based on graph coarsening, which can efficiently extract informative feature inputs for GNNs. Moreover, our original designs of adversarial dual GNN and clustering regularization loss further enable ICD to capture permutation-invariant community labels in the offline training and help derive community-preserved embedding to support the high-quality online CD. Experiments on a set of benchmarks demonstrate that ICD can achieve a significant tradeoff between quality and efficiency over various baselines.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCommunity detection; graph clustering; inductive graph representation learning,Benchmarking; Combinatorial optimization; Efficiency; Graph embeddings; Graphic methods; Heuristic methods; Machine learning; Population dynamics; Additional key word and phrasescommunity detection; Community detection; Detection methods; Graph clustering; Graph neural networks; Graph representation; High quality; Inductive graph representation learning; Key words; NP-hard; Graph neural networks
Modeling Long- and Short-Term User Preferences via Self-Supervised Learning for Next POI Recommendation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168803053&doi=10.1145%2f3597211&partnerID=40&md5=73ec9927adbe1384ac61a0d622d2ca97,"With the accumulation of check-in data from location-based services, next Point-of-Interest (POI) recommendations are gaining increasing attention. It is well known that the spatio-temporal contextual information of user check-in behavior plays a crucial role in handling vital and inherent challenges in next POI recommendation, including capture of user dynamic preferences and the sparsity problem of check-in data. However, many studies either ignore or simply stack the context features with the embedding of POIs while relying only on POI recommendation loss to optimize the entire model, therefore failing to take full advantage of the potential information in contexts. Additionally, users' interests are usually unstable and evolve over time, and accordingly recent studies have proposed various approaches to predict users' next POIs by incorporating contextual information and modeling both their long- and short-term preferences, respectively. Yet many studies overemphasize the final POI recommendation performance, and the association between POI sequences and contextual information is not well embodied in data representations. In this article, we focus on the preceding problems and propose a unified attention framework for next POI recommendation by modeling users' Long- and Short-term Preferences via Self-supervised Learning (LSPSL). Specifically, based on the self-attention network and two self-supervised optimization objectives, LSPSL first deeply exploits the intrinsic correlations between POI sequences and contextual information through pre-training, which strengthens data representations. Then, supported by pre-trained contextualized embeddings, LSPSL models and fuses users' complex long- and short-term preferences in a unified way. Extensive experiments on real-world datasets demonstrate the superiority of our model compared with other state-of-the-art approaches.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNext POI recommendation; attention architecture; pre-training; self-supervised learning; spatio-temporal context,Behavioral research; Embeddings; Location based services; Telecommunication services; User profile; Additional key word and phrasesnext point-of-interest recommendation; Attention architecture; Check-in; Contextual information; Embeddings; Key words; Pre-training; Self-supervised learning; Spatio-temporal; Spatio-temporal context; Supervised learning
Road Network Representation Learning: A Dual Graph-based Approach,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168799426&doi=10.1145%2f3592859&partnerID=40&md5=e54c89ef3f45060545fb4b6fe04883d7,"Road network is a critical infrastructure powering many applications including transportation, mobility and logistics in real life. To leverage the input of a road network across these different applications, it is necessary to learn the representations of the roads in the form of vectors, which is named road network representation learning (RNRL). While several models have been proposed for RNRL, they capture the pairwise relationships/connections among roads only (i.e., as a simple graph), and fail to capture among roads the high-order relationships (e.g., those roads that jointly form a local region usually have similar features such as speed limit) and long-range relationships (e.g., some roads that are far apart may have similar semantics such as being roads in residential areas). Motivated by this, we propose to construct a hypergraph, where each hyperedge corresponds to a set of multiple roads forming a region. The constructed hypergraph would naturally capture the high-order relationships among roads with hyperedges. We then allow information propagation via both the edges in the simple graph and the hyperedges in the hypergraph in a graph neural network context. In addition, we introduce different pretext tasks based on both the simple graph (i.e., graph reconstruction) and the hypergraph (including hypergraph reconstruction and hyperedge classification) for optimizing the representations of roads. The graph reconstruction and hypergraph reconstruction tasks are conventional ones and can capture structural information. The hyperedge classification task can capture long-range relationships between pairs of roads that belong to hyperedges with the same label. We call the resulting model HyperRoad. We further extend HyperRoad to problem settings when additional inputs of road attributes and/or trajectories that are generated on the roads are available. We conduct extensive experiments on two real datasets, for five downstream tasks, and under four problem settings, which demonstrate that our model achieves impressive improvements compared with existing baselines across datasets, tasks, problem settings, and performance metrics.CCS Concepts: • Information systems → Data mining; • Urban computing; • Spatial-temporal systems;  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRoad network; graph neural network; representation learning,Backpropagation; Data mining; Graph theory; Graphic methods; Information dissemination; Motor transportation; Roads and streets; Semantics; Additional key word and phrasesroad network; Graph neural networks; High-order; Hyper graph; Hyperedges; Key words; Network representation; Representation learning; Road network; Simple++; Graph neural networks
STA-TCN: Spatial-temporal Attention over Temporal Convolutional Network for Next Point-of-interest Recommendation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168797885&doi=10.1145%2f3596497&partnerID=40&md5=981250e8166c73916d6a7a9d44ae8b0b,"Recent years have witnessed a vastly increasing popularity of location-based social networks (LBSNs), which facilitates studies on the next Point-of-Interest (POI) recommendation problem. A user's POI visiting behavior shows the sequential transition correlation with previous successive check-ins and the global spatial-temporal correlation with those check-ins that happened a long time ago at a similar time of day and in geographically close areas. Although previous POI recommendation methods attempted to capture these two correlations, several limitations remain to be solved: (1) RNNs are widely adopted to capture the sequential transition correlation, whereas training an RNN is rather time-consuming given the long input check-in sequence. (2) The pairwise proximities on time of day and geographical area of check-ins are crucial for global spatial-temporal correlation learning, but have not been comprehensively considered by previous methods. To tackle these issues, we propose a novel next POI recommendation framework named STA-TCN. Specifically, instead of RNNs, STA-TCN augments the Temporal Convolutional Network with gated input injection to learn sequential transition correlation. Furthermore, STA-TCN fuses two novel grid-difference and time-sensitivity learning mechanisms with attention network to learn the pairwise spatial-temporal proximities among a user's check-ins. Extensive experiments are conducted on two large-scale real-world LBSN datasets, and the results show that STA-TCN outperforms the best state-of-the-art baseline with an average improvement of 9.71% and 7.88% on hit rate and normalized discounted cumulative gain, respectively.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNext POI recommendation; LBSN; self-attention; TCN,Behavioral research; Convolution; Additional key word and phrasesnext point-of-interest recommendation; Convolutional networks; Key words; Learn+; Location-based social networks; Self-attention; Spatial temporals; Spatial-temporal correlation; TCN; Time of day; Large dataset
Self-supervision for Tabular Data by Learning to Predict Additive Homoskedastic Gaussian Noise as Pretext,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168808615&doi=10.1145%2f3594720&partnerID=40&md5=bf2854345af208d3af43457e55f6b8ff,"The lack of scalability of data annotation translates to the need to decrease dependency on labels. Self-supervision offers a solution with data training themselves. However, it has received relatively less attention on tabular data, data that drive a large proportion of business and application domains. This work, which we name the Statistical Self-Supervisor (SSS), proposes a method for self-supervision on tabular data by defining a continuous perturbation as pretext. It enables a neural network to learn representations by learning to predict the level of additive isotropic Gaussian noise added to inputs. The choice of the pretext transformation is motivated by intrinsic characteristics of a neural network fundamentally performing linear fits under the widely adopted assumption of Gaussianity in its fitting error and the preservation of locality of a data example on the data manifold in the presence of small random perturbations. The transform condenses information in the generated representations, making them better employable for further task-specific prediction as evidenced by performance improvement of the downstream classifier. To evaluate the persistence of performance under low-annotation settings, SSS is evaluated against different levels of label availability to the downstream classifier (1% to 100%) and benchmarked against self- and semi-supervised methods. At the most label-constrained, 1% setting, we report a maximum increase of at least 2.5% against the next-best semi-supervised competing method. We report an increase of more than 1.5% against self-supervised state of the art. Ablation studies also reveal that increasing label availability from 0% to 1% results in a maximum increase of up to 50% on either of the five performance metrics and up to 15% thereafter, indicating diminishing returns in additional annotation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSelf-supervised learning; representation learning; tabular data,Additives; Classification (of information); Digital storage; Forecasting; Gaussian noise (electronic); Linear transformations; Metadata; Personnel training; Additional key word and phrasesself-supervised learning; Business domain; Data annotation; Down-stream; Gaussians; Key words; Neural-networks; Performance; Representation learning; Tabular data; Supervised learning
Low-rank Representation with Adaptive Dimensionality Reduction via Manifold Optimization for Clustering,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168807761&doi=10.1145%2f3589767&partnerID=40&md5=5b251e97bba951bed9aaa44015ac48e5,"The dimensionality reduction techniques are often used to reduce data dimensionality for computational efficiency or other purposes in existing low-rank representation (LRR)-based methods. However, the two steps of dimensionality reduction and learning low-rank representation coefficients are implemented in an independent way; thus, the adaptability of representation coefficients to the original data space may not be guaranteed. This article proposes a novel model, i.e., low-rank representation with adaptive dimensionality reduction (LRRARD) via manifold optimization for clustering, where dimensionality reduction and learning low-rank representation coefficients are integrated into a unified framework. This model introduces a low-dimensional projection matrix to find the projection that best fits the original data space. And the low-dimensional projection matrix and the low-rank representation coefficients interact with each other to simultaneously obtain the best projection matrix and representation coefficients. In addition, a manifold optimization method is employed to obtain the optimal projection matrix, which is an unconstrained optimization method in a constrained search space. The experimental results on several real datasets demonstrate the superiority of our proposed method.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesImage clustering; dimensionality reduction; low rank representation; manifold optimization,Computational efficiency; Data reduction; Matrix algebra; Reduction; Additional key word and phrasesimage clustering; Clusterings; Data space; Dimensionality reduction; Key words; Low dimensional; Low-rank representations; Manifold optimization; Optimisations; Projection matrix; Constrained optimization
Fitting Imbalanced Uncertainties in Multi-output Time Series Forecasting,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164280448&doi=10.1145%2f3584704&partnerID=40&md5=6fa8b9763fdbbcd0d8397a7ad085f094,"We focus on multi-step ahead time series forecasting with the multi-output strategy. From the perspective of multi-task learning (MTL), we recognize imbalanced uncertainties between prediction tasks of different future time steps. Unexpectedly, trained by the standard summed Mean Squared Error (MSE) loss, existing multi-output forecasting models may suffer from performance drops due to the inconsistency between the loss function and the imbalance structure. To address this problem, we reformulate each prediction task as a distinct Gaussian Mixture Model (GMM) and derive a multi-level Gaussian mixture loss function to better fit imbalanced uncertainties in multi-output time series forecasting. Instead of using the two-step Expectation-Maximization (EM) algorithm, we apply the self-attention mechanism on the task-specific parameters to learn the correlations between different prediction tasks and generate the weight distribution for each GMM component. In this way, our method jointly optimizes the parameters of the forecasting model and the mixture model simultaneously in an end-to-end fashion, avoiding the need of two-step optimization. Experiments on three real-world datasets demonstrate the effectiveness of our multi-level Gaussian mixture loss compared to models trained with the standard summed MSE loss function. All the experimental data and source code are available at https://github.com/smallGum/GMM-FNN.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; Gaussian Mixture Models; Time series forecasting,Deep learning; Forecasting; Gaussian distribution; Image segmentation; Learning systems; Maximum principle; Time series; Deep learning; Forecasting models; Gaussian Mixture Model; Loss functions; Mean squared error; Multi-output; Multilevels; Prediction tasks; Time series forecasting; Uncertainty; Mean square error
A Generalized Deep Learning Clustering Algorithm Based on Non-Negative Matrix Factorization,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162225663&doi=10.1145%2f3584862&partnerID=40&md5=b526d3861692e6381d0ad449b79b8ce4,"Clustering is a popular research topic in the field of data mining, in which the clustering method based on non-negative matrix factorization (NMF) has been widely employed. However, in the update process of NMF, there is no learning rate to guide the update as well as the update depends on the data itself, which leads to slow convergence and low clustering accuracy. To solve these problems, a generalized deep learning clustering (GDLC) algorithm based on NMF is proposed in this article. Firstly, a nonlinear constrained NMF (NNMF) algorithm is constructed to achieve sequential updates of the elements in the matrix guided by the learning rate. Then, the gradient values corresponding to the element update are transformed into generalized weights and generalized biases, by inputting the elements as well as their corresponding generalized weights and generalized biases into the nonlinear activation function to construct the GDLC algorithm. In addition, for improving the understanding of the GDLC algorithm, its detailed inference procedure and algorithm design are provided. Finally, the experimental results on eight datasets show that the GDLC algorithm has efficient performance.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",clustering; Deep learning; non-negative matrix factorization,Data mining; Deep learning; Inference engines; Learning algorithms; Matrix algebra; Non-negative matrix factorization; Clustering accuracy; Clustering methods; Clusterings; Deep learning; Learning clustering; Learning rates; Matrix factorizations; Nonnegative matrix factorization; Research topics; Slow convergences; Clustering algorithms
Multi-view Ensemble Clustering via Low-rank and Sparse Decomposition: From Matrix to Tensor,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164292449&doi=10.1145%2f3589768&partnerID=40&md5=dc189d949cc654e585f3372b2d78048e,"As a significant extension of classical clustering methods, ensemble clustering first generates multiple basic clusterings and then fuses them into one consensus partition by solving a problem concerning graph partition with respect to the co-association matrix. Although the collaborative cluster structure among basic clusterings can be well discovered by ensemble clustering, most advanced ensemble clustering utilizes the self-representation strategy with the constraint of low-rank to explore a shared consensus representation matrix in multiple views. However, they still encounter two challenges: (1) high computational cost caused by both the matrix inversion operation and singular value decomposition of large-scale square matrices; (2) less considerable attention on high-order correlation attributed to the pursue of the two-dimensional pair-wise relationship matrix. In this article, based on low-rank and sparse decomposition from both matrix and tensor perspectives, we propose two novel multi-view ensemble clustering methods, which tangibly decrease computational complexity. Specifically, our first method utilizes low-rank and sparse matrix decomposition to learn one common co-association matrix, while our last method constructs all co-association matrices into one third-order tensor to investigate the high-order correlation among multiple views by low-rank and sparse tensor decomposition. We adopt the alternating direction method of multipliers to solve two convex models by dividing them into several subproblems with closed-form solution. Experimental results on ten real-world datasets prove the effectiveness and efficiency of the proposed two multi-view ensemble clustering methods by comparing them with other advanced ensemble clustering methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ensemble clustering; low-rank and sparse decomposition; Multi-view clustering,Data mining; Singular value decomposition; Tensors; Clustering methods; Clusterings; Co-association matrix; Ensemble clustering; Higher order correlation; Low-rank and sparse decompositions; matrix; Multi-view clustering; Multi-views; Multiple views; Cluster analysis
Dual-aware Domain Mining and Cross-aware Supervision for Weakly-supervised Semantic Segmentation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164294190&doi=10.1145%2f3589343&partnerID=40&md5=db9250a411e5a980f18f47b254de35df,"Weakly Supervised Semantic Segmentation with image-level annotation uses localization maps from the classifier to generate pseudo labels. However, such localization maps focus only on sparse salient object regions, it is difficult to generate high-quality segmentation labels, which deviates from the requirement of semantic segmentation. To address this issue, we propose a dual-aware domain mining and cross-aware supervision (DDMCAS) method for weakly-supervised semantic segmentation. Specifically, we propose a dual-aware domain mining (DDM) module consisting of graph-based global reasoning unit and salient-region extension controller, which produces dense localization maps by exploring object features in salient regions and adjacent non-salient regions simultaneously. In order to further bridge the gap between salient regions and adjacent non-salient regions to generate more refined localization maps, we propose a cross-aware supervision (CAS) strategy to recover missing parts of the target objects and enhance weak attention in adjacent non-salient regions, leading to pseudo labels of higher quality for training the segmentation network. Based on the generated pseudo-labels, extensive experiments on PASCAL VOC 2012 dataset demonstrate that our method outperforms state-of-the-art methods using image-level labels for weakly supervised semantic segmentation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cross-aware supervision; dual-aware domain mining; image-level label; Weakly-supervised semantic segmentation,Graphic methods; Semantics; Cross-aware supervision; Dual-aware domain mining; High-quality segmentation; Image-level label; Localisation; Object region; Salient objects; Salient regions; Semantic segmentation; Weakly-supervised semantic segmentation; Semantic Segmentation
Robust Influence Maximization Under Both Aleatory and Epistemic Uncertainty,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164294230&doi=10.1145%2f3587100&partnerID=40&md5=4b0eee78fafd6c258278040858213cd8,"Uncertainty is ubiquitous in almost every real-life optimization problem, which must be effectively managed to get a robust outcome. This is also true for the Influence Maximization (IM) problem, which entails locating a set of influential users within a social network. However, most of the existing IM approaches have overlooked the uncertain factors in finding the optimal solution, which often leads to subpar performance in reality. A few recent studies have considered only the epistemic uncertainty (i.e., arises from the imprecise data), while ignoring completely the aleatory uncertainty (i.e., arises from natural or physical variability). In this article, we propose a formulation and a novel algorithm for the Robust Influence Maximization (RIM) problem under both types of uncertainties. First, we develop a robust influence spread function under aleatory uncertainty that, in contrast to the existing IM theory, is no longer monotone and submodular. Thereafter, we expand our RIM formulation to incorporate epistemic uncertainty aiming to maximize the robust ratio between the selected worst-case solution and the best-case optimal solution, adopting a conservative approach. Furthermore, using a chance-constraint-based method, we investigated feasibility robustness by accounting for the uncertainties related to constraint functions. Finally, an Evolutionary Algorithm (named EA-RIM) is designed to solve the proposed formulation of the RIM problem. Experimental evaluation results on four empirical datasets show that our proposed formulation and algorithm are more effective in dealing with uncertainties and finding an optimal solution for the RIM problem.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",evolutionary computation; Robust Influence Maximization; social network analysis; Uncertainty modeling,Computation theory; Economic and social effects; Optimal systems; Uncertainty analysis; Aleatory and epistemic uncertainties; Aleatory uncertainty; Epistemic uncertainties; Influence maximizations; Maximization problem; Optimal solutions; Robust influence maximization; Social Network Analysis; Uncertainty; Uncertainty models; Evolutionary algorithms
Kernel Fisher Dictionary Transfer Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164283653&doi=10.1145%2f3588575&partnerID=40&md5=1b8fff2a8e8206b65b210001808773a9,"Dictionary learning is an efficient knowledge representation method that can learn the essential features of data. Traditional dictionary learning methods are difficult to obtain nonlinear information when processing large-scale and high-dimensional datasets. While most dictionary learning algorithms are based on the assumption that the training data and test data have the same feature distribution, which is not always true in practical applications. To address the above problems, we propose the Kernel Fisher Dictionary Transfer Learning (KFDTL) algorithm. First, we map each sample to high-dimensional space through kernel mapping and use any dictionary learning algorithm to learn the essential features. Then, the feature-based transfer learning method is performed to predict the labels of the target samples. This method includes three main contributions: (1) KFDTL constructs a discriminative Fisher embedding model to make the same class samples have similar coding coefficients; (2) Based on the relationship between profiles and atoms, KFDTL constructs an adaptive model that adapts source domain samples to target domain samples; (3) The kernel method is used to efficiently solve nonlinear problems. Experiments on a large number of public image datasets have proved the effectiveness of the proposed method. The source code of the proposed method is available at https://github.com/zzfan3/KFDTL.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Dictionary learning; kernel sparse representation; maximum mean discrepancy; transfer learning,Knowledge representation; Large dataset; Learning algorithms; Dictionary learning; Dictionary learning algorithms; Essential features; Kernel Fisher; Kernel sparse representation; Knowledge representation method; Learn+; Maximum mean discrepancy; Sparse representation; Transfer learning; Learning systems
Towards Informative and Diverse Dialogue Systems Over Hierarchical Crowd Intelligence Knowledge Graph,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164301483&doi=10.1145%2f3583758&partnerID=40&md5=886d33415dbf6ece19b13c92c88ec6cf,"Knowledge-enhanced dialogue systems aim at generating factually correct and coherent responses by reasoning over knowledge sources, which is a promising research trend. The truly harmonious human-agent dialogue systems need to conduct engaging conversations from three aspects as humans, namely (1) stating factual contents (e.g., records in Wikipedia), (2) conveying subjective and informative opinions about objects (e.g., user discussions on Twitter), and (3) impressing interlocutors with diverse expression styles (e.g., personalized expression habits). The existing knowledge base is a standardized and unified coding for factual knowledge, which could not portray the other two kinds of knowledge to make responses more informative and expressive diverse. To address this, we present CrowdDialog, a crowd intelligence knowledge-enhanced dialogue system, which takes advantage of ""crowd intelligence knowledge""extracted from social media (with rich subjective descriptions and diversified expression styles) to promote the performance of dialogue systems. Firstly, to thoroughly mine and organize the crowd intelligence knowledge underlying large-scale and unstructured online contents, we elaborately design the Crowd Intelligence Knowledge Graph (CIKG) structure, including the domain commonsense subgraph, descriptive subgraph, and expressive subgraph. Secondly, to reasonably integrate heterogeneous crowd intelligence knowledge into responses while ensuring logicality and fluency, we propose the Gated Fusion with Dynamic Knowledge-Dependent (GFDD) model, which generates responses from the semantic and syntactic perspective with the context-aware knowledge gate and dynamic knowledge decoding. Finally, extensive experiments over both Chinese and English dialogue datasets demonstrate that our approach GFDD outperforms competitive baselines in terms of both automatic evaluation and human judgments. Besides, ablation studies indicate that the proposed CIKG has the potential to promote dialogue systems to generate fluent, informative, and diverse dialogue responses.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",crowd intelligence knowledge; Dialogue system; knowledge graph; social media,Hierarchical systems; Knowledge graph; Semantics; Speech processing; Crowd intelligence knowledge; Dialogue systems; Expression styles; Human agent; Knowledge graphs; Knowledge sources; Research trends; Social media; Subgraphs; Wikipedia; Social networking (online)
DuCape: Dual Quaternion and Capsule Network-Based Temporal Knowledge Graph Embedding,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164299671&doi=10.1145%2f3589644&partnerID=40&md5=ae3b13ea71483b106a35cf57e29b1645,"Recently, with the development of temporal knowledge graph technology, more and more Temporal Knowledge Graph Embedded (TKGE) models have been developed. The effectiveness of TKGE largely depends on the ability to model intrinsic relation patterns and capture specific information about entities and relations. However, existing approaches can capture only some of them with insufficient modeling capacity, and none has a ""deep""architecture for modeling the entries in a quadruple at the same dimension. In this article, we propose a more powerful KGE framework named DuCape, which combines a dual quaternion and capsule network in modeling for the first time to make up for the defects of existing TKGE models. In dual quaternion vector space, the head entity learns a k-dimensional rigid transformation parametrized by relation and time, falling near its corresponding tail entity. Further, we employ the embeddings of entities, relations, and time trained from dual quaternion vector space as the input to capsule networks. Experimental results on several basic datasets show that the DuCape model constructed in this article is superior to existing state-of-the-art models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",capsule network; dual quaternion; DuCape; Temporal knowledge graph embedded,Graph embeddings; Vector spaces; Capsule network; Dual quaternion; Ducape; Embedded models; Graph embeddings; Intrinsic relation; Knowledge graphs; Network-based; Temporal knowledge; Temporal knowledge graph embedded; Knowledge graph
Computing Graph Descriptors on Edge Streams,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164280126&doi=10.1145%2f3591468&partnerID=40&md5=84b44622ddacdf90e36c968cf8529564,"Feature extraction is an essential task in graph analytics. These feature vectors, called graph descriptors, are used in downstream vector-space-based graph analysis models. This idea has proved fruitful in the past, with spectral-based graph descriptors providing state-of-the-art classification accuracy. However, known algorithms to compute meaningful descriptors do not scale to large graphs since: (1) they require storing the entire graph in memory, and (2) the end-user has no control over the algorithm's runtime. In this article, we present streaming algorithms to approximately compute three different graph descriptors capturing the essential structure of graphs. Operating on edge streams allows us to avoid storing the entire graph in memory, and controlling the sample size enables us to keep the runtime of our algorithms within desired bounds. We demonstrate the efficacy of the proposed descriptors by analyzing the approximation error and classification accuracy. Our scalable algorithms compute descriptors of graphs with millions of edges within minutes. Moreover, these descriptors yield predictive accuracy comparable to the state-of-the-art methods but can be computed using only 25% as much memory.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",edge stream; graph classification; Graph descriptor,Classification accuracy; Descriptors; Down-stream; Edge stream; Features extraction; Features vector; Graph classification; Graph descriptor; Graph-analytic; Runtimes; Vector spaces
Combining Diverse Meta-Features to Accurately Identify Recurring Concept Drift in Data Streams,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152581046&doi=10.1145%2f3587098&partnerID=40&md5=d6eca6ff0d033dec8082deb9a9ee0f99,"Learning from streaming data is challenging as the distribution of incoming data may change over time, a phenomenon known as concept drift. The predictive patterns, or experience learned under one distribution may become irrelevant as conditions change under concept drift, but may become relevant once again when conditions reoccur. Adaptive learning methods adapt a classifier to concept drift by identifying which distribution, or concept, is currently present in order to determine which experience is relevant. Identifying a concept requires some representation to be stored for comparison, with the quality of the representation being key to accurate identification. Existing concept representations are based on meta-features, efficient univariate summaries of a concept. However, no single meta-feature can fully represent a concept, leading to severe accuracy loss when existing representations cannot describe concept drift. To avoid these failure cases, we propose the first general framework for combining a diverse range of meta-features into a single representation. We solve two main challenges, first presenting a method of efficiently computing, storing, and querying an arbitrary set of meta-features as a single representation, showing that a combination of meta-features may successfully avoid failure cases seen with existing methods. Second, we present the first method for dynamically learning which meta-features distinguish concepts in any given dataset, significantly improving performance. Our proposed approach enables state-of-the-art feature selection methods, such as mutual information, to be applied to concept representation meta-features for the first time. We investigate tradeoffs between memory budget and classification performance, observing accuracy increases of up to 16% by dynamically weighting the contribution of each meta-feature.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive learning; Data Streaming; meta-features,Data mining; Feature Selection; Learning systems; Adaptive learning; Adaptive learning method; Change-over time; Concept drifts; Condition; Data stream; Data streaming; Metafeature; Streaming data; Univariate; Budget control
SMONE: A Session-based Recommendation Model Based on Neighbor Sessions with Similar Probabilistic Intentions,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164297655&doi=10.1145%2f3587099&partnerID=40&md5=7545e0536be6ae70146161f96cf4ebca,"A session-based recommendation system (SRS) tries to predict the next possible choice of anonymous users. In recent years, graph neural network (GNN) models have been successfully applied to SRSs and have achieved great success. Using GNN models in SRSs, each session graph is processed successively to obtain the embedding of the node (i.e, each action on an item), which is then imported into the prediction module to generate recommendation results. However, solely depending on the session graph to obtain the node embeddings is not sufficient because each session only involves a few items. Therefore, neighbor sessions have been used to extend the session graph to learn more informative node representations. In this paper, we introduce a Session-based recommendation MOdel based on Neighbor sessions with similar probabilistic int Entions(SMONE). SMONE models the intentions behind sessions in a probabilistic way and retrieves the neighbor sessions with similar intentions. After the neighbor sessions are found, the target session and its neighbor sessions are modeled as a hypyergraph to learn the contextualized embeddings, which are combined with item embeddings through GNN to produce the final item recommendations. Experiments on real-world datasets prove the effectiveness and superiority of SMONE.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",neighbor sessions; probabilistic intentions; Session-based recommender systems,Graph embeddings; Graph neural networks; Graph theory; Neural network models; Embeddings; Graph neural networks; Learn+; Model-based OPC; Neighbor session; Neural network model; Probabilistic intention; Probabilistics; Real-world datasets; Session-based recommende system; Recommender systems
Graph Neural Networks with Motisf-aware for Tenuous Subgraph Finding,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161317126&doi=10.1145%2f3589643&partnerID=40&md5=77102cebc0154a12e4b84a0391060276,"Tenuous subgraph finding aims to detect a subgraph with few social interactions and weak relationships among nodes. Despite significant efforts made on this task, they are mostly carried out in view of graph-structured data. These methods depend on calculating the shortest path and need to enumerate all the paths between nodes, which suffer the combinatorial explosion. Moreover, they all lack the integration of neighborhood information. To this end, we propose a novel model named Graph Neural Network with Motif-aware for tenuous subgraph finding (GNNM), a neighborhood aggregation-based GNN framework that can capture the latent relationship between nodes. We design a GNN module to project nodes into a low-dimensional vector combining the higher-order correlation within nodes based on a motif-aware module. Then we design greedy algorithms in vector space to obtain a tenuous subgraph whose size is greater than a specified constraint. Particularly, considering that existing evaluation indicators cannot capture the latent friendship between nodes, we introduce a novel Potential Friend concept to measure the tenuity of a graph from a new perspective. Experimental results on the real-world and synthetic datasets demonstrate that our proposed method GNNM outperforms existing algorithms in efficiency and subgraph quality.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph neural network; network motif; Social network; tenuous subgraph,Data mining; Graph theory; Vector spaces; Combinatorial explosion; Graph neural networks; Graph structured data; Neighborhood information; Network motif; Short-path; Social interactions; Social network; Subgraphs; Tenuous subgraph; Graph neural networks
Learning the Explainable Semantic Relations via Unified Graph Topic-Disentangled Neural Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163937900&doi=10.1145%2f3589964&partnerID=40&md5=bb37cf6d0289681ee434933c0762691c,"Graph Neural Networks (GNNs) such as Graph Convolutional Networks (GCNs) can effectively learn node representations via aggregating neighbors based on the relation graph. However, despite a few exceptions, most of the previous work in this line does not consider the topical semantics underlying the edges, making the node representations less effective and the learned relation between nodes hard to explain. For instance, the current GNNs make us usually don't know what is the reason for the connection of network nodes, such as the specific research topics cited in this article and the concerns among friends on social platforms. Some methods have begun to explore the extraction of relation semantics in recent related literature, but existing studies generally face two bottlenecks, i.e., either being unable to explain the mined latent relations to ensure their reasonableness and independence, or demanding the textual content of edges which is unavailable in most real-world datasets. Actually, these two issues are both crucial in practical use. In our work, we propose a novel Topic-Disentangled Graph Neural Network (TDG) to address the above two issues at the same time, which explores the relation topics from the perspective of node contents. We design an optimized graph topic module to handle node features to construct independent and explainable semantic subspaces, then the reasonable relation topics that correspond to these subspaces are assigned to each graph relation via a neighborhood routing mechanism. Our proposed model can be easily combined with related graph tasks to form an end-to-end model, to avoid the risk of deviation between node representation space and task space. To evaluate the efficiency of our model, sufficient node-related tasks are conducted on three public datasets in the experimental section. The results show the obvious superiority of TDG compared with the state-of-the-art models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph neural networks; network analysis; neural topic model; node representation; Relation topics,Data mining; Graph theory; Semantic Web; Semantics; AS graph; Convolutional networks; Graph neural networks; Learn+; Neural topic model; Neural-networks; Node representation; Relation topic; Semantic relations; Topic Modeling; Graph neural networks
ADATIME: A Benchmarking Suite for Domain Adaptation on Time Series Data,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152138478&doi=10.1145%2f3587937&partnerID=40&md5=6a1e6e6171414826241ed299133d3351,"Unsupervised domain adaptation methods aim at generalizing well on unlabeled test data that may have a different (shifted) distribution from the training data. Such methods are typically developed on image data, and their application to time series data is less explored. Existing works on time series domain adaptation suffer from inconsistencies in evaluation schemes, datasets, and backbone neural network architectures. Moreover, labeled target data are often used for model selection, which violates the fundamental assumption of unsupervised domain adaptation. To address these issues, we develop a benchmarking evaluation suite (AdaTime) to systematically and fairly evaluate different domain adaptation methods on time series data. Specifically, we standardize the backbone neural network architectures and benchmarking datasets, while also exploring more realistic model selection approaches that can work with no labeled data or just a few labeled samples. Our evaluation includes adapting state-of-the-art visual domain adaptation methods to time series data as well as the recent methods specifically developed for time series data. We conduct extensive experiments to evaluate 11 state-of-the-art methods on five representative datasets spanning 50 cross-domain scenarios. Our results suggest that with careful selection of hyper-parameters, visual domain adaptation methods are competitive with methods proposed for time series domain adaptation. In addition, we find that hyper-parameters could be selected based on realistic model selection approaches. Our work unveils practical insights for applying domain adaptation methods on time series data and builds a solid foundation for future works in the field. The code is available at github.com/emadeldeen24/AdaTime.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Domain adaptation; time series; transfer learning,Benchmarking; Network architecture; Petroleum reservoir evaluation; Adaptation methods; Domain adaptation; Hyper-parameter; Model Selection; Neural network architecture; Realistic model; Test data; Time-series data; Times series; Transfer learning; Time series
Causal Discovery via Causal Star Graphs,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164286828&doi=10.1145%2f3586997&partnerID=40&md5=fbcf148acc0d39cf7ee5144e03d74cc8,"Discovering causal relationships among observed variables is an important research focus in data mining. Existing causal discovery approaches are mainly based on constraint-based methods and functional causal models (FCMs). However, the constraint-based method cannot identify the Markov equivalence class and the functional causal models cannot identify the complex interrelationships when multiple variables affect one variable. To address the two aforementioned problems, we propose a new graph structure Causal Star Graph (CSG) and a corresponding framework Causal Discovery via Causal Star Graphs (CD-CSG) to divide a causal directed acyclic graph into multiple CSGs for causal discovery. In this framework, we also propose a generalized learning in CSGs based on a variational approach to learn the representative intermediate variable of CSG's non-central variables. Through the generalized learning in CSGs, the asymmetry in the forward and backward model of CD-CSG can be found to identify the causal directions in the directed acyclic graphs. We further divide the CSGs into three categories and provide the causal identification principle under each category in our proposed framework. Experiments using synthetic data show that the causal relationships between variables can be effectively identified with CD-CSG and the accuracy of CD-CSG is higher than the best existing model. By applying CD-CSG to real-world data, our proposed method can greatly augment the applicability and effectiveness of causal discovery.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",additive noise model; Causal discovery; causal star graph; functional causal model,Data mining; Directed graphs; Equivalence classes; Graphic methods; Acyclic graphs; Additive noise model; Causal discovery; Causal modeling; Causal relationships; Causal star graph; Constraint based method; Functional causal model; Noise models; Star graphs; Additive noise
DMGF-Net: An Efficient Dynamic Multi-Graph Fusion Network for Traffic Prediction,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163389326&doi=10.1145%2f3586164&partnerID=40&md5=f5ecf6edaff1f9cb9a7245892730995a,"Traffic prediction is the core task of intelligent transportation system (ITS) and accurate traffic prediction can greatly improve the utilization of public resources. Dynamic interaction of multiple spatial relationships will influence the accuracy of traffic prediction. However, many existing methods only consider static spatial relationships, which restricts the accuracy of the prediction. To address the above problem, in this article, we propose the Dynamic Multi-Graph Fusion Network (DMGF-Net) to model the spatial-temporal correlations in traffic network. In the DMGF-Net, the fusion graph is designed to leverage and extract the various spatial correlations between different regions by fusing spatial graph, semantic graph, and spatial-semantic graph. Further, to dynamically learn the importance of different neighbors, we design the Dynamic Spatial-Temporal Unit (DSTU), which can adjust the aggregation weights of different neighbors by combining the convolution operation and the attention mechanism. It can selectively aggregate spatial-temporal features from different neighbors. Extensive experiments on three datasets demonstrate that effectiveness of our model, especially on PEMS08, our model achieves an increase of about 8.55% and 7.55% in terms of MAE and RMSE than the static model STGCN. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph convolution networks; spatial-temporal correlations; spatial-temporal modeling; Traffic prediction,Forecasting; Graph neural networks; Intelligent systems; Intelligent vehicle highway systems; Semantics; Traffic control; Dynamic interaction; Graph convolution network; Graph semantics; Intelligent transportation systems; Public resources; Semantic graphs; Spatial relationships; Spatial temporal model; Spatial-temporal correlation; Traffic prediction; Convolution
Evolving Social Media Background Representation with Frequency Weights and Co-Occurrence Graphs,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164278305&doi=10.1145%2f3585389&partnerID=40&md5=49d29394c8dbaafbce8ec9f37d0117e7,"Social media as a background information source has been utilized in many practical computational tasks, such as stock price prediction, epidemic tracking, and product recommendation. However, proper representation of an evolving social media background is still in an early research stage. In this article, we propose a representation method that considers temporal novelties as well as the fine details of word inter-dependencies. Our method is based on the tf-idf and graph embedding techniques. The proposed method has superiority over other representation methods because it takes the advantage of both the temporal aspect of tf-idf and the semantic aspect of graph embeddings. We compare our method with a variety of baselines in two practical application scenarios using real-world data. In tweet popularity prediction, our representation achieves 5.7% less error and 12.8% higher correlation compared to the best baseline. In e-commerce product recommendation, our representation achieves 17% higher hit-rate and 20% higher NDCG compared to the best baseline.  © 2023 Copyright held by the owner/author(s).",graph method; product recommendation; Social media representation; temporal method; tweet popularity prediction,Forecasting; Graph embeddings; Social networking (online); Graph embeddings; Graph methods; Media representation; Popularity predictions; Product recommendation; Representation method; Social media; Social medium representation; Temporal method; Tweet popularity prediction; Semantics
Neighborhood Weighted Voting-Based Noise Correction for Crowdsourcing,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153683244&doi=10.1145%2f3586998&partnerID=40&md5=82dcb2067d6d959f3c3f909f4d75153f,"In crowdsourcing scenarios, we can obtain each instance's multiple noisy labels set from different crowd workers and then use a ground truth inference algorithm to infer its integrated label. Despite the effectiveness of ground truth inference algorithms, a certain level of noise still remains in the integrated labels. To reduce the impact of noise, many noise correction algorithms have been proposed in recent years. To the best of our knowledge, however, nearly all existing noise correction algorithms only exploit each instance's own multiple noisy label sets but ignore the multiple noisy label sets of its neighbors. Here neighbors refer to the nearest instances found in the feature space based on the distance metric learning. In this article, we propose neighborhood weighted voting-based noise correction (NWVNC). In NWVNC, we at first take advantage of the multiple noisy label sets of each instance's neighbors (including itself) to estimate the probability that it belongs to its integrated label. Then, we use the estimated probability to identify and filter noise instances and thus obtain a clean set and a noise set. Finally, we train three heterogeneous classifiers on the clean set and correct the noise instances by the consensus voting of three trained classifiers. The experimental results on 34 simulated and two real-world crowdsourced datasets show that NWVNC significantly outperforms all the other state-of-the-art noise correction algorithms used for comparison.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",consensus voting; Crowdsourcing learning; neighborhood weighted voting; noise correction,Inference engines; Consensus voting; Correction algorithms; Crowdsourcing learning; Ground truth; Label sets; Neighborhood weighted voting; Neighbourhood; Noise correction; Noisy labels; Weighted voting; Crowdsourcing
Optimal Scale-Free Small-World Graphs with Minimum Scaling of Cover Time,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164286827&doi=10.1145%2f3583691&partnerID=40&md5=9ba80a46559bd6fa36d68cfdded15716,"The cover time of random walks on a graph has found wide practical applications in different fields of computer science, such as crawling and searching on the World Wide Web and query processing in sensor networks, with the application effects dependent on the behavior of the cover time: the smaller the cover time, the better the application performance. It was proved that over all graphs with N nodes, complete graphs have the minimum cover time N log N. However, complete graphs cannot mimic real-world networks with small average degree and scale-free small-world properties, for which the cover time has not been examined carefully, and its behavior is still not well understood. In this article, we first experimentally evaluate the cover time for various real-world networks with scale-free small-world properties, which scales as N log N. To better understand the behavior of the cover time for real-world networks, we then study the cover time of three scale-free small-world model networks by using the connection between cover time and resistance diameter. For all the three networks, their cover time also behaves as N log N. This work indicates that sparse networks with scale-free and small-world topology are favorable architectures with optimal scaling of cover time. Our results deepen understanding the behavior of cover time in real-world networks with scale-free small-world structure, and have potential implications in the design of efficient algorithms related to cover time.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",complex network; cover time; electrical network; graph mining; Random walk,Complex networks; Data mining; Graph theory; Graphic methods; Random processes; Complete graphs; Cover time; Electrical networks; Graph mining; Property; Random Walk; Real-world networks; Scale-free; Scalings; Small worlds; Sensor networks
Multi-Level Visual Similarity Based Personalized Tourist Attraction Recommendation Using Geo-Tagged Photos,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164284527&doi=10.1145%2f3582015&partnerID=40&md5=f2fe58c11b77e41cd4948f8539c0f8a0,"Geo-tagged photo-based tourist attraction recommendation can discover users' travel preferences from their taken photos, so as to recommend suitable tourist attractions to them. However, existing visual content-based methods cannot fully exploit the user and tourist attraction information of photos to extract visual features, and do not differentiate the significance of different photos. In this article, we propose multi-level visual similarity-based personalized tourist attraction recommendation using geo-tagged photos (MEAL). MEAL utilizes the visual contents of photos and interaction behavior data to obtain the final embeddings of users and tourist attractions, which are then used to predict the visit probabilities. Specifically, by crossing the user and tourist attraction information of photos, we define four visual similarity levels and introduce a corresponding quintuplet loss to embed the visual contents of photos. In addition, to capture the significance of different photos, we exploit the self-attention mechanism to obtain the visual representations of users and tourist attractions. We conducted experiments on two datasets crawled from Flickr, and the experimental results proved the advantage of this method.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Geo-tagged photos; multi-level visual similarity; personalized tourist attraction recommendation; quintuplet loss; self-attention,Content-based methods; Geo-tagged photo; Multi-level visual similarity; Multilevels; Personalized tourist attraction recommendation; Quintuplet loss; Self-attention; Tourist attractions; Visual content; Visual similarity
Privacy-Preserving Personalized Fitness Recommender System P3FitRec: A Multi-level Deep Learning Approach,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154545978&doi=10.1145%2f3572899&partnerID=40&md5=519fa57c738150c257722e2752b6d726,"Recommender systems have been successfully used in many domains with the help of machine learning algorithms. However, such applications tend to use multi-dimensional user data, which has raised widespread concerns about the breach of users' privacy. Meanwhile, wearable technologies have enabled users to collect fitness-related data through embedded sensors to monitor their conditions or achieve personalized fitness goals. In this article, we propose a novel privacy-aware personalized fitness recommender system. We introduce a multi-level deep learning framework that learns important features from a large-scale real fitness dataset that is collected from wearable Internet of Things (IoT) devices to derive intelligent fitness recommendations. Unlike most existing approaches, our approach achieves personalization by inferring the fitness characteristics of users from sensory data, minimizing the need for explicitly collecting user identity or biometric information, such as name, age, height, and weight. Our proposed models and algorithms predict (a) personalized exercise distance recommendations to help users to achieve target calories, (b) personalized speed sequence recommendations to adjust exercise speed given the nature of the exercise and the chosen route, and (c) personalized heart rate sequence to guide the user of the potential health status for future exercises. Our experimental evaluation on a real-world Fitbit dataset demonstrated high accuracy in predicting exercise distance, speed sequence, and heart rate sequence compared with similar studies.1 Furthermore, our approach is novel compared with existing studies, as it does not require collecting and using users' sensitive information. Thus, it preserves the users' privacy.  © 2023 Copyright held by the owner/author(s).",deep learning; fitness; Personalization; recommender system; sensors,Deep learning; Heart; Internet of things; Large dataset; Learning algorithms; Learning systems; Privacy-preserving techniques; Deep learning; Fitness; Heart-rate; Learning approach; Machine learning algorithms; Multi dimensional; Multilevels; Personalizations; Privacy preserving; User privacy; Recommender systems
Review of Clustering Methods for Functional Data,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164297356&doi=10.1145%2f3581789&partnerID=40&md5=400ada786b160f7711a74f5023d0cec2,"Functional data clustering is to identify heterogeneous morphological patterns in the continuous functions underlying the discrete measurements/observations. Application of functional data clustering has appeared in many publications across various fields of sciences, including but not limited to biology, (bio)chemistry, engineering, environmental science, medical science, psychology, social science, and so on. The phenomenal growth of the application of functional data clustering indicates the urgent need for a systematic approach to develop efficient clustering methods and scalable algorithmic implementations. On the other hand, there is abundant literature on the cluster analysis of time series, trajectory data, spatio-temporal data, and so on, which are all related to functional data. Therefore, an overarching structure of existing functional data clustering methods will enable the cross-pollination of ideas across various research fields. We here conduct a comprehensive review of original clustering methods for functional data. We propose a systematic taxonomy that explores the connections and differences among the existing functional data clustering methods and relates them to the conventional multivariate clustering methods. The structure of the taxonomy is built on three main attributes of a functional data clustering method and therefore is more reliable than existing categorizations. The review aims to bridge the gap between the functional data analysis community and the clustering community and to generate new principles for functional data clustering.  © 2023 Copyright held by the owner/author(s).",Curve registration; dependent functional data; multivariate functional data; shape analysis,Taxonomies; Time series analysis; Clustering methods; Continuous functions; Curve registration; Data clustering; Data clustering methods; Dependent functional data; Functional datas; Morphological patterns; Multivariate functional datum; Shape-analysis; Cluster analysis
Effective and Scalable Manifold Ranking-Based Image Retrieval with Output Bound,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153872451&doi=10.1145%2f3565574&partnerID=40&md5=9e16feddd12bf90ff24fa992136d6789,"Image retrieval keeps attracting a lot of attention from both academic and industry over past years due to its variety of useful applications. Due to the rapid growth of deep learning approaches, more better feature vectors of images could be discovered for improving image retrieval. However, most (if not all) existing deep learning approaches consider the similarity between two images locally without considering the similarity among a groupof similar images globally, and thus could not return accurate results. In this article, we study the image retrieval with manifold ranking (MR) which considers both the local similarity and the global similarity, which could give more accurate results. However, existing best-known algorithms have one of the following issues: (1) they require to build a bulky index, (2) some of them do not have any theoretical bound on the output, and (3) some of them are time-consuming. Motivated by this, we propose two algorithms, namely Monte Carlo-based MR (MCMR) and MCMR+, for image retrieval, which do not have the above issues. We are the first one to propose an index-free manifold ranking image retrieval with the output theoretical bound. More importantly, our algorithms give the first best-known time complexity result of where is the total number of images in the database compared with the existing best-known result of in the literature of computing the exact top- results with quality guarantee. Lastly, our experimental result shows that MCMR+ outperforms existing algorithms by up to four orders of magnitude in terms of query time.  © 2023 Association for Computing Machinery.",efficient algorithms; image retrieval; manifold ranking; Similarity measures,Deep learning; Image enhancement; Query processing; Efficient algorithm; Features vector; Learning approach; Manifold ranking; Manifold ranking based image retrievals; Output bounds; Rapid growth; Similar image; Similarity measure; Theoretical bounds; Image retrieval
LoSAC: An Efficient Local Stochastic Average Control Method for Federated Optimization,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156176844&doi=10.1145%2f3566128&partnerID=40&md5=f727c8fa9c1ef9a3dffcf260152b06cb,"Federated optimization (FedOpt), which targets at collaboratively training a learning model across a large number of distributed clients, is vital for federated learning. The primary concerns in FedOpt can be attributed to the model divergence and communication efficiency, which significantly affect the performance. In this article, we propose a new method, i.e., LoSAC, to learn from heterogeneous distributed data more efficiently. Its key algorithmic insight is to locally update the estimate for the global full gradient after each regular local model update. Thus, LoSAC can keep clients' information refreshed in a more compact way. In particular, we have studied the convergence result for LoSAC. Besides, the bonus of LoSAC is the ability to defend the information leakage from the recent technique Deep Leakage Gradients (DLG). Finally, experiments have verified the superiority of LoSAC comparing with state-of-the-art FedOpt algorithms. Specifically, LoSAC significantly improves communication efficiency by more than 100% on average, mitigates the model divergence problem, and equips with the defense ability against DLG.  © 2023 Copyright held by the owner/author(s).",client sampling; communication efficiency; data heterogeneity; Federated optimization; model divergence,Efficiency; Learning systems; Stochastic models; Client sampling; Communication efficiency; Control methods; Data heterogeneity; Federated optimization; Learning models; Model divergence; Optimisations; Performance; Stochastics; Stochastic systems
Ada-MIP: Adaptive Self-supervised Graph Representation Learning via Mutual Information and Proximity Optimization,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153855059&doi=10.1145%2f3568165&partnerID=40&md5=e6b030e65fd56cfdf06a6c5ee0f0423c,"Self-supervised graph-level representation learning has recently received considerable attention. Given varied input distributions, jointly learning graphs' unique and common features is vital to downstream tasks. Inspired by graph contrastive learning (GCL), which targets maximizing the agreement between graph representations from different views, we propose an Adaptive self-supervised framework, Ada-MIP, considering both Mutual Information between views (unique features) and inter-graph Proximity (common features). Specifically, Ada-MIP learns graphs' unique information through a learnable and probably injective augmenter, which can acquire more adaptive views compared to the augmentation strategies applied by existing GCL methods; to learn graphs' common information, we employ graph kernels to calculate graphs' proximity and learn graph representations among which the precomputed proximity is preserved. By sharing a global encoder, graphs' unique and common information can be well integrated into the graph representations learned by Ada-MIP. Ada-MIP is also extendable to semi-supervised scenarios, with our experiments confirming its superior performance in both unsupervised and semi-supervised tasks.  © 2023 Association for Computing Machinery.",graph neural network; Self-supervised learning; semi-supervised learning,Data mining; Graph algorithms; Graphic methods; Supervised learning; Common features; Graph neural networks; Graph representation; Learn+; Mutual informations; Optimisations; Self-supervised learning; Semi-supervised; Semi-supervised learning; Unique features; Graph neural networks
Edge-enhanced Global Disentangled Graph Neural Network for Sequential Recommendation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154610906&doi=10.1145%2f3577928&partnerID=40&md5=c9394d3e4c2f2a8f276df6cc988b81e9,"Sequential recommendation has been a widely popular topic of recommender systems. Existing works have contributed to enhancing the prediction ability of sequential recommendation systems based on various methods, such as recurrent networks and self-attention mechanisms. However, they fail to discover and distinguish various relationships between items, which could be underlying factors which motivate user behaviors. In this article, we propose an Edge-Enhanced Global Disentangled Graph Neural Network (EGD-GNN) model to capture the relation information between items for global item representation and local user intention learning. At the global level, we build a global-link graph over all sequences to model item relationships. Then a channel-aware disentangled learning layer is designed to decompose edge information into different channels, which can be aggregated to represent the target item from its neighbors. At the local level, we apply a variational auto-encoder framework to learn user intention over the current sequence. We evaluate our proposed method on three real-world datasets. Experimental results show that our model can get a crucial improvement over state-of-the-art baselines and is able to distinguish item features.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Disentangled representation learning; graph; sequential recommendation,Behavioral research; Data mining; Graph neural networks; Neural network models; Recurrent neural networks; Attention mechanisms; Disentangled representation learning; Graph; Graph neural networks; Neural network model; Recurrent networks; Sequential recommendation; Underlying factors; User behaviors; User's intentions; Recommender systems
Hypergraph Transformer Neural Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153856470&doi=10.1145%2f3565028&partnerID=40&md5=f55f2a30b340243f4d1697253e15938e,"Graph neural networks (GNNs) have been widely used for graph structure learning and achieved excellent performance in tasks such as node classification and link prediction. Real-world graph networks imply complex and various semantic information and are often referred to as heterogeneous information networks (HINs). Previous GNNs have laboriously modeled heterogeneous graph networks with pairwise relations, in which the semantic information representation for learning is incomplete and severely hinders node embedded learning. Therefore, the conventional graph structure cannot satisfy the demand for information discovery in HINs. In this article, we propose an end-to-end hypergraph transformer neural network (HGTN) that exploits the communication abilities between different types of nodes and hyperedges to learn higher-order relations and discover semantic information. Specifically, attention mechanisms weigh the importance of semantic information hidden in original HINs to generate useful meta-paths. Meanwhile, our method develops a multi-scale attention module to aggregate node embeddings in higher-order neighborhoods. We evaluate the proposed model with node classification tasks on six datasets: DBLP, ACM, IBDM, Reuters, STUD-BJUT, and Citeseer. Experiments on a large number of benchmarks show the advantages of HGTN.  © 2023 Association for Computing Machinery.",attention; Hypergraph; meta-paths; node classification; transformer,Graph neural networks; Graph structures; Graph theory; Graphic methods; Information services; Semantics; Attention; Graph neural networks; Heterogeneous information; Hyper graph; Information networks; Meta-path; Neural-networks; Node classification; Semantics Information; Transformer; Classification (of information)
Causal Disentanglement for Implicit Recommendations with Network Information,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164284304&doi=10.1145%2f3582435&partnerID=40&md5=9e18422507032b9b465c5e0ea56bc1ab,"Online user engagement is highly influenced by various machine learning models, such as recommender systems. These systems recommend new items to the user based on the user's historical interactions. Implicit recommender systems reflect a binary setting showing whether a user interacted (e.g., clicked on) with an item or not. However, the observed clicks may be due to various causes such as user's interest, item's popularity, and social influence factors. Traditional recommender systems consider these causes under a unified representation, which may lead to the emergence and amplification of various biases in recommendations. However, recent work indicates that by disentangling the unified representations, one can mitigate bias (e.g., popularity bias) in recommender systems and help improve recommendation performance. Yet, prior work in causal disentanglement in recommendations does not consider a crucial factor, that is, social influence. Social theories such as homophily and social influence provide evidence that a user's decision can be highly influenced by the user's social relations. Thus, accounting for the social relations while disentangling leads to less biased recommendations. To this end, we identify three separate causes behind an effect (e.g., clicks): (a) user's interest, (b) item's popularity, and (c) user's social influence. Our approach seeks to causally disentangle the user and item latent features to mitigate popularity bias in implicit feedback-based social recommender systems. To achieve this goal, we draw from causal inference theories and social network theories and propose a causality-aware disentanglement method that leverages both the user-item interaction network and auxiliary social network information. Experiments on real-world datasets against various state-of-the-art baselines validate the effectiveness of the proposed model for mitigating popularity bias and generating de-biased recommendations. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Causal recommender systems; conformity bias; popularity bias; social recommender systems,Economic and social effects; Online systems; Causal recommende system; Conformity bias; Item popularities; Network information; Online users; Popularity bias; Social influence; Social recommender systems; Social relations; Users' interests; Recommender systems
Auto-STGCN: Autonomous Spatial-Temporal Graph Convolutional Network Search,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154622576&doi=10.1145%2f3571285&partnerID=40&md5=132c9d429ad33c8d85d4f0cc93a69377,"In recent years, many spatial-temporal graph convolutional network (STGCN) models are proposed to deal with the spatial-temporal network data forecasting problem. These STGCN models have their own advantages, i.e., each of them puts forward many effective operations and achieves good prediction results in the real applications. If users can effectively utilize and combine these excellent operations integrating the advantages of existing models, then they may obtain more effective STGCN models thus create greater value using existing work. However, they fail to do so due to the lack of domain knowledge, and there is lack of automated system to help users to achieve this goal. In this article, we fill this gap and propose Auto-STGCN algorithm, which makes use of existing models to automatically explore high-performance STGCN model for specific scenarios. Specifically, we design Unified-STGCN framework, which summarizes the operations of existing architectures, and use parameters to control the usage and characteristic attributes of each operation, so as to realize the parameterized representation of the STGCN architecture and the reorganization and fusion of advantages. Then, we present Auto-STGCN, an optimization method based on reinforcement learning, to quickly search the parameter search space provided by Unified-STGCN, and generate optimal STGCN models automatically. Extensive experiments on real-world benchmark datasets show that our Auto-STGCN can find STGCN models superior to existing STGCN models used for search space construction, which demonstrates the effectiveness of our proposed method.  © 2023 Association for Computing Machinery.",Automated machine learning; neural architecture search; spatial-temporal graph convolutional network,Automation; Convolution; Convolutional neural networks; Domain Knowledge; Reinforcement learning; Automated machine learning; Automated machines; Convolutional networks; Machine-learning; Network models; Neural architecture search; Neural architectures; Spatial temporals; Spatial-temporal graph convolutional network; Temporal graphs; Network architecture
DeltaShield: Information Theory for Human- Trafficking Detection,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152629228&doi=10.1145%2f3563040&partnerID=40&md5=0b37bf48d7d462dadc9346f7393a5d34,"Given a million escort advertisements, how can we spot near-duplicates? Such micro-clusters of ads are usually signals of human trafficking (HT). How can we summarize them to convince law enforcement to act? Spotting micro-clusters of near-duplicate documents is useful in multiple, additional settings, including spam-bot detection in Twitter ads, plagiarism, and more.We present InfoShield, which makes the following contributions: practical, being scalable and effective on real data; parameter-free and principled, requiring no user-defined parameters; interpretable, finding a document to be the cluster representative, highlighting all the common phrases, and automatically detecting ""slots""(i.e., phrases that differ in every document); and generalizable, beating or matching domain-specific methods in Twitter bot detection and HT detection, respectively, as well as being language independent. Interpretability is particularly important for the anti-HT domain, where law enforcement must visually inspect ads.Our experiments on real data show that InfoShield correctly identifies Twitter bots with an F1 score over 90% and detects HT ads with 84% precision. Moreover, it is scalable, requiring about 8 hours for 4 million documents on a stock laptop. Our incremental version, DeltaShield, allows for fast, incremental updates, with minor loss of accuracy.  © 2023 Copyright held by the owner/author(s).",anti-human trafficking detection; minimum description length (MDL); Text mining,Botnet; Crime; Social networking (online); Anti-human trafficing detection; Bot detections; Human trafficking; Matchings; Micro-clusters; Minimum description length; Near-duplicates; Text-mining; User-defined parameters; Information theory
Summarizing User-item Matrix by Group Utility Maximization,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154542349&doi=10.1145%2f3578586&partnerID=40&md5=cf052678afc10a07a39a235ca662d6ee,"A user-item utility matrix represents the utility (or preference) associated with each (user, item) pair, such as citation counts, rating/vote on items or locations, and clicks on items. A high utility value indicates a strong association of the pair. In this work, we consider the problem of summarizing strong association for a large user-item matrix using a small summary size. Traditional techniques fail to distinguish user groups associated with different items (such as top-l item selection) or fail to focus on high utility (such as similarity- based subspace clustering and biclustering). We formulate a new problem, called Group Utility Maximization (GUM), to summarize the entire user population through k user groups and l items for each group; the goal is to maximize the total utility of selected items over all groups collectively. We show this problem is NP-hard even for l=1. We present two algorithms. One greedily finds the next group, called Greedy algorithm, and the other iteratively refines existing k groups, called k-max algorithm. Greedy algorithm provides the approximation guarantee for a nonnegative utility matrix, whereas k-max algorithm is more efficient for large datasets. We evaluate these algorithms on real-life datasets.  © 2023 Association for Computing Machinery.",Data summarization; Greedy algorithm; group utility maximization; k-max algorithm,Approximation algorithms; Clustering algorithms; Iterative methods; Matrix algebra; Data summarizations; Greedy algorithms; Group utility maximization; K-max algorithm; Max algorithms; User groups; User-item matrix; Utility matrices; Utility maximizations; Utility values; Large dataset
Learnable Graph-Regularization for Matrix Decomposition,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152629501&doi=10.1145%2f3544781&partnerID=40&md5=242ebefb762de72bed83398ddb3036be,"Low-rank approximation models of data matrices have become important machine learning and data mining tools in many fields, including computer vision, text mining, bioinformatics, and many others. They allow for embedding high-dimensional data into low-dimensional spaces, which mitigates the effects of noise and uncovers latent relations. In order to make the learned representations inherit the structures in the original data, graph-regularization terms are often added to the loss function. However, the prior graph construction often fails to reflect the true network connectivity and the intrinsic relationships. In addition, many graph-regularized methods fail to take the dual spaces into account. Probabilistic models are often used to model the distribution of the representations, but most of previous methods often assume that the hidden variables are independent and identically distributed for simplicity. To this end, we propose a learnable graph-regularization model for matrix decomposition (LGMD), which builds a bridge between graph-regularized methods and probabilistic matrix decomposition models for the first time. LGMD incorporates two graphical structures (i.e., two precision matrices) learned in an iterative manner via sparse precision matrix estimation and is more robust to noise and missing entries. Extensive numerical results and comparison with competing methods demonstrate its effectiveness.  © 2023 Copyright held by the owner/author(s).",Dual graph regularization; matrix normal distribution; probabilistic model; sparse precision estimation,Approximation theory; Clustering algorithms; Data mining; Graph theory; Iterative methods; Normal distribution; Numerical methods; Dual graph regularization; Dual graphs; matrix; Matrix decomposition; Matrix normal distribution; Precision estimation; Probabilistic models; Regularisation; Regularized method; Sparse precision estimation; Matrix algebra
Variational Graph Autoencoder with Adversarial Mutual Information Learning for Network Representation Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152620021&doi=10.1145%2f3555809&partnerID=40&md5=ef1754ac9b585b72e9f7c7c1d7bca6a0,"With the success of Graph Neural Network (GNN) in network data, some GNN-based representation learning methods for networks have emerged recently. Variational Graph Autoencoder (VGAE) is a basic GNN framework for network representation. Its purpose is to well preserve the topology and node attribute information of the network to learn node representation, but it only reconstructs network topology, and does not consider the reconstruction of node features. This strategy will make node representation can not well reserve node features information, impairing the ability of the VGAE method to learn higher quality representations. To solve this problem, we arise a new network representation method to improve the VGAE method for well retaining both node features and network structure information. The method utilizes adversarial mutual information learning to maximize the mutual information (MI) of node features and node representations during the encoding process of the variational autoencoder, which forces the variational encoder to get the representation containing the most informative node features. The method consists of three parts: a variational graph autoencoder includes a variational encoder (MI generator (G)) and a decoder, a positive MI sample module (maximizing MI module), and an MI discriminator (D). Furthermore, we explain why maximizing MI between node features and node representation can reconstruct node attributes. Finally, we conduct experiments on seven public representative datasets for nodes classification, nodes clustering, and graph visualization tasks. Experimental results demonstrate that the proposed algorithm significantly outperforms current popular network representation algorithms on these tasks. The best improvement is 17.13% than the VGAE method.  © 2023 Association for Computing Machinery.",adversarial learning; mutual information maximization; Network representation; variational graph auto-encoder,Classification (of information); Graph structures; Graph theory; Learning systems; Network coding; Adversarial learning; Auto encoders; Graph neural networks; In networks; Learn+; Mutual information maximization; Mutual informations; Network representation; Node attribute; Variational graph auto-encoder; Graph neural networks
CausalSE: Understanding Varied Spatial Effects with Missing Data Toward Adding New Bike-sharing Stations,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152632730&doi=10.1145%2f3536427&partnerID=40&md5=ad5c1acf218980a25b8a039164974b41,"To meet the growing bike-sharing demands and make people's travel convenient, the companies need to add new stations at locations where demands exceed supply. Before making reliable decisions on adding new stations, it is required to understand the spatial effects of new stations on the station network. In this article,, we study the deployment of the new station by estimating its varied causal effects on the demands of nearby stations, e.g., how does adding a new station (treatment) causally influence the demands (outcome) of nearby stations? When working with observational data, we should control hidden confounders, which cause spurious relations between treatments and outcomes. However, previous studies use historical data of the individual unit (e.g., the station's historical demands) to approximate its hidden confounders, which cannot deal with the lack of historical data for new stations. And the conventional methods overlook the differences between units, which cannot be applied to our problem. To overcome the challenges, we propose a novel model (CausalSE) to estimate the varied effects of new stations on nearby stations, which uses the shared knowledge (i.e., similar traveling patterns among stations) to approximate hidden confounders. Experimental results on real-world datasets show that CausalSE outperforms six state-of-the-art methods.  © 2023 Association for Computing Machinery.",Causal effect; deployment of new stations; spatial effect on station network; spatial-temporal computing,Causal effect; Confounder; Deployment of new station; Historical data; New stations; Spatial effect; Spatial effect on station network; Spatial temporals; Spatial-temporal computing; Station network
In-Processing Modeling Techniques for Machine Learning Fairness: A Survey,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137620248&doi=10.1145%2f3551390&partnerID=40&md5=1268743d976100455e3c5a0f555c853e,"Machine learning models are becoming pervasive in high-stakes applications. Despite their clear benefits in terms of performance, the models could show discrimination against minority groups and result in fairness issues in a decision-making process, leading to severe negative impacts on the individuals and the society. In recent years, various techniques have been developed to mitigate the unfairness for machine learning models. Among them, in-processing methods have drawn increasing attention from the community, where fairness is directly taken into consideration during model design to induce intrinsically fair models and fundamentally mitigate fairness issues in outputs and representations. In this survey, we review the current progress of in-processing fairness mitigation techniques. Based on where the fairness is achieved in the model, we categorize them into explicit and implicit methods, where the former directly incorporates fairness metrics in training objectives, and the latter focuses on refining latent representation learning. Finally, we conclude the survey with a discussion of the research challenges in this community to motivate future exploration.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bias mitigation; disparate impact; disparate treatment; Machine learning fairness,Decision making; Bias mitigation; Disparate impact; Disparate treatment; Machine learning fairness; Machine learning models; Machine-learning; Minority groups; Modelling techniques; Performance; Processing model; Machine learning
Personalized Federated Learning on Non-IID Data via Group-based Meta-learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152402081&doi=10.1145%2f3558005&partnerID=40&md5=ffdbb538dfeb3ca293bcc5e1e5f51d2b,"Personalized federated learning (PFL) has emerged as a paradigm to provide a personalized model that can fit the local data distribution of each client. One natural choice for PFL is to leverage the fast adaptation capability of meta-learning, where it first obtains a single global model, and each client achieves a personalized model by fine-tuning the global one with its local data. However, existing meta-learning-based approaches implicitly assume that the data distribution among different clients is similar, which may not be applicable due to the property of data heterogeneity in federated learning. In this work, we propose a Group-based Federated Meta-Learning framework, called G-FML, which adaptively divides the clients into groups based on the similarity of their data distribution, and the personalized models are obtained with meta-learning within each group. In particular, we develop a simple yet effective grouping mechanism to adaptively partition the clients into multiple groups. Our mechanism ensures that each group is formed by the clients with similar data distribution such that the group-wise meta-model can achieve ""personalization""at large. By doing so, our framework can be generalized to a highly heterogeneous environment. We evaluate the effectiveness of our proposed G-FML framework on three heterogeneous benchmarking datasets. The experimental results show that our framework improves the model accuracy by up to 13.15% relative to the state-of-the-art federated meta-learning.  © 2023 Association for Computing Machinery.",clustering methods; Federated learning; meta learning; neural networks,Clustering methods; Data distribution; Fast adaptations; Federated learning; Group-based; IID data; Local data; Metalearning; Neural-networks; Personalized model; Learning systems
Scheduling Hyperparameters to Improve Generalization: From Centralized SGD to Asynchronous SGD,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152633306&doi=10.1145%2f3544782&partnerID=40&md5=2695fa0dd243cbc9918bc4b188474c6d,"This article1 studies how to schedule hyperparameters to improve generalization of both centralized single-machine stochastic gradient descent (SGD) and distributed asynchronous SGD (ASGD). SGD augmented with momentum variants (e.g., heavy ball momentum (SHB) and Nesterov's accelerated gradient (NAG)) has been the default optimizer for many tasks, in both centralized and distributed environments. However, many advanced momentum variants, despite empirical advantage over classical SHB/NAG, introduce extra hyperparameters to tune. The error-prone tuning is the main barrier for AutoML.Centralized SGD: We first focus on centralized single-machine SGD and show how to efficiently schedule the hyperparameters of a large class of momentum variants to improve generalization. We propose a unified framework called multistage quasi-hyperbolic momentum (Multistage QHM), which covers a large family of momentum variants as its special cases (e.g., vanilla SGD/SHB/NAG). Existing works mainly focus on only scheduling learning rate α's decay, while multistage QHM allows additional varying hyperparameters (e.g., momentum factor), and demonstrates better generalization than only tuning α. We show the convergence of multistage QHM for general non-convex objectives. Distributed SGD: We then extend our theory to distributed asynchronous SGD (ASGD), in which a parameter server distributes data batches to several worker machines and updates parameters via aggregating batch gradients from workers. We quantify the asynchrony between different workers (i.e., gradient staleness), model the dynamics of asynchronous iterations via a stochastic differential equation (SDE), and then derive a PAC-Bayesian generalization bound for ASGD. As a byproduct, we show how a moderately large learning rate helps ASGD to generalize better. Our tuning strategies have rigorous justifications rather than a blind trial-and-error as we theoretically prove why our tuning strategies could decrease our derived generalization errors in both cases. Our strategies simplify the tuning process and beat competitive optimizers in test accuracy empirically. Our codes are publicly available https://github.com/jsycsjh/centralized-asynchronous-tuning.  © 2023 Copyright held by the owner/author(s).",asynchronous SGD; Deep learning optimization; generalization bound; hyperparameter tuning; multistage QHM; SGD momentum,Deep learning; Differential equations; Errors; Gradient methods; Learning algorithms; Optimization; Stochastic models; Stochastic systems; Asynchronoi stochastic gradient descent; Deep learning optimization; Generalization bound; Hyper-parameter; Hyperparameter tuning; Learning optimizations; Multi-stages; Multistage QHM; Stochastic gradient descent; Stochastic gradient descent momentum; Momentum
A Self-Representation Method with Local Similarity Preserving for Fast Multi-View Outlier Detection,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150194822&doi=10.1145%2f3532191&partnerID=40&md5=9ea96ce5751f907584877753973c9a5f,"With the rapidly growing attention to multi-view data in recent years, multi-view outlier detection has become a rising field with intense research. These researches have made some success, but still exist some issues that need to be solved. First, many multi-view outlier detection methods can only handle datasets that conform to the cluster structure but are powerless for complex data distributions such as manifold structures. This overly restrictive data assumption limits the applicability of these methods. In addition, almost the majority of multi-view outlier detection algorithms cannot solve the online detection problem of multi-view outliers. To address these issues, we propose a new detection method based on the local similarity relation and data reconstruction, i.e., the Self-Representation Method with Local Similarity Preserving for fast multi-view outlier detection (SRLSP). By using the local similarity structure, the proposed method fully utilizes the characteristics of outliers and detects outliers with an applicable objective function. Besides, a well-designed optimization algorithm is proposed, which completes each iteration with linear time complexity and can calculate each instance parallelly. Also, the optimization algorithm can be easily extended to the online version, which is more suitable for practical production environments. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed method on both performance and time complexity.  © 2023 Association for Computing Machinery.",adaptive similarity learning; multi-view data; Outlier detection; subspace learning,Iterative methods; Learning systems; Optimization; Statistics; Adaptive similarities; Adaptive similarity learning; Local similarity; Multi-view datum; Multi-views; Outlier Detection; Representation method; Similarity learning; Similarity preserving; Subspace learning; Anomaly detection
"Efficient and Effective Academic Expert Finding on Heterogeneous Graphs through (k, )-Core based Embedding",2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154595917&doi=10.1145%2f3578365&partnerID=40&md5=795a0959fe974e006d1572f70e9209a8,"Expert finding is crucial for a wealth of applications in both academia and industry. Given a user query and trove of academic papers, expert finding aims at retrieving the most relevant experts for the query, from the academic papers. Existing studies focus on embedding-based solutions that consider academic papers' textual semantic similarities to a query via document representation and extract the top-n experts from the most similar papers. Beyond implicit textual semantics, however, papers' explicit relationships (e.g., co-authorship) in a heterogeneous graph (e.g., DBLP) are critical for expert finding, because they help improve the representation quality. Despite their importance, the explicit relationships of papers generally have been ignored in the literature. In this article, we study expert finding on heterogeneous graphs by considering both the explicit relationships and implicit textual semantics of papers in one model. Specifically, we define the cohesive (k, )-core community of papers w.r.t. a meta-path (i.e., relationship) and propose a (k, )-core based document embedding model to enhance the representation quality. Based on this, we design a proximity graph-based index (PG-Index) of papers and present a threshold algorithm (TA)-based method to efficiently extract top-n experts from papers returned by PG-Index. We further optimize our approach in two ways: (1) we boost effectiveness by considering the (k, )-core community of experts and the diversity of experts' research interests, to achieve high-quality expert representation from paper representation; and (2) we streamline expert finding, going from ""extract top-n experts from top-m (m> n) semantically similar papers""to ""directly return top-n experts"". The process of returning a large number of top-m papers as intermediate data is avoided, thereby improving the efficiency. Extensive experiments using real-world datasets demonstrate our approach's superiority.  © 2023 Association for Computing Machinery.",(k )-core community; document/expert embedding; Expert finding; heterogeneous graph,Graphic methods; Knowledge management; Paper; Semantics; (k )-core community; Academic paper; Document/expert embedding; Embeddings; Expert finding; Graph-based; Heterogeneous graph; K-cores; Proximity graphs; User query; Embeddings
Bavarian: Betweenness Centrality Approximation with Variance-aware Rademacher Averages,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154581891&doi=10.1145%2f3577021&partnerID=40&md5=a1238231bd44ab4183ba28e02efae388,"""[A]llain Gersten, Hopfen, und Wasser""- 1516 ReinheitsgebotWe present Bavarian, a collection of sampling-based algorithms for approximating the Betweenness Centrality (BC) of all vertices in a graph. Our algorithms use Monte-Carlo Empirical Rademacher Averages (MCERAs), a concept from statistical learning theory, to efficiently compute tight bounds on the maximum deviation of the estimates from the exact values. The MCERAs provide a sample-dependent approximation guarantee much stronger than the state-of-the-art, thanks to its use of variance-aware probabilistic tail bounds. The flexibility of the MCERAs allows us to introduce a unifying framework that can be instantiated with existing sampling-based estimators of BC, thus allowing a fair comparison between them, decoupled from the sample-complexity results with which they were originally introduced. Additionally, we prove novel sample-complexity results showing that, for all estimators, the sample size sufficient to achieve a desired approximation guarantee depends on the vertex-diameter of the graph, an easy-to-bound characteristic quantity. We also show progressive-sampling algorithms and extensions to other centrality measures, such as percolation centrality. Our extensive experimental evaluation of Bavarian shows the improvement over the state-of-the-art made possible by the MCERAs (2-4× reduction in the error bound), and it allows us to assess the different trade-offs between sample size and accuracy guarantees offered by the different estimators. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Concentration bounds; dynamic graphs; percolation centrality; random sampling; sample complexity; statistical learning theory,Economic and social effects; Graph theory; Learning algorithms; Sampling; Bavarians; Betweenness centrality; Complexity results; Concentration bounds; Dynamic graph; Percolation centrality; Random sampling; Sample complexity; State of the art; Statistical learning theory; Solvents
A Weighted Ensemble Classification Algorithm Based on Nearest Neighbors for Multi-Label Data Stream,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154617041&doi=10.1145%2f3570960&partnerID=40&md5=0d15dba584445a1d91a4ac92d1f8a38b,"With the rapid development of data stream, multi-label algorithms for mining dynamic data become more and more important. At the same time, when data distribution changes, concept drift will occur, which will make the existing classification models lose effectiveness. Ensemble methods have been used for multi-label classification, but few methods consider both the accuracy and diversity of base classifiers. To address the above-mentioned problem, a Weighted Ensemble classification algorithm based on Nearest Neighbors for Multi-Label data stream (WENNML) is proposed. WENNML uses data blocks to train Active candidate Ensemble Classifiers (AEC) and Passive candidate Ensemble Classifiers (PEC). The base classifiers of AEC and PEC are dynamically updated using geometric and diversity weighting methods. When the difference value between the number of current instances and the number of warning instances reaches the passive warning value, the algorithm selects the optimal base classifiers from AEC and PEC according to the subset accuracy and hamming score and puts them into the predictive ensemble classifiers. Experiments are carried out on 12 kinds of datasets with 9 comparison algorithms. The results show that WENNML achieves the best average rankings among the four evaluation metrics.  © 2023 Association for Computing Machinery.",concept drift; data stream; dynamic update; ensemble classification; Multi-label,Data streams; Base classifiers; Classification algorithm; Concept drifts; Data stream; Dynamic update; Ensemble classification; Ensemble-classifier; Label algorithms; Multi-labels; Nearest-neighbour; Classification (of information)
Slack-Factor-Based Fuzzy Support Vector Machine for Class Imbalance Problems,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154542621&doi=10.1145%2f3579050&partnerID=40&md5=9777db7bb75c0fd3ff8944bb737dc97c,"Class imbalance and noisy data widely exist in real-world problems, and the support vector machine (SVM) is hard to construct good classifiers on these data. Fuzzy SVMs (FSVMs), as variants of SVM, use a fuzzy membership function both to reflect the samples' importance and to remove the impact of noises, and employ cost-sensitive technology to address the class imbalance. They can handle the noise and class imbalance problems in many cases; however, the fuzzy membership functions are often affected by the class imbalance data, leading to inaccurate measures for samples' performance and affecting the performance of FSVMs. To solve this problem, we design a new fuzzy membership function and combine it with cost-sensitive learning to deal with the class imbalance problem with noisy data, named Slack-Factor-based FSVM (SFFSVM). In SFFSVM, the relative distances between samples and an estimated hyperplane, called slack factors, are used to define the fuzzy membership function. To eliminate the impact of class imbalance on the function and gain more accurate samples' importance, we rectify the importance according to the positional relationship between the estimated hyperplane and the optimal hyperplane of the problem, and the slack factors of samples. Comprehensive experiments on artificial and real-world datasets demonstrate that SFFSVM outperforms other comparative methods on F1, MCC, and AUC-PR metrics.  © 2023 Association for Computing Machinery.",class imbalance; Cost-sensitive learning; decision hyperplane; fuzzy membership function; fuzzy support vector machine,Geometry; Membership functions; Class imbalance; Class imbalance problems; Cost-sensitive learning; Decision hyperplanes; Fuzzy membership function; Fuzzy support vector machines; Fuzzy SVM; Imbalance datum; Noisy data; Slack factors; Support vector machines
Characterizing and Forecasting Urban Vibrancy Evolution: A Multi-View Graph Mining Perspective,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154548528&doi=10.1145%2f3568683&partnerID=40&md5=3d336e2ac0a88feeea1935b2f64afe4c,"Urban vibrancy describes the prosperity, diversity, and accessibility of urban areas, which is vital to a city's socio-economic development and sustainability. While many efforts have been made for statically measuring and evaluating urban vibrancy, there are few studies on the evolutionary process of urban vibrancy, yet we know little about the relationship between urban vibrancy evolution and sophisticated spatiotemporal dynamics. In this article, we make use of multi-sourced urban data to develop a data-driven framework, U-Evolve, to investigate urban vibrancy evolution. Specifically, we first exploit the spatiotemporal characteristics of urban areas to create multi-view time-dependent graphs. Then, we analyze the contextual features and graph patterns of multi-view time-dependent graphs in terms of informing future urban vibrancy variations. Our analysis validates the informativeness of multi-view time-dependent graphs for characterizing and informing future urban vibrancy evolution. After that, we construct a feature based model to forecast future urban vibrancy evolution and quantify each feature's importance. Moreover, to further enhance the forecasting effectiveness, we propose a graph learning based model to capture spatiotemporal autocorrelation of urban areas based on multi-view time-dependent graphs in an end-to-end manner. Finally, extensive experiments on two metropolises, Beijing and Shanghai, demonstrate the effectiveness of our forecasting models. The U-Evolve framework has also been deployed in the production environment to deliver real-world urban development and planning insights for various cities in China.  © 2023 Association for Computing Machinery.",graph neural network; spatiotemporal data mining; Urban vibrancy forecasting,Data mining; Economic and social effects; Economics; Graph neural networks; Graphic methods; Sustainable development; Urban growth; Economic sustainability; Evolutionary process; Graph mining; Graph neural networks; Multi-views; Socio-economic development; Spatio-temporal data mining; Time dependent; Urban areas; Urban vibrancy forecasting; Forecasting
GEO: A Computational Design Framework for Automotive Exterior Facelift,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154568406&doi=10.1145%2f3578521&partnerID=40&md5=ef5bacc99acbb1c8c9baaa426479ad9c,"Exterior facelift has become an effective method for automakers to boost the consumers' interest in an existing car model before it is redesigned. To support the automotive facelift design process, this study develops a novel computational framework - Generator, Evaluator, Optimiser (GEO), which comprises three components: a StyleGAN2-based design generator that creates different facelift designs; a convolutional neural network (CNN)-based evaluator that assesses designs from the aesthetics perspective; and a recurrent neural network (RNN)-based decision optimiser that selects designs to maximise the predicted profit of the targeted car model over time. We validate the GEO framework in experiments with real-world datasets and describe some resulting managerial implications for automotive facelift. Our study makes both methodological and application contributions. First, the generator's mapping network and projection methods are carefully tailored to facelift where only minor changes are performed without affecting the family signature of the automobile brands. Second, two evaluation metrics are proposed to assess the generated designs. Third, profit maximisation is taken into account in the design selection. From a high-level perspective, our study contributes to the recent use of machine learning and data mining in marketing and design studies. To the best of our knowledge, this is the first study that uses deep generative models for automotive regional design upgrading and that provides an end-to-end decision-support solution for automakers and designers.  © 2023 Association for Computing Machinery.",aesthetics evaluation; Automotive design; decision optimisation; design generation; exterior facelift,Data mining; Decision support systems; Design; Model automobiles; Recurrent neural networks; Automotive designs; Automotives; Car models; Computational design; Decision optimization; Design generation; Esthetic evaluation; Exterior facelift; Network-based; Optimisations; Profitability
Misinformation Blocking Problem in Virtual and Real Interconversion Social Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154534642&doi=10.1145%2f3578936&partnerID=40&md5=5b81c1c592a9941b0c427b9362a668a1,"With the in-depth development of intelligent media technology, online and offline fusion, reality and virtual entanglement, information content generalization, the boundary between positive and negative information is blurred, all kinds of misinformation in the social network fission spread, and cyberspace governance has become a global consensus. In this article, we comprehensively consider the spread of misinformation in location-based interpersonal social network and online social network, and systematically tackle the novel problem of minimizing the influence of misinformation under individual protection strategies. We first analyze the complexity and modularity of the problem. Then, we leverage the Lovász extension to devise a nonsubmodular set function continuity approximate convex relaxation method, and develop an approximate projected subgradient procedure to obtain a solution with a factor approximate guarantee. Finally, experiments on three assembled real-world datasets demonstrate the effectiveness and feasibility of our designed method and developed the algorithm.  © 2023 Association for Computing Machinery.",individual protection; location-based social network; Lovász extension; Misinformation,Relaxation processes; Social sciences computing; Blockings; Generalisation; Individual protections; Information contents; Interconversions; Location-based social networks; Lovasz extension; Media technology; Misinformation; Offline; Social networking (online)
Three-way Preference Completion via Preference Graph,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154585274&doi=10.1145%2f3580368&partnerID=40&md5=5e746343b4c9bfd34cf67498133871fa,"With the personal partial rankings from agents over a subset of alternatives, the goal of preference completion is to infer the agent's personalized preference over all alternatives including those the agent has not yet handled from uncertain preference of third parties. By combining the partial rankings of the target agent and the partial rankings from third parties to settle some disagreement with three-way preference completion, which includes a general strategy, an optimal strategy, and a pessimistic strategy, it forms the weighted preference graph. Technically, to settle the disagreement and obtain the completed preference of the target agent in the weighted preference graph, maximum likelihood estimation (MLE) under Mallows is proposed and validated theoretically by removing edges with the minimum weight in the weighted preference graph. However, it is not easy to locate the edges with the minimum weight efficiently in a big graph. Hence, an optimal MLE algorithm and three greedy MLE algorithms are proposed to process the MLE. Furthermore, these proposed algorithms are experimentally validated and compared with each other by both the synthetic dataset and the Flixter dataset. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",maximum likelihood estimation; Preference completion; preference graph; three-way decision,Estimation algorithm; Maximum-likelihood estimation; Minimum weight; Optimal strategies; Partial rankings; Preference completion; Preference graph; Synthetic datasets; Third parties; Three-way decision; Maximum likelihood estimation
ONION: Joint Unsupervised Feature Selection and Robust Subspace Extraction for Graph-based Multi-View Clustering,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154590185&doi=10.1145%2f3568684&partnerID=40&md5=eb1aa3a9a850afcc139fdd86531b075b,"Graph-based Multi-View Clustering (GMVC) has received extensive attention due to its ability to capture the neighborhood relationship among data points from diverse views. However, most existing approaches construct similarity graphs from the original multi-view data, the accuracy of which heavily and implicitly relies on the quality of the original multiple features. Moreover, previous methods either focus on mining the multi-view commonality or emphasize on exploring the multi-view individuality, making the rich information contained in multiple features cannot be effectively exploited. In this work, we design a novel GMVC framework via cOmmoNality and Individuality discOvering in lateNt subspace (ONION), seeking for a robust and discriminative subspace representation compatible across multiple features for GMVC. To be specific, our method simultaneously formulates the unsupervised sparse feature selection and the robust subspace extraction, as well as the target graph learning in a unified optimization model, which can help the learning of the discriminative subspace representation and the target graph in a mutual reinforcement manner. Meanwhile, we manipulate the target graph by an explicit structural penalty, rendering the connected components in the graph directly reveal clusters. Experimental results on seven benchmark datasets demonstrate the effectiveness of our proposed method.  © 2023 Association for Computing Machinery.",commonality and individuality; Multi-view clustering; structured graph learning; unsupervised feature selection,Data mining; Feature Selection; Graphic methods; Commonality and individuality; Graph-based; Multi-view clustering; Multi-views; Multiple features; Neighbourhood; Structured graph learning; Structured graphs; Subspace representation; Unsupervised feature selection; Extraction
Traffic Flow Forecasting in the COVID-19: A Deep Spatial-temporal Model Based on Discrete Wavelet Transformation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148694931&doi=10.1145%2f3564753&partnerID=40&md5=d16d04069762ea9945a262838e1acd30,"Traffic flow prediction has always been the focus of research in the field of Intelligent Transportation Systems, which is conducive to the more reasonable allocation of basic transportation resources and formulation of transportation policies. The spread of COVID-19 has seriously affected the normal order in the transportation sector. With the increase in the number of infected people and the government's anti-epidemic policy, human outgoing activities have gradually decreased, resulting in increasingly obvious discreteness and irregularities in traffic flow data. This article proposes a deep-space time traffic flow prediction model based on discrete wavelet transform (DSTM-DWT) to overcome the highly discrete and irregular nature of the new crown epidemic. First, DSTM-DWT decomposes traffic flow into discrete attributes, such as flow trend, discrete amplitude, and discrete baseline. Second, we design the spatial relationship of the transportation network as a graph and integrate the new crown pneumonia epidemic data into the characteristics of each transportation node. Then, we use the graph convolutional network to calculate the spatial correlation of each node, and the temporal convolutional network to calculate the temporal correlation of the data. In order to solve the problem of high discreteness of traffic flow data during the epidemic, this article proposes a graph memory network (GMN), which is used to convert discrete magnitudes separated by discrete wavelet transform into high-dimensional discrete features. Finally, use DWT to segment the predicted traffic data, and then perform the inverse discrete wavelet transform between the newly segmented traffic trend and discrete baseline and the discrete model predicted by GMN to obtain the final traffic flow prediction result. In simulation experiments, this work was compared with the existing advanced baselines to verify the superiority of DSTM-DWT.  © 2023 Association for Computing Machinery.",COVID-19; discrete wavelet transformation; discretized data calculation; temporal convolutional network; Traffic flow forecasting,Convolution; Discrete wavelet transforms; Flow graphs; Forecasting; Intelligent systems; Metadata; Signal reconstruction; Convolutional networks; Discrete wavelets transformations; Discrete-wavelet-transform; Discretized data calculation; Flow data; Model-based OPC; Temporal convolutional network; Traffic flow; Traffic flow forecasting; Traffic flow prediction; COVID-19
SPAP: Simultaneous Demand Prediction and Planning for Electric Vehicle Chargers in a New City,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153894559&doi=10.1145%2f3565577&partnerID=40&md5=5dd622830b08704f25245f61b2d08a5d,"For a new city that is committed to promoting Electric Vehicles (EVs), it is significant to plan the public charging infrastructure where charging demands are high. However, it is difficult to predict charging demands before the actual deployment of EV chargers for lack of operational data, resulting in a deadlock. A direct idea is to leverage the urban transfer learning paradigm to learn the knowledge from a source city, then exploit it to predict charging demands, and meanwhile determine locations and amounts of slow/fast chargers for charging stations in the target city. However, the demand prediction and charger planning depend on each other, and it is required to re-train the prediction model to eliminate the negative transfer between cities for each varied charger plan, leading to the unacceptable time complexity. To this end, we design an effective solution of Simultaneous Demand Prediction And Planning (SPAP): discriminative features are extracted from multi-source data, and fed into an Attention-based Spatial-Temporal City Domain Adaptation Network (AST-CDAN) for cross-city demand prediction; a novel Transfer Iterative Optimization (TIO) algorithm is designed for charger planning by iteratively utilizing AST-CDAN and a charger plan fine-tuning algorithm. Extensive experiments on real-world datasets collected from three cities in China validate the effectiveness and efficiency of SPAP. Specially, SPAP improves at most 72.5% revenue compared with the real-world charger deployment.  © 2023 Association for Computing Machinery.",demand prediction; electric vehicles; infrastructure planning; Urban transfer learning,Charging (batteries); Forecasting; Iterative methods; Knowledge management; Charging demands; Charging infrastructures; Demand planning; Demand prediction; Domain adaptation; Electric vehicles chargers; Infrastructure planning; Spatial temporals; Transfer learning; Urban transfer learning; Electric vehicles
Semi-Supervised Sentiment Classification and Emotion Distribution Learning Across Domains,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154544640&doi=10.1145%2f3571736&partnerID=40&md5=437509316e0cc29e4636a2d01bed62b9,"In this study, sentiment classification and emotion distribution learning across domains are both formulated as a semi-supervised domain adaptation problem, which utilizes a small amount of labeled documents in the target domain for model training. By introducing a shared matrix that captures the stable association between document clusters and word clusters, non-negative matrix tri-factorization (NMTF) is robust to the labeled target domain data and has shown remarkable performance in cross-domain text classification. However, the existing NMTF-based models ignore the incompatible relationship of sentiment polarities and the relatedness among emotions. Besides, their applications on large-scale datasets are limited by the high computation complexity. To address these issues, we propose a semi-supervised NMTF framework for sentiment classification and emotion distribution learning across domains. Based on a many-to-many mapping between document clusters and sentiment polarities (or emotions), we first incorporate the prior information of label dependency to improve the model performance. Then, we develop a parallel algorithm based on message passing interface (MPI) to further enhance the model scalability. Extensive experiments on real-world datasets validate the effectiveness of our method.  © 2023 Association for Computing Machinery.",emotion distribution learning; label dependency; non-negative matrix tri-factorization; Semi-supervised learning; sentiment classification,Classification (of information); Information retrieval systems; Large dataset; Learning algorithms; Learning systems; Message passing; Non-negative matrix factorization; Supervised learning; Text processing; Domain adaptation; Emotion distribution learning; Label dependencies; matrix; Non-negative matrix; Non-negative matrix tri-factorization; Semi-supervised; Semi-supervised learning; Sentiment classification; Target domain; Matrix algebra
Crowdsourcing Truth Inference Based on Label Confidence Clustering,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153886257&doi=10.1145%2f3556545&partnerID=40&md5=e7e0997f7cebf31d873e6bc003e2fcef,"Truth inference can help solve some difficult problems of data integration in crowdsourcing. Crowdsourced workers are not experts and their labeling ability varies greatly; therefore, in practical applications, it is difficult to determine whether the labels collected from a crowdsourcing platform are correct. This article proposes a novel algorithm called truth inference based on label confidence clustering (TILCC) to improve the quality of integrated labels for the single-choice classification problem in crowdsourcing labeling tasks. We obtain the label confidence via worker reliability, which is calculated from multiple noise labels using a truth discovery method, and then we generate the clustering features and use the K-means algorithm to cluster all the tasks into K different clusters. Each cluster corresponds to a specific class, and the tasks in the cluster are assigned a label. Compared with the performances of six state-of-the-art methods, MV, ZenCrowd, PM, CATD, GLAD, and GTIC, on 12 randomly selected real-world datasets, the performance of our algorithm showed many advantages: no need to set complex parameters, faster running speed, and significantly higher accuracy. © 2023 Association for Computing Machinery.",clustering; crowdsourcing truth inference; label confidence; single-choice classification; Truth discovery,Data integration; Data mining; Inference engines; K-means clustering; Clusterings; Crowdsourcing platforms; Crowdsourcing truth inference; Label confidence; Labelings; Novel algorithm; Performance; Single-choice classification; Truth discovery; Workers'; Crowdsourcing
Nonnegative Matrix Factorization Based on Node Centrality for Community Detection,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154583287&doi=10.1145%2f3578520&partnerID=40&md5=c0b347947c75f69a015815e3067b7251,"Community detection is an important topic in network analysis, and recently many community detection methods have been developed on top of the Nonnegative Matrix Factorization (NMF) technique. Most NMF-based community detection methods only utilize the first-order proximity information in the adjacency matrix, which has some limitations. Besides, many NMF-based community detection methods involve sparse regularizations to promote clearer community memberships. However, in most of these regularizations, different nodes are treated equally, which seems unreasonable. To dismiss the above limitations, this article proposes a community detection method based on node centrality under the framework of NMF. Specifically, we design a new similarity measure which considers the proximity of higher-order neighbors to form a more informative graph regularization mechanism, so as to better refine the detected communities. Besides, we introduce the node centrality and Gini impurity to measure the importance of nodes and sparseness of the community memberships, respectively. Then, we propose a novel sparse regularization mechanism which forces nodes with higher node centrality to have smaller Gini impurity. Extensive experimental results on a variety of real-world networks show the superior performance of the proposed method over thirteen state-of-the-art methods.  © 2023 Association for Computing Machinery.",Community detection; nonnegative matrix factorization; optimization; similarity measure; sparse regularization,Data mining; Matrix factorization; Population dynamics; Community detection; Detection methods; Factorization techniques; Gini impurities; In-network analysis; Nonnegative matrix factorization; Optimisations; Regularization mechanism; Similarity measure; Sparse regularizations; Matrix algebra
Trip Reinforcement Recommendation with Graph-based Representation Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153497088&doi=10.1145%2f3564609&partnerID=40&md5=b383ecfe1beaee237000872b2259c868,"Tourism is an important industry and a popular leisure activity involving billions of tourists per annum. One challenging problem tourists face is identifying attractive Places-of-Interest (POIs) and planning the personalized trip with time constraints. Most of the existing trip recommendation methods mainly consider POI popularity and user preferences, and focus on the last visited POI when choosing the next POI. However, the visit patterns and their asymmetry property have not been fully exploited. To this end, in this article, we present a GRM-RTrip (short for Graph-based Representation Method for Reinforce Trip Recommendation) framework. GRM-RTrip learns POI representations from incoming and outgoing views to obtain asymmetric POI-POI transition probability via POI-POI graph networks, and then fuses the trained POI representation into a user-POI graph network to estimate user preferences. Finally, after formulating the personalized trip recommendation as a Markov Decision Process (MDP), we utilize a reinforcement learning algorithm for generating a personalized trip with maximal user travel experience. Extensive experiments are performed on the public datasets and the results demonstrate the superiority of GRM-RTrip compared with the state-of-the-art trip recommendation methods.  © 2023 Association for Computing Machinery.",attention mechanism; graph neural network; Recommender system; reinforcement learning,Graph neural networks; Graphic methods; Learning algorithms; Learning systems; Markov processes; Recommender systems; Attention mechanisms; Graph networks; Graph neural networks; Graph-based representations; Leisure activities; Recommendation methods; Reinforcement learnings; Representation method; Time constraints; User's preferences; Reinforcement learning
Multi-Stage Machine Learning Model for Hierarchical Tie Valence Prediction,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154621950&doi=10.1145%2f3579096&partnerID=40&md5=6579e5442d4a4b5f70db023e7660ca0b,"Individuals interacting in organizational settings involving varying levels of formal hierarchy naturally form a complex network of social ties having different tie valences (e.g., positive and negative connections). Social ties critically affect employees' satisfaction, behaviors, cognition, and outcomes - yet identifying them solely through survey data is challenging because of the large size of some organizations or the often hidden nature of these ties and their valences. We present a novel deep learning model encompassing NLP and graph neural network techniques that identifies positive and negative ties in a hierarchical network. The proposed model uses human resource attributes as node information and web-logged work conversation data as link information. Our findings suggest that the presence of conversation data improves the tie valence classification by 8.91% compared to employing user attributes alone. This gain came from accurately distinguishing positive ties, particularly for male, non-minority, and older employee groups. We also show a substantial difference in conversation patterns for positive and negative ties with positive ties being associated with more messages exchanged on weekends, and lower use of words related to anger and sadness. These findings have broad implications for facilitating collaboration and managing conflict within organizational and other social networks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph neural networks; organizational social network; sentiment embeddings; Signed link prediction; tie-valence prediction,Complex networks; Deep learning; Graph neural networks; Learning systems; Personnel; Embeddings; Graph neural networks; Link prediction; Multi-stages; Organisational; Organizational social network; Sentiment embedding; Signed link prediction; Social ties; Tie-valence prediction; Forecasting
STAD-GAN: Unsupervised Anomaly Detection on Multivariate Time Series with Self-training Generative Adversarial Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156183985&doi=10.1145%2f3572780&partnerID=40&md5=441a657ceb52699d33d7dbc54ad862c4,"Anomaly detection on multivariate time series (MTS) is an important research topic in data mining, which has a wide range of applications in information technology, financial management, manufacturing system, and so on. However, the state-of-the-art unsupervised deep learning models for MTS anomaly detection are vulnerable to noise and have poor performance on the training data containing anomalies. In this article, we propose a novel Self-Training based Anomaly Detection with Generative Adversarial Network (GAN) model called STAD-GAN to address the practical challenge. The STAD-GAN model consists of a generator-discriminator structure for adversarial learning and a neural network classifier for anomaly classification. The generator is learned to capture the normal data distribution, and the discriminator is learned to amplify the reconstruction error of abnormal data for better recognition. The proposed model is optimized with a self-training teacher-student framework, where a teacher model generates reliable high-quality pseudo-labels to train a student model iteratively with a refined dataset so that the performance of the anomaly classifier can be gradually improved. Extensive experiments based on six open MTS datasets show that STAD-GAN is robust to noise and achieves significant performance improvement compared to the state-of-the-art.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",anomaly detection; generative adversarial network; Multivariate time series; self-training; unsupervised learning,Classification (of information); Data mining; Deep learning; Generative adversarial networks; Industrial research; Information management; Learning systems; Personnel training; Time series; Anomaly detection; Financial managements; Learning models; Multivariate time series; Network models; Performance; Research topics; Self-training; State of the art; Unsupervised anomaly detection; Anomaly detection
Random Walk Sampling in Social Networks Involving Private Nodes,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153879933&doi=10.1145%2f3561388&partnerID=40&md5=9683ac7a244e98b46c030aa0d1d83f50,"Analysis of social networks with limited data access is challenging for third parties. To address this challenge, a number of studies have developed algorithms that estimate properties of social networks via a simple random walk. However, most existing algorithms do not assume private nodes that do not publish their neighbors' data when they are queried in empirical social networks. Here we propose a practical framework for estimating properties via random walk-based sampling in social networks involving private nodes. First, we develop a sampling algorithm by extending a simple random walk to the case of social networks involving private nodes. Then, we propose estimators with reduced biases induced by private nodes for the network size, average degree, and density of the node label. Our results show that the proposed estimators reduce biases induced by private nodes in the existing estimators by up to 92.6% on social network datasets involving private nodes.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",estimation; private nodes; random walk; sampling; Social networks,Database systems; Data access; Limited data; Network size; Private node; Property; Random Walk; Sampling algorithm; Simple random walk; Social network; Third parties; Random processes
Static and Streaming Tucker Decomposition for Dense Tensors,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154616543&doi=10.1145%2f3568682&partnerID=40&md5=dce71fb4cfe37e4c84207a3b2fe90fe7,"Given a dense tensor, how can we efficiently discover hidden relations and patterns in static and online streaming settings? Tucker decomposition is a fundamental tool to analyze multidimensional arrays in the form of tensors. However, existing Tucker decomposition methods in both static and online streaming settings have limitations of efficiency since they directly deal with large dense tensors for the result of Tucker decomposition. In a static setting, although few static methods have tried to reduce their time cost by sampling tensors, sketching tensors, and efficient matrix operations, there remains a need for an efficient method. Moreover, streaming versions of Tucker decomposition are still time-consuming to deal with newly arrived tensors.We propose D-Tucker and D-TuckerO, efficient Tucker decomposition methods for large dense tensors in static and online streaming settings, respectively. By decomposing a given large dense tensor with randomized singular value decomposition, avoiding the reconstruction from SVD results, and carefully determining the order of operations, D-Tucker and D-TuckerO efficiently obtain factor matrices and core tensor. Experimental results show that D-Tucker achieves up to 38.4 × faster running times, and requires up to 17.2 × less space than existing methods while having similar accuracy. Furthermore, D-TuckerO is up to 6.1× faster than existing streaming methods for each newly arrived tensor while its running time is proportional to the size of the newly arrived tensor, not the accumulated tensor.  © 2023 Association for Computing Machinery.",Dense tensor; efficiency; online streaming setting; static setting; Tucker decomposition,Singular value decomposition; Tensors; Decomposition methods; Dense tensor; Fundamental tools; Multidimensional arrays; Online streaming setting; Running time; Static method; Static setting; Time cost; Tucker decompositions; Efficiency
CrowdAtlas: Estimating Crowd Distribution within the Urban Rail Transit System,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153890686&doi=10.1145%2f3558521&partnerID=40&md5=b43fc9c8a4b7ed85745c447e551aab29,"While urban rail transit systems are playing an increasingly important role in meeting the transportation demands of people, precise awareness of how the human crowd is distributed within such a system is highly necessary, which serves a range of important applications including emergency response, transit recommendation, and commercial valuation. Most rail transit systems are closed systems where once entered the passengers are free to move around all stations and are difficult to track. In this article, we attempt to estimate the crowd distribution based only on the tap-in and tap-out records of all the rail riders. Specifically, we study Singapore MRT (Mass Rapid Transit) as a vehicle and leverage EZ-Link transit card records to estimate the crowd distribution. Guided by a key observation that the passenger inflows and arrival flows at different MRT stations and time are spatio-temporally correlated due to behavioral consistency of MRT riders, we design and implement a machine learning-based solution, CrowdAtlas, that captures MRT riders' transition probabilities among stations and across time, and based on that accurately estimates the crowd distribution within the MRT system. Our comprehensive performance evaluations with both trace-driven studies and real-world experiments in MRT disruption cases demonstrate the effectiveness of CrowdAtlas.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",crowd distribution; neural network; Rail transit system; transportation data analysis,Behavioral research; Light rail transit; Mass transportation; Probability distributions; Rapid transit; Urban transportation; Crowd distribution; Emergency response; Human crowd; Mass rapid transit; Neural-networks; Rail transit systems; Transit riders; Transportation data analyse; Transportation demand; Urban rail transit systems; Taps
Differentially Private Release of Heterogeneous Network for Managing Healthcare Data,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154602148&doi=10.1145%2f3580367&partnerID=40&md5=b1962878208ab2efdd1fdffcd2cc5221,"With the increasing adoption of digital health platforms through mobile apps and online services, people have greater flexibility connecting with medical practitioners, pharmacists, and laboratories and accessing resources to manage their own health-related concerns. Many healthcare institutions are connecting with each other to facilitate the exchange of healthcare data, with the goal of effective healthcare data management. The contents generated over these platforms are often shared with third parties for a variety of purposes. However, sharing healthcare data comes with the potential risk of exposing patients' sensitive information to privacy threats. In this article, we address the challenge of sharing healthcare data while protecting patients' privacy. We first model a complex healthcare dataset using a heterogeneous information network that consists of multi-type entities and their relationships. We then propose DiffHetNet, an edge-based differentially private algorithm, to protect the sensitive links of patients from inbound and outbound attacks in the heterogeneous health network. We evaluate the performance of our proposed method in terms of information utility and efficiency on different types of real-life datasets that can be modeled as networks. Experimental results suggest that DiffHetNet generally yields less information loss and is significantly more efficient in terms of runtime in comparison with existing network anonymization methods. Furthermore, DiffHetNet is scalable to large network datasets. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",differential privacy; healthcare data management; Heterogeneous information network; information utility,Health care; Heterogeneous networks; Information dissemination; Information services; Large dataset; Sensitive data; Differential privacies; Healthcare data management; Healthcare institutions; Heterogeneous information; Heterogeneous information network; Information networks; Information utility; Medical practitioner; Mobile app; On-line service; Information management
Hierarchical Dense Pattern Detection in Tensors,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154597662&doi=10.1145%2f3577022&partnerID=40&md5=ee4e3b471aab476232aca52ec4b299d8,"Dense subtensor detection gains remarkable success in spotting anomalies and fraudulent behaviors for multi-aspect data (i.e., tensors), like in social media and event streams. Existing methods detect the densest subtensors flatly and separately, with the underlying assumption that those subtensors are exclusive. However, many real-world tensors usually present hierarchical properties, e.g., the core-periphery structure and dynamic communities in networks. It is also unexplored how to fuse the prior knowledge into dense pattern detection to capture the local behavior.In this article, we propose CatchCore, a novel framework to efficiently find the hierarchical dense subtensors. We first design a unified metric for dense subtensor detection, which can be optimized with gradient-based methods. With the proposed metric, CatchCore detects hierarchical dense subtensors through the hierarchy-wise alternative optimization and finds local dense patterns concerning some items in a query manner. Finally, we utilize the minimum description length principle to measure the quality of detection results and select the optimal hierarchical dense subtensors. Extensive experiments on synthetic and real-world datasets demonstrate that CatchCore outperforms the top competitors in accuracy for detecting dense subtensors and anomaly patterns, like network attacks. Additionally, CatchCore successfully identifies a hierarchical researcher co-authorship group with intense interactions in the DBLP dataset; it can also capture core collaboration and multi-hop relations around some query objects. Meanwhile, CatchCore also scales linearly with all aspects of tensors.  © 2023 Association for Computing Machinery.",anomaly detection; dense subtensor; hierarchical structure; pattern query; Tensor,Pattern recognition; Tensors; Anomaly detection; Dense patterns; Dense subtensor; Detection gain; Hierarchical structures; Multi aspects; Pattern detection; Pattern query; Social events; Social media; Anomaly detection
STHAN: Transportation Demand Forecasting with Compound Spatio-Temporal Relationships,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153896020&doi=10.1145%2f3565578&partnerID=40&md5=69810efd36e9b5a0ae84a052f9435fd8,"Transportation demand forecasting is a critical precondition of optimal online transportation dispatch, which will greatly reduce drivers' wasted mileage and customers' waiting time, contributing to economic and environmental sustainability. Though various methods have been developed, the core spatio-temporal complexity remains challenging from three perspectives: (1) Compound spatial relationships. According to our empirical analysis, these relationships widely exist. Previous studies focus on capturing different spatial relationships using multi-homogeneous graphs. However, the information flow across various spatial relationships is not modeled explicitly. (2) Heterogeneity in spatial relationships. A region's neighbors under the same spatial relationship may have different weights for this region. Meanwhile, different relationships may also weigh differently. (3) Synchronicity between compound spatial relationships and temporal relationships. Previous research considers synchronous influences from spatial and temporal relationships in a homogeneous fashion while compound spatial relationships are not captured for this synchronicity.To address the aforementioned perspectives, we propose the Spatio-Temporal Heterogeneous graph Attention Network (STHAN), where the key intuition is capturing the compound spatial relationships via meta-paths explicitly. We first construct a spatio-temporal heterogeneous graph including multiple spatial relationships and temporal relationships and use meta-paths to depict compound spatial relationships. To capture the heterogeneity, we use hierarchical attention, which contains node level attention and meta-path level attention. The synchronicity between temporal relationships and spatial relationships, including compound ones, is modeled in meta-path-level attention. Our framework outperforms state-of-the-art models by reducing 6.58%, 4.57%, and 4.20% of WMAPE in experiments on three real-world datasets, respectively.  © 2023 Copyright held by the owner/author(s).",heterogeneous graph convolution; spatio-temporal prediction; Transportation demand prediction,Convolution; Data mining; Graph theory; Sustainable development; Demand forecasting; Heterogeneous graph; Heterogeneous graph convolution; Spatial relationships; Spatial temporals; Spatio-temporal; Spatio-temporal prediction; Temporal relationships; Transportation demand; Transportation demand prediction; Forecasting
Self-paced Adaptive Bipartite Graph Learning for Consensus Clustering,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144795017&doi=10.1145%2f3564701&partnerID=40&md5=1c24bd12a909bcc4e6f479280048a9ed,"Consensus clustering provides an elegant framework to aggregate multiple weak clustering results to learn a consensus one that is more robust and stable than a single result. However, most of the existing methods usually use all data for consensus learning, whereas ignoring the side effects caused by some unreliable or difficult data. To address this issue, in this article, we propose a novel self-paced consensus clustering method with adaptive bipartite graph learning to gradually involve data from more reliable to less reliable ones in consensus learning. At first, we construct an initial bipartite graph from the base results, where the nodes represent the clusters and instances, and the edges indicate that an instance belongs to a cluster. Then, we adaptively learn a structured bipartite graph from this initial one by self-paced learning, i.e., we automatically determine the reliability of each edge with adaptive cluster similarity measuring and involve the edges in bipartite graph learning in order of their reliability. At last, we obtain the final consensus result from the learned structured bipartite graph. We conduct extensive experiments on both toy and benchmark datasets, and the results show the effectiveness and superiority of our method. The codes of this article are released in http://Doctor-Nobody.github.io/codes/code_SCCABG.zip.  © 2023 Association for Computing Machinery.",bipartite graph learning; clustering ensemble; Consensus clustering; self-paced learning,Cluster analysis; Clustering algorithms; Data mining; Graph theory; Learning systems; Adaptive clusters; Bipartite graph learning; Bipartite graphs; Clustering Ensemble; Clustering methods; Clustering results; Consensus clustering; Learn+; Self-paced learning; Side effect; Toys
Uncovering the Local Hidden Community Structure in Social Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153848464&doi=10.1145%2f3567597&partnerID=40&md5=a56be82ff2aea9724611092627e7e6fd,"Hidden community is a useful concept proposed recently for social network analysis. Hidden communities indicate some weak communities whose most members also belong to other stronger dominant communities. Dominant communities could form a layer that partitions all the individuals of a network, and hidden communities could form other layer(s) underneath. These layers could be natural structures in the real-world networks like students grouped by major, minor, hometown, and so on. To handle the rapid growth of network scale, in this work, we explore the detection of hidden communities from the local perspective, and propose a new method that detects and boosts each layer iteratively on a subgraph sampled from the original network. We first expand the seed set from a single seed node based on our modified local spectral method and detect an initial dominant local community. Then we temporarily remove the members of this community as well as their connections to other nodes, and detect all the neighborhood communities in the remaining subgraph, including some ""broken communities""that only contain a fraction of members in the original network. The local community and neighborhood communities form a dominant layer, and by reducing the edge weights inside these communities, we weaken this layer's structure to reveal the hidden layers. Eventually, we repeat the whole process, and all communities containing the seed node can be detected and boosted iteratively. We theoretically show that our method can avoid some situations that a broken community and the local community are regarded as one community in the subgraph, leading to the inaccuracy of detection which can be caused by global hidden community detection methods. Extensive experiments show that our method could significantly outperform the state-of-the-art baselines designed for either global hidden community detection or multiple local community detection.  © 2023 Association for Computing Machinery.",hidden community; local community detection; local modularity; Social networks,Data mining; Population dynamics; Community detection; Community IS; Community structures; Hidden community; Hidden community detections; Local community; Local community detection; Local modularity; Social network; Subgraphs; Iterative methods
Learning Shared Representations for Recommendation with Dynamic Heterogeneous Graph Convolutional Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153106863&doi=10.1145%2f3565575&partnerID=40&md5=349ecbf0bddb98d84369b26fae5c2b79,"Graph Convolutional Networks (GCNs) have been widely used for collaborative filtering, due to their effectiveness in exploiting high-order collaborative signals. However, two issues have not been well addressed by existing studies. First, usually only one kind of information is utilized, i.e., user preference in user-item graphs or item dependency in item-item graphs. Second, they usually adopt static graphs, which cannot retain the temporal evolution of the information. These can limit the recommendation quality. To address these limitations, we propose to mine three kinds of information (user preference, item dependency, and user behavior similarity) and their temporal evolution by constructing multiple discrete dynamic heterogeneous graphs (i.e., a user-item dynamic graph, an item-item dynamic graph, and a user-subseq dynamic graph) from interaction data. A novel network (PDGCN) is proposed to learn the representations of users and items in these dynamic graphs. Moreover, we designed a structural neighbor aggregation module with novel pooling and convolution operations to aggregate the features of structural neighbors. We also design a temporal neighbor aggregation module based on self-attention mechanism to aggregate the features of temporal neighbors. We conduct extensive experiments on four real-world datasets. The results indicate that our approach outperforms several competing methods in terms of Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). Dynamic graphs are also shown to be effective in improving recommendation performance.  © 2023 Association for Computing Machinery.",Collaborative filtering; dynamic graph embedding; dynamic graphs; Graph Convolutional Networks,Behavioral research; Collaborative filtering; Convolution; Data mining; Graphic methods; Convolutional networks; Dynamic graph; Dynamic graph embedding; Graph convolutional network; Graph embeddings; Heterogeneous graph; Shared representations; Temporal evolution; User's preferences; Aggregates
Triadic Closure Sensitive Influence Maximization,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154553216&doi=10.1145%2f3573011&partnerID=40&md5=4066c28b4449b911092ce21f900ba7ab,"The influence are not linked to any footnote in the text. Please check and suggest. maximization problem aims at selecting the k most influential nodes (i.e., seed nodes) from a social network, where the nodes can maximize the number of influenced nodes activated by a certain propagation model. However, the widely used Independent Cascade model shares the same propagation probability among substantial adjacent node pairs, which is too idealistic and unreasonable in practice. In addition, most heuristic algorithms for influence maximization need to update the expected influence of the remaining nodes in the seed selection process, resulting in high computation cost. To address these non-trivial problems, we propose a novel edge propagation probability calculation method. The method first utilizes the triadic closure structure of social networks to precisely measure the closeness between nodes and assigns different propagation probabilities to each edge, deriving a Triadic Closure-based Independent Cascade (TC-IC) model. Then, we further propose a heuristic influence maximization algorithm named Triadic Closure-based Influence Maximization (TC-IM). The algorithm evaluates the expected influence of a node by integrating the triadic closure weighted propagation probability and the triadic closure weighted degree. Especially, in the seed selection process, only the most influential node that has not been updated in the current round needs to be updated, which significantly improves the efficiency. Besides, we further provide theoretical proofs to guarantee the correctness of this updating strategy. Experimental results on nine real datasets and three propagation models demonstrate that: (1) The TC-IC model can set a proper propagation probability for each node pair, where the IM algorithms could easily identify influential nodes; (2) The TC-IM algorithm can significantly reduce the complexity through an efficient updating strategy with a comparable influence spread to the approximation IM algorithms; (3) Besides, the TC-IM algorithm also exhibits stable performance under other IC models including UIC and WIC, exhibiting good stability and generality.  © 2023 Association for Computing Machinery.",heuristic algorithm; Independent Cascade model; influence maximization; Social network; triadic closure,Approximation algorithms; Integrated circuits; Stream flow; Cascade modeling; Heuristics algorithm; Independent cascade model; Influence maximizations; Influential nodes; Maximization algorithm; Propagation models; Propagation probabilities; Social network; Triadic closures; Heuristic algorithms
Dynamic Multi-View Graph Neural Networks for Citywide Traffic Inference,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153845464&doi=10.1145%2f3564754&partnerID=40&md5=9b62c29863968411e8d3571266a81617,"Accurate citywide traffic inference is critical for improving intelligent transportation systems with smart city applications. However, this task is very challenging given the limited training data, due to the high cost of sensor installment and maintenance across the entire urban space. A more practical scenario to study the citywide traffic inference is effectively modeling the spatial and temporal traffic patterns with limited historical traffic observations. In this work, we propose a dynamic multi-view graph neural network for citywide traffic inference with the method CTVI+. Specifically, for the temporal dimension, we propose a temporal self-attention mechanism that is capable of learning the dynamics of traffic data with the time-evolving traffic volume variations. For spatial dimension, we build a multi-view graph neural network, employing the road-wise message passing scheme to capture the region dependencies. With the designed spatial-temporal learning paradigms, we enable our traffic inference model to encode the dynamism from both spatial and temporal traffic patterns, which is reflective of intra- and inter-road traffic correlations. In our evaluation, CTVI+ achieves consistent better performance compared with different baselines on real-world traffic volume datasets. Further ablation study validates the effectiveness of key components in CTVI+. We release the model implementation at https://github.com/dsj96/TKDD. © 2023 Association for Computing Machinery.",intelligent transportation system; spatio-temporal dependence modeling; Traffic volume inference,Graph neural networks; Intelligent systems; Learning systems; Roads and streets; Graph neural networks; Intelligent transportation systems; Limited training data; Multi-views; Spatio-temporal; Spatio-temporal dependence modeling; Temporal dependence models; Traffic pattern; Traffic volume inference; Traffic volumes; Message passing
Crowdsourcing Truth Inference via Reliability-Driven Multi-View Graph Embedding,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153882482&doi=10.1145%2f3565576&partnerID=40&md5=3007aa7c2c2858802f056bea3e9d0fee,"Crowdsourcing truth inference aims to assign a correct answer to each task from candidate answers that are provided by crowdsourced workers. A common approach is to generate workers' reliabilities to represent the quality of answers. Although crowdsourced triples can be converted into various crowdsourced relationships, the available related methods are not effective in capturing these relationships to alleviate the harm to inference that is caused by conflicting answers. In this research, we propose a Reliability-driven Multi-view Graph Embedding framework for Truth inference (TiReMGE), which explores multiple crowdsourced relationships by organically integrating worker reliabilities into a graph space that is constructed from crowdsourced triples. Specifically, to create an interactive environment, we propose a reliability-driven initialization criterion for initializing vectors of tasks and workers as interactive carriers of reliabilities. From the perspective of multiple crowdsourced relationships, a multi-view graph embedding framework is proposed for reliability information interaction on a task-worker graph, which encodes latent crowdsourced relationships into vectors of workers and tasks for reliability update and truth inference. A heritable reliability updating method based on the Lagrange multiplier method is proposed to obtain reliabilities that match the quality of workers for interaction by a novel constraint law. Our ultimate goal is to minimize the Euclidean distance between the encoded task vector and the answer that is provided by a worker with high reliability. Extensive experimental results on nine real-world datasets demonstrate that TiReMGE significantly outperforms the nine state-of-the-art baselines.  © 2023 Association for Computing Machinery.",Crowdsourcing truth inference; graph embedding; reliability interaction; worker quality match,Crowdsourcing; Data mining; Embeddings; Redundancy; Crowdsourcing truth inference; Graph embeddings; Information interaction; Interactive Environments; Multi-views; Reliability information; Reliability interaction; Reliability-driven; Worker quality match; Workers'; Lagrange multipliers
Lifelong Online Learning from Accumulated Knowledge,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160357771&doi=10.1145%2f3563947&partnerID=40&md5=ec37d674ace0d3d534ef9bf9df80d1aa,"In this article, we formulate lifelong learning as an online transfer learning procedure over consecutive tasks, where learning a given task depends on the accumulated knowledge. We propose a novel theoretical principled framework, lifelong online learning, where the learning process for each task is in an incremental manner. Specifically, our framework is composed of two-level predictions: the prediction information that is solely from the current task; and the prediction from the knowledge base by previous tasks. Moreover, this article tackled several fundamental challenges: arbitrary or even non-stationary task generation process, an unknown number of instances in each task, and constructing an efficient accumulated knowledge base. Notably, we provide a provable bound of the proposed algorithm, which offers insights on the how the accumulated knowledge improves the predictions. Finally, empirical evaluations on both synthetic and real datasets validate the effectiveness of the proposed algorithm.  © 2023 Association for Computing Machinery.",lifelong learning theory; multi-task learning; Online learning; transfer learning,E-learning; Knowledge based systems; Knowledge management; 'current; Learning procedures; Learning process; Learning Theory; Life long learning; Lifelong learning theory; Multitask learning; Online learning; Prediction informations; Transfer learning; Forecasting
"DiVA: A Scalable, Interactive and Customizable Visual Analytics Platform for Information Diffusion on Large Networks",2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153855142&doi=10.1145%2f3558771&partnerID=40&md5=c75f9ad7f3d578f5a54b04424c58c432,"With an increasing outreach of digital platforms in our lives, researchers have taken a keen interest in studying different facets of social interactions. Analyzing the spread of information (aka diffusion) has brought forth multiple research areas such as modelling user engagement, determining emerging topics, forecasting the virality of online posts and predicting information cascades. Despite such ever-increasing interest, there remains a vacuum among easy-to-use interfaces for large-scale visualization of diffusion models. In this article, we introduce DiVA - Diffusion Visualization and Analysis, a tool that provides a scalable web interface and extendable APIs to analyze various diffusion trends on networks. DiVA uniquely offers support for simultaneous comparison of two competing diffusion models and even the comparison with the ground-truth results, which help develop a coherent understanding of real-world scenarios. Along with performing an exhaustive feature comparison and system evaluation of DiVA against publicly-available web interfaces for information diffusion, we conducted a user study to understand the strengths and limitations of DiVA. We noticed that evaluators had a seamless user experience, especially when analyzing diffusion on large networks.  © 2023 Association for Computing Machinery.",diffusion analytics; diffusion visualization; Information diffusion,Application programming interfaces (API); Diffusion; User interfaces; Customizable; Diffusion analytic; Diffusion model; Diffusion visualization; Digital platforms; Information diffusion; Larger networks; Social interactions; Visual analytics; Web interface; Visualization
Diffuse and Smooth: Beyond Truncated Receptive Field for Scalable and Adaptive Graph Representation Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154544388&doi=10.1145%2f3572781&partnerID=40&md5=f1a3a99237da53012158cd9dae78e3c4,"As the scope of receptive field and the depth of Graph Neural Networks (GNNs) are two completely orthogonal aspects for graph learning, existing GNNs often have shallow layers with truncated-receptive field and far from achieving satisfactory performance. In this article, we follow the idea of decoupling graph convolution into propagation and transformation processes, which generates representations over a sequence of increasingly larger neighborhoods. Though this manner can enlarge the receptive field, it has two critical problems unsolved: how to find the suitable receptive field to avoid under-smoothing or over-smoothing? and how to balance different diffusion operators for better capturing the local and global dependencies? We tackle these challenges and propose a Scalable, Adaptive Graph Convolutional Networks (SAGCN) with Transformer architecture. Concretely, we propose a novel non-heuristic metric method that quickly finds the suitable number of diffusing iterations and produces smoothed local embeddings that enable the truncated receptive field to become scalable and independent of prior experience. Furthermore, we devise smooth2seq and diffusion-based position schemes introduced into Transformer architecture for better capturing local and global information among embeddings. Experimental results show that SAGCN enjoys high accuracy, scalability and efficiency on various open benchmarks and is competitive with other state-of-the-art competitors.  © 2023 Association for Computing Machinery.",diffusion and smoothing; Graph Neural Network; graph receptive field; graph Transformer; scalability and adaptability,Backpropagation; Convolution; Convolutional neural networks; Diffusion; Embeddings; Graph neural networks; Heuristic methods; Multilayer neural networks; Network architecture; Convolutional networks; Diffusion and smoothing; Embeddings; Graph neural networks; Graph receptive field; Graph representation; Graph transformer; Receptive fields; Scalability and adaptability; Shallowest layers; Scalability
MIRROR: Mining Implicit Relationships via Structure-Enhanced Graph Convolutional Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153884807&doi=10.1145%2f3564531&partnerID=40&md5=276070ac2761f1b69b8e3313f9751448,"Data explosion in the information society drives people to develop more effective ways to extract meaningful information. Extracting semantic information and relational information has emerged as a key mining primitive in a wide variety of practical applications. Existing research on relation mining has primarily focused on explicit connections and ignored underlying information, e.g., the latent entity relations. Exploring such information (defined as implicit relationships in this article) provides an opportunity to reveal connotative knowledge and potential rules. In this article, we propose a novel research topic, i.e., how to identify implicit relationships across heterogeneous networks. Specially, we first give a clear and generic definition of implicit relationships. Then, we formalize the problem and propose an efficient solution, namely MIRROR, a graph convolutional network (GCN) model to infer implicit ties under explicit connections. MIRROR captures rich information in learning node-level representations by incorporating attributes from heterogeneous neighbors. Furthermore, MIRROR is tolerant of missing node attribute information because it is able to utilize network structure. We empirically evaluate MIRROR on four different genres of networks, achieving state-of-the-art performance for target relations mining. The underlying information revealed by MIRROR contributes to enriching existing knowledge and leading to novel domain insights.  © 2023 Association for Computing Machinery.",graph convolutional networks; heterogeneous networks; implicit relationships; Relation mining,Convolution; Data mining; Digital storage; Mirrors; Semantics; Convolutional networks; Data explosion; Explicit connections; Graph convolutional network; Implicit relationships; Information society; Network models; Relation mining; Research topics; Semantics Information; Heterogeneous networks
Fairness of Information Flow in Social Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154608733&doi=10.1145%2f3578268&partnerID=40&md5=b272ef3eda62bd5ad9f64630744f2344,"Social networks form a major parts of people's lives, and individuals often make important life decisions based on information that spreads through these networks. For this reason, it is important to know whether individuals from different protected groups have equal access to information flowing through a network. In this article, we define the Information Unfairness (IUF) metric, which quantifies inequality in access to information across protected groups. We then introduce MinIUF, an algorithm for reducing inequalities in information flow by adding edges to the network. Finally, we provide an in-depth analysis of information flow with respect to an attribute of interest, such as gender, across different types of networks to evaluate whether the structure of these networks allows groups to equally access information flowing in the network. Moreover, we investigate the causes of unfairness in such networks and how it can be improved.  © 2023 Association for Computing Machinery.",information fairness; information flow; Social Network Analysis,Decision-based; Flow byes; In-depth analysis; Information fairness; Information flows; Network forms; Social Network Analysis; Social networking (online)
GRASP: Scalable Graph Alignment by Spectral Corresponding Functions,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153861423&doi=10.1145%2f3561058&partnerID=40&md5=7dd6974fb7d0f4339a55ddf634b212c2,"What is the best way to match the nodes of two graphs? This graph alignment problem generalizes graph isomorphism and arises in applications from social network analysis to bioinformatics. Some solutions assume that auxiliary information on known matches or node or edge attributes is available, or utilize arbitrary graph features. Such methods fare poorly in the pure form of the problem, in which only graph structures are given. Other proposals translate the problem to one of aligning node embeddings, yet, by doing so, provide only a single-scale view of the graph. In this article, we transfer the shape-analysis concept of functional maps from the continuous to the discrete case, and treat the graph alignment problem as a special case of the problem of finding a mapping between functions on graphs. We present GRASP, a method that first establishes a correspondence between functions derived from Laplacian matrix eigenvectors, which capture multiscale structural characteristics, and then exploits this correspondence to align nodes. We enhance the basic form of GRASP by altering two of its components, namely the embedding method and the assignment procedure it employs, leveraging its modular, hence adaptable design. Our experimental study, featuring noise levels higher than anything used in previous studies, shows that the enhanced form of GRASP outperforms scalable state-of-the-art methods for graph alignment across noise levels and graph types, and performs competitively with respect to the best non-scalable ones. We include in our study another modular graph alignment algorithm, CONE, which is also adaptable thanks to its modular nature, and show it can manage graphs with skewed power-law degree distributions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph alignment; graph matching; graph mining; network alignment,Graph embeddings; Graph theory; Pattern matching; Alignment Problems; Graph alignment; Graph isomorphism; Graph matchings; Graph mining; Modulars; Network alignments; Noise levels; Social Network Analysis; Two-graphs; Matrix algebra
TAP: Traffic Accident Profiling via Multi-Task Spatio-Temporal Graph Representation Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153880170&doi=10.1145%2f3564594&partnerID=40&md5=d2f2d4d289390fb1a1c9dc0c68dd907e,"Predicting traffic accidents can help traffic management departments respond to sudden traffic situations promptly, improve drivers' vigilance, and reduce losses caused by traffic accidents. However, the causality of traffic accidents is complex and difficult to analyze. Most existing traffic accident prediction methods do not consider the dynamic spatio-temporal correlation of traffic data, which leads to unsatisfactory prediction accuracy. To address this issue, we propose a multi-task learning framework (TAP) based on the Spatio-temporal Variational Graph Auto-Encoders (ST-VGAE) for traffic accident profiling. We firstly capture the dynamic spatio-temporal correlation of traffic conditions through a spatio-temporal graph convolutional encoder and embed it as a low-latitude vector. Then, we use a multi-task learning scheme to combine external factors to generate the traffic accident profiling. Furthermore, we propose a traffic accident profiling application framework based on edge computing. This method increases the speed of calculation by offloading the calculation of traffic accident profiling to edge nodes. Finally, the experimental results on real datasets demonstrate that TAP outperforms other state-of-the-art baselines. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph convolutional network; graph representation learning; spatio-temporal data; Traffic accident profiling,Accidents; Convolution; Convolutional neural networks; Deep learning; Graph neural networks; Learning systems; Signal encoding; Convolutional networks; Graph convolutional network; Graph representation; Graph representation learning; Multi tasks; Multitask learning; Spatio-temporal data; Spatio-temporal graphs; Spatiotemporal correlation; Traffic accident profiling; Forecasting
Graph Deep Factors for Probabilistic Time-series Forecasting,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152619555&doi=10.1145%2f3543511&partnerID=40&md5=390b81b9c44b1da5d9a58da902fe2233,"Effective time-series forecasting methods are of significant importance to solve a broad spectrum of research problems. Deep probabilistic forecasting techniques have recently been proposed for modeling large collections of time-series. However, these techniques explicitly assume either complete independence (local model) or complete dependence (global model) between time-series in the collection. This corresponds to the two extreme cases where every time-series is disconnected from every other time-series in the collection or likewise, that every time-series is related to every other time-series resulting in a completely connected graph. In this work, we propose a deep hybrid probabilistic graph-based forecasting framework called Graph Deep Factors (GraphDF) that goes beyond these two extremes by allowing nodes and their time-series to be connected to others in an arbitrary fashion. GraphDF is a hybrid forecasting framework that consists of a relational global and relational local model. In particular, a relational global model learns complex non-linear time-series patterns globally using the structure of the graph to improve both forecasting accuracy and computational efficiency. Similarly, instead of modeling every time-series independently, a relational local model not only considers its individual time-series but also the time-series of nodes that are connected in the graph. The experiments demonstrate the effectiveness of the proposed deep hybrid graph-based forecasting model compared to the state-of-the-art methods in terms of its forecasting accuracy, runtime, and scalability. Our case study reveals that GraphDF can successfully generate cloud usage forecasts and opportunistically schedule workloads to increase cloud cluster utilization by 47.5% on average. Furthermore, we target addressing the common nature of many time-series forecasting applications where time-series are provided in a streaming version; however, most methods fail to leverage the newly incoming time-series values and result in worse performance over time. In this article, we propose an online incremental learning framework for probabilistic forecasting. The framework is theoretically proven to have lower time and space complexity. The framework can be universally applied to many other machine learning-based methods.  © 2023 Association for Computing Machinery.",Graph Neural Network; Incremental online learning; time-series forecasting,Complex networks; Computational efficiency; Deep learning; E-learning; Forecasting; Graph neural networks; Graph theory; Graphic methods; Learning systems; Forecasting accuracy; Global models; Graph neural networks; Graph-based; Incremental online learning; Local model; Online learning; Probabilistic forecasting; Time series forecasting; Times series; Time series
Stratification of Children with Autism Spectrum Disorder Through Fusion of Temporal Information in Eye-gaze Scan-Paths,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153527899&doi=10.1145%2f3539226&partnerID=40&md5=82696e73ca61c154d813129c41f3e43b,"Background: Looking pattern differences are shown to separate individuals with Autism Spectrum Disorder (ASD) and Typically Developing (TD) controls. Recent studies have shown that, in children with ASD, these patterns change with intellectual and social impairments, suggesting that patterns of social attention provide indices of clinically meaningful variation in ASD.Method: We conducted a naturalistic study of children with ASD (n = 55) and typical development (TD, n = 32). A battery of eye-tracking video stimuli was used in the study, including Activity Monitoring (AM), Social Referencing (SR), Theory of Mind (ToM), and Dyadic Bid (DB) tasks. This work reports on the feasibility of spatial and spatiotemporal scanpaths generated from eye-gaze patterns of these paradigms in stratifying ASD and TD groups.Algorithm: This article presents an approach for automatically identifying clinically meaningful information contained within the raw eye-tracking data of children with ASD and TD. The proposed mechanism utilizes combinations of eye-gaze scan-paths (spatial information), fused with temporal information and pupil velocity data and Convolutional Neural Network (CNN) for stratification of diagnosis (ASD or TD).Results: Spatial eye-gaze representations in the form of scanpaths in stratifying ASD and TD (ASD vs. TD: DNN: 74.4%) are feasible. These spatial eye-gaze features, e.g., scan-paths, are shown to be sensitive to factors mediating heterogeneity in ASD: age (ASD: 2-4 y/old vs. 10-17 y/old CNN: 80.5%), gender (Male vs. Female ASD: DNN: 78.0%) and the mixture of age and gender (5-9 y/old Male vs. 5-9 y/old Female ASD: DNN:98.8%). Limiting scan-path representations temporally increased variance in stratification performance, attesting to the importance of the temporal dimension of eye-gaze data. Spatio-Temporal scan-paths that incorporate velocity of eye movement in their images of eye-gaze are shown to outperform other feature representation methods achieving classification accuracy of 80.25%.Conclusion: The results indicate the feasibility of scan-path images to stratify ASD and TD diagnosis in children of varying ages and gender. Infusion of temporal information and velocity data improves the classification performance of our deep learning models. Such novel velocity fused spatio-temporal scan-path features are shown to be able to capture eye gaze patterns that reflect age, gender, and the mixed effect of age and gender, factors that are associated with heterogeneity in ASD and difficulty in identifying robust biomarkers for ASD.  © 2023 Association for Computing Machinery.",Autism Spectrum Disorder; Convolution Neural Network; eye tracking; eye-gaze scan-path,Convolution; Convolutional neural networks; Diseases; Eye movements; Autism spectrum disorders; Children with autisms; Convolution neural network; Convolutional neural network; Eye-gaze; Eye-gaze scan-path; Eye-tracking; Scan path; Temporal information; Velocities data; Eye tracking
An Efficient Aggregation Method for the Symbolic Representation of Temporal Data,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150184960&doi=10.1145%2f3532622&partnerID=40&md5=3aa74aa3b321f36d58e2ba745d5647ee,"Symbolic representations are a useful tool for the dimension reduction of temporal data, allowing for the efficient storage of and information retrieval from time series. They can also enhance the training of machine learning algorithms on time series data through noise reduction and reduced sensitivity to hyperparameters. The adaptive Brownian bridge-based aggregation (ABBA) method is one such effective and robust symbolic representation, demonstrated to accurately capture important trends and shapes in time series. However, in its current form, the method struggles to process very large time series. Here, we present a new variant of the ABBA method, called fABBA. This variant utilizes a new aggregation approach tailored to the piecewise representation of time series. By replacing the k-means clustering used in ABBA with a sorting-based aggregation technique, and thereby avoiding repeated sum-of-squares error computations, the computational complexity is significantly reduced. In contrast to the original method, the new approach does not require the number of time series symbols to be specified in advance. Through extensive tests, we demonstrate that the new method significantly outperforms ABBA with a considerable reduction in runtime while also outperforming the popular SAX and 1d-SAX representations in terms of reconstruction accuracy. We further demonstrate that fABBA can compress other data types such as images.  © 2023 Association for Computing Machinery.",data compression; Knowledge representation; symbolic aggregation; time series mining,Data compression; Data reduction; Digital storage; K-means clustering; Learning algorithms; Machine learning; Noise abatement; Reduction; Time series; Aggregation methods; Brownian bridge; Dimension reduction; Knowledge-representation; Machine learning algorithms; Symbolic aggregation; Symbolic representation; Temporal Data; Time-series mining; Times series; Knowledge representation
Efficient Node PageRank Improvement via Link-building using Geometric Deep Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152557283&doi=10.1145%2f3551642&partnerID=40&md5=58f0404702ab50ceac6a0a9f8109decd,"Centrality is a relevant topic in the field of network research, due to its various theoretical and practical implications. In general, all centrality metrics aim at measuring the importance of nodes (according to some definition of importance), and such importance scores are used to rank the nodes in the network, therefore the rank improvement is a strictly related topic. In a given network, the rank improvement is achieved by establishing new links, therefore the question shifts to which and how many links should be collected to get a desired rank. This problem, also known as link-building has been shown to be NP-hard, and most heuristics developed failed in obtaining good performance with acceptable computational complexity. In this article, we present LB-GDM, a novel approach that leverages Geometric Deep Learning to tackle the link-building problem. To validate our proposal, 31 real-world networks were considered; tests show that LB-GDM performs significantly better than the state-of-the-art heuristics, while having a comparable or even lower computational complexity, which allows it to scale well even to large networks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",best attachment; complex networks; Graph Attention Network; Link-building; Machine Learning; PageRank; ranking,Computational complexity; Deep learning; AS-links; Best attachment; Graph attention network; Link-building; Machine-learning; NP-hard; Page ranks; Performance; Ranking; Real-world networks; Complex networks
Interpretable Embedding and Visualization of Compressed Data,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152635588&doi=10.1145%2f3537901&partnerID=40&md5=d03f7c6f83fd13a52f09d5b8f4ce6b3d,"Traditional embedding methodologies, also known as dimensionality reduction techniques, assume the availability of exact pairwise distances between the high-dimensional objects that will be embedded in a lower dimensionality. In this article, we propose an embedding that overcomes this limitation and can operate on pairwise distances that are represented as a range of lower and upper bounds. Such bounds are typically estimated when objects are compressed in a lossy manner, so our approach is highly applicable in the case of big compressed datasets. Our methodology can preserve multiple aspects of the original data relationships: distances, correlations, and object scores/ranks, whereas existing techniques typically preserve only distances. Comparative experiments with prevalent embedding methodologies (ISOMAP, t-SNE, MDS, UMAP) illustrate that our approach can provide fidelitous preservation of multiple object relationships, even in the presence of inexact distance information. Our visualization method is also easily interpretable.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",compressed data; data embedding; Dimensionality reduction,Data visualization; Visualization; Compressed datum; Data embedding; Dimensionality reduction; Dimensionality reduction techniques; Embeddings; High-dimensional; Higher-dimensional; Low dimensionality; Lower and upper bounds; Pairwise distances; Embeddings
Contact Tracing and Epidemic Intervention via Deep Reinforcement Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152622787&doi=10.1145%2f3546870&partnerID=40&md5=08f938f0a578a94d74429e59f8eb559f,"The recent outbreak of COVID-19 poses a serious threat to people's lives. Epidemic control strategies have also caused damage to the economy by cutting off humans' daily commute. In this article, we develop an Individual-based Reinforcement Learning Epidemic Control Agent (IDRLECA) to search for smart epidemic control strategies that can simultaneously minimize infections and the cost of mobility intervention. IDRLECA first hires an infection probability model to calculate the current infection probability of each individual. Then, the infection probabilities together with individuals' health status and movement information are fed to a novel GNN to estimate the spread of the virus through human contacts. The estimated risks are used to further support an RL agent to select individual-level epidemic-control actions. The training of IDRLECA is guided by a specially designed reward function considering both the cost of mobility intervention and the effectiveness of epidemic control. Moreover, we design a constraint for control-action selection that eases its difficulty and further improve exploring efficiency. Extensive experimental results demonstrate that IDRLECA can suppress infections at a very low level and retain more than 95% of human mobility.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph neural network; mobility intervention; reinforcement learning,Computer viruses; Deep learning; Disease control; Graph neural networks; Risk perception; Viruses; Contact tracing; Control actions; Control agent; Control strategies; Cutting-off; Epidemic control; Graph neural networks; Individual-based; Mobility intervention; Reinforcement learnings; Reinforcement learning
Explainability-Based Mix-Up Approach for Text Data Augmentation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150166742&doi=10.1145%2f3533048&partnerID=40&md5=23f93535b4455fef83330173efc75b83,"Text augmentation is a strategy for increasing the diversity of training examples without explicitly collecting new data. Owing to the efficiency and effectiveness of text augmentation, numerous augmentation methodologies have been proposed. Among them, the method based on modification, particularly the mix-up method of swapping words between two or more sentences, is widely used because it can be applied simply and shows good levels of performance. However, the existing mix-up approaches are limited; they do not reflect the importance of the manipulated word. That is, even if a word that has a critical effect on the classification result is manipulated, it is not considered significant in labeling the augmented data. Therefore, in this study, we propose an effective text augmentation technique that explicitly derives the importance of manipulated words and reflects this importance in the labeling of augmented data. The importance of each word, in other words, explainability, is calculated, and this is explicitly reflected in the labeling process of the augmented data. The results of the experiment confirmed that when the importance of the manipulated word was reflected in the labeling, the performance was significantly higher than that of the existing methods.  © 2023 Association for Computing Machinery.",mix-up approach; soft-labeling; Text augmentation; word-explainability; XAI,Data augmentation; Labelings; Mix-up approach; Performance; Soft-labeling; Text augmentation; Text data; Training example; Word-explainability; XAI; Computer programming
Supervised Contrastive Learning for Interpretable Long-Form Document Matching,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152627565&doi=10.1145%2f3542822&partnerID=40&md5=ee143632a84d44f369aec8ace35199c9,"Recent advancements in deep learning techniques have transformed the area of semantic text matching (STM). However, most state-of-the-art models are designed to operate with short documents such as tweets, user reviews, comments, and so on. These models have fundamental limitations when applied to long-form documents such as scientific papers, legal documents, and patents. When handling such long documents, there are three primary challenges: (i) the presence of different contexts for the same word throughout the document, (ii) small sections of contextually similar text between two documents, but dissimilar text in the remaining parts (this defies the basic understanding of ""similarity""), and (iii) the coarse nature of a single global similarity measure which fails to capture the heterogeneity of the document content. In this article, we describe CoLDE: Contrastive Long Document Encoder - a transformer-based framework that addresses these challenges and allows for interpretable comparisons of long documents. CoLDE uses unique positional embeddings and a multi-headed chunkwise attention layer in conjunction with a supervised contrastive learning framework to capture similarity at three different levels: (i) high-level similarity scores between a pair of documents, (ii) similarity scores between different sections within and across documents, and (iii) similarity scores between different chunks in the same document and across other documents. These fine-grained similarity scores aid in better interpretability. We evaluate CoLDE on three long document datasets namely, ACL Anthology publications, Wikipedia articles, and USPTO patents. Besides outperforming the state-of-the-art methods on the document matching task, CoLDE is also robust to changes in document length and text perturbations and provides interpretable results. The code for the proposed model is publicly available at https://github.com/InterDigitalInc/CoLDE.  © 2023 Copyright held by the owner/author(s).",attention; BERT; contrastive learning; embeddings; interpretability; long documents; Semantic text matching; transformer,Deep learning; Embeddings; Patents and inventions; Attention; BERT; Contrastive learning; Embeddings; Interpretability; Long document; Semantic text matching; Similarity scores; Text-matching; Transformer; Semantics
Detecting Anomalous Graphs in Labeled Multi-Graph Databases,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152628898&doi=10.1145%2f3533770&partnerID=40&md5=580572f6613169e7a66436ff9be63ccc,"Within a large database containing graphs with labeled nodes and directed, multi-edges; how can we detect the anomalous graphs? Most existing work are designed for plain (unlabeled) and/or simple (unweighted) graphs. We introduce CODEtect, the first approach that addresses the anomaly detection task for graph databases with such complex nature. To this end, it identifies a small representative set of structural patterns (i.e., node-labeled network motifs) that losslessly compress database as concisely as possible. Graphs that do not compress well are flagged as anomalous. CODEtect exhibits two novel building blocks: (i) a motif-based lossless graph encoding scheme, and (ii) fast memory-efficient search algorithms for . We show the effectiveness of CODEtect on transaction graph databases from three different corporations and statistically similar synthetic datasets, where existing baselines adjusted for the task fall behind significantly, across different types of anomalies and performance metrics.  © 2023 Association for Computing Machinery.",Graph anomaly detection; graph encoding; graph motifs,Anomaly detection; Directed graphs; Encoding (symbols); Graph algorithms; Graph neural networks; Graph structures; Graphic methods; Signal encoding; Anomaly detection; Complex nature; Detection tasks; Graph anomaly detection; Graph database; Graph encoding; Graph motif; Large database; Simple++; Unweighted graphs; Graph Databases
Distance-Preserving Embedding Adaptive Bipartite Graph Multi-View Learning with Application to Multi-Label Classification,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152625424&doi=10.1145%2f3537900&partnerID=40&md5=970727ca55bf0cdfdf239b8c9a510a55,"Graph-based multi-view learning has attracted much attention due to the efficacy of fusing the information from different views. However, most of them exhibit high computational complexity. We propose an anchor-based bipartite graph embedding approach to accelerate the learning process. Specifically, different from existing anchor-based methods where anchors are obtained from key samples by clustering or weighted averaging strategies, in this article, the anchors are learned in a principled fashion which aims at constructing a distance-preserving embedding for each view from samples to their representations, whose elements are the weights of the edges linking corresponding samples and anchors. In addition, the consistency among different views can be explored by imposing a low-rank constraint on the concatenated embedding representations. We further design a concise yet effective feature collinearity guided feature selection scheme to learn tight multi-label classifiers. The objective function is optimized in an alternating optimization fashion. Both theoretical analysis and experimental results on different multi-label image datasets verify the effectiveness and efficiency of the proposed method.  © 2023 Association for Computing Machinery.",bipartite graph; distance-preserving embedding; multi-label learning; Multi-view learning,Anchors; Classification (of information); Data mining; Graph embeddings; Graph theory; Bipartite graphs; Distance-preserving embedding; Embeddings; Graph embeddings; Graph-based; Learning process; Multi-label classifications; Multi-label learning; Multi-labels; Multi-view learning; Graphic methods
Explainable Integration of Social Media Background in a Dynamic Neural Recommender,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152623808&doi=10.1145%2f3550279&partnerID=40&md5=02300fcb8b2c085641519270db1b261c,"Recommender systems nowadays are commonly deployed in e-commerce platforms to help customers making purchase decisions. Dynamic recommender considers not only static user-item interaction data, but the temporal information at the time of recommendation. Previous researches have suggested to incorporate social media as the temporal information in dynamic neural recommenders after transforming them into embeddings. While such an approach can potentially improve recommendation performance, the effectiveness is difficult to explain. In this article, we propose an explainable method to integrate social media in a dynamic neural recommender. Our method applies association rule mining, which can generate human-understandable behavior patterns from social media and e-commerce platforms. With real-world social media and e-commerce data, we show that the integration can improve accuracy by up to 14% while using the same data. Moreover, we can explain the positive cases by examining relevant association rules.  © 2023 Copyright held by the owner/author(s).",association rules; Dynamic recommender systems; explainable neural network model; social media mining,Association rules; Behavioral research; Data mining; Electronic commerce; Recommender systems; Social networking (online); Commerce platforms; Dynamic recommende system; E- commerces; Explainable neural network model; Neural network model; Purchase decision; Social media; Social media commerces; Social media minings; Temporal information; Neural network models
On Dynamically Pricing Crowdsourcing Tasks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152620653&doi=10.1145%2f3544018&partnerID=40&md5=76a6a6a2551f312782687715a6492662,"Crowdsourcing techniques have been extensively explored in the past decade, including task allocation, quality assessment, and so on. Most of professional crowdsourcing platforms adopt the fixed pricing scheme to offer a fixed price for crowd tasks. It is neither incentive for crowd workers to produce good performance, nor profitable for the requester to gain high utility with low budget. In this article, we study the problem of pricing crowdsourcing tasks with optional bonuses. We propose a dynamic pricing mechanism, named CrowdPricer for incentively delivering bonuses to the crowd workers of completing tasks, in addition to offering a base payment for completing a task. We leverage a deep time sequence model to learn the effect of bonuses on workers' quality for crowd tasks. CrowdPricer makes decisions on whether to provide bonuses on workers, so as to maximize the requester's utility in expectation. We present an efficient bonus delivery algorithm under the help of beam search technique, in order to efficiently solve the decision making problem. Extensive experiments using both a real crowdsourcing platform and simulations demonstrate that CrowdPricer yields the higher utility for the requester. It also obtains more correct crowd answers than the state-of-the-art pricing methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Crowdsourcing; deep learning model; optimization; pricing mechanism,Budget control; Costs; Decision making; Deep learning; Fixed platforms; Simulation platform; Crowdsourcing platforms; Deep learning model; Fixed prices; Learning models; Optimisations; Pricing mechanism; Pricing scheme; Quality assessment; Task allocation; Workers'; Crowdsourcing
Dynamic Graph Convolutional Recurrent Network for Traffic Prediction: Benchmark and Solution,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149369270&doi=10.1145%2f3532611&partnerID=40&md5=269341ddfb729863890d69372b6008a1,"Traffic prediction is the cornerstone of intelligent transportation system. Accurate traffic forecasting is essential for the applications of smart cities, i.e., intelligent traffic management and urban planning. Although various methods are proposed for spatio-temporal modeling, they ignore the dynamic characteristics of correlations among locations on road network. Meanwhile, most Recurrent Neural Network based works are not efficient enough due to their recurrent operations. Additionally, there is a severe lack of fair comparison among different methods on the same datasets. To address the above challenges, in this article, we propose a novel traffic prediction framework, named Dynamic Graph Convolutional Recurrent Network (DGCRN). In DGCRN, hyper-networks are designed to leverage and extract dynamic characteristics from node attributes, while the parameters of dynamic filters are generated at each time step. We filter the node embeddings and then use them to generate dynamic graph, which is integrated with pre-defined static graph. As far as we know, we are first to employ a generation method to model fine topology of dynamic graph at each time step. Furthermore, to enhance efficiency and performance, we employ a training strategy for DGCRN by restricting the iteration number of decoder during forward and backward propagation. Finally, a reproducible standardized benchmark and a brand new representative traffic dataset are opened for fair comparison and further research. Extensive experiments on three datasets demonstrate that our model outperforms 15 baselines consistently. Source codes are available at https://github.com/tsinghua-fib-lab/Traffic-Benchmark.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dynamic graph construction; traffic benchmark; Traffic prediction,Backpropagation; Convolution; Forecasting; Graph neural networks; Intelligent systems; Intelligent vehicle highway systems; Iterative methods; Topology; Traffic control; Dynamic graph; Dynamic graph construction; Dynamics characteristic; Graph construction; Intelligent transportation systems; Recurrent networks; Time step; Traffic benchmark; Traffic Forecasting; Traffic prediction; Recurrent neural networks
Meta-Information Fusion of Hierarchical Semantics Dependency and Graph Structure for Structured Text Classification,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144282358&doi=10.1145%2f3537971&partnerID=40&md5=d2a98ce65f5e08c796445921b1ffdffa,"Structured text with plentiful hierarchical structure information is an important part in real-world complex texts. Structured text classification is attracting more attention in natural language processing due to the increasing complexity of application scenarios. Most existing methods treat structured text from a local hierarchy perspective, focusing on the semantics dependency and the graph structure of the structured text independently. However, structured text has global hierarchical structures with sophisticated dependency when compared to unstructured text. According to the variety of structured texts, it is not appropriate to use the existing methods directly. The function of distinction information within semantics dependency and graph structure for structured text, referred to as meta-information, should be stated more precisely. In this article, we propose HGMETA, a novel meta-information embedding frame network for structured text classification, to obtain the fusion embedding of hierarchical semantics dependency and graph structure in a structured text, and to distill the meta-information from fusion characteristics. To integrate the global hierarchical features with fused structured text information, we design a hierarchical LDA module and a structured text embedding module. Specially, we employ a multi-hop message passing mechanism to explicitly incorporate complex dependency into a meta-graph. The meta-information is constructed from meta-graph via neighborhood-based propagation to distill redundant information. Furthermore, using an attention-based network, we investigate the complementarity of semantics dependency and graph structure based on global hierarchical characteristics and meta-information. Finally, the fusion embedding and the meta-information can be straightforwardly incorporated for structured text classification. Experiments conducted on three real-world datasets show the effectiveness of meta-information and demonstrate the superiority of our method.  © 2023 Association for Computing Machinery.",hierarchical semantics; meta-graph; meta-information; Structured text,Classification (of information); Complex networks; Embeddings; Graphic methods; Message passing; Natural language processing systems; Text processing; Dependency structures; Embeddings; Graph structures; Hierarchical semantic; Meta information; Meta-graph; Semantic dependency; Semantic graphs; Structured text; Text classification; Semantics
Multiple Imputation Ensembles for Time Series (MIE-TS),2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152627178&doi=10.1145%2f3551643&partnerID=40&md5=b5e93db2307ec6ab03d950e5f1380d41,"Time series classification has become an interesting field of research, thanks to the extensive studies conducted in the past two decades. Time series may have missing data, which may affect both the representation and also modeling of time series. Thus, recovering missing data using appropriate time series-based imputation methods is an essential step. Multiple imputation is a data recovery method where it produced multiple imputed data. The method proves its usefulness in terms of reflecting the uncertainty inherit in missing data; however, it is under-researched in time series problems. In this article, we propose two multiple imputation approaches for time series. The first is a multiple imputation method based on interpolation. The second is a multiple imputation and ensemble method. First, we simulate missing consecutive sub-sequences under a Missing Completely at Random mechanism; then, we use single/multiple imputation methods. The imputed data are used to build bagging and stacking ensembles. We build ensembles using standard classification algorithms as well as time series classifiers. The standard classifiers involve Random Forest, Support Vector Machines, K-Nearest Neighbour, C4.5, and PART while TSCHIEF, Proximity Forest, Time Series Forest, RISE, and BOSS are chosen as time series classifiers. Our findings show that the combination of multiple imputation and ensemble improves the performance of the majority of classifiers tested in this study, often above the performance obtained from the complete data, even under increasing missing data scenarios. This may be because the diversity injected by multiple imputation has a very favourable and stabilising effect on the classifier performance, which is a very important finding.  © 2023 Association for Computing Machinery.",ensemble methods; Missing data; multiple imputation; time series,Nearest neighbor search; Support vector machines; Data recovery; Ensemble methods; Imputation methods; Missing data; Multiple imputation; Performance; Recovery methods; Time series classifications; Times series; Uncertainty; Time series
Dual Subgraph-Based Graph Neural Network for Friendship Prediction in Location-Based Social Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152626120&doi=10.1145%2f3554981&partnerID=40&md5=3ff4702d4ec73747653b412f474132be,"With the wide use of Location-Based Social Networks (LBSNs), predicting user friendship from online social relations and offline trajectory data is of great value to improve the platform service quality and user satisfaction. Existing methods mainly focus on some hand-crafted features or graph embedding models based on the user-location bipartite graph, which cannot precisely capture the latent mobility similarity for the majority of users who have no explicit co-visit behaviors and also fail to balance the tradeoff between social features and mobility features for friendship prediction. In this regard, we propose a dual subgraph-based pairwise graph neural network (DSGNN) for friendship prediction in LBSNs, which extracts a pairwise social subgraph and a trajectory subgraph to model the social proximity and mobility similarity, respectively. Specifically, to overcome the co-visit data sparsity, we design an entropy-based random walk to construct a location graph that captures the high-level correlation between locations. Based on this, we characterize the pairwise mobility similarity from trajectory level instead of location level, which is modeled by a graph neural network (GNN) on a labeled trajectory subgraph composed of the two trajectories of the target user pair. Besides, we also utilize another GNN to extract social proximity based on social subgraph of the target user pair. Finally, we propose a gate layer to adaptively balance the fusion of the social and mobility features for friendship prediction. We conduct extensive experiments on the real-world datasets and demonstrate the superiority of our approach, which outperforms other state-of-the-art methods. In particular, the comparative experiments on the trajectory level mobility similarity further validate the effectiveness of the designed trajectory subgraph-based method, which can extract predictive mobility features.  © 2023 Association for Computing Machinery.",friendship prediction; graph neural network; Location-based social network; mobility; random walk,Data mining; Graph neural networks; Location; Random processes; Social networking (online); Trajectories; Friendship prediction; Graph neural networks; Location-based social networks; Mobility; Offline; Random Walk; Social proximity; Social relations; Subgraphs; Trajectories datum; Forecasting
Learning Aspect-Aware High-Order Representations from Ratings and Reviews for Recommendation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150344233&doi=10.1145%2f3532188&partnerID=40&md5=ba1fd05d84902925d49d964b095b413f,"Textual reviews contain rich semantic information that is useful for making better recommendation, as such semantic information may indicate more fine-grained preferences of users. Recent efforts make considerable improvement on recommendation by integrating textual reviews in rating-based recommendations. However, there still exist major challenges on integrating textual reviews for recommendation. On the one hand, most existing works focus on learning a single representation from reviews but ignoring complex relations between users (or items) and reviews, which may fail to capture user preferences and item attributes together. On the other hand, these works independently learn latent representations from ratings and reviews while omitting correlations between rating-based features and review-based features, which may harm recommendation performance. In this article, we capture the aspect-aware relations by constructing heterogeneous graphs from reviews. Furthermore, we propose a new recommendation model, namely AHOR, to jointly distill rating-based features and review-based features, which are derived from ratings and reviews, respectively. To explore the multi-hop connectivity information between users, items, and aspects, a novel graph neural network is introduced to learn aspect-aware high-order representations. Experiments based on public datasets show that our approach outperforms state-of-the-art methods. We also provide detailed analysis on the high-order signals and the aspect importance to show the interpretability of our proposed model.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesReview-based recommendation; aspect-aware high-order representations; attention mechanism; graph neural network,Recommender systems; Semantics; Additional key word and phrasesreview-based recommendation; Aspect-aware high-order representation; Attention mechanisms; Fine grained; Graph neural networks; High-order; Higher-order; Key words; Learn+; Semantics Information; Graph neural networks
Be Causal: De-Biasing Social Network Confounding in Recommendation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150200005&doi=10.1145%2f3533725&partnerID=40&md5=c9105fff3313acd1301c266ed1e602f9,"In recommendation systems, the existence of the missing-not-at-random (MNAR) problem results in the selection bias issue, degrading the recommendation performance ultimately. A common practice to address MNAR is to treat missing entries from the so-called ""exposure""perspective, i.e., modeling how an item is exposed (provided) to a user. Most of the existing approaches use heuristic models or re-weighting strategy on observed ratings to mimic the missing-at-random setting. However, little research has been done to reveal how the ratings are missing from a causal perspective. To bridge the gap, we propose an unbiased and robust method called DENC (De-Bias Network Confounding in Recommendation), inspired by confounder analysis in causal inference. In general, DENC provides a causal analysis on MNAR from both the inherent factors (e.g., latent user or item factors) and auxiliary network's perspective. Particularly, the proposed exposure model in DENC can control the social network confounder meanwhile preserve the observed exposure information. We also develop a deconfounding model through the balanced representation learning to retain the primary user and item features, which enables DENC generalize well on the rating prediction. Extensive experiments on three datasets validate that our proposed model outperforms the state-of-the-art baselines.  © 2023 Association for Computing Machinery.",bias; causal inference; missing-not-at-random; propensity; Recommendation,Data mining; Social networking (online); Bias; Bias networks; Causal inferences; Confounder; De bias; De-biasing; Missing not at random; Propensity; Random problem; Recommendation; User profile
GRACE: A General Graph Convolution Framework for Attributed Graph Clustering,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152621440&doi=10.1145%2f3544977&partnerID=40&md5=48336831daf8336df32593a824d5f5bf,"Attributed graph clustering (AGC) is an important problem in graph mining as more and more complex data in real-world have been represented in graphs with attributed nodes. While it is a common practice to leverage both attribute and structure information for improved clustering performance, most existing AGC algorithms consider only a specific type of relations, which hinders their applicability to integrate various complex relations into node attributes for AGC. In this article, we propose GRACE, an extended graph convolution framework for AGC tasks. Our framework provides a general and interpretative solution for clustering many different types of attributed graphs, including undirected, directed, heterogeneous and hyper attributed graphs. By building suitable graph Laplacians for each of the aforementioned graph types, GRACE can seamlessly perform graph convolution on node attributes to fuse all available information for clustering. We conduct extensive experiments on 14 real-world datasets of four different graph types. The experimental results show that GRACE outperforms the state-of-the-art AGC methods on the different graph types in terms of clustering quality, time, and memory usage.  © 2023 Association for Computing Machinery.",Attributed graph clustering; graph convolution,Data mining; Directed graphs; Graph algorithms; Attribute information; Attributed graph clustering; Attributed graphs; Clusterings; Complex data; General graph; Graph convolution; Graph mining; Node attribute; Real-world; Convolution
A Novel Graph Indexing Approach for Uncovering Potential COVID-19 Transmission Clusters,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152630545&doi=10.1145%2f3538492&partnerID=40&md5=2a56dd9942cf440952784112b8296173,"The COVID-19 pandemic has caused the society lockdowns and a large number of deaths in many countries. Potential transmission cluster discovery is to find all suspected users with infections, which is greatly needed to fast discover virus transmission chains so as to prevent an outbreak of COVID-19 as early as possible. In this article, we study the problem of potential transmission cluster discovery based on the spatio-temporal logs. Given a query of patient user q and a timestamp of confirmed infection tq, the problem is to find all potential infected users who have close social contacts to user q before time tq. We motivate and formulate the potential transmission cluster model, equipped with a detailed analysis of transmission cluster property and particular model usability. To identify potential clusters, one straightforward method is to compute all close contacts on-the-fly, which is simple but inefficient caused by scanning spatio-temporal logs many times. To accelerate the efficiency, we propose two indexing algorithms by constructing a multigraph index and an advanced BCG-index. Leveraging two well-designed techniques of spatio-temporal compression and graph partition on bipartite contact graphs, our BCG-index approach achieves a good balance of index construction and online query processing to fast discover potential transmission cluster. We theoretically analyze and compare the algorithm complexity of three proposed approaches. Extensive experiments on real-world check-in datasets and COVID-19 confirmed cases in the United States validate the effectiveness and efficiency of our potential transmission cluster model and algorithms.  © 2023 Association for Computing Machinery.",COVID-19; Graph index; transmission cluster,Computational complexity; Efficiency; Indexing (of information); Query processing; Transmissions; Viruses; Clusters model; Graph index; Graph indexing; Indexing approaches; Potential transmissions; Spatio-temporal; Time-stamp; Transmission chains; Transmission cluster; Virus transmission; COVID-19
Structure Diversity-Induced Anchor Graph Fusion for Multi-View Clustering,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152633655&doi=10.1145%2f3534931&partnerID=40&md5=5ba96a49d1364037bcdd6982c0e7d899,"The anchor graph structure has been widely used to speed up large-scale multi-view clustering and exhibited promising performance. How to effectively integrate the anchor graphs on multiple views to achieve enhanced clustering performance still remains a challenging task. Existing fusing strategies ignore the structure diversity among anchor graphs and restrict the anchor generation to be same on different views, which degenerates the representation ability of corresponding fused consensus graph. To overcome these drawbacks, we propose a novel structural fusion framework to integrate the multi-view anchor graphs for clustering. Different from traditional integration strategies, we merge the anchors and edges of all the view-specific anchor graphs into a single graph for the structural optimal graph learning. Benefiting from the structural fusion strategy, the anchor generation of each view is not forced to be same, which greatly improves the representation capability of the target structural optimal graph, since the anchors of each view capture the diverse structure of different views. By leveraging the potential structural consistency among each anchor graph, a connectivity constraint is imposed on the target graph to indicate clusters directly without any post-processing such as k-means in classical spectral clustering. Substantial experiments on real-world datasets are conducted to verify the superiority of the proposed method, as compared with the state-of-the-arts over the clustering performance and time expenditure.  © 2023 Association for Computing Machinery.",anchor graph; connectivity constraint; graph fusion; Multi-view clustering; structure diversity,Anchors; K-means clustering; Structural optimization; Anchor graph; Clusterings; Connectivity constraints; Graph fusion; Graph structures; Multi-view clustering; Optimal graphs; Performance; Structural fusion; Structure diversity; Graphic methods
On Equivalence of Anomaly Detection Algorithms,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152614856&doi=10.1145%2f3536428&partnerID=40&md5=337004d152e4a9e785940d8a815fdf8b,"In most domains, anomaly detection is typically cast as an unsupervised learning problem because of the infeasibility of labeling large datasets. In this setup, the evaluation and comparison of different anomaly detection algorithms is difficult. Although some work has been published in this field, they fail to account that different algorithms can detect different kinds of anomalies. More precisely, the literature on this topic has focused on defining criteria to determine which algorithm is better, while ignoring the fact that such criteria are meaningful only if the algorithms being compared are detecting the same kind of anomalies. Therefore, in this article, we propose an equivalence criterion for anomaly detection algorithms that measures to what degree two anomaly detection algorithms detect the same kind of anomalies. First, we lay out a set of desirable properties that such an equivalence criterion should have and why; second, we propose Gaussian Equivalence Criterion (GEC) as equivalence criterion and show mathematically that it has the desirable properties previously mentioned. Finally, we empirically validate these properties using a simulated and a real-world dataset. For the real-world dataset, we show how GEC can provide insight about the anomaly detection algorithms as well as the dataset.  © 2023 Association for Computing Machinery.",anomaly detection; comparison; Unsupervised learning,Anomaly detection; Large dataset; Signal detection; Anomaly detection; Anomaly-detection algorithms; Comparison; Equivalence criteria; Gaussians; Labelings; Large datasets; Learning problem; Property; Real-world datasets; Unsupervised learning
Modeling Cross-session Information with Multi-interest Graph Neural Networks for the Next-item Recommendation,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150309238&doi=10.1145%2f3532192&partnerID=40&md5=fb7da4fe6b4b527b13e0f1d42928f2cd,"Next-item recommendation involves predicting the next item of interest of a given user from their past behavior. Users tend to browse and purchase various items on e-commerce websites according to their varied interests and needs, as reflected in their purchasing history. Most existing next-item recommendation methods aim at extracting the main point of interest in each browsing session and encapsulate it in a single representation. However, past behavior sequences reflect the multiple interests of a single user, which cannot be captured by methods that focus on single-interest contexts. Indeed, multiple interests cannot be captured in a single representation, and doing so results in missing information. Therefore, we propose a model with a multi-interest structure for capturing the various interests of users from their behavior sequence. Moreover, we adopted a method based on a graph neural network to construct interest graphs based on the historical and current behavior sequences of users. These graphs can capture complex item transition patterns related to different interests. In experiments, the proposed method outperforms state-of-the-art session-based recommendation systems on three real-world datasets, achieving 4% improvement of Recall over the SOTAs on Jdata dataset. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph neural network; multi-interest; Next-item recommendation,Behavioral research; Recommender systems; User profile; Behavior sequences; Current behaviors; E-commerce websites; Graph neural networks; Graph-based; Missing information; Multi interests; Next-item recommendation; Recommendation methods; Single users; Graph neural networks
Generative Multi-Label Correlation Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152633551&doi=10.1145%2f3538708&partnerID=40&md5=fd56030f3f4b3a2fc5795a2c5512c64b,"In real-world applications, a single instance could have more than one label. To solve this task, multi-label learning methods emerged in recent years. It is a more challenging problem for many reasons, such as complex label correlation, long-tail label distribution, and data shortage. In general, overcoming these challenges and bettering learning performance could be achieved by utilizing more training samples and including label correlations. However, these solutions are expensive and inflexible. Large-scale, well-labeled datasets are difficult to obtain, and building label correlation maps requires task-specific semantic information as prior knowledge. To address these limitations, we propose a general and compact Multi-Label Correlation Learning (MUCO) framework. MUCO explicitly and effectively learns the latent label correlations by updating a label correlation tensor, which provides highly accurate and interpretable prediction results. In addition, a multi-label generative strategy is deployed to handle the long-tail label distribution challenge. It borrows the visual clues from limited samples and synthesizes more diverse samples. All networks in our model are optimized simultaneously. Extensive experiments illustrate the effectiveness and efficiency of MUCO. Ablation studies further prove the effectiveness of all the modules.  © 2023 Association for Computing Machinery.",Correlation learning; image annotation; image retrieval; multi-label learning,Image retrieval; Large dataset; Learning systems; Semantics; Correlation learning; Image annotation; Label correlations; Label distribution; Learning methods; Learning performance; Long tail; Multi-label learning; Multi-labels; Real-world; Image annotation
SigGAN: Adversarial Model for Learning Signed Relationships in Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150191612&doi=10.1145%2f3532610&partnerID=40&md5=355bf86ba765d09a3b53ed4683e0262c,"Signed link prediction in graphs is an important problem that has applications in diverse domains. It is a binary classification problem that predicts whether an edge between a pair of nodes is positive or negative. Existing approaches for link prediction in unsigned networks cannot be directly applied for signed link prediction due to their inherent differences. Furthermore, signed link prediction must consider the inherent characteristics of signed networks, such as structural balance theory. Recent signed link prediction approaches generate node representations using either generative models or discriminative models. Inspired by the recent success of Generative Adversarial Network (GAN) based models in several applications, we propose a GAN based model for signed networks, SigGAN. It considers the inherent characteristics of signed networks, such as integration of information from negative edges, high imbalance in number of positive and negative edges, and structural balance theory. Comparing the performance with state-of-the-art techniques on five real-world datasets validates the effectiveness of SigGAN.  © 2023 Association for Computing Machinery.",generative adversarial networks; link prediction; Signed networks; structural awareness; structural balance,Forecasting; Binary classification problems; Diverse domains; Generative model; In networks; Inherent characteristics; Link prediction; Network-based modeling; Signed networks; Structural awareness; Structural balance; Generative adversarial networks
Reinforcement Learning for Practical Express Systems with Mixed Deliveries and Pickups,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152633825&doi=10.1145%2f3546952&partnerID=40&md5=ffce436f420ab33a4f1f3514a2284131,"In real-world express systems, couriers need to satisfy not only the delivery demands but also the pick-up demands of customers. Delivery and pickup tasks are usually mixed together within integrated routing plans. Such a mixed routing problem can be abstracted and formulated as Vehicle Routing Problem with Mixed Delivery and Pickup (VRPMDP), which is an NP-hard combinatorial optimization problem. To solve VRPMDP, there are three major challenges as below. (a) Even though successive pickup and delivery tasks are independent to accomplish, the inter-influence between choosing pickup task or delivery task to deal with still exists. (b) Due to the two-way flow of goods between the depot and customers, the loading rate of vehicles leaving the depot affects routing decisions. (c) The proportion of deliveries and pickups will change due to the complex demand situation in real-world scenarios, which requires robustness of the algorithm. To solve the challenges above, we design an encoder-decoder based framework to generate high-quality and robust VRPMDP solutions. First, we consider a VRPMDP instance as a graph and utilize a GNN encoder to extract the feature of the instance effectively. The detailed routing solutions are further decoded as a sequence by the decoder with attention mechanism. Second, we propose a Coordinated Decision of Loading and Routing (CDLR) mechanism to determine the loading rate dynamically after the vehicle returns to the depot, thus avoiding the influence of improper loading rate settings. Finally, the model equipped with a GNN encoder and CDLR simultaneously can adapt to the changes in the proportion of deliveries and pickups. We conduct the experiments to demonstrate the effectiveness of our model. The experiments show that our method achieves desirable results and generalization ability.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Practical express systems; vehicle routing problem with mixed deliveries and pickup; reinforcement learning,Combinatorial optimization; Customer satisfaction; Decoding; Learning systems; Loading; Pickups; Signal encoding; Vehicle routing; Vehicles; Coordinated decision; Delivery demands; Delivery task; Loading rate; Pick-up demands; Practical express system;; Real-world; Reinforcement learnings; Vehicle routing problem with mixed delivery and pickup;; Vehicle Routing Problems; Reinforcement learning
ONP-Miner: One-off Negative Sequential Pattern Mining,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145959541&doi=10.1145%2f3549940&partnerID=40&md5=4aeb60289f9df028dfa87e4d28019e97,"Negative sequential pattern mining (SPM) is an important SPM research topic. Unlike positive SPM, negative SPM can discover events that should have occurred but have not occurred, and it can be used for financial risk management and fraud detection. However, existing methods generally ignore the repetitions of the pattern and do not consider gap constraints, which can lead to mining results containing a large number of patterns that users are not interested in. To solve this problem, this article discovers frequent one-off negative sequential patterns (ONPs). This problem has the following two characteristics. First, the support is calculated under the one-off condition, which means that any character in the sequence can only be used once at most. Second, the gap constraint can be given by the user. To efficiently mine patterns, this article proposes the ONP-Miner algorithm, which employs depth-first and backtracking strategies to calculate the support. Therefore, ONP-Miner can effectively avoid creating redundant nodes and parent-child relationships. Moreover, to effectively reduce the number of candidate patterns, ONP-Miner uses pattern join and pruning strategies to generate and further prune the candidate patterns, respectively. Experimental results show that ONP-Miner not only improves the mining efficiency but also has better mining performance than the state-of-the-art algorithms. More importantly, ONP mining can find more interesting patterns in traffic volume data to predict future traffic.  © 2023 Association for Computing Machinery.",gap constraint; negative sequential pattern; one-off condition; Sequential pattern mining,Data mining; Candidate patterns; Condition; Financial risk management; Fraud detection; Gap constraint; Negative sequential pattern; One-off condition; Research topics; Sequential patterns; Sequential-pattern mining; Risk management
DNformer: Temporal Link Prediction with Transfer Learning in Dynamic Networks,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152615977&doi=10.1145%2f3551892&partnerID=40&md5=930627c1f6ef52369dc968fe7811967a,"Temporal link prediction (TLP) is among the most important graph learning tasks, capable of predicting dynamic, time-varying links within networks. The key problem of TLP is how to explore potential link-evolving tendency from the increasing number of links over time. There exist three major challenges toward solving this problem: temporal nonlinear sparsity, weak serial correlation, and discontinuous structural dynamics. In this article, we propose a novel transfer learning model, called DNformer, to predict temporal link sequence in dynamic networks. The structural dynamic evolution is sequenced into consecutive links one by one over time to inhibit temporal nonlinear sparsity. The self-attention of the model is used to capture the serial correlation between the input and output link sequences. Moreover, our structural encoding is designed to obtain changing structures from the consecutive links and to learn the mapping between link sequences. This structural encoding consists of two parts: the node clustering encoding of each link and the link similarity encoding between links. These encodings enable the model to perceive the importance and correlation of links. Furthermore, we introduce a measurement of structural similarity in the loss function for the structural differences of link sequences. The experimental results demonstrate that our model outperforms other state-of-the-art TLP methods such as Transformer, TGAT, and EvolveGCN. It achieves the three highest AUC and four highest precision scores in five different representative dynamic networks problems.  © 2023 Association for Computing Machinery.",Dynamic network; self-attention; temporal link prediction; transfer learning,Encoding (symbols); Forecasting; Signal encoding; Dynamic network; Dynamic time; Encodings; Learning tasks; Link prediction; Self-attention; Serial correlation; Temporal link prediction; Time varying; Transfer learning; Structural dynamics
Multi-Concept Representation Learning for Knowledge Graph Completion,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150186510&doi=10.1145%2f3533017&partnerID=40&md5=5635f382dae050256db4b74b0e315800,"Knowledge Graph Completion (KGC) aims at inferring missing entities or relations by embedding them in a low-dimensional space. However, most existing KGC methods generally fail to handle the complex concepts hidden in triplets, so the learned embeddings of entities or relations may deviate from the true situation. In this article, we propose a novel Multi-concept Representation Learning (McRL) method for the KGC task, which mainly consists of a multi-concept representation module, a deep residual attention module, and an interaction embedding module. Specifically, instead of the single-feature representation, the multi-concept representation module projects each entity or relation to multiple vectors to capture the complex conceptual information hidden in them. The deep residual attention module simultaneously explores the inter- and intra-connection between entities and relations to enhance the entity and relation embeddings corresponding to the current contextual situation. Moreover, the interaction embedding module further weakens the noise and ambiguity to obtain the optimal and robust embeddings. We conduct the link prediction experiment to evaluate the proposed method on several standard datasets, and experimental results show that the proposed method outperforms existing state-of-the-art KGC methods.  © 2023 Association for Computing Machinery.",attention network; Knowledge graph completion; multi-concept representation,Complex networks; Knowledge graph; Attention network; Completion methods; Embeddings; Feature representation; Knowledge graph completion; Knowledge graphs; Learning methods; Low-dimensional spaces; Multi-concept representation; Multiple vectors; Embeddings
L2MM: Learning to Map Matching with Deep Models for Low-Quality GPS Trajectory Data,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152622105&doi=10.1145%2f3550486&partnerID=40&md5=b4331ddb20a3c6115c4cd354655f6e99,"Map matching is a fundamental research topic with the objective of aligning GPS trajectories to paths on the road network. However, existing models fail to achieve satisfactory performance for low-quality (i.e., noisy, low-frequency, and non-uniform) trajectory data. To this end, we propose a general and robust deep learning-based model, L2MM, to tackle these issues at all. First, high-quality representations of low-quality trajectories are learned by two representation enhancement methods, i.e., enhancement with high-frequency trajectories and enhancement with the data distribution. The former employs high-frequency trajectories to enhance the expressive capability of representations, while the latter regularizes the representation distribution over the latent space to improve the generalization ability of representations. Secondly, to embrace more heuristic clues, typical mobility patterns are recognized in the latent space and further incorporated into the map matching task. Finally, based on the available representations and patterns, a mapping from trajectories to corresponding paths is constructed through a joint optimization method. Extensive experiments are conducted based on a range of datasets, which demonstrate the superiority of L2MM and validate the significance of high-quality representations as well as mobility patterns.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; Map matching; mobility pattern; trajectory data; trajectory representation learning,Deep learning; Geographic information systems; Information use; Deep learning; Fundamental research; High frequency HF; High quality; Low qualities; Map matching; Mobility pattern; Research topics; Trajectories datum; Trajectory representation learning; Trajectories
A Survey on Deep Hashing Methods,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150247954&doi=10.1145%2f3532624&partnerID=40&md5=a757d928ca3df42bafe0b1aeaaecc10b,"Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods, and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing, and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion. © 2023 Association for Computing Machinery.",Approximate nearest neighbor search; deep supervised hashing; learning to hash; similarity preserving; top-k retrieval,Computational efficiency; Data mining; Deep learning; Digital storage; Hash functions; Query languages; Query processing; Semantics; Storage efficiency; 'current; Approximate Nearest Neighbor Search; Deep supervised hashing; Hashing algorithms; Hashing method; Learning to hash; Near neighbor searches; Similarity preserving; Storage efficiency; Top-k retrieval; Nearest neighbor search
Unsupervised Graph-Based Entity Resolution for Complex Entities,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144178499&doi=10.1145%2f3533016&partnerID=40&md5=7b368e8ae291763fcc8e247437ceab0c,"Entity resolution (ER) is the process of linking records that refer to the same entity. Traditionally, this process compares attribute values of records to calculate similarities and then classifies pairs of records as referring to the same entity or not based on these similarities. Recently developed graph-based ER approaches combine relationships between records with attribute similarities to improve linkage quality. Most of these approaches only consider databases containing basic entities that have static attribute values and static relationships, such as publications in bibliographic databases. In contrast, temporal record linkage addresses the problem where attribute values of entities can change over time. However, neither existing graph-based ER nor temporal record linkage can achieve high linkage quality on databases with complex entities, where an entity (such as a person) can change its attribute values over time while having different relationships with other entities at different points in time. In this article, we propose an unsupervised graph-based ER framework that is aimed at linking records of complex entities. Our framework provides five key contributions. First, we propagate positive evidence encountered when linking records to use in subsequent links by propagating attribute values that have changed. Second, we employ negative evidence by applying temporal and link constraints to restrict which candidate record pairs to consider for linking. Third, we leverage the ambiguity of attribute values to disambiguate similar records that, however, belong to different entities. Fourth, we adaptively exploit the structure of relationships to link records that have different relationships. Fifth, using graph measures, we refine matched clusters of records by removing likely wrong links between records. We conduct extensive experiments on seven real-world datasets from different domains showing that on average our unsupervised graph-based ER framework can improve precision by up to 25% and recall by up to 29% compared to several state-of-the-art ER techniques.  © 2023 Copyright held by the owner/author(s).",ambiguity; data cleaning; data linkage; dependency graph; Record linkage; temporal data,Data handling; Data mining; Database systems; Graphic methods; Ambiguity; Attribute values; Complex entities; Data cleaning; Data linkage; Dependency graphs; Entity resolutions; Graph-based; Record linkage; Temporal Data; Information services
Microblog Retrieval Based on Concept-Enhanced Pre-Training Model,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152622874&doi=10.1145%2f3552311&partnerID=40&md5=94c2746c0f5ef04b6b8c7cb9074e77af,"Despite substantial interest in applications of neural networks to information retrieval, neural ranking models have mostly been applied to conventional ad-hoc retrieval tasks over web pages and newswire articles. This article proposes a concept-enhanced pre-training model for microblog retrieval task, leveraging Semantic Matching Model (SMM) objective and Concept Correlation Model (CCM) objective. The proposed model is a novel neural ranking model specifically designed for ranking short-text microblog, which could merge the advantage of pre-training methodology for generating valid contextualized embedding with the superiority of the prior lexical knowledge (e.g., concept knowledge) for understanding short-text language semantic. We conduct experiments on widely used real-world datasets, and the experimental results demonstrate the efficiency of the proposed model, even compared with latest state-of-the-art neural-based models and pre-training based models.  © 2023 Association for Computing Machinery.",concept semantic knowledge; Microblog retrieval; pre-training mechanism; representation learning,Information retrieval; Semantics; Concept semantic knowledge; Micro-blog; Microblog retrieval; Modeling objectives; Pre-training; Pre-training mechanism; Ranking model; Representation learning; Semantics knowledge; Training model; Websites
Semi-Supervised Graph Pattern Matching and Rematching for Expert Community Location,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150223034&doi=10.1145%2f3532623&partnerID=40&md5=dba15805ba643443ace934f3c75b70d3,"Graph pattern matching (GPM) is widely used in social network analysis, such as expert finding, social group query, and social position detection. Technically, GPM is to find matched subgraphs that meet the requirements of pattern graphs in big social networks. In the application of expert community location, the nodes in the pattern graph and data graph represent expert entities, and the edges represent previous cooperations between them. However, the existing GPM methods focus on shortening the matching time and without considering the preference of the decision maker (DM), which makes it difficult for the DM to find ideal teams from numerous matches to complete the assigned task. In this article, as for the process of graph pattern matching and rematching, with a preferred expert set, i.e., the DM hopes that one or more experts in this set will appear in matched subgraphs, we propose a Dual Simulation-based Edge Sequencing-oriented Semi-Supervised GPM method (DsEs-ssGPM). In addition, considering a preferred expert set and a dispreferred expert set together, the DM hopes that experts in the dispreferred expert set will not appear in final matches, so we have the DsEs-ssGPM+ method. Technically, these DsEs-ssGPM methods conduct the matching process from the preferred expert set during dual simulation-based edge sequencing, and based on the edge sequence, these edges are searched recursively. Especially, as for the rematching process, when the preferred and/or the dispreferred expert sets change continuously, to process the GPM again is unnecessary and it is possible to revise the previous matched results partially with DsEs-ssGPM methods. Experiments on four large datasets demonstrate the effectiveness, efficiency and stability of our proposed DsEs-ssGPM methods, and the necessity of introducing an edge sequencing mechanism.  © 2023 Association for Computing Machinery.",expert location; Graph Pattern Matching; Graph Pattern Rematching; semi-supervised learning,Decision making; Large dataset; Pattern matching; Supervised learning; Decision makers; Expert location; Graph pattern matching; Graph pattern rematching; Graph patterns; Matching methods; Re-matching; Semi-supervised graphs; Semi-supervised learning; Subgraphs; Location
US-Rule: Discovering Utility-driven Sequential Rules,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150212301&doi=10.1145%2f3532613&partnerID=40&md5=0895ccf445ef6c988244d8e75bc39769,"Utility-driven mining is an important task in data science and has many applications in real life. High-utility sequential pattern mining (HUSPM) is one kind of utility-driven mining. It aims at discovering all sequential patterns with high utility. However, the existing algorithms of HUSPM can not provide a relatively accurate probability to deal with some scenarios for prediction or recommendation. High-utility sequential rule mining (HUSRM) is proposed to discover all sequential rules with high utility and high confidence. There is only one algorithm proposed for HUSRM, which is not efficient enough. In this article, we propose a faster algorithm called US-Rule, to efficiently mine high-utility sequential rules. It utilizes the rule estimated utility co-occurrence pruning strategy (REUCP) to avoid meaningless computations. Moreover, to improve its efficiency on dense and long sequence datasets, four tighter upper bounds (LEEU, REEU, LERSU, and RERSU) and corresponding pruning strategies (LEEUP, REEUP, LERSUP, and RERSUP) are designed. US-Rule also proposes the rule estimated utility recomputing pruning strategy (REURP) to deal with sparse datasets. Finally, a large number of experiments on different datasets compared to the state-of-the-art algorithm demonstrate that US-Rule can achieve better performance in terms of execution time, memory consumption, and scalability.  © 2023 Association for Computing Machinery.",Data mining; pattern mining; sequential rule; utility mining,Large dataset; Co-occurrence; Fast algorithms; High confidence; Pattern mining; Pruning strategy; Rule mining; Sequential patterns; Sequential rule; Sequential-pattern mining; Utility mining; Data mining
Individuality Meets Commonality: A Unified Graph Learning Framework for Multi-View Clustering,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150227586&doi=10.1145%2f3532612&partnerID=40&md5=e2faacc82c78b199c747813ec85dd3b4,"Multi-view clustering, which aims at boosting the clustering performance by leveraging the individual information and the common information of multi-view data, has gained extensive consideration in recent years. However, most existing multi-view clustering algorithms either focus on extracting the multi-view individuality or emphasize on exploring the multi-view commonality, neither of which can fully utilize the comprehensive information from multiple views. To this end, we propose a novel algorithm named View-specific and Consensus Graph Alignment (VCGA) for multi-view clustering, which simultaneously formulates the multi-view individuality and the multi-view commonality into a unified framework to effectively partition data points. To be specific, the VCGA model constructs the view-specific graphs and the shared graph from original multi-view data and hidden latent representation, respectively. Furthermore, the view-specific graphs of different views and the consensus graph are aligned into an informative target graph, which is employed as a crucial input to the standard spectral clustering method for clustering. Extensive experimental results on six benchmark datasets demonstrate the superiority of our method against other state-of-the-art clustering algorithms. © 2023 Association for Computing Machinery.",individuality and commonality; local structured graph learning; Multi-view clustering; self-representation,Individuality and commonality; Learning frameworks; Local structured graph learning; Multi-view clustering; Multi-view datum; Multi-views; Performance; Self-representation; Structured graphs; Clustering algorithms
Methods and Applications of Clusterwise Linear Regression: A Survey and Comparison,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152616807&doi=10.1145%2f3550074&partnerID=40&md5=18bae8a6c4f2f3614ea2c6be1e8bf295,"Clusterwise linear regression (CLR) is a well-known technique for approximating a data using more than one linear function. It is based on the combination of clustering and multiple linear regression methods. This article provides a comprehensive survey and comparative assessments of CLR including model formulations, description of algorithms, and their performance on small to large-scale synthetic and real-world datasets. Some applications of the CLR algorithms and possible future research directions are also discussed.  © 2023 Association for Computing Machinery.",cluster analysis; clusterwise linear regression; prediction methods; Regression analysis,Large dataset; Linear regression; Clusterings; Clusterwise linear regressions; Comparative assessment; Large-scales; Linear functions; Model formulation; Multiple linear regression method; Performance; Prediction methods; Real-world datasets; Cluster analysis
Integrating Global and Local Feature Selection for Multi-Label Learning,2023,ACM Transactions on Knowledge Discovery from Data,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150172772&doi=10.1145%2f3532190&partnerID=40&md5=8487d678fb61fe584f94ac4036fe37bd,"Multi-label learning deals with the problem where an instance is associated with multiple labels simultaneously. Multi-label data is often of high dimensionality and has many noisy, irrelevant, and redundant features. As an important machine learning task, multi-label feature selection has received considerable attention in recent years due to its promising performance in dealing with high-dimensional multi-label data. Existing multi-label feature selection methods typically select the global features which are shared by all instances in a dataset. However, these multi-label feature selection methods may be suboptimal since they do not consider the specific characteristics of instances. In this paper, we propose a novel algorithm that integrates Global and Local Feature Selection (GLFS) to exploit both the global features and a subset of discriminative features shared only locally by a subgroup of instances in a multi-label dataset. Specifically, GLFS employs linear regression and ĝ.,""2,1-norm on the regression parameters to achieve simultaneous global and local feature selection. Moreover, the proposed algorithm has an effective mechanism for utilizing label correlations to improve the feature selection. Experiments on real-world multi-label datasets show the superiority of GLFS over the state-of-the-art multi-label feature selection methods.  © 2023 Association for Computing Machinery.",label correlations; Local Feature Selection; Multi-label learning,Learning systems; Feature selection methods; Features selection; Global feature; High dimensionality; Label correlations; Local feature selections; Multi-label learning; Multi-labels; Multiple labels; Redundant features; Feature Selection
