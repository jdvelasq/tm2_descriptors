Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Editorial for principles of advanced discrete simulation,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954308720&doi=10.1145%2f2845147&partnerID=40&md5=d3432676e88f94b1680998d0d1f5bfae,[No abstract available],,
Parallel simulation and virtual-machine-based emulation of software-defined networks,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954312940&doi=10.1145%2f2834116&partnerID=40&md5=eb93a4d038891d827d7ec81b25abadb0,"The emerging software-defined networking (SDN) technology decouples the control plane from the data plane in a computer network with open and standardized interfaces, and hence opens up the network designers' options and ability to innovate. The wide adoption of SDN in industry has motivated the development of large-scale, high-fidelity testbeds for evaluation of systems that incorporate SDN. In this article, we develop a framework to support OpenFlow-based SDN simulation and distributed emulation, by leveraging our prior work on a hybrid network testbed with a parallel network simulator and a virtual-machine-based emulation system. We show how to exploit typical SDN controller behaviors to handle performance issues caused by the centralized controller in parallel discrete-event simulation. In particular, we develop an asynchronous synchronization algorithm for passive SDN controllers and design a two-level architecture for active SDN controllers. We evaluate the system performance, showing good scalability. Finally, we present a case study, using the testbed, to evaluate network verification applications in an SDN-based data center network. Copyright © 2015 ACM.",Discrete event; Distributed; Parallel; Software-defined networking; Virtual line,Controllers; Discrete event simulation; Telecommunication networks; Testbeds; Discrete events; Distributed; Parallel; Software-defined networkings; Virtual line; Computer software
Exact sampling of stationary and time-reversed queues,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954325407&doi=10.1145%2f2822892&partnerID=40&md5=a459a5666af597a7923aa3e61e99be8d,"We provide the first algorithm that, under minimal assumptions, allows simulation of the stationary waitingtime sequence of a single-server queue backward in time, jointly with the input processes of the queue (interarrival and service times). The single-server queue is useful in applications of Dominated Coupling from the Past (DCFTP), which is awell-known protocol for simulation without bias from steady-state distributions. Our algorithm terminates in finite time, assuming only finite mean of the interarrival and service times. To simulate the single-server queue in stationarity until the first idle period in finite expected termination time, we require the existence of finite variance. This requirement is also necessary for such idle time (which is a natural coalescence time in DCFTP applications) to have finite mean. Thus, in this sense, our algorithm is applicable under minimal assumptions. Copyright © 2015 ACM.",Simulation output analysis; Simulation theory; Types of simulation: Discrete event,Algorithms; Coalescence time; Coupling from the past; Discrete events; Simulation output analysis; Simulation theory; Single server queue; Steady-state distributions; Termination time; Queueing theory
How hard are steady-state queueing simulations?,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954322233&doi=10.1145%2f2749460&partnerID=40&md5=ec55c7d00b6e5ff9e01e09cf5cbd0882,"Some queueing systems require tremendously long simulation runlengths to obtain accurate estimators of certain steady-state performance measures when the servers are heavily utilized. However, this is not uniformly the case. We analyze a number of single-station Markovian queueing models, demonstrating that several steady-state performance measures can be accurately estimated with modest runlengths. Our analysis reinforces the meta result that if the queue is ""well dimensioned,"" then simulation runlengths will be modest. Queueing systems can be well dimensioned because customers abandon if they are forced to wait in line too long, or because the queue is operated in the ""quality- and efficiency-driven regime"" in which servers are heavily utilized but wait times are short. The results are based on computing or bounding the asymptotic variance and bias for several standard single-station queueing models and performance measures. Copyright © 2015 ACM.",Asymptotic variance; Diffusion approximations; Markovian queues,Asymptotic analysis; Queueing networks; Asymptotic variance; Diffusion approximations; Markovian queue; Performance measure; Quality- and efficiency-driven regimes; Queueing simulation; Queueing system; Steady state performance; Queueing theory
A methodology to model the execution of communication software for accurate network simulation,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939785816&doi=10.1145%2f2746233&partnerID=40&md5=1555f93cd4d4384db258b56e5e0521ad,"Network simulation is commonly used to evaluate the performance of distributed systems, but these approaches do not account for the performance impact that protocol execution on nodes has on performance, which can be significant. We provide a methodology to extract from real devices models of communication software execution that can be used to extend network simulators to improve their accuracy. The models are obtained by instrumenting the target devices to obtain the events necessary to describe software execution. We specify which events must be captured, how to capture them, and how to transform the event traces into models that can be used to extend network simulators. The obtained models are based on high-level abstractions that can be used to describe the execution of a wide range of communication software, and the design principles to extend network simulators are not restricted to any specific network simulator. The same model of communication software execution can be used without modification in all discrete event-based network simulators that are extended according to our principles. The models are represented in a human-readable format that is suitable for modification and can therefore be used to predict how software modifications impact performance. We evaluate our models with two proof-of-concept extensions of Ns-3 that execute the models of two modern smartphones: the Google Nexus One (GN1) and the Nokia N900. We measure the accuracy of our models by comparing results from real experiments with those from simulations with our models and analyze the simulation overhead of our approach. © 2015 ACM 1049-3301/2015/07-ART3 $15.00.",Methodology; Modeling; Network; Operating systems; Performance evaluation; Simulation,Computer operating systems; Computer software; Models; Networks (circuits); Simulators; Communication software; Distributed systems; High-level abstraction; Methodology; Network simulation; Performance evaluation; Simulation; Software modification; Discrete event simulation
Static analysis techniques for semiautomatic synthesis of message passing software skeletons,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935064278&doi=10.1145%2f2778888&partnerID=40&md5=eda615e0a9a85c0a880365cba4edad3b,"The design of high-performance computing architectures requires performance analysis of large-scale parallel applications to derive various parameters concerning hardware design and software development. The process of performance analysis and benchmarking an application can be done in several ways with varying degrees of fidelity. One of the most cost-effective ways is to do a coarse-grained study of large-scale parallel applications through the use of program skeletons. The concept of a ""program skeleton"" that we discuss in this article is an abstracted program that is derived from a larger program where source code that is determined to be irrelevant is removed for the purposes of the skeleton. In this work, we develop a semiautomatic approach for extracting program skeletons based on compiler program analysis.We demonstrate correctness of our skeleton extraction process by comparing details from communication traces, as well as show the performance speedup of using skeletons by running simulations in the SST/macro simulator. © 2015 ACM.",Dependency analysis; Program skeleton; Simulation; Source-to-source code generation; Static analysis,Application programs; Benchmarking; Computer architecture; Cost effectiveness; Message passing; Musculoskeletal system; Program compilers; Software design; Communication traces; Dependency analysis; High performance computing; Performance analysis; Program skeleton; Semiautomatic synthesis; Simulation; Source code generation; Static analysis
Efficient parallel discrete event simulation on cloud/virtual machine platforms,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937917791&doi=10.1145%2f2746232&partnerID=40&md5=77fba6ded64b172c571b4eec0fed7ec2,"Cloud and Virtual Machine (VM) technologies present new challenges with respect to performance and monetary cost in executing parallel discrete event simulation (PDES) applications. Due to the introduction of overall cost as a metric, the traditional use of the highest-end computing configuration is no longer the most obvious choice. Moreover, the unique runtime dynamics and configuration choices of Cloud and VM platforms introduce new design considerations and runtime characteristics specific to PDES over Cloud/VMs. Here, an empirical study is presented to help understand the dynamics, trends, and trade-offs in executing PDES on Cloud/VM platforms. Performance and cost measures obtained from multiple PDES applications executed on the Amazon EC2 Cloud and on a high-end VM host machine reveal new, counterintuitive VM-PDES dynamics and guidelines. One of the critical aspects uncovered is the fundamental mismatch in hypervisor scheduler policies designed for general Cloud workloads versus the virtual time ordering needed for PDES workloads. This insight is supported by experimental data revealing the gross deterioration in PDES performance traceable to VM scheduling policy. To overcome this fundamental problem, the design and implementation of a new deadlock-free scheduler algorithm are presented, optimized specifically for PDES applications on VMs. The scalability of our scheduler has been tested in up to 128 VMs multiplexed on 32 cores, showing significant improvement in the runtime relative to the default Cloud/VM scheduler. The observations, algorithmic design, and results are timely for emerging Cloud/VM-based installations, highlighting the need for PDES-specific support in high-performance discrete event simulations on Cloud/VM platforms. © 2015 ACM.",Global virtual time; Parallel discrete event simulation; Scheduler; Time warp; Virtual machines,Deterioration; Discrete event simulation; Dynamics; Economic and social effects; Network security; Scheduling; Simulation platform; Design and implementations; Design considerations; Global virtual time; Parallel discrete event simulations; Scheduler; Scheduling policies; Time Warp; Virtual machine technology; Virtual machine
Formalization of weak emergence in multiagent systems,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946070869&doi=10.1145%2f2815502&partnerID=40&md5=b3e91f144c0611093fa88489882c5e7b,"Emergence becomes a distinguishing system feature as system complexity grows with the number of components, interactions, and connectivities. Examples of emergent behaviors include the flocking of birds, traffic jams, and hubs in social networks, among others. Despite significant research interest in recent years, there is a lack of formal methods to understand, identify, and predict emergent behavior in multiagent systems. Existing approaches either require detailed prior knowledge about emergent behavior or are computationally infeasible. This article introduces a grammar-based approach to formalize and identify the existence and extent of emergence without the need for prior knowledge of emergent properties. Our approach is based on weak (basic) emergence that is both generated and autonomous from the underlying agents. We employ formal grammars to capture agent interactions in the forms of words written on a common tape. Our formalism captures agents of diverse types and open systems. We propose an automated approach for the identification of emergent behavior and show its benefits through theoretical and experimental analysis. We also propose a significant reduction of state-space explosion through the use of our proposed degree of interaction metrics. Our experiments using the boids model show the feasibility of our approach but also highlight future avenues of improvement. © 2015 ACM.",Complex system; Emergent behavior; Multiagent system; Simulation,Autonomous agents; Complex networks; Computational grammars; Formal methods; Large scale systems; Traffic congestion; Automated approach; Degree of interaction; Emergent behaviors; Experimental analysis; Grammar based approach; Number of components; Simulation; State-space explosion; Multi agent systems
Symbiotic network simulation and emulation,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935058807&doi=10.1145%2f2717308&partnerID=40&md5=616ac5cf201e66feab0a89014b0ca572,"A testbed capable of representing detailed operations of complex applications under diverse network conditions is invaluable for understanding the design and performance of new protocols and applications before their real deployment. We introduce a novel method that combines high-performance large-scale network simulation and high-fidelity network emulation, and thus enables real instances of network applications and protocols to run in real operating environments and be tested under simulated network settings. Using our approach, network simulation and emulation can form a symbiotic relationship, through which they are synchronized for an accurate representation of the network-scale traffic behavior. We introduce a model downscaling method along with an efficient queuing model and a traffic reproduction technique, which can significantly reduce the synchronization overhead and improve accuracy. We validate our approach with extensive experiments via simulation and with a real-system implementation. We also present a case study using our approach to evaluate a multipath data transport protocol. © 2015 ACM.",DDDAS; Network emulation; Network simulation; Online simulation; Symbiotic simulation,Cell proliferation; Queueing theory; DDDAS; Network emulation; Network simulation; Online simulation; Symbiotic simulation; Internet protocols
Adaptive resource provisioning mechanism in VEEs for improving performance of HLA-based simulations,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935116378&doi=10.1145%2f2717309&partnerID=40&md5=2e8aa5b34229f0279a6107f5946de140,"Parallel and distributed simulations (or High-Level Architecture (HLA)-based simulations) employing optimistic synchronization allow federates to advance simulation time freely at the risk of overoptimistic executions and execution rollbacks. As a result, the simulation performance may degrade significantly due to the simulation workload imbalance among federates. In this article, we investigate the execution of parallel and distributed simulations on Cloud and data centers with Virtual Execution Environments (VEEs). In order to speed up simulation execution, an Adaptive Resource Provisioning Mechanism in Virtual Execution Environments (ArmVee) is proposed. It is composed of a performance monitor and a resource manager. The former measures federate performance transparently to the simulation application. The latter distributes available resources among federates based on the measured federate performance. Federates with different simulation workloads are thus able to advance their simulation times with comparable speeds, thus are able to avoid wasting time and resources on overoptimistic executions and execution rollbacks. ArmVee is evaluated using a real-world simulation model with various simulation workload inputs and different parameter settings. The experimental results show that ArmVee is able to speed up the simulation execution significantly. In addition, it also greatly reduces memory usage and is scalable. © 2015 ACM.",Parallel and distributed simulations; Resource provisioning; Time synchronization; Virtual execution environments; Workload balance,Computer simulation; Parallel and distributed simulation; Resource provisioning; Time synchronization; Virtual execution environments; Workload balance; Computer applications
Automatic runtime adaptation for component-based simulation algorithms,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946016792&doi=10.1145%2f2821509&partnerID=40&md5=b898bf13d5e7d4c9fa52185b7aba9a1e,"The state and structure of a model may vary during a simulation and, thus, also its computational demands. Adapting simulation algorithms to these demands at runtime can therefore improve their performance. While this is a general and cross-cutting concern, only few simulation systems offer reusable support for this kind of runtime adaptation. We present a flexible and generic mechanism for the runtime adaptation of component-based simulation algorithms.Itencapsulates simulation algorithms applicable to agiven problem and employs reinforcement learning to explore the algorithms' performance during a simulation run. We evaluate our approach on a modeling formalism from computational biology and on a benchmark model defined in PDEVS, thereby investigating a broad range of options for improving its learning capabilities. © 2015 ACM.",Adaptive algorithms; Component-based simulation software; Reinforcement learning,Adaptive algorithms; Learning algorithms; Reinforcement learning; Computational biology; Computational demands; Cross-cutting concerns; Learning capabilities; Modeling formalisms; Runtime adaptation; Simulation algorithms; Simulation software; Computer software
On transience and recurrence in irreducible finite-state stochastic systems,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930176210&doi=10.1145%2f2699721&partnerID=40&md5=617028dde6e830e739c5dd8e06716490,"Long-run stochastic stability is a precondition for applying steady-state simulation output analysis methods to a discrete-event stochastic system, and is of interest in its own right. We focus on systems whose underlying stochastic process can be represented as a Generalized Semi-Markov Process (GSMP); a wide variety of stochastic systems fall within this framework. A fundamental stability requirement for an irreducible GSMP is that the states be ""recurrent"" in that the GSMP visits each state infinitely often with probability 1. We study recurrence properties of irreducible GSMPs with finite state space. Our focus is on the ""clocks"" that govern the occurrence of events, and we consider GSMPs in which zero, one, or at least two simultaneously active events can have clock-setting distributions that are ""heavy tailed"" in the sense that they have infinite mean. We establish positive recurrence, null recurrence, and, perhaps surprisingly, possible transience of states for these respective regimes. The transience result stands in strong contrast to Markovian or semi-Markovian GSMPs, where irreducibility and finiteness of the state space guarantee positive recurrence. © 2015 ACM 1049-3301/2015/05-ART25 15.00.",Discrete-event stochastic systems; Generalized semi-Markov processes; Hazard rate; Heavy tails; Recurrence; Regular variation; Stability,Clocks; Convergence of numerical methods; Markov processes; System stability; Discrete events; Generalized Semi-Markov Processes; Hazard rates; Heavy-tails; Recurrence; Regular variations; Stochastic systems
Guest editors introduction to special issue honoring Donald L. Iglehart,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946136889&doi=10.1145%2f2822375&partnerID=40&md5=d4ce4e099ca340a814ef710d71f69129,[No abstract available],,
The power of alternative Kolmogorov-Smirnov tests based on transformations of the data,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930149943&doi=10.1145%2f2699716&partnerID=40&md5=d49565db3b10ecf8894686848b7b80c8,"The Kolmogorov-Smirnov (KS) statistical test is commonly used to determine if data can be regarded as a sample from a sequence of independent and identically distributed (i.i.d.) random variables with specified continuous cumulative distribution function (cdf) F, but with small samples it can have insufficient power, that is, its probability of rejecting natural alternatives can be too low. However, in 1961, Durbin showed that the power of the KS test often can be increased, for a given significance level, by a well-chosen transformation of the data. Simulation experiments reported here show that the power can often be more consistently and substantially increased by a different transformation. We first transform the given sequence to a sequence of mean-1 exponential random variables, which is equivalent to a rate-1 Poisson process. We then apply the classical conditional-uniform transformation to convert the arrival times into i.i.d. random variables uniformly distributed on [0, 1]. And then, after those two preliminary steps, we apply the original Durbin transformation. Since these KS tests assume a fully specified cdf, we also investigate the consequence of having to estimate parameters of the cdf. © 2015 ACM 1049-3301/2015/05-ART24 15.00.",Data transformations; Hypothesis tests; Kolmogorov-Smirnov statistical test; Power,Computational complexity; Distribution functions; Random variables; Statistical tests; Cumulative distribution function; Data transformation; Hypothesis tests; I.i.d. random variables; Kolmogorov-Smirnov; Kolmogorov-Smirnov test; Power; Uniform transformations; Metadata
Resampled regenerative estimators,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930175673&doi=10.1145%2f2699718&partnerID=40&md5=c08c5bfdd26c8025f0de3814e196bffa,"We discuss some estimators for simulations of processes having multiple regenerative sequences. The estimators are obtained by resampling trajectories without and with replacement, which correspond to a type of U-statistic and a type of V-statistic, respectively. The U-statistic estimator turns out to be equivalent to the permuted regenerative estimator, which we previously proposed, but the V-statistic estimator is new. We compare analytically some properties of these estimators and the semiregenerative estimator. We show that when estimating the second moment of a cycle reward, the semiregenerative estimator has positive bias, which is strictly larger than the (positive) bias of the V-statistic estimator. The permuted estimator is unbiased. All of the estimators have the same asymptotic central limit behavior, with reduced asymptotic variance compared to the standard regenerative estimator. Some numerical results are included. © 2015 ACM 1049-3301/2015/05-ART23 15.00.",Regenerative method; Variance reduction,Computer applications; Computer simulation; Asymptotic variance; Central limit; Numerical results; Permuted estimators; Positive bias; Regenerative method; Second moments; Variance reductions; Statistics
Regenerative simulation for queueing networks with exponential or heavier tail arrival distributions,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930146243&doi=10.1145%2f2699717&partnerID=40&md5=bd9150b1001ded3aa583a123b28dfff7,"Multiclass open queueing networks find wide applications in communication, computer, and fabrication networks. Steady-state performance measures associated with these networks is often a topic of interset. Conceptually, under mild conditions, a sequence of regeneration times exists in multiclass networks, making them amenable to regenerative simulation for estimating steady-state performance measures. However, typically, identification of such a sequence in these networks is difficult. A well-known exception is when all interarrival times are exponentially distributed, where the instants corresponding to customer arrivals to an empty network constitute a sequence of regeneration times. In this article, we consider networks in which the interarrival times are generally distributed but have exponential or heavier tails. We show that these distributions can be decomposed into a mixture of sums of independent random variables such that at least one of the components is exponentially distributed. This allows an easily implementable embedded sequence of regeneration times in the underlying Markov process. We show that among all such interarrival time decompositions, the one with an exponential component that has the largest mean minimizes the asymptotic variance of the standard deviation estimator. We also show that under mild conditions on the network primitives, the regenerative mean and standard deviation estimators are consistent and satisfy a joint central limit theorem useful for constructing asymptotically valid confidence intervals. © 2015 ACM 1049-3301/2015/05-ART2215.00.",Multiclass queueing network; Optimal sequence of regeneration times; Regenerative process; Regenerative simulation,Markov processes; Queueing theory; Statistics; Applications in communications; Mean and standard deviations; Multi-class queueing networks; Optimal sequence; Regenerative process; Regenerative simulation; Steady state performance; Sums of independent random variables; Queueing networks
Model continuity in discrete event simulation: A framework for model-driven development of simulation models,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928521970&doi=10.1145%2f2699714&partnerID=40&md5=4564b497a7181ef0277dbb632e8f0fa6,"Most of the well-known modeling and simulation (M&S) methodologies state the importance of conceptual modeling in simulation studies, and they suggest the use of conceptual models during the simulation model development process. However, only a limited number of methodologies refers to how to move from a conceptual model to an executable simulation model. Besides, existing M&S methodologies do not typically provide a formal method for model transformations between the models in different stages of the development process. Hence, in the current M&S practice, model continuity is usually not fulfilled. In this article, a model-driven development framework for M&S is presented to bridge the gap between different stages of a simulation study and to obtain model continuity. The applicability of the framework is illustrated with a prototype modeling environment and a case study in the discrete event simulation domain. © 2015 ACM.",Conceptual modeling; Discrete event simulation; Metamodeling; Model transformation; Modeldriven development,Data mining; Formal methods; Conceptual model; Development process; Discrete-event simulations; Metamodeling; Model continuity; Model transformation; Model-driven-development; Modeling and simulation methodologies; Simulation model; Simulation studies; Discrete event simulation
AIR: Application-level interference resilience for PDES on multicore systems,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928554372&doi=10.1145%2f2701420&partnerID=40&md5=b4ee3cc07265823173c073a242393a99,"Parallel discrete event simulation (PDES) harnesses parallel processing to improve the performance and capacity of simulation, supporting bigger and more detailed models simulated for more scenarios. The presence of interference from other users can lead to dramatic slowdown in the performance of the simulation. Interference is typically managed using operating system scheduling support (e.g., gang scheduling), a heavyweight approach with some drawbacks. We propose an application-level approach to interference resilience through alternative simulation scheduling and mapping algorithms. More precisely, the most resilient simulators allow dynamic mapping of simulation event execution to processing resources (a work pool model). However, this model has significant scheduling overhead and poor cache locality. Thus, we investigate using application-level interference mitigation where the application detects the presence of interference and reacts by changing the thread task allocation. Specifically, we propose a locality-aware adaptive dynamic mapping (LADM) algorithm that adjusts the number of active threads on the fly by detecting the presence of interference. LADM avoids having the application stall when threads are inactive due to context switching. We investigate different mechanisms for monitoring the level of interference and different approaches for remapping tasks. We show that LADM can substantially reduce the impact of interference while maintaining memory locality. © 2015 ACM.",Application adaptation; Interference; PDES; Proportional slowdown,Conformal mapping; Scheduling; Wave interference; Application adaptation; Application level approach; Interference mitigation; Interference resiliences; Parallel discrete event simulations; PDES; Proportional slowdowns; Simulation scheduling; Discrete event simulation
"Calibration, validation, and prediction in random simulation models:Gaussian process metamodels and a bayesian integrated solution",2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928521567&doi=10.1145%2f2699713&partnerID=40&md5=af6a22b6eae7d40161ec585180c8603b,"Model calibration and validation are important processes in the development of stochastic computer models of real complex systems. This article introduces an integrated approach for model calibration, validation, and prediction based on Gaussian process metamodels and a Bayesian approach. Within this integrated approach, a sequential approach is further proposed for stochastic computer model calibration. Several design criteria for this sequential stage are proposed and studied, including an entropy-based criterion and one based on minimizing prediction error. To further use the data resources to improve the performance of both calibration and prediction, an adaptive procedure that combines these criteria is introduced to balance the resource allocation between the calibration and prediction. The accuracy and efficiency of the proposed sequential calibration approach and the integrated approach are illustrated with several numerical examples.Categories and Subject Descriptors: G.3 [Mathematics of Computing]: Probability and Statistics-Probabilistic algorithms (including Monte Carlo)-Experimental design; I.6 [Computing Methodologies]: Simulation and Modeling-Model validation and analysis-Simulation output analysis ©c 2015 ACM.",Computer model calibration; Gaussian process; Sequential experimental design; Stochastic computer simulation,Bayesian networks; Forecasting; Gaussian distribution; Gaussian noise (electronic); Integrated control; Monte Carlo methods; Statistics; Stochastic systems; Computer modeling; Entropy-based criterion; Gaussian Processes; Mathematics of computing; Model calibration and validation; Probability and statistics; Sequential experimental design; Simulation output analysis; Stochastic models
TADSim: Discrete event-based performance prediction for temperature-accelerated dynamics,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928566070&doi=10.1145%2f2699715&partnerID=40&md5=c8e8aa8b81c93a0f26bb1c230b124ca6,"Next-generation high-performance computing will require more scalable and flexible performance prediction tools to evaluate software-hardware co-design choices relevant to scientific applications and hardware architectures. We present a new class of tools called application simulators-parameterized fast-running proxies of large-scale scientific applications using parallel discrete event simulation. Parameterized choices for the algorithmic method and hardware options provide a rich space for design exploration and allow us to quickly find well-performing software-hardware combinations. We demonstrate our approach with a TADSim simulator that models the temperature-accelerated dynamics (TAD) method, an algorithmically complex and parameter-rich member of the accelerated molecular dynamics (AMD) family of molecular dynamics methods. The essence of the TAD application is captured without the computational expense and resource usage of the full code. We accomplish this by identifying the time-intensive elements, quantifying algorithm steps in terms of those elements, abstracting them out, and replacing them by the passage of time. We use TADSim to quickly characterize the runtime performance and algorithmic behavior for the otherwise long-running simulation code. We extend TADSim to model algorithm extensions, such as speculative spawning of the compute-bound stages, and predict performance improvements without having to implement such a method. Validation against the actual TAD code shows close agreement for the evolution of an example physical system, a silver surface. Focused parameter scans have allowed us to study algorithm parameter choices over far more scenarios than would be possible with the actual simulation. This has led to interesting performance-related insights and suggested extensions.",Accelerated molecular dynamics; Temperature-accelerated dynamics,Application programs; Discrete event simulation; Forecasting; Molecular dynamics; Accelerated molecular dynamics; Hardware architecture; High performance computing; Molecular dynamics methods; Parallel discrete event simulations; Performance prediction; Scientific applications; Temperature accelerated dynamics; Hardware-software codesign
Overlapping batches for the assessment of solution quality in stochastic programs,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928565542&doi=10.1145%2f2701421&partnerID=40&md5=cb6885a2600f348038daf09845ad16f6,"Overlapping Batch Means (OBM) has long been used in simulation as a method of reusing data to generate variance estimators with asymptotically lower variance. In this article, we apply the OBM method to stochastic programming by formulating a variant of the multiple replications procedure used for assessing solution quality. We give conditions under which the resulting optimality gap point estimators are strongly consistent, the optimality gap interval estimators are asymptotically valid, and the OBM variance estimators for optimality gap have asymptotically lower variances relative to their nonoverlapping counterparts [Meketon and Schmeiser 1984; Welch 1987]. We investigate computational efficiency, a combined measure of variance and computation time, providing guidelines on the degree of overlap. Numerical experiments on several test problems are presented, examining the small-sample behavior and the empirical computational efficiency of the overlapping batches method in this context. © 2015 ACM.",Optimality gap estimation; Overlapping batch means; Sample average approximation (SAA); Stochastic programming,Efficiency; Numerical methods; Stochastic programming; Stochastic systems; Batch means; Computation time; Interval estimators; Numerical experiments; Optimality; Sample average approximation; Solution quality; Variance estimators; Computational efficiency
Statistical debugging for simulations,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928576056&doi=10.1145%2f2699722&partnerID=40&md5=d1b6ff041322b2e6b59375fe7118f60f,"Predictions from simulations have entered the mainstream of public policy and decision-making practices. Unfortunately, methods for gaining insight into faulty simulations outputs have not kept pace. Ideally, an insight gathering method would automatically identify the cause of a faulty output and explain to the simulation developer how to correct it. In the field of software engineering, this challenge has been addressed for general-purpose software through statistical debuggers.We present two research contributions, elastic predicates and many-valued labeling functions, that enable debuggers designed for general-purpose software to become more effective for simulations employing random variates and continuous numbers. Elastic predicates address deficiencies of existing debuggers related to continuous numbers, whereas manyvalued labeling functions support the use of random variates.When used in combinations, these contributions allow a simulation developer tasked with localizing the program statement causing the faulty simulation output to examine 40% fewer statements than the leading alternatives. Our evaluation shows that elastic predicates and many-valued labeling functions maintain their ability to reduce the number of program statements that need to be examined under the imperfect conditions that developers experience in practice. Additional Key Words and Phrases: Modeling and simulation, automated debugging, fault localization ©2015 ACM.",,Decision making; Software engineering; Automated debugging; Decision-making practices; Fault localization; General purpose software; Model and simulation; Simulation developers; Simulation outputs; Statistical debugging; Program debugging
Model reconstruction for moment-based stochastic chemical kinetics,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930660368&doi=10.1145%2f2699712&partnerID=40&md5=c6feec97eb8abd2d11daea4897b22874,"Based on the theory of stochastic chemical kinetics, the inherent randomness of biochemical reaction networks can be described by discrete-state continuous-time Markov chains. However, the analysis of such processes is computationally expensive and sophisticated numerical methods are required. Here, we propose an analysis framework in which we integrate a number of moments of the process instead of the state probabilities. This results in a very efficient simulation of the time evolution of the process. To regain the state probabilities from the moment representation, we combine the fast moment-based simulation with a maximum entropy approach for the reconstruction of the underlying probability distribution.We investigate the usefulness of this combined approach in the setting of stochastic chemical kinetics and present numerical results for three reaction networks showing its efficiency and accuracy. Besides a simple dimerization system, we study a bistable switch system and a multiattractor network with complex dynamics. © 2015.",Chemical master equation; maximum entropy; moment closure; stochastic chemical kinetics,Complex networks; Continuous time systems; Entropy; Kinetics; Markov processes; Maximum entropy methods; Numerical methods; Probability; Reaction kinetics; Stochastic models; Switching circuits; Biochemical reaction network; Chemical master equation; Continuous time Markov chain; Efficient simulation; Maximum-entropy approaches; Model reconstruction; Moment closure; Stochastic chemical kinetics; Stochastic systems
Hybrid simulations of heterogeneous biochemical models in SBML,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930676874&doi=10.1145%2f2742545&partnerID=40&md5=adc65ece4dbe16d4fca4b1efd674a965,"Models of biochemical systems presented as a set of formal reaction rules can be interpreted in different formalisms, most notably as either deterministic Ordinary Differential Equations, stochastic continuoustime Markov Chains, Petri nets, or Boolean transition systems. While the formal composition of reaction systems can be syntactically defined as the (multiset) union of the reactions, the composition and simulation of models in different formalisms remain a largely open issue. In this article, we show that the combination of reaction rules and events, as already present in SBML, can be used in a nonstandard way to define stochastic and Boolean simulators and give meaning to the hybrid composition and simulation of heterogeneousmodels of biochemical processes. In particular, we show how two SBML reaction models can be composed into one hybrid continuous-stochastic SBML model through a high-level interface for composing reaction models and specifying their interpretation. Furthermore, we describe dynamic strategies for automatically partitioning reactions with stochastic or continuous interpretations according to dynamic criteria. The performances are then compared to static partitioning. The proposed approach is illustrated and evaluated on several examples, including the reconstructions of the hybrid model of the mammalian cell cycle regulation of Singhania et al. as the composition of a Boolean model of cell cycle phase transitions with a continuous model of cyclin activation, the hybrid stochastic-continuous models of bacteriophage T7 infection of Alfonsi et al., and the bacteriophage λ model of Goutsias, showing the gain in both accuracy and simulation time of the dnamic partitioning strategy. ©2015.",computational systems biology; Hybrid simulation; stochastic simulation; synthetic biology,Bacteriophages; Biochemistry; Differential equations; Mammals; Markov processes; Ordinary differential equations; Petri nets; Stochastic systems; Computational Systems Biology; Continuous time Markov chain; High level interface; Hybrid simulation; Partitioning strategies; Simulation of models; Stochastic simulations; Synthetic biology; Stochastic models
Guest editors' introduction to special issue on computational methods in systems biology,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930638476&doi=10.1145%2f2745799&partnerID=40&md5=1acf4ea46ba8597615a08e91eff8d46f,[No abstract available],,
Spatial-temporal modelling and analysis of bacterial colonies with phase variable genes,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930669139&doi=10.1145%2f2742546&partnerID=40&md5=a808701d95dd3013ae98a7059b1ca428,"This article defines a novel spatial-temporal modelling and analysis methodology applied to a systems biology case study, namely phase variation patterning in bacterial colony growth. We employ coloured stochastic Petri nets to construct the model and run stochastic simulations to record the development of the circular colonies over time and space. The simulation output is visualised in 2D, and sector-like patterns are automatically detected and analysed. Space is modelled using 2.5 dimensions considering both a rectangular and circular geometry, and the effects of imposing different geometries on space are measured. We close by outlining an interpretation of the Petri net model in terms of finite difference approximations of partial differential equations (PDEs). One result is the derivation of the ""best"" nine-point diffusion model. Our multidimensional modelling and analysis approach is a precursor to potential future work on more complex multiscale modelling. 2015 Copyright is held by the owner/author(s).",Algorithms; Experimentation; Theory,Algorithms; Differential equations; Finite difference method; Geometry; Petri nets; Stochastic systems; Coloured stochastic petri nets; Experimentation; Finite difference approximations; Modelling and analysis; Multidimensional modelling; Partial Differential Equations (PDEs); Stochastic simulations; Theory; Stochastic models
Computing cumulative rewards using fast adaptive uniformization,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923684591&doi=10.1145%2f2688907&partnerID=40&md5=d57582a46bd4fdbb989fef33e1234018,"The computation of transient probabilities for continuous-time Markov chains often employs uniformization, also known as the Jensen method. The fast adaptive uniformization method introduced by Mateescu et al. approximates the probability by neglecting insignificant states and has proven to be effective for quantitative analysis of stochastic models arising in chemical and biological applications. However, this method has only been formulated for the analysis of properties at a given point of time t. In this article, we extend fast adaptive uniformization to handle expected reward properties that reason about the model behavior until time t, for example, the expected number of chemical reactions that have occurred until t. To show the feasibility of the approach, we integrate the method into the probabilistic model checker PRISM and apply it to a range of biological models. The performance of the method is enhanced by the use of interval splitting. We compare our implementation to standard uniformization implemented in PRISM and to fast adaptive uniformization without support for cumulative rewards implemented in MARCIE, demonstrating superior performance. © 2015 ACM.",DNA computation; DNA strand displacement; Fast adaptive uniformisation; Markov models; Probabilistic model checking; Quantitative model checking,Chemical analysis; Continuous time systems; Markov chains; Model checking; Prisms; Stochastic systems; DNA computation; Dna strand displacements; Fast adaptive uniformisation; Markov model; Probabilistic model checking; Quantitative model checking; Stochastic models
Moment-based methods for parameter inference and experiment design for stochastic biochemical reaction networks,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923668303&doi=10.1145%2f2688906&partnerID=40&md5=371c4cb8c4ea89d6e4d65d5494de0134,"Continuous-time Markov chains are commonly used in practice for modeling biochemical reaction networks in which the inherent randomness of themolecular interactions cannot be ignored. This has motivated recent research effort into methods for parameter inference and experiment design for such models. The major difficulty is that such methods usually require one to iteratively solve the chemical master equation that governs the time evolution of the probability distribution of the system. This, however, is rarely possible, and even approximation techniques remain limited to relatively small and simple systems. An alternative explored in this article is to base methods on only some low-order moments of the entire probability distribution. We summarize the theory behind such moment-based methods for parameter inference and experiment design and provide new case studies where we investigate their performance. © 2015 ACM.",Continuous-timeMarkov chains; Experiment design; Fisher information; Moment equations; Parameter inference,Continuous time systems; Fisher information matrix; Markov chains; Probability distributions; Stochastic systems; Approximation techniques; Biochemical reaction network; Chemical master equation; Continuous time Markov chain; Experiment design; Fisher information; Moment equations; Parameter inference; Iterative methods
Simulating organogenesis: Algorithms for the image-based determination of displacement fields,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923694183&doi=10.1145%2f2688908&partnerID=40&md5=c154bff2ccec834ac57cc33cd6d501c0,"Recent advances in imaging technology now provide us with 3D images of developing organs. These can be used to extract 3D geometries for simulations of organ development. To solve models on growing domains, the displacement fields between consecutive image frames need to be determined. Here we develop and evaluate different landmark-free algorithms for the determination of such displacement fields from image data. In particular, we examine minimal distance, normal distance, diffusion-based, and uniform mapping algorithms and test these algorithms with both synthetic and real data in 2D and 3D. We conclude that in most cases, the normal distance algorithm is the method of choice and wherever it fails, diffusion-based mapping provides a good alternative. © 2015 ACM.",Curve mapping; Displacement field; Image-based; Modelling,Imaging techniques; Models; Consecutive images; Displacement field; Image-based; Imaging technology; Mapping algorithms; Minimal distance; Organ development; Synthetic and real data; Conformal mapping
Constrained community-based gene regulatory network inference,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923677683&doi=10.1145%2f2688909&partnerID=40&md5=c5d39c4479a9250120dc2034810936d9,"The problem of gene regulatory network inference is a major concern of systems biology. In recent years, a novel methodology has gained momentum, called community network approach. Community networks integrate predictions from individual methods in a ""metapredictor,"" in order to compose the advantages of different methods and soften individual limitations. This article proposes a novel methodology to integrate prediction ensembles using constraint programming, a declarative modeling and problem solving paradigm. Constraint programming naturally allows the modeling of dependencies among components of the problem as constraints, facilitating the integration and use of different forms of knowledge. The new paradigm, referred to as constrained community network, uses constraints to capture properties of the regulatory networks (e.g., topological properties) and to guide the integration of knowledge derived from different families of network predictions. The article experimentally shows the potential of this approach: The addition of biological constraints can offer significant improvements in prediction accuracy. © 2015 ACM.",Bioinformatics; Constraint programming; Gene regulatory networks,Bioinformatics; Computer programming; Constraint theory; Forecasting; Topology; Biological constraints; Constraint programming; Declarative models; Gene regulatory networks; Integration of knowledge; Prediction accuracy; Regulatory network; Topological properties; Genes
A cross-entropy scheme for mixtures,2015,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925367879&doi=10.1145%2f2685030&partnerID=40&md5=75c06924df8d0237a676830cc3715020,"We discuss how to generalize the classic cross-entropy method in the case where a family of mixture distributions, such as the mixture of multiple Gaussian modes, is used as an importance sampling distribution. A new iterative cross-entropy scheme, based on the idea of the EM method, is proposed to overcome the challenge of deciding the optimal weights for each mode in the mixture. Detailed studies of this new algorithm and its applications to the estimation of rainbow option prices are presented to demonstrate the efficiency of the scheme. © 2015 ACM 1049-3301/2015/01-ART6 $15.00.",Algorithms; Performance; Stochastic analysis,Algorithms; Importance sampling; Mixtures; Stochastic systems; Cross-entropy method; Iterative cross entropies; ITS applications; Mixture distributions; Performance; Rainbow options; Sampling distribution; Stochastic analysis; Iterative methods
Screening for dispersion effects by sequential bifurcation,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920762841&doi=10.1145%2f2651364&partnerID=40&md5=84f586b72ed5ff67059c5905985c3479,"The mean of the output of interest obtained from a run of a computer simulation model of a system or process often depends on many factors; many times, however, only a few of these factors are important. Sequential bifurcation is a method that has been considered by several authors for identifying these important factors using as few runs of the simulation model as possible. In this article, we propose a new sequential bifurcation procedure whose steps use a key stopping rule that can be calculated explicitly, something not available in the best-methods previously considered. Moreover, we show how this stopping rule can also be easily modified to efficiently identify those factors that are important in influencing the variability rather than the mean of the output. In empirical studies, the new method performs better than previously published fully sequential bifurcation methods in terms of achieving the prescribed Type I error. It also achieves higher power for detecting moderately large effects using fewer replications than earlier methods. To achieve this control for midrange effects, the new method sometimes requires more replications than other methods in the case where there are many very large effects. © 2014 ACM.",Controlled sequential bifurcation; Fixed-width confidence intervals; Location effects; Multiplicative model; Multiplicative variance model; Simulation; Stopping rules; Variance effects,Confidence interval; Location effects; Multiplicative model; Sequential bifurcation; Simulation; Stopping rule; Variance effects; Variance models; Bifurcation (mathematics)
Balanced and approximate zero-variance recursive estimators for the network reliability problem,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911481392&doi=10.1145%2f2674914&partnerID=40&md5=9502ab5c2d8e488ded756f63805bdd10,"Exact evaluation of static network reliability parameters belongs to the NP-hard family, and Monte Carlo simulation is therefore a relevant tool to provide their estimations. The first goal of this work is to review a Recursive Variance Reduction (RVR) estimator, which approaches the unreliability by recursively reducing the graph from the random choice of the first working link on selected cuts.We show that themethod does not verify the bounded relative error (BRE) property as reliability of individual links goes to one-that is, that the estimator is not robust in general to high reliability of links. We then propose to use the decomposition ideas of the RVR estimator in conjunction with the importance sampling technique. Two new estimators are presented: the first one-the Balanced Recursive Decomposition estimator-chooses the first working link on cuts uniformly, whereas the second-the Zero-Variance Approximation Recursive Decomposition estimator-tries to mimic the estimator with variance zero for this technique. We show that in both cases the BRE property is verified and, moreover, that a vanishing relative error (VRE) property can be obtained for the Zero-Variance Approximation RVR under specific sufficient conditions. A numerical illustration of the power of the methods is provided on several benchmark networks. © 2014 ACM 1049-3301/2014/07-ART1 $15.00.",Approximate zero variance; Importance sampling; Monte Carlo simulation; Network reliability; Variance reduction,Estimation; Importance sampling; Intelligent systems; Metal drawing; Numerical methods; Reliability; Approximate zeros; Benchmark networks; Bounded relative error; High reliability; Network reliability; Recursive decomposition; Recursive estimators; Variance reductions; Monte Carlo methods
Site-specific models for realistic wireless network simulation,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911462929&doi=10.1145%2f2661630&partnerID=40&md5=653d9580dd61be3aba783a2f075e8186,"The utility of simulation-based performance evaluation for wireless networking has been under scrutiny as the community relies increasingly on testbed-based performance evaluations. While testbeds are invaluable tools for realistic network and protocol evaluation, these results are generally obtained after cumbersome system implementation and debugging. On the other hand, realistic simulation models can reduce the time and effort for concept testing of ideas. To this end, we develop BOWLsim PHY layer models-propagation, frame detection, and frame error models-based on extensive measurements in the Berlin Open Wireless Lab indoor and outdoor testbeds. Our models are integrated into the ns-3 simulator. We run an extensive measurement and simulation study, which illustrates that BOWLsim models represent network conditions at the physical (PHY) layer and transport layer accurately. © 2014 ACM 1049-3301/2014/07-ART1 $15.00.",PHY layer; Site specific; Wireless networks,Physical layer; Program debugging; Testbeds; Measurement and simulation; PHY layer; Protocol evaluation; Realistic simulation; Site-specific; System implementation; Wireless network simulation; Wireless networking; Wireless networks
Cluster-based spatiotemporal background traffic generation for network simulation,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911484181&doi=10.1145%2f2667222&partnerID=40&md5=c4a4536e6d94b26cde16e3a555f17b19,"To reduce the computational complexity of large-scale network simulation, one needs to distinguish foreground traffic generated by the target applications one intends to study from background traffic that represents the bulk of the network traffic generated by other applications. Background traffic competes with foreground traffic for network resources and consequently plays an important role in determining the behavior of network applications. Existing background traffic models either operate only at coarse time granularity or focus only on individual links. There is little insight on how to meaningfully apply realistic background traffic over the entire network. In this article, we propose a method for generating background traffic with spatial and temporal characteristics observed from real traffic traces.We apply data clustering techniques to describe the behavior of end hosts as a function of multidimensional attributes and group them into distinct classes, and then map the classes to simulated routers so that we can generate traffic in accordance with the cluster-level statistics. The proposed traffic generator makes no assumption on the target network topology. It is also capable of scaling the generated traffic so that the traffic intensity can be varied accordingly in order to test applications under different and yet realistic network conditions. Experiments show that our method is able to generate traffic that maintains the same spatial and temporal characteristics as in the observed traffic traces. © 2014 ACM 1049-3301/2014/07-ART1 $15.00.",Background traffic model; Network simulation; Network traffic clustering; Spatiotemporal network traffic characteristics,Clustering algorithms; Background traffic; Large-scale network simulation; Multidimensional attributes; Network applications; Network simulation; Network traffic; Spatiotemporal networks; Temporal characteristics; Cluster analysis
Stochastically constrained ranking and selection via score,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911483950&doi=10.1145%2f2630066&partnerID=40&md5=f4d1e67d59ec0fa61d67eab594a25a3c,"Consider the context of constrained Simulation Optimization (SO); that is, optimization problems where the objective and constraint functions are known through dependent Monte Carlo estimators. For solving such problems on large finite spaces, we provide an easily implemented sampling framework called SCORE (Sampling Criteria for Optimization using Rate Estimators) that approximates the optimal simulation budget allocation. We develop a general theory, but, like much of the existing literature on ranking and selection, our focus is on SO problems where the distribution of the simulation observations is Gaussian. We first characterize the nature of the optimal simulation budget as a bi-level optimization problem. We then show that under a certain asymptotic limit, the solution to the bi-level optimization problem becomes surprisingly tractable and is expressed through a single intuitive measure, the score.We provide an iterative SO algorithm that repeatedly estimates the score and determines how the available simulation budget should be expended across contending systems. Numerical experience with the algorithm resulting from the proposed sampling approximation is very encouraging-in numerous examples of constrained SO problems having 1,000 to 10,000 systems, the optimal allocation is identified to negligible error within a few seconds to 1 minute on a typical laptop computer. Corresponding times to solve the full bi-level optimization problem range from tens of minutes to several hours. © 2014 ACM 1049-3301/2014/07-ART1 $15.00.",Constrained simulation optimization; Ranking and selection,Approximation algorithms; Budget control; Constrained optimization; Iterative methods; Laptop computers; Monte Carlo methods; Optimization; Asymptotic limits; Bi-level optimization; Constrained simulations; Constraint functions; Monte Carlo Estimators; Optimal allocation; Optimization problems; Ranking and selection; Problem solving
Gradient extrapolated stochastic kriging,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911483291&doi=10.1145%2f2658995&partnerID=40&md5=d870139738fb62cc89f1452370d9569c,"We introduce an approach for enhancing stochastic kriging in the setting where additional direct gradient information is available (e.g., provided by techniques such as perturbation analysis or the likelihood ratio method). The new approach, called gradient extrapolated stochastic kriging (GESK), incorporates direct gradient estimates by extrapolating additional responses. For two simplified settings, we show that GESK reduces meansquared error (MSE) compared to stochastic kriging under certain conditions on step sizes. Since extrapolation step sizes are crucial to the performance of the GESK model, we propose two different approaches to determine the step sizes: maximizing penalized likelihood and minimizing integrated mean squared error. Numerical experiments are conducted to illustrate the performance of the GESK model and to compare it with alternative approaches. © 2014 ACM 1049-3301/2014/08-ART21 $15.00.",Response surface; Simulation; Stochastic gradients; Stochastic kriging,Extrapolation; Interpolation; Mean square error; Perturbation techniques; Gradient informations; Kriging; Likelihood ratio method; Numerical experiments; Perturbation Analysis; Response surface; Simulation; Stochastic gradient; Stochastic systems
Guest editors' introduction to special issue on the 2012 NSF workshop,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911487557&doi=10.1145%2f2676546&partnerID=40&md5=53b28a80ed5b15884d8277983ca28b40,[No abstract available],,
Model-based annealing random search with stochastic averaging,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911497374&doi=10.1145%2f2641565&partnerID=40&md5=744ed518835ddec6db6caadeac3275f8,"The model-based methods have recently found widespread applications in solving hard nondifferentiable optimization problems. These algorithms are population-based and typically require hundreds of candidate solutions to be sampled at each iteration. In addition, recent convergence analysis of these algorithms also stipulates a sample size that increases polynomially with the number of iterations. In this article, we aim to improve the efficiency of model-based algorithms by reducing the number of candidate solutions generated per iteration. This is carried out through embedding a stochastic averaging procedure within these methods to make more efficient use of the past sampling information. This procedure not only can potentially reduce the number of function evaluations needed to obtain high-quality solutions, but also makes the underlying algorithms more amenable for parallel computation. The detailed implementation of our approach is demonstrated through an exemplary algorithm instantiation called Model-based Annealing Random Search with Stochastic Averaging (MARS-SA), which maintains the per iteration sample size at a small constant value. We establish the global convergence property of MARS-SA and provide numerical examples to illustrate its performance. © 2014 ACM 1049-3301/2014/08-ART21 $15.00.",Global optimization; Model-based algorithms; Stochastic approximation,Approximation algorithms; Global optimization; Iterative methods; Quality control; Stochastic systems; Convergence analysis; Global convergence properties; High-quality solutions; Model-based algorithms; Non-differentiable optimization; Number of iterations; Parallel Computation; Stochastic approximations; Stochastic models
Drug resistance or re-emergence? Simulating equine parasites,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911482711&doi=10.1145%2f2627736&partnerID=40&md5=ad39b58f5bc601250e1ae7e0041086c6,"Emerging drug resistance in parasitology and its impact on human and animal health are of serious concern. Attempts by the parasitology community to address this issue led to the introduction of so-called selective therapy where a proportion of the population is left untreated. This has led to re-emergence of parasites that have heretofore been controlled. Using stochastic simulations, this article explores the tradeoff between drug resistance and re-emergence. More importantly, the article identifies the importance of the parasite fitness parameter vector and its role in drug resistance. Suggestions for further biological work and statistical analyses are also provided. © 2014 ACM 1049-3301/2014/06-ART20.",Drug resistance; Equine parasite; Re-emergence; Selective therapy; Statisticalmodel; Stochastic simulation,Stochastic models; Veterinary medicine; Drug resistance; Parasite; Selective therapy; Statisticalmodel; Stochastic simulations; Stochastic systems
Confidence intervals for quantiles using sectioning when applying variance-reduction techniques,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911500290&doi=10.1145%2f2558328&partnerID=40&md5=3a3b8e868940a72c5f2a0370be12da87,"We develop confidence intervals (CIs) for quantiles when applying variance-reduction techniques (VRTs) and sectioning. Similar to batching, sectioning partitions the independent and identically distributed (i.i.d.) outputs into nonoverlapping batches and computes a quantile estimator from each batch. But rather than centering the CI at the average of the quantile estimators across the batches, as in batching, sectioning centers the CI at the overall quantile estimator based on all the outputs. A similar modification is made to the sample variance, which is used to determine the width of the CI. We establish the asymptotic validity of the sectioning CI for importance sampling and control variates, and the proofs rely on first showing that the corresponding quantile estimators satisfy a Bahadur representation, which we have done in prior work. Here, we present some numerical results. © 2014 ACM 1049-3301/2014/03-ART19.",Control variates; Importance sampling; Quantile; Value-at-risk; Variance reduction,Monte Carlo methods; Value engineering; Bahadur representation; Confidence interval; Control variates; Quantile; Quantile estimators; Value at Risk; Variance reduction techniques; Variance reductions; Importance sampling
Monte carlo methods for value-at-risk and conditional value-at-risk: A review,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911481899&doi=10.1145%2f2661631&partnerID=40&md5=cb031c80d33c82645415a73b59b7102e,"Value-at-risk (VaR) and conditional value-at-risk (CVaR) are two widely used risk measures of large losses and are employed in the financial industry for risk management purposes. In practice, loss distributions typically do not have closed-form expressions, but they can often be simulated (i.e., random observations of the loss distribution may be obtained by running a computer program). Therefore, Monte Carlo methods that design simulation experiments and utilize simulated observations are often employed in estimation, sensitivity analysis, and optimization of VaRs and CVaRs. In this article, we review some of the recent developments in these methods, provide a unified framework to understand them, and discuss their applications in financial risk management. © 2014 ACM 1049-3301/2014/08-ART21 $15.00.",Conditional value-at-risk; Financial risk management; Value-at-risk,Finance; Monte Carlo methods; Risk assessment; Sensitivity analysis; Value engineering; Closed-form expression; Conditional Value-at-Risk; Design simulations; Financial industry; Financial risk management; Loss distribution; Unified framework; Value at Risk; Risk management
Multidimensional stochastic approximation: Adaptive algorithms and applications,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995555434&doi=10.1145%2f2553085&partnerID=40&md5=cb8195cc0aa777c97900e28ecce97819,"We consider prototypical sequential stochastic optimization methods of Robbins-Monro (RM), Kiefer-Wolfowitz (KW), and Simultaneous Perturbations Stochastic Approximation (SPSA) varieties and propose adaptive modifications for multidimensional applications. These adaptive versions dynamically scale and shift the tuning sequences to better match the characteristics of the unknown underlying function, as well as the noise level. We test our algorithms on a variety of representative applications in inventory management, health care, revenue management, supply chain management, financial engineering, and queueing theory. © 2014 ACM.",Adaptive algorithms; Algorithms; G.1.6. [optimization]: stochastic programming; G.4. [mathematical software]: algorithm design and analysis; I.6.3. [simulation and modelling]: applications; Kiefer-Wolfowitz; Numerical examples; Performance; Robbins-Monro; Simultaneous perturbations stochastic approximation; Stochastic approximations,Adaptive algorithms; Algorithms; Approximation theory; Inventory control; Modal analysis; Multimedia services; Optimization; Queueing theory; Stochastic programming; Stochastic systems; Supply chain management; Algorithm design and analysis; Kiefer-Wolfowitz; Numerical examples; Performance; Robbins-Monro; Simulation and modelling; Stochastic approximations; Approximation algorithms
Space-time matching algorithms for interest management in distributed virtual environments,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905055706&doi=10.1145%2f2567922&partnerID=40&md5=6a0b88e79af04f31b79ceee00ee5319b,"Interest management in Distributed Virtual Environments (DVEs) is a data-filtering technique designed to reduce bandwidth consumption and therefore enhances the scalability of the system. This technique usually involves a process called interest matching, which determines what data should be sent to the participants as well as what data should be filtered. Although most of the existing interest matching approaches have been shown to meet their runtime performance requirements, they have a fundamental disadvantage-they perform interest matching at discrete time intervals. As a result, they would fail to report events between discrete timesteps. If participants of the DVE ignore these missing events, they would most likely perform incorrect simulations. This article presents a new approach called space-time interest matching, which aims to capture the missing events between discrete timesteps. Although this approach requires additional matching effort, a number of novel algorithms are developed to significantly improve its runtime efficiency. © 2014 ACM.",Data distribution management; Distributed Virtual Environments; High-level architecture; Interest management; Massively multiplayer online games,Algorithms; Distributed computer systems; Interactive computer graphics; Data distribution management; Distributed Virtual Environments; High level architecture; Interest managements; Massively multi-player online games; Information management
Selecting stopping rules for confidence interval procedures,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904971297&doi=10.1145%2f2627734&partnerID=40&md5=3b18161860f444975fd97e1a9e329b81,"The sample size decision is crucial to the success of any sampling experiment. More samples imply better confidence and precision in the results, but require higher costs in terms of time, computing power, and money. Analysts often choose sequential stopping rules on an ad hoc basis to obtain confidence intervals with desired properties without requiring large sample sizes. However, the choice of stopping rule can affect the quality of the interval produced in terms of the coverage, precision, and replication cost. This article introduces methods for choosing and evaluating stopping rules for confidence interval procedures. Wedevelop a general framework for assessing the quality of a broad class of stopping rules applied to independent and identically distributed data. We introduce coverage profiles that plot the coverage according to the stopping time and reveal situations when the coverage could be unexpectedly low. Finally, we recommend simple techniques for obtaining acceptable or optimal rules. © 2014 ACM.",Coverage profiles; Finite-sample performance,Sampling; Computing power; Confidence interval; Confidence-interval procedures; Coverage profiles; Distributed data; Finite-sample performance; Replication cost; Sequential Stopping Rules; Financial data processing
Selection procedures for simulations with multiple constraints under independent and correlated sampling,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904992210&doi=10.1145%2f2567921&partnerID=40&md5=5d76cdb1c7541045483a60568c616bfc,"We consider the problem of selecting the best feasible system with constraints on multiple secondary performance measures. We develop fully sequential indifference-zone procedures to solve this problem that guarantee a nominal probability of correct selection. In addition, we address two issues critical to the efficiency of these procedures: namely, the allocation of error between feasibility determination and selection of the best system, and the use of Common Random Numbers. We provide a recommended error allocation as a function of the number of constraints, supported by an experimental study and an approximate asymptotic analysis. The validity and efficiency of the new procedures with independent and CRN are demonstrated through both analytical and experimental results. © 2014 ACM.",Common Random Numbers; Constraints; Fully sequential algorithms; Multiple performance measures,Asymptotic analysis; Common random numbers; Constraints; Correlated sampling; Fully sequential algorithms; Multiple constraint; Multiple performance measures; Probability of correct selections; Selection procedures; Random number generation
"Discrete event execution with one-sided and two-sided GVT algorithms on 216,000 processor cores",2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905000711&doi=10.1145%2f2611561&partnerID=40&md5=060ea54f73b1284d3ce2001932d4794d,"Global Virtual Time (GVT) computation is a key determinant of the efficiency and runtime dynamics of Parallel Discrete Event Simulations (PDES), especially on large-scale parallel platforms. Here, three execution modes of a generalized GVT computation algorithm are studied on high-performance parallel computing systems: (1) a synchronous GVT algorithm that affords ease of implementation, (2) an asynchronous GVT algorithm that is more complex to implement but can relieve blocking latencies, and (3) a variant of the asynchronous GVT algorithm to exploit one-sided communication in extant supercomputing platforms. Performance results are presented of implementations of these algorithms on up to 216,000 cores of a Cray XT5 system, exercised on a range of parameters: optimistic and conservative synchronization, fine- to mediumgrained event computation, synthetic and nonsynthetic applications, and different lookahead values. Detailed PDES-specific runtime metrics are presented to further the understanding of tightly coupled discrete event dynamics on massively parallel platforms. © 2014 ACM.",Asynchrony; Global Virtual Time; Onesided communication; Parallel Discrete Event Simulation; Time warp,Discrete event simulation; Distributed computer systems; Asynchrony; Global virtual time; One-sided communications; Parallel discrete event simulations; Time Warp; Algorithms
Smoothed functional algorithms for stochastic optimization using q-gaussian distributions,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904975463&doi=10.1145%2f2628434&partnerID=40&md5=fdfcf6235123c32a9e61c97db1eac69b,"Smoothed functional (SF) schemes for gradient estimation are known to be efficient in stochastic optimization algorithms, especially when the objective is to improve the performance of a stochastic system. However, the performance of these methods depends on several parameters, such as the choice of a suitable smoothing kernel. Different kernels have been studied in the literature, which include Gaussian, Cauchy, and uniform distributions, among others. This article studies a new class of kernels based on the q-Gaussian distribution, which has gained popularity in statistical physics over the last decade. Though the importance of this family of distributions is attributed to its ability to generalize the Gaussian distribution, we observe that this class encompasses almost all existing smoothing kernels. This motivates us to study SF schemes for gradient estimation using the q-Gaussian distribution. Using the derived gradient estimates, we propose two-timescale algorithms for optimization of a stochastic objective function in a constrained setting with a projected gradient search approach. We prove the convergence of our algorithms to the set of stationary points of an associated ODE. We also demonstrate their performance numerically through simulations on a queuing model. © 2014 ACM.",Projected gradient based search; Q-Gaussian; Smoothed functional algorithms; Two-timescale stochastic approximation,Gaussian distribution; Optimization; Queueing theory; Objective functions; Projected gradient; Q-gaussian; Statistical physics; Stochastic optimization algorithm; Stochastic optimizations; Two timescale stochastic approximation; Uniform distribution; Algorithms
Approximate inference for observation-driven time series models with intractable likelihoods,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904957897&doi=10.1145%2f2592254&partnerID=40&md5=21a6cc6a455985532952547faf503f41,"In this article, we consider approximate Bayesian parameter inference for observation-driven time series models. Such statistical models appear in a wide variety of applications, including econometrics and applied mathematics. This article considers the scenario where the likelihood function cannot be evaluated pointwise; in such cases, one cannot perform exact statistical inference, including parameter estimation, which often requires advanced computational algorithms, such as Markov Chain Monte Carlo (MCMC). We introduce a new approximation based upon Approximate Bayesian Computation (ABC). Under some conditions, we show that as n → ∞, with n the length of the time series, the ABC posterior has, almost surely, a Maximum A Posteriori (MAP) estimator of the parameters that is often different from the true parameter. However, a noisy ABC MAP, which perturbs the original data, asymptotically converges to the true parameter, almost surely. In order to draw statistical inference, for the ABC approximation adopted, standard MCMC algorithms can have acceptance probabilities that fall at an exponential rate in n and slightly more advanced algorithms can mix poorly. We develop a new and improved MCMC kernel, which is based upon an exact approximation of a marginal algorithm, whose cost per iteration is random, but the expected cost, for good performance, is shown to be O(n2) per iteration. We implement our new MCMC kernel for parameter inference from models in econometrics. © 2014 ACM.",Approximate Bayesian Computation; Asymptotic consistency; Markov Chain Monte Carlo; Observation-driven time series models,Bayesian networks; Estimation; Iterative methods; Statistical methods; Time series; Approximate Bayesian; Approximate inference; Asymptotic consistencies; Computational algorithm; Likelihood functions; Markov Chain Monte-Carlo; Statistical inference; Time series models; Approximation algorithms
Efficient simulations for the exponential integrals of Hölder continuous Gaussian random fields,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897429059&doi=10.1145%2f2567892&partnerID=40&md5=a5d67c797e1c72164d4cd13aeb736720,"In this article, we consider a Gaussian random field f (t) living on a compact set T ⊂ Rd and the computation of the tail probabilities P(∫ T e f (t)dt > eb) as b→∞. We design asymptotically efficient importance sampling estimators for a general class of Hölder continuous Gaussian random fields. In addition to the variance control, we also analyze the bias (relative to the interesting tail probabilities) caused by the discretization. © 2014 ACM.",Exponential integral; Gaussian random fields; Importance sampling,Importance sampling; Asymptotically efficient; Discretizations; Efficient simulation; Exponential integrals; Gaussian random fields; General class; Tail probability; Variance control; Gaussian distribution
ConceVE: Conceptual modeling and formal validation for everyone,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897460011&doi=10.1145%2f2567897&partnerID=40&md5=fefede20bef1c2591a794cc7cead12e2,"In this article, we present ConceVE, an approach for designing and validating models before they are implemented in a computer simulation. The approach relies on (1) domain-specific languages for model specification, (2) the Alloy Specification Language and its constraint solving analysis capabilities for exploring the state space of the model dynamically, and (3) supporting visualization tools to relay the results of the analysis to the user. We show that our approach is applicable with generic languages such as the Web Ontology Language as well as special XML-based languages such as the Coalition Battle Management Language. © 2014 ACM.",Conceptual modeling; Interoperability; M&S formalism; Validity; Verification,Computer simulation; Interoperability; Problem oriented languages; Specification languages; Verification; Analysis capabilities; Coalition battle management languages; Conceptual model; Domain specific languages; Model specifications; Validity; Web ontology language; XML-based languages; Data mining
Stochastic kriging with biased sample estimates,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897431730&doi=10.1145%2f2567893&partnerID=40&md5=50ee2fdee2bd8301898b35a43358b3f4,"Stochastic kriging has been studied as an effective metamodeling technique for approximating response surfaces in the context of stochastic simulation. In a simulation experiment, an analyst typically needs to estimate relevant metamodel parameters and further do prediction; therefore, the impact of parameter estimation on the performance of the metamodel-based predictor has drawn some attention in the literature. However, how the standard stochastic kriging predictor is affected by the presence of bias in finite-sample estimates has not yet been fully investigated. In this article, we study the predictive performance and investigate optimal budget al.location rules subject to a fixed computational budget constraint. Furthermore, we extend the analysis to two-level or nested simulation, which has been recently documented in the risk management literature, with biased estimators. © 2014 ACM.",Metamodeling; Nested simulation; Optimal budget al.location; Simulation experimental design; Simulation output analysis; Simulation theory,Budget control; Interpolation; Optimization; Risk management; Stochastic models; Stochastic systems; Metamodeling; Nested simulation; Optimal budget al.location; Simulation output analysis; Simulation theory; Estimation
Variance estimation and sequential stopping in steady-state simulations using linear regression,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897374401&doi=10.1145%2f2567907&partnerID=40&md5=6bf5c03dd2b841fb418f47ea458169e0,"We propose a method for estimating the variance parameter of a discrete, stationary stochastic process that involves combining variance estimators at different run lengths using linear regression. We show that the estimator thus obtained is first-order unbiased and consistent under two distinct asymptotic regimes. In the first regime, the number of constituent estimators used in the regression is fixed and the numbers of observations corresponding to the component estimators grow in a proportional manner. In the second regime, the number of constituent estimators grows while the numbers of observations corresponding to each estimator remain fixed. We also show that for m-dependent stochastic processes, one can use regression to obtain asymptotically normally distributed variance estimators in the second regime. Analytical and numerical examples indicate that the new regression-based estimators give good mean-squared-error performance in steady-state simulations. The regression methodology presented in this article can also be applied to estimate the bias of variance estimators. As an example application, we present a new sequential-stopping rule that uses the estimate for bias to determine appropriate run lengths. Monte Carlo experiments indicate that this ""bias-controlling"" sequential-stopping method has the potential to work well in practice. © 2014 ACM.",Regression; Simulation; Stationary processes; Stopping rule; Variance estimation,Information dissemination; Monte Carlo methods; Normal distribution; Random processes; Regression analysis; Regression; Simulation; Stationary process; Stopping rule; Variance estimation; Estimation
A restricted multinomial hybrid selection procedure,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897378203&doi=10.1145%2f2567891&partnerID=40&md5=bbb3b8984a788def4e39d69c7bb08ad3,"Analysts using simulation models often must assess a large number of alternatives in order to determine which are most effective. If effectiveness corresponds to the likelihood of yielding the best outcome, this becomes a multinomial selection problem. Unfortunately, existing procedures were developed primarily for evaluating small sets of alternatives, so parameters required to implement themmay not be readily available or the sampling costs may be prohibitive when a large number of alternatives are present. We propose a truncated, sequential multinomial subset selection procedure that restricts the maximum subset size. Numerical comparisons show that our procedure can be much more efficient than the leading unrestricted procedure. Our procedure requires only one calculated parameter rather than four. We provide extensive tables for cases involving large numbers of alternatives. © 2014 ACM.",Ranking and selection; Restricted multinomial subset selection,Computer applications; Hybrid selection; Multinomials; Numerical comparison; Ranking and selection; Selection problems; Subset selection; Computer simulation
SESSL: A domain-specific language for simulation experiments,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897473124&doi=10.1145%2f2567895&partnerID=40&md5=121a63c6bd9e1c1e96cac057a4ea6a75,"This article introduces SESSL (Simulation Experiment Specification via a Scala Layer), an embedded domain-specific language for simulation experiments. It serves as an additional software layer between users and simulation systems and is implemented in Scala. SESSL supportsmultiple simulation systems and offers various features (e.g., for experiment design, performance analysis, result reporting, and simulation-based optimization). It supports ""cutting-edge"" experiments by allowing to add custom code, enables a reuse of functionality across simulation systems, and improves the reproducibility of simulation experiments. © 2014 ACM.",Domain-specific language; Experiments; Scala; Simulation,Experiments; Problem oriented languages; Domain specific languages; Embedded domain-specific languages; Performance analysis; Reproducibilities; Scala; Simulation; Simulation systems; Simulation-based optimizations; Computer software
GDCSim: A simulator for green data center design and analysis,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893954545&doi=10.1145%2f2553083&partnerID=40&md5=bb51873add55d5a3fc8052afedab36c9,"Energy-efficient data center design and management has been a challenge of increasing importance in the past decade due to its potential to save billions of dollars in energy costs. However, the state of the art in design and evaluation of data centers require designers to be expertly familiar with a prohibitively large number of domain-specific design tools that necessitate user intervention in each step of the design process. This is due to the lack of a holistic data center design tool. To fill this gap, this article presents an iterative green data center design framework, the Green Data Center Simulator (GDCSim), for the design and development of energy-efficient data centers. © 2014 ACM.",Cyber-physical system; Energy efficiency; Green data center; Modeling; Simulation,
Data-driven simulation of complex multidimensional time series,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893964815&doi=10.1145%2f2553082&partnerID=40&md5=4ac55091cb39f4a0e6b503beb285d27e,"This article introduces a new framework for resampling general time series data. The approach, inspired by computer agent flocking algorithms, can be used to generate inputs to complex simulation models or for generating pseudo-replications of expensive simulation outputs. The method has the flexibility to enable replicated sensitivity analysis for trace-driven simulation, which is critical for risk assessment. The article includes two simple implementations to illustrate the approach. These implementations are applied to nonstationary and state-dependent multivariate time series. Examples using emergency department data are presented. © 2014 ACM.",Flocking algorithms,
INDEMICS: An interactive high-performance computing framework for data-intensive epidemic modeling,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893954189&doi=10.1145%2f2501602&partnerID=40&md5=7f5e0363020ed00762338bfa8baf30e1,"We describe the design and prototype implementation of INDEMICS (Interactive Epidemic Simulation)-a modeling environment utilizing high-performance computing technologies for supporting complex epidemic simulations. INDEMICS can support policy analysts and epidemiologists interested in planning and control of pandemics. INDEMICS goes beyond traditional epidemic simulations by providing a simple and powerful way to represent and analyze policy-based as well as individual-based adaptive interventions. Users can also stop the simulation at any point, assess the state of the simulated system, and add additional interventions. INDEMICS is available to end-users via a web-based interface. Detailed performance analysis shows that INDEMICS greatly enhances the capability and productivity of simulating complex intervention strategies with a marginal decrease in performance. We also demonstrate how INDEMICS was applied in some real case studies where complex interventions were implemented. © 2014 ACM.",Infectious disease; Interactive computation; Modeling and simulation; Network dynamics; Parallel computation,
Two-stage stochastic optimization for optimal power flow under renewable generation uncertainty,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893976198&doi=10.1145%2f2553084&partnerID=40&md5=5958fdbc10be9af46b6dbbefe898178b,"We propose a two-stage stochastic version of the classical economic dispatch problem with alternatingcurrent power flow constraints, a nonconvex optimization formulation that is central to power transmission and distribution over an electricity grid. Certain generation decisions made in the first stage cannot further be changed in the second stage, where the uncertainty due to various factors such as renewable generation is realized. Any supply-demand mismatch in the second stage must be alleviated using high marginal cost power sources that can be tapped in short order. We solve a Sample-Average Approximation (SAA) of this formulation by capturing the uncertainty using a finite number of scenario samples. We propose two outer approximation algorithms to solve this nonconvex program to global optimality. We use recently discovered structural properties for the classical deterministic problem to show that when these properties hold the sequence of approximate solutions obtained under both alternatives has a limit point that is a globally optimal solution to the two-stage nonconvex SAA program. We also present an alternate local optimization approach to solving the SAA problem based on the Alternating Direction Method of Multipliers (ADMM). Numerical experiments for a variety of parameter settings were carried out to demonstrate the efficiency and usability of our method over ADMM for large practical instances. © 2014 ACM.",Decomposition algorithms; Global solution; Optimal power flow; Sample-average approximation; Stochastic optimization,
Guest editors' introduction to special issue on the third informs simulation society research workshop,2014,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893968071&doi=10.1145%2f2555690&partnerID=40&md5=54fb9a4235298a429352649669d0ad21,[No abstract available],,
Efficient Importance Sampling Schemes for a Feed-Forward Network,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024242765&doi=10.1145%2f2517450&partnerID=40&md5=6063345a14373b4b4d46bddd3f10b487,"The aim of this article is to construct efficient importance sampling schemes for a rare event, namely, the buffer overflow associated with a feed-forward network with discontinuous dynamics. This is done through a piecewise constant change of measure, which is based on a suitably constructed subsolution to an HJB equation. The main task is to change the measure such that the logarithmic asymptotic optimality is achieved. To that end, we find an upper bound on the second moment of the importance sampling estimator that yields optimality. Numerical simulations illustrate the validity of theoretical results. © 2013, ACM. All rights reserved.",Algorithms; asymptotic optimality; Importance sampling; Performance; Stochastic Analysis; subsolution,
Rare-Event Simulation for Stochastic Recurrence Equations with Heavy-Tailed Innovations,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945550411&doi=10.1145%2f2517451&partnerID=40&md5=8251596ffb83968ba94db96956bd56c0,"In this article, rare-event simulation for stochastic recurrence equations of the form [formula ommitted] is studied, where {An; n ≥ 1} and {Bn; n ≥ 1} are independent sequences consisting of independent and identically distributed real-valued random variables. It is assumed that the tail of the distribution of B1 is regularly varying, whereas the distribution of A1 has a suitably light tail. The problem of efficient estimation, via simulation, of quantities such as P{Xn> b} and P{supk=nXk> b} for large b and n is studied. Importance sampling strategies are investigated that provide unbiased estimators with bounded relative error as b and n tend to infinity. © 2013, ACM. All rights reserved.",Algorithms; heavy-tails; Importance sampling; Performance; stochastic recurrence equations; Theory,
Synchronised range queries in distributed simulations of multiagent systems,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888340939&doi=10.1145%2f2517449&partnerID=40&md5=3312d85ac31d5eaa0248ec6a8c7e625b,"Range queries are an increasingly important associative form of data access encountered in different computational environments including peer-to-peer systems, wireless communications, database systems, distributed virtual environments, and, more recently, distributed simulations. In this article, we present and evaluate a system for performing logical-time synchronised Range-Queries over data in the context of distributed simulations of multiagent systems. This article presents algorithms performing instantaneous queries within an optimistic synchronisation framework and in the presence of dynamic migration of the simulation state. A quantitative evaluation of the effectiveness of the proposed algorithms under different conditions and for different benchmarks, including Boids, is also presented. © 2013 ACM.",Data management; Distributed simulation; Distributed virtual environments; Load balancing; Multiagent systems; Range queries,Algorithms; Distributed database systems; Information management; Multi agent systems; Query languages; Resource allocation; Synchronization; Wireless telecommunication systems; Computational environments; Distributed simulations; Distributed Virtual Environments; Dynamic migration; Peer-to-Peer system; Quantitative evaluation; Range query; Wireless communications; Distributed computer systems
Parameterized Activity Cycle Diagram and Its Application,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961606138&doi=10.1145%2f2501593&partnerID=40&md5=054d0600e20b15f13b7b5e62015e8262,"The classical activity cycle diagram (ACD), which is a bipartite directed graph, is easy to learn and use for describing the dynamic behavior of a discrete-event system. However, the complexity of the classical ACD model increases rapidly as the system size increases. This article presents an enriched ACD called the parameterized ACD (P-ACD). In P-ACD, each node is allowed to have parameter variables, and parameter values are passed to the parameter variables through a directed arc. This article demonstrates how a single PACD model can be used to represent an entire class of very large-scale systems instead of requiring different ACD models for every instance. We also illustrate that the well-known activity scanning algorithm can be used to execute a P-ACD model. A prototype P-ACD simulator implemented in C# programming language is provided, and an illustrative example of a conveyor-driven serial production line with the prototype simulator is presented to illustrate construction and execution of a P-ACD model. In addition, it is demonstrated that the proposed P-ACD allows an effective and concise modeling of a job shop, which was not possible with the classical ACD. © 2013, ACM. All rights reserved.",Activity cycle diagram (ACD); activity scanning algorithm; activity transition table; Algorithms; Design; discrete-event simulation modeling; Experimentation; parameterized ACD,
On the Validity of Flow-level TCP Network Models for Grid and Cloud Simulations,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002716104&doi=10.1145%2f2517448&partnerID=40&md5=ac18458d87cdb90eaf754aad0005ae59,"Researchers in the area of grid/cloud computing perform many of their experiments using simulations that must capture network behavior. In this context, packet-level simulations, which are widely used to study network protocols, are too costly given the typical large scales of simulated systems and applications. An alternative is to implement network simulations with less costly flow-level models. Several flow-level models have been proposed and implemented in grid/cloud simulators. Surprisingly, published validations of these models, if any, consist of verifications for only a few simple cases. Consequently, even when they have been used to obtain published results, the ability of these simulators to produce scientifically meaningful results is in doubt. This work evaluates these state-of-the-art flow-level network models of TCP communication via comparison to packet-level simulation. While it is straightforward to show cases in which previously proposed models lead to good results, instead we follow the critical method, which places model refutation at the center of the scientific activity, and we systematically seek cases that lead to invalid results. Careful analysis of these cases reveals fundamental flaws and also suggests improvements. One contribution of this work is that these improvements lead to a new model that, while far from being perfect, improves upon all previously proposed models in the context of simulation of grids or clouds. A more important contribution, perhaps, is provided by the pitfalls and unexpected behaviors encountered in this work, leading to a number of enlightening lessons. In particular, this work shows that model validation cannot be achieved solely by exhibiting (possibly many) “good cases.” Confidence in the quality of a model can only be strengthened through an invalidation approach that attempts to prove the model wrong. © 2013, ACM. All rights reserved.",Experimentation; Grid and cloud computing simulation; scalability; SimGrid; validation,
Integer-ordered simulation optimization using R-SPLINE: Retrospective search with piecewise-linear interpolation and neighborhood enumeration,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887034583&doi=10.1145%2f2499913.2499916&partnerID=40&md5=510ee4cc7b9e711d73032a228ab27f95,"We consider simulation-optimization (SO) models where the decision variables are integer ordered and the objective function is defined implicitly via a simulation oracle, which for any feasible solution can be called to compute a point estimate of the objective-function value. We develop R-SPLINE-a Retrospective-search algorithm that alternates between a continuous Search using Piecewise-Linear Interpolation and a discrete Neighborhood Enumeration, to asymptotically identify a local minimum. R-SPLINE appears to be among the first few gradient-based search algorithms tailored for solving integer-ordered local SO problems. In addition to proving the almost-sure convergence of R-SPLINE's iterates to the set of local minima, we demonstrate that the probability of R-SPLINE returning a solution outside the set of true local minima decays exponentially in a certain precise sense. R-SPLINE, with no parameter tuning, compares favorably with popular existing algorithms. © 2013 ACM.",,Computer simulation; Learning algorithms; Optimization; Piecewise linear techniques; Decision variables; Feasible solution; Gradient-based search; Objective functions; Parameter-tuning; Piecewise-linear; Point estimate; Simulation optimization; Integer programming
Optimizing pairwise box intersection checking on GPUs for large-scale simulations,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887035811&doi=10.1145%2f2499913.2499918&partnerID=40&md5=8cef5a322037e6ae5b85b549806a044c,"Box intersection checking is a common task used in many large-scale simulations. Traditional methods cannot provide fast box intersection checking with large-scale datasets. This article presents a parallel algorithm to perform Pairwise Box Intersection checking on Graphics processing units (PBIG). The PBIG algorithm consists of three phases: planning, mapping and checking. The planning phase partitions the space into small cells, the sizes of which are determined to optimize performance. The mapping phase maps the boxes into the cells. The checking phase examines the box intersections in the same cell. Several performance optimizations, including load-balancing, output data compression/encoding, and pipelined execution, are presented for the PBIG algorithm. The experimental results show that the PBIG algorithm can process large-scale datasets and outperforms three well-performing algorithms. © 2013 ACM.",Box intersection checking; Data compression; Load-balancing; Pipelined execution,Computer graphics; Data compression; Optimization; Program processors; Box intersection checking; Graphics Processing Unit; Large scale simulations; Large-scale datasets; Load-Balancing; Performance optimizations; Pipelined execution; Planning phase; Algorithms
Asymptotic simulation efficiency based on large deviations,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887046892&doi=10.1145%2f2499913.2499919&partnerID=40&md5=c7a1dc86c4196f42c48b93e93ef3268c,"Consider a simulation estimator α(c) based on expending c units of computer time to estimate a quantity α. In comparing competing estimators for α, a natural figure of merit is to choose the estimator that minimizes the computation time needed to reduce the error probability P(|α(c)-α| > ε) to below some prescribed value δ. In this paper, we develop large deviations results that provide approximations to the computational budget necessary to reduce the error probability to below δ when δ is small. This approximation depends critically on both the distribution of the estimator itself and that of the random amount of computer time required to generate the estimator, and leads to different conclusions regarding the choice of preferred estimator than those obtained when one requires the error tolerance ε to be small. The ""small ε"" regime leads to variancebased selection criteria, and has a long history in the simulation literature going back to Hammersley and Handscomb. © 2013 ACM.",Efficiency; Large deviations; Monte Carlo simulation,Computer simulation; Efficiency; Monte Carlo methods; Computation time; Computational budget; Error probabilities; Error tolerance; Figure of merits; Large deviations; Selection criteria; Simulation efficiency; Estimation
Fitting statistical models of random search in simulation studies,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887093070&doi=10.1145%2f2499913.2499914&partnerID=40&md5=e9a22b1f4ea0a83823502f55561c06ea,"We consider optimization of expected system performance by random search. There are two sources of random variation in this process: (i) a search-induced variability because the expected performance of the system will vary randomly according to the alternatives randomly selected for examination, and (ii) a simulation induced variability, because there will be random error in estimating expected system performance from finite simulation runs. We show that, in altering the balance between these two sources of variability, three distinct forms of asymptotic behavior of the estimate of the optimal expected system performance are possible. The form of the asymptotic results shows that they may be not be easy to apply in practical work. As an alternative, a methodology for fitting a statistical model that accounts for both types of variability is suggested. This then allows the distributional properties of quantities of interest, like the optimum performance value and the best value obtained by the search, to be estimated by resampling and which also allows a test of goodness of fit of the model. Four numerical examples are given. © 2013 ACM.",Bootstrapping; Convolution models; Embedded models; Optimization by random search,Asymptotic analysis; Optimization; Asymptotic behaviors; Bootstrapping; Convolution model; Distributional property; Embedded models; Quantities of interests; Random searches; Statistical modeling; Computer simulation
Deriving feasible deployment alternatives for parallel and distributed simulation systems,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887090087&doi=10.1145%2f2499913.2499917&partnerID=40&md5=6e6125f8c09c63bef198744fb903a823,"Parallel and distributed simulations (PADS) realize the distributed execution of a simulation system over multiple physical resources. To realize the execution of PADS, different simulation infrastructures such as HLA, DIS and TENA have been defined. Recently, the Distributed Simulation Engineering and Execution Process (DSEEP) that supports the mapping of the simulations on the infrastructures has been defined. An important recommended task in DSEEP is the evaluation of the performance of the simulation systems at the design phase. In general, the performance of a simulation is largely influenced by the allocation of member applications to the resources. Usually, the deployment of the applications to the resources can be done in many different ways. DSEEP does not provide a concrete approach for evaluating the deployment alternatives. Moreover, current approaches that can be used for realizing various DSEEP activities do not yet provide adequate support for this purpose. We provide a concrete approach for deriving feasible deployment alternatives based on the simulation system and the available resources. In the approach, first the simulation components and the resources are designed. The design is used to define alternative execution configurations, and based on the design and the execution configuration; a feasible deployment alternative can be algorithmically derived. Tool support is developed for the simulation design, the execution configuration definition and the automatic generation of feasible deployment alternatives. The approach has been applied within a large-scale industrial case study for simulating Electronic Warfare systems. © 2013 ACM.",DSEEP; High-level architecture; Metamodeling; Model transformations; Parallel and distributed simulations; Software architecture,Concretes; Design; Electronic warfare; Industrial applications; Software architecture; DSEEP; High level architecture; Metamodeling; Model transformation; Parallel and distributed simulation; Computer simulation
Limit theorems for simulation-based optimization via random search,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887056544&doi=10.1145%2f2499913.2499915&partnerID=40&md5=22b03da3d88182a3edfc5c2c11cf8635,"This article develops fundamental theory related to the use of simulation-based nonadaptive random search as a means of optimizing a function that can be expressed as an expectation. Our results establish rates of convergence that express the trade-off between exploration and estimation, and fully characterize the limit distributions that arise. Our rates of convergence results should be viewed as a baseline against which to compare more intelligent algorithms. © 2013 ACM.",Limit theorems; Optimization; Random search; Simulation,Computer applications; Computer simulation; Fundamental theory; Intelligent Algorithms; Limit distribution; Limit theorem; Random searches; Rates of convergence; Simulation; Simulation-based optimizations; Optimization
Reversible simulations of elastic collisions,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878600467&doi=10.1145%2f2457459.2457461&partnerID=40&md5=fc6452a4745f38e7e4d5a867514e8df7,"Consider a system of N identical hard spherical particles moving in a d-dimensional box and undergoing elastic, possibly multiparticle, collisions. We develop a new algorithm that recovers the precollision state from the post-collision state of the system, across a series of consecutive collisions, with essentially no memory overhead. The challenge in achieving reversibility for an n-particle collision (where, in general, n ≤≤ N) arises from the presence of nd - d - 1 degrees of freedom (arbitrary angles) during each collision, as well as from the complex geometrical constraints placed on the colliding particles. To reverse the collisions in a traditional simulation setting, all of the particular realizations of these degrees of freedom (angles) during the forward simulation must be tracked. This requires memory proportional to the number of collisions, which grows very fast with N and d, thereby severely limiting the de facto applicability of the scheme. This limitation is addressed here by first performing a pseudorandomization of angles, which ensures determinism in the reverse path for any values of n and d. To address the more difficult problem of geometrical and dynamic constraints, a new approach is developed which correctly samples the constrained phase space. Upon combining the pseudorandomization with correct phase space sampling, perfect reversibility of collisions is achieved, as illustrated for n ≤ 3, d = 2, and n = 2, d = 3. This result enables, for the first time, reversible simulations of elastic collisions with essentially zero memory accumulation. In principle, the approach presented here could be generalized to larger values of n. The reverse computation methodology presented here uncovers important issues of irreversibility in conventional models, and the difficulties encountered in arriving at a reversible model for one of the most basic and widely used physical system processes, namely, elastic collisions for hard spheres. Insights and solution methodologies, with regard to accurate phase space coverage with reversible random sampling proposed in this context, can help serve as models and/or starting points for other reversible simulations. © 2013 ACM.",Billiards; Conservation laws; Phase space coverage; Reverse execution; Reversible pseudorandom; Time warp,Elastic scattering; Mechanics; Models; Phase space methods; Billiards; Conservation law; Phase spaces; Pseudo random; Reverse execution; Time Warp; Computer simulation
Better estimation of small sobol' sensitivity indices,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878592630&doi=10.1145%2f2457459.2457460&partnerID=40&md5=ad4d2ac79dc2496cec0e7f214268f3a6,"A new method for estimating Sobol' indices is proposed. The new method makes use of 3 independent input vectors rather than the usual 2. It attains much greater accuracy on problems where the target Sobol' index is small, even outperforming some oracles that adjust using the true but unknown mean of the function. The new estimator attains a better rate of convergence than the old one in a small effects limit. When the target Sobol' index is quite large, the oracles do better than the new method. © 2013 ACM.",Analysis of variance; Fixing methods; Global sensitivity analysis,Analysis of variance (ANOVA); Computer applications; Computer simulation; Fixing methods; Global sensitivity analysis; Input vector; Rate of convergence; Sobol' sensitivity index; Estimation
Modeling BitTorrent-like systems with many classes of users,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878576383&doi=10.1145%2f2457459.2457462&partnerID=40&md5=69e0954c45e72658bb66575c2b01efa1,"BitTorrent is one of the most successful peer-to-peer systems. Researchers have studied a number of aspects of the system, including its scalability, performance, efficiency and fairness. However, the complexity of the system has forced most prior analytical work to make a number of simplifying assumptions, for example, user homogeneity, or even ignore some central aspects of the protocol altogether, for example, the rate-based Tit-for-Tat (TFT) unchoking scheme, in order to keep the analysis tractable. Motivated by this, in this article we propose two analytical models that accurately predict the performance of the system while considering the central details of the BitTorrent protocol. Our first model is a steady-state one, in the sense that it is valid during periods of time where the number of users remains fixed. Freed by the complications of user time-dynamics, we account for many of the central details of the BitTorrent protocol and accurately predict a number of performance metrics. Our second model combines prior work on fluid models with our first model to capture the transient behavior as new users join or old users leave, while modelling many major aspects of BitTorrent. To the best of our knowledge, this is the first model that attempts to capture the transient behavior of many classes of heterogeneous users. Finally, we use our analytical methodology to introduce and study the performance of a flexible token-based scheme for BitTorrent, show how this scheme can be used to block freeriders and tradeoff between higher-bandwidth and lower-bandwidth users performance, and evaluate the scheme's parameters that achieve a target operational point. © 2013 ACM.",BitTorrent; Heterogeneous peer-to-peer (P2P) networks,Bandwidth; Complex networks; Peer to peer networks; Analytical methodology; Bit torrents; Efficiency and fairness; Heterogeneous users; Peer to Peer (P2P) network; Peer-to-Peer system; Performance metrics; Simplifying assumptions; Distributed computer systems
Characterizing per-application network traffic using entropy,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878605191&doi=10.1145%2f2457459.2457463&partnerID=40&md5=0ad2c7666081b5fe41fe4cf057cbcb6a,"The Internet has been evolving into a more heterogeneous internetwork with diverse new applications imposing more stringent bandwidth and QoS requirements. Already new applications such as YouTube, Hulu, and Netflix are consuming a large fraction of the total bandwidth. We argue that, in order to engineer future internets such that they can adequately cater to their increasingly diverse and complex set of applications while using resources efficiently, it is critical to be able to characterize the load that emerging and future applications place on the underlying network. In this article, we investigate entropy as a metric for characterizing per-flow network traffic complexity. While previous work has analyzed aggregated network traffic, we focus on studying isolated traffic flows. Per-application flow characterization caters to the need of network control functions such as traffic scheduling and admission control at the edges of the network. Such control functions necessitate differentiating network traffic on a per-application basis. The ""entropy fingerprints"" that we get from our entropy estimator summarize many characteristics of each application's network traffic. Not only can we compare applications on the basis of peak entropy, but we can also categorize them based on a number of other properties of the fingerprints. © 2013 ACM.",Data analysis; Entropy estimator; Self-similarity; Statistics; Traffic complexity,Bandwidth; Data reduction; Entropy; Internet; Quality of service; Statistics; Aggregated networks; Entropy estimator; Flow characterization; Future applications; Self-similarities; Traffic complexity; Traffic scheduling; Underlying networks; Complex networks
Massive parallelization of serial inference algorithms for a complex generalized linear model,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873670495&doi=10.1145%2f2414416.2414791&partnerID=40&md5=bb77d26b1ad1add62e874643b21bfec7,"Following a series of high-profile drug safety disasters in recent years, many countries are redoubling their efforts to ensure the safety of licensed medical products. Large-scale observational databases such as claims databases or electronic health record systems are attracting particular attention in this regard, but present significant methodological and computational concerns. In this article we show how high-performance statistical computation, including graphics processing units, relatively inexpensive highly parallel computing devices, can enable complex methods in large databases. We focus on optimization and massive parallelization of cyclic coordinate descent approaches to fit a conditioned generalized linear model involving tens of millions of observations and thousands of predictors in a Bayesian context. We find orders-of-magnitude improvement in overall run-time. Coordinate descent approaches are ubiquitous in high-dimensional statistics and the algorithms we propose open up exciting new methodological possibilities with the potential to significantly improve drug safety. © 2013 ACM 1049-3301/2013/01-ART2.",Big data; Optimization; Parallel processing,Algorithms; Computer graphics; Database systems; Optimization; Parallel architectures; Program processors; Bayesian; Big datum; Complex methods; Coordinate descent; Cyclic coordinate descents; Drug safety; Electronic health record systems; Generalized linear model; Graphics Processing Unit; High-dimensional statistics; Highly parallels; Inference algorithm; Large database; Medical products; Orders-of-magnitude; Parallel processing; Parallelizations; Runtimes; Statistical computations; Inference engines
Efficient MCMC for binomial logit models,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873655855&doi=10.1145%2f2414416.2414419&partnerID=40&md5=1980864248adddab9d86b95499ca79ca,"This article deals with binomial logit models where the parameters are estimated within a Bayesian framework. Such models arise, for instance, when repeated measurements are available for identical covariate patterns. To perform MCMC sampling, we rewrite the binomial logit model as an augmented model which involves some latent variables called random utilities. It is straightforward, but inefficient, to use the individual random utility model representation based on the binary observations reconstructed from each binomial observation. Alternatively, we present in this article a new method to aggregate the random utilities for each binomial observation. Based on this aggregated representation, we have implemented an independence Metropolis-Hastings sampler, an auxiliary mixture sampler, and a novel hybrid auxiliary mixture sampler. A comparative study on five binomial datasets shows that the new aggregation method leads to a superior sampler in terms of efficiency compared to previously published data augmentation samplers. © 2013 ACM 1049-3301/2013/01-ART2.",Binomial data; Data augmentation; Logit model; Markov chain Monte Carlo; Random utility model,Mathematical models; Binomial data; Data augmentation; Logit models; Markov Chain Monte-Carlo; Random utility model; Aggregates
Adaptive equi-energy sampler: Convergence and illustration,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873638556&doi=10.1145%2f2414416.2414421&partnerID=40&md5=195fa3e4ab25497e5228b80bebb1501b,"Markov chain Monte Carlo (MCMC) methods allow to sample a distribution known up to a multiplicative constant. Classical MCMC samplers are known to have very poor mixing properties when sampling multimodal distributions. The Equi-Energy sampler is an interacting MCMC sampler proposed by Kou, Zhou and Wong in 2006 to sample difficult multimodal distributions. This algorithm runs several chains at different temperatures in parallel, and allow lower-tempered chains to jump to a state from a higher-tempered chain having an energy ""close"" to that of the current state. A major drawback of this algorithm is that it depends on many design parameters and thus, requires a significant effort to tune these parameters. In this article, we introduce an Adaptive Equi-Energy (AEE) sampler that automates the choice of the selection mecanism when jumping onto a state of the higher-temperature chain. We prove the ergodicity and a strong law of large numbers for AEE, and for the original Equi-Energy sampler as well. Finally, we apply our algorithm to motif sampling in DNA sequences. © 2013 ACM 1049-3301/2013/01-ART2.",Adaptive sampler; Equi-energy sampler; Ergodicity; Interacting Markov chain Monte Carlo; Law of large numbers; Motif sampling,Computer applications; Computer simulation; Adaptive sampler; Equi-energy sampler; Ergodicity; Law of large numbers; Markov Chain Monte-Carlo; Algorithms
Particle algorithms for optimization on binary spaces,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873640478&doi=10.1145%2f2414416.2414424&partnerID=40&md5=aa637bbe8dbb7f03ec957bb2dce56644,"We discuss a unified approach to stochastic optimization of pseudo-Boolean objective functions based on particle methods, including the cross-entropy method and simulated annealing as special cases. We point out the need for auxiliary sampling distributions, meaning parametric families on binary spaces, which are able to reproduce complex dependency structures, and illustrate their usefulness in our numerical experiments. We provide numerical evidence that particle-driven optimization algorithms based on parametric families yield superior results on strongly multimodal optimization problems while local search heuristics outperform them on easier problems. © 2013 ACM 1049-3301/2013/01-ART2.",Binary parametric families; Cross-entropy method; Pseudo-Boolean optimization; Sequential Monte Carlo; Simulated annealing,Algorithms; Boolean functions; Simulated annealing; Cross-entropy method; Dependency structures; Local search heuristics; Multimodal optimization problems; Numerical evidence; Numerical experiments; Objective functions; Optimization algorithms; Parametric family; Particle methods; Pseudo-Boolean; Pseudo-Boolean optimization; Sampling distribution; Sequential Monte Carlo; Stochastic optimizations; Unified approach; Monte Carlo methods
Posterior expectation of regularly paved random histograms,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873691183&doi=10.1145%2f2414416.2414422&partnerID=40&md5=26cf0a328fa2fc81513a64cb134302aa,"We present a novel method for averaging a sequence of histogram states visited by a Metropolis-Hastings Markov chain whose stationary distribution is the posterior distribution over a dense space of tree-based histograms. The computational efficiency of our posterior mean histogram estimate relies on a statistical data-structure that is sufficient for nonparametric density estimation of massive, multidimensional metric data. This data-structure is formalized as statistical regular paving (SRP). A regular paving (RP) is a binary tree obtained by selectively bisecting boxes along their first widest side. SRP augments RP by mutably caching the recursively computable sufficient statistics of the data. The base Markov chain used to propose moves for the Metropolis-Hastings chain is a random walk that data-adaptively prunes and grows the SRP histogram tree. We use a prior distribution based on Catalan numbers and detect convergence heuristically. The performance of our posterior mean SRP histogram is empirically assessed for large sample sizes simulated from several multivariate distributions that belong to the space of SRP histograms. © 2013 ACM 1049-3301/2013/01-ART2.",Averaging tree-based histograms; Bayesian estimation; Catalan Prior; Markov chain Monte Carlo; Multivariate histogram; Statistical regular pavings,Bayesian networks; Binary trees; Graphic methods; Markov processes; Maximum likelihood estimation; Pavements; Trees (mathematics); Bayesian estimations; Catalans; Markov Chain Monte-Carlo; Multivariate histogram; Statistical regular pavings; Tree-based; Statistical methods
Introduction to special issue on monte carlo methods in statistics,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873655234&doi=10.1145%2f2414416.2414417&partnerID=40&md5=cc0b6835801071e0cb1fdf4ce1cdb4a8,[No abstract available],,
Small variance estimators for rare event probabilities,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873617116&doi=10.1145%2f2414416.2414423&partnerID=40&md5=7b4f78384edec1a4947f12c233d0b1b7,"Improving Importance Sampling estimators for rare event probabilities requires sharp approximations of conditional densities. This is achieved for events defined through large exceedances of the empirical mean of summands of a random walk, in the domain of large or moderate deviations. The approximation of conditional density of the trajectory of the random walk is handled on long runs. The length of those runs which is compatible with a given accuracy is discussed; simulated results are presented, which enlight the gain of the present approach over classical Importance Sampling schemes. Detailed algorithms are proposed. © 2013 ACM 1049-3301/2013/01-ART2.",Importance sampling; Large deviation; Moderate deviation; Rare event,Random processes; Conditional density; Large deviations; Moderate-deviations; Random Walk; Rare event; Rare event probabilities; Simulated results; Summands; Variance estimators; Importance sampling
Bayesian learning of noisy markov decision processes,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873697423&doi=10.1145%2f2414416.2414420&partnerID=40&md5=963d9ac0971a83a74919a0df94ea096b,"We consider the inverse reinforcement learning problem, that is, the problem of learning from, and then predicting or mimicking a controller based on state/action data. We propose a statistical model for such data, derived from the structure of a Markov decision process. Adopting a Bayesian approach to inference, we show how latent variables of the model can be estimated, and how predictions about actions can be made, in a unified framework. A new Markov chain Monte Carlo (MCMC) sampler is devised for simulation from the posterior distribution. This step includes a parameter expansion step, which is shown to be essential for good convergence properties of the MCMC sampler. As an illustration, the method is applied to learning a human controller. © 2013 ACM 1049-3301/2013/01-ART2.",Bayesian inference; Data augmentation; Markov Chain Monte Carlo; Markov decision process; Parameter expansion,Bayesian networks; Inference engines; Inverse problems; Markov processes; Reinforcement learning; Bayesian inference; Data augmentation; Markov Chain Monte-Carlo; Markov Decision Processes; Parameter expansion; Learning algorithms
Self-avoiding random dynamics on integer complex systems,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873695090&doi=10.1145%2f2414416.2414790&partnerID=40&md5=a3d8f8bca9ded9fae81698381af29d28,"This article introduces a new specialized algorithm for equilibrium Monte Carlo sampling of binary-valued systems, which allows for large moves in the state space. This is achieved by constructing self-avoiding walks (SAWs) in the state space. As a consequence, many bits are flipped in a single MCMC step. We name the algorithm SARDONICS, an acronym for Self-Avoiding Random Dynamics on Integer Complex Systems. The algorithm has several free parameters, but we show that Bayesian optimization can be used to automatically tune them. SARDONICS performs remarkably well in a broad number of sampling tasks: toroidal ferromagnetic and frustrated Ising models, 3D Ising models, restricted Boltzmann machines and chimera graphs arising in the design of quantum computers. © 2013 ACM 1049-3301/2013/01-ART2.",Adaptive MCMC; Algorithm configuration; Bayesian optimization; Boltzmann machines; Gibbs sampling; Ising models; Markov chain Monte Carlo; Monte Carlo; SARDONICS; Swendswen-Wang,Acoustic surface wave devices; Dynamics; Ising model; Monte Carlo methods; Optimization; Quantum computers; Adaptive mcmc; Algorithm configurations; Bayesian optimization; Boltzmann machines; Gibbs sampling; Markov Chain Monte-Carlo; MONTE CARLO; SARDONICS; Swendswen-Wang; Algorithms
Convergence of a particle-based approximation of the block online expectation maximization algorithm,2013,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873632768&doi=10.1145%2f2414416.2414418&partnerID=40&md5=85807603a2579d339f08fe7db4ac6be0,"Online variants of the Expectation Maximization (EM) algorithm have recently been proposed to perform parameter inference with large data sets or data streams, in independent latent models and in hidden Markov models. Nevertheless, the convergence properties of these algorithms remain an open problem at least in the hidden Markov case. This contribution deals with a new online EM algorithm that updates the parameter at some deterministic times. Some convergence results have been derived even in general latent models such as hidden Markov models. These properties rely on the assumption that some intermediate quantities are available in closed form or can be approximated by Monte Carlo methods when the Monte Carlo error vanishes rapidly enough. In this article, we propose an algorithm that approximates these quantities using Sequential Monte Carlo methods. The convergence of this algorithm and of an averaged version is established and their performance is illustrated through Monte Carlo experiments. © 2013 ACM 1049-3301/2013/01-ART2.",Expectation maximization; Hidden markov models; Online estimation; Sequential Monte Carlo methods,Hidden Markov models; Image segmentation; Inference engines; Maximum principle; Monte Carlo methods; Closed form; Convergence properties; Convergence results; Data stream; Expectation - maximizations; Expectation-maximization algorithms; Hidden markov; Large datasets; Latent models; MONTE CARLO; Monte Carlo experiments; On-line estimation; Online EM; Online expectation maximizations; Parameter inference; Sequential Monte Carlo methods; Algorithms
Stochastic approximation over multidimensional discrete sets with applications to inventory systems and admission control of queueing networks,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878502779&doi=10.1145%2f2379810.2379812&partnerID=40&md5=82b2b055c2ba914fd62d9f3c3b5ae910,"We propose new methods to solve simulation optimization problems over multidimensional discrete sets. The proposed methods are based on extending the objective function from a discrete domain to a continuous domain and applying stochastic approximation to the extended function. The extension of the objective function is constructed as a piecewise linear interpolation of the original objective function over a particular partition of ℝd. The advantage of the proposed approach lies in that stochastic approximation is applied to the extension, not the original function, over ℝd, so the estimated optimal solution at each iteration of the proposed methods is not restricted to be an integer point. Rather, we are free to approach the optimal solution aggressively by moving toward the direction of the steepest descent, thereby skipping over intervening points, thereby resulting in fast convergence in the early stage of the procedures. We provide a set of sufficient conditions under which the proposed methods guarantee the almost sure (a.s.) convergence to the optimal solution. One of such conditions is the multimodularity or L 〈〉-convexity of the objective function, which arises in various inventory systems and queueing networks with controlled admission. Numerical examples illustrate the effectiveness of the proposed methods in such settings. © 2012 ACM.",Discrete optimization; Simulation-based optimization,Approximation theory; Inventory control; Optimal systems; Optimization; Piecewise linear techniques; Queueing networks; Control of queueing networks; Discrete optimization; Objective functions; Piecewise linear interpolations; Simulation optimization; Simulation-based optimizations; Stochastic approximations; Sufficient conditions; Iterative methods
Constructing nearly orthogonal Latin hypercubes for any nonsaturated run-variable combination,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878488059&doi=10.1145%2f2379810.2379813&partnerID=40&md5=41ed8c697576a17d66aceb5519457369,"We present a new method for constructing nearly orthogonal Latin hypercubes that greatly expands their availability to experimenters. Latin hypercube designs have proven useful for exploring complex, highdimensional computational models, but can be plagued with unacceptable correlations among input variables. To improve upon their effectiveness, many researchers have developed algorithms that generate orthogonal and nearly orthogonal Latin hypercubes. Unfortunately, these methodologies can have strict limitations on the feasible number of experimental runs and variables. To overcome these restrictions, we develop a mixed integer programming algorithm that generates Latin hypercubes with little or no correlation among their columns for most any determinate run-variable combination-including fully saturated designs. Moreover, many designs can be constructed for a specified number of runs and factors-thereby providing experimenters with a choice of several designs. In addition, our algorithm can be used to quickly adapt to changing experimental conditions by augmenting existing designs by adding new variables or generating new designs to accommodate a change in runs. © 2012 ACM.",Correlation; Latin hypercube; Mixed integer program; Multicollinearity; Nearly orthogonal; Optimization; Orthogonal; Saturated; Simulation,Algorithms; Design; Geometry; Integer programming; Optical correlation; Optimization; Regression analysis; Latin hypercube; Mixed-integer programs; Multicollinearity; Nearly orthogonal; Orthogonal; Saturated; Simulation; Availability
Data assimilation using sequential Monte Carlo methods in wildfire spread simulation,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878475927&doi=10.1145%2f2379810.2379816&partnerID=40&md5=bdf959897a28c7b97de933c815e07204,"Assimilating real-time sensor data into large-scale spatial-temporal simulations, such as simulations of wildfires, is a promising technique for improving simulation results. This asks for advanced data assimilation methods that can work with the complex structures and nonlinear behaviors associated with the simulation models. This article presents a data assimilation framework using Sequential Monte Carlo (SMC) methods for wildfire spread simulations. The models and algorithms of the framework are described, and experimental results are provided. This work demonstrates the feasibility of applying SMC methods to data assimilation of wildfire spread simulations. The developed framework can potentially be generalized to other application areas where sophisticated simulation models are used. © 2012 ACM.",Data assimilation; DEVS; Sequential Monte Carlo methods; Wildfire,Computer simulation; Data processing; Fires; Complex structure; Data assimilation; Data assimilation methods; DEVS; Models and algorithms; Nonlinear behavior; Sequential Monte Carlo methods; Wildfire; Monte Carlo methods
Bridging the gap: A standards-based approach to OR/MS distributed simulation,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878502302&doi=10.1145%2f2379810.2379811&partnerID=40&md5=f776646b8878acec54cc515860efd98e,"In Operations Research and Management Science (OR/MS), Discrete Event Simulation (DES) models are typically created using commercial off-the-shelf simulation packages (CSPs) such as AnyLogic™, Arena™, Flexsim™, Simul8™, SLX™, Witness™, and so on. A DES model represents the processes associated with a system of interest. Some models may be composed of submodels running in their own CSPs on different computers linked together over a communications network via distributed simulation software. The creation of a distributed simulation with CSPs is still complex and typically requires a partnership of problem owners, modelers, CSP vendors, and distributed simulation specialists. In an attempt to simplify this development and foster discussion between modelers and technologists, the SISO-STD-006-2010 Standard for COTS Simulation Package Interoperability Reference Models has been developed. The standard makes it possible to capture interoperability capabilities and requirements at a DES modeling level rather than a computing technical level. For example, it allows requirements for entity transfer between models to be clearly specified in DES terms (e.g. the relationship between departure and arrival simulation times and input element (queue, workstation, etc.), buffering rules, and entity priority, instead of using specialist technical terminology. This article explores the motivations for distributed simulation in this area, related work, and the rationale for the standard. The four Types of Interoperability Reference Model described in the standard are discussed and presented (A. Entity Transfer, B. Shared Resource, C. Shared Event, and D. Shared Data Structure). Case studies in healthcare and manufacturing are given to demonstrate how the standard is used in practice. © 2012 ACM.",Commercial-off-the-shelf simulation packages; Healthcare; Manufacturing,Complex networks; Discrete event simulation; Health care; Interoperability; Management science; Manufacture; Models; Operations research; Commercial off the shelves; Communications networks; Cots simulation packages; Distributed simulations; Reference models; Shared data structure; Simulation packages; Technical terminology; Industrial applications
Transparent optimistic synchronization in the High-Level Architecture via time-management conversion,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878475195&doi=10.1145%2f2379810.2379814&partnerID=40&md5=e0ccd955063c4c6016b29f522c79a414,"Distributed simulation allows the treatment of large/complex models by having several interacting simulators running concurrently, each one in charge of a portion of the model. In order to effectively manage integration and interoperability aspects, the standard known as High Level Architecture (HLA) has been developed, which is based on a middleware component known as Run-Time-Infrastructure (RTI). One of the main issues faced by such a standard is synchronization, so that HLA supports both conservative and optimistic approaches. However, technical issues, combined with some peculiarities of the optimistic approach, force most simulators to use the conservative approach. In order to tackle these issues, we present the design and implementation of a Time Management Converter (TiMaC) for HLA based simulation systems. TiMaC is a state machine designed to be transparently interposed between the application layer and the underlying RTI, which performs mapping of the conservative HLA synchronization interface onto the optimistic one. Such a mapping allows transparent optimistic execution (and the related benefits) for simulators originally designed to rely on conservative synchronization. This is achieved without the need to modify the RTI services or alter the HLA standard. An experimental evaluation demonstrating the viability and effectiveness of our proposal is also reported, by integrating our TiMaC implementation with the Georgia Tech B-RTI package and running on it both (A) benchmarks relying on traces from simulated demonstration exercises collected using the Joint Semi-Automated Forces (JSAF) simulation program and (B) a self-federated Personal Communication System simulation application. © 2012 ACM.",Optimistic synchronization,Benchmarking; Middleware; Personal communication systems; Simulators; Conservative approaches; Conservative Synchronization; Design and implementations; Distributed simulations; Experimental evaluation; High level architecture; Middleware components; Optimistic synchronization; Synchronization
Sharpening comparisons via gaussian copulas and semidefinite programming,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878479023&doi=10.1145%2f2379810.2379815&partnerID=40&md5=1ec1de2cb60ba70e97b4a56a2960ae65,"A common problem in operations research involves comparing two system designs through simulation of both systems. The comparison can often be made more accurate through careful control (coupling) of the random numbers that are used in simulating each system, with common random numbers being the standard example. We describe a new approach for coupling the random-number inputs to two systems that involves generating realizations of a Gaussian random vector and then transforming the Gaussian random vector into the desired random-number inputs. We use nonlinear semidefinite programming to select the correlation matrix of the Gaussian random vector, with the goal of sharpening the comparison. © 2012 ACM.",Selection of the best; Variance reduction,Gaussian distribution; Operations research; Random number generation; Common random numbers; Correlation matrix; Gaussian copula; Gaussian random vectors; Nonlinear semidefinite programming; Selection of the best; Semi-definite programming; Variance reductions; Mathematical programming
Bayesian kriging analysis and design for stochastic simulations,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866033686&doi=10.1145%2f2331140.2331145&partnerID=40&md5=ba85610bfaf8467196bcf366dd7ce4c8,"Kriging is an increasingly popular metamodeling tool in simulation due to its flexibility in global fitting and prediction. In the fitting of this metamodel, the parameters are often estimated from the simulation data, which introduces parameter estimation uncertainties into the overall prediction error. Traditional plug-in estimators usually ignore these uncertainties, which can be substantial in stochastic simulations. This typically leads to an underestimation of the total variability and an overconfidence in the results. In this article, a Bayesian metamodeling approach for kriging prediction is proposed for stochastic simulations to more appropriately account for the parameter uncertainties. We derive the predictive distribution under certain assumptions and also provide a general Markov Chain Monte Carlo analysis approach to handle more general assumptions on the parameters and design. Numerical results indicate that the Bayesian approach has better coverage and better predictive variance than a previously proposed modified nugget effect kriging model, especially in cases where the stochastic variability is high. In addition, we further consider the important problem of planning the experimental design. We propose a two-stage design approach that systematically balances the allocation of computing resources to new design points and replication numbers in order to reduce the uncertainties and improve the accuracy of the predictions. © 2012 ACM.",Bayesian statistics; Design of experiments; Kriging; Parameter uncertainty; Stochastic simulation,Bayesian networks; Design; Design of experiments; Image recording; Interpolation; Parameter estimation; Allocation of computing resources; Bayesian approaches; Bayesian statistics; Design approaches; Estimation uncertainties; Global fitting; Kriging; Kriging analysis; Kriging model; Kriging prediction; Markov chain Monte Carlo; Meta model; Metamodeling; New design; Nugget-effect; Numerical results; Parameter uncertainty; Plug-in estimators; Prediction errors; Predictive distributions; Replication number; Simulation data; Stochastic simulations; Stochastic models
Simulating multivariate nonhomogeneous poisson processes using projections,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866025257&doi=10.1145%2f2331140.2331143&partnerID=40&md5=cc6372b7e17c94dbf9f3eac938888d12,"Established techniques for generating an instance of a multivariate NonHomogeneous Poisson Process (NHPP) such as thinning can become highly inefficient as the dimensionality of the process is increased, particularly if the defining intensity (or rate) function has a pronounced peak. To overcome this inefficiency, we propose an alternative approach where one first generates a projection of the NHPP onto a lowerdimensional space, and then extends the generated points to points in the original space by generating from appropriate conditional distributions. One version of this algorithm replaces a high-dimensional problem with a series of one-dimensional problems. Several examples are presented. © 2012 ACM.",Input modeling; Nonhomogeneous Poisson process; Random process generation; Random variate generation; Simulation,Random processes; Input modeling; Non-homogeneous Poisson process; Process generation; Random variates; Simulation; Poisson distribution
Simulating Lévy processes from their characteristic functions and financial applications,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866027310&doi=10.1145%2f2331140.2331142&partnerID=40&md5=052b0418dcc7620c40a057f409a3ece4,"The simulation of a discrete sample path of a Lévy process reduces to simulating from the distribution of a Lévy increment. For a general Lévy process with exponential moments, the inverse transform method proposed in Glasserman and Liu [2010] is reliable and efficient. The values of the cumulative distribution function (cdf) are computed by inverting the characteristic function and tabulated on a uniform grid. The inverse of the cumulative distribution function is obtained by linear interpolation. In this article, we apply a Hilbert transform method for the characteristic function inversion. The Hilbert transform representation for the cdf can be discretized using a simple rule highly accurately. Most importantly, the error estimates admit explicit and computable expressions, which allow us to compute the cdf to any desired accuracy. We present an explicit bound for the estimation bias in terms of the range of the grid where probabilities are tabulated, the step size of the grid, and the approximation error for the probabilities. The bound can be computed from the characteristic function directly and allows one to determine the size and fineness of the grid and numerical parameters for evaluating the Hilbert transforms for any given bias tolerance level in one-dimensional problems. For multidimensional problems, we present a procedure for selecting the grid and the numerical parameters that is shown to converge theoretically and works well practically. The inverse transform method is attractive not only for Lévy processes that are otherwise not easy to simulate, but also for processes with special structures that could be simulated in different ways. The method is very fast and accurate when combined with quasi-Monte Carlo schemes and variance reduction techniques. The main results we derived are not limited to Lévy processes and can be applied to simulating from tabulated cumulative distribution functions in general and characteristic functions in our analytic class in particular. © 2012 ACM.",Analytic characteristic function; Control variates; Hilbert transform; Inverse transform method; Lévy process; Options pricing; Randomized quasi-Monte Carlo method,Communication channels (information theory); Hilbert spaces; Inverse transforms; Monte Carlo methods; Characteristic functions; Control variates; Hilbert transform; Inverse transform method; Options pricing; Quasi Monte Carlo methods; Fading channels
A framework for selecting a selection procedure,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866039133&doi=10.1145%2f2331140.2331144&partnerID=40&md5=c11a8168a20cb8a8fcb8659e131406e7,"For many discrete simulation optimization applications, it is often difficult to decide which Ranking and Selection (R&S) procedure to use. To efficiently compare R&S procedures, we present a three-layer performance evaluation process. We show that the two most popular performance formulations, namely the Bayesian formulation and the indifference zone formulation, have a common representation analogous to convex risk measures used in mathematical finance. We then specify how a decision maker can impose a performance requirement on R&S procedures that is more adequate for her risk attitude than the indifference zone or the Bayesian performance requirements. Such a performance requirement partitions the space of R&S procedures into acceptable and nonacceptable procedures. The minimal computational budget required for a procedure to become acceptable introduces an easy-to-interpret preference order on the set of R&S policies. We demonstrate with a numerical example how the introduced framework can be used to guide the choice of selection procedure in practice. © 2012 ACM.",Acceptable procedures; Bayesian; Convex risk measures; Decision theory; Indifference zone; Ranking and selection; Robustness analysis,Computer simulation; Decision theory; Acceptable procedures; Bayesian; Convex risk measures; Indifference zone; Ranking and selection; Robustness analysis; Computer applications
On lyapunov inequalities and subsolutions for efficient importance sampling,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866013800&doi=10.1145%2f2331140.2331141&partnerID=40&md5=d988881fcc0142d372a604ba72f570e2,"In this article we explain some connections between Lyapunov methods and subsolutions of an associated Isaacs equation for the design of efficient importance sampling schemes. As we shall see, subsolutions can be derived by taking an appropriate limit of an associated Lyapunov inequality. They have been recently proposed in several works of Dupuis, Wang, and others and applied to address several important problems in rare-event simulation. Lyapunov inequalities have been used for testing the efficiency of state-dependent importance sampling schemes in heavy-tailed or discrete settings in a variety of works by Blanchet, Glynn, and others. While subsolutions provide an analytic criterion for the construction of efficient samplers, Lyapunov inequalities are useful for finding more precise information, in the form of bounds, for the behavior of the coefficient of variation of the associated importance sampling estimator in the prelimit. In addition, Lyapunov inequalities provide insight into the various mollification procedures that are often required in constructing associated subsolutions. Our aim is to demonstrate that applying Lyapunov inequalities for verification of efficiency can help both guide the selection of various mollification parameters and sharpen the information on the efficiency gain induced by the sampler. © 2012 ACM.",Importance sampling; Isaacs equation; Lyapunov inequalities; Rare events; Variance reduction,Efficiency; Importance sampling; Coefficient of variation; Efficiency gain; Heavy-tailed; Isaacs equation; Lyapunov inequality; Rare event; Rare event simulation; State-dependent; Variance reductions; Lyapunov methods
The effects of common random numbers on stochastic kriging metamodels,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859471547&doi=10.1145%2f2133390.2133391&partnerID=40&md5=7fc7bb856408120a5fc11a2cd709e7b2,"Ankenman et al. introduced stochastic kriging as a metamodeling tool for representing stochastic simulation response surfaces, and employed a very simple example to suggest that the use of Common Random Numbers (CRN) degrades the capability of stochastic kriging to predict the true response surface. In this article we undertake an in-depth analysis of the interaction between CRN and stochastic kriging by analyzing a richer collection of models; in particular, we consider stochastic kriging models with a linear trend term. We also perform an empirical study of the effect of CRN on stochastic kriging. We also consider the effect of CRN on metamodel parameter estimation and response-surface gradient estimation, as well as response-surface prediction. In brief, we confirm that CRN is detrimental to prediction, but show that it leads to better estimation of slope parameters and superior gradient estimation compared to independent simulation. © 2012 ACM 1049-3301/2012/03-ART7 $10.00.",Experimentation,Estimation; Forecasting; Interpolation; Parameter estimation; Random number generation; Stochastic systems; Surface properties; Common random numbers; Empirical studies; Experimentation; Gradient estimation; In-depth analysis; Kriging; Kriging metamodels; Kriging model; Meta model; Metamodeling; Response surface; Slope parameter; Stochastic simulations; Stochastic models
Evolutionary optimization of low-discrepancy sequences,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859475872&doi=10.1145%2f2133390.2133393&partnerID=40&md5=404c26eb0bf3924ab44b73da514abf16,"Low-discrepancy sequences provide a way to generate quasi-random numbers of high dimensionality with a very high level of uniformity. The nearly orthogonal Latin hypercube and the generalized Halton sequence are two popular methods when it comes to generate low-discrepancy sequences. In this article, we propose to use evolutionary algorithms in order to find optimized solutions to the combinatorial problem of configuring generators of these sequences. Experimental results show that the optimized sequence generators behave at least as well as generators from the literature for the Halton sequence and significantly better for the nearly orthogonal Latin hypercube. © 2012 ACM 1049-3301/2012/03- ART9 $10.00.",Algorithms; Performance,Algorithms; Optimization; Combinatorial problem; Evolutionary optimizations; Halton sequences; High dimensionality; Latin hypercube; Low-discrepancy sequences; Optimized solutions; Performance; Quasi-random; Sequence generators; Random number generation
Confidence intervals for quantiles when applying variance-reduction techniques,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859467885&doi=10.1145%2f2133390.2133394&partnerID=40&md5=019c029f986214d89851cb94d8bc806a,"Quantiles, which are also known as values-at-risk in finance, frequently arise in practice as measures of risk. This article develops asymptotically valid confidence intervals for quantiles estimated via simulation using variance-reduction techniques (VRTs). We establish our results within a general framework for VRTs, which we show includes importance sampling, stratified sampling, antithetic variates, and control variates. Our method for verifying asymptotic validity is to first demonstrate that a quantile estimator obtained via a VRT within our framework satisfies a Bahadur-Ghosh representation. We then exploit this to show that the quantile estimator obeys a central limit theorem (CLT) and to develop a consistent estimator for the variance constant appearing in the CLT, which enables us to construct a confidence interval. We provide explicit formulae for the estimators for each of the VRTs considered. © 2012 ACM 1049-3301/2012/03-ART10 $10.00.",Performance; Theory,Estimation; Antithetic variates; Central Limit Theorem; Confidence interval; Consistent estimators; Control variates; Explicit formula; Performance; Quantile estimators; Stratified sampling; Theory; Variance constant; Variance reduction techniques; Financial data processing
Fast synthesis of persistent fractional Brownian motion,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859475773&doi=10.1145%2f2133390.2133395&partnerID=40&md5=49b46c2d6815a63bbba70c005c671168,"Due to the relevance of self-similarity analysis in several research areas, there is an increased interest in methods to generate realizations of self-similar processes, namely in the ones capable of simulating long-range dependence. This article describes a new algorithm to approximate persistent fractional Brownian motions with a predefined Hurst parameter. The algorithm presents a computational complexity of O(n) and generates sequences with n (n ε N) values with a small multiple of log 2(n) variables. Because it operates in a sequential manner, the algorithm is suitable for simulations demanding real-time operation. A network traffic simulator is presented as one of its possible applications. © 2012 ACM 1049-3301/2012/03-ART11 $10.00.",Algorithms; Performance; Theory; Verification,Routers; Vehicle actuated signals; Verification; Fractional brownian motion; Hurst parameter; Long range dependence; Network traffic; Performance; Real-time operation; Research areas; Self-similar process; Self-similarities; Small multiples; Theory; Algorithms
On importance sampling with mixtures for random walks with heavy tails,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859453675&doi=10.1145%2f2133390.2133392&partnerID=40&md5=6b0d6f85f2f902c128541c8caadc4169,"State-dependent importance sampling algorithms based on mixtures are considered. The algorithms are designed to compute tail probabilities of a heavy-tailed random walk. The increments of the random walk are assumed to have a regularly varying distribution. Sufficient conditions for obtaining bounded relative error are presented for rather general mixture algorithms. Two new examples, called the generalized Pareto mixture and the scaling mixture, are introduced. Both examples have good asymptotic properties and, in contrast to some of the existing algorithms, they are very easy to implement. Their performance is illustrated by numerical experiments. Finally, it is proved that mixture algorithms of this kind can be designed to have vanishing relative error. © 2012 ACM 1049-3301/2012/03-ART8 $10.00.",Algorithms; Performance; Theory,Algorithms; Importance sampling; Random processes; Asymptotic properties; Bounded relative error; Heavy-tailed; Heavy-tails; Mixture algorithm; Numerical experiments; Performance; Random Walk; Relative errors; State-dependent; Sufficient conditions; Tail probability; Theory; Mixtures
On simulating a class of Bernstein polynomials,2012,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859454635&doi=10.1145%2f2133390.2133396&partnerID=40&md5=9a3eade1f7788963e987d55b21bb7e5a,"Given a black box that generates independent Bernoulli samples with an unknown bias p, we consider the problem of simulating a Bernoulli random variable with bias f (p) (where f is a given function) using a finite (computable in advance) number of independent Bernoulli samples from the black box. We show that this is possible if and only if f is a Bernstein polynomial with coefficients between 0 and 1, and we explicitly give the algorithm. Our results differ from Keane and O'Brien [1994] in that our goal is more modest/stringent, since we are considering algorithms that use a finite number of samples as opposed to allowing a random number (such as in acceptance rejection algorithms). © 2012 ACM 1049-3301/2012/03-ART12 ©10.00.",Algorithms; Performance,Computer applications; Computer simulation; Bernoulli; Bernoulli random variables; Bernstein polynomial; Black boxes; Finite number; O'Brien; Performance; Random Numbers; Rejection algorithm; Algorithms
Reversible parallel discrete event formulation of a TLM-based radio signal propagation model,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857184509&doi=10.1145%2f2043635.2043639&partnerID=40&md5=f1fefc82b0a2998a2a077a34e53ead05,"Radio signal strength estimation is essential in many applications, including the design of military radio communications and industrial wireless installations. For scenarios with large or richly featured geographical volumes, parallel processing is required to meet the memory and computation time demands. Here, we present a scalable and efficient parallel execution of the sequential model for radio signal propagation recently developed by Nutaro et al. [2008]. Starting with that model, we (a) provide a vector-based reformulation that has significantly lower computational overhead for event handling, (b) develop a parallel decomposition approach that is amenable to reversibility with minimal computational overheads, (c) present a framework for transparently mapping the conservative time-stepped model into an optimistic parallel discrete event execution, (d) present a new reversible method, along with its analysis and implementation, for inverting the vector-based event model to be executed in an optimistic parallel style of execution, and (e) present performance results from implementation on Cray XT platforms. We demonstrate scalability, with the largest runs tested on up to 127,500 cores of a Cray XT5, enabling simulation of larger scenarios and with faster execution than reported before on the radio propagation model. This also represents the first successful demonstration of the ability to efficiently map a conservative time-stepped model to an optimistic discrete-event execution. © 2011 ACM.",Electromagnetic wave propagation; Parallel simulation; Radio signal; Reverse computation; Time warp; Transmission line matrix,Electromagnetic wave propagation; Military applications; Radio waves; Parallel simulations; Radio signal; Reverse computation; Time Warp; Transmission line matrix; Discrete event simulation
A distributed platform for global-scale agent-based models of disease transmission,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857147980&doi=10.1145%2f2043635.2043637&partnerID=40&md5=0863c58fee83e139965778351677e06e,"The Global-Scale Agent Model (GSAM) is presented. The GSAM is a high-performance distributed platform for agent-based epidemic modeling capable of simulating a disease outbreak in a population of several billion agents. It is unprecedented in its scale, its speed, and its use of Java. Solutions to multiple challenges inherent in distributing massive agent-based models are presented. Communication, synchronization, and memory usage are among the topics covered in detail. The memory usage discussion is Java specific. However, the communication and synchronization discussions apply broadly. We provide benchmarks illustrating the GSAM's speed and scalability. © 2011 ACM.",Agent behavior; Agent-based modeling; Epidemiology,Communication; Epidemiology; Agent based; Agent behavior; Agent model; Agent-based model; Agent-based modeling; Disease outbreaks; Disease transmission; Distributed platforms; Epidemic modeling; Memory usage; Computational methods
The effect of robust decisions on the cost of uncertainty in military airlift operations,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857144469&doi=10.1145%2f2043635.2043636&partnerID=40&md5=22881e7abafff26d502499bdd7eee39f,"There are a number of sources of randomness that arise in military airlift operations. However, the cost of uncertainty can be difficult to estimate, and is easy to overestimate if we use simplistic decision rules. Using data from Canadian military airlift operations, we study the effect of uncertainty in customer demands as well as aircraft failures, on the overall cost. The system is first analyzed using the types of myopic decision rules widely used in the research literature. The performance of the myopic policy is then compared to the results obtained using robust decisions that account for the uncertainty of future events. These are obtained by modeling the problem as a dynamic program, and solving Bellman's equations using approximate dynamic programming. The experiments show that even approximate solutions to Bellman's equations produce decisions that reduce the cost of uncertainty. © 2011 ACM.",Approximate dynamic programming; Military logistics; Robust control,Costs; Robust control; Uncertainty analysis; Aircraft failures; Approximate dynamic programming; Approximate solution; Bellman's equations; Canadian military; Customer demands; Decision rules; Dynamic programs; Military logistics; Myopic policy; Overall costs; Robust decisions; Cost benefit analysis
A decision support system for placement of intrusion detection and prevention devices in large-scale networks,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857160340&doi=10.1145%2f2043635.2043640&partnerID=40&md5=0f89c58898b668cbb0f88edffd949577,"This article describes an innovative Decision Support System (DSS) for Placement of Intrusion Detection and Prevention Systems (PIDPS) in large-scale communication networks. PIDPS is intended to support network security personnel in optimizing the placement and configuration of malware filtering and monitoring devices within Network Service Providers' (NSP) infrastructure, and enterprise communication networks. PIDPS meshes innovative and state-of-the-art mechanisms borrowed from the domains of graph theory, epidemic modeling, and network simulation. Scalable network exploitation models enable to define the communication patterns induced by network users (thereby establishing a virtual overlay network), and parallel attack models enable a PIDPS user to define various interdependent network attacks such as: Internet worms, Trojans horses, Denial of Service (DoS) attacks, and others. PIDPS incorporates a set of deployment strategies (employing graph-theoretic centrality measures) in order to facilitate intelligent placement of filtering and monitoring devices; as well as a dedicated network simulator in order to evaluate the various deployments. Experiments with PIDPS indicate that incorporating knowledge on the overlay network (network exploitation patterns) into the placement and configuration of malware filtering and monitoring devices substantially improves the effectiveness of intrusion detection and prevention systems in NSP and enterprise networks. © 2011 ACM.",Decision support systems; Intrusion detection; Overlay networks,Artificial intelligence; Computer worms; Decision support systems; Graph theory; Industry; Intrusion detection; Overlay networks; Attack model; Centrality measures; Communication pattern; Dedicated networks; Denial of service attacks; Deployment strategy; Enterprise communications; Enterprise networks; Epidemic modeling; Graph-theoretic; Internet worm; Intrusion detection and prevention; Intrusion detection and prevention systems; Large-scale network; Malwares; Monitoring device; Network attack; Network service providers; Network simulation; Network users; Scalable networks; Security personnel; Support networks; Trojans; Virtual overlay; Computer simulation
Sampling exponentially tilted stable distributions,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857166975&doi=10.1145%2f2043635.2043638&partnerID=40&md5=3adfb4c515eabf8f8fee62f8edc0309c,"Several algorithms for sampling exponentially tilted positive stable distributions have recently been suggested. Three of them are known as exact methods, that is, neither do they rely on approximations nor on numerically critical procedures. One of these algorithms is outperformed by another one uniformly over all parameters. The remaining two algorithms are based on different ideas and both have their advantages. After a brief overview of sampling algorithms for exponentially tilted positive stable distributions, the two algorithms are compared. A rule is derived when to apply which for sampling these distributions. © 2011 ACM.",Exponentially tilted stable distributions; Laplace-Stieltjes transforms; Sampling algorithms; Stable distributions,Learning algorithms; Exact methods; Laplace-Stieltjes transform; Positive stable; Sampling algorithm; Sampling algorithms; Stable distributions; Laplace transforms
A new algorithm for simulating wildfire spread through cellular automata,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857153703&doi=10.1145%2f2043635.2043641&partnerID=40&md5=6049dbded3ae9d613a0a37627ad8be81,"Cell-based methods for simulating wildfires can be computationally more efficient than techniques based on the fire perimeter expansion. In spite of this, their success has been limited by the distortions that plague the simulated shapes. This article presents a novel algorithmfor wildfire simulation through Cellular Automata (CA), which is able to effectively mitigate the problem of distorted fire shapes. Such a result is obtained allowing spread directions that are not constrained to the few angles imposed by the lattice of cells and the neighborhood size. The characteristics of the proposed algorithm are empirically investigated under homogeneous conditions through some comparisons with the outcomes of a typical CA-based simulator. Also, using two significant heterogeneous landscapes, a comparison with the vector-based simulator FARSITE is discussed. According to the results of this study, the proposed approach performs significantly better, in terms of accuracy, than the CA taken as reference. In addition, at a far less computational cost, it provides burned regions that are equivalent, for practical purposes, to those given by FARSITE. © 2011 ACM.",Cellular automata; Wildfire spread,Algorithms; Fires; Cell-based; Computational costs; Heterogeneous landscapes; Homogeneous conditions; Neighborhood size; Wildfire simulation; Wildfire spread; Cellular automata
Introduction to special issue on healthcare modeling and simulation,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053041652&doi=10.1145%2f2000494.2000495&partnerID=40&md5=b9bb3766b4c869b468278418080bb6c6,[No abstract available],,
A modeling framework that combines Markov models and discrete-event simulation for stroke patient care,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052982831&doi=10.1145%2f2000494.2000498&partnerID=40&md5=ec5eb930c28d78f6d42ad88d037ea1fc,"Stroke disease places a heavy burden on society, incurring long periods of hospital and community care. Also stroke is a highly complex disease with heterogeneous outcomes and multiple strategies for therapy and care. In this article we develop a modeling framework that clusters patients with respect to their length of stay (LOS); phase-type models are then used to describe patient flows for each cluster. In most cases, there are multiple outcomes, such as discharge to normal residence, nursing home, or death. We therefore derive a novel analytical model for the distribution of LOS in such situations. A model of the whole care system is developed, based on Poisson admissions to hospital, and results obtained for expected numbers in different states of the system at any time. We can thus describe the whole integrated system of stroke patient care, which will facilitate planning of services. We also use the basic model to build a discrete-event simulation, which incorporates back-up queues to model delayed discharge. Based on stroke patients' data from the Belfast City Hospital, various scenarios are explored with a focus on the potential efficiency gains if LOS, prior to discharge to a private nursing home, can be reduced. Predictions for bed occupancy are also provided. The overall modeling framework characterizes the behavior of stroke patient populations, with a focus on integrated system-wide planning, encompassing hospital and community services. Within this general framework we can develop either analytic or simulation models that take account of patient heterogeneity and multiple care options. © 2011 ACM.",Design; Theory,Design; Discrete event simulation; Disease control; Hospitals; Integrated control; Markov processes; Nursing; Analytical model; Basic models; Bed-occupancy; Community care; Community services; Complex disease; Discrete events; Integrated systems; Length of stay; Markov model; Modeling frameworks; Multiple strategy; Nursing homes; Patient flow; Potential efficiency; Simulation model; Stroke patients; Theory; Mathematical models
Impacts of radio-identification on cryo-conservation centers,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052577109&doi=10.1145%2f2000494.2000500&partnerID=40&md5=80b46b51e6a8f8735d97151aead55e85,"This article deals with the use of discrete-event simulation as a decision support tool for estimating the impact of Radio Frequency IDentification (RFID) technologies on processes and activities of biological sample storage areas (called biobanks). We first give a detailed description of biobank flows and identify subprocesses improved using RFID technologies. Several indicators, such as inventory reliability and human resource utilization, are compared and discussed for different scenarios involving the use of different RFID technologies. A special emphasis is put on the so-called rewarehousing activity, which RFID makes possible and which consists in reassigning tubes to empty places when boxes are emptied. For this particular activity, optimization algorithms are developed and embedded in the simulator. This study shows the potential use of RFID in biobanks and the value of simulation for estimating and optimizing its introduction in such complex socio-technical systems. © 2011 ACM.",Algorithms; Experimentation; Performance,Algorithms; Cryptography; Decision support systems; Optimization; Biobanks; Biological samples; Decision support tools; Discrete events; Experimentation; Optimization algorithms; Performance; Radio frequency identification technology; RFID Technology; Sociotechnical systems; Radio frequency identification (RFID)
"Simulation-based models of emergency departments: Operational, tactical, and strategic staffing",2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053040473&doi=10.1145%2f2000494.2000497&partnerID=40&md5=b669419908b5a2c8ef5f242c64e59845,"The Emergency Department (ED) of a modern hospitalisahighly complex system that gives rise to numerous managerial challenges. It spans the full spectrum of operational, clinical, and financial perspectives, over varying horizons: operational - a few hours or days ahead; tactical - weeks or a few months ahead; and strategic, which involves planning on monthly and yearly scales. Simulation offers a natural framework within which to address these challenges, as realistic ED models are typically intractable analytically. We apply a general and flexible ED simulator to address several significant problems that arose in a large Israeli hospital. The article focuses mainly, but not exclusively, on workforce staffing problems over these time horizons. First, we demonstrate that our simulation model can support real-time control, which enables short-term prediction and operational planning (physician and nurse staffing) for several hours or days ahead. To this end, we present a novel simulation-based technique that implements the concept of offered-load and discover that it performs better than a common alternative. Then we evaluate ED staff scheduling that adjusts for midterm changes (tactical horizon, several weeks or months ahead). Finally, we analyze the design and staffing problems that arose from physical relocation of the ED (strategic yearly horizon). Application of the simulation-based approach led to the implementation of our design and staffing recommendations. © 2011 ACM.",Management; Measurement; Performance,Emergency rooms; Management; Measurements; Personnel selection; Real time control; Emergency departments; Full spectrum; Operational planning; Performance; Short term prediction; Simulation model; Simulation-based; Staff scheduling; Time horizons; Computer simulation
DGHPSIM: Generic simulation of hospital performance,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053000573&doi=10.1145%2f2000494.2000496&partnerID=40&md5=e31d844dd23a4f4182899f14ba74b7c4,"The British National Health Service (NHS) has a performance management framework that aims to guarantee short waiting times for patients by including mandatory targets for hospitals. DGHPSim is a suite of four components that simulates the activities of an NHS general hospital to show the effect of different policies on waiting times in these hospitals. DGHPSim has a generic structure that is used to simulate a particular hospital by employing data appropriate to that hospital from available data sets. Two of the components of DGHPSim, the accident and emergency simulator and the outpatient simulator, may be used independently as stand-alone simulators of these hospital functions. The DGHPSim suite incorporates a novel way of simulating the multitasking behavior of clinicians and uses transition matrices, extracted from standard datasets, to represent the states through which patients pass and the wards in which they may be treated. As a whole, the DGHPSim suite may be used to investigate improvement options before their implementation or to investigate how a hospital has improved its performance. We show how DGHPSim is used to investigate reported performance improvements in an English general hospital. © 2011 ACM.",Management; Performance,Management; Patient treatment; Simulators; Data sets; Generic simulation; Generic structure; National health services; Performance; Performance improvements; Performance management; Transition matrices; Waiting time; Hospitals
Incorporating household structure into a discrete-event simulation model of tuberculosis and HIV,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053007312&doi=10.1145%2f2000494.2000499&partnerID=40&md5=4c1dbeeb423203985ec8968590571953,"Human immunodeficiency virus (HIV) increases the risks of developing tuberculosis (TB) disease following infection, and speeds up disease progression. This has had a devastating effect on TB epidemics in sub-Saharan Africa, where incidence rates have more than trebled in the past twenty years. Current control methods for TB disease have failed to keep pace with this growth, and there is an urgent need to find TB control strategies that are effective in high-HIV prevalent settings. This article describes a discrete-event simulation model of endemic TB that includes the effects of HIV and of household structure on the transmission dynamics of TB. Incorporating a social structure allows us to compare the effectiveness of contact-tracing interventions with case-finding targeted at high risk groups. We describe the modeling of the household structure in some detail, as this has applications to the modeling of other infectious diseases. © 2011 ACM.",Experimentation,Discrete event simulation; Disease control; Viruses; Control strategies; Current-control method; Devastating effects; Discrete-event simulation model; Disease progression; Experimentation; Household structures; Human immunodeficiency virus; Incidence rate; Infectious disease; Social structure; Sub-Saharan Africa; Transmission dynamics; Diseases
"SubsetTrio: An evolutionary, geometric, and statistical benchmark subsetting framework",2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952550796&doi=10.1145%2f1921598.1921605&partnerID=40&md5=49ff3e5ccb31c342c8b39e86ebda715b,"Motivated by excessively high benchmarking efforts caused by a rapidly expanding design space, increasing system complexity, and prevailing practices based on ad-hoc and subjective schemes, this article seeks to enhance architecture exploration and evaluation efficiency by strategically integrating a genetic algorithm, 3-D geometrical rendering, and multivariate statistical analysis into one unified methodology framework-SubsetTrio-capable of subsetting any given benchmark suite based on its inherent workload characteristics, desired workload space coverage, and the total execution time intended by the user. By encoding both representativity (i.e., workload space coverage represented by the volume of the convex hull of benchmarks) and efficiency (i.e., total run time) as a co-optimization objective of a survival-of-the- fittest evolutionary algorithm, we can systematically determine a globally ""fittest"" (i.e., most representative and efficient) benchmark subset according to the workload space coverage threshold specified by the user. We demonstrate the usage, efficacy, and efficiency of the proposed technique by conducting a thorough case study on the SPEC benchmark suite, and evaluate its validity based on 50 commercial computer systems. Compared to the state-of-the-art statistical subsetting approach based on the Principal Component Analysis (PCA), SubsetTrio could select a significantly more time-efficient subset, while covering the same or higher workload space. © 2011 ACM.",Benchmark; Convex hull; Evolutionary process; Fitness; Principal component analysis; Subsetting,Computational geometry; Evolutionary algorithms; Multivariant analysis; Benchmark; Convex hull; Evolutionary process; Fitness; Subsetting; Principal component analysis
A dynamic sort-based DDM matching algorithm for HLA applications,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952548386&doi=10.1145%2f1921598.1921601&partnerID=40&md5=310e337ff3ed750f37fb5afa14fbbf10,"Simulation is a low-cost and safe alternative to solve complex problems in various areas. To promote reuse and interoperability of simulation applications and link geographically dispersed simulation components, distributed simulation was introduced. The High-Level Architecture (HLA) is the IEEE standard for distributed simulation. To optimize communication efficiency between simulation components, HLA defines a Data Distribution Management (DDM) service group for filtering out unnecessary data exchange. It relies on the computation of overlap between update and subscription regions, which is called ""matching"". There are many existing matching algorithms, among which a sort-based approach improves efficiency by sorting region bounds before the actual matching process, and is found to outperform other existing matching algorithms in many situations. However, the existing algorithm performs matching for all regions in one round and cannot dynamically deal with a selective region modification without processing all the regions once again. Realizing that in many spatial applications, only a small subset of all regions are actually modified in each time step, this article proposes a dynamic sort-based matching algorithm to deal with this efficiently. Theoretical analysis has been carried out for the proposed algorithm and experimental results show that the proposed algorithm has significantly better performance than major existing matching algorithms at dynamic matching. © 2011 ACM.",Data distribution management; Distributed simulation; High-level architecture; Matching algorithm,Information management; Communication efficiency; Complex problems; Data distribution management; Data exchange; Distributed simulation; Distributed simulations; Dynamic matching; High-level architecture; IEEE standards; Matching algorithm; Matching process; Service groups; Simulation applications; Simulation components; Sort-based matching algorithms; Spatial applications; Time step; Algorithms
Stochastic approximation algorithms for constrained optimization via simulation,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952548696&doi=10.1145%2f1921598.1921599&partnerID=40&md5=ef9725ad05d5f197dbb562b0f2d6cd0a,"We develop four algorithms for simulation-based optimization under multiple inequality constraints. Both the cost and the constraint functions are considered to be long-run averages of certain state-dependent single-stage functions. We pose the problem in the simulation optimization framework by using the Lagrange multiplier method. Two of our algorithms estimate only the gradient of the Lagrangian, while the other two estimate both the gradient and the Hessian of it. In the process, we also develop various new estimators for the gradient and Hessian. All our algorithms use two simulations each. Two of these algorithms are based on the smoothed functional (SF) technique, while the other two are based on the simultaneous perturbation stochastic approximation (SPSA) method. We prove the convergence of our algorithms and show numerical experiments on a setting involving an open Jackson network. The Newton-based SF algorithm is seen to show the best overall performance. © 2011 ACM.",Inequality constraints; Lagrange multiplier; SF estimates; Simulation-based constrained optimization; SPSA estimates; Stochastic approximation,Approximation theory; Coercive force; Constrained optimization; Convergence of numerical methods; Estimation; Frequency multiplying circuits; Lagrange multipliers; Stochastic systems; Inequality constraint; Lagrange; SF estimates; Simulation-based; SPSA estimates; Stochastic approximations; Approximation algorithms
An analysis of queuing network simulation using GPU-based hardware acceleration,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952564755&doi=10.1145%2f1921598.1921602&partnerID=40&md5=4a8b5f72fe00be99592fb39ba7a569cc,"Queuing networks are used widely in computer simulation studies. Examples of queuing networks can be found in areas such as the supply chains, manufacturing work flow, and internet routing. If the networks are fairly small in size and complexity, it is possible to create discrete event simulations of the networks without incurring significant delays in analyzing the system. However, as the networks grow in size, such analysis can be time consuming, and thus require more expensive parallel processing computers or clusters. We have constructed a set of tools that allow the analyst to simulate queuing networks in parallel, using the fairly inexpensive and commonly available graphics processing units (GPUs) found in most recent computing platforms. We present an analysis of a GPU-based algorithm, describing benefits and issues with the GPU approach. The algorithm clusters events, achieving speedup at the expense of an approximation error which grows as the cluster size increases. We were able to achieve 10-× speedup using our approach with a small error in a specific implementation of a synthetic closed queuing network simulation. This error can be mitigated, based on error analysis trends, obtaining reasonably accurate output statistics. The experimental results of the mobile ad hoc network simulation show that errors occur only in the time-dependent output statistics. © 2011 ACM.",Accuracy; CUDA; GPU; Parallel discrete event simulation; Queuing network; SIMD; Time interval,Approximation algorithms; Discrete event simulation; Error analysis; Error statistics; Program processors; Queueing networks; Queueing theory; Supply chains; Accuracy; CUDA; GPU; Parallel discrete event simulations; Queuing network; SIMD; Time interval; Mobile ad hoc networks
"The stochastic root-finding problem: Overview, solutions, and open questions",2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952563147&doi=10.1145%2f1921598.1921603&partnerID=40&md5=5d7ec5649944fdab8b87729c3e6d5a1a,"The stochastic root-finding problem (SRFP) is that of finding the zero(s) of a vector function, that is, solving a nonlinear system of equations when the function is expressed implicitly through a stochastic simulation. SRFPs are equivalently expressed as stochastic fixed-point problems, where the underlying function is expressed implicitly via a noisy simulation. After motivating SRFPs using a few examples, we review available methods to solve such problems on constrained Euclidean spaces. We present the current literature as three broad categories, and detail the basic theoretical results that are currently known in each of the categories. With a view towards helping the practitioner, we discuss speciic variations in their implementable form, and provide references to computer code when easily available. Finally, we list a few questions that are worthwhile research pursuits from the standpoint of advancing our knowledge of the theoretical underpinnings and the implementation aspects of solutions to SRFPs. © 2011 ACM.",Sample-average approximation; Stochastic approximation; Stochastic root finding,Approximation theory; Nonlinear equations; Stochastic models; Stochastic systems; Computer codes; Euclidean spaces; Fixed-point problem; Implementation aspects; Sample-average approximation; Stochastic approximation; Stochastic root finding; Stochastic root-finding problem; Stochastic simulations; System of equations; Theoretical result; Vector functions; Problem solving
Modeling and simulation of pedestrian behaviors in crowded places,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952557048&doi=10.1145%2f1921598.1921604&partnerID=40&md5=dc4b64518642391c67446edd30099265,"Pedestrian simulation has many applications in computer games, military simulations, and animation systems. A realistic pedestrian simulation requires a realistic pedestrian behavioral model that takes into account the various behavioral aspects of a real pedestrian. In this article, we describe our work on such a model, which aims to generate human-like pedestrian behaviors. To this end, various important factors in a real-pedestrian's decision-making process are considered in our model. These factors include a pedestrian's sensory attention, memory, and navigational behaviors. In particular, a two-level navigation model is proposed to generate realistic navigational behavior. As a result, our pedestrian model is able to generate various realistic behaviors such as overtaking, waiting, side-stepping and lane-forming in a crowded area. The simulated pedestrians are also able to navigate through complex environment, given an abstract map of the environment. © 2011 ACM.",Autonomous agents; Behaviorial modeling; Pedestrian simulation,Animation; Autonomous agents; Behavioral research; Decision making; Fuzzy clustering; Military applications; Navigation; Animation systems; Behavioral model; Behaviorial modeling; Complex environments; Computer game; Decision making process; Military simulation; Modeling and simulation; Navigation model; Pedestrian behavior; Pedestrian models; Pedestrian simulation; Computer simulation
An analysis of a variation of hit-and-run for uniform sampling from general regions,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952557210&doi=10.1145%2f1921598.1921600&partnerID=40&md5=3b064b77a2f1074d207e624bc50fb059,"Hit-and-run, a class of MCMC samplers that converges to general multivariate distributions, is known to be unique in its ability to mix fast for uniform distributions over convex bodies. In particular, its rate of convergence to a uniform distribution is of a low order polynomial in the dimension. However, when the body of interest is difficult to sample from, typically a hyperrectangle is introduced that encloses the original body, and a one-dimensional acceptance/rejection is performed. The fast mixing analysis of hit-and-run does not account for this one-dimensional sampling that is often needed for implementation of the algorithm. Here we show that the effect of the size of the hyperrectangle on the efficiency of the algorithm is only a linear scaling effect. We also introduce a variation of hit-and-run that accelerates the sampler and demonstrate its capability through a computational study. © 2011 ACM.",Hit-and-run algorithm; Markov chain Monte Carlo; Uniform sampling from a convex body,Algorithms; Approximation theory; Markov processes; Monte Carlo methods; One dimensional; Acceptance/rejection; Computational studies; Convex body; Hit-and-run algorithm; Hyperrectangles; Linear scaling; Low-order polynomials; Markov chain Monte Carlo; Mixing analysis; Multivariate distributions; Rate of convergence; Uniform distribution; Uniform sampling; Uniform sampling from a convex body; Sampling
Forwarding devices: From measurements to simulations,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951758017&doi=10.1145%2f1899396.1899400&partnerID=40&md5=e849f176539ae7f172ad9ed353ea2490,"Most popular simulation and emulation tools use high-level models of forwarding behavior in switches and routers, and give little guidance on setting model parameters such as buffer sizes. Thus, a myriad of papers report results that are highly sensitive to the forwarding model or buffer size used. Incorrect conclusions are often drawn from these results about transport or application protocol performance, service provisioning, or vulnerability to attacks. In this article, we argue that measurement-based models for routers and other forwarding devices are necessary. We devise such a model and validate it with measurements from three types of Cisco routers and one Juniper router, under varying traffic conditions. The structure of our model is device-independent, but the model uses device specific parameters. The compactness of the parameters and simplicity of the model make it versatile for high-fidelity simulations that preserve simulation scalability. We construct a profiler to infer the parameters within a few hours. Our results indicate that our model approximates different types of routers significantly better than the default ns-2 simulator models. The results also indicate that queue characteristics vary dramatically among the devices we measure, and that backplane contention can be a factor. © 2011 ACM.",Emulation; Router benchmarking; Router modeling; Simulation,Benchmarking; Equipment; Routers; Structure (composition); Application protocols; Backplanes; Buffer sizes; Emulation; High-fidelity simulations; High-level models; Highly sensitive; Measurement based model; Model approximates; Model parameters; NS-2 simulators; Router benchmarking; Router modeling; Service provisioning; Simulation; Traffic conditions; Computer simulation
A variant of importance splitting for rare event estimation: Fixed number of successes,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951731457&doi=10.1145%2f1899396.1899401&partnerID=40&md5=d88882670ef122f2c1586917d89bda94,"Importance splitting is a simulation technique to estimate very small entrance probabilities for Markov processes by splitting sample paths at various stages before reaching the set of interest. This can be done in many ways, yielding different variants of the method. In this context, we propose a new one, called fixed number of successes.We prove unbiasedness for the new and some known variants, because in many papers, the proof is based on an incorrect argument. Further, we analyze its behavior in a simplified setting in terms of efficiency and asymptotics in comparison to the standard variant. The main difference is that it controls the imprecision of the estimator rather than the computational effort. Our analysis and simulation examples show that it is rather robust in terms of parameter choice and we present a two-stage procedure which also yields confidence intervals. © 2011 ACM.",Entrance probability; Importance splitting; Markov chain; Monte Carlo; Rare event; Robust estimation; Tandem queue,Estimation; Markov processes; Monte Carlo methods; Vehicular tunnels; Entrance probability; Importance splitting; Markov Chain; MONTE CARLO; Rare event; Robust estimation; Tandem queue; Probability
The double CFTP method,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951753191&doi=10.1145%2f1899396.1899398&partnerID=40&md5=74ea007f59fc450c322f7849ffb09e96,"We consider the problem of the exact simulation of random variables Z that satisfy the distributional identity Z L= VY + (1 - V)Z, where V ε [0, 1] and Y are independent, and L= denotes equality in distribution. Equivalently, Zis the limit of aMarkov chain driven by that map.We give an algorithm that can be automated under the condition that we have a source capable of generating independent copies of Y, and that V has a density that can be evaluated in a black-box format. The method uses a doubling trick for inducing coalescence in coupling from the past. Applications include exact samplers for many Dirichlet means, some two-parameter Poisson-Dirichlet means, and a host of other distributions related to occupation times of Bessel bridges that can be described by stochastic fixed point equations. © 2011 ACM.",Bessel bridge; Coupling from the past; Dirichlet means; Distribution theory; Expected time analysis; Markov chain; Monte Carlo; Perpetuities; Poisson-Dirichlet; Random partitions; Random variate generation; Simulation; Stochastic fixed point equations; Stochastic recurrences,Coalescence; Markov processes; Monte Carlo methods; Random variables; Stochastic systems; Bessel bridge; Coupling from the past; Dirichlet; Dirichlet means; Distribution theory; Expected time; Fixed point equation; Markov chain; MONTE CARLO; Perpetuities; Random partitions; Random variates; Simulation; Stochastic recurrences; Poisson distribution
Efficient rare event simulation for heavy-tailed compound sums,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951761458&doi=10.1145%2f1899396.1899397&partnerID=40&md5=46a3724088673ea5450acf2b10f5667f,"We develop an efficient importance sampling algorithm for estimating the tail distribution of heavy-tailed compound sums, that is, random variables of the form SM = Z1 + ⋯+ ZM where the Zi 's are independently and identically distributed (i.i.d.) random variables in ℝ and M is a nonnegative, integer-valued random variable independent of the Zi 's. We construct the first estimator that can be rigorously shown to be strongly efficient only under the assumption that the Zi 's are subexponential and M is light-tailed. Our estimator is based on state-dependent importance sampling and we use Lyapunov-type inequalities to control its second moment. The performance of our estimator is empirically illustrated in various instances involving popular heavy-tailed models. © 2011 ACM.",Compound sums; Importance sampling; Lyapunov inequalities; Rare events; Strong efficiency; Subexponential tails,Estimation; Lyapunov methods; Compound sums; Importance sampling; Lyapunov inequality; Rare event; Strong efficiency; Subexponential tails; Random variables
On deriving and incorporating multihop path duration estimates in VANET protocols,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951751374&doi=10.1145%2f1899396.1899402&partnerID=40&md5=9bfbdc6c452b827442b7fcb1ea39189a,"The expected duration of multihop paths can be incorporated at different layers in the protocol stack to improve the performance of mobile ad hoc networks. This article presents two discrete-time and discretespace Markov chain-based methods, DTMC-CA and DTMC-MFT, to estimate the duration of multihop roadbased paths in vehicular ad hoc networks (VANET). The duration of such paths does not depend on individual nodes because packets can be forwarded by any vehicle located along the roads forming the path. DTMCCA derives probabilistic measures based only on vehicle density for a traffic mobility model, which in this article is the microscopic Cellular Automaton (CA) freeway traffic model. DTMC-MFT generalizes the approach used by DTMC-CA to any vehicular mobility model by focusing on the macroscopic information of vehicles rather than their microscopic characteristics. The proposed analytical models produce performancemeasure values comparable to simulation estimates from the validated CA traffic model. Furthermore, this article demonstrates the benefits of incorporating expected path durations into a VANET routing protocol. Simulation results show that the network overhead associated with route maintenance can be reduced to less than half by using the expected path durations. © 2011 ACM.",Multihop path duration; Road-based routing; Vehicular ad hoc networks,Cellular automata; Computer simulation; Estimation; Markov processes; Mathematical models; Mobile ad hoc networks; Mobile telecommunication systems; Models; Pattern recognition systems; Roads and streets; Analytical model; Discrete-time; Freeway traffic; Markov Chain; Microscopic characteristics; Mobility model; Multi-hop path; Multihop; Network overhead; Probabilistic measures; Protocol stack; Road-based routing; Route maintenance; Simulation result; Traffic mobility; Traffic model; Vehicle density; Vehicular ad hoc networks; Ad hoc networks
Modeling and simulation of SIP tandem server with finite buffer,2011,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551651732&doi=10.1145%2f1899396.1899399&partnerID=40&md5=91679ad09254646e9576fa8ef85a5566,"Recent collapses of SIP servers (e.g., Skype outage) indicate that the built-in SIP overload control mechanism cannot mitigate overload effectively. We introduce our analytical approach by investigating an overloaded tandem server scenario. Our analytical model: (1) considers a general case that both arrival rate and service rate for signaling messages are generic random processes; (2) makes a detailed analysis of departure processes; (3) allows us to run fluid-based simulations to observe and analyze SIP system performance under some specific scenarios. This approach is much faster than event-driven simulation which needs to track thousands of retransmission timers for outstanding messages and may crash a simulator due to limited computing resources. Our numerical results help us reach a counterintuitive conclusion: A SIP system with a large buffer size may continuously exhibit overload and long queuing delay after experiencing a short period of demand burst or a temporary server slowdown. Small buffer size, on the other hand, can mitigate overload quickly by rejecting a large portion of the requests from a demand burst, and then resume normal operation after a short period of time. Furthermore, numerical results demonstrate that overload at a downstream server may propagate or migrate to its upstream servers and therefore cause widespread server crashes in a real SIP network. © 2011 ACM.",Overload; Resource utilization; Retransmission; SIP,Computer simulation; Mathematical models; Random processes; Safety factor; Servers; Analytical approach; Analytical model; Arrival rates; Buffer sizes; Computing resource; Departure process; Event-driven simulations; Finite buffer; Large buffer; Modeling and simulation; Normal operations; Numerical results; Overload; Overload control; Queuing delay; Resource utilizations; Retransmission; Retransmission timers; Service rates; Short periods; Signaling messages; SIP; SIP server; SIP systems; Internet protocols
Optimal scheduling in High-Speed Downlink Packet Access networks,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650661186&doi=10.1145%2f1870085.1870088&partnerID=40&md5=ff71d096a8830a725f0041bc0a903bcc,"We present an analytic model and a methodology to determine the optimal packet scheduling policy in a High-Speed Downlink Packet Access (HSDPA) system. The optimal policy is the one that maximizes cell throughput while maintaining a level of fairness between the users in the cell. A discrete stochastic dynamic programming model for the HSDPA downlink scheduler is presented. Value iteration is then used to solve for the optimal scheduling policy. We use a FSMC (Finite State Markov Channel) to model the HSDPA downlink channel. A near-optimal heuristic scheduling policy is developed. Simulation is used to study the performance of the resulting heuristic policy and compare it to the computed optimal policy. The results show that the performance of the heuristic policy is very close to that of the optimal policy. The heuristic policy has much less computational complexity, which makes it easy to deploy, with only slight reduction in performance compared to the optimal policy. © 2010 ACM.",3G wireless networks; Cross-layer design; Dynamic programming; HSDPA systems; Markov decision process; Optimal scheduling; Resource allocation,Computational complexity; Markov processes; Optimization; Packet networks; Resource allocation; Scheduling; Stochastic models; Wireless networks; 3G wireless networks; Cross-layer design; HSDPA systems; Markov Decision Processes; Optimal scheduling; Dynamic programming
Cross-layer design for efficient resource utilization in WiMedia UWB-based WPANs,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650648206&doi=10.1145%2f1870085.1870093&partnerID=40&md5=5877d87213bc1a8b0bc1b4863daacc43,"Ultra-WideBand (UWB) communications has emerged as a promising technology for high data rate Wireless Personal Area Networks (WPANs). In this article, we address two key issues that impact the performance of a multihop UWB-based WPAN: throughput and transmission range. Arbitrary selection of routes in such a network may result in reserving an unnecessarily long channel time, and hence low network throughput and high blocking rate for prospective reservations. To remedy this situation, we propose a novel cross-layer resource allocation design. At the core of this design is a routing technique (called RTERU) that uses the allocated channel time as a routing metric. RTERU exploits the dependence of this metric on the multiple-rate capability of an UWB system. We show that selecting the route that consumes the minimum channel time while satisfying a target packet delivery probability over the selected route is an NP-hard problem. Accordingly, RTERU resorts to approximate path selection algorithms (implemented proactively and reactively) to find near-optimal solutions at reasonable computational/communication overhead. We further enhance the performance of RTERU by integrating into its design a packet overhearing capability. Simulations are used to demonstrate the performance of our proposed solutions. © 2010 ACM.",Cross-layer design; OFDM-based UWB; Packet overhearing; Routing; Slots reservation,Broadband networks; Communication channels (information theory); Computational complexity; Design; Orthogonal functions; Personal communication systems; Wireless local area networks (WLAN); Cross-layer design; OFDM-based UWB; Packet overhearing; Routing; Slots reservation; Ultra-wideband (UWB)
A theoretical framework for interaction measure and sensitivity analysis in cross-layer design,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650641364&doi=10.1145%2f1870085.1870091&partnerID=40&md5=c915845885ed49dfd4c2413786f3de92,"Cross-layer design has become one of the most effective and efficient methods to provide Quality of Service (QoS) over various communication networks, especially over wireless multimedia networks. However, current research on cross-layer design has been carried out in various piecemeal approaches, and lacks a methodological foundation to gain in-depth understanding of complex cross-layer behaviors such as multiscale temporal-spatial behavior, leading to a design paradox and/or unmanageable design problems. In this article, we propose a theoretical framework for quantitative interaction measures, which is further extended to sensitivity analysis by quantifying the contribution made by each design variable and by the interactions among them on the design objective. Thus, the proposed framework can significantly enhance our capability for crosslayer behavior characterization and provide design insights for future design. Furthermore, a case study on cross-layer optimized wireless multimedia communications has been adopted to illustrate major cross-layer design trade-offs and validate the proposed framework. Both analytical and experimental results show the correctness and effectiveness of the proposed framework. © 2010 ACM.",Choquet integral; Cross-layer design; Ellsberg paradox; Interactionmeasure; Nonadditive measure; Sensitivity analysis,Integral equations; Multimedia systems; Quality of service; Sensitivity analysis; Choquet integral; Cross-layer design; Ellsberg paradox; Interactionmeasure; Non-additive measure; Design
Guest Editors' Introduction: Special Issue on Modeling and Simulation of Cross-Layer Interactions in Communication Networks,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650668610&doi=10.1145%2f1870085.1870086&partnerID=40&md5=d54ceaecba3a7cf4bc87f674cfb033a6,[No abstract available],,
Steepest-ascent constrained simultaneous perturbation for multiobjective optimization,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650649234&doi=10.1145%2f1870085.1870087&partnerID=40&md5=60cadc65e4ec695c02de8802894d78ed,"The simultaneous optimization of multiple responses in a dynamic system is challenging. When a response has a known gradient, it is often easily improved along the path of steepest ascent. On the contrary, a stochastic approximation technique may be used when the gradient is unknown or costly to obtain. We consider the problem of optimizing multiple responses in which the gradient is known for only one response. We propose a hybrid approach for this problem, called simultaneous perturbation stochastic approximation steepest ascent, SPSA-SA or SP(SA)2 for short. SP(SA)2 is an SPSA technique that leverages information about the known gradient to constrain the perturbations used to approximate the others. We apply SP(SA)2 to the cross-layer optimization of throughput, packet loss, and end-to-end delay in a mobile ad hoc network (MANET), a selforganizing wireless network. The results show that SP(SA)2 achieves higher throughput and lower packet loss and end-to-end delay than the steepest ascent, SPSA, and the Nelder-Mead stochastic approximation approaches. It also reduces the cost in the number of iterations to perform the optimization. © 2010 ACM.",Cross-layer optimization; Mobile ad hoc networks; Multiobjective optimization; Nongradient optimization; Stochastic approximation,Ad hoc networks; Approximation theory; Constrained optimization; Mobile ad hoc networks; Mobile telecommunication systems; Packet loss; Stochastic systems; Telecommunication links; Cross layer optimization; Dynamic Systems; End to end delay; Hybrid approach; Multiple response; Nongradient optimization; Number of iterations; Self-organizing wireless networks; Simultaneous optimization; Simultaneous perturbation; Simultaneous perturbation stochastic approximation; Steepest ascent; Stochastic approximations; Multiobjective optimization
Cross-layer interactions in multihop wireless sensor networks: A constrained queueing model,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650634884&doi=10.1145%2f1870085.1870089&partnerID=40&md5=ee510a3ca70c094c558cbea74e51ecd2,"In this article, we propose a constrained queueingmodel to investigate the performance of multihop wireless sensor networks. Specifically, the cross-layer interactions of rate admission control, traffic engineering, dynamic routing, and adaptive link scheduling are studied jointly with the proposed queueing model. In addition, the stochastic network utility maximization problem in wireless sensor networks is addressed within this framework. We propose an adaptive network resource allocation scheme, called the ANRA algorithm, which provides a joint solution to the multiple-layer components of the stochastic network utility maximization problem.We show that the proposed ANRA algorithm achieves a near-optimal solution, that is, (1 - ε) of the global optimum network utility where ε can be arbitrarily small, with a trade-off with the average delay experienced in the network. The proposed ANRA algorithm enjoys the merit of self-adaptability through its online nature and thus is of particular interest for time-varying scenarios such as multihop wireless sensor networks. © 2010 ACM.",Cross-layer design; Online algorithms; Stochastic network optimization; Stochastic utility maximization,Algorithms; Optimization; Queueing networks; Queueing theory; Sensor networks; Stochastic systems; Time varying networks; Adaptive networks; Admission Control; Average delay; Cross-layer design; Cross-layer interaction; Dynamic routing; Global optimum; Layer components; Link scheduling; Multihop wireless; Near-optimal solutions; Online algorithms; Queueing model; Self-adaptability; Stochastic networks; Time varying; Traffic Engineering; Utility maximizations; Wireless sensor networks
Modeling the interactions between MAC and higher layer: A systematic approach to generate high-level scenarios from MAC-layer scenarios,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650630572&doi=10.1145%2f1870085.1870092&partnerID=40&md5=79d37f6cdc1b9809b86a31fffce79f3e,"We propose a new framework for worst-case performance evaluation of MAC protocols for wireless ad hoc networks. Given a protocol, its performance metrics and a network topology, our framework first generates MAC scenarios which achieve poor performance at MAC level. In order to evaluate the impact of these MAC scenarios on the end performance, we model the interactions between MAC interface and the MAC layer using a state transition graph and generate high-level scenarios using enumeration techniques. These high-level scenarios can be simulated and compared with heuristics developed by others to identify high-level scenarios that are expected to lead to the worst-case end performance. In order to demonstrate its usefulness, we use our framework to evaluate the worst-case performance of IEEE 802.11 DCF protocol by generating a library of MAC- and high-level scenarios. We simulate the high-level scenarios to demonstrate that the scenarios we generate exhibit the worst performance among all the scenarios, including those generated by using heuristics recently proposed by other researchers. © 2010 ACM.",Finite state machine models; IEEE 802.11 performance; Medium access control; Modeling for performance; Worst-case performance,Ad hoc networks; Computer network performance evaluation; Contour followers; Electric network topology; Network protocols; Security systems; Standards; Wireless ad hoc networks; Finite state machine model; IEEE 802.11 performance; Medium access; Modeling for performance; Worst-case performance; Medium access control
Joint congestion control and distributed scheduling for throughput guarantees in wireless networks,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650631785&doi=10.1145%2f1870085.1870090&partnerID=40&md5=1ec7dca9c7e7067b9f29a580d1824e8b,"We consider the problem of throughput-optimal cross-layer design of wireless networks. We propose a joint congestion control and scheduling algorithm that achieves a fraction 1/dI (G) of the capacity region, where dI (G) depends on certain structural properties of the underlying connectivity graph G of the wireless network, and also on the type of interference constraints. For a wide range of wireless networks, dI (G) can be upper bounded by a constant, independent of the number of nodes in the network. The scheduling element of our algorithm is the maximal scheduling policy. Although this scheduling policy has been considered in several previous works, the challenges underlying its practical implementation in a fully distributed manner while accounting for necessary message exchanges have not been addressed in the literature. In this article, we propose two algorithms for the distributed implementation of the maximal scheduling policy accounting for message exchanges, and analytically show that they still can achieve the performance guarantee under the 1-hop and 2-hop interference models.We also evaluate the performance of our cross-layer solutions in more realistic network settings with imperfect synchronization under the Signal-to- Interference-Plus-Noise Ratio (SINR) interference model, and compare with the standard layered approaches such as TCP over IEEE 802.11b DCF networks. © 2010 ACM.",Cross-layer design; Distributed algorithm; Maximal scheduling; Wireless communication systems simulation and modeling,Communication systems; Design; Global system for mobile communications; Standards; Transmission control protocol; Wireless networks; Wireless telecommunication systems; Capacity regions; Connectivity graph; Cross-layer; Cross-layer design; Distributed algorithm; Distributed implementation; Distributed scheduling; IEEE 802.11b; Imperfect synchronization; Interference constraints; Interference models; Joint congestion; Maximal scheduling; Message exchange; Network settings; Performance guarantees; Practical implementation; Scheduling policies; Signal to interference plus noise ratio; Wireless communication system; Scheduling algorithms
An integrated human decision making model for evacuation scenarios under a BDI framework,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649357796&doi=10.1145%2f1842722.1842728&partnerID=40&md5=cad8ec9fad13f89c2423dc415cfaa0a7,"An integrated Belief-Desire-Intention (BDI) modeling framework is proposed for human decision making and planning for evacuation scenarios, whose submodules are based on a Bayesian Belief Network (BBN), Decision-Field-Theory (DFT), and a Probabilistic Depth-First Search (PDFS) technique. A key novelty of the proposed model is its ability to represent both the human decisionmaking and decision-planning functions in a unified framework. To mimic realistic human behaviors, attributes of the BDI framework are reverse-engineered fromhuman-in-the-loop experiments conducted in the Cave Automatic Virtual Environment (CAVE). The proposed modeling framework is demonstrated for a human's evacuation behaviors in response to a terrorist bomb attack. The simulated environment and agents (models of humans) conforming to the proposed BDI framework are implemented in AnyLogic®agent-based simulation software, where each agent calls external Netica BBN software to perform its perceptual processing function and Soar software to perform its real-time planning and decision-execution functions. The constructed simulation has been used to test the impact of several factors (e.g., demographics, number of police officers, information sharing via speakers) on evacuation performance (e.g., average evacuation time, percentage of casualties). © 2010 ACM.",Bayesian belief network; BDI; Emergency evacuation; Human decision behavior; Planning,Behavioral research; Caves; Decision making; Decision theory; Knowledge based systems; Law enforcement; Software agents; Virtual reality; Agent based simulation; Bayesian Belief Networks; BDI; Belief-desire-intentions; Cave automatic virtual environments; Depth first search; Emergency evacuation; Evacuation time; Human behaviors; Human decision-making; Human decision-making model; Human decisions; Information sharing; Modeling frameworks; Perceptual processing; Police officers; Real-time planning; Simulated environment; Submodules; Terrorist bomb attacks; Unified framework; Bayesian networks
Generalized lindley-type recursive representations for multiserver tandem queues with blocking,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149418821&doi=10.1145%2f1842722.1842726&partnerID=40&md5=8b377700c5a358f3ebaebf8e3fd7949c,"Lindley's recursion is an explicit recursive equation that describes the recursive relationship between consecutive waiting times in a single-stage single-server queue. In this paper, we develop explicit recursive representations for multiserver tandem queues with blocking. We demonstrate the application of these recursive representations with simulations of large-scale tandem queueing networks. We compare the computational efficiency of these representations with two other popular discrete-event simulation approaches, namely, event scheduling and process interaction. Experimental results show that these representations are seven (or more) times faster than their counterparts based on the event-scheduling and process-interaction approaches. © 2010 ACM.",Blocking; Fast simulation; Lindley recursion; Tandem queue,Computational efficiency; Computer simulation; Queueing networks; Scheduling; Blocking; Event scheduling; Fast simulation; Multi-server; Process interaction; Recursions; Recursive equations; Single server queue; Single stage; Tandem queue; Waiting time; Queueing theory
A mixed reality approach for interactively blending dynamic models with corresponding physical phenomena,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649384796&doi=10.1145%2f1842722.1842727&partnerID=40&md5=ec1d3e35a25303c105db79b7e7a09a49,"The design, visualization, manipulation, and implementation of models for computer simulation are key parts of the discipline. Models are constructed as a means to understand physical phenomena as state changes occur over time. One issue that arises is the need to correlate models and their components with the phenomena being modeled. For example, a part of an automotive engine needs to be placed into cognitive context with the diagrammatic icon that represents that part's function. A typical solution to this problem is to display a dynamic model of the engine in one window and the engine's CAD model in another. Users are expected to, on their own, mentally blend the dynamic model and the physical phenomenon into the same context. However, this contextualization is not trivial in many applications. Our approach expands upon this form of user interaction by specifying two ways in which dynamic models and the corresponding physical phenomena may be viewed, and experimented with, within the same human interaction space. We present a methodology and implementation of contextualization for diagram-based dynamic models using an anesthesia machine, and then follow up with a human study of its effects on spatial cognition. © 2010 ACM.",Human computer interaction; Mixed reality; Modeling; Simulation,Blending; Computer aided design; Dynamics; Human computer interaction; Human form models; Virtual reality; Visualization; Anesthesia machine; Automotive engine; CAD models; Contextualization; Follow up; Human interactions; Human study; Key parts; Mixed reality; Modeling; Physical phenomena; Simulation; Spatial cognition; User interaction; Dynamic models
Crowd modeling and simulation technologies,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649377807&doi=10.1145%2f1842722.1842725&partnerID=40&md5=a37e16763938c53404744f38e520f10a,"As a collective and highly dynamic social group, the human crowd is a fascinating phenomenon that has been frequently studied by experts from various areas. Recently, computer-based modeling and simulation technologies have emerged to support investigation of the dynamics of crowds, such as a crowd's behaviors under normal and emergent situations. This article assesses the major existing technologies for crowd modeling and simulation. We first propose a two-dimensional categorization mechanism to classify existing work depending on the size of crowds and the time-scale of the crowd phenomena of interest. Four evaluation criteria have also been introduced to evaluate existing crowd simulation systems from the point of view of both a modeler and an end-user. We have discussed some influential existing work in crowd modeling and simulation regarding their major features, performance as well as the technologies used in this work. We have also discussed some open problems in the area. This article will provide the researchers with useful information and insights on the state of the art of the technologies in crowd modeling and simulation as well as future research directions. © 2010 ACM.",Crowd dynamics; Crowd simulation; Human behavior; Multiagent system,Behavioral research; Multi agent systems; Technology; Computer-based modeling; Crowd dynamics; Crowd modeling; Crowd Simulation; End users; Evaluation criteria; Fascinating phenomena; Future research directions; Human behaviors; Human crowd; Open problems; Social groups; State of the art; Time-scales; Computer simulation
Random variate generation by numerical inversion when only the density is known,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956393949&doi=10.1145%2f1842722.1842723&partnerID=40&md5=fe04aaea9e244501f754a0a292b8c712,"We present a numerical inversion method for generating random variates from continuous distributions when only the density function is given. The algorithm is based on polynomial interpolation of the inverse CDF and Gauss-Lobatto integration. The user can select the required precision, which may be close to machine precision for smooth, bounded densities; the necessary tables have moderate size. Our computational experiments with the classical standard distributions (normal, beta, gamma, t-distributions) and with the noncentral chi-square, hyperbolic, generalized hyperbolic, and stable distributions showed that our algorithm always reaches the required precision. The setup time is moderate and the marginal execution time is very fast and nearly the same for all distributions. Thus for the case that large samples with fixed parameters are required the proposed algorithm is the fastest inversion method known. Speed-up factors up to 1000 are obtained when compared to inversion algorithms developed for the specific distributions. This makes our algorithm especially attractive for the simulation of copulas and for quasi-Monte Carlo applications. © 2010 ACM.",Black-box algorithm; Gauss-Lobatto integration; Inversion method; Newton interpolation; Nonuniform random variates; Universal method,Gaussian distribution; Interpolation; Black-box algorithm; Gauss-Lobatto integration; Inversion methods; Newton interpolation; Nonuniform; Universal method; Algorithms
The impact of service demand variability on resource allocation strategies in a grid system,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955306507&doi=10.1145%2f1842722.1842724&partnerID=40&md5=32794c5874b7558aff1d519577fafc47,"Scheduling and resource management play an important role in building complex distributed systems, such as grids. In this article we study the impact on performance of job service demand variability in a two-level grid architecture, given that the grid and local schedulers are unaware of each job's service demand. We examine two scheduling policies at grid level, which utilize site load information and three policies at local level. A simulation model is used to evaluate performance. Results show that service demand variability degrades performance, and thus a suitable local resource allocation policy is needed to reduce its impact. © 2010 ACM.",Grid; Resource allocation; Simulation,Computer simulation; Resource allocation; Complex distributed system; Grid; Grid architectures; Grid levels; Grid systems; In-buildings; Load information; Local schedulers; Resource allocation policy; Resource allocation strategies; Resource management; Scheduling policies; Service demand; Simulation; Simulation model; Scheduling
State-dependent importance sampling for a jackson tandem network,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958072646&doi=10.1145%2f1842713.1842718&partnerID=40&md5=cee390a922d73c45227715553b78d2c8,"This article considers importance sampling as a tool for rare-event simulation. The focus is on estimating the probability of overflow in the downstream queue of a Jacksonian two-node tandem queue; it is known that in this setting ""traditional"" state-independent importance-sampling distributions perform poorly. We therefore concentrate on developing a state-dependent change of measure, that we prove to be asymptotically efficient. More specific contributions are the following. (i)We concentrate on the probability of the second queue exceeding a certain predefined threshold before the system empties. Importantly, we identify an asymptotically efficient importance-sampling distribution for any initial state of the system. (ii) The choice of the importance-sampling distribution is backed up by appealing heuristics that are rooted in large-deviations theory. (iii) The method for proving asymptotic efficiency relies on probabilistic arguments only. The article is concluded by simulation experiments that show a considerable speedup. © 2010 ACM.",Algorithms; Performance; Theory,Asymptotic analysis; Queueing theory; Asymptotic efficiency; Asymptotically efficient; Change of measure; Importance sampling; Initial state; Jackson; Performance; Probabilistic arguments; Probability of overflow; Rare event simulation; Sampling distribution; Simulation experiments; State-dependent; Tandem networks; Tandem queue; Theory; Probability distributions
A stochastic approximation method with max-norm projections and its applications to the Q-learning algorithm,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958023639&doi=10.1145%2f1842713.1842715&partnerID=40&md5=9d095313a17fcfcec30b207bc9b580c1,"In this article, we develop a stochastic approximation method to solve a monotone estimation problem and use this method to enhance the empirical performance of the Q-learning algorithm when applied toMarkov decision problems with monotone value functions.We begin by considering a monotone estimation problem where we want to estimate the expectation of a random vector, nη. We assume that the components of E{nη} are known to be in increasing order. The stochastic approximation method that we propose is designed to exploit this information by projecting its iterates onto the set of vectors with increasing components. The novel aspect of the method is that it uses projections with respect to the max norm. We show the almost sure convergence of the stochastic approximation method. After this result, we consider the Q-learning algorithm when applied to Markov decision problems with monotone value functions. We study a variant of the Q-learning algorithm that uses projections to ensure that the value function approximation obtained at each iteration is also monotone. Computational results indicate that the performance of the Q-learning algorithm can be improved significantly by exploiting the monotonicity property of the value functions. © 2010 ACM.",Max-norm projection; Q-learning; Stochastic approximation,Approximation algorithms; Approximation theory; Signal filtering and prediction; Stochastic systems; A-monotone; Almost sure convergence; Computational results; Decision problems; Empirical performance; Estimation problem; Markov decision problem; Max-norm projection; Max-norms; Monotonicity property; Q-learning; Q-learning algorithms; Random vectors; Stochastic approximation methods; Stochastic approximations; Value function approximation; Value functions; Learning algorithms
Finding feasible systems in the presence of constraints on multiple performance measures,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958059413&doi=10.1145%2f1842713.1842716&partnerID=40&md5=de61d28cbf0db4b8c5bb8455ff1eb382,"We consider the problem of finding a set of feasible or near-feasible systems among a finite number of simulated systems in the presence of constraints on secondary performance measures. We first present a generic procedure that detects the feasibility of one system in the presence of one constraint and extend it to the case of two or more systems and constraints. To accelerate the elimination of infeasible systems, a method that reuses collected observations and its varianceupdating version are discussed. Experimental results are presented to compare the performance of the proposed procedures. © 2010 ACM.",Multiple performance measures; Ranking and selection; Stochastic constraints,Finite number; Generic procedures; Performance measure; Ranking and selection; Simulated system; Stochastic constraints; Stochastic systems
A survey of customization support in agent-based business process simulation tools,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958034463&doi=10.1145%2f1842713.1842717&partnerID=40&md5=a599f7f7e7dd669d996527d65c486bae,"Agent-based business process simulation has grown in popularity, in part because of its analysis capabilities. The analyses depend on the kinds of simulations that can be built, adapted, and extended, which in turn depend on the underlying simulation framework.We report the results of our analysis of 19 agent-based process simulation tools and their simulation frameworks. We conclude that a growing number of simulation tools are using component-based software techniques. Nevertheless, most simulation tools do not directly support requirements models, their transformation into executable simulations, or the management of model variants over time. Such practices are becoming more widely applied in software engineering under the term software product line engineering (SPLE). Based on our analysis, agent-based process simulation tools may improve their customization capacity by: (1) supporting object modeling more completely and (2) supporting software product line engineering issues. © 2010 ACM.",Agent-based modeling; Application frameworks; Encapsulation; Event-driven simulation; Modularity; Software product line engineering,Computational methods; Engineering; Production engineering; Software agents; Software design; Tools; Agent-based modeling; Application frameworks; Event-driven simulations; Modularity; Software product line engineerings; Computer simulation
Probabilistic analysis of simulation-based games,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958070423&doi=10.1145%2f1842713.1842719&partnerID=40&md5=0875252913b754c98a21be7c8233e59f,"The field of game theory has proved to be of great importance in modeling interactions between self-interested parties in a variety of settings. Traditionally, game-theoretic analysis relied on highly stylized models to provide interesting insights about problems at hand. The shortcoming of such models is that they often do not capture vital detail. On the other hand, many real strategic settings, such as sponsored search auctions and supply-chains, can be modeled in high resolution using simulations. Recently, a number of approaches have been introduced to perform analysis of game-theoretic scenarios via simulation-based models. The first contribution of this work is the asymptotic analysis of Nash equilibria obtained from simulation-based models. The second contribution is to derive expressions for probabilistic bounds on the quality of Nash equilibrium solutions obtained using simulation data. In this vein, we derive very general distribution-free bounds, as well as bounds which rely on the standard normality assumptions, and extend the bounds to infinite games via Lipschitz continuity. Finally, we introduce a new maximum-a-posteriori estimator of Nash equilibria based on game-theoretic simulation data and show that it is consistent and almost surely unique. © 2010 ACM.",Game theory; Nash equilibrium; Simulation; Simulation and modeling,Asymptotic analysis; Computer simulation; Supply chains; Telecommunication networks; Distribution-free; High resolution; Infinite game; Lipschitz continuity; Maximum a posteriori; Modeling interactions; Nash equilibria; Nash Equilibrium; Probabilistic analysis; Probabilistic bounds; Simulation; Simulation and modeling; Simulation data; Simulation-based; Sponsored search auctions; Theoretic analysis; Game theory
Performance of folded variance estimators for simulation,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958045616&doi=10.1145%2f1842713.1842714&partnerID=40&md5=298fbee507626d20eb701a0f28ff673c,"We extend and analyze a new class of estimators for the variance parameter of a steady-state simulation output process. These estimators are based on ""folded"" versions of the standardized time series (STS) of the process, and are analogous to the area and Cram'er-von Mises estimators calculated from the original STS. In fact, one can apply the folding mechanism more than once to produce an entire class of estimators, all of which reuse the same underlying data stream. We show that these folded estimators share many of the same properties as their nonfolded counterparts, with the added bonus that they are often nearly independent of the nonfolded versions. In particular, we derive the asymptotic distributional properties of the various estimators as the run length increases, as well as their bias, variance, and mean squared error. We also study linear combinations of these estimators, and we show that such combinations yield estimators with lower variance than their constituents. Finally, we consider the consequences of batching, and we see that the batched versions of the new estimators compare favorably to benchmark estimators such as the nonoverlapping batch means estimator. © 2010 ACM.",Folded estimators; Method of batch means; Simulation output analysis; Standardized time series; Steady-state simulation,Time series; Time series analysis; Batch means; Folded estimators; Simulation output analysis; Standardized time series; Steady-state simulations; Estimation
Profile-driven regression for modeling and runtime optimization of mobile networks,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958076546&doi=10.1145%2f1842713.1842720&partnerID=40&md5=4341c74bb6abd3998414e0c4acbc1252,"Computer networks often display nonlinear behavior when examined over a wide range of operating conditions. There are few strategies available for modeling such behavior and optimizing such systems as they run. Profile-driven regression is developed and applied to modeling and runtime optimization of throughput in a mobile ad hoc network, a self-organizing collection of mobile wireless nodes without any fixed infrastructure. The intermediate models generated in profile-driven regression are used to fit an overall model of throughput, and are also used to optimize controllable factors at runtime. Unlike others, the throughput model accounts for node speed. The resulting optimization is very effective; locally optimizing the network factors at runtime results in throughput as much as six times higher than that achieved with the factors at their default levels. © 2010 ACM.",Mobile ad hoc networks; Regression modeling; Runtime optimization,Mathematical models; Mobile ad hoc networks; Mobile telecommunication systems; Optimization; Regression analysis; Telecommunication links; Throughput; Intermediate model; Mobile networks; Mobile wireless; Nonlinear behavior; Operating condition; Overall-model; Regression modeling; Run-time results; Runtime optimization; Runtimes; Self organizing; Throughput models; Ad hoc networks
Asymptotically optimal allocation of stratified sampling with adaptive variance reduction by strata,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952078227&doi=10.1145%2f1734222.1734225&partnerID=40&md5=61f0d61bca8702211a3217436625c6f7,"To enhance efficiency in Monte Carlo simulations, we develop an adaptive stratified sampling algorithm for allocation of sampling effort within each stratum, in which an adaptive variance reduction technique is applied. Given the number of replications in each batch, our algorithm updates allocation fractions to minimize the work-normalized variance of the stratified estimator of the mean. We establish the asymptotic normality of the stratified estimator of the mean as the number of batches tends to infinity. Although implementation of the proposed algorithm requires a small amount of initial work, the algorithm has the potential to yield substantial improvements in estimator efficiency. Equally important is that the adaptive framework avoids the need for frequent recalibration of the parameters of the variance reduction methods applied within each stratum when changes occur in the experimental conditions governing system performance. To illustrate the applicability and effectiveness of our algorithm, we provide numerical results for a Black - Scholes option pricing, where we stratify the underlying Brownian motion with respect to its terminal value and apply an importance sampling method to normal random variables filling in the Brownian path. Relative to the estimator variance with proportional allocation, the proposed algorithm achieved a fourfold reduction in estimator variance with a negligible increase in computing time. © 2010 ACM.",Brownian bridge; Control variates; Importance sampling; Poisson stratification; Stochastic approximation algorithm; Stratified sampling; Variance reduction,Approximation algorithms; Approximation theory; Asymptotic analysis; Brownian movement; Computer simulation; Estimation; Learning algorithms; Monte Carlo methods; Random variables; Stochastic control systems; Stochastic systems; Brownian bridge; Importance sampling; Stochastic approximation algorithms; Stratified sampling; Variance reductions; Adaptive algorithms
CDNsim: A simulation tool for content distribution networks,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952087450&doi=10.1145%2f1734222.1734226&partnerID=40&md5=341e365992e18ea48b50549e823c37dd,"Content distribution networks (CDNs) have gained considerable attention in the past few years. Hence there is need for developing frameworks for carrying out CDN simulations. In this article we present a modeling and simulation framework for CDNs, called CDNsim. CDNsim has been designated to provide a realistic simulation for CDNs, simulating the surrogate servers, the TCP/IP protocol, and the main CDN functions. The main advantages of this tool are its high performance, its extensibility, and its user interface, which is used to configure its parameters. CDNsim provides an automated environment for conducting experiments and extracting client, server, and network statistics. The purpose of CDNsim is to be used as a testbed for CDN evaluation and experimentation. This is quite useful to both the research community (to experiment with new CDN data management techniques), and for CDN developers (to evaluate profits on prior certain CDN installations). © 2010 ACM.",Caching; Content distribution network; Services; Trace-driven simulation,Distributed parameter networks; Experiments; Profitability; Servers; User interfaces; Content distribution networks; Data management techniques; Modeling and simulation; Network statistics; Realistic simulation; Research communities; Simulation tool; TCP/IP protocol; Trace driven simulation; Computer simulation
Information models for queueing system simulation,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952037563&doi=10.1145%2f1734222.1734224&partnerID=40&md5=30041bb382f6f3bf015474fb6c326de2,"When planning simulations of large-scale systems, it is important to anticipate what information is required to model the system and obtain desired output. This can be done without tying the study to a specific simulation package or language. It is valuable to do so to avoid unnecessarily long development and execution times. In this article, we offer a simulation information model (SIM) designed to help organize system information in the early stages of a project. (It can also be used to analyze existing models.) The SIM allows complexity analysis of the system to be performed, and may lead to a better selection of simulation language. The SIM is illustrated using two examples, and its relationship to current formalisms is discussed. © 2010 ACM.","Model development, gimulation theory; model classification; Simulation and modeling, general",Computer simulation languages; Linguistics; Complexity analysis; Execution time; Information models; Model classification; Model development; Queueing system; Simulation and modeling; Simulation language; Simulation packages; System information; Computer simulation
Setwise and filtered gibbs samplers for teletraffic analysis,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952084796&doi=10.1145%2f1734222.1734223&partnerID=40&md5=acaa674a7e97be6c4a80c2fac23b542d,"A setwise Gibbs sampler (SGS) method is developed to simulate stationary distributions and performance measures of network occupancy of Baskett-Chandy-Muntz-Palacios (BCMP) telecommunication models. It overcomes the simulation difficulty encountered in applying the standard Gibbs sampler to closed BCMP networks with constant occupancy constraints. We show Markov chains induced by SGS converge to the target stationary distributions. This article also investigates the filtered Gibbs sampler (FGS) as an efficient method for estimating various network performance measures. It shows that FGS's efficiency is considerable, but may be improperly overestimated. A more conservative performance estimator is then presented. © 2010 ACM.",Gibbs sampler; Markov chain Monte Carlo; Product form; Queueing networks,Computer simulation; Estimation; Markov processes; Monte Carlo methods; Network performance; Efficient method; Gibbs samplers; Markov Chain; Markov chain Monte Carlo; Performance estimator; Performance measure; Product form; Product forms; Stationary distribution; Teletraffic analysis; Queueing networks
Guest editors' introduction to special issue on the first INFORMS Simulation Society Research Workshop,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849112131&doi=10.1145%2f1667072.1667073&partnerID=40&md5=3c8e04f8379bec7e88b5fd7e502e1ccf,[No abstract available],,
Gradient estimation for discrete-event systems by measure-valued differentiation,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849096186&doi=10.1145%2f1667072.1667077&partnerID=40&md5=6ffd3bed9d9150348d1cb75d8a992339,"In simulation of complex stochastic systems, such as Discrete-Event Systems (DES), statistical distributions are used to model the underlying randomness in the system. A sensitivity analysis of the simulation output with respect to parameters of the input distributions, such as the mean and the variance, is therefore of great value. The focus of this article is to provide a practical guide for robust sensitivity, respectively, gradient estimation that can be easily implemented along the simulation of a DES. We study the Measure-Valued Differentiation (MVD) approach to sensitivity estimation. Specifically, we will exploit the modular structure of the MVD approach, by firstly providing measure-valued derivatives for input distributions that are of importance in practice, and subsequently, by showing that if an input distribution possesses a measure-valued derivative, then so does the overall Markov kernel modeling the system transitions. This simplifies the complexity of applying MVD drastically: one only has to study the measure-valued derivative of the input distribution, a measure-valued derivative of the associated Markov kernel is then given through a simple formula in canonical form. The derivative representations of the underlying simple distributions derived in this article can be stored in a computer library. Combined with the generic MVD estimator, this yields an automated gradient estimation procedure. The challenge in automating MVD so that it can be included into a simulation package is the verification of the integrability condition to guarantee that the estimators are unbiased. The key contribution of the article is that we establish a general condition for unbiasedness which is easily checked in applications. Gradient estimators obtained by MVD are typically phantom estimators and we discuss the numerical efficiency of phantom estimators with the example of waiting times in the G/G/1 queue. © 2010 ACM.",Measure-valued differentiation; Simulation optimization; Stochastic approximation,Approximation theory; Maximum likelihood estimation; Sensitivity analysis; Stochastic models; Stochastic systems; Canonical form; Computer library; Discrete event systems; G/G/1 queue; Gradient estimation; Gradient estimator; Integrability; Markov kernels; Modular structures; Numerical efficiency; Practical guide; Robust sensitivity; Sensitivity estimation; Simulation optimization; Simulation packages; Statistical distribution; Stochastic approximations; System transitions; Waiting time; Estimation
Industrial strength COMPASS: A comprehensive algorithm and software for optimization via simulation,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849089103&doi=10.1145%2f1667072.1667075&partnerID=40&md5=b94683ff91cffedbf26b17e850552838,"Industrial Strength COMPASS (ISC) is a particular implementation of a general framework for optimizing the expected value of a performance measure of a stochastic simulation with respect to integer-ordered decision variables in a finite (but typically large) feasible region defined by linear-integer constraints. The framework consists of a global-search phase, followed by a local-search phase, and ending with a clean-up (selection of the best) phase. Each phase provides a probability 1 convergence guarantee as the simulation effort increases without bound: Convergence to a globally optimal solution in the global-search phase; convergence to a locally optimal solution in the local-search phase; and convergence to the best of a small number of good solutions in the clean-up phase. In practice, ISC stops short of such convergence by applying an improvement-based transition rule from the global phase to the local phase; a statistical test of convergence from the local phase to the clean-up phase; and a ranking-and-selection procedure to terminate the clean-up phase. Small-sample validity of the statistical test and ranking-and-selection procedure is proven for normally distributed data. ISC is compared to the commercial optimization via simulation package OptQuest on five test problems that range from 2 to 20 decision variables and on the order of 104 to 1020 feasible solutions. These test cases represent response-surface models with known properties and realistic system simulation problems. © 2010 ACM.",Optimization via simulation; Random search; Ranking and selection,Computer software; Normal distribution; Optimal systems; Optimization; Probability density function; Statistical tests; Decision variables; Distributed data; Expected values; Feasible regions; Feasible solution; Industrial strength; Integer constraints; Local phase; Optimal solutions; Performance measure; Random searches; Ranking and selection; Realistic systems; Selection of the best; Selection procedures; Simulation packages; Stochastic simulations; Surface models; Test case; Test problem; Transition rule; Stochastic models
Simulation optimization using the cross-entropy method with optimal computing budget allocation,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849090697&doi=10.1145%2f1667072.1667076&partnerID=40&md5=0721e3b0d120a5d9862f841a3e34839a,"We propose to improve the efficiency of simulation optimization by integrating the notion of optimal computing budget allocation into the Cross-Entropy (CE) method, which is a global optimization search approach that iteratively updates a parameterized distribution from which candidate solutions are generated. This article focuses on continuous optimization problems. In the stochastic simulation setting where replications are expensive but noise in the objective function estimate could mislead the search process, the allocation of simulation replications can make a significant difference in the performance of such global optimization search algorithms. A new allocation scheme is developed based on the notion of optimal computing budget allocation. The proposed approach improves the updating of the sampling distribution by carrying out this computing budget allocation in an efficient manner, by minimizing the expected mean-squared error of the CE weight function. Numerical experiments indicate that the computational efficiency of the CE method can be substantially improved if the ideas of computing budget allocation are applied. © 2010 ACM.",Computing budget allocation; Cross-entropy method; Estimation of distribution algorithms; Simulation optimization,Computational efficiency; Entropy; Evolutionary algorithms; Function evaluation; Global optimization; Learning algorithms; Optimization; Stochastic models; Candidate solution; Computing budget; Continuous optimization problems; Cross-entropy method; Estimation of distribution algorithms; Mean squared error; Numerical experiments; Objective functions; Optimal computing budget allocation; Parameterized; Sampling distribution; Search Algorithms; Search process; Simulation optimization; Simulation replication; Stochastic simulations; Weight functions; Budget control
Asymptotic robustness of estimators in rare-event simulation,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849083641&doi=10.1145%2f1667072.1667078&partnerID=40&md5=22741a0c7a6c7c7c2ea0a363f644bfbd,"The asymptotic robustness of estimators as a function of a rarity parameter, in the context of rare-event simulation, is often qualified by properties such as bounded relative error (BRE) and logarithmic efficiency (LE), also called asymptotic optimality. However, these properties do not suffice to ensure that moments of order higher than one are well estimated. For example, they do not guarantee that the variance of the empirical variance remains under control as a function of the rarity parameter. We study generalizations of the BRE and LE properties that take care of this limitation. They are named bounded relative moment of order k (BRM-k) and logarithmic efficiency of order k (LE-k), where k 1 is an arbitrary real number. We also introduce and examine a stronger notion called vanishing relative centered moment of order k, and exhibit examples where it holds. These properties are of interest for various estimators, including the empirical mean and the empirical variance. We develop (sufficient) Lyapunov-type conditions for these properties in a setting where state-dependent importance sampling (IS) is used to estimate first-passage time probabilities. We show how these conditions can guide us in the design of good IS schemes, that enjoy convenient asymptotic robustness properties, in the context of random walks with light-tailed and heavy-tailed increments. As another illustration, we study the hierarchy between these robustness properties (and a few others) for a model of highly reliable Markovian system (HRMS) where the goal is to estimate the failure probability of the system. In this setting, for a popular class of IS schemes, we show that BRM-k and LE-k are equivalent and that these properties become strictly stronger when k increases. We also obtain a necessary and sufficient condition for BRM-k in terms of quantities that can be readily computed from the parameters of the model. © 2010 ACM.",Bounded relative error; Importance sampling; Logarithmic efficiency; Rare-event simulation; Robustness; Zero-variance approximation,Equivalence classes; Estimation; Number theory; Probability; Asymptotic optimality; Asymptotic robustness; Bounded relative error; Failure Probability; First-passage; Heavy-tailed; Importance sampling; Lyapunov; Markovian; Random Walk; Rare event simulation; Real number; Relative moments; Robustness properties; State-dependent; Sufficient conditions; Asymptotic analysis
Simulation modeling for analysis,2010,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849104420&doi=10.1145%2f1667072.1667074&partnerID=40&md5=2c2c0e0423cb3ff372568ae2f3cad8c3,"This article explores possibilities for designing and executing simulation models with specific analysis goals in mind, and shows that a tight coupling of the modeling and analysis phases in a simulation project can lead to dramatic improvements in the study results. Suggestions are made for how simulation analysis, considered in the explicit context of discrete-event simulation models, can create new opportunities for meaningful research and more efficient modeling. Modeling decisions can play a significant role in the performance of analytical procedures. How a simulation model is designed can enable, inhibit, or even invalidate analytical procedures and methodology research results. © 2010 ACM.",Analysis; Discrete-event simulation; Event graphs; Experimental design,Computer simulation; Analytical procedure; Discrete-event simulation model; Event graphs; Experimental design; Modeling and analysis; Modeling decisions; New opportunities; Research results; Simulation analysis; Simulation model; Simulation modeling; Simulation projects; Tight coupling; Statistics
Erratum: Behavior of the NORTA method for correlated random vector generation as the dimension increases (ACM Transactions on Modeling and Computer Simulation (2009) 19: 4 (3)),2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649124483&doi=10.1145%2f1596519.1596525&partnerID=40&md5=0eac66b95316a9e066b75fa7575642a1,[No abstract available],NORTA method; Onion method; Sampling random matrices; Semidefinite programming,
Integrated simulation and optimization for wildfire containment,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649151636&doi=10.1145%2f1596519.1596524&partnerID=40&md5=5fc8608f3f5f5254f394796c61181d38,"Wildfire containment is an important but challenging task. The ability to predict fire spread behavior, optimize a plan for firefighting resource dispatch and evaluate such a plan using several firefighting tactics is essential for supporting decision making for containing wildfires. In this article, we present an integrated framework for wildfire spread simulation, firefighting resource optimization and wildfire suppression simulation. We present a stochastic mixed-integer programming model for initial attack to generate firefighting resource dispatch plans using as input fire spread scenario results from a standard wildfire behavior simulator. A new agent-based discrete event simulation model for fire suppression is used to simulate fire suppression based on dispatch plans from the stochastic optimization model, and in turn provides feedback to the optimization model for revising the dispatch plans if necessary. We report on several experimental results, which demonstrate that different firefighting tactics can lead to significantly different fire suppression results for a given dispatch plan, and simulation of these tactics can provide valuable information for fire managers in selecting dispatch plans from optimization models before actual implementation in the field. © 2009 ACM.",Containment; Suppression; Wildfire spread,Discrete event simulation; Fire extinguishers; Integer programming; Optimization; Stochastic models; Stochastic systems; Agent based; Discrete-event simulation model; Fire managers; Fire spread; Fire suppression; Integrated frameworks; Integrated simulations; Optimization models; Resource dispatch; Resource optimization; Stochastic mixed integer programming; Stochastic optimization model; Wildfire suppression; Fires
Generalized Halton sequences in 2008: A comparative study,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649143987&doi=10.1145%2f1596519.1596520&partnerID=40&md5=266860dedd34d19c32f9a50eb51b55de,"Halton sequences have always been quite popular with practitioners, in part because of their intuitive definition and ease of implementation. However, in their original form, these sequences have also been known for their inadequacy to integrate functions in moderate to large dimensions, in which case (t,s)-sequences such as the Sobol' sequence are usually preferred. To overcome this problem, one possible approach is to include permutations in the definition of Halton sequencesthereby obtaining generalized Halton sequencesan idea that goes back to almost thirty years ago, and that has been studied by many researchers in the last few years. In parallel to these efforts, an important improvement in the upper bounds for the discrepancy of Halton sequences has been made by Atanassov in 2004. Together, these two lines of research have revived the interest in Halton sequences. In this article, we review different generalized Halton sequences that have been proposed recently, and compare them by means of numerical experiments. We also propose a new generalized Halton sequence which, we believe, offers a practical advantage over the surveyed constructions, and that should be of interest to practitioners. © 2009 ACM.",Discrepancy; Halton sequences; Permutations; Scrambling,Comparative studies; Halton sequences; Large dimensions; Numerical experiments; Two-line; Upper Bound
Random variate generation for exponentially and polynomially tilted stable distributions,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649122884&doi=10.1145%2f1596519.1596523&partnerID=40&md5=6c912c9bd2901df6100580f7a02b2d4e,"We develop exact random variate generators for the polynomially and exponentially tilted unilateral stable distributions. The algorithms, which generalize Kanter's method, are uniformly fast over all choices of the tilting and stable parameters. The key to the solution is a new distribution which we call Zolotarev's distribution. We also present a novel double rejection method that is useful whenever densities have an integral representation involving an auxiliary variable. © 2009 ACM.",Expected time analysis; Importance sampling; Monte Carlo method; Probability inequalities; Random variate generation; Rejection method; Simulation; Stable distribution; Tempered distributions,Monte Carlo methods; Expected time; Importance sampling monte carlo methods; Probability inequalities; Random variates; Rejection methods; Stable distributions; Tempered distributions; Probability distributions
Graph annotations in modeling complex network topologies,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450254440&doi=10.1145%2f1596519.1596522&partnerID=40&md5=9c283e9509e81da37c0ed8798868474c,"The coarsest approximation of the structure of a complex network, such as the Internet, is a simple undirected unweighted graph. This approximation, however, loses too much detail. In reality, objects represented by vertices and edges in such a graph possess some nontrivial internal structure that varies across and differentiates among distinct types of links or nodes. In this work, we abstract such additional information as network annotations. We introduce a network topology modeling framework that treats annotations as an extended correlation profile of a network. Assuming we have this profile measured for a given network, we present an algorithm to rescale it in order to construct networks of varying size that still reproduce the original measured annotation profile. Using this methodology, we accurately capture the network properties essential for realistic simulations of network applications and protocols, or any other simulations involving complex network topologies, including modeling and simulation of network evolution. We apply our approach to the Autonomous System (AS) topology of the Internet annotated with business relationships between ASs. This topology captures the large-scale structure of the Internet. In depth understanding of this structure and tools to model it are cornerstones of research on future Internet architectures and designs. We find that our techniques are able to accurately capture the structure of annotation correlations within this topology, thus reproducing a number of its important properties in synthetically-generated random graphs. © 2009 ACM.",Annotations; AS relationships; Complex networks; Topology,Electric network topology; Internet; Internet protocols; Annotations; Autonomous systems; Business relationships; Complex networks; Future internet architecture; In-depth understanding; Internal structure; Large-scale structure; Modeling and simulation; Network applications; Network evolution; Network properties; Network topology; Random graphs; Realistic simulation; Unweighted graphs; Computer simulation
FISTE: A black box approach for end-to-end QoS management,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649086944&doi=10.1145%2f1596519.1596521&partnerID=40&md5=9f58a0f75358a6f0a08d1bde2e0b74f1,"The goal of traffic engineering is to achieve a target Quality of Service (QoS) while maximizing network utilization. While determining the QoS for end-to-end paths in a network under self-similar traffic models is difficult, end-to-end network performance analysis is still essential in providing QoS to networks such as Virtual Private Networks (VPN) and Peer-to-Peer (P2P) networks. The Fast Importance Sampling based Traffic Engineering (FISTE) approach proposed in this article is a prediction-based approach that maps the ingress traffic levels of a network to the QoS of end-to-end path(s) in the network. Because FISTE is a hybrid of simulation analysis and closed-form analysis, it can treat a complex network as a black box. When we combined Simulated Annealing (SA) with FISTE, the resulting approach can provide a traffic engineering solution so that multiple end-to-end QoS requirements are satisfied while the network resource utilization is maximized. FISTE originated from the concept of Importance Sampling (IS), and our approach differs from the previous Importance Sampling based approaches since this is the first time that IS is applied to multi-queue systems under Fractional Gaussian Noise (FGN) input and traffic engineering. © 2009 ACM.",Buffer overflow; Congestion; End-to-end QoS; Fractal gaussian noise; Heuristic search; Importance Sampling; Latency; Monte Carlo; Overlay network; Packet loss; Peer-to-peer; Prediction; Response surface; Self-similar; Simulated annealing; Traffic engineering,Annealing; Buffer storage; Computer architecture; Computer simulation; Engineering; Fractals; Gaussian noise (electronic); Heuristic methods; Highway engineering; Modular robots; Monte Carlo methods; Network performance; Overlay networks; Packet loss; Simulated annealing; Surface properties; Traffic congestion; Buffer overflows; Congestion; End-to-end QoS; Gaussian noise; Heuristic search; Importance sampling; Latency; MONTE CARLO; Peer to peer; Prediction; Response surface; Self-similar; Traffic Engineering; Peer to peer networks
Simulation output analysis using integrated paths II: Low bias estimators,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149086742&doi=10.1145%2f1540530.1540532&partnerID=40&md5=563dde486e7b83cd52c2c78e81530b18,"This article is a sequel to a previous article that introduced a class of variance estimators for steady-state simulation output analysis. The estimators were constructed by applying a quadratic function to a vector obtained from iterated integrations of the simulation output. The previous article concentrated on deriving the limiting distributions of the estimators and on their computational efficiency for a particular choice of quadratic function. The present article considers estimators constructed from different quadratic functions, chosen mainly to reduce bias compared to the estimators of the previous article. Overlapping and nonoverlapping batch means versions of the estimators are discussed. © 2009 ACM.",Efficiency improvement; Variance reduction,Computational efficiency; Efficiency; Batch means; Efficiency improvement; Limiting distributions; Low bias; Quadratic function; Simulation output analysis; Steady-state simulations; Variance estimators; Variance reduction; Estimation
Reduction of closed queueing networks for efficient simulation,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149083763&doi=10.1145%2f1540530.1540531&partnerID=40&md5=699979a49c9024df842e8209d09ed088,"This article gives several methods for approximating a closed queueing network with a smaller one. The objective is to reduce the simulation time of the network. We consider Jackson-like networks with Markovian routing and with general service distributions. The basic idea is to first divide the network into two partsthe core nodes of interest and the remaining nodes. We suppose that only metrics at the core nodes are of interest. The remaining nodes are collapsed into a reduced set of nodes, in an effort to approximate the flows into and out of the set of core nodes. The core nodes and their interactions are preserved in the reduced network. We test the network reductions for accuracy and speed. By randomly generating sample networks, we test the reductions on a large variety of test networks, rather than on a few specific cases. The main conclusion is that the reductions work well when the squared coefficients of variation of the service distributions are not all small (that is, the network is not close to being deterministic) and for nodes where the utilization is not too high or too low. © 2009 ACM.",Network decomposition; Queueing networks,Queueing theory; Basic idea; Closed queueing network; Coefficients of variations; Core nodes; Efficient simulation; Jackson; Markovian; Network decomposition; Network reduction; Service distribution; Simulation time; Test network; Queueing networks
Gradient estimation for a class of systems with bulk services: A problem in public transportation,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149088327&doi=10.1145%2f1540530.1540534&partnerID=40&md5=1d941c0da14feebe019821ab591bd7ac,"This article presents a comparison of different gradient estimators for the sensitivity of waiting times in a bulk server system. Inspired by a transportation network, our model is that of a bursty arrival process that waits at a platform until the server is available (representing a train or bus ready for departure). At the departure epochs, all waiting passengers leave at once. The departure process is assumed to be a renewal process and, based on a limiting result, the interdeparture times are approximated by truncated normal random variables. The interarrival times are assumed to be identically and independently distributed (i.i.d.), with a general distribution of bounded density. We are interested in calculating the sensitivities of the total cumulative waiting time of all passengers with respect to the interdeparture times. For this general model where neither the interarrival times nor the interdeparture times are exponential, there is no analytical formula available. However, the estimation of such sensitivities is an important problem for flow control in such networks. We establish a Smoothed Perturbation Analysis (SPA), a Measure-Valued Differentiation (MVD), and a Score Function (SF) estimator, including numerical experiments. © 2009 ACM.",Bulk servers; Measure-valued differentiation; Score function; Sensitivity analysis; Smoothed perturbation analysis,Estimation; Random variables; Servers; Transportation routes; A-train; Analytical formulas; Bulk servers; Bulk service; Bursty arrivals; Departure process; General model; Gradient estimation; Gradient estimator; Inter-arrival time; Measure-valued differentiation; Numerical experiments; Perturbation Analysis; Public transportation; Renewal process; Score function; Server system; Smoothed perturbation analysis; Transportation network; Truncated normal; Waiting time; Sensitivity analysis
The optimizing-simulator: An illustration using the military airlift problem,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149099695&doi=10.1145%2f1540530.1540535&partnerID=40&md5=d737eef8651f730c75bbd7551382804e,"There have been two primary modeling and algorithmic strategies for modeling operational problems in transportation and logistics: simulation, offering tremendous modeling flexibility, and optimization, which offers the intelligence of math programming. Each offers significant theoretical and practical advantages. In this article, we show that you can model complex problems using a range of decision functions, including both rule-based and cost-based logic, and spanning different classes of information. We show how different types of decision functions can be designed using up to four classes of information. The choice of which information classes to use is a modeling choice, and requires making specific choices in the representation of the problem. We illustrate these ideas in the context of modeling military airlift, where simulation and optimization have been viewed as competing methodologies. Our goal is to show that these are simply different flavors of a series of integrated modeling strategies. © 2009 ACM.",Approximate dynamic programming; Control of simulation; Military logistics; Modeling information; Optimizing-simulator,Logistics; Optimization; Simulators; Systems engineering; Approximate dynamic programming; Decision functions; Integrated modeling; Military logistics; Model complexes; Modeling flexibility; Modeling information; Operational problems; Optimizing-simulator; Rule based; Simulation and optimization; Dynamic programming
Finding probably best systems quickly via simulations,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149084198&doi=10.1145%2f1540530.1540533&partnerID=40&md5=712893355115479f31aba68677f9de3e,"We propose an indifference-zone approach for a ranking and selection problem with the goal of reducing both the number of simulated samples of the performance and the frequency of configuration changes. We prove that with a prespecified high probability, our algorithm finds the best system configuration. Our proof hinges on several ideas, including the use of Anderson's probability bound, that have not been fully investigated for the ranking and selection problem. Numerical experiments show that our algorithm can select the best system configuration using up to 50% fewer simulated samples than existing algorithms without increasing the frequency of configuration changes. © 2009 ACM.",Brownian motion; Indifference zone; Ranking and selection; Simulation output analysis; Switching; Two-stage,Brownian motion; Indifference zone; Ranking and selection; Simulation output analysis; Two-stage; Brownian movement
Analysis of nonstationary stochastic simulations using classical time-series models,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149121802&doi=10.1145%2f1502787.1502792&partnerID=40&md5=ead35476c86b2f034097e3f22aedbb34,"This article extends the use of classical autoregressive and moving average time-series models to the analysis of a variety of nonstationary discrete-event simulations. A thorough experimental evaluation shows that integrated and seasonal time-series models constitute very promising metamodels, especially for analyzing queueing system simulations under congested or cyclical traffic conditions. In some situations, stationarity-inducing transformations may be required before this methodology can be used. Our approach for efficient estimation of meaningful performance measures of selected responses in the target system is illustrated using a set of case studies taken from the simulation literature. © 2009 ACM.",Discrete-event simulation; Output analysis; Simulation metamodels; Time-series models,Simulators; Time series analysis; Auto-regressive; Efficient estimation; Experimental evaluation; Meta model; Moving averages; Nonstationary; Output analysis; Performance measure; Queueing system; Simulation metamodels; Stationarity; Stochastic simulations; Target systems; Time-series models; Traffic conditions; Stochastic models
Probabilistic cost-effectiveness comparison of screening strategies for colorectal cancer,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149099129&doi=10.1145%2f1502787.1502789&partnerID=40&md5=d130d2bef9db9fc0559280f97c48cb0b,"A stochastic discrete-event simulation model of the natural history of Colorectal Cancer (CRC) is augmented with screening technology representations to create a base for simulating various screening strategies for CRC. The CRC screening strategies recommended by the American Gastroenterological Association (AGA) and the newest screening strategies for which clinical efficacy has been established are simulated. In addition to verification steps, validation of screening is pursued by comparison with the Minnesota Colon Cancer Control Study. The model accumulates discounted costs and quality-adjusted life-years. The natural variability in the modeled random variables for natural history is conditioned using a probabilistic sensitivity analysis through a two-stage sampling process that adds other random variables representing parametric uncertainty. The analysis of the screening alternatives in a low-risk population explores both deterministic and stochastic dominance to eliminate some screening alternatives. Net benefit analysis, based on willingness to pay for quality-adjusted life-years, is used to compare the most cost-effective strategies through acceptability curves and to make a screening recommendation. Methodologically, this work demonstrates how variability from the natural variation in the development, screening, and treatment of a disease can be combined with the variation in parameter uncertainty. Furthermore, a net benefit analysis that characterizes cost-effectiveness alternatives can explicitly depend on variation from all sources producing a probabilistic cost-effectiveness analysis of decision alternatives. © 2009 ACM.",Acceptability curves; Colorectal cancer screening strategies; Cost-effectiveness analysis; Medical decision-making; Net benefit analysis; Probabilistic sensitivity analysis,Cost effectiveness; Costs; Decision making; Random processes; Random variables; Risk analysis; Sensitivity analysis; Stochastic models; Uncertainty analysis; Acceptability curves; Colorectal cancer screening strategies; Cost-effectiveness analysis; Medical decision-making; Net benefit analysis; Probabilistic sensitivity analysis; Cost benefit analysis
Optimal parameter trajectory estimation in parameterized SDEs: An algorithmic procedure,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149105512&doi=10.1145%2f1502787.1502791&partnerID=40&md5=3c0e4c463b92eaeff089babfd2eea7e0,"We consider the problem of estimating the optimal parameter trajectory over a finite time interval in a parameterized stochastic differential equation (SDE), and propose a simulation-based algorithm for this purpose. Towards this end, we consider a discretization of the SDE over finite time instants and reformulate the problem as one of finding an optimal parameter at each of these instants. A stochastic approximation algorithm based on the smoothed functional technique is adapted to this setting for finding the optimal parameter trajectory. A proof of convergence of the algorithm is presented and results of numerical experiments over two different settings are shown. The algorithm is seen to exhibit good performance. We also present extensions of our framework to the case of finding optimal parameterized feedback policies for controlled SDE and present numerical results in this scenario as well. © 2009 ACM.",Optimal parameter trajectory; Parameterized stochastic differential equations (SDEs); Simulation optimization; Smoothed functional algorithm,Approximation algorithms; Approximation theory; Differential equations; Measurement theory; Optimization; Parameter estimation; Random processes; Stochastic control systems; Trajectories; Algorithmic procedure; Discretization; Finite time; Finite time intervals; Functional technique; Numerical experiments; Numerical results; Optimal parameter; Optimal parameter trajectory; Parameterized; Parameterized stochastic differential equations (SDEs); Simulation optimization; Simulation-based algorithms; Smoothed functional algorithm; Stochastic approximation algorithms; Stochastic differential equations; Trajectory estimation; Convergence of numerical methods
Two-phase screening procedure for simulation experiments,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149093167&doi=10.1145%2f1502787.1502790&partnerID=40&md5=fbef90e9b160f7af9d6df8d8899748be,"Analysts examining complex simulation models often conduct screening experiments to identify important factors. The controlled sequential bifurcation screening procedures CSB and CSB-X use a sequence of tests to classify factors as important or unimportant, while controlling Type I error and power. These procedures require analysts to identify the directions of the effects prior to experimentation, which can be problematic. We propose hybrid two-phase approaches, FFCSB and FFCSBX, as alternatives. Phase 1 uses an efficient fractional factorial to estimate factor effect directions; phase 2 uses CSB or CSB-X. Empirical investigations show these outperform CSB(X) in efficiency and effectiveness for many situations of practical interest. © 2009 ACM.",Controlled sequential bifurcation; Experimentation; Simulation theory,Bifurcation (mathematics); Complex simulation; Controlled sequential bifurcation; Empirical investigation; Experimentation; Fractional factorials; Phase 1; Phase screening; Screening experiments; Sequential bifurcation; Simulation experiments; Simulation theory; Type-I error; Experiments
Retrospective-approximation algorithms for the multidimensional stochastic root-finding problem,2009,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149111579&doi=10.1145%2f1502787.1502788&partnerID=40&md5=0cfe2190ecc555826efbaa27578752f8,"The stochastic root-finding problem (SRFP) is that of solving a nonlinear system of equations using only a simulation that provides estimates of the functions at requested points. Equivalently, SRFPs seek locations where an unknown vector function attains a given target using only a simulation capable of providing estimates of the function. SRFPs find application in a wide variety of physical settings. We develop a family of retrospective-approximation (RA) algorithms called Bounding RA that efficiently solves a certain class of multidimensional SRFPs. During each iteration, Bounding RA generates and solves a sample-path problem by identifying a polytope of stipulated diameter, with an image that bounds the given target to within stipulated tolerance. Across iterations, the stipulations become increasingly stringent, resulting in a sequence of shrinking polytopes that approach the correct solution. Efficiency results from: (i) the RA structure, (ii) the idea of using bounding polytopes to exploit problem structure, and (iii) careful step-size and direction choice during algorithm evolution. Bounding RA has good finite-time performance that is robust with respect to the location of the initial solution, and algorithm parameter values. Empirical tests suggest that Bounding RA outperforms Simultaneous Perturbation Stochastic Approximation (SPSA), which is arguably the best-known algorithm for solving SRFPs. © 2009 ACM.",Retrospective approximation; Sample-average approximation; Stochastic root finding,Approximation theory; Location; Nonlinear equations; Nonlinear systems; Random processes; Topology; Algorithm parameters; Best-known algorithms; Correct solution; Empirical test; Initial solution; Path problems; Polytope; Polytopes; Problem structure; Retrospective approximation; Sample-average approximation; Simultaneous perturbation stochastic approximation; Stochastic root finding; Stochastic root-finding problem; System of equations; Vector functions; Approximation algorithms
Fidelity of network simulation and emulation: A case study of TCP-targeted denial of service attacks,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67549086487&doi=10.1145%2f1456645.1456649&partnerID=40&md5=6a5b4604411c6cf77e26dc5b4e393a5a,"In this article, we investigate the differences between simulation and emulation when conducting denial of service (DoS) attack experiments. As a case study, we consider low-rate TCP-targeted DoS attacks. We design constructs and tools for emulation testbeds to achieve a level of control comparable to simulation tools. Through a careful sensitivity analysis, we expose difficulties in obtaining meaningful measurements from the DETER, Emulab, and WAIL testbeds with default system settings. We find dramatic differences between simulation and emulation results for DoS experiments. Our results also reveal that software routers such as Click provide a flexible experimental platform, but require understanding and manipulation of the underlying network device drivers. Our experiments with commercial Cisco routers demonstrate that they are highly susceptible to the TCP-targeted attacks when ingress/egress IP filters are used. © 2008 Association for Computing Machinery.",Congestion control; Denial of service attacks; Emulation; Low-rate TCP-targeted attacks; Simulation; TCP; Testbeds,Computer crime; Experiments; Routers; Security of data; Sensitivity analysis; Test facilities; Testbeds; Congestion control; Denial of service attacks; Emulation; Low-rate TCP-targeted attacks; Simulation; TCP; Transmission control protocol
On constructing optimistic simulation algorithms for the discrete event system specification,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67549112287&doi=10.1145%2f1456645.1456646&partnerID=40&md5=4457cad5445a345e3d130efc39fd5424,"This article describes a Time Warp simulation algorithm for discrete event models that are described in terms of the Discrete Event System Specification (DEVS). The article shows how the total state transition and total output function of a DEVS atomic model can be transformed into an event processing procedure for a logical process. A specific Time Warp algorithm is constructed around this logical process, and it is shown that the algorithm correctly simulates a DEVS coupled model that consists entirely of interacting atomic models. The simulation algorithm is presented abstractly; it is intended to provide a basis for implementing efficient and scalable parallel algorithms that correctly simulate DEVS models. © 2008 Association for Computing Machinery.",DEVS; Discrete-event simulation; Parallel simulation; Time Warp,Atoms; Discrete event simulation; Distributed computer systems; Mathematical models; Parallel algorithms; Specifications; Weaving; Atomic models; Coupled models; DEVS; DEVS models; Discrete event models; Discrete event system specification; Logical process; Optimistic simulation; Output functions; Parallel simulation; Processing procedures; Simulation algorithms; Specific time; State transitions; Time Warp; Simulators
Mathematical programming models of closed tandem queueing networks,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60749110047&doi=10.1145%2f1456645.1456648&partnerID=40&md5=3fee06cfe57d14ac126c0a3b7b9ebb42,"Closed tandem queueing networks are an important class of queueing models. Common approaches for analyzing these systems include Markov processes, renewal theory, and random walks. This article presents optimization models for sample paths of closed tandem queues. These mathematical models provide a new tool for analyzing these queueing systems using the techniques and algorithms from mathematical programming, and from graph theory in particular. We then apply operators from computer graphics (electronic picture manipulation) to graph theoretic representations of discrete-event system dynamics to establish some fundamental mathematical properties for these queueing systems. © 2008 Association for Computing Machinery.",Blocking; Event relationship graph; Mathematical programming representation; Queueing network; Reversibility; Symmetry,Computer graphics; Graph theory; Markov processes; Mathematical models; Mathematical operators; Mathematical programming; Queueing networks; Blocking; Discrete event systems; Electronic pictures; Event relationship graph; Graph-theoretic; Mathematical programming models; Mathematical properties; New tools; Optimization models; Queueing model; Queueing system; Random Walk; Renewal theory; Reversibility; Sample path; Symmetry; Tandem queue; Queueing theory
Symbiotic adaptive multisimulation: An autonomic simulation framework for real-time decision support under uncertainty,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67549132890&doi=10.1145%2f1456645.1456647&partnerID=40&md5=0fe2462c5c913468c29cddf19b5ac25a,"Inspired by the compound arthropod eye, Symbiotic Adaptive Multisimulation (SAMS) introduces an autonomic decision support capability for systems in shifting, ill-defined, uncertain environments. Rather than rely on a single authoritative model, SAMS explores an ensemble of plausible models, which are individually flawed but collectively provide more insight than would be possible otherwise. A case study based on a UAV team search and attack model is presented to illustrate the potential of SAMS. Results demonstrate the capability of SAMS to produce a large degree of exploratory behavior, followed by increased exploitative search behavior as the physical system unfolds. © 2008 Association for Computing Machinery.",Agent simulation; Decision support; Robust intelligence; Uncertainty,Decision support systems; Agent simulation; Attack model; Decision support; Decision supports; Physical systems; Plausible model; Robust intelligence; Sams; Search behavior; Simulation framework; Uncertain environments; Uncertainty; Simulators
Approximate bivariate gamma generator with prespecified correlation and different marginal shapes,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53349173582&doi=10.1145%2f1391978.1391982&partnerID=40&md5=7d8bba9988dbfe4550287883056f1ed9,"A new algorithm is proposed for generating approximate bivariate gamma random samples with a prespecified correlation coefficient and different marginal shapes. A distinctive feature of this approach is computational simplicity and ease of control. Extensive testing demonstrates high accuracy of the proposed algorithm. An S-PLUS code implementing the algorithm is provided. Regression lines produced by the technique are nearly linear, even when marginal shapes are drastically different. This feature makes the approach especially useful in simulation studies associated with linear regression problems. A real-life example of application to the analysis of heteroscedastic regression models is presented. This analysis is a part of a bigger study aimed at quantification of risk factors in cancer research. Two-dimensional probabilistic patterns produced by the algorithm are compared to those generated by the well-known technique by Schmeiser and Lal [1982]. © 2008 ACM.",Bivariate gamma distribution; Correlation coefficient; Dietary assessment; Random sampling,Boolean functions; Correlation methods; Risk analysis; Risk assessment; Bivariate gamma distribution; Correlation coefficient; Dietary assessment; Random sampling; Regression analysis
A framework for the simulation of structural software evolution,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53349108818&doi=10.1145%2f1391978.1391983&partnerID=40&md5=65a4da468d2408c375e56fd1b8d89556,"As functionality is added to an aging piece of software, its original design and structure will tend to erode. This can lead to high coupling, low cohesion and other undesirable effects associated with spaghetti architectures. The underlying forces that cause such degradation have been the subject of much research. However, progress in this field is slow, as its complexity makes it difficult to isolate the causal flows leading to these effects. This is further complicated by the difficulty of generating enough empirical data, in sufficient quantity, and attributing such data to specific points in the causal chain. This article describes a framework for simulating the structural evolution of software. A complete simulation model is built by incrementally adding modules to the framework, each of which contributes an individual evolutionary effect. These effects are then combined to form a multifaceted simulation that evolves a fictitious code base in a manner approximating real-world behavior. We describe the underlying principles and structures of our framework from a theoretical and user perspective; a validation of a simple set of evolutionary parameters is then provided and three empirical software studies generated from open-source software (OSS) are used to support claims and generated results. The research illustrates how simulation can be used to investigate a complex and under-researched area of the development cycle. It also shows the value of incorporating certain human traits into a simulation - factors that, in real-world system development, can significantly influence evolutionary structures. © 2008 ACM.",Agent; Evolution; Feedback; Framework software; Metrics; Object-oriented; Plug-in; Simulation; Tool; User,Agent; Evolution; Feedback; Framework software; Metrics; Object-oriented; Plug-in; Simulation; Tool; User; Computer software
Guest editors' introduction to special issue on successes in modeling and simulation methodologies,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53749104630&doi=10.1145%2f1391978.1391979&partnerID=40&md5=b210d9a576024ea9591b1a3dbb1c6f86,"The September 2008 issue of ACM Transactions on Modeling and Computer Simulation (TOMACS) presents an opportunity to showcase successes of M&S (modeling and simulation) in other domains. Ten articles were eventually submitted for review, and of those, two were finally accepted for publication in this issue. The first two articles appearing in this issue are excellent examples of how domain-specific applications can lead to advances in both the theory and practice of M&S. The Editor-in-Chief added two articles on simulation methodology that were motivated by specific applications of simulation to problems in medical decision making and software engineering.",,Computational methods; Computer simulation; Decision theory; Medical problems; Problem solving; Software engineering; Domain-specific; Medical decision-making; Modeling and simulation; Simulation methodology; Decision making
Stochastic formulation of SPICE-type electronic circuit simulation with polynomial chaos,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53349108824&doi=10.1145%2f1391978.1391981&partnerID=40&md5=c3dfd0d8489c51b0eeb4de5802844867,"A methodology for efficient tolerance analysis of electronic circuits based on nonsampling stochastic simulation of transients is formulated, implemented, and validated. We model the stochastic behavior of all quantities that are subject to tolerance spectrally with polynomial chaos. A library of stochastic models of linear and nonlinear circuit elements is created. In analogy to the deterministic implementation of the SPICE electronic circuit simulator, the overall stochastic circuit model is obtained using nodal analysis. In the proposed case studies, we analyze the influence of device tolerance on the response of a lowpass filter, the impact of temperature variability on the output of an amplifier, and the effect of changes of the load of a diode bridge on the probability density function of the output voltage. The case studies demonstrate that the novel methodology is computationally faster than the Monte Carlo method and more accurate and flexible than the root-sum-square method. This makes the stochastic circuit simulator, referred to as PolySPICE, a compelling candidate for the tolerance study of reliability-critical electronic circuits. © 2008 ACM.",Circuit modeling; Circuit simulation; Electronic circuit; Galerkin projection; Nonsampling stochastic analysis; Polynomial chaos; Power electronics; Spectral methods; SPICE; Stochastic differential equations; Tolerance analysis; Transients,Circuit simulation; Electric network analysis; Electron tubes; Fits and tolerances; Mathematical models; Monte Carlo methods; Networks (circuits); Polynomial approximation; Polynomials; Probability density function; Risk assessment; Stochastic control systems; Stochastic models; Stochastic programming; Circuit modeling; Electronic circuit; Galerkin projection; Nonsampling stochastic analysis; Polynomial chaos; Power electronics; Spectral methods; Stochastic differential equations; Tolerance analysis; Transients; SPICE
3D reconstruction and visualization of astrophysical wind volumes using physical models,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53349089750&doi=10.1145%2f1391978.1391980&partnerID=40&md5=388566be5c73289476041009d6520d31,"This article reports on the application of a framework used to model, simulate and visualize the 3D structure of astrophysical wind volumes. The modeling methodology is similar to multidirectional medical tomography in that the spatial structure of an extended target can be reconstructed from a sequence of images obtained by scanning that target from several directions. Even though the controlled environment realized in diagnostic radiology cannot be replicated in the study of astrophysical phenomena, strong candidates for astrophysical tomography do exist in hot, close double stars locked in orbits around a common center of mass. As the Keplerian orbit is traced out, the geometry presented to the observer varies so that each star constitutes an analyzer upon its companion's wind and probes its structure. Since these targets are too far to be resolved spatially, we study and model the UV spectral lines of prominent wind ions, which scatter photospheric UV light so that line shapes vary as the stars revolve and as inhomogeneities form, propagate, and evolve in the composite wind. The framework presented is applied to two hot close binaries near the applicability limits of the discussed methodology. Two novel custom-made tools that aid the analysis of the spectra and the visualization of the results are also presented. The first of these, the Spectrum Analyzer and Animator, automates the derivation of light curves from the observed spectra and the generation of synthetic binary wind-line profiles, which reproduce the morphologies and variabilities of the observed wind profiles. After the composite wind structure of a binary has been recovered, the second tool, the Binary 3D Renderer - also authored in IDL - aids the visualization of the results by simulating the motion of the system (stars, winds and wind-interaction effects) around the binary's center of mass. The Binary 3D Renderer thus repackages the end product of a lengthy physical modeling process to generate physically sound, realistic multimedia content and increase the effectiveness and communication impact of the research results. © 2008 ACM.",3D volume reconstruction; Early type binaries; Photometry; Physical models; Spectroscopic binaries; Synthetic light curves; Volume visualization; Wind modeling,Animation; Astrophysics; Diagnostic radiography; Electromagnetic waves; Image reconstruction; Medical imaging; Models; Multimedia systems; Orbits; Spectrum analyzers; Stars; Targets; Three dimensional; Three dimensional computer graphics; Tomography; Visualization; Wind effects; 3D volume reconstruction; Early type binaries; Photometry; Physical models; Spectroscopic binaries; Synthetic light curves; Volume visualization; Wind modeling; Structural design
A framework for simulation of surrounding vehicles in driving simulators,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249135768&doi=10.1145%2f1371574.1371575&partnerID=40&md5=4a2e3745a451a1fa53641737b453840a,"This article describes a framework for generation and simulation of surrounding vehicles in a driving simulator. The proposed framework generates a traffic stream, corresponding to a given target flow and simulates realistic interactions between vehicles. The framework is based on an approach in which only a limited area around the driving simulator vehicle is simulated. This closest neighborhood is divided into one inner area and two outer areas. Vehicles in the inner area are simulated according to a microscopic simulation model including advanced submodels for driving behavior while vehicles in the outer areas are updated according to a less time-consuming mesoscopic simulation model. The presented work includes a new framework for generating and simulating vehicles within a moving area. It also includes the development of an enhanced model for overtakings and a simple mesoscopic traffic model. The framework has been validated on the number of vehicles that catch up with the driving simulator vehicle and vice versa. The agreement is good for active and passive catch-ups on rural roads and for passive catch-ups on freeways, but less good for active catch-ups on freeways. The reason for this seems to be deficiencies in the utilized lane-changing model. It has been verified that the framework is able to achieve the target flow and that there is a gain in computational time of using the outer areas. The framework has also been tested within the VTI Driving simulator III. © 2008 ACM.",Behavior modeling; Driving behavior; Driving simulators; Mesoscopic traffic simulation; Microscopic traffic simulation; Real-time simulation; Traffic generation; Traffic simulation,Automobile simulators; Highway systems; Superconducting materials; Vehicles; Computational time; Driving behaviors; Driving simulators; Lane changing models; Mesoscopic; Mesoscopic simulations; Microscopic simulations; Number of vehicles; Sub-models; Target flow; Traffic modelling; Traffic streams; Simulators
Large-scale testing of the Internet's Border Gateway Protocol (BGP) via topological scale-down,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249091291&doi=10.1145%2f1371574.1371577&partnerID=40&md5=45ec9e201739e6752acd89072b84e396,"The Internet is a critical communication infrastructure servicing billions of end-users world-wide. Ongoing studies of the Internet's operations show that data loss and increased latency are occurring due to weaknesses in its interdomain routing protocol, BGP. Many solutions have been proposed, but few have experienced widespread adoption. Both the delayed discovery of the protocol's shortcomings, and apathy for its proposed solutions, are partially due to inadequate testing practices. Internet interdomain routing technologies are not evaluated at appropriate scale. Better testing is suggested, which incorporates the specification of large-scale experimental topologies. This is necessary, as BGP performs the distributed operation of interdomain routing across the thousands of networks composing the Internet. However, only small to moderately sized topologies can be currently accommodated by today's testing platforms. A modeling methodology based on path preserving scale-down is proposed to extend the topological scale of interdomain routing experimentation. A given Internet topology is reduced in terms of its autonomous systems (ASes) using a combination of Gaussian elimination and several graphical heuristics. The interdomain routing paths generated by BGP on this reduced topology are also preserved. Path preservation keeps the length, composition, and ordering of these routing paths unchanged. When the routing paths guiding Internet traffic among ASes are preserved across the size reduction, the large-scale traffic engineering induced by BGP can be estimated at much lower scales. Internet data losses due to certain inappropriate interdomain routing behaviors can be identified. As an example, a persistent multiple origin autonomous system (MOAS) conflict is characterized over a topology containing 8826 ASes. It is shown that this problem's large-scale characterization can be obtained using scale-down models that are 70% smaller, and thus more accommodating to common testing platforms (e.g., simulation and networking testbeds).",BGP; Interdomain routing; Model reduction; Network topology,Arsenic compounds; Canning; Food preservation; Highway engineering; Internet; Internet protocols; Routing protocols; Telecommunication networks; Topology; Autonomous System (AS); Border Gateway Protocol (BGP); Communication infrastructures; Data losses; End-users; Gaussian elimination; Inter-domain routing; Internet data; Internet topologies; Internet traffic; Modeling methodologies; Routing paths; Scale characterization; Size reductions; Test-beds; Testing platforms; Traffic Engineering (TE); Gateways (computer networks)
A metamodel for federation architectures,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249139060&doi=10.1145%2f1371574.1371576&partnerID=40&md5=08687913adefa0687e9e75cc7a9ed0e9,"This article proposes a metamodel for describing the architecture of a High Level Architecture (HLA) compliant federation. A salient feature of the Federation Architecture Metamodel (FAMM) is the behavioral description of federates based on live sequence charts. FAMM formalizes the standard HLA Object Model and Federate Interface Specification. FAMM supports processing through automated tools, and in particular through code generation. It is formulated in metaGME, the metamodel for the Generic Modeling Environment. © 2008 ACM.",Architecture; Code generation; Generic modeling environment; High-level architecture; Live sequence charts; Message sequence charts; Metamodel; Simulation,Graphic methods; Standards; Automated tools; Code generations; Generic Modeling Environment (GME); High level architecture (HLA); Interface specifications; Live sequence charts; Meta modelling; Object modelling; Salient features; Codes (symbols)
"Modeling, scheduling, and simulation of switched processing systems",2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249103529&doi=10.1145%2f1371574.1371578&partnerID=40&md5=fffb1a0dce2d6976d566e6424ab1e866,"Switched Processing Systems (SPS) serve as canonical models in a wide area of applications such as high performance computing, wireless networking, call centers, and flexible manufacturing. In this article, we model the SPS by considering both slotted and continuous time and analyze it under fairly mild stochastic assumptions. Two classes of scheduling policies are introduced and shown to maximize the throughput and maintain strong stability of the system. In addition, their performance with respect to the average job sojourn time is examined by simulating small SPS subject to different types of input traffic. By utilizing the simulation result of the proposed policies, a hybrid control policy is constructed to reduce the average job sojourn time when the system has unknown and changing input loads. © 2008 ACM.",Average sojourn time; Maximal throughput; Scheduling policy; Simulation; Strong stability; Switched processing systems,Flexible manufacturing systems; Scheduling; Stochastic models; Call centers; Canonical modeling; Continuous-time (CT); Flexible manufacturing; High performance computing (HiPC); Hybrid control; Input traffic; Processing systems; Scheduling policies; Simulation results; Sojourn time; Strong stability; Wide area; Wireless networking; System stability
Efficient simulation of Internet worms,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149107469&doi=10.1145%2f1346325.1346326&partnerID=40&md5=e0a7e7575cb99c076d4683a09a4e99db,"Simulation of Internet worms (and other malware) requires tremendous computing resources when every packet generated by the phenomena is modeled individually; on the other hand, models of worm growth based on differential equations lack the significant variability inherent in worms that sample targets randomly. This article addresses the problem with a model that focuses on times of infection. We propose a hybrid discrete-continuous model that minimizes execution time subject to an accuracy constraint on variance. We also develop an efficiently executed model of preferential random scanning and use it to investigate the sensitivity of worm propagation speed to the distribution of susceptible hosts through the network, and to the local preference probability. Finally, we propose and study two optimizations to a fluid-based simulation of scan traffic through a backbone network, observing an order-of-magnitude improvement in execution speed. © 2008 ACM.",Denial-of-service; Modeling; Simulation; Worms,Computer simulation; Differential equations; Internet; Mathematical models; Denial-of-service; Discrete-continuous model; Random scanning; Computer worms
A behavioral theory of insider-threat risks: A system dynamics approach,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149107471&doi=10.1145%2f1346325.1346328&partnerID=40&md5=e98ae573fa1bc3d5655e35e4ac9418cf,"The authors describe a behavioral theory of the dynamics of insider-threat risks. Drawing on data related to information technology security violations and on a case study created to explain the dynamics observed in that data, the authors constructed a system dynamics model of a theory of the development of insider-threat risks and conducted numerical simulations to explore the parameter and response spaces of the model. By examining several scenarios in which attention to events, increased judging capabilities, better information, and training activities are simulated, the authors theorize about why information technology security effectiveness changes over time. The simulation results argue against the common presumption that increased security comes at the cost of reduced production. © 2008 ACM.",Behavioral theory; Insider threat; Judgment and decision making; Policy analysis; Risk; Security modeling; Signal detection theory; System dynamics modeling,Computer crime; Computer simulation; Mathematical models; Risk assessment; Security of data; Behavioral theory; Insider threat; Policy analysis; Security modeling; Signal detection theory; System dynamics modeling; Behavioral research
Deterministic and stochastic models for the detection of random constant scanning worms,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149085067&doi=10.1145%2f1346325.1346329&partnerID=40&md5=cfeaa9bd3d247107221e6e763350dfbf,"This article discusses modeling and detection properties associated with the stochastic behavior of Random Constant Scanning (RCS) worms. Although these worms propagate by randomly scanning network addresses to find hosts that are susceptible to infection, traditional RCS worm models are fundamentally deterministic. A density-dependent Markov jump process model for RCS worms is presented and analyzed herein. Conditions are shown for when some stochastic properties of RCS worm propagation can be ignored and when deterministic RCS worm models can be used. A computationally simple hybrid deterministic/ stochastic point-process model for locally observed scanning behavior due to the global propagation of an RCS scanning worm epidemic is presented. An optimal hypothesis-testing approach is presented to detect epidemics of these under idealized conditions based on the cumulative sums of log-likelihood ratios using the hybrid RCS worm model. This article presents in a mathematically rigorous fashion why detection techniques that are only based on passively monitoring local IP addresses cannot quickly detect the global propagation of an RCS worm epidemic with a low false alarm rate, even under idealized conditions. © 2008 ACM.",Epidemic modeling; Hypothesis testing; Stochastic analysis; Worms,Computer networks; Markov processes; Mathematical models; Software testing; Epidemic modeling; Hypothesis testing; Stochastic analysis; Computer worms
A model of the spread of randomly scanning Internet worms that saturate access links,2008,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149116118&doi=10.1145%2f1346325.1346327&partnerID=40&md5=a7851d302f78c4fefdcb1c0b17c46382,"We present a simple, deterministic mathematical model for the spread of randomly scanning and bandwidth-saturating Internet worms. Such worms include Slammer and Witty, both of which spread extremely rapidly. Our model, consisting of coupled Kermack-McKendrick (a.k.a. stratified susceptibles-infectives (SI)) equations, captures both the measured scanning activity of the worm and the network limitation of its spread, that is, the effective scan-rate per worm/infective. The Internet is modeled as an ideal core network to which each peripheral (e.g., enterprise) network is connected via a single access link. It is further assumed in this note that as soon as a single end-system in the peripheral network is infected by the worm, the subsequent scanning of the rest of the Internet saturates the access link, that is, there is instant saturation. We fit our model to available data for the Slammer worm and demonstrate the model's ability to accurately represent Slammer's total scan-rate to the core. © 2008 ACM.",Communications/computer networking; Epidemiology; Internet worms; Modeling,Computer worms; Database systems; Epidemiology; Mathematical models; Random processes; Internet worms; Stratified susceptibles-infectives (SI)) equations; Internet
Adaptive Newton-based multivariate smoothed functional algorithms for simulation optimization,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349128504&doi=10.1145%2f1315575.1315577&partnerID=40&md5=2c6495077e1435b13fd440569493b258,"In this article, we present three smoothed functional (SF) algorithms for simulation optimization. While one of these estimates only the gradient by using a finite difference approximation with two parallel simulations, the other two are adaptive Newton-based stochastic approximation algorithms that estimate both the gradient and Hessian. One of the Newton-based algorithms uses only one simulation and has a one-sided estimate in both the gradient and Hessian, while the other uses two-sided estimates in both quantities and requires two simulations. For obtaining gradient and Hessian estimates, we perturb each parameter component randomly using independent and identically distributed (i.i.d) Gaussian random variates. The earlier SF algorithms in the literature only estimate the gradient of the objective function. Using similar techniques, we derive two unbiased SF-based estimators for the Hessian and develop suitable three-timescale stochastic approximation procedures for simulation optimization. We present a detailed convergence analysis of our algorithms and show numerical experiments with parameters of dimension 50 on a setting involving a network of M/G/1 queues with feedback. We compare the performance of our algorithms with related algorithms in the literature. While our two-simulation Newton-based algorithm shows the best results overall, our one-simulation algorithm shows better performance compared to other one-simulation algorithms.© 2007 ACM.",Gaussian perturbations; Newton-based algorithms; Simulation optimization; Smoothed functional algorithms; Three-timescale stochastic approximation,Approximation algorithms; Computer simulation; Functional analysis; Optimization; Perturbation techniques; Stochastic models; Gaussian perturbations; Simulation optimization; Smoothed functional algorithms; Adaptive systems
Development of a simulation model of colorectal cancer,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349186619&doi=10.1145%2f1315575.1315579&partnerID=40&md5=36806d9554762c13f9c8ff619779d3f5,"Colorectal cancer (CRC) is deadly if not found early. Any protocols developed for screening and surveillance and any policy decisions regarding the availability of CRC resources should consider the nature of the disease and its impact over time on costs and quality-adjusted life years in a population. Simulation models can provide a flexible representation needed for such analysis; however, the development of a credible simulation model of the natural history of CRC is hindered by limited data and incomplete knowledge. To accommodate the extensive modeling and remodeling required to produce a credible model, we created an object-oriented simulation platform driven by a model-independent database within the .NET environment. The object-oriented structure not only encapsulated the needs of a simulation replication but created an extensible framework for specialization of the CRC components. This robust framework allowed development to focus modeling on the CRC events and their event relationships, conveniently facilitating extensive revision during model construction. As a second-generation CRC modeling activity, this model development benefited from prior experience with data sources and modeling difficulties. A graphical user interface makes the model accessible by displaying existing scenarios, showing input variables and their values, and permitting the creation of new scenarios and changes to its input. Output from the simulation is captured in familiar tabbed worksheets and stored in the database. The eventual CRC model was conceptualized through a series of assumptions that conformed to beliefs and data regarding the natural history of CRC. Throughout the development cycle, extensive verification and validation calibrated the model. The result is a simulation model that characterizes the natural history of CRC with sufficient accuracy to provide an effective means of evaluating numerous issues regarding the burden of this disease on individuals and society. Generalizations from this study are offered regarding the use of discrete-event simulation in disease modeling and medical decision making. © 2007 ACM.",Colorectal cancer; Medical applications; Medical decision making,Computer simulation; Database systems; Graphical user interfaces; Object oriented programming; Population dynamics; Colorectal cancer (CRC); Medical decision making; Oncology
Sample-based estimation of correlation ratio with polynomial approximation,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349139780&doi=10.1145%2f1315575.1315578&partnerID=40&md5=64477cd78fd2875e24bc4814b1da4340,"Sensitivity analysis has become a natural step in the uncertainty analysis framework. As there is no general sensitivity measure that would capture all information on impact of input factors on model output, analysts tend to combine various measures to obtain a broader image of interactions between different modes. This article concentrates on the correlation ratio, demonstrates methods for calculating this quantity efficiently and accurately, and compares the results. A new method inspired by artificial intelligence techniques emerges as outperforming the familiar methods. © 2007 ACM.",Correlation ratio; Sensitivity analysis; Sobol indices,Artificial intelligence; Information analysis; Polynomial approximation; Sensitivity analysis; Uncertainty analysis; Correlation ratio; Sobol indices; Correlation methods
An adaptive approach to accelerated evaluation of highly available services,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349091401&doi=10.1145%2f1315575.1315576&partnerID=40&md5=1c50de44d2a8f6e024ec227aacff3732,"We motivate and describe improved fast simulation techniques for the accelerated performance evaluation of highly available services. In systems that provide such services, service unavailability events are rare due to a low component failure rate or high resource capacity. Using traditional Monte Carlo simulation to evaluate such services requires a large amount of runtime. Importance sampling (IS) has been applied to certain instances of such systems, focusing on single-class and/or homogeneous resource demands. In this article, we formulate highly available services as multiresource losstype systems, and we present two IS methods for fast simulation, extending to multiple classes and nonhomogeneous resource demands. First, for the cases in which component failure rates are small, we prove that static IS using the Standard Clock (S-ISSC) method exhibits the bounded relative error (BRE) property. Second, for estimating failure probabilities due to large capacity or fast service in systems that have nonrare component failure rates, we propose adaptive ISSC (A-ISSC), which estimates the relative probability of reaching each possible state of system failure in every step of the simulation. Using A-ISSC, IS methods which are proven to be efficient can be extended to multidimensional cases, while still retaining a very favorable performance, as supported by our validation experiments. © 2007 ACM.",Adaptive importance sampling; Highly available services; Rare event simulation,Computer simulation; Computer system recovery; Monte Carlo methods; Probability; Bounded relative error (BRE); Importance sampling (IS); Rare event simulation; Adaptive systems
Inverse transformed density rejection for unbounded monotone densities,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34648827242&doi=10.1145%2f1276927.1276931&partnerID=40&md5=7df3528da20e12af83be39163996828b,"A new algorithm for sampling from largely arbitrary monotone, unbounded densities is presented. The user has to provide a program to evaluate the density and its derivative and the location of the pole. Then the setup of the new algorithm constructs different hat functions for the pole region and tail region, respectively. For the pole region a new method is developed that uses a transformed density rejection hat function of the inverse density. As the order of the pole is calculated in the setup, conditions that guarantee correctness of the constructed hat functions are provided. Numerical experiments indicate that the new algorithm works correctly and moderately fast for many different unbounded densities. © 2007 ACM.",Black-box algorithm; Nonuniform random variates; Transformed density rejection; Unbounded densities; Universal method,Algorithms; Computer simulation; Inverse problems; Mathematical transformations; Numerical analysis; User interfaces; Black-box algorithm; Nonuniform random variates; Transformed density rejection; Unbounded densities; Universal method; Density functional theory
Multistep-ahead neural-network predictors for network traffic reduction in distributed interactive applications,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34648831108&doi=10.1145%2f1276927.1276929&partnerID=40&md5=0ea1b679a7c0b29a1b65521722a7af4a,"Predictive contract mechanisms such as dead reckoning are widely employed to support scalable remote entity modeling in distributed interactive applications (DIAs). By employing a form of controlled inconsistency, a reduction in network traffic is achieved. However, by relying on the distribution of instantaneous derivative information, dead reckoning trades remote extrapolation accuracy for low computational complexity and ease-of-implementation. In this article, we present a novel extension of dead reckoning, termed neuro-reckoning, that seeks to replace the use of instantaneous velocity information with predictive velocity information in order to improve the accuracy of entity position extrapolation at remote hosts. Under our proposed neuro-reckoning approach, each controlling host employs a bank of neural network predictors trained to estimate future changes in entity velocity up to and including some maximum prediction horizon. The effect of each estimated change in velocity on the current entity position is simulated to produce an estimate for the likely position of the entity over some short time-span. Upon detecting an error threshold violation, the controlling host transmits a predictive velocity vector that extrapolates through the estimated position, as opposed to transmitting the instantaneous velocity vector. Such an approach succeeds in reducing the spatial error associated with remote extrapolation of entity state. Consequently, a further reduction in network traffic can be achieved. Simulation results conducted using several human users in a highly interactive DIA indicate significant potential for improved scalability when compared to the use of IEEE DIS standard dead reckoning. Our proposed neuro-reckoning framework exhibits low computational resource overhead for real-time use and can be seamlessly integrated into many existing dead reckoning mechanisms. © 2007 ACM.",Collaborative virtual environments; Consistency; Dead reckoning; Distributed interactive applications; Distributed interactive simulation; Multistep-ahead prediction; Network bandwidth reduction; Networked multiplayer computer games,Computational complexity; Computer simulation; Distributed computer systems; Error analysis; Extrapolation; Interactive computer systems; Predictive control systems; Real time systems; User interfaces; Collaborative virtual environments; Dead reckoning; Distributed interactive applications; Network bandwidth reduction; Networked multiplayer computer games; Neural networks
A framework for locally convergent random-search algorithms for discrete optimization via simulation,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34648813290&doi=10.1145%2f1276927.1276932&partnerID=40&md5=c4e391c46b34099b6aa7d5cd180a5259,"The goal of this article is to provide a general framework for locally convergent random-search algorithms for stochastic optimization problems when the objective function is embedded in a stochastic simulation and the decision variables are integer ordered. The framework guarantees desirable asymptotic properties, including almost-sure convergence and known rate of convergence, for any algorithms that conform to its mild conditions. Within this framework, algorithm designers can incorporate sophisticated search schemes and complicated statistical procedures to design new algorithms. © 2007 ACM.",Discrete stochastic optimization; Random search,Computer simulation; Convergence of numerical methods; Discrete time control systems; Optimization; Problem solving; Stochastic models; Decision variables; Discrete stochastic optimization; Random search; Rate of convergence; Algorithms
"Discrete-time heavy-tailed chains, and their properties in modeling network traffic",2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34648828094&doi=10.1145%2f1276927.1276930&partnerID=40&md5=0bff50e22db994bda2f8adc650fd9cd2,"The particular statistical properties found in network measurements, namely self-similarity and long-range dependence, cannot be ignored in modeling network and Internet traffic. Thus, despite their mathematical tractability, traditional Markov models are not appropriate for this purpose, since their memoryless nature contradicts the burstiness of transmitted packets. However, it is desirable to find a similarly tractable model which is, at the same time, rigorous at capturing the features of network traffic. This work presents discrete-time heavy-tailed chains, a tractable approach to characterize network traffic as a superposition of discrete-time on/off sources. This is a particular case of the generic on/off heavy-tailed model, thus shows the same statistical features as the former, particularly self-similarity and long-range dependence, when the number of aggregated sources approaches infinity. The model is then applicable to characterize a number of discrete-time communication systems, for instance, ATM and optical packet switching, to further derive meaningful performance metrics such as average burst duration and the number of active sources in a random instant. © 2007 ACM.",Discrete-time heavy-tailed chains; Fractional Brownian motion; Heavy-tailed distributions; Long-range dependence; Self-similar processes,Automatic teller machines; Computer simulation; Discrete time control systems; Internet; Mathematical models; Packet switching; Discrete-time heavy-tailed chains; Fractional Brownian motion; Heavy-tailed distributions; Internet traffic; Long-range dependence; Network measurements; Network traffic; Neural networks
Common defects in initialization of pseudorandom number generators,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34648834264&doi=10.1145%2f1276927.1276928&partnerID=40&md5=8253d4b864aa48145aef93044ddf358a,"We demonstrate that a majority of modern random number generators, such as the newest version of rand.c, ranlux, and combined multiple recursive generators, have some manifest correlations in their outputs if the initial state is filled up using another linear recurrence with similar modulus. Among 58 available generators in the GNU scientific library, 40 show such defects. This is not because of the recursion, but because of carelessly chosen initialization schemes in the implementations. A good initialization scheme eliminates this phenomenon. © 2007 ACM.",Difference collision; Interstream correlation; Monte-Carlo simulation; Nearly affine dependence; Pseudorandom number generator,Linear systems; Monte Carlo methods; Recursive functions; Difference collision; Interstream correlation; Nearly affine dependence; Pseudorandom number generator; Random number generation
Distributed simulation of agent-based systems with HLA,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547550389&doi=10.1145%2f1243991.1243992&partnerID=40&md5=8295d5743ebc2beaa85f7ebab32902af,"In this article we describe HLA_AGENT, a tool for the distributed simulation of agent-based systems, which integrates the SIM_AGENT agent toolkit and the High Level Architecture (HLA) simulator interoperability framework. HLA_AGENT offers enhanced simulation scalability and allows interoperation with other HLA-compliant simulators, promoting simulation reuse. Using a simple Tileworld example, we show how HLA_AGENT can be used to flexibly distribute a SIM_AGENT simulation so as to exploit available computing resources. We present experimental results that illustrate the performance of HLA_AGENT on a Linux cluster running a distributed version of Tileworld and compare this with the original nondistributed SIM_AGENT version. © 2007 ACM.",High level architecture; HLA_AGENT; IEEE 1516; Multiagent systems,Cluster analysis; Interoperability; Network architecture; Scalability; Simulators; Standards; High Level Architecture (HLA); Interoperation; SIM_AGENT simulation; Multi agent systems
Importance sampling for sums of random variables with regularly varying tails,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547536530&doi=10.1145%2f1243991.1243995&partnerID=40&md5=e121d525c87222330556045da5ee3496,"Importance sampling is a variance reduction technique for efficient estimation of rare-event probabilities by Monte Carlo. For random variables with heavy tails there is little consensus on how to choose the change of measure used in importance sampling. In this article we study dynamic importance sampling schemes for sums of independent and identically distributed random variables with regularly varying tails. The number of summands can be random but must be independent of the summands. For estimating the probability that the sum exceeds a given threshold, we explicitly identify a class of dynamic importance sampling algorithms with bounded relative errors. In fact, these schemes are nearly asymptotically optimal in the sense that the second moment of the corresponding importance sampling estimator can be made as close as desired to the minimal possible value. © 2007 ACM.",Asymptotically optimal relative error; Bounded relative error; Dynamic importance sampling; Rare events; Regularly varying tails; Variance reduction,Algorithms; Asymptotic analysis; Error analysis; Monte Carlo methods; Asymptotically optimal relative error; Bounded relative error; Dynamic importance sampling; Rare events; Regularly varying tails; Variance reduction; Random variables
Simulation output analysis using integrated paths,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547523913&doi=10.1145%2f1243991.1243994&partnerID=40&md5=619df97a2a1e6e5059b95110a93f752f,"This article considers the steady-state simulation output analysis problem for a process that satisfies a functional central limit theorem. We construct an estimator for the time-average variance constant that is based on iterated integrations of the sample path. When the observations are batched, the method generalizes the method of batch means. One advantage of the method is that it can be used without batching the observations; that is, it can allow for the process variance to be estimated at any time as the simulation runs without waiting for a fixed time horizon to complete. When used in conjunction with batching, the method can improve efficiency (the reciprocal of work times mean-squared error) compared with the standard method of batch means. In numerical experiments, efficiency improvement ranged from a factor of 1.5 (for the waiting time sequence in an M/M/1 queueing system with a single integrated path) up to a factor of 14 (for an autoregressive process and 19 integrated paths). © 2007 ACM.",Efficiency improvement; Variance reduction,Computational efficiency; Computer simulation; Iterative methods; Problem solving; Regression analysis; Batch means; Efficiency improvement; Time sequence; Variance reduction; Theorem proving
A Co-Plot analysis of logs and models of parallel workloads,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547530642&doi=10.1145%2f1243991.1243993&partnerID=40&md5=6e63b9b0c9a1fba15c44f663bb6588e2,"We present a multivariate analysis technique called Co-Plot that is especially suitable for few samples of many variables. Co-Plot embeds the multidimensional samples in two dimensions, in a way that allows key variables to be identified, and relations between both variables and observations to be analyzed together. When applied to the workloads on parallel supercomputers, we find two stable perpendicular axes of highly correlated variables, one representing individual job attributes and the other representing multijob attributes. The different workloads, on the other hand, are rather different from one another, and may also change over time. Synthetic models for workload generation are also analyzed, and found to be reasonable in the sense that they span the same range of variable combinations as the real workloads. However, the spread of real workloads implies that a single model cannot be similar to all of them. This leads us to construct a parameterized model, with parameters that correspond to the two axes identified above. We also find that existing models do not model the temporal structure of the workload well, and hence are wanting for tasks such as comparing schedulers, and that the common methodology for load manipulation of workloads is problematic. © 2007 ACM.",Co-plot; Load manipulation; Multivariate analysis; Nonstationary workload; Parallel workloads; Parametric model; Synthetic workload; Workload modeling,Correlation methods; Embedded systems; Mathematical models; Multivariable control systems; Parameterization; Supercomputers; Load manipulation; Multivariate analysis; Nonstationary workload; Parallel workloads; Parametric model; Synthetic workload; Parallel processing systems
Efficient importance sampling heuristics for the simulation of population overflow in Jackson networks,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247277188&doi=10.1145%2f1225275.1225281&partnerID=40&md5=74bd49842d89ea77ef4168e71f22bdab,"In this article, we propose state-dependent importance sampling heuristics to estimate the probability of population overflow in Jackson queueing networks. These heuristics capture state-dependence along the boundaries (when one or more queues are empty), which is crucial for the asymptotic efficiency of the change of measure. The approach does not require difficult (and often intractable) mathematical analysis and is not limited by storage and computational requirements involved in adaptive importance sampling methodologies, particularly for a large state space. Experimental results on tandem, parallel, feed-forward, and feedback networks with a moderate number of nodes suggest that the proposed heuristics may yield asymptotically efficient estimators, possibly with bounded relative error, when applied to queueing networks wherein no other state-independent importance sampling techniques are known to be efficient. The heuristics are robust and remain effective for larger networks. Moreover, insights drawn from the basic networks considered in this article help understand sample path behavior along the boundaries, conditional on reaching the rare event of interest. This is key to the application of the methodology to networks of more general topologies. It is hoped that empirical findings and insights in this paper will encourage more research on related practical and theoretical issues. © 2007 ACM.",Asymptotic efficiency; Importance sampling; Jackson networks; Queuing models; Rare event simulation; Variance reduction,Computer simulation; Data flow analysis; Error analysis; Heuristic methods; Probability distributions; Requirements engineering; Robust control; Asymptotic efficiency; Importance sampling; Jackson networks; Queuing models; Rare event simulation; Variance reduction; Computer systems
Exploiting regenerative structure to estimate finite time averages via simulation,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247193129&doi=10.1145%2f1225275.1225279&partnerID=40&md5=8c0c8108e0dc1eb50a38dcac08f8ebb5,"We propose nonstandard simulation estimators of expected time averages over finite intervals [0, t], seeking to enhance estimation efficiency. We make three key assumptions: (i) the underlying stochastic process has regenerative structure, (ii) the time average approaches a known limit as time t increases and (iii) time 0 is a regeneration time. To exploit those properties, we propose a residual-cycle estimator, based on data from the regenerative cycle in progress at time t, using only the data after time t. We prove that the residual-cycle estimator is unbiased and more efficient than the standard estimator for all sufficiently large t. Since the relative efficiency increases in t, the method is ideally suited to use when applying simulation to study the rate of convergence to the known limit. We also consider two other simulation techniques to be used with the residual-cycle estimator. The first involves overlapping cycles, paralleling the technique of overlapping batch means in steady-state estimation; multiple observations are taken from each replication, starting a new observation each time the initial regenerative state is revisited. The other technique is splitting, which involves independent replications of the terminal period after time t, for each simulation up to time t. We demonstrate that these alternative estimators provide efficiency improvement by conducting simulations of queueing models. © 2007 ACM.",Efficiency improvement; Regenerative processes; Time averages; Variance reduction,Computer simulation; Finite difference time domain method; Mathematical models; State estimation; Efficiency improvement; Regenerative processes; Time averages; Variance reduction; Queueing theory
"Perwez Shahabuddin, 1962 - 2005: A professional appreciation",2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247248353&doi=10.1145%2f1225275.1225277&partnerID=40&md5=928935a4d97e4a49a22d02ea161f7a21,"Perwez Shahabuddin was an accomplished researcher, teacher, and participant in the simulation community. This article provides an overview of his career and a summary of some of his many professional accomplishments. © 2007 ACM.",Finance; Heavy tails; Rare event simulation; Splitting,Computer simulation; Cost accounting; Professional aspects; Heavy tails; Rare event simulation; Computer science
"Rare events, splitting, and quasi-Monte Carlo",2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247182656&doi=10.1145%2f1225275.1225280&partnerID=40&md5=1eae932a49d1ab0a3d4abd8610db8ca0,"In the context of rare-event simulation, splitting and importance sampling (IS) are the primary approaches to make important rare events happen more frequently in a simulation and yet recover an unbiased estimator of the target performance measure, with much smaller variance than a straightforward Monte Carlo (MC) estimator. Randomized quasi-Monte Carlo (RQMC) is another class of methods for reducing the noise of simulation estimators, by sampling more evenly than with standard MC. It typically works well for simulations that depend mostly on very few random numbers. In splitting and IS, on the other hand, we often simulate Markov chains whose sample paths are a function of a long sequence of independent random numbers generated during the simulation. In this article, we show that RQMC can be used jointly with splitting and/or IS to construct better estimators than those obtained by either of these methods alone. We do that in a setting where the goal is to estimate the probability of reaching B before reaching (or returning to) A when starting A from a distinguished state not in B, where A and B are two disjoint subsets of the state space, and B is very rarely reached. This problem has several practical applications. The article is in fact a two-in-one: the first part provides a guided tour of splitting techniques, introducing along the way some improvements in the implementation of multilevel splitting. At the end of the article, we also give examples of situations where splitting is not effective. For these examples, we compare different ways of applying IS and combining it with RQMC. © 2007 ACM.",Highly-reliable Markovian systems; Importance sampling; Markov chain; Quasi-Monte Carlo; RESTART; Splitting; Variance reduction,Computer simulation; Function evaluation; Monte Carlo methods; Probabilistic logics; Sampling; Highly-reliable Markovian systems; Importance sampling; Quasi-Monte Carlo; Variance reduction; Markov processes
Asymptotics and fast simulation for tail probabilities of maximum of sums of few random variables,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247245296&doi=10.1145%2f1225275.1225278&partnerID=40&md5=2fb8d86358184f84129803ff7b5fb517,"We derive tail asymptotics for the probability that the maximum of sums of a few random variables exceeds an increasing threshold, when the random variables may be light as well as heavy tailed. These probabilities arise in many applications including in PERT networks where our interest may be in measuring the probability of large project delays. We also develop provably asymptotically optimal importance sampling techniques to efficiently estimate these probabilities. In the light-tailed settings we show that an appropriate mixture of exponentially twisted distributions efficiently estimates these probabilities. As is well known, exponential twisting based methods are not applicable in the heavy-tailed settings. To remedy this, we develop techniques that rely on asymptotic hazard rate twisting and prove their effectiveness in both light and heavy-tailed settings. We show that in many cases the latter may have implementation advantages over exponential twisting based methods in the light-tailed settings. However, our experiments suggest that when easily implementable, the exponential twisting based methods significantly outperform asymptotic hazard rate twisting based methods. © 2007 ACM.",Importance sampling; PERT networks; Rare event simulation; Tail asymptotics,Computer networks; Computer simulation; Random processes; Importance sampling; PERT networks; Rare event simulation; Tail asymptotics; Probabilistic logics
Editor's introduction: Special issue honoring Perwez Shahabuddin,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247261625&doi=10.1145%2f1225275.1225276&partnerID=40&md5=afcc7d1cdbe0507b6b9933fe66dd51a1,[No abstract available],,
Conversion of high-period random numbers to floating point,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846402058&doi=10.1145%2f1189756.1189759&partnerID=40&md5=bcf9f2a1ef459a2e91ef025204eb219c,Conversion of unsigned 32-bit random integers to double precision floating point is discussed. It is shown that the standard practice can be unnecessarily slow and inflexible. It is argued that simulation experiments could benefit from making better use of the available precision. © 2007 ACM.,Conversion to floating point; Precision,Binary codes; Boolean algebra; Computer simulation; Integers; Precision floating point; Digital arithmetic
A state event detection algorithm for numerically simulating hybrid systems with model singularities,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846381965&doi=10.1145%2f1189756.1189757&partnerID=40&md5=46ba4c90d3334c9d6c7e7055db25cf91,"This article describes an algorithm for detecting the occurrence of events, which signify discontinuities in the first derivative of the state variables, while simulating a set of nonsmooth differential equations. Such combined-discrete continuous systems arise in many contexts and are often referred to as hybrid systems, switched systems, or nonsmooth systems. In all cases, the state events are triggered at simulated times which generate states corresponding to the zeros of some algebraic event function. It has been noted that all existing simulators are prone to failure when these events occur in the neighborhood of model singularities - -regions of the state space where the right-hand side of the differential equation is undefined. Such model singularities are often the impetus for using nonsmooth models in the first place. This failure occurs because existing algorithms blindly attempt to interpolate across singular regions, checking for possible events after the fact. The event detection algorithm described here overcomes this limitation using an approach inspired by feedback control theory. A carefully constructed extrapolation polynomial is used to select the integration step size by checking for potential future events, avoiding the need to evaluate the differential equation in potentially singular regions. It is shown that this alternate approach gives added functionality with little impact on the simulation efficiency. © 2007 ACM.",Discontinuities; Event detection; Hybrid systems; Model singularities; Numerical integration,Algebra; Algorithms; Differential equations; Extrapolation; Integration; Mathematical models; Polynomials; Simulators; Event detection algorithms; Hybrid systems; Model singularities; State variables; Pattern recognition
An efficient single-pass trace compression technique utilizing instruction streams,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846377488&doi=10.1145%2f1189756.1189758&partnerID=40&md5=0aff8384eef56766ea355d5a8ba0653a,"Trace-driven simulations have been widely used in computer architecture for quantitative evaluations of new ideas and design prototypes. Efficient trace compression and fast decompression are crucial for contemporary workloads, as representative benchmarks grow in size and number. This article presents Stream-Based Compression (SBC), a novel technique for single-pass compression of address traces. The SBC technique compresses both instruction and data addresses by associating them with a particular instruction stream, that is, a block of consecutively executing instructions. The compressed instruction trace is a trace of instruction stream identifiers. The compressed data address trace encompasses the data address stride and the number of repetitions for each memory-referencing instruction in a stream, ordered by the corresponding stream appearances in the trace. SBC reduces the size of SPEC CPU2000 Dinero instruction and data address traces from 18 to 309 times, outperforming the best trace compression techniques presented in the open literature. SBC can be successfully combined with general-purpose compression techniques. The combined SBC-gzip compression ratio is from 80 to 35,595, and the SBC-bzip2 compression ratio is from 75 to 191,257. Moreover, SBC outperforms other trace compression techniques when both decompression time and compression time are considered. This article also shows how the SBC algorithm can be modified for hardware implementation with very modest resources and only a minor loss in compression ratio. © 2007 ACM.",Instruction and data traces; Instruction streams; Trace compression,Computer architecture; Computer software; Data flow analysis; Data storage equipment; Logic design; Compression ratio; Data address; Decompression; Instruction streams; Stream Based Compression (SBC); Data compression
On the theoretical comparison of low-bias steady-state estimators,2007,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846378777&doi=10.1145%2f1189756.1189760&partnerID=40&md5=88126252960c4a9d8322ba886f5236f7,"The time-average estimator is typically biased in the context of steady-state simulation, and its bias is of order 1/t, where t represents simulated time. Several low-bias estimators have been developed that have a lower order bias, and, to first-order, the same variance of the time-average. We argue that this kind of first-order comparison is insufficient, and that a second-order asymptotic expansion of the mean square error (MSE) of the estimators is needed. We provide such an expansion for the time-average estimator in both the Markov and regenerative settings. Additionally, we provide a full bias expansion and a second-order MSE expansion for the Meketon - Heidelberger low-bias estimator, and show that its MSE can be asymptotically higher or lower than that of the time-average depending on the problem. The situation is different in the context of parallel steady-state simulation, where a reduction in bias that leaves the first-order variance unaffected is arguably an improvement in performance. © 2007 ACM.",Low-bias estimators; Mean-square error expansion; Steady-state simulation,Asymptotic stability; Computer simulation; Markov processes; Measurement errors; Low bias estimators; Mean square error (MSE); Steady state simulation; Method of moments
A fuzzy set theoretic approach to validate simulation models,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750503433&doi=10.1145%2f1176249.1176253&partnerID=40&md5=e33194a029412e787129aef0351e3abe,"We develop a new approach to the validation of simulation models by exploiting elements from fuzzy set theory and machine learning. A fuzzy resemblance relation concept is used to set up a mathematical framework for measuring the degree of similarity between the input-output behavior of a simulation model and the corresponding behavior of the real system. A neuro-fuzzy inference algorithm is employed to automatically learn the required resemblance relation from real and simulated data. Ultimately, defuzzification strategies are applied to obtain a coefficient on the unit interval that characterizes the degree of model validity. An example in the airline industry illustrates the practical application of this methodology. © 2006 ACM.",Nefprox; Resemblance relations; Validity grades,Algorithms; Computer simulation; Learning systems; Mathematical models; Defuzzification; Inference algorithm; Resemblance relations; Validity grades; Fuzzy sets
Simulation optimization with countably infinite feasible regions: Efficiency and convergence,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750530850&doi=10.1145%2f1176249.1176252&partnerID=40&md5=a49fbf64ef15f476f4424ffee267eea4,"This article is concerned with proving the almost sure and global convergence of a broad class of algorithms for solving simulation optimization problems with countably infinite number of feasible points. We first describe the class of simulation optimization algorithms under consideration and discuss how the estimate of the optimal solution should be chosen when the feasible region of the underlying optimization problem is countably infinite. Then, we present a general result that guarantees the global convergence with probability one of the simulation optimization algorithms in this class. The assumptions of this result are sufficiently weak to allow the algorithms under consideration to be efficient, in that they are not required to either allocate the same amount of computer effort to all the feasible points these algorithms visit, or to spend an increasing amount of computer effort per iteration as the number of iterations grows. This article concludes with a discussion of how our assumptions can be satisfied and also generalized. © 2006 ACM.",Almost sure convergence; Discrete stochastic optimization; Estimating the optimal solution; Global convergence; Random search methods; Unequal allocation of effort to solutions,Algorithms; Computer simulation; Global optimization; Problem solving; Random processes; Almost sure convergence; Discrete stochastic optimization; Global convergence; Random search methods; Simulation optimization algorithms; Convergence of numerical methods
Replicated batch means variance estimators in the presence of an initial transient,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750524446&doi=10.1145%2f1176249.1176250&partnerID=40&md5=9ec8333d4cfedb456145215e2c4ecfe9,"Independent replications (IR) and batch means (BM) are two of the most widely used variance-estimation methods for simulation output analysis. Alexopoulos and Goldsman conducted a thorough examination of IR and BM; and Andradóttir and Argon proposed the method of replicated batch means (RBM), which combines good characteristics of IR and BM. This article gives analytical results for the mean and variance of the RBM estimator for a class of processes having initial transients with an additive form. Along the way, we provide succinct complementary extensions of some of the results in the aforementioned papers. Our expressions explicitly show how the transient function affects estimator performance and suggest that in some cases, the RBM estimator is a good compromise choice with respect to bias and variance. However, care must be taken to avoid an excessive number of replications when the transient function is pervasive. An example involving a simple moving average process illustrates our findings. © 2006 ACM.",Batch means; Independent replications; Steady-state analysis; Stochastic simulation; Transient analysis; Variance estimation,Computational complexity; Function evaluation; Batch means; Independent replications; Steady state analysis; Stochastic simulation; Transient analysis; Variance estimation; Statistical methods
Improving scalability of wireless network simulation with bounded inaccuracies,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750525538&doi=10.1145%2f1176249.1176251&partnerID=40&md5=6c4622ac2e7ac68586820db862e50846,"Discrete event network simulators have emerged as popular tools for verification and performance evaluation of wireless networks. Nevertheless, the desire to model such networks at high fidelity implies high computational costs, limiting most researchers the ability to simulate networks with thousands of nodes. Previous attempts to optimize simulation of large-scale wireless networks have not appropriately modeled accumulation of weak interference, thereby suffering inaccuracies that may be further magnified in the evaluation of upper-layer protocols. This article presents a comprehensive analysis on the effects of common optimization techniques for large-scale wireless network simulation on the overall network performance. Based on the analysis, it formulates distance limit derivation and mobility update reduction that introduce bounded inaccuracy to the radio propagation simulation. It further proposes a novel technique, Lazy Event Scheduling with Corrective Retrospection, that reduces simulation events twenty-five fold without introducing any inaccuracy at all. The experimental results show that these optimizations can substantially improve the run-time performance of an already efficient wireless network simulator, by a factor of up to 55 for wireless networks with 3200 nodes without compromising the simulation's accuracy. © 2006 ACM.",Optimization; Scalability; Simulation,Computability and decidability; Computational complexity; Computer simulation; Costs; Network protocols; Optimization; Telecommunication networks; Inaccuracies; Network simulators; Radio propagation simulation; Scalability; Wireless networks; Wireless telecommunication systems
On the efficiency of RESTART for multidimensional state systems,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748288281&doi=10.1145%2f1147224.1147227&partnerID=40&md5=2b92a4b75e9c97c4619c47dc751ad6b1,"RESTART (Repetitive Simulation Trials After Reaching Thresholds) is a widely applicable accelerated simulation technique that allows the evaluation of extremely low probabilities. The focus of this article is on providing guidelines for achieving a high efficiency in a simulation with RESTART. Emphasis is placed on the choice of the importance function, that is, the function of the system state for determining when retrials are made. A heuristic approach which is shown to be effective for some systems is proposed for this choice. A two-queue tandem network is used to illustrate the efficiency achieved following these guidelines. The importance function chosen in this example shows that an appropriate choice of the importance function leads to an efficient simulation of a system with multidimensional state space. Also presented are sufficient conditions for achieving asymptotic efficiency, and it is shown that they are not very restrictive in RESTART simulation. © 2006 ACM.",Efficiency; Queues; Rare events; RESTART; Simulation; Splitting; Variance reduction,Codes (standards); Computer simulation; Efficiency; Functions; Heuristic methods; Queueing networks; Multidimensional state systems; Rare events; Repetitive Simulation Trials After Reaching Thresholds( RESTART); Splitting; Variance reduction; Computer systems
Analysis of state-independent importance-sampling measures for the two-node tandem queue,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748288282&doi=10.1145%2f1147224.1147226&partnerID=40&md5=88c71eefd247b38182f840aadfb1bb60,"We investigate the simulation of overflow of the total population of a Markovian two-node tandem queue model during a busy cycle, using importance sampling with a state-independent change of measure. We show that the only such change of measure that may possibly result in asymptotically efficient simulation for large overflow levels is exchanging the arrival rate with the smallest service rate. For this change of measure, we classify the model's parameter space into regions of asymptotic efficiency, exponential growth of the relative error, and infinite variance, using both analytical and numerical techniques. © 2006 ACM.",Importance-sampling; Rare-event simulation; Tandem queueing networks,Computer simulation; Error analysis; Markov processes; Mathematical models; Sampling; Asymptotic efficiency; Importance sampling; Rare event simulation; Tandem queueing networks; Queueing networks
Statistical sampling of microarchitecture simulation,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748333312&doi=10.1145%2f1147224.1147225&partnerID=40&md5=957a0a049565226a3e202e1ad0b0609d,"Current software-based micro architecture simulators are many orders of magnitude slower than the hardware they simulate. Hence, most microarchitecture design studies draw their conclusions from drastically truncated benchmark simulations that are often inaccurate and misleading. This article presents the Sampling Microarchitecture Simulation (SMARTS) framework as an approach to enable fast and accurate performance measurements of full-length benchmarks. SMARTS accelerates simulation by selectively measuring in detail only an appropriate benchmark subset. SMARTS prescribes a statistically sound procedure for configuring a systematic sampling simulation run to achieve a desired quantifiable confidence in estimates. Analysis of the SPEC CPU2000 benchmark suite shows that CPI and energy per instruction (EPI) can be estimated to within ±3% with 99.7% confidence by measuring fewer than 50 million instructions per benchmark. In practice, inaccuracy in microarchitectural state initialization introduces an additional uncertainty which we empirically bound to ∼2% for the tested benchmarks. Our implementation of SMARTS achieves an actual average error of only 0.64% on CPI and 0.59% on EPI for the tested benchmarks, running with average speedups of 35 and 60 over detailed simulation of 8-way and 16-way out-of-order processors, respectively. © 2006 ACM.",Cold-start bias; Micro architecture simulation; Simulation sampling; SPEC CPU2000 simulation; Statistical sampling,Benchmarking; Computer hardware; Computer simulation; Computer software; Simulators; Statistical methods; Cold start bias; Micro architecture simulation; Simulation sampling; SPEC CPU2000 simulation; Statistical sampling; Computer architecture
The semi-regenerative method of simulation output analysis,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748316661&doi=10.1145%2f1147224.1147228&partnerID=40&md5=4996d10250008e59832b6ebc4ee84719,"We develop a class of techniques for analyzing the output of simulations of a semi-regenerative process. Called the semi-regenerative method, the approach is a generalization of the regenerative method, and it can increase efficiency. We consider the estimation of various performance measures, including steady-state means, expected cumulative reward until hitting a set of states, derivatives of steady-state means, and time-average variance constants. We also discuss importance sampling and a bias-reduction technique. In each case, we develop two estimators: one based on a simulation of a single sample path, and the other a type of stratified estimator in which trajectories are generated in an independent and identically distributed manner. We establish a central limit theorem for each estimator so confidence intervals can be constructed. © 2006 ACM.",Bias reduction; Efficiency improvement; Importance sampling; Regenerative processes; Variance reduction,Computer simulation; Mathematical models; Parameter estimation; Systems analysis; Theorem proving; Bias reduction; Efficiency improvement; Importance sampling; Regenerative processes; Variance reduction; Computational methods
Fast simulation of overflow probabilities in a queue with Gaussian Input,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745202557&doi=10.1145%2f1138464.1138466&partnerID=40&md5=4f207c4752ba4e767d77d714822347a6,"In this article, we study a queue fed by a large number n of independent discrete-time Gaussian processes with stationary increments. We consider the many-sources asymptotic regime, that is, the buffer-exceedance threshold B and the service capacity C are scaled by the number of sources (B = nb and C = nc). We discuss four methods for simulating the steady-state probability that the buffer threshold is exceeded: the single-twist method (suggested by large deviation theory), the cut-and-twist method (simulating timeslot by timeslot), the random-twist method (the twist is sampled from a discrete distribution), and the sequential-twist method (simulating source by source). The asymptotic efficiency of these four methods is analytically investigated for n →∞. A necessary and sufficient condition is derived for the efficiency of the single-twist method, indicating that it is nearly always asymptotically inefficient. The other three methods, however, are asymptotically efficient. We numerically evaluate the four methods by performing a detailed simulation study where it is our main objective to compare the three efficient methods in practical situations. © 2006 ACM.",Asymptotic efficiency; Gaussian processes; Importance sampling; Large deviations; Overflow probability; Queueing theory,Asymptotic stability; Computer simulation; Numerical methods; Queueing theory; Asymptotic efficiency; Gaussian processes; Importance sampling; Large deviations; Overflow probability; Probability
Erratum: Behavior of the NORTA method for correlated random vector generation as the dimension increases (ACM Transactions on Modeling and Computer Simulation (2003) 13 (276-294)),2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745202795&doi=10.1016%2fj.memsci.2005.07.022&partnerID=40&md5=b325cd686de5f4ffdc4c02840b60749f,[No abstract available],NORTA method; Onion method; Sampling random matrices; Semidefinite programming,
Reducing parameter uncertainty for stochastic systems,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745192475&doi=10.1016%2fj.rapm.2005.09.009&partnerID=40&md5=f331708f790cbfc69e04e5357f9e0642,"The design of many production and service systems is informed by stochastic model analysis. But the parameters of statistical distributions of stochastic models are rarely known with certainty, and are often estimated from field data. Even if the mean system performance is a known function of the model's parameters, there may still be uncertainty about the mean performance because the parameters are not known precisely. Several methods have been proposed to quantify this uncertainty, but data sampling plans have not yet been provided to reduce parameter uncertainty in a way that effectively reduces uncertainty about mean performance. The optimal solution is challenging, so we use asymptotic approximations to obtain closed-form results for sampling plans. The results apply to a wide class of stochastic models, including situations where the mean performance is unknown but estimated with simulation. Analytical and empirical results for the M/M/1 queue, a quadratic response-surface model, and a simulated critical care facility illustrate the ideas. © 2006 ACM.",Bayesian statistics; Parameter estimation; Stochastic simulation; Uncertainty analysis,Mathematical models; Optimal control systems; Probability density function; Statistics; Stochastic control systems; Uncertain systems; Bayesian statistics; Stochastic simulation; Uncertainty analysis; Parameter estimation
A discrete event method for wave simulation,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745183750&doi=10.1145%2f1138464.1138468&partnerID=40&md5=694e27c2b9924efa3df43311cd704945,This article describes a discrete event interpretation of the finite difference time domain (FDTD) and digital wave guide network (OWN) wave simulation schemes. The discrete event method is formalized using the discrete event system specification (DEVS). The scheme is shown to have errors that are proportional to the resolution of the spatial grid. A numerical example demonstrates the relative efficiency of the scheme with respect to FDTD and DWN schemes. The potential for the discrete event scheme to reduce numerical dispersion and attenuation errors is discussed. © 2006 ACM.,DEVS; Digital waveguide networks; Wave propagation,Computer simulation; Error analysis; Finite difference method; Specifications; Time domain analysis; Wave effects; Waveguides; Attenuation errors; Digital waveguide networks; Discrete event system specification (DEVS); Finite difference time domain (FDTD); Discrete time control systems
Experiences creating three implementations of the repast agent modeling toolkit,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745204609&doi=10.1179%2f174963006X99394&partnerID=40&md5=63e3ec6528854dd7d1320f15d3707711,"Many agent-based modeling and simulation researchers and practitioners have called for varying levels of simulation interoperability ranging from shared software architectures to common agent communications languages. These calls have been at least partially answered by several specifications and technologies. In fact, Tanenbaum [1988] has remarked that the ""nice thing about standards is that there are so many to choose from."" Tanenbaum goes on to say that ""if you do not like any of them, you can just wait for next year's model."" This article does not seek to introduce next year's model. Rather, the goal is to contribute to the larger simulation community the authors' accumulated experiences from developing several implementations of an agent-based simulation toolkit. As such, this article focuses on the implementation of simulation architectures rather than agent communications languages. It is hoped that ongoing architecture standards efforts will benefit from this new knowledge and use it to produce architecture standards with increased robustness. © 2006 ACM.",Agent-based Modeling and Simulation; Java; Microsoft .NET; Python,Computer architecture; Computer programming languages; Computer simulation; Interoperability; Mathematical models; Standards; Agent-based Modeling and Simulation; Microsoft .NET; Python; Intelligent agents
How heavy-tailed distributions affect simulation-generated time averages,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745213089&doi=10.1145%2f1138464.1138467&partnerID=40&md5=f5cacb8549f448637679b3cb5e3b9aa1,"For statistical inference based on telecommunications network simulation, we examine the effect of a heavy-tailed file-size distribution whose corresponding density follows an inverse power law with exponent α + 1, where the shape parameter α is strictly between 1 and 2. Representing the session-initiation and file-transmission processes as an infinite-server queueing system with Poisson arrivals, we derive the transient conditional mean and covariance function that describes the number of active sessions as well as the steady-state counterparts of these moments. Assuming the file size (service time) for each session follows the Lomax distribution, we show that the variance of the sample mean for the time-averaged number of active sessions tends to zero as the power of 1 - α of the simulation run length. Therefore, unpractically large sample-path lengths are required to achieve point estimators with acceptable levels of statistical accuracy. This study compares the accuracy of point estimators based on the Lomax distribution with those for lognormal and Weibull file-size distributions whose parameters are determined by matching their means and a selected extreme quantile with those of the Lomax. Both alternatives require shorter run lengths than the Lomax to achieve a given level of accuracy. Although the lognormal requires longer sample paths than the Weibull, it better approximates the Lomax and leads to practicable run lengths in almost all scenarios. © 2006 ACM.",Discrete-event simulation; Lomax distribution; M/G/∞ queue; Network modeling; Sample-path average,Computer simulation; Inference engines; Parameter estimation; Statistical methods; Telecommunication networks; Weibull distribution; Discrete-event simulation; Lomax distribution; M/G/∞ queue; Network modeling; Sample-path average; Probability distributions
Automated container transport system between inland port and terminals,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745194977&doi=10.1145%2f1138464.1138465&partnerID=40&md5=8cb6f599a1f4a668a43a81508ea2bc5b,"In this article we propose a new concept called automated container transportation system between inland port and terminals (ACTIPOT) which involves the use of automated trucks to transfer containers between an inland port and container terminals. The inland port is located a few miles away from the terminals and is used for storing and processing import/export containers before distribution to customers or transfer to the terminals. We design and analyze the ACTIPOT system with particular attention paid to the overall supervisory controller that synchronizes all the operations inside the ACTIPOT system. We employ the technique of truck platooning in order to simplify the control of the overall system and to minimize the possibility of deadlocks, congestion, and failures. A microscopic simulation model is developed and used to demonstrate the overall performance of the ACTIPOT system. The contribution of this article is the design, analysis, and evaluation of the new concept ACTIPOT. © 2006 ACM.",Automated container transportation system; Automated truck; Petri nets; Supervisory control; Vehicle control,Automatic programming; Computer simulation; Mathematical models; Petri nets; Ports and harbors; SCADA systems; Automated container transportation system; Automated truck; Supervisory control; Vehicle control; Cargo handling
On the xorshift random number generators,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745243279&doi=10.1145%2f1113316.1113319&partnerID=40&md5=b8b56fa77755759c7412c3a4924b4020,"G. Marsaglia recently introduced a class of very fast xorshift random number generators, whose implementation uses three ""xorshift"" operations. They belong to a large family of generators based on linear recurrences modulo 2, which also includes shift-register generators, the Mersenne twister, and several others. In this article, we analyze the theoretical properties of xorshift generators, search for the best ones with respect to the equidistribution criterion, and test them empirically. We find that the vast majority of xorshift generators with only three xorshift operations, including those having good equidistribution, fail several simple statistical tests. We also discuss generators with more than three xorshifts. © 2005 ACM.",Linear feedback shift register; Linear recurrence modulo 2; Random number generation; Xorshift,Linear systems; Shift registers; Statistical methods; Equidistribution criterion; Linear feedback shift register; Linear recurrence modulo 2; Xorshift; Random number generation
Universal nonuniform random vector generator based on acceptance-rejection,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745221859&doi=10.1145%2f1103323.1103325&partnerID=40&md5=d34273525180b8631289d5e1b5b5f924,"The acceptance/rejection approach is widely used in universal nonuniform random number generators. Its key part is an accurate approximation of a given probability density from above by a hat function. This article uses a piecewise constant hat function, whose values are overestimates of the density on the elements of the partition of the domain. It uses a sawtooth overestimate of Lipschitz continuous densities, and then examines all local maximizers of such an overestimate. The method is applicable to multivariate multimodal distributions. It exhibits relatively short preprocessing time and fast generation of random variates from a very large class of distributions. © 2005 ACM.",Acceptance/rejection; Lipschitz approximation; Nonuniform random variates; Random number generator,Approximation theory; Probability density function; Program processors; Vectors; Acceptance/rejection; Lipschitz approximation; Nonuniform random variates; Random number generator; Random processes
"Control variates for screening, selection, and estimation of the best",2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745193463&doi=10.1145%2f1122012.1122015&partnerID=40&md5=03f9372c377f4bd4415b38d9baef9623,"Ranking and selection procedures (R&S) were developed by statisticians to search for the best among a small collection of populations or treatments, where the ""best"" treatment is typically the one with the largest or smallest expected (long-run average) response. R&S procedures have been successfully extended to address situations that are encountered in stochastic simulation of alternative system designs, including unequal variances across alternatives, dependence both within the output of each system and across the outputs from alternative systems, and large numbers of alternatives to compare. In nearly all cases the estimator of the expected response is a (perhaps generalized) sample mean of the output of interest. In this article we derive R&S procedures that employ control-variate estimators instead of sample means. Control variates can be much more statistically efficient than sample means, leading to R&S procedures that are correspondingly more efficient. We also consider the related problem of estimating the expected value of the best (as opposed to the selected) system design. © 2006 ACM.",Control variates; Ranking and selection,Computer simulation; Control system analysis; Estimation; Problem solving; Statistics; Stochastic control systems; Control variates; Ranking and selection; Stochastic simulation; Computer software selection and evaluation
Algorithms for HLA-based distributed simulation cloning,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745283951&doi=10.1145%2f1113316.1113318&partnerID=40&md5=90cb031cbbb87da1c74e4f003099902e,"Distributed simulation cloning technology is designed to analyze alternative scenarios of a distributed simulation concurrently within the same execution session. One important goal is to optimize execution by avoiding repeated computation among independent scenarios. Our research is concerned with the cloning of High Level Architecture (HLA)-based distributed simulations; a federate may spawn clones to explore different scenarios at a decision point. This article introduces the cloning mechanism and the supporting infrastructure. When enabling cloning, our approach ensures the state consistency and supports user transparency and reusability of federate codes. When a federate clones, it is desirable to replicate only those federates whose states will be affected while the rest are shared among the old and new scenarios. This article discusses the theory and issues involved in such an incremental cloning mechanism, which guarantees accurate sharing and initiates cloning only when absolutely necessary. Experiments have been carried out to compare the performance of entire cloning and incremental cloning mechanisms. Experimental results indicate that the proposed approach provides correct cloning and can significantly reduce the execution time for evaluating different scenarios of a distributed simulation. Moreover the incremental cloning mechanism significantly surpasses entire cloning in terms of execution efficiency. © 2005 ACM.",Distributed simulation cloning; High level architecture; Incremental cloning; Runtime infrastructure,Cloning; Computation theory; Computer architecture; Computer simulation; Reusability; Distributed simulation cloning; High level architecture; Incremental cloning; Runtime infrastructure; Algorithms
Perfect sampling for queues and network models,2006,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745207308&doi=10.1145%2f1122012.1122016&partnerID=40&md5=38c4f73c6ce4c0bd6d9e06d6f71edfaa,"We review Propp and Wilson's [1996] CFTP algorithm and Wilson's [2000] ROCFTP algorithm. We then use these to construct perfect samplers for several queueing and network models: Poisson arrivals and exponential service times, several types of customers, and a trunk reservation protocol for accepting new customers; a similar protocol on a network switching model; a queue with a general arrival process; and a queue with both general arrivals and service times. Our samplers give effective ways to generate random samples from the steady-state distributions of these queues. © 2006 ACM.",Coupling from the past; Networks; Perfect simulation; Queues,Algorithms; Mathematical models; Network protocols; Poisson distribution; Probability distributions; Coupling from the past; Network models; Perfect simulation; Queues; Queueing networks
Very large fractional factorial and central composite designs,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745285728&doi=10.1145%2f1113316.1113320&partnerID=40&md5=3837d42d9e117e0075626dc465a385c5,"We present a concise representation of fractional factorials and an algorithm to quickly generate resolution V designs. The description is based on properties of a complete, orthogonal discrete-valued basis set called Walsh functions. We tabulate two-level resolution V fractional factorial designs, as well as central composite designs allowing estimation of full second-order models, for experiments involving up to 120 factors. The simple algorithm provided can be used to characterize even larger designs, and a fast Walsh transform method quickly generates design matrices from our representation. © 2005 ACM.",Design of experiments; Simulation experiments; Walsh functions,Algorithms; Digital arithmetic; Estimation; Mathematical models; Walsh transforms; Design of experiments; Simulation experiments; Walsh functions; Composite materials
Model representation with aesthetic computing: Method and empirical study,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745202030&doi=10.1145%2f1103323.1103327&partnerID=40&md5=4b8b5aaf98cf94143d7566ea32095d3b,"We define a new methodology, aesthetic computing, for customizing discrete structures found in mathematics, programming, and computer simulation. The methodology is presented as a procedure, defined within the context of the semantic web, involving a successive chain of model transformations from a source model to a target model, with the source model being the discrete structure to represent and the target model being the geometric model. We also present an implementation based on this methodology, and a class where empirical studies were performed to assess student perceptions on how customized model structures affected their understanding and preferences regarding visual and interactive model representations. Students appear to prefer the methodology as a way to inject creativity into model building, and to allow them to explore alternative, analogical representations influenced by the fields of aesthetics and the arts. © 2005 ACM.",Aesthetics; Customization; Dynamic modeling; Finite state automata; Modeling; RUBE,Computer programming; Computer simulation; Finite automata; Mathematical models; Mathematical transformations; World Wide Web; Aesthetics; Dynamic modeling; RUBE; Discrete time control systems
Efficient simulation of buffer overflow probabilities in Jackson networks with feedback,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744829164&doi=10.1145%2f1113316.1113317&partnerID=40&md5=5952b14d6d204f73afeb7685e67efd42,"Consider a Jackson network that allows feedback and that has a single server at each queue. The queues in this network are classified as a single 'target' queue and the remaining 'feeder' queues. In this setting we develop the large deviations limit and an asymptotically efficient importance sampling estimator for the probability that the target queue overflows during its busy period, under some regularity conditions on the feeder queue-length distribution at the initiation of the target queue busy period. This importance sampling distribution is obtained as a solution to a non-linear program. We especially focus on the case where the feeder queues, at the initiation of the target queue busy period, have the steady state distribution corresponding to these instants. In this setting, we explicitly identify the importance sampling distribution when the feeder queue service rates exceed a specified threshold. We also relate our work to the existing large deviations literature to develop a perspective on successes and limitations of our results. © 2005 ACM.",Asymptotic optimality; Importance sampling; Jackson networks; Queueing networks; Rare event simulation; Variance reduction,Computer simulation; Feedback; Probability; Queueing networks; Asymptotic optimality; Importance sampling; Jackson networks; Rare event simulation; Variance reduction; Computer networks
Ladder queue: An O(1) priority queue structure for large-scale discrete event simulation,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745199604&doi=10.1145%2f1103323.1103324&partnerID=40&md5=e52e084966e64c9c0353b325fc04daa4,"This article describes a new priority queue implementation for managing the pending event set in discrete event simulation. Extensive empirical results demonstrate that it consistently outperforms other current popular candidates. This new implementation, called Ladder Queue, is also theoretically justified to have O(1) amortized access time complexity, as long as the mean jump parameter of the priority increment distribution is finite and greater than zero, regardless of its variance. Many practical priority increment distributions satisfy this condition including unbounded variance distributions like the Pareto distribution. This renders the LadderQ the ideal discrete event queue structure for stable O(1) performance even under practical queue distributions with infinite variance. Numerical simulations ranging from 100 to 10 million events affirm the O(1) property of LadderQ and that it is a superior structure for large-scale discrete event simulation. © 2005 ACM.",Calendar queue; Pending event set implementations; Priority queue,Computational complexity; Computer simulation; Distributed computer systems; Queueing networks; Calendar queue; Pending event set implementations; Priority queue; Large scale systems
Estimation of internet file-access/modification rates from indirect data,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745190015&doi=10.1145%2f1103323.1103326&partnerID=40&md5=45cef69243fc870108e2bb92969c1a2a,"Consider an Internet file for which data on last time of access/modification (A/M) of the file are collected at periodic intervals, but for which direct A/M data are not available. Methodology is developed here that enables estimation of the A/M rates, in spite of having only indirect data of this nature. Both parametric and nonparametric methods are developed. Theoretical and empirical analyses are presented that indicate that the problem is indeed statistically tractable, and that the methods developed are of practical value. Behavior of the parametric estimators is examined when these assumptions are violated, and these estimators are found to be robust against some such violations. © 2005 ACM.",Access rate; Asymptotic distributions; Nonparametric density estimation; Poisson and renewal processes; Statistical estimation; World Wide Web,Data acquisition; Poisson distribution; Statistical methods; World Wide Web; Access rate; Asymptotic distributions; Nonparametric density estimation; Poisson and renewal processes; Internet
Simulating Markov-reward processes with rare events,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27344432551&doi=10.1145%2f1060576.1060578&partnerID=40&md5=10447773a4b0f7669c3f231361969f4d,"Simulating continuous-time Markov reward processes containing rarely visited, but economically important states requires long simulation times unless special measures are taken. In this article, we consider Markov reward processes in equilibrium, and we use the equilibrium equations to reallocate rewards. The effect of this reallocation is determined analytically for a number of small examples. In these examples, significant savings in run lengths were possible, especially in the case where the expected rewards are strongly influenced by low-probability boundary states. The emphasis of the article is on exploration; no large simulation problems have been considered. © 2005 ACM.",Markov reward processes; Markovian event systems; Rare event simulation; Rare events; Simulation; Variance of time averages,Boundary conditions; Computer simulation; Mathematical models; Probability; Problem solving; Markov reward processes; Markovian event systems; Rare event simulation; Rare events; Variance of time averages; Markov processes
Comparison with a standard via fully sequential procedures,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27344438965&doi=10.1145%2f1060576.1060579&partnerID=40&md5=af1bd55901c0055bb45d9ec9a98ce328,"We develop fully sequential procedures for comparison with a standard. The goal is to find systems whose expected performance measures are larger or smaller than a single system referred to as a standard and, if there is any, to find the one with the largest or smallest performance. The general formulation of comparison with a standard gives the standard a special status and tries to protect it when its performance is better than or even equal to performance measures of all the other alternatives. Therefore, the problem cannot be formulated as the selection of the best and a specialized procedure is required. Our procedures allow for unequal variances across systems, the use of common random numbers, and known or unknown expected performance of the standard. Experimental results are provided to compare the efficiency of the procedure with other existing procedures. © 2005 ACM.",Comparison with a standard; Output analysis; Ranking and selection; Simulation; Variance reduction,Computer simulation; Performance; Problem solving; Comparison with a standard; Output analysis; Ranking and selection; Variance reduction; Standards
An alternative time management mechanism for distributed simulations,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27344460781&doi=10.1145%2f1060576.1060577&partnerID=40&md5=640afdb488bf6fca0717c454efddb05c,"Over the past few years, there has been a keen interest in the management of time in distributed simulation environments. Previous emphasis in time management (TM) services has been based on time stamp ordering, which is both computation and bandwidth intensive. This article discusses an alternative approach to time management based on causal ordering. Traditional causal ordering protocols incur a large amount of communication overhead, which is generally of the order of N 2 for a distributed system of N processes. A new causal ordering protocol proposed by the authors, the Modified Schiper-Eggli-Sandoz (MSES) protocol, is presented in this article. This new protocol minimizes the control information overhead of causal ordering by using the direct dependency tracking technique. The MSES protocol works well in both unicast and multicast environments, without relying on information about the underlying network topology and communication pattern among the processes of the distributed system. The MSES protocol has been successfully implemented as a middleware on top of DMSO RTI. Experiments have been conducted to benchmark the performance of the new time management mechanism with respect to the existing TM mechanisms available in DMSO RTI. The simulation scenarios of the experiments vary with different degrees of inter-federate dependency and federate event granularities. The ordering limitations of the causality based TM mechanism are addressed in this article and the trade-off of the degree of event ordering and execution speed of simulations is discussed. © 2005 ACM.",Causal order; Distributed simulation; High level architecture; Time management,Bandwidth; Benchmarking; Computer simulation; Distributed computer systems; Management science; Middleware; Multicasting; Network protocols; Causal order; Distributed simulation; High level architecture; Time management; Time and motion study
A sort-based DDM matching algorithm for HLA,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644419651&doi=10.1145%2f1044322.1044324&partnerID=40&md5=139082add345d4aa1ef3b233700910d8,"The High Level Architecture (HLA) is an architecture for reuse and interoperation of simulations. It provides several Data Distribution Management (DDM) services to reduce the transmission and reception of irrelevant data. These services rely on the computation of the intersection between ""update"" and ""subscription"" regions. Currently, there are several main DDM filtering algorithms. Since each approach still has some shortcomings, we have focused our research on the design and the evaluation of intersection algorithms for the DDM. In this article, we introduce a new algorithm in which extents are sorted before computing the intersections. Our experiments show that usually the sort-based algorithm has the best performance among all approaches. The improvement of its performance ranges between 30% and 99% over the brute force and hybrid approaches. © 2005 ACM.",Data distribution management; Data filtering; Distributed Simulation; High level architecture; Sort-based matching algorithm,Algorithms; Computational methods; Computer simulation; Data processing; Database systems; Mathematical models; Performance; Sorting; Data distribution management (DDM); Data filtering; Distributed simulation; High level architecture (HLA); Sort-based matching algorithms; Computer architecture
Adaptive multivariate three-timescale stochastic approximation algorithms for simulation based optimization,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644410396&doi=10.1145%2f1044322.1044326&partnerID=40&md5=2b60283c41751ef16f9426d2b5dcee66,"We develop in this article, four adaptive three-timescale stochastic approximation algorithms for simulation optimization that estimate both the gradient and Hessian of average cost at each update epoch. These algorithms use four, three, two, and one simulation(s), respectively, and update the values of the decision variable and Hessian matrix components simultaneously, with estimates based on the simultaneous perturbation methodology. Our algorithms use coupled stochastic recursions that proceed using three different timescales or step-size schedules. We present a detailed convergence analysis of the algorithms and show numerical experiments using all the developed algorithms on a two-node network of M/G/1 queues with feedback for a 50-dimensional parameter vector. We provide comparisons of the performance of these algorithms with two recently developed two-timescale steepest descent simultaneous perturbation analogs that use randomized and deterministic perturbation sequences, respectively. We also present experiments to explore the sensitivity of the algorithms to their associated parameters. The algorithms that use four and three simulations, respectively, perform significantly better than the rest of the algorithms. © 2005 ACM.",Adaptive three-timescale stochastic approximation algorithms; Newton-type algorithms; Simulation optimization; Simultaneous perturbation stochastic approximation,Approximation theory; Computer simulation; Convergence of numerical methods; Matrix algebra; Optimization; Performance; Perturbation techniques; Probability; Random processes; Sensitivity analysis; Adaptive three-timescale stochastic approximation algorithms; Newton-type algorithms; Simulation optimization; Simultaneous perturbation stochastic performance; Algorithms
Efficient and portable multiple recursive generators of large order,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644416411&doi=10.1145%2f1044322.1044323&partnerID=40&md5=e2a6071f5830745322b2f3ab03d47491,"Deng and Xu [2003] proposed a system of multiple recursive generators of prime modulus p and order k, where all nonzero coefficients of the recurrence are equal. This type of generator is efficient because only a single multiplication is required. It is common to choose p -2 31 - 1 and some multipliers to further improve the speed of the generator. In this case, some fast implementations are available without using explicit division or multiplication. For such a p, Deng and Xu [2003] provided specific parameters, yielding the maximum period for recurrence of order k, up to 120. One problem of extending it to a larger k is the difficulty of finding a complete factorization of p k - 1. In this article, we apply an efficient technique to find k such that it is easy to factor p k - 1, with p = 2 31 - 1. The largest one found is k = 1597. To find multiple recursive generators of large order k, we introduce an efficient search algorithm with an early exit strategy in case of a failed search. For k = 1597, we constructed several efficient and portable generators with the period length approximately 10 1490301. © 2005 ACM.",DX-k generator; GMP; Irreducible polynomial; Linear congruential generator; MRG; Primitive polynomial,Algorithms; Approximation theory; Computational methods; Design; Parameter estimation; Performance; Polynomials; Problem solving; DX-k generator; GMP; Irreducible polynomials; Linear conruential generators (LCG); Multiple recursive generators (MRG); Primitive polynomials; Random number generation
ASAP3: A batch means procedure for steady-state simulation analysis,2005,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644380620&doi=10.1145%2f1044322.1044325&partnerID=40&md5=dbfe8a44a57e156065b2f9bfc7f43ed6,"We introduce ASAP3, a refinement of the batch means algorithms ASAP and ASAP2, that delivers point and confidence-interval estimators for the expected response of a steady-state simulation. ASAP3 is a sequential procedure designed to produce a confidence-interval estimator that satisfies user-specified requirements on absolute or relative precision as well as coverage probability. ASAP3 operates as follows: the batch size is progressively increased until the batch means pass the Shapiro-Wilk test for multivariate normality; and then ASAP3 fits a first-order autoregressive (AR(1)) time series model to the batch means. If necessary, the batch size is further increased until the autoregressive parameter in the AR(1) model does not significantly exceed 0.8. Next, ASAP3 computes the terms of an inverse Cornish-Fisher expansion for the classical batch means t-ratio based on the AR(1) parameter estimates; and finally ASAP3 delivers a correlation-adjusted confidence interval based on this expansion. Regarding not only conformance to the precision and coverage-probability requirements but also the mean and variance of the half-length of the delivered confidence interval, ASAP3 compared favorably to other batch means procedures (namely, ABATCH, ASAP, ASAP2, and LBATCH) in an extensive experimental performance evaluation. © 2005 ACM.",Batch means; Confidence interval estimation; Inverse Cornish-Fisher expansion; Sequential analysis; Simulation start-up problem; Steady-state simulation,Algorithms; Approximation theory; Correlation methods; Estimation; Mathematical models; Parameter estimation; Probability; Problem solving; Batch means; Confidence interval estimation; Inverse Cornish-Fisher expansion; Sequential analysis; Simulation and start-up problem; Steady-state simulation; Computer simulation
Calculation of confidence intervals for simulation output,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4744362349&doi=10.1145%2f1029174.1029176&partnerID=40&md5=fef3f065dd64f3bd208d5f68ea48aeee,"This article is concerned with the calculation of confidence intervals for simulation output that is dependent on two sources of variability. One, referred to as simulation variability, arises from the use of random numbers in the simulation itself; and the other, referred to as parameter variability, arises when the input parameters are unknown and have to be estimated from observed data. Three approaches to the calculation of confidence intervals are presented-the traditional asymptotic normality theory approach, a bootstrap approach and a new method which produces a conservative approximation based on performing just two simulation runs at carefully selected parameter settings. It is demonstrated that the traditional and bootstrap approaches provide similar degrees of accuracy and that whilst the new method may sometimes be very conservative, it can be calculated in a small fraction of the computational time of the exact methods.",δ-Method; Parameter variability; Simulation variability; Twopoint method; Uncertainty analysis,Approximation theory; Asymptotic stability; Communication systems; Computational methods; Mathematical models; Parameter estimation; Theorem proving; Parameter variability; Simulation variability; Two-point method; Uncertainty analysis; Computer simulation
Scalable fluid models and simulations for large-scale IP networks,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17044381780&doi=10.1145%2f1010621.1010625&partnerID=40&md5=8f2a3b79dff6a64128725521cc2ee723,"In this article we present a scalable model of a network of Active Queue Management (AQM) routers serving a large population of Transport Control Protocol (TCP) flows. We present efficient solution techniques that allow one to obtain the transient behavior of the average queue lengths and packet loss/mark probabilities of AQM routers, and average end-to-end throughput and latencies of TCP users. We model different versions of TCP as well as different implementations of RED Random Early Detection (RED), the most popular AQM scheme currently in use. Comparisons between the models and ns simulation show our models to be quite accurate while at the same time requiring substantially less time to solve than packet level simulations, especially when workloads and bandwidths are high. © 2004 ACM.",Fluid model; Large-scale IP networks; Simulation,Bandwidth; Computer simulation; Fluid dynamics; Large scale systems; Mathematical models; Network protocols; Random processes; Routers; Active queue management (AQM); Fluid models; Large-scale IP networks; Random Early Detection (RED); Transport control protocols (TCP); Queueing networks
Modeling of discrete event systems: A holistic and incremental approach using Petri nets,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10944258434&doi=10.1145%2f1029174.1029178&partnerID=40&md5=b2e36f1e9bc4ee1affda13b8866fa32e,"In this article, the authors provide an alternative view on Petri nets modeling of discrete event systems. The proposed modeling procedure follows the Systems Specification guidelines underlying the well-known DEVS modeling formalism. The authors' endeavour is towards perfecting the design of reusable Petri nets-based models by searching for a good primitive for a modular model construction and the introduction of coupling templates as standardised means to couple building block components. Assuming that the real-world system to be modeled has been analyzed in depth beforehand through a suitable system analysis method (which itself is beyond the scope of the article), we present a systematic step-by-step approach to construct a model in the Petri nets domain together with its experimental frame. The construction adheres to well-defined rules, which enable computer-based model construction. The input for this systematic bottom-up construction of Petri nets-models is information (about, e.g., primitive system components, entity flows, routing constructs) gathered from the top-down system analysis. In the article, attention is also paid to the algebraic backgrounds underlying the proposed model construction. These provide the basis for formal correctness proofs, mapping Petri nets onto DEVS-models, and complexity reduction of the found Petri nets-models. By offering to the model builder the possibility to handle multiple abstraction levels and by addressing important issues related to the interfacing question of coupled models and model components described in Petri nets and DEVS formalism, the authors' work addresses two of the main research directions of Computer Automated Multi-Paradigm Modeling ([Mosterman and Vangheluwe 2002]): model abstraction and multiformalism modeling. The article concludes with an illustrative application example.",,Algorithms; Computer simulation; Computer software; Couplings; Mathematical models; Problem solving; Systems analysis; Coupled models; Discrete event systems; Multi-paradigm modeling; Systems specification guidelines; Petri nets
Discrete event fluid modeling of background TCP traffic,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17044435120&doi=10.1145%2f1010621.1010622&partnerID=40&md5=20f0f622b82afe13016cfa4cca81e7f1,"TCP is the most widely used transport layer protocol used in the Internet today. A TCP session adapts the demands it places on the network to observations of bandwidth availability on the network. Because TCP is adaptive, any model of its behavior that aspires to be accurate must be influenced by other network traffic. This point is especially important in the context of using simulation to evaluate some new network algorithm of interest (e.g., reliable multicast) in an environment where the background traffic affects - and is affected by - its behavior. We need to generate background traffic efficiently in a way that captures the salient features of TCP, while the reference and background traffic representations interact with each other. This article describes a fluid model of TCP and a switching model that has flows represented by fluids interacting with packet-oriented flows. We describe conditions under which a fluid model produces exactly the same behavior as a packet-oriented model, and we quantify the performance advantages of the approach both analytically and empirically. We observe that very significant speedups may be attained while keeping high accuracy. © 2004 ACM.",TCP; Traffic modeling,Algorithms; Bandwidth; Computer simulation; Discrete time control systems; Fluid dynamics; Internet; Multicasting; Packet networks; Telecommunication traffic; Switching models; TCP; Traffic modeling; Transport layer protocol (TCP); Network protocols
An asynchronous integration and event detection algorithm for simulating multi-agent hybrid systems,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10944228428&doi=10.1145%2f1029174.1029177&partnerID=40&md5=2ea384c7ad422388a3f60690cf6a6bcb,"A simulation algorithm is presented for multi-agent hybrid systems - systems consisting of many sets of nonsmooth differential equations - such as systems involving multiple rigid bodies, vehicles, or airplanes. The differential equations are partitioned into coupled subsystems, called ""agents""; and the conditions which trigger the discontinuities in the derivatives, called ""events"", may depend on the global state vector. Such systems normally require significant computational resources to simulate because a global time step is used to ensure the discontinuity is properly handled. When the number of systems is large, forcing all system to be simulated at the same rate creates a computational bottleneck, dramatically decreasing efficiency. By using a control systems approach for selecting integration step sizes, we avoid using a global time step. Each subsystem can be simulated asynchronously when the state is away from the event. As the state approaches the event, the simulation is able to synchronize each of the local time clocks in such a way that the discontinuities are properly handled without the need for ""roll back"". The algorithm's operation and utility is demonstrated on an example problem inspired by autonomous highway vehicles. Using a combination of stochastic modelling and numerical experiments we show that the algorithm requires significantly less computation time when compared with traditional simulation techniques for such problems, and scales more favorably with problem size.",Event detection; Hybrid systems; Multi-agent systems; Numerical integration,Algorithms; Computer simulation; Control systems; Differential equations; Integration; Problem solving; Random processes; Vectors; Coupled subsystems; Event detection; Hybrid systems; Numerical integration; Multi agent systems
HNS: A streamlined hybrid network simulator,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17044439811&doi=10.1145%2f1010621.1010623&partnerID=40&md5=d97acd34ba7e6f95fedc00f9d28f8b81,"The design of a new hybrid Java simulator called Hybrid Network Simulator (HNS) was described. HNS simulates the movement of workload in queueing network where transactions may be of two types: traditional discrete transactions and continuous (fluid) transactions, which arrives discretely at the network traffic flows, and each discrete arrival carries a workload. The simulator admits models with both discrete and continuous traffic flows, and collects detailed statistics for both, including arrival, loss, buffer contents, departure, and delay statistics. Incorporation of Infinitesimal Pertrubation Analysis (IPA) derivative into HNS would provide a tool to assess their efficacy.",Fluid TCP simulation; Fluid-flow models; Fluid-flow simulation; Hybrid simulation; Mixed models; Packet models; Streamlining,Delay circuits; Discrete time control systems; Java programming language; Mathematical models; Network protocols; Perturbation techniques; Simulators; Statistical methods; Telecommunication traffic; Fluid flow models; Fluid TCP simulation; Fluid-flow simulation; Hybrid simulation; Mixed models; Packet models; Streamlining; Telecommunication networks
Error analysis of burst level modeling of active-idle sources,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17044379490&doi=10.1145%2f1010621.1010624&partnerID=40&md5=4edbe0cf0b24af51b78f31f78d3b636d,"Burst level modeling of active-idle sources was analyzed and a queue fed by the sources was investigated. The sources of the accuracy degradation in abstract simulation was also determined. It was found that the arrival rates vary during each active period. The burst level model assumes constant rates during each active period. It was also found that the burst level model significantly underevaluated the queue length if such intervals occurred often. High utilizations, strong traffic burstiness at the active-idle level, and small arrival granularity tend to reduce such occurrences and the error in the mean queue length.",Abstract simulation; Burst level modeling; Local rate variations; Simulation fidelity; Traffic burstiness,Computer simulation; Data reduction; Errors; Iterative methods; Mathematical models; Probability; Routers; Telecommunication traffic; Abstract simulation; Burst level modeling; Local rate variations; Traffic burstiness; Abstracting
Empirical performance of bias-reducing estimators for regenerative steady-state simulations,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10944223451&doi=10.1145%2f1029174.1029175&partnerID=40&md5=5fb93027b7d8c43c17aa1d0d0c00b91f,"When simulating a stochastic system, simulationists often are interested in estimating various steady-state performance measures. The classical point estimator for such a measure involves simply taking the time average of an appropriate function of the process being simulated. Since the simulation can not be initiated with the (unknown) steady-state distribution, the classical point estimator is generally biased. In the context of regenerative steady-state simulation, a variety of other point estimators have been developed in an attempt to minimize the bias. In this paper, we provide an empirical comparison of these estimators in the context of four different continuous-time Markov chain models. The bias of the point estimators and the coverage probabilities of the associated confidence intervals are reported for the four models. Conclusions are drawn from this experimental work as to which methods are most effective in reducing bias.",Bias-reducing estimators; Regeneration; Simulation; Steady-state estimation,Algorithms; Computer simulation; Markov processes; Mathematical models; Probability distributions; Statistical methods; Stochastic control systems; Bias-reducing estimators; Empirical performance; Regeneration; Steady-state estimation; Steady flow
Network simulation enhancing network management in real-time,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042547939&doi=10.1145%2f985793.985798&partnerID=40&md5=e09bc83b3c6e62584cc6258e618f46cd,"This research investigates the application of network simulation to enhance network management in real-time. Its focus is on ad hoc wireless networks as needed for future public safety emergency response, homeland security, and future combat system networks where the network requirements and topology are rapidly changing. The network simulation is used to analyze user satisfaction with the network and to enable real-time what-if studies that show what the user satisfaction will be if the network is reconfigured in various ways. User satisfaction as a metric is defined to correspond to the impact of the network performance on the ability of the network user to complete their public safety, security or military functions. The current network size of interest is up to ad hoc wireless 500 nodes, a typical target load for a single network manager. At the same time, the approach is made scalable to use manager of managers approach to scale to larger networks. The article reports on the approach and performance results.",Communications network simulation; Quality of service; User satisfaction metrics,Algorithms; Computational methods; Computer simulation; Mathematical models; Optimization; Performance; Quality of service; Real time systems; Risk assessment; User interfaces; Communications network simulation; Human factors; Simulation support systems; User satisfaction metrics; Network protocols
Staged simulation: A general technique for improving simulation scale and performance,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042554077&doi=10.1145%2f985793.985797&partnerID=40&md5=7848d9986ff2cd8fdd0ac83292de0281,"This article describes staged simulation, a technique for improving the run time performance and scale of discrete event simulators. Typical network simulations are limited in speed and scale due to redundant computations encountered both within a single simulation run and between successive runs. Staged simulation proposes to restructure discrete event simulators to operate in stages that precompute, cache, and reuse partial results to drastically reduce redundant computation within and across simulations. We present a general and flexible framework for staging, and identify the advantages and trade-offs of its application to wireless network simulations, a particularly challenging simulation domain. Experience with applying staged simulation to the ns2 simulator shows that staging can improve execution time by an order of magnitude or more and enable the simulation of wireless networks with tens of thousands of nodes.",Performance and scale; Staged simulation; Wireless simulation,Computational methods; Computer simulation; Distributed computer systems; Mathematical models; Optimization; Wireless telecommunication systems; Performance and scale; Simulation support systems; Staged simulation; Wireless simulation; Network protocols
A federated approach to distributed network simulation,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042598860&doi=10.1145%2f985793.985795&partnerID=40&md5=c7208a6d018f0edead86819f3d800a25,"We describe an approach and our experiences in applying federated simulation techniques to create large-scale parallel simulations of computer networks. Using the federated approach, the topology and the protocol stack of the simulated network is partitioned into a number of submodels, and a simulation process is instantiated for each one. Runtime infrastructure software provides services for interprocess communication and synchronization (time management). We first describe issues that arise in homogeneous federations where a sequential simulator is federated with itself to realize a parallel implementation. We then describe additional issues that must be addressed in heterogeneous federations composed of different network simulation packages, and describe a dynamic simulation backplane mechanism that facilitates interoperability among different network simulators. Specifically, the dynamic simulation backplane provides a means of addressing key issues that arise in federating different network simulators: differing packet representations, incomplete implementations of network protocol models, and differing levels of detail among the simulation processes. We discuss two different methods for using the backplane for interactions between heterogeneous simulators: the cross-protocol stack method and the split-protocol stack method. Finally, results from an experimental study are presented for both the homogeneous and heterogeneous cases that provide evidence of the scalability of our federated approach on two moderately sized computing clusters. Two different homogeneous implementations are described: Parallel / Distributed ns (pdns) and the Georgia Tech Network Simulator (GTNetS). Results of a heterogeneous implementation federating ns with GloMoSim are described. This research demonstrates that federated simulations are a viable approach to realizing efficient parallel network simulation tools.",Distributed simulation; Networks; Simulation,Computational methods; Computer networks; Computer simulation; Mathematical models; Network protocols; Numerical methods; Problem solving; Topology; Distributed simulation; Dynamic simulation backplane; Georgia tech network simulator (GTNetS); Parallel network simulation tools; Distributed computer systems
ACM Tarnsactions on Modeling snd Computer Simulation: Editorial,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042642837&doi=10.1145%2f985793.985794&partnerID=40&md5=0664186ae76d125376aa8e8f9ca2bf0d,[No abstract available],,
MAYA: Integrating hybrid network modeling to the physical world,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042552020&doi=10.1145%2f985793.985796&partnerID=40&md5=ac075d36bffa0b01677b5b5dabd625f1,"The flourish of large-scale network applications across the Internet and or MANET has raised a challenge to network modeling environments that support experimentation and analysis of close interactions between real applications and network dynamics. To facilitate such experimentations, this paper presents MAYA, a multiparadigm network modeling framework including discrete event models, analytical models and physical network interfaces, together with its illustrative implementation using QualNet, fluid flow TCP model and physical network interface. MAYA framework allows users to interface simulated networks directly with physical networks, while attaining real-time constraints even for large-scale networks by incorporating above multiparadigm network modeling techniques. It also gives user the flexibility to emulate applications on nodes in both real and simulated networks. Experiments are conducted to validate the interoperation of QualNet and fluid flow model, to examine the performance of MAYA as well as to evaluate the optimization techniques, namely interleaved execution of fluid flow model and causality-preserve realtime synchronization relaxation. Experimental results indicate that MAYA is a scalable and extensible solution to modeling of close interactions between real application and network dynamics.",Fluid flow model; Network modeling and simulation; Physical network interface; QualNet; Realtime simulation,Bandwidth; Client server computer systems; Computational methods; Computer simulation; Interfaces (computer); Internet; Mathematical models; Real time systems; Web browsers; Fluid flow model; Network modeling and simulation; Physical network interface; QualNet; Realtime simulation; Network protocols
To batch or not to batch?,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042645040&doi=10.1145%2f974734.974738&partnerID=40&md5=f48066ba7f40d48475577af17d405d3c,"When designing steady-state computer simulation experiments, one may be faced with the choice of batching observations in one long run or replicating a number of smaller runs. Both methods are potentially useful in the course of undertaking simulation output analysis. The tradeoffs between the two alternatives are well known: batching ameliorates the effects of initialization bias, but produces batch means that might be correlated; replication yields independent sample means, but may suffer from initialization bias at the beginning of each of the runs. We present several new results and specific examples to lend insight as to when one method might be preferred over the other. In steady-state, batching and replication perform similarly in terms of estimating the mean and variance parameter, but replication tends to do better than batching with regard to the performance of confidence intervals for the mean. Such a victory for replication may be hollow, for in the presence of an initial transient, batching often performs better than replication when it comes to point and confidence-interval estimation of the steady-state mean. We conclude-like other classic references - that in the context of estimation of the steady-state mean, batching is typically the wiser approach.",Batch means; Confidence intervals; Independent replications; Steady-state analysis; Stochastic simulation; Transient analysis; Variance estimation,Computer simulation; Correlation methods; Parameter estimation; Research; Sampling; Batch means; Confidence intervals; Independent replications; Steady-state analysis; Stochastic simulation; Transient analysis; Variance estimation; Stochastic control systems
Modeling train movements through complex rail networks,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042554079&doi=10.1145%2f974734.974737&partnerID=40&md5=edd713ebeeade4c5bc336a881dc832f5,"Trains operating in densely populated metropolitan areas typically encounter complex trackage configurations. To make optimal use of the available rail capacity, some portions of the rail network may consist of single-track lines while other locations may consist of double- or triple-track lines. Because of varying local conditions, different points in the rail network may have different speed limits. We formulate a graphical technique for modeling such complex rail networks; and we use this technique to develop a deadlock-free algorithm for dispatching each train to its destination with nearly minimal travel time while (a) abiding by the speed limits at each point on each train's route, and (b) maintaining adequate headways between trains. We implemented this train-dispatching algorithm in a simulation model of the movements of passenger and freight trains in Los Angeles County, and we validated the simulation as yielding an adequate approximation to the current system performance.",Deadlock; Dispatching; Modeling; Trains,Algorithms; Computer simulation; Freight transportation; Graph theory; Problem solving; Deadlock; Dispatching; Rail networks; Trains; Rails
Combining importance sampling and temporal difference control variates to simulate Markov chains,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042683131&doi=10.1145%2f974734.974735&partnerID=40&md5=9be65e2425ab13da9d9a3adf2e9a1042,"It is well known that in estimating performance measures associated with a stochastic system a good importance sampling distribution (IS) can give orders of magnitude of variance reduction while a bad one may lead to large, even infinite, variance. In this paper we study how this sensitivity of the estimator variance to the importance sampling change of measure may be ""dampened"" by combining importance sampling with stochastic approximation based temporal difference (TD) method. We consider a finite state space discrete time Markov chain (DTMC) with one-step transition rewards and an absorbing set of states and focus on estimating the cumulative expected reward to absorption starting from any state. In this setting we develop sufficient conditions under which the estimate resulting from the combined approach has a mean square error that asymptotically equals zero even when the estimate formed by using only importance sampling change of measure has infinite variance. In particular, we consider the problem of estimating the small buffer overflow probability in a queuing network, where the change of measure suggested in literature is shown to have infinite variance under certain parameters and where the appropriate combination of IS and TD method can be empirically seen to have a much faster convergence rate compared to naive simulation.",Importance sampling; Markov chains; Rare events; Stochastic approximation; Temporal difference methods; Variance reduction,Algorithms; Approximation theory; Computer simulation; Sampling; Set theory; Importance sampling; Rare events; Stochastic approximation; Temporal difference methods; Variance reduction; Markov processes
Time-space consistency in large-scale distributed virtual environments,2004,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042541703&doi=10.1145%2f974734.974736&partnerID=40&md5=089b535aa078e5df978c4540d9f650a0,"Maintaining a consistent view of the simulated world among different simulation nodes is a fundamental problem in large-scale distributed virtual environments (DVEs). In this paper, we characterize this problem by quantifying the time-space inconsistency in a DVE. To this end, a metric is defined to measure the time-space inconsistency in a DVE. One major advantage of the metric is that it may be estimated based on some characteristic parameters of a DVE, such as clock asynchrony, message transmission delay, the accuracy of the dead reckoning algorithm, the kinetics of the moving entity, and human factors. Thus the metric can be used to evaluate the time-space consistency property of a DVE without the actual execution of the DVE application, which is especially useful in the design stage of a DVE. Our work also clearly shows how the characteristic parameters of a DVE are interrelated in deciding the time-space inconsistency, so that we may fine-tune the DVE to make it as consistent as possible. To verify the effectiveness of the metric, a Ping-Pong game is developed. Experimental results show that the metric is effective in evaluating the time-space consistency property of the game.",Consistency; Dead reckoning algorithm; Distributed virtual environments,Algorithms; Computer simulation; Graph theory; Parameter estimation; Problem solving; Virtual reality; Consistency; Dead reckoning algorithm; Design stages; Distributed virtual environments; Large scale systems
HAVEGE: A user-level software heuristic for generating empirically strong random numbers,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4243195934&doi=10.1145%2f945511.945516&partnerID=40&md5=2cbf59258cc219c2cc9185da1f8617da,"Random numbers with high cryptographic quality are needed to enhance the security of cryptography applications. Software heuristics for generating empirically strong random number sequences rely on entropy gathering by measuring unpredictable external events. These generators only deliver a few bits per event. This limits them to being used as seeds for pseudorandom generators. General-purpose processors feature a large number of hardware mechanisms that aim to improve performance: caches, branch predictors,.... The state of these components is not architectural (i.e., the result of an ordinary application does not depend on it). It is also volatile and cannot be directly monitored by the user. On the other hand, every operating system interrupt modifies thousands of these binary volatile states. In this article, we present and analyze HAVEGE (HArdware Volatile Entropy Gathering and Expansion), a new user-level software heuristic to generate practically strong random numbers on general-purpose computers. The hardware clock cycle counter of the processor can be used to gather part of the entropy/uncertainty introduced by operating system interrupts in the internal states of the processor. Then, we show how this entropy gathering technique can be combined with pseudorandom number generation in HAVEGE. Since the internal state of HAVEGE includes thousands of internal volatile hardware states, it seems impossible even for the user itself to reproduce the generated sequences.",Cryptography; Hardware clock counters; Random number generation; Superscalar processor,Computer operating systems; Computer peripheral equipment; Computer simulation; Computer systems programming; Entropy; General purpose computers; Heuristic methods; Mathematical models; Probability; Random number generation; Statistics; Hardware clock counters; Pseudorandom generators; Superscalar processors; Volatile hardware states; Cryptography
Efficient multiply-with-carry random number generators with maximal period,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4243141795&doi=10.1145%2f945511.945514&partnerID=40&md5=4f341f2fcee318f8844e03f2161ae8bc,"In this (largely expository) article, we propose a simple modification of the multiply-with-carry random number generators of Marsaglia [1994] and Couture and L'Écuyer [1997]. The resulting generators are both efficient (since they may be configured with a base b which is a power of 2) and exhibit maximal period. These generators are analyzed using a simple but powerful algebraic technique involving b-adic numbers.",fcsr; Feedback shift register; k-distribution; Lattice structure; m-sequences; Multiply-with-carry; p-adic number; Primitive element; Random number generation,Algorithms; Computer simulation; Difference equations; Feedback; Linear algebra; Mathematical models; Monte Carlo methods; Random number generation; Statistics; Exhibit maximal period; Feedback shift register; K-distribution; M-sequences; Numerical methods
Empirical evidence concerning AES,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4243068589&doi=10.1145%2f945511.945515&partnerID=40&md5=fad9204b26be4dcccdc1be5768563014,"AES, the Advanced Encryption Standard, is one of the most important algorithms in modern cryptography. Certain randomness properties of AES are of vital importance for its security. At the same time, these properties make AES an interesting candidate for a fast nonlinear random number generator for stochastic simulation. In this article, we address both of these two aspects of AES. We study the performance of AES in a series of statistical tests that are related to cryptographic notions like confusion and diffusion. At the same time, these tests provide empirical evidence for the suitability of AES in stochastic simulation. A substantial part of this article is devoted to the strategy behind our tests and to their relation to other important test statistics like Maurer's Universal Test.",AES; Block ciphers; Confusion; Diffusion; Statistical tests,Algorithms; Mathematical models; Nonlinear systems; Random number generation; Statistical tests; Stochastic control systems; Advanced encryption standard (AES); Block ciphers; Confusion matrix; Stochastic simulation; Cryptography
Variance with alternative scramblings of digital nets,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4243059534&doi=10.1145%2f945511.945518&partnerID=40&md5=8a1ca491128ec179c0cf351cc95bc5c0,"There have been many proposals for randomizations of digital nets. Some of those proposals greatly reduce the computational burden of random scrambling. This article compares the sampling variance under different scrambling methods. Some scrambling methods adversely affect the variance, even to the extent of deteriorating the rate at which variance converges to zero. Surprisingly, a new scramble proposed here, has the effect of improving the rate at which the variance converges to zero, but so far, only for one dimensional integrands. The mean squared L 2 discrepancy is commonly used to study scrambling schemes. In this case, it does not distinguish among some scrambles with different convergence rates for the variance.",Derandomization; Quasi-Monte Carlo; Randomization,Computational complexity; Computer simulation; Convergence of numerical methods; Mathematical models; Monte Carlo methods; Random access storage; Statistical methods; Derandomization; Quasi-Monte Carlo; Randomization; Scrambling methods; Digital arithmetic
Continuous random variate generation by fast numerical inversion,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4243063993&doi=10.1145%2f945511.945517&partnerID=40&md5=1a49818fddf0b90be0904fdbcc71797b,"The inversion method for generating nonuniform random variates has some advantages compared to other generation methods, since it monotonically transforms uniform random numbers into nonuniform random variates. Hence, it is the method of choice in the simulation literature. However, except for some simple cases where the inverse of the cumulative distribution function is a simple function we need numerical methods. Often inversion by ""brute force"" is used, applying either very slow iterative methods or linear interpolation of the CDF and huge tables. But then the user has to accept unnecessarily large errors or excessive memory requirements, that slow down the algorithm. In this article, we demonstrate that with Hermite interpolation of the inverse CDF we can obtain very small error bounds close to machine precision. Using our adaptive interval splitting method, this accuracy is reached with moderately sized tables that allow for a fast and simple generation procedure.",Inversion method; Nonuniform random variates; Spline approximation; Universal method,Algorithms; Approximation theory; Computer simulation; Interpolation; Inverse kinematics; Linear systems; Machine design; Mathematical models; Splines; Inversion method; Non uniform random variates; Slow iterative methods; Spliting methods; Truncated distribution; Random number generation
"A system of high-dimensional, efficient, long-cycle and portable uniform random number generators",2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4243066342&doi=10.1145%2f945511.945513&partnerID=40&md5=6f3abcb2c1aac786cb6f45bc4287ab31,"We propose a system of multiple recursive generators of modulus p and order k where all nonzero coefficients of the recurrence are equal. The advantage of this property is that a single multiplication is needed to compute the recurrence, so the generator would run faster than the general case, For p = 2 31 -1, the most popular modulus used, we provide tables of specific parameter values yielding maximum period for recurrence of order k = 102 and 120. For p = 2 31 - 55719 and k = 1511, we have found generators with a period length approximately 10 14100-5.",DX-k generator; FMRG-k generator; Linear congruential generator; MT19937; Multiple recursive generator; Portable and efficient generator; Primitive polynomial,Algorithms; Decision tables; Linear systems; Mathematical models; Parameter estimation; Polynomials; Random number generation; Computation in finite fields; Congruential generators; Numerical algorithms; Primitive polynomials; Number theory
ACM Transactions on Modeling and Computer Simulation: Guest Editorial,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348162564&doi=10.1145%2f858481.858482&partnerID=40&md5=508c20789fec37b0bbbc10918a4cfece,[No abstract available],,
Dynamic Structure Multiparadigm Modeling and Simulation,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346243613&doi=10.1145%2f937332.937335&partnerID=40&md5=7088cc5551fadfd7887d4a0aac819fe3,"This article presents the Heterogeneous Flow System Specification (HFSS), a formalism aimed to represent hierarchical and modular hybrid flow systems with dynamic structure. The concept of hybrid flow systems provides a generalization of the conventional concept of hybrid system and it can represent a whole plethora of systems, namely: discrete event systems, multicomponent and multirate numerical methods, multirate and multicomponent sampling systems, event locators and time-varying systems. The ability to join all these types of models makes HFSS an excellent framework for merging components built in different paradigms. We present several examples of model definition in the HFSS formalism and we also exploit the ability of the HFSS formalism to represent mutirate numerical integrators.",Dynamic Structure systems; Hybrid systems; Multirate sampling; Variable step integration,Computer simulation; Digital computers; Hybrid computers; Integration; Mathematical models; Ordinary differential equations; Heterogeneous flow system specification (HFSS); Systems analysis
Behavior of the NORTA Method for Correlated Random Vector Generation as the Dimension Increases,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348134540&doi=10.1145%2f937332.937336&partnerID=40&md5=bd10e87a51bbc47f5e4c3e5a356d59f2,"The NORTA method is a fast general-purpose method for generating samples of a random vector with given marginal distributions and given correlation matrix. It is known that there exist marginal distributions and correlation matrices that the NORTA method cannot match, even though a random vector with the prescribed qualities exists. We investigate this problem as the dimension of the random vector increases. Simulation results show that the problem rapidly becomes acute, in the sense that NORTA fails to work with an increasingly large proportion of correlation matrices. Simulation results also show that if one is willing to settle for a correlation matrix that is ""close"" to the desired one, then NORTA performs well with increasing dimension. As part of our analysis, we develop a method for sampling correlation matrices uniformly (in a certain precise sense) from the set of all such matrices. This procedure can be used more generally for sampling uniformly from the space of all symmetric positive definite matrices with diagonal elements fixed at positive values.",NORTA method; Onion method; Sampling random matrices; Semidefinite programming,Algorithms; Computer simulation; Correlation methods; Matrix algebra; Random number generation; Random processes; Vectors; Onion method; Semidefinite programming (SDP); Computer programming
Modeling and Generating Multivariate Time-Series Input Processes Using a Vector Autoregressive Technique,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346873994&doi=10.1145%2f937332.937333&partnerID=40&md5=ee6459d7e59ded310d6e81e7fba63ace,We present a model for representing stationary multivariate time-series input processes with marginal distributions from the Johnson translation system and an autocorrelation structure specified through some finite lag. We then describe how to generate data accurately to drive computer simulations. The central idea is to transform a Gaussian vector autoregressive process into the desired multivariate time-series input process that we presume as having a VARTA (Vector-Autoregressive-To-Anything) distribution. We manipulate the autocorrelation structure of the Gaussian vector autoregressive process so that we achieve the desired autocorrelation structure for the simulation input process. We call this the correlation-matching problem and solve it by an algorithm that incorporates a numerical-search procedure and a numerical-integration technique. An illustrative example is included.,Input modeling; Multivariate time series; Numerical integration; Vector autoregressive process,Algorithms; Computer programming languages; Computer simulation; Integration; Mathematical models; Problem solving; Regression analysis; Theorem proving; Time series analysis; Vectors; Vector autoregressive process; Computer systems
"Discrete-Event Simulation Optimization Using Ranking, Selection, and Multiple Comparison Procedures: A Survey",2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346271770&doi=10.1145%2f858481.858484&partnerID=40&md5=2d5523d26d747f85a13dca0fb7a6df1e,An important use for discrete-event simulation models lies in comparing and contrasting competing design alternatives without incurring any physical costs. This article presents a survey of the literature for two widely used classes of statistical methods for selecting the best design from among a finite set of k alternatives: ranking and selection (R&S) and multiple comparison procedures (MCPs). A comprehensive survey of each topic is presented along with a summary of recent unified R&S-MCP approaches. Procedures are recommended based on their statistical efficiency and ease of application; guidelines for procedure application are offered.,Multiple comparisons; Ranking and selection; Simulation optimization,Algorithms; Computer simulation; Discrete time control systems; Mathematical models; Optimization; Probability; Selection; Statistical methods; Discrete-event systems (DES); Systems analysis
Computing the Distribution Function of a Conditional Expectation via Monte Carlo: Discrete Conditioning Spaces,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346152767&doi=10.1145%2f937332.937334&partnerID=40&md5=22f0885301badbd6de47ca5cd7a9c18d,"We examine different ways of numerically computing the distribution function of conditional expectations where the conditioning element takes values in a finite or countably infinite outcome space. Both the conditional expectation and the distribution function itself are computed via Monte Carlo simulation. Given a limited (and fixed) computer budget, the quality of the estimator is gauged by the inverse of its mean square error. It is a function of the fraction of the budget allocated to estimating the conditional expectation versus the amount of sampling done relative to the ""conditioning variable."" We will present the asymptotically optimal rates of convergence for different estimators and resolve the trade-off between the bias and variance of the estimators. Moreover, central limit theorems are established for some of the estimators proposed. We will also provide algorithms for the practical implementation of two of the estimators and illustrate how confidence intervals can be formed in each case. Major potential application areas include calculation of Value at Risk (VaR) in the field of mathematical finance and Bayesian performance analysis.",Conditional expectation; Distribution functions; Probability algorithms,Algorithms; Computer simulation; Error analysis; Monte Carlo methods; Probability distributions; Distribution functions; Mean square errors; Computational methods
Variable-Sample Methods for Stochastic Optimization,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346902109&doi=10.1145%2f858481.858483&partnerID=40&md5=343a9cd2f689159b8d3dd1299bbb06fc,"In this article we discuss the application of a certain class of Monte Carlo methods to stochastic optimization problems. Particularly, we study variable-sample techniques, in which the objective function is replaced, at each iteration, by a sample average approximation. We first provide general results on the schedule of sample sizes, under which variable-sample methods yield consistent estimators as well as bounds on the estimation error. Because the convergence analysis is performed pathwisely, we are able to obtain our results in a flexible setting, which requires mild assumptions on the distributions and which includes the possibility of using different sampling distributions along the algorithm. We illustrate these ideas by studying a modification of the well-known pure random search method, adapting it to the variable-sample scheme, and show conditions for convergence of the algorithm. Implementation issues are discussed and numerical results are presented to illustrate the ideas.",Monte Carlo methods; Pathwise bounds; Random search; Stochastic optimization,Algorithms; Approximation theory; Computer simulation; Convergence of numerical methods; Error analysis; Global optimization; Iterative methods; Monte Carlo methods; Probability distributions; Random processes; Sampling; Scheduling; Estimation errors; Pathwise bounds; Stochastic programming
A Combined Procedure for Optimization via Simulation,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346902104&doi=10.1145%2f858481.858485&partnerID=40&md5=6c22ca6f673af01e502c2ed076bfd171,"The problem of optimizing the expected performance of a discrete-event, stochastic system was studied. The study proposed an optimization-via-simulation algorithm for solving the stochastic, discrete-event simulation problem. The decision variables of the system were such that they might be subjected to the deterministic linear integer constraints. The proposed approach, which consisted of a global guidance system, a selection-of-the-best procedure, and local improvement method, was globally convergent under mild conditions.",Random search; Ranking and selection; Stochastic optimization,Algorithms; Computational complexity; Computer simulation; Constraint theory; Convergence of numerical methods; Decision making; Discrete time control systems; Global optimization; Heuristic methods; Integer programming; Inventory control; Linear programming; Markov processes; Simulated annealing; Theorem proving; Deterministic methods; Discrete-event systems (DES); Stochastic programming
Two-Timescale Simultaneous Perturbation Stochastic Approximation using Deterministic Perturbation Sequences,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346902105&doi=10.1145%2f858481.858486&partnerID=40&md5=2ffd4543450977c65a9515aa5030d163,"Simultaneous perturbation stochastic approximation (SPSA) algorithms have been found to be very effective for high-dimensional simulation optimization problems. The main idea is to estimate the gradient using simulation output performance measures at only two settings of the N -dimensional parameter vector being optimized rather than at the N + 1 or 2N settings required by the usual one-sided or symmetric difference estimates, respectively. The two settings of the parameter vector are obtained by simultaneously changing the parameter vector in each component direction using random perturbations. In this article, in order to enhance the convergence of these algorithms, we consider deterministic sequences of perturbations for two-timescale SPSA algorithms. Two constructions for the perturbation sequences are considered: complete lexicographical cycles and much shorter sequences based on normalized Hadamard matrices. Recently, one-simulation versions of SPSA have been proposed, and we also investigate these algorithms using deterministic sequences. Rigorous convergence analyses for all proposed algorithms are presented in detail. Extensive numerical experiments on a network of M/G/1 queues with feedback indicate that the deterministic sequence SPSA algorithms perform significantly better than the corresponding randomized algorithms.",Deterministic perturbations; Hadamard matrices; Simulation optimization; SPSA; Stochastic approximation; Two-timescale algorithms,Algorithms; Approximation theory; Computer simulation; Gradient methods; Matrix algebra; Monte Carlo methods; Parameter estimation; Perturbation techniques; Probability distributions; Random processes; Vectors; Deterministic methods; Lexicography; Stochastic programming
Efficient Simulation of Queues in Heavy Traffic,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042499494&doi=10.1145%2f778553.778556&partnerID=40&md5=6e9984c5f11eb74ca9e1f9a1074261e2,"When simulating queues in heavy traffic, estimators of quantities such as average delay in queue d converge slowly to their true values. This problem is exacerbated when interarrivai and service distributions are irregular. For the GI/G/1 queue, delay moments can be expressed in terms of moments of idle period I. Instead of estimating d directly by a standard regenerative estimator that we call DD, a method we call DI estimates d from estimated moments of I. DI was investigated some time ago and shown to be much more efficient than DD in heavy traffic. We measure efficiency as the factor by which variance is reduced. For the GI/G/1 queue, we show how to generate a sequence of realized values of the equilibrium idle period, I e that are not independent and identically distributed, but have the correct statistical properties in the long run. We show how to use this sequence to construct a new estimator of d, called DE, and of higher moments of delay as well. When arrivals are irregular, we show that DE is more efficient than DI, in some cases by a large factor, independent of the traffic intensity. Comparing DE with DD, these factors multiply. For GI/G/c, we construct a control-variates estimator of average delay in queue d c that is efficient in heavy traffic. It uses DE to estimate the average delay for the corresponding fast single server. We compare the efficiency of this method with another method in the literature. For M/G/c, we use insensitivity to construct another control-variates estimator of d c. We compare the efficiency of this estimator with the two c-server estimators above.",Control variates; Delay; Equilibrium distribution; GI/G/1; GI/G/c; Heavy traffic; Idle periods; Insensitivity; M/G/c; Processor sharing; Variance reduction,C (programming language); Computer simulation; Parameter estimation; Probability; Program processors; Queueing theory; Random number generation; Processor sharing; Variance reduction; Telecommunication traffic
Modeling Methodology for Integrated Simulation of Embedded Systems,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344462466&doi=10.1145%2f778553.778557&partnerID=40&md5=a1beb9e3669dc2c10d62abd4f83ba2b0,"Developing a single embedded application involves a multitude of different development tools including several different simulators. Most tools use different abstractions, have their own formalisms to represent the system under development, utilize different input and output data formats, and have their own semantics. A unified environment that allows capturing the system in one place and one that drives all necessary simulators and analysis tools from this shared representation needs a common representation technology that must support several different abstractions and formalisms seamlessly. Describing the individual formalisms by metamodels and carefully composing them is the underlying technology behind MILAN, a Model-based Integrated Simulation Framework. MILAN is an extensible framework that supports multigranular simulation of embedded systems by seamlessly integrating existing simulators into a unified environment. Formal metamodels and explicit constraints define the domain-specific modeling language developed for MILAN that combines hierarchical, heterogeneous, parametric dataflow representation with strong data typing. Multiple modeling aspects separate orthogonal concepts. The language also allows the representation of the design space of the application, not just a point solution. Nonfunctional requirements are captured as formal, application-specific constraints. MILAN has integrated tool support for design-space exploration and pruning. The models are used to automatically configure the integrated functional simulators, high-level performance and power estimators, cycle-accurate performance simulators, and power-aware simulators. Simulation results are used to automatically update the system models. The article focuses on the modeling methodology and briefly describes how the integrated models are utilized in the framework.",Domain specific languages; Metamodeling; Model integrated computing; Modeling; Simulation; Simulation integration,Abstracting; Computer simulation; Metadata; Semantics; Domain specific languages; Integrated simulation; Metamodeling; Embedded systems
Guest Introduction,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025399108&doi=10.1145%2f945511.945512&partnerID=40&md5=71eb51b62c382c48bbd1090246cf9d4e,[No abstract available],,
Declaration of Unknowns in DAE-Based Hybrid System Specification,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348220544&doi=10.1145%2f778553.778555&partnerID=40&md5=a2ef1f79f1e6bade322860c342c1ef33,"The majority of hybrid languages are based on the assumption that discontinuities in differential variables at discrete events are modeled by explicit mappings. When there are algebraic equations restricting the allowed new values of the differential variables, explicit remapping of differential variables forces the modeler to solve the algebraic equations. To overcome this difficulty, hybrid languages use many different language elements. This article shows that only one language element is needed for this purpose: an unknown declaration, which allows the explicit declaration of a variable as unknown. The syntax and semantics of unknown declarations are discussed. Examples are given, using the Chi language, in which unknown declarations are used for modeling multi-ALURbody collision, steady-state initialization, and consistent initialization of higher index systems. It is also illustrated how the declaration of unknowns can help to clarify the structure of the system of equations, and how it can help the modeler detect structurally singular systems of equations.",Consistent initial conditions; Hybrid systems; Initial value problem; Semantics,Algebra; Computer simulation; Degrees of freedom (mechanics); Differential equations; Initial value problems; Semantics; Hybrid systems; Syntax; Hybrid computers
Flexible Reference Trace Reduction for VM Simulations,2003,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348220543&doi=10.1145%2f778553.778554&partnerID=40&md5=53a5fe441a40af2f6f965aff533c406e,[No abstract available],Cache hierarchies; Locality; Reference traces; Trace compression; Trace reduction,Algorithms; C (programming language); Computer program listings; Computer simulation; Virtual storage; Cache hierarchies; Trace compression; Trace reduction; Cache memory
"Modeling, simulation, sensitivity analysis, and optimization of hybrid systems",2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542515619&doi=10.1145%2f643120.643122&partnerID=40&md5=50a9cc1ea15541a8800497effcb8e9cb,"Hybrid (discrete/continuous) systems exhibit both discrete state and continuous state dynamics which interact to such a significant extent that they cannot be decoupled and must be analyzed simultaneously. We present an overview of the work that has been done in the modeling, simulation, sensitivity analysis, and optimization of hybrid systems, paying particular attention to the interaction between discrete and continuous dynamics. A concise intuitive framework for hybrid system modeling is presented, together with discussions on robust state event location, transfer functions of the continuous state at discontinuities, parametric sensitivity analysis of hybrid systems, and challenges in optimization.",Combined discrete/continuous simulation; Consistent reinitialization; Discontinuities; Hybrid automata; Sensitivity analysis; State events; Transitions,Automation; Computer simulation; Discrete time control systems; Embedded systems; Hybrid computers; Optimization; Parameter estimation; Sensitivity analysis; Consistent reinitialization; Continuos dynamics; Hybrid systems; State events; Transitions; Systems analysis
A component-based approach to modeling and simulating mixed-signal and hybrid systems,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1942537327&doi=10.1145%2f643120.643125&partnerID=40&md5=96c9408cf5fb8288ab99449b2910aad3,"Systems with both continuous and discrete behaviors can be modeled using a mixed-signal style or a hybrid systems style. This article presents a component-based modeling and simulation framework that supports both modeling styles. The component framework, based on an actor metamodel, takes a hierarchical approach to manage heterogeneity in modeling complex systems. We describe how ordinary differential equations, discrete event systems, and finite-state machines can be built under this metamodel. A mixed-signal system is a hierarchical composition of continuous-time and discrete event models, and a hybrid system is a hierarchical composition of continuous-time and finite-state-machine models. Hierarchical composition and information hiding help build clean models and efficient execution engines. Simulation technologies, in particular, the interaction between a continuous-time ODE solving engine and various discrete simulation engines are discussed. A signal type system is introduced to schedule hybrid components inside a continuous-time environment. Breakpoints are used to control the numerical integration step sizes so that discrete events are handled properly. A ""refiring"" mechanism and a ""rollback"" mechanism are designed to manage continuous components inside a discrete event environment. The technologies are implemented in the Ptolemy II software environment. Examples are given to show the applications of this framework in mixed-signal and hybrid systems.",Actors-oriented design; Component-based modeling; Hierarchical heterogeneity; Hybrid systems; Mixed-signal systems; Ptolemy II; Simulation,Computer simulation; Finite automata; Hierarchical systems; Large scale systems; Mathematical models; Operations research; Ordinary differential equations; Software engineering; Actors-oriented design; Component-based modeling; Hierarchial heterogeneity; Hybrid systems; Mixed-signal systems; Potolemy II; Systems analysis
Metamodeling in EIA/CDIF - Meta-metamodel and metamodels,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4243195971&doi=10.1145%2f643120.643124&partnerID=40&md5=cbd2e5f952eefa6c02e4ca2db2a9729b,"This article introduces the EIA/CDIF set of standards for the modeling of information systems and its exchange among computer-aided software tools of different vendors. It lays out the meta-metamodel and the standardized metamodels which are fully depicted in a hierarchical layout and annotated with the unique identifiers of all the standardized modeling concepts. The article also stresses the fact that EIA/CDIF has been used as the baseline in the creation of an international standard, the ISO/CDIF set of models, an ongoing project.",CASE Data Interchange Format; CDIF; Clear text encoding; EIA; Languages; Meta-metamodels; Metamodels; System design,Computer aided design; Computer aided software engineering; Computer programming languages; Computer simulation; Data processing; Eigenvalues and eigenfunctions; Hierarchical systems; Project management; Systems analysis; CASE data interchange format; Clear text encoding; International standard; Meta-metamodels; Information analysis
Guest Editorial: Special Issue on Computer Automated Multi-Paradigm Modeling,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988926909&doi=10.1145%2f643120.643121&partnerID=40&md5=f297fde57e8f2fb70755a6848799da71,[No abstract available],,
Rearchitecting the UML infrastructure,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141796082&doi=10.1145%2f643120.643123&partnerID=40&md5=2b977dbfe2c0b2b46bdd3300a0129d90,"Metamodeling is one of the core foundations of computer-automated multiparadigm modeling. However, there is currently little agreement about what form the required metamodeling approach should take and precisely what role metamodels should play. This article addresses the problem by first describing some fundamental problems in the industry's leading metamodeling technology, the UML framework, and then explaining how this framework could be rearchitected to overcome these problems. Three main issues are identified in the current framework: the dual classification problem arising from the need to capture both the logical and physical classification of model elements, the class / object duality problem arising from the need to capture both the classlike and objectlike facets of some model elements, and the replication of concepts problem arising from the need to define certain concepts multiple times. Three main proposals for rearchitecting the UML framework to overcome these problems are then presented: the separation of logical and physical classification dimensions, the unification of the class and object facets of model elements, and the enhancement of the instantiation mechanism to allow definitions to transcend multiple levels. The article concludes with a discussion of other practical issues involved in rearchitecting the UML modeling framework in the proposed way.",Classification; Classification dimensions; Deep instantiation; Metamodeling; Strict metamodeling; UML infrastructure; Unified modeling language,Abstracting; Computer simulation; Formal logic; Information analysis; Problem solving; Classification; Deep instantiation; Metamodeling; Strict metamodeling; UML infrastructure; Unified modeling language; Object oriented programming
Simulating Heavy Tailed Processes Using Delayed Hazard Rate Twisting,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903995767&doi=10.1145%2f566392.566394&partnerID=40&md5=0193def41f57cd6711dd203b12d14036,"Consider the problem of estimating the small probability that the maximum of a random walk exceeds a large threshold, when the process has a negative drift and the underlying random variables may have heavy tailed distributions, that is, their tail distribution decays at a subexponential rate. We consider one class of such problems that has applications in estimating the ruin probability associated with insurance claim processes with subexponentially distributed claim sizes, and in estimating the probability of large delays in an M/G/1 queue with subexponentially distributed service times. Significant work has been done on analogous problems for the light tailed case (when the tail distribution decreases at an exponential rate or faster) that involve importance sampling methods using appropriate exponential twisting. However, for the subexponential case, such exponential twisting is infeasible and alternative techniques are needed. In this paper we introduce importance sampling techniques where the new probability measure is obtained by twisting the hazard rate of the original distribution. For subexponential distributions this amounts to subexponential twisting-twisting at a subexponential rate. In addition, we introduce the technique of “delaying” the twisting and show that the combination of the two techniques produces asymptotically optimal estimates of the small probability mentioned above. © 2002, ACM. All rights reserved.",Algorithms; Hazard rate twisting; heavy tailed distributions; importance sampling; Performance; rare events; subexponential twisting; Theory; variance reduction,
Fast Generation of Order Statistics,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024262514&doi=10.1145%2f566392.566393&partnerID=40&md5=5c8fc124773b859a7ab98ab7589f59fe,"Generating a single order statistic without generating the full sample can be an important task for simulations. If the density and the CDF of the distribution are given, then it is no problem to compute the density of the order statistic. In the main theorem it is shown that the concavity properties of that density depend directly on the distribution itself. Especially for log-concave distributions, all order statistics have log-concave distributions themselves. So recently suggested automatic transformed density rejection algorithms can be used to generate single order statistics. This idea leads to very fast generators. For example for the normal and gamma distributions, the suggested new algorithms are between 10 and 60 times faster than the algorithms suggested in the literature. © 2002, ACM. All rights reserved.",Algorithms; automatic algorithms; order statistics; Rejection method; T-concave; transformed density rejection,
Rapid model parameterization from traffic measurements,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542597362&doi=10.1145%2f643114.643117&partnerID=40&md5=c71f6e87b005d04c6bff43b1a9ab6c71,"The utility of simulations and analysis heavily relies on good models of network traffic. While network traffic constantly is changing over time, existing approaches typically take years from collecting trace, analyzing the data to finally generating and implementing models. In this paper, we describe approaches and tools that support rapid parameterization of traffic models from live network measurements. Rather than treating measured traffic as a time-series of statistics, we utilize the traces to estimate end-user behavior and network conditions to generate application-level simulation models. We also show multi-scaling analytic techniques are helpful for debugging and validating the model. To demonstrate our approaches, we develop structural source-level models for web and FTP traffic and evaluate their accuracy by comparing the outputs of simulation against the original trace. We also compare our work with existing traffic generation tools and show our approach is more flexible in capturing the heterogeneity of traffic. Finally, we automate and integrate the process from trace analysis to model validation for easy model parameterization from new data.",Model parameterization; Network; Traffic model,Bandwidth; Computer aided network analysis; Computer programming; Computer simulation; Data recording; Internet; Program debugging; Statistical methods; Time series analysis; Topology; Model parametrization; Simulation models; Traffic generation tools; Traffic model; Telecommunication traffic
On the processor scheduling problem in time warp synchronization,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4243118676&doi=10.1145%2f643114.643115&partnerID=40&md5=4eb596c433a2bfd7cf25f1679f457927,"Time Warp is a synchronization mechanism for parallel/distributed simulation. It allows logical processes (LPs) to execute events without the guarantee of a causally consistent execution. Upon the detection of a causality violation, rollback procedures recover the state of the simulation to a correct value. When a rollback occurs there are two primary sources of performance loss: (1) CPU time must be spent for the execution of the rollback procedures and (2) waste of CPU time arises from the invalidation of event executions. In this paper we present a general framework for the problem of scheduling the next LP to be run on a processor in Time Warp simulations. The framework establishes a class of scheduling algorithms having the twofold aim to keep low the CPU time for the execution of the rollback procedures and also to guarantee low waste of time due to event executions invalidated by rollback. The combination of these two aims should actually lead to short completion time of Time Warp simulations. We collocate existing scheduling algorithms within the framework, pointing out how they miss previous aims, at least partially. Then we instantiate a Window-based Grain Sensitive (WGS) scheduling algorithm relying on the framework, which pursues the above twofold aim. We also identify the proper conditions, associated with the simulation model execution, under which any algorithm exploiting the framework structure is expected to benefit the performance of the Time Warp mechanism. Empirical evidence from an experimental study of WGS on classical benchmarks and on a mobile communication system simulation fully confirms the theoretical outcomes.",Optimistic synchronization,Algorithms; Computation theory; Computer simulation; Mathematical models; Mobile telecommunication systems; Sensitivity analysis; Specifications; Synchronization; Time series analysis; Experimentations; Mechanical machining; Performances; Primary sources; Program processors
Efficient Simulation of a Tandem Jackson Network,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015390741&doi=10.1145%2f566392.566395&partnerID=40&md5=4b90481f0fc15a0c88b4f34866ad4d3c,"The two-node tandem Jackson network serves as a convenient reference model for the analysis and testing of different methodologies and techniques in rare event simulation. In this paper we consider a new approach to efficiently estimate the probability that the content of the second buffer exceeds some high level L before it becomes empty, starting from a given state. The approach is based on a Markov additive process representation of the buffer processes, leading to an exponential change of measure to be used in an importance sampling procedure. Unlike changes of measures proposed and studied in recent literature, the one derived here is a function of the content of the first buffer. We prove that when the first buffer is finite, this method yields asymptotically efficient simulation for any set of arrival and service rates. In fact, the relative error is bounded independent of the level L; a new result which is not established for any other known method. When the first buffer is infinite, we propose a natural extension of the exponential change of measure for the finite buffer case. In this case, the relative error is shown to be bounded (independent of L) only when the second server is the bottleneck; a result which is known to hold for some other methods derived through large deviations analysis. When the first server is the bottleneck, experimental results using our method seem to suggest that the relative error is bounded linearly in L. © 2002, ACM. All rights reserved.",Algorithms; Bounded relative error; Experimentation; importance sampling; Markov additive processes; orthogonal polynomials; Performance; rare event simulation; tandem Jackson network; Theory,
Parallel simulation of chip-multiprocessor architectures,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4243073044&doi=10.1145%2f643114.643116&partnerID=40&md5=1c41ce05a495b849d6213ba6d9d08e2f,"Chip-multiprocessor (CMP) architectures present a challenge for efficient simulation, combining the requirements of a detailed microprocessor simulator with that of a tightly-coupled parallel system. In this paper, a distributed simulator for target CMPs is presented based on the Message Passing Interface (MPI) designed to run on a host cluster of workstations. Microbenchmark-based evaluation is used to narrow the parallelization design space concerning the performance impact of distributed vs. centralized target L2 simulation, blocking vs. non-blocking remote cache accesses, null-message vs. barrier techniques for clock synchronization, and network interconnect selection. The best combination is shown to yield speedups of up to 16 on a 9-node cluster of dual-CPU workstations, partially due to cache effects.",Chip multiprocessors (CMP); Microbenchmarks; Myrinet; Scalable Coherent Interface (SCI),Cache memory; Computer aided design; Computer architecture; Computer simulation; Coupled circuits; Interfaces (computer); Microprocessor chips; Parallel algorithms; Synchronization; Barrier techniques; Chip-multiprocessors (CMP); Microbenchmark-based passing interference (MPI); Scalable coherent interfaces (SCI); Multiprocessing systems
A two-stage modeling and simulation process for web-based modeling and simulation,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242598287&doi=10.1145%2f643114.643118&partnerID=40&md5=510b4bd4dd76e21826bfaf35b85a7df5,"The area of web-based simulation has thrived primarily on novel methods for executing models, with contributions to both client and server side applications based in Java and other web-accessible languages. However, there has not been a commensurate degree of research in the area of model design, and in the dissemination of models over the web to support discrete event simulation. For a web-based modeling framework to succeed, that framework must support at least one markup language for specifying model content, as well as methods for converting the markup to a programming language, which is used to simulate the model. We present such a framework, called rube, and focus on a two-stage translation process from one markup language (MXL) to another (DXL), with a final process to create either Java or Javascript target code. This framework has allowed us to leverage the extensible Markup Language (XML) for capturing the dynamics of a small number of key modeling types commonly found in computer simulation applications. Our approach allows us to achieve customization of a model's appearance, as well to obtain the transformative benefits of specifying model information within an XML format.",DOM; Framework; Methodology; Modeling; Multimodeling; Simulation; Web-based; X3D; XML; XSLT,Computer architecture; Computer simulation; Information dissemination; Information science; Java programming language; Mathematical models; Mathematical transformations; Systems analysis; World Wide Web; XML; Framework; Methodology; Multimodeling; Simulation analysis; Web-based; Interactive computer systems
Estimation of Blocking Probabilities in Cellular Networks with Dynamic Channel Assignment,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348224539&doi=10.1145%2f511442.511445&partnerID=40&md5=3cd812444cfff033646275ae618eed32,"Blocking probabilities in cellular mobile communication networks using dynamic channel assignment are hard to compute for realistic sized systems. This computational difficulty is due to the structure of the state space, which imposes strong coupling constraints amongst components of the occupancy vector. Approximate tractable models have been proposed, which have product form stationary state distributions. However, for real channel assignment schemes, the product form is a poor approximation and it is necessary to simulate the actual occupancy process in order to estimate the blocking probabilities. Meaningful estimates of the blocking probability typically require an enormous amount of CPU time for simulation, since blocking events are usually rare. Advanced simulation approaches use importance sampling (IS) to overcome this problem. In this article, we study two regimes under which blocking is a rare event: low-load and high cell capacity. Our simulations use the standard clock (SC) method. For low load, we propose a change of measure that we call static ISSC, which has bounded relative error. For high capacity, we use a change of measure that depends on the current state of the network occupancy. This is the dynamic ISSC method. We prove that this method yields zero variance estimators for single clique models, and we empirically show the advantages of this method over naïve simulation for networks of moderate size and traffic loads.",,Approximation theory; Channel capacity; Computational complexity; Computational methods; Computer simulation; Error analysis; Probability; Problem solving; State space methods; Telecommunication traffic; Vectors; Cellular networks; Dynamic channel assignment; Importance sampling (IS); Traffic loads; Mobile telecommunication systems
A Large Deviations Analysis of the Transient of a Queue with Many Markov Fluid Inputs: Approximations and Fast Simulation,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348224598&doi=10.1145%2f511442.511443&partnerID=40&md5=da54e7bfb8736a55f0efaa35367755d7,"This article analyzes the transient buffer content distribution of a queue fed by a large number of Markov fluid sources. We characterize the probability of overflow at time t, given the current buffer level and the number of sources in the on-state. After scaling buffer and bandwidth resources by the number of sources n, we can apply large deviations techniques. The transient overflow probability decays exponentially in n. In case of exponential on/off sources, we derive an expression for the decay rate of the rare event probability under consideration. For general, Markov fluid sources, we present a plausible conjecture. We also provide the ""most likely path"" from the initial state to overflow (at time t). Knowledge of the decay rate and the most likely path to overflow leads to (i) approximations of the transient overflow probability, and (ii) efficient simulation methods of the rare event of buffer overflow, The simulation methods, based on importance sampling, give a huge speed-up compared to straightforward simulations. The approximations are of low computational complexity, and accurate, as verified by means of simulation experiments.",ATM multiplexers; Buffer overflow; Calculus of variations; Importance sampling simulations; IP routers; Large deviations asymptotics; Queuing theory; Transient probabilities,Approximation theory; Asynchronous transfer mode; Bandwidth; Buffer storage; Computational complexity; Computational methods; Computer simulation; Heuristic methods; Laplace transforms; Markov processes; Mathematical models; Network protocols; Probability; Routers; Telecommunication traffic; Buffer overflow; Decay rate; Importance sampling; Transient probabilities; Queueing theory
Cross-Entropy and Rare Events for Maximal Cut and Partition Problems,2002,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012303220&doi=10.1145%2f511442.511444&partnerID=40&md5=4f955133481075140fd65b878a577bd0,"We show how to solve the maximal cut and partition problems using a randomized algorithm based on the cross-entropy method. For the maximal cut problem, the proposed algorithm employs an auxiliary Bernoulli distribution, which transforms the original deterministic network into an associated stochastic one, called the associated stochastic network (ASN). Each iteration of the randomized algorithm for the ASN involves the following two phases: (1) Generation of random cuts using a multidimensional Ber(p) distribution and calculation of the associated cut lengths (objective functions) and some related quantities, such as rare-event probabilities. (2) Updating the parameter vector p on the basis of the data collected in the first phase. We show that the Ber(p) distribution converges in distribution to a degenerated one, Ber(pd*), pd* = (pd,1,. . ., pd,n) in the sense that some elements of pd*, will be unities and the rest zeros. The unity elements of pd* uniquely define a cut which will be taken as the estimate of the maximal cut. A similar approach is used for the partition problem. Supporting numerical results are given as well. Our numerical studies suggest that for the maximal cut and partition problems the proposed algorithm typically has polynomial complexity in the size of the network.",Combinatorial optimization; Cross-entropy; Importance sampling; Rare event simulation,Algorithms; Combinatorial mathematics; Computational complexity; Computer simulation; Heuristic methods; Iterative methods; Monte Carlo methods; Parameter estimation; Polynomial approximation; Probability; Random processes; Simulated annealing; Telecommunication networks; Traveling salesman problem; Vectors; Combinatorial optimization; Importance sampling; Rare event simulation; Stochastic networks; Problem solving
A Fully Sequential Procedure for Indifference-Zone Selection in Simulation,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346963698&doi=10.1145%2f502109.502111&partnerID=40&md5=6443eea6288d5678ebc8fcdce387293f,"We present procedures for selecting the best or near-best of a finite number of simulated systems when best is defined by maximum or minimum expected performance. The procedures are appropriate when it is possible to repeatedly obtain small, incremental samples from each simulated system. The goal of such a sequential procedure is to eliminate, at an early stage of experimentation, those simulated systems that are apparently inferior, and thereby reduce the overall computational effort required to find the best. The procedures we present accommodate unequal variances across systems and the use of common random numbers. However, they are based on the assumption of normally distributed data, so we analyze the impact of batching (to achieve approximate normality or independence) on the performance of the procedures. Comparisons with some existing indifference-zone procedures are also provided.",Multiple comparisons; Output analysis; Ranking and selection; Variance reduction,Algorithms; Approximation theory; Computational methods; Data reduction; Probability; Random number generation; Random processes; Statistical methods; Multivariate statistics; Steady state simulation; Computer simulation
Packet Delay in Models of Data Networks,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141834551&doi=10.1145%2f502109.502110&partnerID=40&md5=f77dea01cb0b9c1a243a9483293cdb14,"We investigate individual packet delay in a model of data networks with table-free, partial table and full table routing. We present analytical estimation for the average packet delay in a network with small partial routing table. Dependence of the delay on the size of the network and on the size of the partial routing table is examined numerically. Consequences for network scalability are discussed.",Hitting time; Packet delay; Packet switching; Random walk; Routing table,Algorithms; Computer networks; Computer simulation; Convergence of numerical methods; Decision tables; Linear systems; Packet switching; Random processes; Network scalability; Packet delay; Packet networks
Random Number Generation with Primitive Pentanomials,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941149175&doi=10.1145%2f508366.508368&partnerID=40&md5=0d2753d2d11f323e3ab763a959e90970,"This paper presents generalized feedback shift register (GFSR) generators with primitive polynomials xp+ xp-1+ xq+ xq-1+ 1. The recurrence of these generators can be efficiently computed. We adopt Fushimi's initialization scheme, which guarantees the k-distribution property. Statistical and timing results are presented. © 2001, ACM. All rights reserved.",Algorithms; Design; generalized feedback shift register; Primitive trinomials,Shift registers; Feedback shift register; K distribution; Pentanomials; Primitive polynomials; Primitive trinomials; Random number generation
A Methodology for Certification of Modeling and Simulation Applications,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013260241&doi=10.1145%2f508366.508369&partnerID=40&md5=ae94e86f44e7f5cab6873c417acfefd0,"Certification of modeling and simulation (M'S) applications poses significant technical challenges for M'S program managers, engineers, and practitioners. Certification is becoming increasingly more important as M'S applications are used more and more for military training, complex system design evaluation, M'S-based acquisition, problem solving, and critical decision making. Certification, a very complex process, involves the measurement and evaluation of hundreds of qualitative and quantitative elements, mandates subject matter expert evaluation, and requires the integration of different evaluations. Planning and managing such measurements and evaluations requires a unifying methodology and should not be performed in an ad hoc manner. This paper presents such a methodology. The methodology consists of the following body of methods, rules, and postulates: (a) employment of subject matter experts, (b) construction of a hierarchy of indicators, (c) relative criticality weighting of indicators using the analytic hierarchy process, (d) using a rule-based expert knowledge base with an object-oriented specification language, (e) assignment of crisp, fuzzy, and nominal scores for the indicators, (f) aggregation of indicator scores, (g) graphical representation of the indicator scores and weights, (h) hypertext certification report, and (i) interpretation of the results. The methodology can be used for certification of any kind of M'S application either throughout the M'S development life cycle or after the development is completed. © 2001, ACM. All rights reserved.",accreditation; certification; credibility assessment; evaluation; Measurement; quality assessment; validation; Verification; verification,Accreditation; Application programs; C (programming language); Decision making; Hypertext systems; Knowledge based systems; Life cycle; Military applications; Personnel training; Problem solving; Specification languages; Verification; Certification; Credibility assessment; Evaluation; Quality assessment; Validation; Quality control
Cloning Parallel Simulations,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883422130&doi=10.1145%2f508366.508370&partnerID=40&md5=2afd048b94940d51acbba9b5a48218ea,"We present a cloning mechanism that enables the evaluation of multiple simulated futures. Performance of the mechanism is analyzed and evaluated experimentally on a shared memory multiprocessor. A running parallel discrete event simulation is dynamically cloned at decision points to explore different execution paths concurrently. In this way, what-if and alternative scenario analysis can be performed in applications such as gaming or tactical and strategic battle management. A construct called virtual logical processes avoids repeating common computations among clones and improves efficiency. The advantages of cloning are preserved regardless of the number of clones (or execution paths). Our performance results with a commercial air traffic control simulation demonstrate that cloning can significantly reduce the time required to compute multiple alternate futures. © 2001, ACM. All rights reserved.",Algorithms; Cloning; multiprocessors; parallel algorithms; parallel simulation; Performance; pruning,Air traffic control; Discrete event simulation; Multiprocessing systems; Parallel algorithms; Battle management; Logical process; Multiprocessors; Parallel discrete event simulations; Parallel simulations; Pruning; Scenario analysis; Shared memory multiprocessor; Cloning
On the Statistical Independence of Compound Pseudorandom Numbers Over Part of the Period,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347594748&doi=10.1145%2f502109.502113&partnerID=40&md5=4900bf3b32b66f7f8a29b965cd9c93ea,"This article deals with the compound methods with modulus m for generating uniform pseudorandom numbers, which have been introduced recently. Equidistribution and statistical independence properties of the generated sequences over part of the period are studied based on the discrepancy of d-tuples of successive pseudorandom numbers. It is shown that there exist parameters in compound methods such that the discrepancy over part of the period of the corresponding point sets in the d-dimensional unit cube is of an order magnitude of O(N-1/2(log N)d+3) for all N = 1,..., m. This result is applied to the compound nonlinear, inversive and explicit inversive congruential methods.",Diaphony; Discrepancy; Theoretical tests,Computer simulation; Heuristic methods; Inverse problems; Iterative methods; Linear systems; Mathematical models; Nonlinear systems; Parameter estimation; Statistical methods; Theorem proving; Pseudorandom numbers; Random number generation
Fast Simulation of Broadband Telecommunications Networks Carrying Long-Range Dependent Bursty Traffic,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347594753&doi=10.1145%2f502109.502112&partnerID=40&md5=768722fe221b2bd94d5b4db113dd05a4,"A technique for the fast simulation of broadband communication systems is proposed, which is based on regenerative Importance Sampling techniques and on large-deviation results. Our algorithm is applicable to estimate the probability of rare events when modeling the offered traffic using Fractional Stable Noise (FSN) processes (including Fractional Gaussian Noise as a particular case), which have been recently proved to be able to capture both the long-range dependence and the burstiness of today's aggregate network traffic. An exact description of FSN processes is given, as well as an approximation that allows for the application of Importance Sampling techniques. The results obtained for a simple example are also included.",Alpha-stable distributions; Importance Sampling; Network simulation; Self-similar traffic models; Variance reduction techniques,Algorithms; Computer networks; Computer simulation; Error analysis; Gaussian noise (electronic); Parameter estimation; Probabilistic logics; Statistical methods; Telecommunication traffic; Broadband communication systems; Network simulation; Broadband networks
Regenerative Steady-State Simulation of Discrete-Event Systems,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009890377&doi=10.1145%2f508366.508367&partnerID=40&md5=3daa5caf7f7f05602cbda39f7e42ceab,"The regenerative method possesses certain asymptotic properties that dominate those of other steady-state simulation output analysis methods, such as batch means. Therefore, applying the regenerative method to steady-state discrete-event system simulations is of great interest. In this paper, we survey the state of the art in this area. The main difficulty in applying the regenerative method in our context is perhaps in identifying regenerative cycle boundaries. We examine this issue through the use of the “smoothness index.” Regenerative cycles are easily identified in systems with unit smoothness index, but this is typically not the case for systems with nonunit smoothness index.We show that “most” (in a certain precise sense) discrete-event simulations will have nonunit smoothness index, and extend the asymptotic theory of regenerative simulation estimators to this context. © 2001, ACM. All rights reserved.",Theory,Computer applications; Computer simulation; Asymptotic properties; Asymptotic theories; Discrete event system simulations; Regenerative cycle; Smoothness indices; State of the art; Steady-state simulations; Theory; Discrete event simulation
Dynamic Structures in Modeling and Simulation: A Reflective Approach,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012330873&doi=10.1145%2f384169.384173&partnerID=40&md5=2bf9bbcfe1bb31fc895bee382ae2bc2c,"As the number of flexible, adaptable systems grows so does the need for specification and analysis tools that support adaptable system structures. The increasing number of simulation tools that equip models with the capability of changing their behavior patterns, composition, and interactions raises the desire for a theoretical and methodological approach. A formalism is introduced based on DEVS which emphasizes the reflective nature of variable structure models. The proposed formalism and DEVS are shown to be bisimilar, which emphasizes the role of variable structure models as an agency of modularization. The formalism is used to reveal general problems and solutions in implementing variable structure models.",Bisimulation; DEVS; Modeling and simulation formalism; Reflection; Variable structure models,Computer networks; Computer simulation; Control equipment; Input output programs; Large scale systems; Problem solving; State space methods; Bisimulation; Variable structure models; Computer aided software engineering
A Hybrid Technique for Accelerated Simulation of ATM Networks and Network Elements,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344810630&doi=10.1145%2f384169.384172&partnerID=40&md5=876c72f43ceb119c1ccd966f2e0df67e,"Conventional simulation of cell- or packet-switched networks involves the use of discrete event simulators that model each individual cell through the network, typically called cell-level simulation. Each cell's arrival at, or departure from, a network element is represented by an event. However, statistical considerations are such that very large numbers of cells have to be simulated to guarantee the accuracy of the results. This has always caused very long simulation times, often amounting to many hours of ""real"" time just to simulate a few minutes of ""simulated"" time. In this article we describe a novel methodology for accelerating simulation studies in cell-based communication networks, e.g., ATM, by using a hybrid analytical/ simulation combination. The methodology uses a mathematical technique to seperate foreground traffic from background traffic, and focuses on accelerating cell by cell simulation.",Accelerated simulation; Modeling; Queueing analysis; WANs,Asynchronous transfer mode; Computer simulation; Mathematical models; Packet networks; Queueing networks; Statistical methods; Switching; Telecommunication traffic; Wide area networks; Accelerated simulation; Discrete event simulators; Telecommunication networks
Achieving Per-Flow Fair Rate Allocation In Diffserv,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346333862&doi=10.1145%2f384169.384171&partnerID=40&md5=d95ee30a0b206435c487cb454da14719,"This article addresses the fundamental issue of providing per-flow fairness. In particular, it focuses on fairness within the Diffserv framework. We propose the Fair Allocation Derivative Estimation (FADE) algorithm for estimating flow fair share in the absence of per-flow information. FADE calculates fair share feedback using a modified quasi-Newton method This efficient method for estimating fair share provides a more precise model than other existing fairness estimation approaches. As such, it is able to more accurately estimate fair share and quickly converge to the proper rate. The simulation compares FADE to other proposals. The results demonstrate the overall effectiveness of the algorithm.",Differentiated services; Fairness; Feedback control,Algorithms; Computer simulation; Distributed computer systems; Estimation; Feedback control; Internet; Telecommunication traffic; Differentiated services; Per-flow fairness; Quasi-Newton method; Telecommunication services
FASTSLIM: Prefetch-Safe Trace Reduction for I/O Cache Simulation,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347594749&doi=10.1145%2f384169.384170&partnerID=40&md5=cdaf320a36a375561ffce5a8a8dc4d3a,"Trace-driven simulation is a valuable tool for evaluating I/O systems. This article presents a new algorithm, called FASTSLIM, that reduces the size of I/O traces and improves simulation performance without compromising simulation accuracy. FASTSLIM is more general than existing trace reduction algorithms in two ways. First, it is prefetch-safe: traces reduced by FASTSLIM yield provably exact simulations of I/O systems that use prefetching, a key technique for improving I/O performance. Second, FASTSLIM is compatible with a wide range of replacement policies, including common practical approximations to LRU. FASTSLIM-reduced traces are safe for simulations of storage hierarchies and systems with parallel disks. This article gives a formal treatment of prefetching and replacement issues for trace reduction, introduces the FASTSLIM algorithm, proves that FASTSLIM and variants are safe for a broad range of I/O caching and prefetching systems, and presents empirical results comparing FASTSLIM to competing trace reduction algorithms. © 2001, ACM. All rights reserved.",File caching; I/O architectures; operating systems; Performance; performance evaluation; prefetching; trace reduction; trace-driven simulation; virtual memory,Algorithms; Approximation theory; Bandwidth; Computer architecture; Computer simulation; Data storage equipment; Information retrieval; Input output programs; Reliability; File caching; Trace reduction; Trace-driven simulation; Virtual memory; Cache memory
Estimating Small Cell-Loss Ratios in ATM Switches via Importance Sampling,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012260732&doi=10.1145%2f379525.379528&partnerID=40&md5=5bd983a6cb2a616250182c8f4eaf0b2c,"The cell-loss ratio at a given node in an ATM switch, defined as the steady-state fraction of packets of information that are lost at that node due to buffer overflow, is typically a very small quantity that is hard to estimate by simulation. Cell losses are rare events, and importance sampling is sometimes the appropriate tool in this situation. However, finding the right change of measure is generally difficult. In this article, importance sampling is applied to estimate the cell-loss ratio in an ATM switch modeled as a queuing network that is fed by several sources emitting cells according to a Markov-modulated ON/OFF process, and in which all the cells from the same source have the same destination. The change of measure is obtained via an adaptation of a heuristic proposed by Chang et al. [1994] for intree networks. The numerical experiments confirm important efficiency improvements even for large nonintree networks and a large number of sources. Experiments with different variants of the importance sampling methodology are also reported, and a number of practical issues are illustrated and discussed.",ATM; Importance sampling; Rare events; Variance reduction,Buffer storage; Computer simulation; Heuristic methods; Least squares approximations; Markov processes; Packet networks; Parameter estimation; Probability; Queueing networks; Regression analysis; Statistical methods; Switching networks; Telecommunication traffic; Cell-loss ratios; Importance sampling; Rare events; Variance reduction; Asynchronous transfer mode
Comparing the QoS of Internet Audio Mechanisms via Formal Methods,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011122743&doi=10.1145%2f379525.379526&partnerID=40&md5=3926f694825575f55fc553a25f071225,"We compute and compare the quality of service (QoS) of three soft real-time applications for audio transmission over the Internet. The main metric we want to capture is the average packet audio playout delay vs. the packet loss rate as perceived by users. Other metrics we take into account are the packet loss rate vs. the receiving buffer capacity, the lateness of discarded packets vs, average packet audio playout delay, and the waiting time in the receiver buffer for the played packets vs. the average packet audio playout delay. The study is conducted in the algebraic language EMPA, by way of formal descriptions of the three audio mechanisms. The mechanisms are analyzed via simulation using the software tool TwoTowers under various (experimentally obtained or randomly generated) traffic conditions. The stochastic process algebra EMPA is used because it compositionally supports system modeling, it allows functional properties of systems to be formally verified (unlike conventional simulators), and it represents generally distributed durations (which come into play in the three audio mechanisms). The comparison reveals that in general no one of the three mechanisms outperforms the other two, as their performance depends on the traffic conditions.",Case studies; Discrete event simulation; Internet audio mechanisms; Quality of service; Software tools; Stochastic process algebras,Algebra; Asynchronous transfer mode; Buffer storage; Computer simulation; Computer software; Formal languages; Markov processes; Network protocols; Packet networks; Quality of service; Real time systems; Semantics; Telecommunication services; Telecommunication traffic; Audio mechanisms; Audio playout; Buffer capacity; Discrete event simulation; Packet loss rates; Internet
Tables of 64-bit Mersenne Twisters,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007894557&doi=10.1145%2f369534.369540&partnerID=40&md5=5e7665b675b11d322596ba12cfd9f5eb,[No abstract available],64-bit; Finite fields; K -distribution; Linear recurrence; Mersenne Twister; Random rumber generation,Algorithms; Computer simulation; Integer programming; Matrix algebra; Polynomials; Vectors; Finite fields; Linear recurrence; Mersenne twisters; Random number generation
Nearly Optimal Importance Sampling for Monte Carlo Simulation of Loss Systems,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347593872&doi=10.1145%2f369534.369541&partnerID=40&md5=6c12dc86471d36eed6825bba7c60e43a,"In this paper we consider the problem of estimating blocking probabilities in the multiservice loss system via simulation, applying the static Monte Carlo method with importance sampling. Earlier approaches to this problem include the use of either a single exponentially twisted version of the steady state distribution of the system or a composite of individual exponentially twisted distributions. Here, a different approach is introduced, where the original estimation problem is first decomposed into independent simpler subproblems, each roughly corresponding to estimating the blocking probability contribution from a single link. Then two importance sampling distributions are presented, which very closely approximate the ideal importance sampling distribution for each subproblem. In both methods, the idea is to try to generate samples directly into the blocking state region. The difference between the methods is that the first method, the inverse convolution method, achieves this exactly, while the second one, using a fitted Gaussian distribution, only approximately. The inverse convolution algorithm, however, has a higher memory requirement. Finally, a dynamic control algorithm is given for optimally allocating the samples between different subproblems. The numerical results demonstrate that the variance reduction obtained with the methods, especially with the inverse convolution method, is truly remarkable, between 670 and 1,000,000 in the examples under consideration.",Importance sampling; Loss system; Monte Carlo methods; Simulation; Variance reduction,Computer simulation; Convolution; Monte Carlo methods; Parameter estimation; Sampling; Loss systems; Variance reduction; Broadband networks
Source-Oriented Topology Aggregation with Multiple QoS Parameters in Hierarchical Networks,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038126832&doi=10.1145%2f369534.369542&partnerID=40&md5=f539da5d46271e38db03a33204667fff,"In this paper, we investigate the problem of topology aggregation (TA) for scalable, QoS-based routing in hierarchical networks. TA is the process of summarizing the topological information of a subset of network elements. This summary is flooded throughout the network and used by various nodes to determine appropriate routes for connection requests. A key issue in the design of a TA scheme is the appropriate balance between compaction and the corresponding routing performance. The contributions of this paper are twofold. First, we introduce a source-oriented approach to TA, which provides better performance than existing approaches. The intuition behind this approach is that the advertised topology-state information is used by source nodes to determine tentative routes for connection requests. Accordingly, only information relevant to source nodes needs to be advertised. We integrate the source-oriented approach into three new TA schemes that provide different trade-offs between compaction and accuracy. Second, we extend our source-oriented approach to multi-QoS-based TA. A key issue here is the determination of appropriate values for the multiple QoS parameters associated with a logical link. Two new approaches to computing these values are introduced. Extensive simulations are used to evaluate the performance of our proposed schemes.",ATM networks; PNNl; QoS-based routing; Scalable routing; Topology aggregation,Asynchronous transfer mode; Computer simulation; Network protocols; Quality of service; Routers; Topology; Logical links; Scalable routing; Topology aggregation; Hierarchical systems
Supporting Large-Scale Distributed Simulation Using HLA,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344462458&doi=10.1145%2f361026.361034&partnerID=40&md5=b21c542f2ace050d09b97ce58d27655e,"This article describes the design of a Web-based environment to support large-scale distributed simulation using Java and IEEE standard P1516 high level architecture (HLA) framework and rules. Based on the run-time infrastructure (RTI) services within the HLA and Java application programmer's interfaces (APIs) of the RTI, the proposed HLA-based environment provides an architectural foundation to enhance interactivity, portability, and interoperability for Web-based simulations. In addition, the proposed architectural design not only provides a client/server mechanism for simulation on the Web, but also supports a distributed federation execution over the network. A 3-level control mechanism (3LCM) was implemented to HLA-based middleware federateServer, in order to adaptively maintain information consistency and minimize message traffic for distributing information among client hosts and the federateServers. A dynamic filtering strategy (DFS), associated with the data distribution management (DDM) in the HLA RTI, is proposed to minimize false positive updates and enhance the filtering efficiency of subscription regions within an HLA federation. To verify the feasibility of this prototype, a distributed discrete event simulation application in Java was developed and performance of the proposed modeling design and Java RMI's distributed object model presented. From the experimental results, we show that the proposed environment based on HLA using 3LCM and DFS is workable and practical for supporting a large-scale distributed simulation.",Data distribution management (DDM); Distributed interactive simulation (DIS); High level architecture (HLA); Modeling and simulation; Networked virtual environment; Run-time infrastructure (RTI),Computer simulation; Data transfer; Interfaces (computer); Java programming language; Middleware; World Wide Web; Data distribution management (DDM); Distributed interactive simulation (DIS); Run time infrastructure (RTI); Distributed computer systems
The Implementation of Temporal Intervals in Qualitative Simulation Graphs,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346962849&doi=10.1145%2f361026.361030&partnerID=40&md5=8b5b47c51313eb8db38a50e5c30323f4,In this paper we develop and implement a simulation modeling methodology that combines discrete event simulation with qualitative simulation. Our main reason for doing so is to extend the application of discrete event simulation to systems found in business for which precise quantitative information is lacking. The approach discussed in this paper is the implementation of temporal interval specifications in the discrete event model and the construction of a temporal interval clock for the qualitative simulation model.,Discrete event simulation; Event graphs; Qualitative simulation; Simulation modeling,Computer simulation; Data handling; Information management; Probability distributions; Discrete event simulation; Event graphs; Qualitative simulation; Graph theory
Simultaneous Events and Lookahead in Simulation Protocols,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348223672&doi=10.1145%2f361026.361032&partnerID=40&md5=24169a98c4362963422e2da9fe1efde7,"A discrete event simulation model may contain several events that have the same timestamp, referred to as simultaneous events. In general, the results of a simulation depend on the order in which simultaneous events are executed. Simulation languages and protocols use different, sometimes ad hoc, tie-breaking mechanisms to order simultaneous events. As a result, it may be impossible to reproduce the results of a simulation model across different simulators. This article presents a systematic analysis of the lookahead requirements for sequential and parallel simulation protocols, utilizing the process-oriented world view, with respect to their ability to execute models with simultaneous events in a deterministic order. In particular, the article shows that most protocols, including the global event list protocol and commonly used parallel conservative and optimistic protocols, require that the simulation model provide some form of lookahead guarantee to enforce deterministic ordering of simultaneous events. The article also shows that the lookahead requirements for many protocols can be weakened if the model allows simultaneous events to be processed in a nondeterministic order. Finally, the lookahead properties that must be satisfied by a model in order for its execution to make guaranteed progress are derived using various simulation protocols.",Conditional event; Conservative; Lookahead; Null message; Optimistic; Simulation protocols; Simultaneous events; Time ties; Time warp,Computer programming; Computer simulation; Data processing; Large scale systems; Concurrent programming; Distributed programming; Parallel programming; Simulation protocols; Computer simulation languages
Performance and Fluid Simulations of a Novel Shared Buffer Management System,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346333869&doi=10.1145%2f379525.379527&partnerID=40&md5=7f1fb61bbefbe57cd554c7056db77d15,"We consider a switching system that has multiple ports that share a common buffer, in which there is a FIFO logical queue for each port. Each port may support a large number of flows or connections, which are approximately homogeneous in their statistical characteristics, with common QoS requirements in cell loss and maximum delay. Heterogeneity may exist across ports. Our first contribution is a buffer management scheme based on Buffer Admission Control, which is integrated with Connection Admission Control at the switch. At the same time, this scheme is fair, efficient, and robust in sharing the buffer resources across ports. Our scheme is based on the resource-sharing technique of Virtual Partitioning. Our second major contribution is to advance the practice of discrete-event fluid simulations. Such simulations are approximations to cell-level simulations and offer orders of magnitude speed-up. A third contribution of the paper is the formulation and solution of a problem of optimal allocation of bandwidth and buffers to each port having specific delay bounds, in a lossless multiplexing framework. Finally, we report on extensive simulation results. The scheme is found to be effective, efficient, and robust.",Buffer management; Fluid simulations; Virtual partitioning,Algorithms; Approximation theory; Bandwidth; Buffer storage; Computer simulation; Data storage equipment; Multiplexing; Probability; Problem solving; Quality of service; Queueing networks; Random processes; Statistical methods; Virtual storage; Buffer management system; Buffer occupancy; Fluid simulations; Virtual partitioning; Switching systems
Optimizing Static Calendar Queues,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011218488&doi=10.1145%2f361026.361028&partnerID=40&md5=5ea77a4eee15624f9c21a3f80f41e62b,"The calendar queue is an important implementation of a priority queue that is particularly useful in discrete event simulators. We investigate the performance of the static calendar queue that maintains N active events. The main contribution of this article is to prove that, under reasonable assumptions and with the proper parameter settings, the calendar queue data structure will have constant (independent of N) expected time per event processed. A simple formula is derived to approximate the expected time per event. The formula can be used to set the parameters of the calendar queue to achieve optimal or near optimal performance. In addition, a technique is given to calibrate a specific calendar queue implementation so that the formula can be applied in a practical setting.",Algorithm analysis; Calendar queue; Data structures; Discrete event simulation; Markov chain; Optimization; Priority queue,Algorithms; Computer simulation; Markov processes; Optimization; Discrete event simulation; Static calendar queues; Data structures
The Distributed Mission Training Integrated Threat Environment System Architecture and Design,2001,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346333857&doi=10.1145%2f379525.379529&partnerID=40&md5=f96e685f8a297b877064f14613121cf3,"We describe the architecture, design, components, and functionality of the Distributed Mission Training Integrated Threat Environment (DMTITE) software. The DMTITE architecture and design support the development and run-time operation of computer-generated actors (CGAs) in distributed simulations. The architecture and design employ object-oriented techniques, component software, object frameworks, containerization, and rapid prototyping technologies. The DMTITE architecture and design consist of highly modular components where interdependences are well defined and minimized. DMTITE is an open architecture and open design, and most component and framework code is open source. The DMTITE architecture and design have been implemented (including all system components and frameworks) and currently support a number of types of computer-generated actors. The DMTITE architecture, design, and implementation are capable of supporting multiple reasoning, vehicle dynamics, skill level, and migration requirements for any type of CGA.",Advanced distributed simulation; Components; Computer-generated actors; Computer-generated forces; Distributed mission training; Distributed simulation; Distributed virtual environments; Frameworks; Open architecture; Synthetic environments,Computer architecture; Computer simulation; Computer software; Dynamic programming; Object oriented programming; Software prototyping; Switching systems; Systems analysis; Virtual reality; Advanced distributed simulation; Computer-generated actors; Computer-generated forces; Synthetic environment; Wargames; Distributed computer systems
Parallel Shared-Memory Simulator Performance for Large ATM Networks,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012355038&doi=10.1145%2f369534.369537&partnerID=40&md5=eec5d21ce434a5da932f86daae1e6f1c,"A performance comparison between an optimistic and a conservative parallel simulation kernel is presented. Performance of the parallel kernels is also compared to a central-event-list sequential kernel. A spectrum of ATM network and traffic scenarios representative of those used by ATM networking researchers are used for the comparison. Experiments are conducted with a cell-level ATM network simulator and an 18-processor SGI Power Challenge shared-memory multiprocessor. The results show the performance advantages of parallel simulation over sequential simulation for ATM networks. Speedups of 4-5 relative to a fast sequential kernel are achieved on 16 processors for several large irregular ATM benchmark scenarios and the optimistic kernel achieves 2 to 5 times speedup on all 7 benchmarks. However, the relative performance of the two parallel simulation kernels is dependent on the size of the ATM network, the number of traffic sources, and the traffic source types used in the simulation. For some benchmarks the best single point performance is provided by the conservative kernel even on a single processor. Unfortunately, the conservative kernel performance is susceptible to small changes in the modeling code and is outperformed by the optimistic kernel on 5 of the 7 benchmarks. The optimistic parallel simulation kernel thus provides more robust performance, but its speedup is limited by the overheads of its implementation, which make it approximately half the speed of the sequential kernel on one processor. These performance results represent the first comparative analysis of parallel simulation for a spectrum of realistic, irregular, low-granularity, communication network models.",ATM network modeling; Conservative synchronization; Optimistic synchronization; Parallel discrete event simulation; Time warp,Benchmarking; Computer architecture; Computer networks; Computer simulation; Network protocols; Parallel processing systems; Synchronization; Kernels; Shared memory; Asynchronous transfer mode
Computing Blocking Probabilities in Multiclass Wavelength Routing Networks,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003962263&doi=10.1145%2f364996.365001&partnerID=40&md5=da3329f469e9160c7a75c7859bcde46b,"We present an approximate analytical method to evaluate efficiently and accurately the call blocking probabilities in wavelength routing networks with multiple classes of calls. The model is fairly general and allows each source-destination pair to service calls of different classes, with each call occupying one wavelength per link. Our approximate analytical approach involves two steps. The arrival process of calls on some routes is first modified slightly to obtain an approximate multiclass network model. Next, all classes of calls on a particular route are aggregated to give an equivalent single-class model. Thus, path decomposition algorithms for single-class wavelength routing networks maybe readily extended to the multiclass case. This article is a first step towards understanding the issues arising in wavelength routing networks that serve multiple classes of customers.",Blocking Probabilities; Multiclass networks; Wavelength division multiplexing (WDM); Wavelength routing,Algorithms; Approximation theory; Bandwidth; Computer simulation; Optical communication; Optical fibers; Optoelectronic devices; Probability; Routers; Wide area networks; Multiclass networks; Source-destination pairs; Wavelength routing; Wavelength division multiplexing
Improving the Aircraft Design Process Using Web-Based Modeling and Simulation,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348163912&doi=10.1145%2f353735.353739&partnerID=40&md5=50204e75dbd64c8f8e394a48efafd6f6,"Designing and developing new aircraft systems is time-consuming and expensive. Computational simulation is a promising means for reducing design cycle times, but requires a flexible software environment capable of integrating advanced multidisciplinary and multifidelity analysis methods, dynamically managing data across heterogeneous computing platforms, and distributing computationally complex tasks. Web-based simulation, with its emphasis on collaborative composition of simulation models, distributed heterogeneous execution, and dynamic multimedia documentation, has the potential to meet these requirements. This paper outlines the current aircraft design process, highlighting its problems and complexities, and presents our vision of an aircraft design process using Web-based modeling and simulation.",Aircraft design; Java; Object-oriented; Web-based simulation,Computer simulation; Data transfer; Java programming language; Object oriented programming; World Wide Web; Dynamic multimedia documentation; Heterogeneous computing platforms; Product design
ACM Transactions on Modeling and Computer Simulation: Guest editorial,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346332973&partnerID=40&md5=c0a22064a22b529037f0bd778a469d9d,[No abstract available],,
On the Use of Self-Similar Processes in Network Simulation,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0008496361&doi=10.1145%2f364996.365004&partnerID=40&md5=bdbe25487330de2956326f5e579f5569,"Several recent traffic measurement studies have convincingly shown the presence of self-similarity in modern high-speed networks, involving a very important revolution in the stochastic modeling of traffic. Thus the use of self-similar processes has opened new problems and research fields in network performance analysis, mainly in simulation studies, where the efficient synthetic generation of sample paths (traces) corresponding to self-similar traffic is one of the main topics. In this article, we justify the selection of interarrival time instead of counting processes for modeling arrivals. Also, we discuss the advantages and drawbacks of the most important self-similar processes when applied to traffic modeling in simulation studies, proposing the use of models based in F-ARIMA, mainly due to their flexibility to capture both long- and short-range correlations. However, F-ARIMA processes have been little used in simulation studies, mainly because the synthetic generation methods available in the literature are very inefficient compared with those for FGN. In order to solve this problem, we propose a new method that can generate high-quality traces corresponding to a F-ARIMA(p, d, q) process. A comparison with existing methods shows that the new method is significantly more efficient, and even slightly better than the best method for FGN.",Fractional ARIMA; Long-range dependence; Self-similar processes; Synthetic generation,Computer networks; Computer simulation; Markov processes; Multimedia systems; Telecommunication networks; Long-range dependence (LRD); Self-similar processes; Synthetic generation; Telecommunication traffic
Guest Editorial: Special Issue on Modeling and Simulation of Communication Networks,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346962851&partnerID=40&md5=c8627156ae72cb34f47bfec28bba3f7a,[No abstract available],,
Web-Based Network Analysis and Design,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346962836&doi=10.1145%2f353735.353737&partnerID=40&md5=0178de796d23ea1898d38c26b3af8d34,"The gradual acceptance of high-performance networks as a fundamental component of today's computing environment has allowed applications to evolve from static entities located on specific hosts to dynamic, distributed entities that are resident on one or more hosts. In addition, vital components of software and data used by an application may be distributed across the local/wide area network. Given such a fluid and dynamic environment, the design and analysis of high-performance communication networks (using off-the-shelf components offered by third party manufacturers) has been further complicated by the diversity of the available components. To alleviate these problems and to address the verification and validation issues involved in engineering such complex networks, a web-based framework for the design and analysis of computer networks was developed. Using the framework, a designer can explore design alternatives by constructing and analyzing configurations of the design using components offered by different researchers and manufacturers. The framework provides a flexible and robust environment for selecting and verifying the optimal solution from a large and complex solution space. This paper presents issues involved in the design and development of the framework.",Computer networks; Formal specification and verification; Parallel discrete event simulation; Web-based design and analysis,Computer simulation; Computer workstations; Data transfer; Rapid prototyping; Software engineering; World Wide Web; Distributed simulation; Parallel discrete event simulation; Computer networks
On the Equivalent Bandwidth of Self-Similar Sources,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000628475&doi=10.1145%2f364996.365003&partnerID=40&md5=13ff05c6194515438f57f1a416b55898,[No abstract available],Envelope process; Equivalent bandwidth; Leaky bucket; Policing; Self-similar traffic; Statistical multiplexing,Bandwidth; Brownian movement; Computer simulation; Fractals; Local area networks; Multiplexing; Probability; Queueing theory; Wide area networks; Long-range dependence (LRD); Policing mechanism; Self-similar traffic; Telecommunication traffic
Performance Evaluation of Multiple Time Scale TCP Under Self-Similar Traffic Conditions,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0342284170&doi=10.1145%2f364996.365005&partnerID=40&md5=b37a395e9bf1c221350edda3e7358891,[No abstract available],Congestion control; Multiple time scale; Network protocols; Performance evaluation; Self-similar traffic; Simulation; TCP,Algorithms; Bandwidth; Broadband networks; Computer simulation; Congestion control (communication); Feedback control; Local area networks; Network protocols; Wide area networks; Default values; Multiple time scale; Self-similar traffic; Telecommunication traffic
Web-Based Simulation: Revolution or Evolution?,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002154924&doi=10.1145%2f353735.353736&partnerID=40&md5=7a85e7008d2f38be9e27dd1d5d37e554,"The nature of the emerging field of web-based simulation is examined in terms of its relationship to the fundamental aspects of simulation research and practice. The presentation, assuming a form of debate, is based on a panel session held at the first International Conference on Web-Based Modeling and Simulation, which was sponsored by the Society for Computer Simulation during 11-14 January 1998 in San Diego, California. While no clear ""winner"" is evident in this debate, the issues raised here certainly merit ongoing attention and contemplation.",Digital objects; Distributed modeling; Java,Computer simulation; Data processing; Distributed computer systems; Java programming language; Digital objects; World Wide Web
The Purdue University Network-Computing Hubs: Running Unmodified Simulation Tools via the WWW,2000,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0004042694&doi=10.1145%2f353735.353738&partnerID=40&md5=5fe2e644e41d8c8acd6e5f0ee550de7c,"This paper describes the Web interface management infrastructure of a functioning network-computing system (PUNCH) that allows users to run unmodified simulation packages at geographically dispersed sites. The system currently contains more than fifty university and commercial simulation tools, and has been used to carry out more than two hundred thousand simulations via the World Wide Web. Dynamically-constructed virtual URLs allow the Web interface management infrastructure to support the semantics associated with an interface to computing services without requiring any changes to Web browsers or WWW protocols. Virtual URLs also facilitate customizable control of access to networked resources. Simulation tools with text-based interfaces are supported via dynamically-generated, virtual interfaces, whereas tools with graphical interfaces are supported by leveraging available remote display-management technologies. Virtual interface generation and interactivity emulation are handled by a programmable state machine in conjunction with a mechanism to embed variables and objects within standard HTML.",Internet computing; Network-computing; Web-based simulation,Computer simulation; Data transfer; Distributed computer systems; Interfaces (computer); Internet computing; Network computing; World Wide Web
Automating Parallel Simulation Using Parallel Time Streams,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348223638&doi=10.1145%2f333296.333359&partnerID=40&md5=abd249dc3b23ed2bafa64256afcb2754,"This paper describes a package for parallel steady-state stochastic simulation that was designed to overcome problems caused by long simulation times experienced in our ongoing research in performance evaluation of high-speed and integrated-services communication networks, while maintaining basic statistical rigors of proper analysis of simulation output data. The package, named AKAROA, accepts ordinary (nonparallel) simulation programs, and all further stages of stochastic simulation should be transparent for users. The package employs a new method of sequential estimation for the Multiple-Replications-in-Parallel scenario. All basic functions, including the transformation of originally nonparallel simulators into ones suitable for parallel execution, control of the precision of estimates, and stopping of parallel simulation processes when the required precision of the overall steady-state estimates is achieved, are automated. The package can be used on multiprocessor systems and/or heterogeneous computer networks, involving an arbitrary number of processors. The design issues, architecture, and implementation of AKAROA, as well as the results of its preliminary performance studies are presented.",Distributed simulation; Interprocess commmunication; Output analysis methodology; Parallel processing; Parallel simulation; Parallel time streams; Spectral analysis; Speedup,Computer simulation; Estimation; Parallel processing systems; Program processors; Random processes; Statistical methods; Parallel simulation; Parallel time streams; Telecommunication networks
The Theory of Direct Probability Redistribution and its Application to Rare Event Simulation,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0008132741&doi=10.1145%2f333296.333349&partnerID=40&md5=7f6576b00be7374d05fec7907cde05b5,"Rare event simulation is an important area of simulation theory, producing algorithms that can significantly reduce the simulation time when analyzing problems that involve rare events. However, existing rare event simulation techniques are rather restrictive, i.e., applicable only to systems with modest complexity. In this paper, we first develop a Markov chain transformation theory that can redistribute steady-state probabilities in a finite-size discrete-time Markov chain in an arbitrary and controlled manner. We descriptively name the theoretical procedure ""direct probability redistribution"" (DPR). In the second part of the paper, we develop DPR theory into a simulation algorithm that uses trajectory splitting to realize the DPR effect without knowledge of the transition probability matrix, thus allowing for easy application to systems with realistic complexity. DPR-based splitting can significantly reduce the simulation time by increasing the visiting frequency of rare states, and it avoids the problems associated with the decreasing likelihood ratio, which can be a limitation in conventional Importance Sampling techniques. The main advantage of the DPR-based simulation technique over existing splitting techniques is that DPR does not impose any restrictions on the state transitions and it provides asymptotically unbiased estimates even if the rare event set overlaps many splitting partitions. We conclude by providing examples where DPR-based simulation has been successfully applied to nontrivial queuing problems, including a system with flow control.",Accelerated simulation; Discrete event simulation; DPR; Importance sampling; Markov chains; Queuing; Rare event simulation; RESTART; Splitting,Algorithms; Computational complexity; Estimation; Markov processes; Matrix algebra; Probability; Problem solving; Simulation theory; Splitting partitions; Queueing networks
Getting Rid of Correlations Among Pseudorandom Numbers: Discarding Versus Tempering,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011866222&doi=10.1145%2f347823.347835&partnerID=40&md5=387493d14dac572450cffec7774d69d2,[No abstract available],Correlation analysis; Discarding; Empirical tests; Gambling test; Pseudorandom number generator; Tempering,Algorithms; Computer simulation; Correlation methods; Digital arithmetic; Number theory; Random walk generation; Random number generation
An empirical evaluation of several methods to select the best system,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012353958&doi=10.1145%2f352222.352226&partnerID=40&md5=b1439563a548b230381040bca4687a71,"Simulation is an important tool for comparing the performance of several alternative systems. There is therefore significant interest in procedures that efficiently select the best system, where best is defined by the maximum or minimum expected simulation output. In this paper, we examine both two-stage and sequential procedures that represent three structurally different modeling methodologies for allocating simulation replications to identify the best system, and we evaluate them empirically with respect to several measures of effectiveness. Empirical evidence suggests that sequential procedures perform better than their two-stage counterparts, including a heuristic sequential variation on Rinott's procedure. Further, there appears to be significant benefit to using procedures based on a Bayesian, average-case analysis as opposed to the statistically-conservative indifference-zone formulation. © 2000 ACM.",Discrete-event simulation; Multiple selection procedures; Ranking and selection,Computer applications; Computer simulation; Average-case analysis; Empirical evaluations; Measures of effectiveness; Modeling methodology; Multiple selection; Ranking and selection; Sequential procedures; Simulation replication; Discrete event simulation
Bootstrap confidence intervals for ratios of expectations,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11144325940&doi=10.1145%2f352222.352224&partnerID=40&md5=c10a84216570fadf3d7bfa1de5d16710,"We are concerned with computing a confidence interval for the ratio E[Y]/E[X], where (X, Y) is a pair of random variables. This ratio estimation problem arises in, for instance, regenerative simulation. As an alternative to confidence intervals based on asymptotic normality, we study and compare different variants of the bootstrap for one-sided and two-sided intervals. We point out situations where these techniques provide confidence intervals with coverage much closer to the nominal value than do the classical methods. © 2000 ACM.",Bootstrap; Confidence intervals; Ratio estimation problem; Regenerative simulation,Financial data processing; Asymptotic normality; Bootstrap; Bootstrap confidence interval; Classical methods; Confidence interval; Nominal values; Ratio estimations; Regenerative simulation; Statistical methods
Confidence intervals using orthonormally weighted standardized time series,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17744364708&doi=10.1145%2f352222.352223&partnerID=40&md5=735693e21f3e85d41cec85d53d633e5a,"We extend the standardized time series area method for constructing confidence intervals for the mean of a stationary stochastic process. The proposed intervals are based on orthonormally weighted standardized time series area variance estimators. The underlying area estimators possess two important properties: they are first-order unbiased, and they are asymptotically independent of each other. These properties are largely the result of a careful choice of weighting functions, which we explicitly describe. The asymptotic independence of the area estimators yields more degrees of freedom than various predecessors; this, in turn, produces smaller mean and variance of the length of the resulting confidence intervals. We illustrate the efficacy of the new procedure via exact and Monte Carlo examples. We also provide suggestions for efficient implementation of the method. © 2000 ACM.",Confidence intervals; Orthonormal area estimator; Standardized time series; Steady-state output analysis; Stochastic simulation; Variance estimation,Degrees of freedom (mechanics); Random processes; Stochastic models; Stochastic systems; Time series; Confidence interval; Orthonormal; Output analysis; Standardized time; Stochastic simulations; Variance estimation; Time series analysis
OOPM/RT: A Multimodeling Methodology for Real-Time Simulation,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000645873&doi=10.1145%2f333296.333339&partnerID=40&md5=a6308e28f182f41204da054664044177,"When we build a model of real-time systems, we need ways of representing the knowledge about the system and also time requirements for simulating the model. Considering these different needs, our question is ""How can we determine the optimal model that simulates the system by a given deadline while still producing valid outputs at an acceptable level of detail?"" We have designed OOPM/RT (Object-Oriented Physical Modeler for Real-Time Simulation) methodology. The OOPM/RT framework has three phases: (1) Generation of multimodels in OOPM using both structural and behavioral abstraction techniques, (2) Generation of AT (Abstraction Tree) which organizes the multimodels based on the abstraction relationship to facilitate the optimal model selection process, and (3) Selection of the optimal model that guarantees to deliver simulation results by the given amount of time. A more-detailed model (low abstraction model) is selected when we have enough time to simulate, while a less-detailed model (high abstraction model) is selected when the deadline is immediate. The basic idea of selection is to trade structural information for a faster runtime while minimizing the loss of behavioral information. We propose two possible approaches for the selection: an integer-programming-based approach and a search-based approach. By systematically handling simulation deadlines while minimizing the modeler's interventions, OOPM/RT provides an efficient modeling environment for real-time systems.",Model abstraction; Model selection; Modeling methodology; Real-time simulation; Real-time systems,Abstracting; Computational methods; Computer simulation; Constraint theory; Integer programming; Knowledge engineering; Mathematical models; Optimal systems; Multimodels; Real-time simulation; Real time systems
Redundancy in Model Specifications for Discrete Event Simulation,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242430304&doi=10.1145%2f347823.347831&partnerID=40&md5=ff05d9af894fc27693c92b6328439344,"Although redundancy in model specification generally has negative connotations, we offer arguments for revising those convictions. Defining ""representational redundancy"" as the inclusion of any symbols not required to fulfill the study objectives, we cite several sources of redundancy, classified as accidental or intentional, that contribute positively to the model development tasks. Comparative benefits and detriments are discussed briefly. Focusing on the most interesting source of redundancy - that which is intentionally induced by a modeling methodology - we demonstrate that automated elimination of redundancy can actually improve model execution time. Using four models drawn from the literature that are easily understood, but which represent some differences in size and complexity, the direct execution of the graphical representations shows improvements over a base case ranging from 27.3 percent to 68.1 percent in execution time. Further, increasing improvement is realized with increasing model size and complexity. These results are encouraging because they suggest that modeling methodologies with automated model diagnosis can significantly reduce both execution and development time and cost.",Discrete event simulation; Model analysis; Model development environments; Uses of redundancy,Automation; Computer simulation; Mathematical models; Redundancy; Discrete event simulation; Systems analysis
Accelerating the convergence of random search methods for discrete stochastic optimization,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000450089&doi=10.1145%2f352222.352225&partnerID=40&md5=c7e0180b829df5adcf4eb4f8c4194efb,"We discuss the choice of the estimate of the optimal solution when random search methods are applied to solve discrete stochastic optimization problems. At the present time, such optimization methods usually estimate the optimal solution using either the feasible solution the method is currently exploring or the feasible solution visited most often so far by the method. We propose using all the observed objective function values generated as the random search method moves around the feasible region seeking an optimal solution to obtain increasingly more precise estimates of the objective function values at the different points in the feasible region. At any given time, the feasible solution that has the best estimated objective function value (the largest one for maximization problems; the smallest one for minimization problems) is used as the estimate of the optimal solution. We discuss the advantages of using this approach for estimating the optimal solution and present numerical results showing that modifying an existing random search method to use this approach for estimating the optimal solution appears to yield improved performance. We also present several rate of convergence results for random search methods using our approach for estimating the optimal solution. One of these random search methods is a new variant of the stochastic comparison method; in addition to specifying the rate of convergence of this method, we prove that it is guaranteed to converge almost surely to the set of global optimal solutions and present a result that demonstrates that this method is likely to perform well in practice. © 2000 ACM.",Accelerated convergence; Convergence rate; Discrete stochastic optimization; Estimating the optimal solution; Random search methods,Approximation theory; Numerical methods; Optimal systems; Stochastic systems; Accelerated convergence; Convergence rates; Discrete stochastic optimization; Optimal solutions; Random search method; Optimization
Efficient Optimistic Parallel Simulations Using Reverse Computation,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002614346&doi=10.1145%2f347823.347828&partnerID=40&md5=929f8d8172f7be4c36866d7ce160d5de,"In optimistic parallel simulations, state-saving techniques have traditionally been used to realize rollback. In this article, we propose reverse computation as an alternative approach, and compare its execution performance against that of state-saving. Using compiler techniques, we describe an approach to automatically generate reversible computations, and to optimize them to reap the performance benefits of reverse computation transparently. For certain fine-grain models, such as queuing network models, we show that reverse computation can yield significant improvement in execution speed coupled with significant reduction in memory utilization, as compared to traditional state-saving. On sample models using reverse computation, we observe as much as a six-fold improvement in execution speed over traditional state-saving.",Parallel discrete event simulation; Reverse computation; Rollback; State-saving,Computer programming languages; Computer simulation; Data storage equipment; Optimization; Parallel algorithms; Program compilers; Random number generation; Memory utilization; State saving; Computational methods
Real-Time Simulation of Dust Behavior Generated by a Fast Traveling Vehicle,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346962826&doi=10.1145%2f333296.333366&partnerID=40&md5=cc1e3677444b21856d2167e09bd6200b,"Simulation of physically realistic complex dust behavior is very useful in training, education, art, advertising, and entertainment. There are no published models for real-time simulation of dust behavior generated by a traveling vehicle. In this paper, we use particle systems, computational fluid dynamics, and behavioral simulation techniques to simulate dust behavior in real time. First, we analyze the forces and factors that affect dust generation and the behavior after dust particles are generated. Then, we construct physically-based empirical models to generate dust particles and control the behavior accordingly. We further simplify-the numerical calculations by dividing dust behavior into three stages, and establishing simplified particle system models for each stage. We employ motion blur, particle blending, texture mapping, and other computer graphics techniques to achieve the final results. Our contributions include constructing physiPhysically-basedcally-based empirical models to generate dust behavior and achieving simulation of the behavior in real time.",Computational Fluid Dynamics; Particle Systems; Physically-based Modeling; Real-time Simulation; Vehicle,Animation; Computational fluid dynamics; Education; Large scale systems; Personnel training; Surface properties; Textures; Vehicles; Dust behavior; Particle blending; Dust
Fast Simulation of Networks of Queues with Effective and Decoupling Bandwidths,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042793163&doi=10.1145%2f301677.301684&partnerID=40&md5=39fe4fc59d4f006a14d42862701a8e3a,"A significant difficulty when using Monte Carlo simulation for the performance analysis of communication networks is the long runtime required to obtain accurate statistical estimates. Under the proper conditions, importance sampling (IS) is a technique that can speed up simulations involving rare events in network (queueing) systems. Large speed-up factors in simulation runtime can be obtained with IS if the modification or bias of the underlying probability measures of certain random processes is carefully chosen. Fast simulation methods based on large deviation theory (LDT) have been successfully applied in many cases. In this paper, we set up an IS-based simulation of various elementary network topologies. These configurations are frequently encountered in broadband ATM-based network components such as switches and multiplexers. Our objective in this study is to obtain the optimal or near-optimal biasing parameter values of the arrival processes for the importance sampling simulation. For this purpose we appropriately apply a technique presented by Chang et al. for certain portions of the networks (intree) while we develop a new algorithm, inspired by the work of De Veciana et al. on decoupling bandwidths, for the non-intree portion of the network.",Asynchronous transfer mode; Fast simulation; Importance sampling; Rare events,Algorithms; Approximation theory; Asynchronous transfer mode; Bandwidth; Computer simulation; Electric network topology; Monte Carlo methods; Optimal systems; Random processes; Sampling; Importance sampling (IS); Large deviation theory (LDT); Telecommunication networks
Two-Stage Multiple-Comparison Procedures for Steady-State Simulations,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012334247&doi=10.1145%2f301677.301679&partnerID=40&md5=0c4c3222a42439f52b49a5f87b7c21c9,"Procedures for multiple comparisons with the best are investigated in the context of steady-state simulation, whereby a number k of different systems (stochastic processes) are compared based upon their (asymptotic) means μ i ( i = 1,2, . . ., k). The variances of these (asymptotically stationary) processes are assumed to be unknown and possibly unequal. We consider the problem of constructing simultaneous confidence intervals for μ j≠iμ j ( i = 1,2, . . ., k), which is known as multiple comparisons with the best (MCB). Our intervals are constrained to contain 0, and so are called constrained MCB intervals. In particular, two-stage procedures for construction of absolute- and relative-width confidence intervals are presented. Their validity is addressed by showing that the confidence intervals cover the parameters with probability of at least some user-specified threshold value, as the confidence intervals' width parameter shrinks to 0. The general assumption about the processes is that they satisfy a functional central limit theorem. The simulation output analysis procedures are based on the method of standardized time series (the batch means method is a special case). The techniques developed here extend to other multiple-comparison procedures such as unconstrained MCB, multiple comparisons with a control, and all-pairwise comparisons. Although simulation is the context in this paper, the results naturally apply to (asymptotically) stationary time series.",Multiple comparisons; Standardized time series; Steady-state output analysis; Stochastic simulation; Two-stage procedures,Asymptotic stability; Computer simulation; Computer systems; Estimation; Probability; Problem solving; Queueing theory; Set theory; Time series analysis; Standardized time series; Steady-state output analysis; Stochastic simulation; Two-stage procedures; Stochastic control systems
The Patchwork Rejection Technique for Sampling from Unimodal Distributions,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0009892704&doi=10.1145%2f301677.301685&partnerID=40&md5=19d6145ca6dd1955be4cc07919b63264,"We report on both theoretical developments and computational experience with the patchwork rejection technique in Zechner and Stadlober [1993] and Zechner [1997]. The basic approach is due to Minh [1988], who suggested a special sampling method for the gamma distribution. This method's general objective is to rearrange the area below the density or histogram f(x) in the body of the distribution by certain point reflections such that variates may be generated efficiently within a large center interval. This is carried out via uniform hat functions, combined with minorizing rectangles for immediate acceptance of one transformed uniform deviate. The remaining tails of f(x) are covered by exponential functions. Experiments show that patchwork rejection algorithms are in general faster than their competitors at the cost of higher set-up times.",Patchwork rejection; Random variate generation; Random variate transformations; Sampling techniques; Stochastic simulation; Unimodal distributions,Algorithms; Computer simulation; Computer software; Mathematical transformations; Probability; Random number generation; Random processes; Reliability; Sampling; Statistical methods; Patchwork rejection; Random variate generation; Random variate tranformations; Stochastic simulation; Unimodal distributions; Computational methods
Simulation in Exponential Families,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346962824&doi=10.1145%2f347823.347824&partnerID=40&md5=f37191ffaad768bcad1519dd3fe072ca,"An acceptance-rejection algorithm for the simulation of random variables in statistical exponential families is described. This algorithm does not require any prior knowledge of the family, except sufficient statistics and the value of the parameter. It allows simultaneous simulatation from many members of the exponential family. We present some bounds on computing time, as well as the main properties of the empirical measures of samples simulated by our methods (functional Glivenko-Cantelli and central limit theorems). This algorithm is applied in order to evaluate the distribution of M-estimators under composite alternatives; we also propose its use in Bayesian statistics in order to simulate from posterior distributions.",Bayesian statistics; Computing time; Empirical measure; Exponential families; M-estimators; Posterior distributions; Simulation,Algorithms; Computational complexity; Computer simulation; Laplace transforms; Probability distributions; Statistical methods; Bayesian statistics; Exponential families; Random processes
Performance and dependability evaluation of scalable massively parallel computer systems with conjoint simulation,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032267774&doi=10.1145%2f295251.295254&partnerID=40&md5=be425d1b467950c65bae6d3951243273,"Computer systems are becoming more and more a part of our daily life; business and industry rely on their service, and the health of human beings depends on their correct functioning. Computer systems used for critical tasks have to be carefully designed and tested during the early design stage, the prototype phase, and their operational life. Methods and tools are required to support and facilitate this vital task. In this article, we tackle the issue of system-level performance and dependability analysis of fault-tolerant scalable computer systems. A modeling methodology called `Conjoint Simulation' is presented, which is based on the partitioning of the system model and the combination of several modeling techniques. Object-oriented model construction and process-based simulation are applied for architecture and workload modeling, and timed Petri nets are the core modeling technique representing the failure scenarios and repair policies. Splitting the overall model and exploiting appropriate modeling techniques ease the development, maintenance, and extensibility of large-scale and complex simulation models. Furthermore, techniques are provided for hierarchical model construction, object-oriented workload modeling, and simulated error injection in order to perform combined performance and dependability analysis.",,Computer architecture; Computer simulation; Fault tolerant computer systems; Large scale systems; Mathematical models; Object oriented programming; Petri nets; Systems analysis; Conjoint simulation; Parallel processing systems
Hierarchical computer architecture design and simulation environment,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032278814&doi=10.1145%2f295251.295259&partnerID=40&md5=e7c28d5265680d96a7889a7689a3ca3c,"A hierarchical computer architecture design and simulation environment (HASE) has been developed at the University of Edinburgh. HASE allows rapid development and exploration of computer architectures at multiple levels of abstraction, encompassing both hardware and software. It has five modes of operation (Design, Model Validation, Build Simulation, Simulate System, and Experiment) which formalize the design cycle and allow a proper separation of concerns among the different phases of simulation activity. The software of HASE itself includes a project data storage facility, a discrete-event simulation engine, graphical display/editing mechanisms, a visualization mechanism, and tools for setting up experiments and gathering results. HASE has been used in a number of research and student projects and these exemplify many of the interesting features of HASE and their relation to designing, simulating, and evaluating scalable systems. They include the modeling of scalable implementations of the hierarchical PRAM model of parallel computation on a 2-D mesh, the evaluation of the performance of multiprocessor interconnection networks, and a model of the Stanford DASH architecture.",,Computer simulation; Computer software; Data storage equipment; Interconnection networks; Mathematical models; Parallel processing systems; Discrete-event simulation; Software package HASE; Computer architecture
"Design-time simulation of a large-scale, distributed object system",1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032258161&doi=10.1145%2f295251.295255&partnerID=40&md5=f05c14d7bbc16e186e2c91ed0784c10b,"We present a case study in using simulation at design time to predict the performance and scalability properties of a large-scale distributed object system. The system, called Consul, is a network management system designed to support hundreds of operators managing millions of network devices. It is essential that a system such as Consul be designed with performance and scalability in mind, but due to Consul's complexity and scale, it is hard to reason about performance and scalability using ad hoc techniques. We built a simulation of Consul's design to guide the design process by enabling performance and scalability analysis of various design alternatives. A major challenge in doing design-time simulation is that many parameters for the simulation are based on estimates rather than measurements. We developed analysis methods that derive conclusions that are valid in the presence of estimation errors. In this article, we describe our scalability analysis method for design simulations of distributed object systems. The main idea is to use relative and comparative reasoning to analyze design alternatives and compare transaction behaviors. We demonstrate the analysis approach by describing its application to Consul.",,Computer networks; Computer operating systems; Computer simulation; Large scale systems; Systems analysis; Distributed object systems; Scalability analysis; Distributed computer systems
Minimum cost adaptive synchronization: Experiments with the PARASOL system,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032272760&doi=10.1145%2f295251.295257&partnerID=40&md5=79188358c0cf0fd660dd410e4516c726,"We present a novel adaptive synchronization algorithm, called the minimum average cost (MAC) algorithm, in the context of the PARASOL parallel simulation system. PARASOL is a multithreaded system for parallel simulation on shared- and distributed-memory environments, designed to support domain-specific Simulation Object Libraries. The proposed MAC algorithm is based on minimizing the cost of synchronization delay and rollback at a process, whenever its simulation driver must decide whether to either proceed optimistically or to delay processing. In the former case the risk is rollback cost, in the event of a straggler's arrival. In the latter case the risk is unnecessary delay, in the event a latecomer is not a straggler. In addition to the MAC algorithm and an optimal delay computation model, we report on some early experiments comparing the performance of MAC-based adaptive synchronization to optimistic synchronization.",,Adaptive algorithms; Computer simulation; Distributed computer systems; Mathematical models; Synchronization; Adaptive synchronization; Minimum average cost (MAC) algorithm; Parallel processing systems
Parallel Streams of Linear Random Numbers in the Spectral Test,1999,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002068814&doi=10.1145%2f301677.301682&partnerID=40&md5=4e3f57b15e3edbf2cf5d8351cb464f92,"This paper reports analyses of subsequences of linear congruential pseudorandom numbers by means of the spectral test. Such subsequences occur in particular simulation setups or as methods to obtain parallel streams of pseudorandom numbers for parallel and distributed simulation. Especially in the latter case, two kinds of substreams are of special interest: lagged random numbers with step sizes k, and consecutive streams of random numbers of length l. We show how to analyze correlations within and between lagged subsequences with arbitrary step sizes k. Analyzing consecutive streams with the spectral test is related to the well-known long-range correlation analysis of linear congruential generators. Whereas the latter was carried out to show correlations between pairs of processors only, the spectral test provides a convenient method to study correlations between larger numbers of parallel streams as well.",Lattice structure; Leap-frog technique; Linear congruential generator; Long-range correlations; Parallel random numbers; Random number generation; Spectral test; Stochastic simulation,Computer simulation; Correlation methods; Mathematical transformations; Parallel algorithms; Parallel processing systems; Problem solving; Spectrum analysis; Stochastic control systems; Vectors; Lattice structures; Leap-frog techniques; Linear congruential generators; Long-range correlations; Parallel random numbers; Stochastic simulation; Random number generation
A New Approach Combining Simulation and Randomization for the Analysis of Large Continuous Time Markov Chains,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032036166&doi=10.1145%2f280265.280274&partnerID=40&md5=49a26155ba839517ae3f03c1f4c0c3bf,"A new analysis method for continuous time Markov chains is introduced. The method combines, in some sense, simulation and numerical techniques for the analysis of large Markov chains. The basis of the new method is the description of a continuous-time Markov chain as a set of communicating processes. The state of all or some of the processes is described by a state vector, including a probability distribution over the set of locally reachable states. Simulation is used to determine the event times and message types exchanged between processes, Local transitions are realized by vector matrix products describing the next state distribution from the current one, In this way, the state explosion problem of numerical analysis is avoided, but it is still possible to obtain more accurate results than with pure simulation. The approach is therefore especially useful for the analysis of quantities with small probabilities or for the analysis of rare events.",Markov chains; Numerical techniques; Randomization; Simulation,Computer simulation; Numerical analysis; Petri nets; Probability distributions; Vectors; Algorithms; Random number generation; Randomization; Markov chains; Numerical techniques; Markov processes
Automatic Modeling of File System Workloads Using Two-Level Arrival Processes,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000155912&doi=10.1145%2f290274.290317&partnerID=40&md5=b09522fdc1f79ab1c90fd88a677206af,"This article describes a method for analyzing, modeling, and simulating a two-level arrival-counting process. This method is particularly appropriate when the number of independent processes is large, as is the case in our motivating application which requires analyzing and representing computer file system trace data for activity on nearly 8,000 files. The method is also applicable to network trace data characterizing communication patterns between pairs of computers. We apply cluster analysis to separate the arrival process into groups or bursts of activity on a file. We then characterize the arrival process in terms of the time between bursts of activity on a file, the time between file events within bursts, and the number of events in a burst. Finally, we model these three components individually, then reassemble the results to produce a synthetic trace generator. In order to gauge the effectiveness of this method, we use synthetically generated (simulated) trace data produced in this way to drive a discrete-event simulation of a distributed replicated file system. We compare the results of the simulation driven by the synthetic trace with the same simulation driven by the original trace data, and conclude that the synthetic data capture the essential characteristics of the empirical trace.",Clustering; Data replication; File access patterns; File system; Input modeling; Replication; Synthetic traces; Trace driven simulation,Computer architecture; Computer networks; Computer simulation; Telecommunication traffic; Data replication; File access patterns; Synthetic traces; Data handling
Using Permutations in Regenerative Simulations to Reduce Variance,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032036146&doi=10.1145%2f280265.280273&partnerID=40&md5=0efe81335f5174d00b408e8a0c3f77ce,"We propose a new estimator for a large class of performance measures obtained from a regenerative simulation of a system having two distinct sequences of regeneration times. To construct our new estimator, we first generate a sample path of a fixed number of cycles based on one sequence of regeneration times, divide the path into segments based on the second sequence of regeneration times, permute the segments, and calculate the performance on the new path using the first sequence of regeneration times, We average over all possible permutations to construct the new estimator. This strictly reduces variance when the original estimator is not simply an additive functional of the sample path. To use the new estimator in practice, the extra computational effort is not large since all permutations do not actually have to be computed as we derive explicit formulas for our new estimators. We examine the small-sample behavior of our estimators. In particular, we prove that for any fixed number of cycles from the first regenerative sequence, our new estimator has smaller mean squared error than the standard estimator. We show explicitly that our method can be used to derive new estimators for the expected cumulative reward until a certain set of states is hit, and the time-average variance parameter of a regenerative simulation.",Efficiency improvement; Permutations; Regenerative simulation; Variance reduction,Problem solving; Stochastic control systems; Theorem proving; Algorithms; Efficiency; Estimation; Permutations; Regenerative simulation; Variance reduction; Computer simulation
Elastic Time,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032036142&doi=10.1145%2f280265.280267&partnerID=40&md5=30ff20e1a306719f7453ac0c9184857b,"We introduce a new class of synchronization protocols for parallel discrete event simulation, those based on near-perfect state information (NPSI). NPSI protocols are adaptive, dynamically controlling the rate at which processes constituting a parallel simulation proceed, with the goal of completing a simulation efficiently. We show by analysis that a class of adaptive protocols (that includes NPSI and several others) can both arbitrarily outperform and be arbitrarily outperformed by the Time Warp synchronization protocol. This mixed result both substantiates the promising results we and other adaptive protocol designers have observed, and cautions those who might assume that any adaptive protocol will always be better than any nonadaptive one. We establish in an experimental study that a particular NPSI protocol, the Elastic Time Algorithm, outperforms Time Warp, both temporally and spatially, on every workload tested. Although significant options remain with respect to the design of ETA, the work presented here establishes the class of NPSI protocols as a very promising approach.",Adaptive protocols; Aggressiveness; Near-perfect state information; Optimistic protocols; Risk,Adaptive systems; Algorithms; Computer simulation; Heuristic methods; Synchronization; Parallel processing systems; Adaptive protocols; Optimistic protocols; State information; Network protocols
Statistical Independence Properties of Inversive Pseudorandom Vectors over Parts of the Period,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032036141&doi=10.1145%2f280265.280271&partnerID=40&md5=818e7c3252078e316b7107523a2b7e67,This article deals with the inversive method for generating uniform pseudorandom vectors. Statistical independence properties of the generated pseudorandom vector sequences over parts of the period are considered based on the discrete discrepancy of corresponding point sets. An upper bound for the average value of these discrete discrepancies is established.,Average statistical independence properties; Discrete discrepancy; Exponential sums; Inversive method; Uniform pseudorandom vectors,Computer simulation; Random processes; Vectors; Algorithms; Probability; Statistics; Discrete discrepancy; Pseudorandom vectors; Inversive methods; Random number generation
A Rejection Technique for Sampling from Log-Concave Multivariate Distributions,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012717593&doi=10.1145%2f290274.290287&partnerID=40&md5=048ece1970b6fdd79104189ced071b81,"Different universal methods (also called automatic or black-box methods) have been suggested for sampling from univariate log-concave distributions. The description of a suitable universal generator for multivariate distributions in arbitrary dimensions has not been published up to now. The new algorithm is based on the method of transformed density rejection. To construct a hat function for the rejection algorithm the multivariate density is transformed by a proper transformation T into a concave function (in the case of log-concave density T(x) = log(x).) Then it is possible to construct a dominating function by taking the minimum of several tangent hyperplanes that are transformed back by T-1 into the original scale. The domains of different pieces of the hat function are polyhedra in the multivariate case. Although this method can be shown to work, it is too slow and complicated in higher dimensions. In this article we split the ℝn into simple cones. The hat function is constructed piecewise on each of the cones by tangent hyperplanes. The resulting function is no longer continuous and the rejection constant is bounded from below but the setup and the generation remains quite fast in higher dimensions; for example, n = 8. The article describes the details of how this main idea can be used to construct algorithm TDRMV that generates random tuples from a multivariate log-concave distribution with a computable density. Although the developed algorithm is not a real black box method it is adjustable for a large class of log-concave densities.",Multivariate log-concave distributions; Rejection method,Algorithms; Matrix algebra; Sampling; Students; Vectors; Multivariate distributions; Rejection method; Multivariable systems
A Denotational Semantics for a Process-Based Simulation Language,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000315140&doi=10.1145%2f290274.290303&partnerID=40&md5=08d167e4baa5807603f26a4c6fa1f83c,"In this article, we present semantic translations for the actions of μDemos, a process-based, discrete event simulation language. Our formal translation schema permits the automatic construction of a process algebraic representation of the underlying simulation model which can then be checked for freedom from deadlock and livelock, as well as system-specific safety and liveness properties. As simulation methodologies are increasingly being used to design and implement complex systems of interacting objects, the ability to perform such verifications is of increasing methodological importance. We also present a normal form for the syntactic construction of μDemos programs that allows for the direct comparison of such programs (two programs with the same normal form must execute in identical fashion), reduces model proof obligations by minimizing the number of language constructs, and permits an implementer to concentrate on the basic features of the language (since any program implementation that efficiently evaluates normal forms will be an efficient evaluator for the complete language).",Discrete event simulation; Process algebra; Semantics,Computer simulation; Large scale systems; Random processes; Semantics; Discrete event simulation; Process algebra; Computer programming languages
Towards a Polynomial-Time Randomized Algorithm for Closed Product-Form Networks,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346271690&doi=10.1145%2f290274.290277&partnerID=40&md5=f07c58b1542a0d257f3dfc6e97d812f0,"We present a Markov chain Monte Carlo method for class throughputs in closed multiclass product-form networks. The method is as follows. For a given network, we construct a ""regularized"" network with a highly simplified structure that has the same steady-state distribution. We then simulate the regularized network. The method has performed reasonably well across a broad range of problems. We give a heuristic explanation of this and prove that the regularized network ""mixes in polynomial time"" in some special cases.",Closed product-form multiclass networks; Markov chain Monte Carlo; Rapid mixing; Simulation,Algorithms; Computer simulation; Convolution; Heuristic methods; Markov processes; Monte Carlo methods; Polynomials; Queueing networks; Closed product-form multiclass networks; Rapid mixing; Steady-state distributions; Computer networks
Guest Editors' Introduction: Special Issue on Uniform Random Number Generation,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348162499&doi=10.1145%2f272991.275457&partnerID=40&md5=637edc588b0ca478bbfd4227ed08f2b9,[No abstract available],,
Mersenne Twister: A 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031599142&doi=10.1145%2f272991.272995&partnerID=40&md5=1c4d172a52d4f787f136ef159050b1f6,"A new algorithm called Mersenne Twister (MT) is proposed for generating uniform pseudorandom numbers. For a particular choice of parameters, the algorithm provides a super astronomical period of 219937 - 1 and 623-dimensional equidistribution up to 32-bit accuracy, while using a working area of only 624 words. This is a new variant of the previously proposed generators, TGFSR, modified so as to admit a Mersenne-prime period. The characteristic polynomial has many terms. The distribution up to v bits accuracy for 1 ≤ v ≤ 32 is also shown to be good. An algorithm is also given that checks the primitivity of the characteristic polynomial of MT with computational complexity O(p2) where p is the degree of the polynomial. We implemented this generator in portable C-code. It passed several stringent statistical tests, including diehard. Its speed is comparable to other modern generators. Its merits are due to the efficient algorithms that are unique to polynomial calculations over the two-element field.",Finite fields; GFSR; Incomplete array; Inversive-decimation method; k-distribution; M-sequences; Mersenne Primes; Mersenne Twister; MT19937; Multiple-recursive matrix method; Primitive polynomials; Random number generation; Tempering; TGFSR,Algorithms; Computational complexity; Computer simulation; Difference equations; Inverse problems; Polynomials; Random processes; Statistical methods; Inverse-decimal method; Pseudorandom number generators; Random number generation
Simple Cellular Automata as Pseudorandom m-Sequence Generators for Built-In Self-Test,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031599401&doi=10.1145%2f272991.273007&partnerID=40&md5=025d449caa3f4e4910df923fb2a81d7d,"We propose an extremely simple and explicit construction of cellular automata (CA) generating pseudorandom m-sequences, which consist of only one type of cells. This construction has advantages over the previous researches in the following points. (1) There is no need to search for primitive polynomials; a simple sufficient number-theoretic condition realizes maximal periodic CA with periods 2m - 1, m = 2, 3, 5, 89, 9689, 21701, 859433. (2) The configuration does not require hybrid constructions. This makes the implementation much easier. This is a modification of the Rule-90 by Wolfram. We list our CAs with maximal period, up to the size 300. We also discuss the controllability of the CA, randomness of the generated sequence, and a two-dimensional version.",Cellular automata; Finite fields; M-sequence; Pseudorandom number generation; VLSI,Algorithms; Computer simulation; Finite automata; Integer programming; Integrated circuit layout; Linear algebra; Microprocessor chips; Pattern recognition systems; Polynomials; Random processes; VLSI circuits; Cellular arrays; Self tuning control systems; Cellular automata (CA); Finite fields; Cellular automata; Pseudorandom number generation; Random number generation
Latin Supercube Sampling for Very High-Dimensional Simulations,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031599238&doi=10.1145%2f272991.273010&partnerID=40&md5=bc6285fd6cc8a3c0303a6959b5cc87cc,"This article introduces Latin supercube sampling (LSS) for very high-dimensional simulations such as arise in particle transport, finance, and queueing. LSS is developed as a combination of two widely used methods: Latin hypercube sampling (LHS) and quasi-Monte Carlo (QMC). In LSS, the input variables are grouped into subsets, and a lower-dimensional QMC method is used within each subset. The QMC points are presented in random order within subsets. QMC methods have been observed to lose effectiveness in high-dimensional problems. This article shows that LSS can extend the benefits of QMC to much higher dimensions, when one can make a good grouping of input variables. Some suggestions for grouping variables are given for the motivating examples. Even a poor grouping can still be expected to do as well as LHS. The article also extends LHS and LSS to infinite-dimensional problems. The paper includes a survey of QMC methods, randomized versions of them (RQMC), and previous methods for extending QMC to higher dimensions. Furthermore it shows that LSS applied with RQMC is more reliable than LSS with QMC.","(t, m, s)-nets; Integration; Latin hypercube; Lattice methods; Low discrepancy sequences; Orthogonal array sampling; Quasi-Monte Carlo",Algorithms; Computer simulation; Differential equations; Fourier transforms; Monte Carlo methods; Probability; Queueing networks; Random number generation; Random processes; Sampling; Low discrepancy sequences; Orthogonal array sampling; Latin supercube sampling; Orthogonal array sampling; Integration; Computer simulation
Bad Subsequences of Well-Known Linear Congruential Pseudorandom Number Generators,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031599234&doi=10.1145%2f272991.273009&partnerID=40&md5=e9a7fd6203da5d931e0b821e61bf2a7d,"We present a spectral test analysis of full-period subsequences with small step sizes generated by well-known linear congruential pseudorandom number generators. Subsequences may occur in certain simulation problems or as a method to get parallel streams of pseudorandom numbers. Applying the spectral test, it is possible to find bad subsequences with small step sizes for almost all linear pseudorandom number generators currently in use.",Lattice structure; Linear congruential generator; Parallel pseudorandom number generator; Random number generation; Spectral test; Stochastic simulation,Algorithms; Computer simulation; Correlation methods; Linear control systems; Probability; Robustness (control systems); Stochastic programming; Spectrum analysis; Lattice structure; Linear congruential generators (LCG); Stochastic simulation; Random number generation
The Weighted Spectral Test: Diaphony,1998,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031599101&doi=10.1145%2f272991.273008&partnerID=40&md5=02d822b6363520ca228dc1582e725e09,"In this article, we present a new approach to assessing uniform random number generators, the weighted spectral test, or diaphony. In contrast to the usual spectral test, the weighted spectral test is not limited to random number generators with a lattice structure. Its computational complexity is script O sign(s · N2) for any point set of cardinality N in the s-dimensional unit cube. As the main results of this article, we prove an analog of the classical inequality of Erdös-Turán-Koksma, present the necessary tools to transcribe known discrepancy bounds into bounds for diaphony, and provide bounds for the diaphony of multiplicative congruential pseudorandom numbers. The last section contains numerical results.",Correlation analysis; Discrepancy; Empirical tests; Pseudorandom number generators; Spectral test; Stochastic stimulation; Theoretical tests,Computational complexity; Computer simulation; Correlation methods; Fourier transforms; Polynomials; Random processes; VLSI circuits; Computational complexity; Correlation methods; Spectrum analysis; Diaphony; Weighted spectral test; Lattice structures; Stochastic simulation; Random number generation; Random number generation
Adaptive memory management and optimism control in time warp,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031108694&doi=10.1145%2f249204.249207&partnerID=40&md5=9f1670797916a66bf59ffe3f73e8d34c,"It is widely believed that the Time Warp protocol for parallel discrete event simulation is prone to two potential problems: an excessive amount of wasted, rolled back computation resulting from 'rollback thrashing' behaviors, and inefficient use of memory, leading to poor performance of virtual memory and/or multiprocessor cache systems. An adaptive mechanism is proposed based on the Cancelback memory management protocol for shared-memory multiprocessors that dynamically controls the amount of memory used in the simulation in order to maximize performance. The proposed mechanism is adaptive in the sense that it monitors the execution of the Time Warp program, and using simple models, automatically adjusts the amount of memory used to reduce Time Warp overheads (fossil collection, Cancelback, the amount of rolled back computation, etc.) to a manageable level. We describe an implementation of this mechanism on a shared memory, Kendall Square Research KSR-1, multiprocessor and demonstrate its effectiveness in automatically maximizing performance while minimizing memory utilization, for several synthetic and benchmark discrete event simulation applications. We also demonstrate the adaptive ability of the mechanism by showing that it 'tracks' the time-varying nature of a communication network simulation.",,Buffer storage; Computer systems; Data storage equipment; Parallel processing systems; Performance; Parallel discrete event simulation; Shared memory multiprocessor; Time warp; Computer simulation
Consistency maintenance in multiresolution simulations,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031186068&doi=10.1145%2f259207.259235&partnerID=40&md5=fb90d90c33a50c2bf3b0cef0ade64831,Simulations that run at multiple levels of resolution often encounter consistency problems because of insufficient correlation between the attributes at multiple levels of the same entity. Inconsistency may occur despite the existence of valid models at each resolution level. Cross-Resolution Modeling (CRM) attempts to build effective multiresolution simulations. The traditional approach to CRM - aggregation-disaggregation - causes chain disaggregation and puts an unacceptable burden on resources. We present four fundamental observations that would help guide future approaches to CRM. These observations form the basis of an approach we propose that involves the design of Multiple Resolution Entities (MREs). MREs are the foundation of a design that incorporates maintaining internal consistency. We also propose maintenance of core attributes as an approach to maintaining internal consistency within an MRE.,,Reliability; Cross resolution modeling (CRM); Multiple resolution entities (MRE); Multiresolution simulations; Computer simulation
Random variate generation for multivariate unimodal densities,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031249049&doi=10.1145%2f268403.268413&partnerID=40&md5=c5cfc1c54157e66657dae8aad71fff6b,"A probability density on a finite-dimensional Euclidean space is orthounimodal with a given mode if within each orthant (quadrant) defined by the mode, the density is a monotone function of each of its arguments individually. Up to a linear transformation, most of the commonly used random vectors possess orthounimodal densities. To generate a random vector from a given orthounimodal density, several general-purpose algorithms are presented; and an experimental performance evaluation illustrates the potential efficiency increases that can be achieved by these algorithms versus naive rejection.",,Algorithms; Mathematical models; Mathematical transformations; Probability; Vectors; Finite dimensional Euclidean space; Multivariate unimodal density; Nonparametric classes; Orthounimodal density; Random variate generation; Random number generation
Computing global virtual time in shared-memory multiprocessors,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031246983&doi=10.1145%2f268403.268404&partnerID=40&md5=5f52a0e77272cd653bd38b475da28d21,"Global virtual time (GVT) is used in the time-warp synchronization mechanism to perform irrevocable operations and to reclaim storage. GVT computation is much simpler in shared-memory multiprocessors than in message-passing systems because they normally guarantee that no two processors will observe a set of memory operations as occurring in different orders. Exploiting this fact, an efficient, asynchronous, shared-memory GVT algorithm is proposed and its correctness is proven. A new mechanism called on-the-fly fossil collection which enables efficient storage reclamation for simulations containing large numbers is also proposed. It can be used in time-warp systems executing on shared-memory or message-based machines.",,Algorithms; Computer architecture; Computer operating systems; Computer simulation; Data storage equipment; Global virtual time (GVT); Time warp synchronization mechanisms; Multiprocessing systems
Performance and reliability analysis of relevance filtering for scalable distributed interactive simulation,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031189635&doi=10.1145%2f259207.259209&partnerID=40&md5=edde74a565ea8028347140675bc93469,"Achieving the real-time linkage among multiple, geographically-distant, local area networks that support distributed interactive simulation (DIS) requires tremendous bandwidth and communication resources. Today, meeting the bandwidth and communication requirements of DIS is one of the major challenges facing the design and implementation of large scale DIS training exercises. In this article, we discuss the DIS scalability problem, briefly overview the major bandwidth reduction techniques currently being investigated and implemented in contemporary DIS systems, and present a detailed analysis on the performance and reliability of relevance filtering - a promising technique to improve the scalability of distributed simulation. The idea of relevance filtering is to analyze the semantic contents of the state update messages of a simulated entity (vehicle) and transmit only the ones found to be relevant to other entities. We present our entity-based model for relevance filtering and discuss the implementation of filtering-at-transmission and filtering-at-reception. We introduce the concept of filtering reliability and present different methods to eliminate or reduce filtering errors. Methods that can ensure complete filtering reliability while providing significant bandwidth reduction are developed. Performance evaluation results of relevance filtering and of the filtering reliability methods are presented. The insight gained from our work and the challenges still facing the design of large scale DIS training exercises are discussed.",,Algorithms; Bandwidth; Computational linguistics; Data communication systems; Distributed computer systems; Interactive computer systems; Large scale systems; Local area networks; Network protocols; Real time systems; Reliability; Resource allocation; Distributed interactive simulation (DIS); Relevance filtering; Computer simulation
Modeling formalisms for dynamic structure systems,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031251544&doi=10.1145%2f268403.268423&partnerID=40&md5=e927f74cf507f5bfa5267d4d8950e4b4,"We present a new concept for a system network to represent systems that are able to undergo structural change. Change in structure is defined in general terms, and includes the addition and deletion of systems and the modification of the relations among components. The structure of a system network is stored in the network executive. Any change in structure-related information is mapped into modifications in the network structure. Based on these concepts, we derive three new system specifications that provide a shorthand notation to specify classes of dynamic structure systems. These new formalisms are: dynamic structure discrete time system, dynamic structure differential equation specified systems, and dynamic structure discrete event system specification. We demonstrate that these formalisms are closed under coupling, making hierarchical model construction possible. Formalisms are described using set theoretic notation and general systems theory concepts.",,Computer simulation; Differential equations; Mathematical models; Dynamic structure systems; Discrete time control systems
Editorial,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024278598&doi=10.1145%2f268403.269944&partnerID=40&md5=162ac1e06f3ab19852f8295739cdd6f0,[No abstract available],,
Editorial,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024261413&doi=10.1145%2f244804.257875&partnerID=40&md5=f3924146977113b7b1f9b31d03663f14,[No abstract available],,
Guest Editorial: Simulation for Training: Foundations and Techniques,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000943417&doi=10.1145%2f259207.259208&partnerID=40&md5=b3a2eb9dd5c458e583540a652c8d96c9,[No abstract available],,
"Case study of verification, validation, and accreditation for advanced distributed simulation",1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031187350&doi=10.1145%2f259207.259375&partnerID=40&md5=f2bab03debeb40d49560051656365298,"The techniques and methodologies for verification and validation of software-based systems have arguably realized their greatest utility within the context of simulation. Advanced Distributed Simulation (ADS), a major initiative within the defense modeling and simulation community, presents a variety of challenges to the classical approaches. A case study of the development process and concomitant verification and validation activities for the Joint Training Confederation (JTC) is presented. The JTC is one of the largest current ADS efforts, and the primary application of the Aggregate Level Simulation Protocol. A dichotomy between classical verification and validation approaches and the requirements of a prototypical ADS environment is illustrated. Mechanisms and research directions to resolve these differences are briefly discussed.",,Distributed computer systems; Software engineering; Advanced distributed simulation (ADS); Aggregate level simulation protocol; Computer simulation
Variance reduction applied to product-form multiclass queuing networks,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031249243&doi=10.1145%2f268403.268419&partnerID=40&md5=b027dbfb88d819159f6ad3305faddcbc,"Performance of product-form multiclass queuing networks can be determined from normalization constants. For large models, the evaluation of these performance metrics is not possible because of the required amount of computer resources (either by using normalization constants or by using MVA approaches). Such large models can be evaluated with Monte Carlo summation and integration methods. This article proposes two cluster sampling Monte Carlo techniques to deal with such models. First, for a particular type of network, we propose a variance reduction technique based on antithetic variates. It leads to an improvement of Ross, Tsang and Wang's algorithm which is designed to analyze the same family of models. Second, for a more general class of models, we use a mixture of Monte Carlo and quasi-Monte Carlo methods to improve the estimate with respect to Monte Carlo alone.",,Algorithms; Computer simulation; Integration; Mathematical models; Monte Carlo methods; Probability; Antithetic variates; Cluster sampling method; Low discrepancy sequences; Product form networks; Summation method; Variance reduction technique; Computer networks
Inversive and linear congruential pseudorandom number generators in empirical tests,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031108712&doi=10.1145%2f249204.249208&partnerID=40&md5=a39dd3cc071a166f79669663a83fbb24,"We present results from a series of empirical tests of pseudorandom number generators. The tests cover a broad range of designs due to bit-oriented, efficient test statistics and a testing procedure in which we vary the sample size, dimension, and the statistics' resolution within vast bounds. Inversive generation methods pass the tests for a broader range of test parameters than linear generators with equal period length. The results exemplify how the lattice structure of linear generators can affect a stochastic simulation and suggest the use of inversive generators for cross-checking the results.",,Probability; Random number generation; Random processes; Statistical methods; Empirical test; Pseudorandom number generator; Stochastic simulation; Computer simulation
Instability and performance limits of distributed simulators of feedforward queueing networks,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031109402&doi=10.1145%2f249204.249206&partnerID=40&md5=ab3709201b1664d941f92c878eecacb5,"In this article we study the performance of distributed simulation of open feedforward queueing networks, by analyzing queueing models of message flows in distributed discrete event simulators. We view each logical process in a distributed simulation as comprising a message sequencer with associated message queues, followed by an event processor. We introduce the idealized, but analytically useful, concept of maximum lookahead. We show that, with quite general stochastic assumptions for message arrival and time-stamp processes, the message queues are unstable for conservative sequencing, and for conservative sequencing with maximum lookahead and hence for optimistic resequencing, and for any resequencing algorithm that does not employ interprocessor flow control. Finally, we provide formulas for the throughput of distributed simulators of feedforward queueing networks.",,Algorithms; Distributed computer systems; Models; Performance; Stability; Distributed discrete event simulation; Feedforward queueing network; Message arrival; Time stamp process; Computer simulation
Comparative study of parallel and sequential priority queue algorithms,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031108708&doi=10.1145%2f249204.249205&partnerID=40&md5=a971652cedabb2e26ea788184a176410,"Priority queues are used in many applications including real-time systems, operating systems, and simulations. Their implementation may have a profound effect on the performance of such applications. In this article, we study the performance of well-known sequential priority queue implementations and the recently proposed parallel access priority queues. To accurately assess the performance of a priority queue, the performance measurement methodology must be appropriate. We use the Classic Hold, the Markov Model, and an Up/Down access pattern to measure performance and look at both the average access time and the worst-case time that are of vital interest to real-time applications. Our results suggest that the best choice for priority queue algorithms depends heavily on the application. For queue sizes smaller than 1,000 elements, the Splay Tree, the Skew Heap, and Henriksen's algorithm show good average access times. For large queue sizes of 5,000 elements or more, the Calendar Queue and the Lazy Queue offer good average access times but have very long worst-case access times. The Skew Heap and the Splay Tree exhibit the best worst-case access times. Among the parallel access priority queues tested, the Parallel Access Skew Heap provides the best performance on small shared memory multiprocessors.",,Algorithms; Data storage equipment; Markov processes; Parallel processing systems; Performance; Queueing theory; Real time systems; Parallel access priority queue; Sequential priority queue; Skew heap; Splay tree; Computer simulation
Terrain database interoperability issues in training with distributed interactive simulation,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031193012&doi=10.1145%2f259207.259221&partnerID=40&md5=0df43cbf584b069324bad73ccddc9f8f,"In Distributed Interactive Simulation (DIS), each participating node is responsible for maintaining its own model of the synthetic environment. Problems may arise if significant inconsistencies are allowed to exist between these separate world views, resulting in unrealistic simulation results or negative training, and a corresponding degradation of interoperability in a DIS simulation exercise. In the DIS community, this is known as the simulator terrain database (TDB) correlation problem. This is part of the larger synthetic environment correlation problem in DIS, which includes atmosphere, ocean, space, and a wide variety of dynamic effects, behaviors and models. In this article, we investigate the terrain database correlation problem and the resultant effects on interoperability in DIS systems. The fundamental elements of terrain databases designed for real-time distributed simulation are introduced. A generic data pipeline for terrain database generation systems is developed for the purpose of illustrating causes of the correlation problem and issues of terrain database fidelity. Implications of the problem are discussed, and testing methodologies are recommended for its mitigation. Several statistical methods have been developed to analyze consistency between various elements of the synthetic environment across DIS platforms. Correlation metrics have been formulated for terrain elevations and features. Comparisons and consistency of final rendered images have been addressed. Finally, a suite of software tools that has been developed for interoperability investigations and visual comparison of terrain databases is presented.",,Correlation methods; Distributed computer systems; Distributed database systems; Interactive computer systems; Real time systems; Distributed interactive simulation (DIS); Terrain database systems; Computer simulation
Reconfigurable hardware approach to network simulation,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030645215&doi=10.1145%2f244804.244809&partnerID=40&md5=09fe6a6801a1d6806b040cb541e96cb6,"Providing Quality-of-Service (QoS) guarantees in a broadband Asynchronous Transfer Mode (ATM) network places stringent demands on the network switches. Although analytical techniques may be used to bound the worst-case performance of traffic scheduling and congestion control algorithms, these are often inadequate for modeling the switch algorithms at the needed level of detail. Simulation is often the only alternative for evaluating the actual performance of the network. However, owing to the small size of the ATM cell and the high link-speeds, simulation of ATM switches and networks in a computationally demanding problem. To address this problem, we have developed a flexible hardware testbed for simulation of ATM-based networks. The testbed, called FAST (FPGA-based ATM simulation testbed), uses high-density field-programmable gate arrays (FPGAs) to allow implementation of the key algorithms in an ATM switch in hardware. The first version of the testbed (FAST-1) consists of a set of boards, each providing a total of 336,000 usable gates and 17 Mbytes of static RAM. Each board can be used to simulate an ATM switch, and multiple boards may be interconnected to simulate networks of ATM switches. Software tools have been developed for specifying the components of the switch model and algorithms, configuring the testbed for simulation, and monitoring the simulation process. This article provides an overview of the architecture of the FAST-1 board, describes its key components, and discusses some example simulation models of traffic scheduling algorithms using the board.",,Algorithms; Asynchronous transfer mode; Broadband networks; Computational complexity; Computer architecture; Computer software; Congestion control (communication); Switching networks; Telecommunication traffic; Field programmable gate array (FPGA); Computer simulation
Modeling cost/performance of a parallel computer simulator,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030712794&doi=10.1145%2f244804.244808&partnerID=40&md5=fbc57f6654bb981ac614c63be88f7bb7,"This article examines the cost/performance of simulating a hypothetical target parallel computer using a commercial host parallel computer. We address the question of whether parallel simulation is simply faster than sequential simulation, or if it is also more cost-effective. To answer this, we develop a performance model of the Wisconsin Wind Tunnel (WWT), a system that simulates cache-coherent shared-memory machines on a message-passing Thinking Machines CM-5. The performance model uses Kruskal and Weiss's fork-join model to account for the effect of event processing time variability on WWT's conservative fixed-window simulation algorithm. A generalization of Thiebaut and Stone's footprint model accurately predicts the effect of cache interference on the CM-5. The model is calibrated using parameters extracted from a fully parallel simulation (p = N), and validated by measuring the speedup as the number of processors (p) ranges from 1 to the number of target nodes (N). Together with simple cost models, the performance model indicates that for target system sizes of 32 nodes and larger, parallel simulation is more cost-effective than sequential simulation. The key intuition behind this result is that large simulations require large memories, which dominate the cost of a uniprocessor; parallel computers allow multiple processors to simultaneously access this large memory.",,Algorithms; Buffer storage; Computational complexity; Parallel processing systems; Cache coherent shared memory machines; Computer simulation
Active memory: A new abstraction for memory system simulation,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030717855&doi=10.1145%2f244804.244806&partnerID=40&md5=da758ee8b6a3d5944fee161009f47f9f,"This article describes the active memory abstraction for memory-system simulation. In this abstraction - designed specifically for on-the-fly simulation - memory references logically invoke a user-specified function depending upon the reference's type and accessed memory block state. Active memory allows simulator writers to specify the appropriate action on each reference, including `no action' for the common case of cache hits. Because the abstraction hides implementation details, implementations can be carefully tuned for particular platforms, permitting much more efficient on-the-fly simulation than the traditional trace-driven abstraction. Our SPARC implementation, Fast-Cache, executes simple data cache simulation 2 to 6 times slower than the original, uninstrumented program on a SPARCstation 10; a procedure call based trace-driven simulator is 7 to 16 times slower than the original program, and a trace-driven simulator that buffers references in memory to amortize procedure call overhead is 3 to 8 times slower. Fast-Cache implements active memory by performing a fast table look up of the memory block state, taking as few as 3 cycles on a SuperSPARC for the no-action case. Modeling the effects of Fast-Cache's additional lookup instructions qualitatively shows that Fast-Cache is likely to be the most efficient simulator for miss ratios between 3% and 40%.",,Buffer storage; Computer programming; Data storage equipment; Hierarchical systems; Active memory; Direct execution simulation; On the fly simulation; Trace driven simulation; Computer simulation
Using the SimOS machine simulator to study complex computer systems,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030653560&doi=10.1145%2f244804.244807&partnerID=40&md5=b7d40b58277d7621813989bfb8b22816,"SimOS is an environment for studying the hardware and software of computer systems. SimOS simulates the hardware of a computer system in enough detail to boot a commercial operating system and run realistic workloads on top of it. This paper identifies two challenges that machine simulators such as SimOS must overcome in order to effectively analyze large complex workloads: handling long workload execution times and collecting data effectively. To study long-running workloads, SimOS includes multiple interchangeable simulation models for each hardware component. By selecting the appropriate combination of simulation models, the user can explicitly control the tradeoff between simulation speed and simulation detail. To handle the large amount of low-level data generated by the hardware simulation models, SimOS contains flexible annotation and event classification mechanisms that map the data back to concepts meaningful to the user. SimOS has been extensively used to study new computer hardware designs, to analyze application performance, and to study operating systems. We include two case studies that demonstrate how a low-level machine simulator such as SimOS can be used to study large and complex workloads.",,Computer architecture; Computer hardware; Computer operating systems; Data acquisition; Large scale systems; Workload execution; Computer simulation
Trap-driven memory simulation with Tapeworm II,1997,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030646202&doi=10.1145%2f244804.244805&partnerID=40&md5=0b9fbb53f032c0b1a4b4e3138ce7388a,"Trap-driven simulation is a new approach for analyzing the performance of memory-system components such as caches and translation-lookaside buffers (TLBs). Unlike the more traditional trace-driven approach to simulating memory systems, trap-driven simulation uses the hardware of a host machine to drive simulations with operating-system kernel traps instead of with address traces. A prototype trap-driven simulator named Tapeworm II was developed to explore the strengths and the weaknesses of trap-driven simulation with respect to speed, accuracy, completeness, portability, flexibility, ease-of-use, and memory overhead.",,Buffer storage; Computer operating systems; Data storage equipment; Multiprogramming; Trap driven simulation; Computer simulation
Parallel execution for serial simulators,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030182734&doi=10.1145%2f235025.235031&partnerID=40&md5=a53be9a6483f70ee7946e27cfbcc6a7a,"This article describes an approach to discrete event simulation modeling that appears to be effective for developing portable and efficient parallel execution of models of large distributed systems and communication networks. In this approach, the modeler develops submodels with an existing sequential simulation modeling tool, using the full expressive power of the tool. A set of modeling language extensions permits automatically synchronized communication between submodels; however, the automation requires that any such communication must take a nonzero amount of simulation time. Within this modeling paradigm, a variety of conservative synchronization protocols can transparently support conservative execution of submodels on potentially different processors. A specific implementation of this approach, U.P.S. (Utilitarian Parallel Simulator), is described, along with performance results on the Intel Paragon and on the IBM SP2.",,Automation; Computer programming languages; Network protocols; Parallel algorithms; Parallel processing systems; Synchronization; Telecommunication networks; Discrete event simulation; Sequential simulation modeling tool; Utilitarian parallel simulator (UPS); Computer simulation
Radix-b extensions to some common empirical tests for pseudorandom number generators,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030261015&doi=10.1145%2f240896.240906&partnerID=40&md5=ce292a8841645d76aff6a12a535c5161,"Empirical testing of computer generated pseudo-random sequences is widely practiced. Extensions to the coupon collector's and gap tests are presented that examine the distribution and independence of radix-b digit patterns in sequences with modulo of the form bw. An algorithm is given and the test is applied to a number of popular generators. Theoretical expected values are derived for a number of defects that may be present in a pseudorandom sequence and additional empirical evidence is given to support these values. The test has a simple model and a known distribution function. It is easily and efficiently implemented and easily adaptable to testing only the bits of interest, given a certain application.",,Algorithms; Mathematical models; Monte Carlo methods; Probability; Random number generation; Sampling; Distribution function; Empirical test; Pseudorandom number; Computer simulation
State event location in differential-algebraic models,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030125594&doi=10.1145%2f232807.232809&partnerID=40&md5=3f35b8bd6e7668ea8e78bc573c8ea2a7,"An efficient discontinuity handling algorithm for initial value problems in differential-algebraic equations is presented. The algorithm supports flexible representation of state conditions in propositional logic, and guarantees the location of all state events in strict time order. The algorithm consists of two phases: (1) event detection and (2) consistent event location. In the event detection phase, the entire integration step is searched for the state event by solving the interpolation polynomials for the discontinuity functions generated by the BDF method. An efficient hierarchical polynomial root-finding procedure based upon interval arithmetic guarantees detection of the state event even if multiple state condition transitions exist in an integration step, in which case many existing algorithms may fail. As a second phase of the algorithm, a consistent event location calculation is developed that accurately locates the state event detected earlier while completely eliminating incorrect reactivation of the same state event immediately after the consistent initialization calculation that may follow. This numerical phenomenon has not been explained before and is termed discontinuity sticking. Results from various test problems are presented to demonstrate the correctness and efficiency of the algorithm.",,Algebra; Algorithms; Calculations; Computer simulation languages; Differential equations; Functions; Integration; Interpolation; Mathematical models; Nonlinear equations; Numerical analysis; Polynomials; Consistent event detection; Correctness; Discontinuity functions; Discontinuity sticking; Event detection; Initial value problems; Interval arithmetic; Multistop methods; Propositional logic; State event; Computer simulation
Strong deviations from randomness in m-sequences based on trinomials,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030125599&doi=10.1145%2f232807.232815&partnerID=40&md5=49ed1565321129f9ef325f92e25a0539,The fixed vector of any m-sequence based on a trinomial is explicitly obtained. Local nonrandomness around the fixed vector is analyzed through model-construction and experiments. We conclude that the initial vector near the fixed vector should be avoided.,,Algorithms; Computer simulation; Difference equations; Mathematical models; Monte Carlo methods; Polynomials; Random processes; Shift registers; Statistical methods; Vectors; Characteristic polynomial; Experimentation; Nonrandomness; Primitive trinomials; Pseudo random numbers; Standard deviation; Three term linear recursion; Trinomials; Random number generation
Combining antithetic variates and control variates in simulation experiments,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030259049&doi=10.1145%2f240896.240899&partnerID=40&md5=0edb30e359ce4a01aa79d0d9905595a6,"Antithetic variates and control variates are two well-known variance reduction techniques. We consider combining antithetic variates and control variates to estimate the mean response in a stochastic simulation experiment. When applying antithetic variates to generate control variates across paired replications, we show that the integrated control-variate estimator is unbiased and yields, under the assumption of common correlations induced for all control variates, a smaller variance than the conventional control-variate estimator without using antithetic variates. We examine the proposed estimator and two alternative integrated control-variate estimators when applying antithetic variates on control variates and show that the proposed estimator is the optimal integrated control-variate estimator. We implement these three integrated control-variate estimators and the conventional control-variate estimator in a simulation model of a stochastic network to evaluate the performance of each control-variate estimator. Empirical results show that the proposed estimator outperforms the other control-variate estimators.",,Estimation; Mathematical models; Random processes; Antithetic variate; Control variate; Discrete event simulation; Variance reduction; Computer simulation
Superfast parallel discrete event simulations,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030129025&doi=10.1145%2f232807.232818&partnerID=40&md5=35d67971ea7683ae271f07263b684d47,"Nonconventional parallel simulations methods are presented, wherein speed-ups are not limited by the number of simulated components. The methods capitalize on Chandy and Sherman's space-time relaxation paradigm, and incorporate fast algorithms for solving recurrences. Special attention is paid to implementing these algorithms on currently available massively parallel SIMD computers. As examples, 'superfast' simulations for open and closed queuing networks and for the slotted ALOHA communication protocol are discussed. Several of the simulations are implemented and show impressive computational speeds. This paper summarizes previous results of the authors and presents some new experiments and simulations.",,Algorithms; Calculations; Computer networks; Computer programming; Mathematical models; Monte Carlo methods; Network protocols; Parallel processing systems; Performance; Queueing theory; Fixed point computations; Massively parallel computations; Parallel simulation; Recurrences; Space time relaxation; Superlinear speedup; Unbounded parallelism; Computer simulation
Analysis of bounded time warp and comparison with YAWNS,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030264773&doi=10.1145%2f240896.240913&partnerID=40&md5=65d56c195c93db78eaf1a823bc26bc6e,"This article studies an analytic model of parallel discrete-event simulation, comparing the YAWNS conservative synchronization protocol with Bounded Time Warp. The assumed simulation problem is a heavily loaded queuing network where the probability of an idle server is close to zero. We model workload and job routing in standard ways, then develop and validate methods for computing approximated performance measures as a function of the degree of optimism allowed, overhead costs of state-saving, rollback, and barrier synchronization, and workload aggregation. We find that Bounded Time Warp is superior when the number of servers per physical processor is low (i.e., sparse load), but that aggregating workload improves YAWNS relative performance.",,Mathematical models; Performance; Probability; Queueing theory; Bounded time warp; Parallel simulation; Synchronization protocol; Computer simulation
Mean square discrepancy of randomized nets,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030259017&doi=10.1145%2f240896.240909&partnerID=40&md5=2211116fbe403ba60a99848038820997,"One popular family of low discrepancy sets is the (t, m, s)-nets. Recently a randomization of these nets that preserves their net property has been introduced. In this article a formula for the mean square L2-discrepancy of (0, m, s)-nets in base b is derived. This formula has a computational complexity of only O(s log(N) + s2) for large N or s, where N = bm is the number of points. Moreover, the root mean square L2-discrepancy of (0, m, s)-nets is shown to be O(N-1[log(N)](s-1)/2) as N tends to infinity, the same asymptotic order as the known lower bound for the L2-discrepancy of an arbitrary set.",,Integration; Monte Carlo methods; Probability; Random processes; Statistics; Mean square discrepancy; Multidimensional integration; Randomized net; Computer simulation
Rejection-inversion to generate variates from monotone discrete distributions,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030182550&doi=10.1145%2f235025.235029&partnerID=40&md5=6f93be2db43a8f920a1b1d16773aa9cf,"For discrete distributions a variant of rejection from a continuous hat function is presented. The main advantage of the new method, called rejection-inversion, is that no extra uniform random number to decide between acceptance and rejection is required, which means that the expected number of uniform variates required is halved. Using rejection-inversion and a squeeze, a simple universal method for a large class of monotone discrete distributions is developed. It can be used to generate variates from the tails of most standard discrete distributions. Rejection-inversion applied to the Zipf (or zeta) distribution results in algorithms that are short and simple and at least twice as fast as the fastest methods suggested in the literature.",,Algorithms; Probability; Statistical methods; Continuous hat function; Monotone discrete distributions; Rejection inversion method; Zeta distributions; Random number generation
Analysis of an efficient algorithm for the hard-sphere problem,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030182825&doi=10.1145%2f235025.235030&partnerID=40&md5=c39a3c53f45213ee516c4a3e4f8f9e8d,"Many similar algorithms for performing simulations of hard-sphere systems have been presented. Among these algorithms are those designed by Rapaport (RAP), Lubachevsky (LUB), Krantz (HAD), and Marin (HYBRID). These algorithms exhibit a similar design in that they each use an O(logn) event queue, which becomes the overwhelming bottleneck when simulating large systems. In this paper the design of HAD is presented and contrasted to RAP, LUB and HYBRID. Next, both an empirical and analytic analysis of HAD's performance are presented which show that HAD scales well. Finally, using the design differences of these algorithms, the performance of HAD is compared to RAP, LUB and HYBRID.",,Computer simulation; Molecular dynamics; Discrete event simulation; Hard sphere systems; Algorithms
Simulation run lengths to estimate blocking probabilities,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029719279&doi=10.1145%2f229493.229496&partnerID=40&md5=b4273b1140b3900f7148c9600c0cf2fe,"Formulas approximating the asymptotic variance of four estimators are derived for the steady-state blocking probability in a multisaver loss system, exploiting diffusion process limits. These formulas can be used to predict simulation run lengths required to obtain desired statistical precision before the simulation has been run, which can aid in the design of simulation experiments. They also indicate that one estimator can be much better than another, depending on the loading. These formulas are validated by comparing them to exact numerical results for the special case of the classical Erlang M/M/s/O model and simulation estimates from more general G/G/s/O model.",,Approximation theory; Computational methods; Diffusion; Parameter estimation; Probability; Statistical methods; Systems analysis; Arrival process; Asymptotic variance; Blocking probability; Multiserver system; Service time variability; Simulation run lengths; Statistical precision; Computer simulation
Knowledge-based approach for the validation of simulation models: the foundation,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029709004&doi=10.1145%2f229493.229511&partnerID=40&md5=d08838ae02c49650a6c522a90d68807e,"A new perspective of the validation problems for simulation models is formulated in this article. The approach is knowledge-based and focuses on behavioral validation. It has the important feature of providing a basis for the development of a software environment that can automate the validation activity. Discrete, continuous and combined simulation models can be treated in a uniform manner. The key element of the approach is a validation knowledge base (VKB). This is developed as three disjoint sets of relationships among the input and output variables of the simulation model. These relationships serve to capture all aspects of expected behavior of the simulation model. A simple characterization of model behavior is presented which provides the basis for specifying the relationships from which the VKB is constructed. The utilization of all the information in the VKB in an efficient way is an important subgoal of our system architecture. This requirement gives rise to an experiment design problem. This problem is carefully formulated and examined within the framework of the behavior characterizations that exist in the VKB. In particular, a basis for its solution is established in a constraint set framework by carrying out a transformation on the relationships within the VKB. The constraint set context for the problem has the advantage of providing an environment which not only facilitates analysis but also enables the application of a variety of solution techniques.",,Computer software; Constraint theory; Knowledge based systems; Mathematical models; Mathematical transformations; Systems analysis; Behavioral validation; Experiment design; Simulation models; Validation knowledge base systems; Computer simulation
Computational issues for accessibility in discrete event simulation,1996,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029706675&doi=10.1145%2f229493.229509&partnerID=40&md5=61f1ce543976721ee2c3d4db5ae8c71e,"Several simulation model building and analysis issues have been studied using a computational complexity approach. More specifically, four problems related to simulation model building and analysis (accessibility of states, ordering of events, noninterchangeability of model implementations, and execution stalling) have been shown to be NP-hard search problems. These results imply that it is unlikely that a polynomial-time algorithm can be devised to verify structural properties of discrete event simulation models, unless P = NP. This article considers the problem of accessibility, identifies special cases that are polynomially solvable or remain NP-hard, and discusses implications with respect to the other three problems. A local search procedure and variations of simulated annealing are presented to address accessibility. Computational results illustrate these heuristics and demonstrate their strengths and limitations.",,Algorithms; Computational complexity; Discrete time control systems; Heuristic methods; Performance; Polynomials; Problem solving; Simulated annealing; Accessibility; Discrete event simulation; Polynomial time algorithm; Computer simulation
A General Framework to Simulate Diffusions with Discontinuous Coefficients and Local Times,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149174854&doi=10.1145%2f3559541&partnerID=40&md5=5c39aa2e3b308ee774c0b60fc3c27f27,"In this article, we propose an efficient general simulation method for diffusions that are solutions to stochastic differential equations with discontinuous coefficients and local time terms. The proposed method is based on sampling from the corresponding continuous-time Markov chain approximation. In contrast to existing time discretization schemes, the Markov chain approximation method corresponds to a spatial discretization scheme and is demonstrated to be particularly suited for simulating diffusion processes with discontinuities in their state space. We establish the theoretical convergence order and also demonstrate the accuracy and robustness of the method in numerical examples by comparing it to the known benchmarks in terms of root mean squared error, runtime, and the parameter sensitivity.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CTMC approximation; Diffusion; discontinuous coefficient; local time,Continuous time systems; Convergence of numerical methods; Differential equations; Markov processes; Mean square error; Stochastic systems; Approximation methods; Continous time Markov chain; CTMC approximation; Discontinuous coefficients; Discretization scheme; Local time; Markov chain approximations; Spatial discretization schemes; Stochastic differential equations; Time discretization; Diffusion
Dynamic Data-driven Microscopic Traffic Simulation using Jointly Trained Physics-guided Long Short-Term Memory,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149168458&doi=10.1145%2f3558555&partnerID=40&md5=45e9b37c1aa2d2fa6596da5215d7e494,"Symbiotic simulation systems that incorporate data-driven methods (such as machine/deep learning) are effective and efficient tools for just-in-time (JIT) operational decision making. With the growing interest on Digital Twin City, such systems are ideal for real-time microscopic traffic simulation. However, learning-based models are heavily biased towards the training data and could produce physically inconsistent outputs. In terms of microscopic traffic simulation, this could lead to unsafe driving behaviours causing vehicle collisions in the simulation. As for symbiotic simulation, this could severely affect the performance of real-time base simulation models resulting in inaccurate or unrealistic forecasts, which could, in turn, mislead JIT what-if analyses. To overcome this issue, a physics-guided data-driven modelling paradigm should be adopted so that the resulting model could capture both accurate and safe driving behaviours. However, very few works exist in the development of such a car-following model that can balance between simulation accuracy and physical consistency. Therefore, in this paper, a new ""jointly-trained physics-guided Long Short-Term Memory (JTPG-LSTM)""neural network, is proposed and integrated to a dynamic data-driven simulation system to capture dynamic car-following behaviours. An extensive set of experiments was conducted to demonstrate the advantages of the proposed model from both modelling and simulation perspectives.  © 2022 Association for Computing Machinery.",car-following; Data-driven modelling; Digital Twin; just-in-time simulation; online learning; physics-guided machine learning; real-time simulation; symbiotic simulation,Brain; Decision making; Dynamics; E-learning; Just in time production; Learning systems; Real time systems; Car following; Data-driven model; Just-in-time; Just-in-time simulation; Machine-learning; Online learning; Physic-guided machine learning; Realtime simulation (RTS); Symbiotic simulation; Symbiotics; Time simulations; Long short-term memory
Performance Analysis of Speculative Parallel Adaptive Local Timestepping for Conservation Laws,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149179462&doi=10.1145%2f3545996&partnerID=40&md5=d6c3ebcebae01d7c8dfc45fcb81b9acf,"Stable simulation of conservation laws, such as those used to model fluid dynamics and plasma physics applications, requires the satisfaction of the so-called Courant-Friedrichs-Lewy condition. By allowing regions of the mesh to advance with different timesteps that locally satisfy this stability constraint, significant work reduction can be attained when compared to a time integration scheme using a single timestep size. However, parallelizing this algorithm presents considerable difficulty. Since the stability condition depends on the state of the system, dependencies become dynamic and potentially non-local. In this article, we present an adaptive local timestepping algorithm using an optimistic (Timewarp-based) parallel discrete event simulation. We introduce waiting heuristics to limit misspeculation and a semi-static load balancing scheme to eliminate load imbalance as parts of the mesh require finer or coarser timesteps. Last, we outline an interface for separating the physics of the specific conservation law from the temporal integration allowing for productive adoption of our proposed algorithm. We present a misspeculation study for three conservation laws, demonstrating both the productivity of the local timestepping API, for which 74% of the lines of code are reused across different conservation laws, and the robustness of the waiting heuristics- A t most 1.5% of element updates are rolled back. Our performance studies demonstrate up to a 2.8× speedup versus a baseline unoptimized local timestepping approach, a 4x improvement in per-node throughput compared to an MPI parallelization of synchronous timestepping, and scalability up to 3,072 cores on NERSC's Cori Haswell partition.  © 2022 Copyright held by the owner/author(s).",conservation laws; Local timestepping; parallel discrete event simulation; shallow water equations; Timewarp,Discrete event simulation; Mesh generation; Physical properties; Conservation law; Fluid-dynamics; Local time stepping; Model fluids; Parallel discrete-event simulation; Performances analysis; Shallow water equations; Stable simulation; Time step; Timewarp; Equations of motion
Reflective Nested Simulations Supporting Optimizations within Sequential Railway Traffic Simulators,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127055366&doi=10.1145%2f3467965&partnerID=40&md5=54f39953e3607a981f4c42408d1ab81e,"This article describes and discusses railway-traffic simulators that use reflective nested simulations. Such simulations support optimizations (decision-making) with a focus on the selection of the most suitable solution where selected types of traffic problems are present. This approach allows suspension of the ongoing main simulation at a given moment and, by using supportive nested simulations (working with an appropriate lookahead), assessment of the different acceptable solution variants for the problem encountered-that is, a what-if analysis is carried out. The variant that provides the best predicted operational results (based on a specific criterion) is then selected for continuing the suspended main simulation. The proposed procedures are associated, in particular, with the use of sequential simulators specifically developed for railway traffic simulations. Special attention is paid to parallel computations of replications both of the main simulation and of supportive nested simulations. The concept proposed, applicable to railway traffic modelling, has the following advantages. First, the solution variants for the existing traffic situation are analyzed with respect to the feasibility of direct monitoring and evaluation of the natural traffic indicators or the appropriate (multi-criterial) function. The indicator values compare the results obtained from the variants being tested. Second, the supporting nested simulations, which potentially use additional hierarchic nesting, can also include future occurrences of random effects (such as train delay), thereby enabling us to realistically assess future traffic in stochastic conditions. The guidelines presented (for exploiting nested simulations within application projects with time constraints) are illustrated on a simulation case study focusing on traffic assessment related to the track infrastructure of a passenger railway station. Nested simulations support decisions linked with dynamic assignments of platform tracks to delayed trains. The use of reflective nested simulations is appropriate particularly in situations in which a reasonable number of admissible variants are to be analyzed within decision-making problem solution. This method is applicable especially to the support of medium-term (tactical) and long-term (strategic) planning. Because of rather high computational and time demands, nested simulations are not recommended for solving short-term (operative) planning/control problems. © 2021 Copyright held by the owner/author(s)",automated decision-making support; nested/recursive simulations; parallel discrete event simulation; Railway traffic simulation,Discrete event simulation; Railroad transportation; Railroads; Random processes; Simulation platform; Simulators; Stochastic models; Stochastic systems; Vehicle actuated signals; Automated decision making; Automated decision-making support; Decision making support; Nested simulation; Nested/recursive simulation; Parallel discrete-event simulation; Railway traffic; Railway traffic simulation; Recursive simulation; Traffic simulations; Decision making
"Replicated Computational Results (RCR) Report for ""A New Test for Hamming-Weight Dependencies""",2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135167128&doi=10.1145%2f3527583&partnerID=40&md5=3cd6c8e3eaa2ba478a8e7cc7537b4221,"In the paper ""A New Test for Hamming-Weight Dependencies"", Blackman and Vigna propose a new statistical test for pseudorandom number generators (PRNG). Compared with the state-of-the-art tests, the proposed test could find statistical bias in the Hamming weights of the output of the generator. The proposed test is evaluated by using generators in TestU01 [2]. The authors provide the C code used in the tests and the code successfully reproduced the results shown in the article. © 2022 Association for Computing Machinery.",Replication of computational results,C (programming language); Codes (symbols); Random number generation; Testing; C++ codes; Computational results; Hamming weights; Pseudorandom number generators; Replication of computational result; State of the art; Statistical bias; TestU01; Number theory
Drawing Random Floating-point Numbers from an Interval,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135144270&doi=10.1145%2f3503512&partnerID=40&md5=97a91c47645df312e05c01280386f0de,"Drawing a floating-point number uniformly at random from an interval [a, b) is usually performed by a location-scale transformation of some floating-point number drawn uniformly from [0, 1). Due to the weak properties of floating-point arithmetic, such a transformation cannot ensure respect of the bounds, uniformity or spatial equidistributivity. We investigate and quantify precisely these shortcomings while reviewing the actual implementations of the method in major programming languages and libraries, and we propose a simple algorithm to avoid these shortcomings without compromising performances. © 2022 held by the owner/author(s).",Floating-point number; IEEE 754 standard; random float,Digital arithmetic; Floating point numbers; Floating-point arithmetic; IEEE-754 standard; Performance; Property; Random float; Scale transformation; SIMPLE algorithm; SIMPLER algorithms; IEEE Standards
Bayesian Optimisation vs. Input Uncertainty Reduction,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134756535&doi=10.1145%2f3510380&partnerID=40&md5=18257c661c51f69af9ded9f67cb3d7c8,"Simulators often require calibration inputs estimated from real-world data, and the estimate can significantly affect simulation output. Particularly when performing simulation optimisation to find an optimal solution, the uncertainty in the inputs significantly affects the quality of the found solution. One remedy is to search for the solution that has the best performance on average over the uncertain range of inputs yielding an optimal compromise solution. We consider the more general setting where a user may choose between either running simulations or querying an external data source, improving the input estimate and enabling the search for a more targeted, less compromised solution. We explicitly examine the trade-off between simulation and real data collection to find the optimal solution of the simulator with the true inputs. Using a value of information procedure, we propose a novel unified simulation optimisation procedure called Bayesian Information Collection and Optimisation that, in each iteration, automatically determines which of the two actions (running simulations or data collection) is more beneficial. We theoretically prove convergence in the infinite budget limit and perform numerical experiments demonstrating that the proposed algorithm is able to automatically determine an appropriate balance between optimisation and data collection. © 2022 held by the owner/author(s).",Bayesian optimisation; Input uncertainty; simulation optimisation,Budget control; Data acquisition; Economic and social effects; Optimal systems; Bayesian optimization; Data collection; Input uncertainty; Optimal solutions; Optimisations; Real-world; Running simulations; Simulation optimization; Simulation outputs; Uncertainty reduction; Iterative methods
A New Test for Hamming-Weight Dependencies,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135184893&doi=10.1145%2f3527582&partnerID=40&md5=beb469e598653105fb244753b272cc30,"We describe a new statistical test for pseudorandom number generators (PRNGs). Our test can find bias induced by dependencies among the Hamming weights of the outputs of a PRNG, even for PRNGs that pass state-of-the-art tests of the same kind from the literature, and particularly for generators based on F2-linear transformations such as the dSFMT [22], xoroshiro1024+ [1], and WELL512 [19]. © 2022 held by the owner/author(s).",Pseudorandom number generators,Number theory; Random number generation; Hamming weights; Pseudorandom number generators; State of the art; Linear transformations
The DEVStone Metric: Performance Analysis of DEVS Simulation Engines,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135159687&doi=10.1145%2f3543849&partnerID=40&md5=c37a3b3c301a12761d254ad4ce73fca1,"The DEVStone benchmark allows us to evaluate the performance of discrete-event simulators based on the Discrete Event System (DEVS) formalism. It provides model sets with different characteristics, enabling the analysis of specific issues of simulation engines. However, this heterogeneity hinders the comparison of the results among studies, as the results obtained on each research work depend on the chosen subset of DEVStone models. We define the DEVStone metric based on the DEVStone synthetic benchmark and provide a mechanism for specifying objective ratings for DEVS-based simulators. This metric corresponds to the average number of times that a simulator can execute a selection of 12 DEVStone models in 1 minute. The variety of the chosen models ensures that we measure different particularities provided by DEVStone. The proposed metric allows us to compare various simulators and to assess the impact of new features on their performance. We use the DEVStone metric to compare some popular DEVS-based simulators. © 2022 Association for Computing Machinery.",benchmarking; Discrete-event simulation; performance,Discrete event simulation; Engines; Simulators; Average numbers; Discrete event system simulations; Discrete events systems; Discrete-event simulations; Discrete-event simulators; Model set; Performance; Performances analysis; Simulation engine; Synthetic benchmark; Benchmarking
Rare-event Simulation for Neural Network and Random Forest Predictors,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133674910&doi=10.1145%2f3519385&partnerID=40&md5=1f1ae88c8810ee7476beee87d95281d6,"We study rare-event simulation for a class of problems where the target hitting sets of interest are defined via modern machine learning tools such as neural networks and random forests. This problem is motivated from fast emerging studies on the safety evaluation of intelligent systems, robustness quantification of learning models, and other potential applications to large-scale simulation in which machine learning tools can be used to approximate complex rare-event set boundaries. We investigate an importance sampling scheme that integrates the dominating point machinery in large deviations and sequential mixed integer programming to locate the underlying dominating points. Our approach works for a range of neural network architectures including fully connected layers, rectified linear units, normalization, pooling and convolutional layers, and random forests built from standard decision trees. We provide efficiency guarantees and numerical demonstration of our approach using a classification model in the UCI Machine Learning Repository. © 2022 Association for Computing Machinery.",importance sampling; large deviations; neural network; random forest; safety evaluation; Variance reduction,Importance sampling; Integer programming; Intelligent systems; Learning systems; Machine learning; Machinery; Multilayer neural networks; Network architecture; Random forests; Hitting sets; Large deviations; Learning tool; Machine-learning; Neural-networks; Random forests; Rare event simulation; Safety evaluations; Target hitting; Variance reductions; Decision trees
Introduction to the Special Section on PADS 2020,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127437706&doi=10.1145%2f3498363&partnerID=40&md5=45952603ff5b92847cf94796c7d899b0,[No abstract available],,
A Method Using Generative Adversarial Networks for Robustness Optimization,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127409520&doi=10.1145%2f3503511&partnerID=40&md5=45556c8c9e7f6d4b8c47f0365af4e021,"The evaluation of robustness is an important goal within simulation-based analysis, especially in production and logistics systems. Robustness refers to setting controllable factors of a system in such a way that variance in the uncontrollable factors (noise) has minimal effect on a given output. In this paper, we present an approach for optimizing robustness based on deep generative models, a special method of deep learning. We propose a method consisting of two Generative Adversarial Networks (GANs) to generate optimized experiment plans for the decision factors and the noise factors in a competitive, turn-based game. In a case study, the proposed method is tested and compared to traditional methods for robustness analysis including Taguchi method and Response Surface Method.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; Generative Adversarial Networks; Machine learning; Robustness optimization,Deep learning; Taguchi methods; Case-studies; Decision factors; Deep learning; Generative model; Minimal effects; Noise factor; Production and logistic systems; Robustness optimizations; Simulation-based analysis; Uncontrollable factors; Generative adversarial networks
Generating Fast Specialized Simulators for Stochastic Reaction Networks via Partial Evaluation,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127430859&doi=10.1145%2f3485465&partnerID=40&md5=628c51d14febd0b1f7ab5577beca51c2,"Domain-specific modeling languages allow a clear separation between simulation model and simulator and, thus, facilitate the development of simulation models and add to the credibility of simulation results. Partial evaluation provides an effective means for efficiently executing models defined in such languages. However, it also implies some challenges of its own. We illustrate this and solutions based on a simple domain-specific language for biochemical reaction networks as well as on the network representation of the established BioNetGen language. We implement different approaches adopting the same simulation algorithms: one generic simulator that parses models at runtime and one generator that produces a simulator specialized to a given model based on partial evaluation and code generation. For the purpose of better understanding, we additionally generate intermediate variants, where only some parts are partially evaluated. Akin to profile-guided optimization, we use dynamic execution of the model to further optimize the simulators. The performance of the approaches is carefully benchmarked using representative models of small to large biochemical reaction networks. The generic simulator achieves a performance similar to state-of-the-art simulators in the domain, whereas the specialized simulator outperforms established simulation tools with a speedup of more than an order of magnitude. Technical limitations in regard to the size of the generated code are discussed and overcome using a combination of link-time optimization and code separation. A detailed performance study is undertaken, investigating how and where partial evaluation has the largest effect.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Code generation; High performance; Modelling; Partial evaluation; Simulation; SSA,Codes (symbols); Computer aided software engineering; Computer simulation languages; Modeling languages; Network coding; Problem oriented languages; Specification languages; Stochastic systems; Biochemical reaction network; Codegeneration; Generic simulators; High performance; Modeling; Partial evaluation; Performance; Simulation; Simulation model; SSA; Simulators
Efficient Protocol Testing under Temporal Uncertain Event Using Discrete-event Network Simulations,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127431279&doi=10.1145%2f3490028&partnerID=40&md5=1f666cf7a27f4b80c75072b2f1d832f2,"Testing network protocol implementations is difficult mainly because of the temporal uncertain nature of network events. To evaluate the worst-case performance or detect the bugs of a network protocol implementation using network simulators, we need to systematically simulate the behavior of the network protocol under all possible cases of the temporal uncertain events, which is time consuming. The recently proposed Symbolic Execution based Interval Branching (SEIB) simulates a group of uncertain cases together in a single simulation branch and thus is more efficient than brute force testing. In this article, we argue that the efficiency of SEIB could be further significantly improved by eliminating unnecessary comparisons of the event timestamps. Specifically, we summarize and present three general types of unnecessary comparisons when SEIB is applied to a general network simulator, and then correspondingly propose three novel techniques to eliminate them. Our extensive simulations show that our techniques can improve the efficiency of SEIB by several orders of magnitude, such as from days to minutes.  © 2022 Association for Computing Machinery.",Discrete event simulator; Network protocol testing; Symbolic execution; Temporal uncertainty,Consumer behavior; Internet protocols; Model checking; Simulators; Uncertainty analysis; Discrete events; Discrete-event simulators; Efficient protocols; Network protocol testing; Network simulators; Protocol implementation; Protocol testing; Symbolic execution; Temporal uncertainty; Uncertain events; Efficiency
Mechanisms for Precise Virtual Time Advancement in Network Emulation,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127400181&doi=10.1145%2f3478867&partnerID=40&md5=631d7fcb169648797954268ab0b4041b,"Network emulators enable rapid prototyping and testing of applications. In a typical emulation, the execution order and process execution burst lengths are managed by the host platform's operating system, largely independent of the emulator. Timerbased mechanisms are typically used, but the imprecision of timer firings introduces imprecision in the advancement of time. This leads to statistical variation in behavior that is not due to the model.This article describes an open-source tool called Kronos, which provides a set of mechanisms for precise instruction-level tracking of process execution and control over execution order of containers, thus improving the mapping of executed behavior to advancement in time. This, and control of execution and placement of emulated processes in virtual time make the behavior of the emulation independent of the CPU resources of the platform that hosts the emulation. Under Kronos each process has its own virtual clock that is advanced based on a count of the number of 86 assembly instructions executed by its children. Two types of instruction counting techniques are discussed: (1) hardware-assisted mechanisms that are transparent to the executing application and (2) binary instrumentation-assisted mechanisms that modify the executing binary. We analyze the overheads associated with each approach and experimentally demonstrate the impact of Kronos' time advancement precision by comparing it against emulations that, like Kronos, are embedded in virtual time, but unlike Kronos rely on Linux timers to control virtual machines and measure their progress in virtual time. We present two useful applications where Kronos aids in generating high-fidelity emulation results at low hardware costs: (1) analyzing protocol performance and (2) enabling analysis of cyber physical control systems. We also discuss limitations associated with simple linear conversions between instruction counts and ascribed virtual time and develop and evaluate more accurate virtual time conversion models.  © 2022 Association for Computing Machinery.",INS-SCHED; Instruction counting; Kronos; Network emulation; Virtual time; Virtualization,Linux; Process control; Scheduling; Virtualization; In networks; INS-SCHED; Instruction counting; Kronos; Network emulation; Network emulators; Process execution; Rapid-prototyping; Virtual-time; Virtualizations; E-learning
A Scalable Quantum Key Distribution Network Testbed Using Parallel Discrete-Event Simulation,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127381578&doi=10.1145%2f3490029&partnerID=40&md5=1d7001d4247ee2bf85316db8d115c976,"Quantum key distribution (QKD) has been promoted as a means for secure communications. Although QKD has been widely implemented in many urban fiber networks, the large-scale deployment of QKD remains challenging. Today, researchers extensively conduct simulation-based evaluations for their designs and applications of large-scale QKD networks for cost efficiency. However, the existing discrete-event simulators offer models for QKD hardware and protocols based on sequential event execution, which limits the scale of the experiments. In this work, we explore parallel simulation of QKD networks to address this issue. Our contributions lay in the exploration of QKD network characteristics to be leveraged for parallel simulation as well as the development of a parallel simulation framework for QKD networks. We also investigate three techniques to improve the simulation performance including (1) a ladder queue based event list, (2) memoization for computationally intensive quantum state transformation information, and (3) optimization of the network partition scheme for workload balance. The experimental results show that our parallel simulator is 10 times faster than a sequential simulator when simulating a 128-node QKD network. Our linear-regression-based network partition scheme can further accelerate the simulation experiments up to two times over using a randomized network partition scheme.  © 2022 Association for Computing Machinery.",BB84; Parallel discrete-event simulation; Quantum key distribution,Discrete event simulation; Quantum theory; Simulators; Bb84; Design and application; Fiber networks; Large-scale deployment; Large-scales; Network partitions; Network testbeds; Parallel discrete-event simulation; Parallel simulations; Partition schemes; Quantum cryptography
Importance Sampling for a Simple Markovian Intensity Model Using Subsolutions,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127447384&doi=10.1145%2f3502432&partnerID=40&md5=4a1b9970bd4fa7b9a6423159ea167ce5,"This article considers importance sampling for estimation of rare-event probabilities in a specific collection of Markovian jump processes used for, e.g., modeling of credit risk. Previous attempts at designing importance sampling algorithms have resulted in poor performance and the main contribution of the article is the design of efficient importance sampling algorithms using subsolutions. The dynamics of the jump processes cause the corresponding Hamilton-Jacobi equations to have an intricate state-dependence, which makes the design of efficient algorithms difficult. We provide theoretical results that quantify the performance of importance sampling algorithms in general and construct asymptotically optimal algorithms for some examples. The computational gain compared to standard Monte Carlo is illustrated by numerical examples.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Credit risk; Importance sampling; Large deviations; Markovian intensity models; Monte Carlo,Importance sampling; Learning algorithms; Markov processes; Risk perception; Credit risks; Intensity models; Jump process; Large deviations; Markovian; Markovian intensity model; Rare event probabilities; Sampling algorithm; Simple++; Subsolution; Risk assessment
A Workflow Architecture for Cloud-based Distributed Simulation,2022,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127434180&doi=10.1145%2f3503510&partnerID=40&md5=df000bf9e2dcf7ad761dc1ec570d25b2,"Distributed Simulation has still to be adopted significantly by the wider simulation community. Reasons for this might be that distributed simulation applications are difficult to develop and access to multiple computing resources are required. Cloud computing offers low-cost on-demand computing resources. Developing applications that can use cloud computing can be also complex, particularly those that can run on different clouds. Cloud-based Distributed Simulation (CBDS) is potentially attractive, as it may solve the computing resources issue as well as other cloud benefits, such as convenient network access. However, as possibly shown by the lack of sustainable approaches in the literature, the combination of cloud and distributed simulation may be far too complex to develop a general approach. E-Infrastructures have emerged as large-scale distributed systems that support high-performance computing in various scientific fields. Workflow Management Systems (WMS) have been created to simplify the use of these e-Infrastructures. There are many examples of where both technologies have been extended to use cloud computing. This article therefore presents our investigation into the potential of using these technologies for CBDS in the above context and the WORkflow architecture for cLoud-based Distributed Simulation (WORLDS), our contribution to CBDS. We present an implementation of WORLDS using the CloudSME Simulation Platform that combines the WS-PGRADE/gUSE WMS with the CloudBroker Platform as a Service. The approach is demonstrated with a case study using an agent-based distributed simulation of an Emergency Medical Service in REPAST and the Portico HLA RTI on the Amazon EC2 cloud.  © 2022 Association for Computing Machinery.",Cloud computing; Distributed simulation; HLA; Modeling & simulation; Science gateway; Scientific workflows,Complex networks; Computer architecture; Emergency services; Network architecture; Simulation platform; Work simplification; Cloud-based; Cloud-computing; Computing resource; Distributed simulations; E-infrastructures; HLA; Modeling simulation; Science gateway; Scientific workflows; Work-flows; Cloud computing
A Practical Approach to Subset Selection for Multi-objective Optimization via Simulation,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113146953&doi=10.1145%2f3462187&partnerID=40&md5=4930f5615cfcf6a326ce6dcdf758fe5b,"We describe a practical two-stage algorithm, BootComp, for multi-objective optimization via simulation. Our algorithm finds a subset of good designs that a decision-maker can compare to identify the one that works best when considering all aspects of the system, including those that cannot be modeled. BootComp is designed to be straightforward to implement by a practitioner with basic statistical knowledge in a simulation package that does not support sequential ranking and selection. These requirements restrict us to a two-stage procedure that works with any distributions of the outputs and allows for the use of common random numbers. Comparisons with sequential ranking and selection methods suggest that it performs well, and we also demonstrate its use analyzing a real simulation aiming to determine the optimal ward configuration for a UK hospital.  © 2021 Owner/Author.",chance constraints; Ranking and selection; simulation; subset selection,Decision making; Common random numbers; Ranking and selection; Ranking and selection methods; Simulation packages; Statistical knowledge; Subset selection; Two stage procedure; Two-stage algorithm; Multiobjective optimization
Random Variate Generation for Exponential and Gamma Tilted Stable Distributions,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113153493&doi=10.1145%2f3449357&partnerID=40&md5=01298d432f9ea33f6cb690d851e133d6,"We develop a new efficient simulation scheme for sampling two families of tilted stable distributions: exponential tilted stable (ETS) and gamma tilted stable (GTS) distributions. Our scheme is based on two-dimensional single rejection. For the ETS family, its complexity is uniformly bounded over all ranges of parameters. This new algorithm outperforms all existing schemes. In particular, it is more efficient than the well-known double rejection scheme, which is the only algorithm with uniformly bounded complexity that we can find in the current literature. Beside the ETS family, our scheme is also flexible to be further extended for generating the GTS family, which cannot easily be done by extending the double rejection scheme. Our algorithms are straightforward to implement, and numerical experiments and tests are conducted to demonstrate the accuracy and efficiency.  © 2021 ACM.",Lévy process; Monte Carlo simulation; random variate generation; tempered stable distribution; two-dimensional single rejection,Computer simulation; Efficient simulation; Numerical experiments; Random variate generations; Stable distributions; Uniformly bounded; Computer applications
Distributed Approaches to Supply Chain Simulation,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113155808&doi=10.1145%2f3466170&partnerID=40&md5=37872e5077c0f208b5dde58bb2b7a768,"The field of Supply Chain Management (SCM) is experiencing rapid strides in the use of Industry 4.0 technologies and the conceptualization of new supply chain configurations for online retail, sustainable and green supply chains, and the Circular Economy. Thus, there is an increasing impetus to use simulation techniques such as discrete-event simulation, agent-based simulation, and hybrid simulation in the context of SCM. In conventional supply chain simulation, the underlying constituents of the system like manufacturing, distribution, retail, and logistics processes are often modelled and executed as a single model. Unlike this conventional approach, a distributed supply chain simulation (DSCS) enables the coordinated execution of simulation models using specialist software. To understand the current state-of-the-art of DSCS, this paper presents a methodological review and categorization of literature in DSCS using a framework-based approach. Through a study of over 130 articles, we report on the motivation for using DSCS, the modelling techniques, the underlying distributed computing technologies and middleware, its advantages and a future agenda, and also limitations and trade-offs that may be associated with this approach. The increasing adoption of technologies like Internet-of-Things and Cloud Computing will ensure the availability of both data and models for distributed decision-making, which is likely to enable data-driven DSCS of the future. This review aims to inform organizational stakeholders, simulation researchers and practitioners, distributed systems developers and software vendors, as to the current state-of-the art of DSCS, and which will inform the development of future DSCS using new applied computing approaches.  © 2021 ACM.",distributed simulation; review; simulation methods; Supply chain management,Decision making; Discrete event simulation; Distributed database systems; Economic and social effects; Green manufacturing; Middleware; Supply chain management; Distributed approaches; Distributed computing technology; Distributed decision making; Distributed supply chain simulations; Framework-based approach; Supply chain configuration; Supply chain managements (SCM); Supply chain simulation; Distributed computer systems
Explicit Modeling of Personal Space for Improved Local Dynamics in Simulated Crowds,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113157801&doi=10.1145%2f3462202&partnerID=40&md5=16a28448cec93dbf92ccb2343aa09e21,"Crowd simulation demands careful consideration in regard to the classic trade-off between accuracy and efficiency. Particle-based methods have seen success in various applications in architecture, military, urban planning, and entertainment. This method focuses on local dynamics of individuals in large crowds, with a focus on serious games and entertainment. The technique uses an area-based penalty force that captures the infringement of each entity's personal space. This method does not need a costly nearest-neighbor search and allows for an inherently data-parallel implementation capable of simulating thousands of entities at interactive frame rates. The algorithm reproduces personal space compression around motion barriers for moving crowds and around points of interest for static crowds.  © 2021 ACM.",Agent-based modeling; crowd pedestrian models; GPU; personal space,Economic and social effects; Entertainment; Military applications; Nearest neighbor search; Crowd Simulation; Explicit modeling; Interactive frame rates; Local dynamics; Particle-based methods; Penalty force; Personal spaces; Points of interest; Serious games
Improved Penalty Function with Memory for Stochastically Constrained Optimization via Simulation,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113181985&doi=10.1145%2f3465333&partnerID=40&md5=93fbf808fb022c2c6c41f0b83e260e52,"Penalty function with memory (PFM) in Park and Kim [2015] is proposed for discrete optimization via simulation problems with multiple stochastic constraints where performance measures of both an objective and constraints can be estimated only by stochastic simulation. The original PFM is shown to perform well, finding a true best feasible solution with a higher probability than other competitors even when constraints are tight or near-tight. However, PFM applies simple budget allocation rules (e.g., assigning an equal number of additional observations) to solutions sampled at each search iteration and uses a rather complicated penalty sequence with several user-specified parameters. In this article, we propose an improved version of PFM, namely IPFM, which can combine the PFM with any simulation budget allocation procedure that satisfies some conditions within a general DOvS framework. We present a version of a simulation budget allocation procedure useful for IPFM and introduce a new penalty sequence, namely PS2+, which is simpler than the original penalty sequence yet holds convergence properties within IPFM with better finite-sample performances. Asymptotic convergence properties of IPFM with PS2+ are proved. Our numerical results show that the proposed method greatly improves both efficiency and accuracy compared to the original PFM.  © 2021 ACM.",Discrete optimization via simulation; penalty function with memory; ranking and selection; simulation budget allocation; stochastic constraints,Constrained optimization; Iterative methods; Numerical methods; Sampling; Stochastic models; Stochastic systems; Asymptotic convergence; Convergence properties; Discrete optimization; Finite sample performance; Numerical results; Performance measure; Stochastic constraints; Stochastic simulations; Budget control
Bias-corrected Estimation of the Density of a Conditional Expectation in Nested Simulation Problems,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113156842&doi=10.1145%2f3462201&partnerID=40&md5=a14ed84f8f5912b561473aa737b39e3a,"Many two-level nested simulation applications involve the conditional expectation of some response variable, where the expected response is the quantity of interest, and the expectation is with respect to the inner-level random variables, conditioned on the outer-level random variables. The latter typically represent random risk factors, and risk can be quantified by estimating the probability density function (pdf) or cumulative distribution function (cdf) of the conditional expectation. Much prior work has considered a naïve estimator that uses the empirical distribution of the sample averages across the inner-level replicates. This results in a biased estimator, because the distribution of the sample averages is over-dispersed relative to the distribution of the conditional expectation when the number of inner-level replicates is finite. Whereas most prior work has focused on allocating the numbers of outer- and inner-level replicates to balance the bias/variance tradeoff, we develop a bias-corrected pdf estimator. Our approach is based on the concept of density deconvolution, which is widely used to estimate densities with noisy observations but has not previously been considered for nested simulation problems. For a fixed computational budget, the bias-corrected deconvolution estimator allows more outer-level and fewer inner-level replicates to be used, which substantially improves the efficiency of the nested simulation.  © 2021 ACM.",Deconvolution; estimating a conditional variance; quadratic programming; rate of convergence; shape constraints; Stein's unbiased risk estimation,Budget control; Distribution functions; Information dissemination; Random variables; Risk perception; Computational budget; Conditional expectation; Cumulative distribution function; Empirical distributions; Nested simulation; Noisy observations; Probability density function (pdf); Quantity of interest; Probability density function
"Replicated Computational Results (RCR) Report for ""A Practical Approach to Subset Selection for Multi-Objective Optimization via Simulation""",2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113173042&doi=10.1145%2f3453987&partnerID=40&md5=964b14e0aa47a2a59d75be3c2dc5ba21,"In ""A Practical Approach to Subset Selection for Multi-Objective Optimization via Simulation,""Currie and Monks propose an algorithm for multi-objective simulation-based optimization. In contrast to sequential ranking and selection schemes, their algorithm follows a two-stage scheme. The approach is evaluated by comparing the results to those obtained using the existing OCBA-m algorithm for synthetic problems and for a hospital ward configuration problem. The authors provide the Python code used in the experiments in the form of Jupyter notebooks. The code successfully reproduced the results shown in the article.  © 2021 ACM.",Ranking and selection; replication of computational results; subset selection,Hospitals; Computational results; Hospital wards; M-algorithms; Multi objective; Ranking and selection; Simulation-based optimizations; Subset selection; Synthetic problem; Multiobjective optimization
Doping Tests for Cyber-physical Systems,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113895093&doi=10.1145%2f3449354&partnerID=40&md5=887d39a71a68a5eb7a887a6a20daef65,"The software running in embedded or cyber-physical systems is typically of proprietary nature, so users do not know precisely what the systems they own are (in)capable of doing. Most malfunctionings of such systems are not intended by the manufacturer, but some are, which means these cannot be classified as bugs or security loopholes. The most prominent examples have become public in the diesel emissions scandal, where millions of cars were found to be equipped with software violating the law, altogether polluting the environment and putting human health at risk. The behaviour of the software embedded in these cars was intended by the manufacturer, but it was not in the interest of society, a phenomenon that has been called software doping. Due to the unavailability of a specification, the analysis of doped software is significantly different from that for buggy or insecure software and hence classical verification and testing techniques have to be adapted. The work presented in this article builds on existing definitions of software doping and lays the theoretical foundations for conducting software doping tests, so as to enable uncovering unethical manufacturers. The complex nature of software doping makes it very hard to effectuate doping tests in practice. We explain the main challenges and provide efficient solutions to realise doping tests despite this complexity. © 2021 ACM.",automotive exhaust emissions; Cyber-physical systems; model-based testing; software doping,Automobile manufacture; Cyber Physical System; Embedded systems; Environmental regulations; Health risks; Software testing; Testing; Verification; Complex nature; Diesel emission; Doping test; Human health; Running-in; Theoretical foundations; Verification and testing; Program debugging
Replication of computational results report for doping tests for cyber-physical systems,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113878475&doi=10.1145%2f3459667&partnerID=40&md5=22ad746db09cda5d2bdda5e9b9c9c0cb,"The article Doping Tests for Cyber-Physical Systems is accompanied by a prototype implementation in Python 2.7. The artifact (i.e., code and observational data) is hosted on a publicly available repository. The article contains comprehensive documentation in the appendix, and running the code is straightforward. The article Doping Tests for Cyber-Physical Systems can thus receive the Artifacts Evaluated - Reusable badge. © 2021 ACM.",ACM TOMACS; Replication of computational results,Embedded systems; Software prototyping; Comprehensive documentation; Computational results; Doping test; Observational data; Prototype implementations; Cyber Physical System
Introduction to the Special Issue on QEST 2019,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113827237&doi=10.1145%2f3463764&partnerID=40&md5=4dba313e4de049fe143820cab790ba70,[No abstract available],,
Transfer Reinforcement Learning for Autonomous Driving: FromWiseMove toWiseSim,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113850108&doi=10.1145%2f3449356&partnerID=40&md5=a5c19a2005376813f90c4ba7b5ffc41d,"Reinforcement learning (RL) is an attractive way to implement high-level decision-making policies for autonomous driving, but learning directly from a real vehicle or a high-fidelity simulator is variously infeasible. We therefore consider the problem of transfer reinforcement learning and study how a policy learned in a simple environment using WiseMove can be transferred to our high-fidelity simulator, WiseMove. WiseMove is a framework to study safety and other aspects of RL for autonomous driving. WiseMove accurately reproduces the dynamics and software stack of our real vehicle. We find that the accurately modelled perception errors in WiseMove contribute the most to the transfer problem. These errors, when even naively modelled in WiseMove, provide an RL policy that performs better in WiseMove than a hand-crafted rule-based policy. Applying domain randomization to the environment in WiseMove yields an even better policy. The final RL policy reduces the failures due to perception errors from 10% to 2.75%. We also observe that the RL policy has significantly less reliance on velocity compared to the rule-based policy, having learned that its measurement is unreliable. © 2021 ACM.",autonomous driving; deep reinforcement learning; policy distillation; Transfer reinforcement learning,Autonomous vehicles; Decision making; Errors; Transfer learning; Autonomous driving; Handcrafted rules; High-fidelity simulators; Real vehicles; Rule-based policies; Simple environments; Software stacks; Transfer problems; Reinforcement learning
Falsification of Hybrid Systems Using Adaptive Probabilistic Search,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113861133&doi=10.1145%2f3459605&partnerID=40&md5=32e6914415b604932f8e331bf59bccde,"We present and analyse an algorithm that quickly finds falsifying inputs for hybrid systems. Our method is based on a probabilistically directed tree search, whose distribution adapts to consider an increasingly fine-grained discretization of the input space. In experiments with standard benchmarks, our algorithm shows comparable or better performance to existing techniques, yet it does not build an explicit model of a system. Instead, at each decision point within a single trial, it makes an uninformed probabilistic choice between simple strategies to extend the input signal by means of exploration or exploitation. Key to our approach is the way input signal space is decomposed into levels, such that coarse segments are more probable than fine segments. We perform experiments to demonstrate how and why our approach works, finding that a fully randomized exploration strategy performs as well as our original algorithm that exploits robustness. We propose this strategy as a new baseline for falsification and conclude that more discriminative benchmarks are required. © 2021 ACM.",Falsification; hybrid systems; principled probabilistic search,Benchmarking; Crime; Decision trees; Hybrid systems; Signal processing; Decision points; Directed trees; Discretizations; Explicit modeling; Exploration strategies; Original algorithms; Probabilistic choices; Probabilistic search; Adaptive systems
A Modest Approach to Markov Automata,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113849212&doi=10.1145%2f3449355&partnerID=40&md5=8ab2cede7f333095c96adc3bf3d9417f,"Markov automata are a compositional modelling formalism with continuous stochastic time, discrete probabilities, and nondeterministic choices. In this article, we present extensions to MODEST, an expressive high-level language with roots in process algebra, that allow large Markov automata models to be specified in a succinct, modular way. We illustrate the advantages of MODEST over alternative languages. Model checking Markov automata models requires dedicated algorithms for time-bounded and long-run average reward properties. We describe and evaluate the state-of-the-art algorithms implemented in the mcsta model checker of the MODEST TOOLSET. We find that mcsta improves the performance and scalability of Markov automata model checking compared to earlier and alternative tools. We explain a partial-exploration approach based on the BRTDP method designed to mitigate the state space explosion problem of model checking, and experimentally evaluate its effectiveness. This problem can be avoided entirely by purely simulation-based techniques, but the nondeterminism in Markov automata hinders their straightforward application. We explain how lightweight scheduler sampling can make simulation possible, and provide a detailed evaluation of its usefulness on several benchmarks using the MODEST TOOLSET's modes simulator. © 2021 Owner/Author.",dependability; formal methods; Markov automata; performance evaluation; probabilistic model checking; simulation; statistical model checking,Automata theory; High level languages; Robots; Stochastic systems; Automata models; Average reward; Compositional modelling; Non-determinism; Nondeterministic choice; Performance and scalabilities; State-of-the-art algorithms; State-space explosion; Model checking
State-space Construction of Hybrid Petri Nets with Multiple Stochastic Firings,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113854352&doi=10.1145%2f3449353&partnerID=40&md5=fcde9991402facfa015f23a062b9769d,"Hybrid Petri nets have been extended to include general transitions that fire after a randomly distributed amount of time. With a single general one-shot transition the state space and evolution over time can be represented either as a Parametric Location Tree or as a Stochastic Time Diagram. Recent work has shown that both representations can be combined and then allow multiple stochastic firings. This work presents an algorithm for building the Parametric Location Tree with multiple general transition firings and shows how its transient probability distribution can be computed using multi-dimensional integration. We discuss the (dis-)advantages of an interval arithmetic and a geometric approach to compute the areas of integration. Furthermore, we provide details on how to perform a Monte Carlo integration either directly on these intervals or convex polytopes, or after transformation to standard simplices. A case study on a battery-backup system shows the feasibility of the approach and discusses the performance of the different integration approaches. © 2021 ACM.",multi-dimensional integration; Petri nets; stochastic hybrid model; transient probability,Forestry; Integration; Petri nets; Probability distributions; Geometric approaches; Integration approach; Interval arithmetic; Monte Carlo integration; Multi dimensional; Randomly distributed; Shot transitions; Transition firings; Stochastic systems
Introduction to the Special Issue on PADS 2019,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104985032&doi=10.1145%2f3451235&partnerID=40&md5=7867d9cd525a444ec4cb24385908e6a8,[No abstract available],,
Fluid approximation-based analysis for mode-switching population dynamics,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104992525&doi=10.1145%2f3441680&partnerID=40&md5=0458b6760ae461cd1bdfcf5b08db6f79,"Fluid approximation results provide powerful methods for scalable analysis of models of population dynamics with large numbers of discrete states and have seen wide-ranging applications in modelling biological and computer-based systems and model checking. However, the applicability of these methods relies on assumptions that are not easily met in a number of modelling scenarios. This article focuses on one particular class of scenarios in which rapid information propagation in the system is considered. In particular, we study the case where changes in population dynamics are induced by information about the environment being communicated between components of the population via broadcast communication. We see how existing hybrid fluid limit results, resulting in piecewise deterministic Markov processes, can be adapted to such models. Finally, we propose heuristic constructions for extracting the mean behaviour from the resulting approximations without the need to simulate individual trajectories. © 2021 ACM.",Fluid approximation; hybrid models; population dynamics; stochastic modelling,Computer aided analysis; Dynamics; Information dissemination; Markov processes; Population dynamics; Broadcast communication; Computer-based system; Fluid approximation; Information propagation; Mode-switching; Piecewise deterministic Markov process; Scalable analysis; Wide-ranging applications; Model checking
Distributed virtual time-based synchronization for simulation of cyber-physical systems,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105015505&doi=10.1145%2f3446237&partnerID=40&md5=30aebfa47d81f2f8f8ecd8d38895ae5f,"Our world today increasingly relies on the orchestration of digital and physical systems to ensure the successful operations of many complex and critical infrastructures. Simulation-based testbeds are useful tools for engineering those cyber-physical systems and evaluating their efficiency, security, and resilience. In this article, we present a cyber-physical system testing platform combining distributed physical computing and networking hardware and simulation models. A core component is the distributed virtual time system that enables the efficient synchronization of virtual clocks among distributed embedded Linux devices. Virtual clocks also enable high-fidelity experimentation by interrupting real and emulated cyber-physical applications to inject offline simulation data. We design and implement two modes of the distributed virtual time: periodic mode for scheduling repetitive events like sensor device measurements, and dynamic mode for on-demand interrupt-based synchronization. We also analyze the performance of both approaches to synchronization including overhead, accuracy, and error introduced from each approach. By interconnecting the embedded devices' general purpose IO pins, they can coordinate and synchronize with low overhead, under 50 microseconds for eight processes across four embedded Linux devices. Finally, we demonstrate the usability of our testbed and the differences between both approaches in a power grid control application. © 2021 ACM.",Cyber-physical systems; distributed systems; simulation; synchronization; testbed,Clocks; Electric power system control; Electric power transmission networks; Embedded systems; Linux; Synchronization; Testbeds; Virtual reality; Core components; Cyber physical systems (CPSs); Cyber physicals; Design and implements; Embedded device; Off-line simulations; Physical computing; Physical systems; Cyber Physical System
Transitioning spiking neural network simulators to heterogeneous hardware,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105021875&doi=10.1145%2f3422389&partnerID=40&md5=41a9f6bc3e70313021ea3e794b6b76e3,"Spiking neural networks (SNN) are among the most computationally intensive types of simulation models, with node counts on the order of up to 1011. Currently, there is intensive research into hardware platforms suitable to support large-scale SNN simulations, whereas several of the most widely used simulators still rely purely on the execution on CPUs. Enabling the execution of these established simulators on heterogeneous hardware allows new studies to exploit the many-core hardware prevalent in modern supercomputing environments, while still being able to reproduce and compare with results from a vast body of existing literature. In this article, we propose a transition approach for CPU-based SNN simulators to enable the execution on heterogeneous hardware (e.g., CPUs, GPUs, and FPGAs), with only limited modifications to an existing simulator code base and without changes to model code. Our approach relies on manual porting of a small number of core simulator functionalities as found in common SNN simulators, whereas the unmodified model code is analyzed and transformed automatically. We apply our approach to the well-known simulator NEST and make a version executable on heterogeneous hardware available to the community. Our measurements show that at full utilization, a single GPU achieves the performance of about 9 CPU cores. A CPU-GPU co-execution with load balancing is also demonstrated, which shows better performance compared to CPU-only or GPU-only execution. Finally, an analytical performance model is proposed to heuristically determine the optimal parameters to execute the heterogeneous NEST. © 2021 ACM.",automatic code transformation; Spiking neural network simulators; types of simulation: parallel & heterogeneous,Computer hardware; Program processors; Simulators; Analytical performance model; Core simulators; Hardware platform; Heterogeneous hardware; Intensive research; Optimal parameter; Spiking neural network(SNN); Spiking neural networks; Neural networks
Machine learning-enabled scalable performance prediction of scientific codes,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105023096&doi=10.1145%2f3450264&partnerID=40&md5=affafe64a1b8aec2985e0597755db8e8,"Hardware architectures become increasingly complex as the compute capabilities grow to exascale. We present the Analytical Memory Model with Pipelines (AMMP) of the Performance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and hardware architecture parameters as input and predicts runtime of that code on the target hardware platform, which is defined in the input parameters. PPT-AMMP transforms the code to an (architecture-independent) intermediate representation, then (i) analyzes the basic block structure of the code, (ii) processes architecture-independent virtual memory access patterns that it uses to build memory reuse distance distribution models for each basic block, and (iii) runs detailed basic-block level simulations to determine hardware pipeline usage. PPT-AMMP uses machine learning and regression techniques to build the prediction models based on small instances of the input code, then integrates into a higher-order discrete-event simulation model of PPT running on Simian PDES engine. We validate PPT-AMMP on four standard computational physics benchmarks and present a use case of hardware parameter sensitivity analysis to identify bottleneck hardware resources on different code inputs. We further extend PPT-AMMP to predict the performance of a scientific application code, namely, the radiation transport mini-app SNAP. To this end, we analyze multi-variate regression models that accurately predict the reuse profiles and the basic block counts. We validate predicted SNAP runtimes against actual measured times. © 2021 Public Domain.",deep learning; genetic programming; performance modeling; Performance modeling; symbolic regression,Discrete event simulation; Forecasting; Machine learning; Memory architecture; Pipelines; Regression analysis; Sensitivity analysis; Computational physics; Discrete-event simulation model; Hardware architecture; Intermediate representations; Performance prediction; Regression techniques; Scalable performance; Scientific application codes; Predictive analytics
"Fixed-confidence, fixed-tolerance guarantees for ranking-and-selection procedures",2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104972283&doi=10.1145%2f3432754&partnerID=40&md5=2eddadb7e9436707b76bacbf6682fc55,"Ever since the conception of the statistical ranking-and-selection (R8S) problem, a predominant approach has been the indifference-zone (IZ) formulation. Under the IZ formulation, R8S procedures are designed to provide a guarantee on the probability of correct selection (PCS) whenever the performance of the best system exceeds that of the second-best system by a specified amount. We discuss the shortcomings of this guarantee and argue that providing a guarantee on the probability of good selection (PGS) - selecting a system whose performance is within a specified tolerance of the best - is a more justifiable goal. Unfortunately, this form of fixed-confidence, fixed-tolerance guarantee has received far less attention within the simulation community. We present an overview of the PGS guarantee with the aim of reorienting the simulation community toward this goal. We examine numerous techniques for proving the PGS guarantee, including sufficient conditions under which selection and subset-selection procedures that deliver the IZ-inspired PCS guarantee also deliver the PGS guarantee. © 2021 ACM.",indifference zone; probability of good selection; Ranking and selection,Computer simulation; Indifference zone; Probability of correct selections; Ranking and selection procedures; Statistical ranking and selection; Subset selection; Computer applications
Application of Simulation in Healthcare Service Operations,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101045028&partnerID=40&md5=43e91ff17a90634c35ea5884d1b22a39,"The health system is intricate due to its dynamic nature and critical service requirements. The involvement of multiple layers of health service providers quadrupled this complexity and results in a complicated operating environment. Simulation is often considered an apt technique to model and study complex systems in the literature. The popularity of simulation in the healthcare domain had only accelerated with time and resulted in a large number of articles intended to solve myriad healthcare problems. This article analyzes healthcare simulation literature of the past decade (2007-2016) that addresses operations management issues in various healthcare service delivery levels and categorizes the literature accordingly. In the next step, we attempt to assimilate the entire literature to capture specific health issues addressed, operations management concepts applied, and simulation methods used, and identify major research gaps. Finally, we develop the research agenda from dividing these gaps into the contextual, conceptual, and methodological genre that is consistent with the previous state-of-the-art literature reviews in operations management. Furthermore, this article demonstrates other minute aspects such as ""sources of funding""and ""tools used for the research""to maintain coherence with the previous reviews in the healthcare simulation. The objective of this work is twofold: To connect the knowledge continuum to the present, and to provide potential research directions for future academicians. © 2020 ACM.",Healthcare; literature review; service operations; simulation,Computer applications; Computer simulation; Critical service; Healthcare domains; Healthcare problems; Healthcare services; Literature reviews; Operating environment; Operations management; Potential researches; Health care
"Replication of Computational Results Report for ""green Simulation with Database Monte Carlo",2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101031559&partnerID=40&md5=2a63ec89ce3998b6f910fe65aac52c3a,"This article presents the reproducibility results associated with the article ""Green Simulation with Database Monte Carlo,""by Mingbin Feng and Jeremy Staum. The authors have uploaded their artifact to Zenodo, which ensures a long-term retention of the artifact. The artifact, which is based on a set of R scripts, allows to easily regenerate data for the figures and the tables, it completes successfully, and allows to reproduce all the experimental results in the article. The article can thus receive the Artifacts Available, the Artifacts Evaluated-Functional, and the Results Reproduced badges. © 2020 ACM.",ACM TOMACS; Replication of computational results,Computational results; Long-term retention; Reproducibilities; Monte Carlo methods
Global-local Metamodel-assisted Stochastic Programming via Simulation,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100996759&partnerID=40&md5=f67dce988ef9af0561bd6e7c929aa020,"To integrate strategic, tactical, and operational decisions, stochastic programming has been widely used to guide dynamic decision-making. In this article, we consider complex systems and introduce the global-local metamodel-assisted stochastic programming via simulation that can efficiently employ the simulation resource to iteratively solve for the optimal first- A nd second-stage decisions. Specifically, at each visited first-stage decision, we develop a local metamodel to simultaneously solve a set of scenario-based second-stage optimization problems, which also allows us to estimate the optimality gap. Then, we construct a global metamodel accounting for the errors induced by: (1) using a finite number of scenarios to approximate the expected future cost occurring in the planning horizon, (2) second-stage optimality gap, and (3) finite visited first-stage decisions. Assisted by the global-local metamodel, we propose a new simulation optimization approach that can efficiently and iteratively search for the optimal first- A nd second-stage decisions. Our framework can guarantee the convergence of optimal solution for the discrete two-stage optimization with unknown objective, and the empirical study indicates that it achieves substantial efficiency and accuracy. © 2020 ACM.",dynamic decision-making; Gaussian process metamodel; Simulation optimization; stochastic programming; two-stage optimization,Decision making; Iterative methods; Stochastic programming; Dynamic decision making; Empirical studies; First stage decisions; Operational decisions; Optimization problems; Simulation optimization; Simulation resource; Two stage optimizations; Stochastic systems
Green Simulation with Database Monte Carlo,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100972005&partnerID=40&md5=f87278cfb713cf6917d4d3ab16216774,"In a setting in which experiments are performed repeatedly with the same simulation model, green simulation means reusing outputs from previous experiments to answer the question currently being asked of the model. In this article, we address the setting in which experiments are run to answer questions quickly, with a time limit providing a fixed computational budget, and then idle time is available for further experimentation before the next question is asked. The general strategy is database Monte Carlo for green simulation: The output of experiments is stored in a database and used to improve the computational efficiency of future experiments. In this article, the database provides a quasi-control variate, which reduces the variance of the estimated mean response in a future experiment that has a fixed computational budget. We propose a particular green simulation procedure using quasi-control variates, addressing practical issues such as experiment design, and analyze its theoretical properties. We show that, under some conditions, the variance of the estimated mean response in an experiment with a fixed computational budget drops to zero over a sequence of repeated experiments, as more and more idle time is invested in creating databases. Our numerical experiments on the procedure show that using idle time to create databases of simulation output provides variance reduction immediately, and that the variance reduction grows over time in a way that is consistent with the convergence analysis. © 2021 ACM.",control variates; database Monte Carlo; low-discrepancy sequence; low-dispersion sequence; quasi Monte Carlo; simulation budget allocation; Simulation experiment design and analysis,Budget control; Computational efficiency; Database systems; Computational budget; Control variates; Convergence analysis; Experiment design; Numerical experiments; Simulation outputs; Simulation procedures; Variance reductions; Monte Carlo methods
Novel Approaches to Feasibility Determination,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101046445&partnerID=40&md5=30229e4b1e187bf6b10090d0839a2a41,"This article proposes two-stage Bayesian and frequentist procedures for determining whether a number of systems-each characterized by the same number of performance measures-belongs to a set Γ defined by a finite collection of linear inequalities. A system is ""in (not in) Γ""if the vector of the means is in (not in) Γ, where the means must be estimated using Monte Carlo simulation. We develop algorithms for classifying the systems with a user-specified level of confidence using the minimum number of simulation replications so the probability of correct classification over all r systems satisfies a user-specified minimum value. Once the analyst provides prior values for the means and standard deviations of the random variables in each system, an initial number of simulation replications is performed to obtain current estimates of the means and standard deviations to assess whether the systems can be classified with the desired level of confidence. For any system that cannot be classified, heuristics are proposed to determine the number of additional simulation replications that would enable correct classification. Our contributions include the introduction of intuitive algorithms that are not only easy to implement, but also effective with their performance. Compared to other feasibility determination approaches, they also appear to be competitive. While the algorithms were initially developed in settings where system variance is assumed to be known and the random variables are independent, their performance remains satisfactory when those assumptions are relaxed. © 2021 ACM.",classifying system; Multiple performance measures; ranking and selection; stochastic constraints,Random variables; Statistics; Current estimates; Frequentist; Linear inequalities; Minimum value; Performance measure; Probability of correct classifications; Simulation replication; Standard deviation; Monte Carlo methods
Discrete-Event Modeling and Simulation of Diffusion Processes in Multiplex Networks,2021,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101053227&partnerID=40&md5=8a828afa0cf7c899610981588447f44a,"A variety of phenomena (such as the spread of diseases, pollution in rivers, etc.) can be studied as diffusion processes over networks (i.e., the diffusion of the phenomenon over a set of interconnected entities). This research introduces a method to study such diffusion processes in multiplex dynamic networks. We use a formal Modeling and Simulation methodology (in our case, DEVS, Discrete-Event System Specification). We use DEVS formal models to integrate models defined using Agent-Based Modeling and Network Theory. We present (1) an Architecture to study Diffusion Processes in Multiplex dynamic networks (ADPM) and (2) a systematic Process to define, implement, and simulate diffusion processes over such networks. We show a theoretical definition and a concrete implementation of ADPM. We show how to use ADPM and the process in a case study based on a real nuclear emergency plan; this illustrates the application of the process, the architecture, and the developed software. Different scenarios are studied as Diffusion Processes to demonstrate the usability of ADPM. © 2020 ACM.",agent-based modeling; DEVS; Dynamic multiplex networks; formal modeling and simulation; network theory,Application programs; Autonomous agents; Computational methods; Diffusion; Memory architecture; Network architecture; River pollution; Specifications; Agent-based model; Diffusion process; Discrete event models; Discrete event system specification; Multiplex networks; Nuclear emergencies; Spread of disease; Systematic process; Discrete event simulation
Parametric Scenario Optimization under Limited Data,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097354397&doi=10.1145%2f3410152&partnerID=40&md5=a7da1ea960d6a3740bd9b858d7ef5058,"We consider optimization problems with uncertain constraints that need to be satisfied probabilistically. When data are available, a common method to obtain feasible solutions for such problems is to impose sampled constraints following the so-called scenario optimization approach. However, when the data size is small, the sampled constraints may not statistically support a feasibility guarantee on the obtained solution. This article studies how to leverage parametric information and the power of Monte Carlo simulation to obtain feasible solutions for small-data situations. Our approach makes use of a distributionally robust optimization (DRO) formulation that translates the data size requirement into a Monte Carlo sample size requirement drawn from what we call a generating distribution. We show that, while the optimal choice of this generating distribution is the one eliciting the data or the baseline distribution in a nonparametric divergence-based DRO, it is not necessarily so in the parametric case. Correspondingly, we develop procedures to obtain generating distributions that improve upon these basic choices. We support our findings with several numerical examples. © 2020 ACM.",Chance constraint; distributionally robust optimization; parametric uncertainty; scenario optimization,Optimization; Feasible solution; Limited data; Non-parametric; Optimal choice; Optimization problems; Parametric information; Robust optimization; Scenario optimizations; Monte Carlo methods
Knowledge Discovery in Simulation Data,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097351436&doi=10.1145%2f3391299&partnerID=40&md5=348f817f023371c045db646494640317,"This article provides a comprehensive and in-depth overview of our work on knowledge discovery in simulations. Application-wise, we focus on manufacturing simulations. Specifically, we propose and discuss a methodology for designing, executing, and analyzing large-scale simulation experiments with a broad coverage of possible system behavior targeted at generating knowledge about the system. Based on the concept of data farming, we suggest a two-phase process which starts with a data generation phase, in which a smart experiment design is used to set up and efficiently execute a large number of simulation experiments. In the second phase, the knowledge discovery phase, data mining and visually aided analysis methods are applied on the gathered simulation input and output data. This article gives insights into this knowledge discovery phase by discussing different machine learning approaches and their suitability for different manufacturing simulation problems. With this, we provide guidelines on how to conduct knowledge discovery studies within the manufacturing simulation context. We also introduce different case studies, both academic and applied, and use them to validate our methodology. © 2020 ACM.",data farming; data mining; knowledge discovery; Simulation; visual analytics,Data visualization; Manufacture; Data generation; Experiment design; Input and outputs; Large scale simulations; Machine learning approaches; Manufacturing simulation; System behaviors; Two-phase process; Data mining
"Data Farming: Methods for the Present, Opportunities for the Future",2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097350088&doi=10.1145%2f3425398&partnerID=40&md5=0d5081ea6112f73b11ff4491d89e118a,"Data farming is a descriptive metaphor that captures the notion of generating data purposefully to maximize the information ""yield""from simulation models. Large-scale designed experiments let us grow the simulation output efficiently and effectively. We can explore massive input spaces, uncover interesting features of complex simulation response surfaces, and explicitly identify cause-and-effect relationships. Data farming has been used in the defense community over the past two decades, and has resulted in quantum leaps in the breadth, depth, and timeliness of the insights yielded by simulation models. In this article, we provide an overview of current data farming capabilities and their relationship to emerging techniques in data science and analytics. We use graphics to motivate insight into some of the benefits of a data farming approach. Finally, we share some thoughts about opportunities and challenges for further improving the state of the art, and transforming the state of the practice, in the data farming domain. © 2020 Owner/Author.",data mining; design of experiments; metamodeling; Simulation,Data Science; Cause-and-effect relationships; Complex simulation; Current data; Designed experiments; Quantum leaps; Simulation outputs; State of the art; State of the practice; Metadata
Introduction to the Special Issue for towards an Ecosystem of Simulation Models and Data,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097399712&doi=10.1145%2f3425907&partnerID=40&md5=3135bad3a927665260ab951553a5b51b,[No abstract available],,
Hierarchical Gaussian Process Models for Improved Metamodeling,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097334429&doi=10.1145%2f3384470&partnerID=40&md5=390f75cf6428ffb28b7d16a3818c6f3d,"Simulations are often used for the design of complex systems as they allow one to explore the design space without the need to build several prototypes. Over the years, the simulation accuracy, as well as the associated computational cost, has increased significantly, limiting the overall number of simulations during the design process. Therefore, metamodeling aims to approximate the simulation response with a cheap to evaluate mathematical approximation, learned from a limited set of simulator evaluations. Kernel-based methods using stationary kernels are nowadays widely used. In many problems, the smoothness of the function varies in space, which we call nonstationary behavior [20]. However, using stationary kernels for nonstationary responses can be inappropriate and result in poor models when combined with sequential design. We present the application of two recent techniques: Deep Gaussian Processes and Gaussian Processes with nonstationary kernel, which are better able to cope with these difficulties. We evaluate the method for nonstationary regression on a series of real-world problems, showing that these recent approaches outperform the standard Gaussian Processes with stationary kernels. Results show that these techniques are suitable for the simulation community, and we outline the variational inference method for the Gaussian Process with nonstationary kernel. © 2020 ACM.",Gaussian Processes; Machine learning; probabilistic models,Gaussian noise (electronic); Variational techniques; Computational costs; Gaussian process models; Kernel based methods; Non-stationary behaviors; Non-stationary response; Simulation accuracy; Simulation response; Variational inference methods; Gaussian distribution
Inhomogeneous CTMC Birth-and-Death Models Solved by Uniformization with Steady-State Detection,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089341195&doi=10.1145%2f3373758&partnerID=40&md5=bb3d458a8e93c2d8d48ee19088922931,"Time-inhomogeneous queueing models play an important role in service systems modeling. Although the transient solutions of corresponding continuous-time Markov chains (CTMCs) are more precise than methods using stationary approximations, most authors consider their computational costs prohibitive for practical application. This article presents a new variant of the uniformization algorithm that utilizes a modified steady-state detection technique. The presented algorithm is applicable for CTMCs when their stationary solution can be efficiently calculated in advance, particularly for many practically applicable birth-and-death models with limited size. It significantly improves computational efficiency due to an early prediction of an occurrence of a steady state, using the properties of the convergence function of the embedded discrete-time Markov chain. Moreover, in the case of an inhomogeneous CTMC solved in consecutive timesteps, the modification guarantees that the error of the computed probability distribution vector is strictly bounded at each point of the considered time interval.  © 2020 ACM.",abandonment; balking; call center; inhomogeneous CTMC; spectral gap; Uniformization,Computational efficiency; Continuous time systems; Probability distributions; Queueing theory; Birth-and-death model; Computational costs; Continuous time Markov chain; Discrete time Markov chains; Probability distribution vectors; Stationary solutions; Steady-state detections; Transient solutions; Markov chains
Exact Simulation of a Truncated Lévy Subordinator,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089349851&doi=10.1145%2f3391407&partnerID=40&md5=6152addbc1adc14330b22f5909d59451,"A truncated Lévy subordinator is a Lévy subordinator in R+ with Lévy measure restricted from above by a certain level b. In this article, we study the path and distribution properties of this type of process in detail and set up an exact simulation framework based on a marked renewal process. In particular, we focus on a typical specification of truncated Lévy subordinator, namely the truncated stable process. We establish an exact simulation algorithm for the truncated stable process, which is very accurate and efficient. Compared to the existing algorithm suggested in Chi, our algorithm outperforms over all parameter settings. Using the distributional decomposition technique, we also develop an exact simulation algorithm for the truncated tempered stable process and other related processes. We illustrate an application of our algorithm as a valuation tool for stochastic hyperbolic discounting, and numerical analysis is provided to demonstrate the accuracy and effectiveness of our methods. We also show that variations of the result can also be used to sample two-sided truncated Lévy processes, two-sided Lévy processes via subordinating Brownian motions, and truncated Lévy-driven Ornstein-Uhlenbeck processes.  © 2020 ACM.",Brownian motion subordination; exact simulation; Lévy-driven Ornstein-Uhlenbeck process; marked renewal representation; Monte Carlo simulation; stable process; truncated stable process; truncated tempered stable process; two-sided truncated Lévy process,Numerical methods; Stochastic systems; Decomposition technique; Distribution property; Hyperbolic discounting; Ornstein-Uhlenbeck process; Parameter setting; Renewal process; Simulation algorithms; Simulation framework; Stability criteria
ChunkedTejas: A Chunking-based Approach to Parallelizing a Trace-Driven Architectural Simulator,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089348245&doi=10.1145%2f3375397&partnerID=40&md5=abee1802eb5b2ce7c03d46e1665ff6a2,"Research in computer architecture is commonly done using software simulators. The simulation speed of such simulators is therefore critical to the rate of progress in research. One of the less commonly used ways to increase the simulation speed is to decompose the benchmark's execution into contiguous chunks of instructions and simulate these chunks in parallel. Two issues arise from this approach. The first is of correctness, as each chunk (other than the first chunk) starts from an incorrect state. The second is of performance: The decomposition must be done in such a way that the simulation of all chunks finishes at nearly the same time, allowing for maximum speedup. In this article, we study these two aspects and compare three different chunking approaches (two of them are novel) and two warmup approaches (one of them is novel). We demonstrate that average speedups of up to 5.39X can be achieved (while employing eight parallel instances), while constraining the error to 0.2% on average.  © 2020 ACM.",Architectural simulator; parallelization; simulator performance,Computer architecture; Architectural simulators; Parallelizing; Simulation speed; Software simulator; Simulators
Modeling Resources to Simulate Business Process Reliability,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087756691&doi=10.1145%2f3381453&partnerID=40&md5=673e966eb20621494c2d119f7d2fb3fb,"The combination of process modeling and simulation-based analysis provides a quantitative approach to analyze business processes, and to evaluate design alternatives before committing the required resources, to properly align operations with business strategies, improve operational efficiency, and gain competitive advantage. However, the use of simulation-based analysis is still limited in practice, mainly because it does not exploit process modeling standards and typically addresses performance-related properties only, such as time and cost. This article proposes a methodology that first extends the standard language for process modeling (i.e., BPMN) to introduce a flexible and accurate specification of business process resources, and then exploits the extended process specification to analyze and predict the process behavior by use of a simulation approach that takes into account reliability-related properties, to consider unexpected failures of process resources. The simulation-based analysis is implemented by use of a domain-specific process simulation language that preserves the BPMN execution semantics. An example application is introduced to show the importance of addressing both performance and reliability properties for the simulation-based analysis of business processes.  © 2020 ACM.",BPMN; business process reliability; Business process simulation; eBPMN; resource modeling,Competition; Computer simulation languages; Process engineering; Reliability analysis; Semantics; Specifications; Competitive advantage; Operational efficiencies; Performance and reliabilities; Process model standards; Process modeling and simulations; Process specification; Quantitative approach; Simulation-based analysis; Modeling languages
Toward a Theory of Superdense Time in Simulation Models,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089353067&doi=10.1145%2f3379489&partnerID=40&md5=d6181f3b27041d8af64ad71ef60cba1b,"We develop a theory of superdense time that encompasses existing uses of superdense time in discrete event simulations and points to new forms that have not previously been explored. A central feature of our development is a set of axioms for superdense time. The sufficiency of these axioms is demonstrated by using them to prove that a general model of a discrete event simulation procedure, expressed in terms of a mathematical system, constitutes a state transition function. Several forms of superdense time, both known and novel, are shown to satisfy the axioms.  © 2020 Public Domain.",agent/discrete models; discrete-event simulation; modeling methodologies; Systems theory,Functions; General model; New forms; State transition functions; Discrete event simulation
Dimensionally Aware Multi-Objective Genetic Programming for Automatic Crowd Behavior Modeling,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089353982&doi=10.1145%2f3403635&partnerID=40&md5=01a4a66152924ea173249eee0daf88e7,"One limitation of current data-driven automatic crowd modeling methods is that the models generated have low interpretability, which limits the practical applications of the models. In this article, we propose a new data-driven crowd modeling approach that can generate universal behavior rules with better interpretability. Higher interpretability helps people better understand and analyze the rules. Furthermore, the proposed approach considers both static and dynamic features during modeling to generate a realistic crowd, based on the assumption that humans tend to consider different features with respect to their states. In the proposed method, the automatic behavior rule generation problem is formulated as a symbolic regression problem. Then, the problem is solved by multi-objective genetic programming. On one hand, to improve the interpretability of the behavior rules found, a new mechanism is proposed to guide the algorithm to find concise and dimensionally consistent solutions. On the other hand, decisions made by considering static and dynamic features respectively are combined to improve the generated crowd realism. To validate the effectiveness of the proposed method, three real-world datasets are utilized for training and testing. The simulation results demonstrate that the proposed method is able to find universal behavior rules that are competitive to previous work in accuracy while having better interpretability.  © 2020 ACM.",Agent-based modeling; crowd simulation; decision rules; evolutionary algorithm; gene expression programming,Genetic algorithms; Genetic programming; Dynamic features; Interpretability; Multi objective; New mechanisms; Real-world datasets; Symbolic regression problems; Training and testing; Universal behaviors; Behavioral research
Risk quantification in stochastic simulation under input uncertainty,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079487161&doi=10.1145%2f3329117&partnerID=40&md5=270d0e53c68a33477e07864150db7d01,"When simulating a complex stochastic system, the behavior of output response depends on input parameters estimated from finite real-world data, and the finiteness of data brings input uncertainty into the system. The quantification of the impact of input uncertainty on output response has been extensively studied. Most of the existing literature focuses on providing inferences on the mean response at the true but unknown input parameter, including point estimation and confidence interval construction. Risk quantification of mean response under input uncertainty often plays an important role in system evaluation and control, because it provides inferences on extreme scenarios of mean response in all possible input models. To the best of our knowledge, it has rarely been systematically studied in the literature. In this article, first we introduce risk measures of mean response under input uncertainty and propose a nested Monte Carlo simulation approach to estimate them. Then we develop asymptotical properties such as consistency and asymptotic normality for the proposed nested risk estimators. We further study the associated budget allocation problem for efficient nested risk simulation and finally use a sharing economy example to illustrate the importance of accessing and controlling risk due to input uncertainty. © 2020 Association for Computing Machinery.",Budget allocation; Input uncertainty; Monte Carlo simulation; Nested risk estimators; Risk quantification,Budget control; Intelligent systems; Monte Carlo methods; Parameter estimation; Risk assessment; Stochastic control systems; Stochastic models; Stochastic systems; Uncertainty analysis; Asymptotic normality; Asymptotical properties; Budget allocation; Complex stochastic systems; Input uncertainty; Risk estimators; Risk quantification; Stochastic simulations; Risk perception
Enhancing response predictions with a joint Gaussian process model for stochastic simulation models,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079508272&doi=10.1145%2f3364219&partnerID=40&md5=b3de1765b27a287b7187da95cc1514c0,"The stochastic Gaussian process model has been widely used in stochastic simulation metamodeling. In practice, the performance of this model can be largely affected by the noise in the observations. In this article, we propose an approach to mitigate the impact of the noisy observations by jointly modeling the response of interest with a correlated but less-noisy auxiliary response. The main idea is to leverage on and learn from the correlated and more accurate response to improve the prediction. To achieve this, we extend the existing deterministic multi-response model for stochastic simulation to jointly model the two responses, use some simplified examples to show the benefit of the proposed model, and investigate the input estimation of this model. Quantile prediction is used to illustrate the efficiency of the proposed approach by jointly modeling it with the expectation, which typically has a less noisy estimator compared with that of the quantile. Several numerical examples are then conducted, and the results show that the joint model can provide better performance. These promising results illustrate the potential of this joint model especially in situations where the response of interest is much noisier or when observations are scarce. We further propose a two-stage design approach based on the multi-response model to more efficiently utilize limited computing budget to improve predictions. We also see from these designs the benefits of the joint model, where the more accurate auxiliary response observations can be used to improve the response of interest. © 2020 Association for Computing Machinery.",Multi-response simulation metamodeling; Quantile prediction,Budget control; Forecasting; Gaussian distribution; Gaussian noise (electronic); Stochastic systems; Accurate response; Gaussian process models; Metamodeling; Noisy observations; Response prediction; Stochastic simulation model; Stochastic simulations; Two stage designs; Stochastic models
An integrated method for simultaneous calibration and parameter selection in computer models,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079507500&doi=10.1145%2f3364217&partnerID=40&md5=98ccb0fdd0cc8cc3a43e781ed1e14c13,"For many large and complex computer models, there usually exist a large number of unknown parameters. To improve the computer model's predictive performance for more reliable and confident decision making, two important issues have to be addressed. The first is to calibrate the computer model and the second is to select the most influential set of parameters. However, these two issues are often addressed independently, which may waste computational effort. In this article, a Gaussian process-based Bayesian method is first proposed to simultaneously calibrate and select parameters in stochastic computer models. The whole procedure can be conducted more efficient by sharing the data information between these two steps. To further ease the computational burden, an approximation approach based on a weighted normal approximation is proposed to evaluate the posteriors in the proposed Bayesian method. A sequential approximation procedure is further proposed to improve the approximation accuracy by allocating the sequential design points more appropriately. The efficiency and accuracy of the proposed approaches are compared in a building energy model and a pandemic influenza simulation model. © 2020 Association for Computing Machinery.",Gaussian process; Model calibration; Parameter selection; Posterior approximation; Sequential design,Bayesian networks; Data Sharing; Decision making; Energy efficiency; Gaussian distribution; Gaussian noise (electronic); Stochastic systems; Gaussian Processes; Model calibration; Parameter selection; Posterior approximation; Sequential design; Stochastic models
Fidelity and Performance of State Fast-forwarding in Microscopic Traffic Simulations,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102071173&doi=10.1145%2f3366019&partnerID=40&md5=8f9e5737e6d27d21c3961a0c0f08a8c0,"Common car-following models for microscopic traffic simulation assume a time advancement using fixed-sized time steps. However, a purely time-driven execution is inefficient when the states of some agents are independent of other agents and thus predictable far into the simulated future. We propose a method to accelerate microscopic traffic simulations based on identifying independence among agent state updates. Instead of iteratively updating an agent's state throughout a sequence of time steps, a computationally inexpensive ""fast-forward""function advances the agent's state to the time of its earliest possible interaction with other agents. We present an algorithm to determine independence intervals in microscopic traffic simulations and derive fast-forward functions for several well-known traffic models. In contrast to existing approaches based on reducing the level of detail, our approach retains the microscopic nature of the simulation. An evaluation is performed for a synthetic scenario and on the road network of Singapore. At low traffic densities, maximum speedup factors of about 2.6 and 1.6 are achieved, while at the highest considered densities, only few opportunities for fast-forwarding exist. We show that the deviation from purely time-driven execution is reduced to a minimum when choosing an adequate numerical integration scheme to execute the time-driven updates. Verification results show that the overall deviation in vehicle travel times is marginal.  © 2020 ACM.",approach; Fidelity; microscopic traffic simulation; performance,Iterative methods; Travel time; Car following models; Fast forwarding; Level of detail; Microscopic traffic simulation; Numerical integration scheme; Speed-up factors; Traffic densities; Verification results; Traffic control
Generalized probabilistic bisection for stochastic root finding,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079504164&doi=10.1145%2f3355607&partnerID=40&md5=69208791b7b5f6a67b87d486fc83cb50,"We consider numerical schemes for root finding of noisy responses through generalizing the Probabilistic Bisection Algorithm (PBA) to the more practical context where the sampling distribution is unknown and location dependent. As in standard PBA, we rely on a knowledge state for the approximate posterior of the root location. To implement the corresponding Bayesian updating, we also carry out inference of oracle accuracy, namely learning the probability of the correct response. To this end we utilize batched querying in combination with a variety of frequentist and Bayesian estimators based on majority vote, as well as the underlying functional responses, if available. For guiding sampling selection we investigate both entropy-directed sampling and quantile sampling. Our numerical experiments show that these strategies perform quite differently; in particular, we demonstrate the efficiency of randomized quantile sampling, which is reminiscent of Thompson sampling. Our work is motivated by the root-finding subroutine in pricing of Bermudan financial derivatives, illustrated in the last section of the article. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Quantile sampling; Sequential design; Stochastic root-finding,Stochastic systems; Bayesian estimators; Bisection algorithms; Financial derivatives; Numerical experiments; Root finding; Sampling distribution; Sampling selections; Sequential design; Probability distributions
Parallel data distribution management on shared-memory multiprocessors,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079490507&doi=10.1145%2f3369759&partnerID=40&md5=a86af4307dd8347c99be55c9ee12fc5f,"The problem of identifying intersections between two sets of d-dimensional axis-parallel rectangles appears frequently in the context of agent-based simulation studies. For this reason, the High Level Architecture (HLA) specification-a standard framework for interoperability among simulators-includes a Data Distribution Management (DDM) service whose responsibility is to report all intersections between a set of subscription and update regions. The algorithms at the core of the DDM service are CPU-intensive, and could greatly benefit from the large computing power of modern multi-core processors. In this article, we propose two parallel solutions to the DDM problem that can operate effectively on shared-memory multiprocessors. The first solution is based on a data structure (the interval tree) that allows concurrent computation of intersections between subscription and update regions. The second solution is based on a novel parallel extension of the Sort Based Matching algorithm, whose sequential version is considered among the most efficient solutions to the DDM problem. Extensive experimental evaluation of the proposed algorithms confirm their effectiveness on taking advantage of multiple execution units in a shared-memory architecture. © 2020 Association for Computing Machinery.",Data distribution management (DDM); High level architecture (HLA); Parallel algorithms; Parallel and distributed simulation (PADS),Concurrency control; Interoperability; Memory architecture; Multiprocessing systems; Parallel algorithms; Trees (mathematics); Axis parallel rectangles; Data distribution management; Experimental evaluation; High level architecture; Parallel and distributed simulation; Shared memory architecture; Shared memory multiprocessor; Sort-based matching algorithms; Information management
Editorial from the new editor-in-chief,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079494327&doi=10.1145%2f3377148&partnerID=40&md5=96b2218fbb46effaf433eda9fb10130a,[No abstract available],,
Extending explicitly modelled simulation debugging environments with dynamic structure,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079478704&doi=10.1145%2f3338530&partnerID=40&md5=f120aadedab1ec0a1ec3358613592b71,"The widespread adoption of Modelling and Simulation (M&S) techniques hinges on the availability of tools supporting each phase in the M&S-based workflow. This includes tasks such as specifying, implementing, experimenting with, as well as debugging, simulation models. We have previously developed a technique where advanced debugging environments are generated from an explicit behavioural model of the user interface and the simulator. These models are extracted from the code of existing modelling environments and simulators and instrumented with debugging operations. This technique can be reused for a large family of modelling formalisms but was not yet considered for dynamic-structure formalisms; debugging models in these formalisms is challenging, as entities can appear and disappear during simulation. In this article, we adapt and apply our approach to accommodate dynamic-structure formalisms. To this end, we present a modular, reusable approach, which includes an architecture and a workflow. We observe that to effectively debug dynamic-structure models, domain-specific visualizations developed by the modeller should be (re)used for debugging tasks. To demonstrate our technique, we use Dynamic-Structure DEVS (a formalism that includes the characteristics of discrete-event and agent-based modelling paradigms) and an implementation of its simulation semantics in the PythonPDEVS tool as a running example. We apply our technique on NetLogo, a popular multi-agent simulation tool, to demonstrate the generality of our approach. © 2020 Association for Computing Machinery.",Debugging; Dynamic-structure formalisms; Experimentation interfaces; Visual modelling,Autonomous agents; Computational methods; Computer debugging; Multi agent systems; Semantics; User interfaces; Agent-based modelling; Behavioural model; Dynamic structure; Dynamic structure models; Modelling and simulations; Modelling environment; Multi agent simulation; Visual modelling; Discrete event simulation
Omnithermal perfect simulation for multi-server queues,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079484146&doi=10.1145%2f3361743&partnerID=40&md5=f220db7121bba84556566f703dd8c89d,"A number of perfect simulation algorithms for multi-server First Come First Served queues have recently been developed. Those of Connor and Kendall [6] and Blanchet et al. [4] use dominated Coupling from the Past (domCFTP) to sample from the equilibrium distribution of the Kiefer-Wolfowitz workload vector for stable M/G/c and GI/GI/c queues, respectively, using Random Assignment queues as dominating processes. In this article, we answer a question posed by Connor and Kendall [6] by demonstrating how these algorithms may be modified to carry out domCFTP simultaneously for a range of values of c (the number of servers). © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Dominated coupling from the past; First come first served discipline; Kiefer-Wolfowitz workload vector; M/G/c queue; Perfect simulation; Random assignment discipline; Sandwiching; Stochastic ordering,Stochastic systems; Coupling from the past; First come first served; M/g/c queues; Perfect simulation; Random assignment; Sandwiching; Stochastic order; Queueing theory
The Square Root Rule for Adaptive Importance Sampling,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102056419&doi=10.1145%2f3350426&partnerID=40&md5=39f9017e6fe7c5e9f10f33d89c74bfed,"In adaptive importance sampling and other contexts, we have K > 1 unbiased and uncorrelated estimates μk of a common quantity μ. The optimal unbiased linear combination weights them inversely to their variances, but those weights are unknown and hard to estimate. A simple deterministic square root rule based on a working model that Var(μk) ∝ k-1/2 gives an unbiased estimate of μ that is nearly optimal under a wide range of alternative variance patterns. We show that if Var(μk) ∝ k-y for an unknown rate parameter y ∈ [0,1], then the square root rule yields the optimal variance rate with a constant that is too large by at most 9/8 for any 0 ≤ y ≤ 1 and any number K of estimates. Numerical work shows that rule is similarly robust to some other patterns with mildly decreasing variance as k increases.  © 2020 ACM.",Graphics; particle transport; rare events,Value engineering; Adaptive importance sampling; Linear combinations; Rate parameters; Rule based; Square roots; Unbiased estimates; Variance rate; Working models; Importance sampling
Simulation Study to Identify the Characteristics of Markov Chain Properties,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102031225&doi=10.1145%2f3361744&partnerID=40&md5=eb3df23caf202905f9c3f9a3c6d99c19,"Markov models have a long tradition in modeling and simulation of dynamic systems. In this article, we look at certain properties of a discrete-time Markov chain, including entropy, trace, and second-largest eigenvalue to better understand their role for time-series analysis. We simulate a number of possible input signals, fit a discrete-time Markov chain, and explore properties with the help of Sobol indices, partial correlation coefficients, and the Morris elementary effect screening method. Our analysis suggests that the presence of a trend, periodicity, and autocorrelation impact entropy, trace, and second-largest eigenvalue to varying degrees but not independently of each other and with Markov chain parameter settings as other influencing factors. The properties of interest are promising to distinguish time-series data, as evidenced for the entropy measure by recent results in the analysis of cell development for Xenopus laevis in cell biology.  © 2020 ACM.",discrete-time Markov chain (DTMC); eigenvalue; entropy; Monte Carlo simulation; Morris method; partial correlation coefficients (PCC); sensitivity analysis; Sobol indices; trace,Cytology; Eigenvalues and eigenfunctions; Entropy; Time series analysis; Trace analysis; Cell development; Discrete time Markov chains; Model and simulation; Parameter setting; Partial correlation; Screening methods; Second largest eigenvalue; Simulation studies; Markov chains
An Adaptive Persistence and Work-stealing Combined Algorithm for Load Balancing on Parallel Discrete Event Simulation,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102047567&doi=10.1145%2f3364218&partnerID=40&md5=3d2ff749f69bdaf0fabb23f647ece55c,"Load imbalance has always been a crucial challenge in Parallel Discrete Event Simulation (PDES). In the past few years, we have witnessed an increased interest in using multithreading PDES on multi/many-core platforms. In multithreading PDES, migrating logical processes and coordinating threads are more convenient and cause lower overhead, which provides a better circumstance for load balancing. However, current algorithms, including the persistence-based scheme and work-stealing-based scheme, have their drawbacks. On one hand, persistence-based load balancers, which use the historical data to predict the future, will inevitably make some error. On the other hand, the work-stealing scheme ignores the application-related characteristic, which may limit the potential performance improvement. In this article, we propose an adaptive persistence and work-stealing combined dynamic load balancing algorithm (APWS). The algorithm detects load imbalance, adaptively rebalances the distribution of logical processes, and uses a greedy lock-free work-stealing scheme to eliminate bias at runtime. We assess the performance of the APWS algorithm by a series of experiments. Results demonstrate that our APWS algorithm achieves better performance in different scenarios.  © 2020 ACM.",load balance; Parallel discrete event simulation; work-stealing,Multitasking; Combined algorithms; Dynamic load balancing algorithms; Historical data; Load balancer; Load imbalance; Logical process; Multi-threading; Parallel discrete event simulations; Discrete event simulation
Editorial to the Special Issue on the Principles of Advanced Discrete Simulation (PADS),2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102060374&doi=10.1145%2f3381903&partnerID=40&md5=a5235f03dbb57a528e5b33fc6a1e9378,[No abstract available],,
A distributed shared memory middleware for speculative parallel discrete event simulation,2020,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084614152&doi=10.1145%2f3373335&partnerID=40&md5=cc41255208f423cf58091464cbe2cda0,"The large diffusion of multi-core machines has pushed the research in the field of Parallel Discrete Event Simulation (PDES) toward new programming paradigms, based on the exploitation of shared memory. On the opposite side, the advent of Cloud computing-and the possibility to group together many (low-cost) virtual machines to form a distributed memory cluster capable of hosting simulation applications-has raised the need to bridge shared memory programming and seamless distributed execution. In this article, we present the design of a distributed middleware that transparently allows a PDES application coded for shared memory systems to run on clusters of (Cloud) resources. Our middleware is based on a synchronization protocol called Event and Cross State Synchronization. It allows cross-simulation-object access by event handlers, thus representing a powerful tool for the development of various types of PDES applications. We also provide data for an experimental assessment of our middleware architecture, which has been integrated into the open source ROOT-Sim speculative PDES platform. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Distributed shared memory; Memory consistency; Parallel discrete event simulation; Simulation state; Virtual time,Cluster computing; Discrete event simulation; Middleware; Multicore programming; Open source software; Distributed memory clusters; Distributed shared memory; Experimental assessment; Middleware architecture; Parallel discrete event simulations; Shared memory programming; Simulation applications; Synchronization protocols; Memory architecture
Statistical abstraction for multi-scale spatio-temporal systems,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076814975&doi=10.1145%2f3366023&partnerID=40&md5=5b4384959e24578c49c20fd7b2d7493f,"Modelling spatio-temporal systems exhibiting multi-scale behaviour is a powerful tool in many branches of science, yet it still presents significant challenges. Here, we consider a general two-layer (agent-environment) modelling framework, where spatially distributed agents behave according to external inputs and internal computation; this behaviour may include influencing their immediate environment, creating a medium over which agent-agent interaction signals can be transmitted. We propose a novel simulation strategy based on a statistical abstraction of the agent layer, which is typically the most detailed component of the model and can incur significant computational cost in simulation. The abstraction makes use of Gaussian Processes, a powerful class of non-parametric regression techniques from Bayesian Machine Learning, to estimate the agent's behaviour given the environmental input. We show on two biological case studies how this technique can be used to speed up simulations and provide further insights into model behaviour. Copyright © 2019 held by the owner/author(s).",Agent-based; Coarsening; Multi-scale systems; Spatio-temporal; Statistical abstraction,Abstracting; Bioinformatics; Coarsening; Agent based; Immediate environment; Multi-scale system; Non-parametric regression; Simulation strategies; Spatio temporal; Spatio-temporal system; Statistical abstraction; Multi agent systems
"Replicated computations results (RCR) Report for ""statistical abstraction for multi-scale spatio-t Systems""",2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076814425&doi=10.1145%2f3341094&partnerID=40&md5=137d30bd2629ada630520cd4f8741ade,"""Statistical abstraction for multi-scale spatio-temporal systems"" proposes a methodology that supports analysis of large-scaled spatio-temporal systems. These are represented via a set of agents whose behaviour depends on a perceived field. The proposed approach is based on a novel simulation strategy based on a statistical abstraction of the agents. The abstraction makes use of Gaussian Processes, a powerful class of nonparametric regression techniques from Bayesian Machine Learning, to estimate the agent's behaviour given the environmental input. The authors use two biological case studies to showhowthe proposed technique can be used to speed up simulations and provide further insights into model behaviour. This replicated computation results report focuses on the scripts used in the paper to perform such analysis. The required software was straightforward to install and use. All the experimental results from the paper have been reproduced. ©2019 Association for Computing Machinery.",Agent-based; Multi-scale systems; RCR report; Spatio-temporal; Statistical abstractionCoarsening,Abstracting; Bioinformatics; Agent based; Multi-scale system; RCR report; Spatio temporal; Statistical abstractionCoarsening; Multi agent systems
Analysis of spatio-temporal properties of stochastic systems using TSTL,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076821409&doi=10.1145%2f3326168&partnerID=40&md5=abf62c838283ac13001a5d5a371a6d82,"In this article,we present Three-Valued spatio-temporal Logic (TSTL), which enriches the available spatiotemporal analysis of properties expressed in Signal spatio-temporal Logic (SSTL), to give further insight into the dynamic behavior of systems. Our novel analysis starts from the estimation of satisfaction probabilities of given SSTL properties and allows the analysis of their temporal and spatial evolution. Moreover, in our verification procedure, we use a three-valued approach to include the intrinsic and unavoidable uncertainty related to the simulation-based statistical evaluation of the estimates; this can be also used to assess the appropriate number of simulations to use depending on the analysis needs. We present the syntax and three-valued semantics of TSTL and specific extended monitoring algorithms to check the validity of TSTL formulas. We introduce a reliability requirement for TSTL monitoring and an automatic procedure to verify it. Two case studies demonstrate how TSTL broadens the application of spatio-temporal logics in realistic scenarios, enabling analysis of threat monitoring and privacy preservation based on spatial stochastic population models. © 2019 Association for Computing Machinery. .",Multi-valued logics; Spatio-temporal logics; Statistical Model Checking; Stochastic spatial population models,Many valued logics; Model checking; Population statistics; Semantics; Stochastic systems; Temporal logic; Uncertainty analysis; Multi-valued; Population model; Reliability requirements; Spatio temporal; Spatio-temporal properties; Spatiotemporal analysis; Statistical model checking; Temporal and spatial evolutions; Stochastic models
Mean-payoff optimization in continuous-time markov chains with parametric alarms,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075548712&doi=10.1145%2f3310225&partnerID=40&md5=fa7b5fcd6f74dbdfd918cb33d3b2cd3a,"Continuous-timeMarkov chains with alarms (ACTMCs) allowfor alarm events that can be non-exponentially distributed. Within parametric ACTMCs, the parameters of alarm-event distributions are not given explicitly and can be the subject of parameter synthesis. In this line, an algorithm is presented that solves the å-optimal parameter synthesis problem for parametric ACTMCs with long-run average optimization objectives. The approach provided in this article is based on a reduction of the problem to finding long-run average optimal policies in semi-Markov decision processes (semi-MDPs) and sufficient discretization of the parameter (i.e., action) space. Since the set of actions in the discretized semi-MDP can be very large, a straightforward approach based on an explicit action-space construction fails to solve even simple instances of the problem. The presented algorithm uses an enhanced policy iteration on symbolic representations of the action space. Soundness of the algorithm is established for parametric ACTMCs with alarm-event distributions that satisfy four mild assumptions, fulfilled by many kinds of distributions. Exemplifying proofs for the satisfaction of these requirements are provided for Dirac, uniform, exponential, Erlang, andWeibull distributions in particular. An experimental implementation shows that the symbolic technique substantially improves the efficiency of the synthesis algorithm and allows us to solve instances of realistic size. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Generalized semi-Markov processes; Markov regenerative processes; Non-Markovian distributions; Parameter synthesis; Semi-Markov decision processes,Alarm systems; Continuous time systems; Iterative methods; Markov processes; Generalized Semi-Markov Processes; Markov regenerative process; Non-Markovian; Parameter synthesis; Semi-Markov decision process; Parameter estimation
Integrating simulation and numerical analysis in the evaluation of generalized stochastic petri nets,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075550203&doi=10.1145%2f3321518&partnerID=40&md5=3541f4dad270db5d3975e42ab0769ea8,"The standard existing performance evaluation methods for discrete-state stochastic models such as Petri nets either generate the reachability graph followed by a numerical solution of equations or use some variant of simulation. Both methods have characteristic advantages and disadvantages depending on the size of the reachability graph and type of performance measure. This article proposes a hybrid performance evaluation algorithm for the steady-state solution of Generalized Stochastic Petri Nets that integrates elements of both methods. It automatically adapts its behavior depending on the available size of main memory and number of model states. As such, the algorithm unifies simulation and numerical analysis in a joint framework. It is proved to result in an unbiased estimator whose variance tends to zero with increasing simulation time. The article extends earlier results with an algorithm variant that starts with a small maximum number of particles and increases them during the run to increase the efficiency in cases that are rapidly solved by regular simulation. The algorithm's applicability is demonstrated through case studies, including an example where it outperforms the standard methods. © 2019 Copyright held by the owner/author(s).",Generalized stochastic Petri net; Markov chain; Numerical analysis; Simulation,Markov processes; Numerical analysis; Numerical methods; Particle size analysis; Petri nets; Random access storage; Stochastic systems; Evaluation algorithm; Evaluation methods; Generalized Stochastic Petri nets; Performance measure; Reachability graphs; Simulation; Steady state solution; Unbiased estimator; Stochastic models
RCR report for analysis of spatiotemporal properties of stochastic systems using TSTL,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075552078&doi=10.1145%2f3341093&partnerID=40&md5=5dc1c5361982f4645f57d24735c7dc23,"""Analysis of Spatiotemporal Properties of Stochastic Systems Using TSTL"" [1] proposes a three-valued spatiotemporal logic to enrich the analysis framework for Signal Spatiotemporal Logic previously developed by the authors. This allows one to reason on the evolution of the satisfaction of properties expressed in a spatiotemporal logic, providing additional insight on the behavior of the studied system. The approach has been validated on two case studies: the fire spread and evacuation models originally presented in [2], and a novel case study on privacy in a communication network. This replicated computation result report focuses on the artifact accompanying the article, consisting in a prototypical tool implementation of the techniques presented in the article, together with all files necessary to replicate the analysis performed thereof. The artifact is available at https://ludovicalv.github.io/TOMACS/. After a few iterations with the authors, I found that the artifact agrees with the guidelines on availability (Artifact Avaliable) and replicability (Results Replicated) dictated in https://www.acm.org/publications/policies/artifact-review-badging. The software was made available in an accessible archival repository, and thanks to the instructions provided in the accompanying webapge, it has been straightforward to replicate the experimental results from the article. © 2019 Copyright held by the owner/author(s).",Multivalued logics; RCR replicated computation result report; Spatio-temporal logics,Computer circuits; Fires; HTTP; Stochastic systems; Analysis frameworks; Case-studies; Evacuation models; RCR replicated computation result report; Replicability; Spatio temporal; Spatio-temporal properties; Three-valued; Many valued logics
Introduction to the special issue on quest 2017,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075563622&doi=10.1145%2f3363784&partnerID=40&md5=81944334cf2547c709c8fcfcea7732cd,[No abstract available],,
Interval Markov decision processes with multiple objectives: From Robust strategies to Pareto curves,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075547597&doi=10.1145%2f3309683&partnerID=40&md5=a59e532ce0027139f0a5849f5f54b44b,"Accurate Modelling of a real-world system with probabilistic behaviour is a difficult task. Sensor noise and statistical estimations, among other imprecisions, make the exact probability values impossible to obtain. In this article, we consider Interval Markov decision processes (IMDPs), which generalise classical MDPs by having interval-valued transition probabilities. They provide a powerful modelling tool for probabilistic systems with an additional variation or uncertainty that prevents the knowledge of the exact transition probabilities. We investigate the problem of robust multi-objective synthesis for IMDPs and Pareto curve analysis of multi-objective queries on IMDPs. We study how to find a robust (randomised) strategy that satisfies multiple objectives involving rewards, reachability, and more general ω-regular properties against all possible resolutions of the transition probability uncertainties, as well as to generate an approximate Pareto curve providing an explicit view of the trade-offs between multiple objectives. We show that the multi-objective synthesis problem is PSPACE-hard and provide a value iteration-based decision algorithm to approximate the Pareto set of achievable points. We finally demonstrate the practical effectiveness of our proposed approaches by applying them on several case studies using a prototype tool. © 2019 Copyright held by the owner/author(s).",Complexity; Interval Markov decision processes; Multi-objective optimisation; Pareto curves; Robust synthesis,Economic and social effects; Iterative methods; Markov processes; Multiobjective optimization; Approximate pareto curve; Complexity; Markov Decision Processes; Multi-objective synthesis; Pareto curve; Robust synthesis; Statistical estimation; Transition probabilities; Uncertainty analysis
Data-driven model-based detection of malicious insiders via physical access logs,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075578046&doi=10.1145%2f3309540&partnerID=40&md5=c9eeebbdec984b1476f3ded7fbd35cc2,"The risk posed by insider threats has usually been approached by analyzing the behavior of users solely in the cyber domain. In this article, we show the viability of using physical movement logs, collected via a building access control system, together with an understanding of the layout of the building housing the system's assets, to detect malicious insider behavior that manifests itself in the physical domain. In particular, we propose a systematic framework that uses contextual knowledge about the system and its users, learned from historical data gathered from a building access control system, to select suitable models for representing movement behavior. We suggest two different models of movement behavior in this article and evaluate their ability to represent normal user movement. We then explore the online usage of the learned models, together with knowledge about the layout of the building being monitored, to detect malicious insider behavior. Finally, we show the effectiveness of the developed framework using real-life data traces of usermovement in railway transit stations. © 2019 Copyright held by the owner/author(s).",Intrusion detection; Physical movement; Railway transportation system,Control systems; Intrusion detection; Railroads; Contextual knowledge; Data-driven model; Historical data; Malicious insiders; Movement behavior; Physical movements; Railway transportation; Systematic framework; Access control
Sequential schemes for frequentist estimation of properties in statistical model checking,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075585415&doi=10.1145%2f3310226&partnerID=40&md5=addfa2d11b2f00ba124da8a6465ddbbc,"Statistical Model Checking (SMC) is an approximate verification method that overcomes the state space explosion problem for probabilistic systems by Monte Carlo simulations. Simulations might, however, be costly if many samples are required. It is thus necessary to implement efficient algorithms to reduce the sample size while preserving precision and accuracy. In the literature, some sequential schemes have been provided for the estimation of property occurrence based on predefined confidence and absolute or relative error. Nevertheless, these algorithms remain conservative and may result in huge sample sizes if the required precision standards are demanding. In this article, we compare some useful bounds and some sequential methods. We propose outperforming and rigorous alternative schemes based on Massart bounds and robust confidence intervals. Our theoretical and empirical analyses show that our proposal reduces the sample size while providing the required guarantees on error bounds. © 2019 Copyright held by the owner/author(s).",Absolute and relative errors; Approximate and exact confidence intervals; Chernoff and Massart bounds; Monte Carlo simulations; Sampling size; Statistical model checking,Error analysis; Intelligent systems; Monte Carlo methods; Sampling; Approximate verification; Chernoff and Massart bounds; Estimation of properties; Exact confidence interval; Probabilistic systems; Relative errors; State-space explosion; Statistical model checking; Model checking
New performance modeling methods for parallel data processing applications,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068194591&doi=10.1145%2f3309684&partnerID=40&md5=e41dc19cdcce95cb428d12b1e8d5cbd3,"Predicting the performance of an application running on parallel computing platforms is increasingly becoming important because of its influence on development time and resource management. However, predicting the performance with respect to parallel processes is complex for iterative and multi-stage applications. This research proposes a performance approximation approach FiM to predict the calculation time with FiM-Cal and communication time with FiM-Com of an application running on a distributed framework. FiM-Cal consists of two key components that are coupled with each other: (1) a Stochastic Markov Model to capture non-deterministic runtime that often depends on parallel resources, e.g., number of processes, and (2) a machine-learning model that extrapolates the parameters for calibrating our Markov model when we have changes in application parameters such as dataset. Along with the parallel calculation time, parallel computing platforms consume some data transfer time to communicate among different nodes. FiM-Com consists of a simulation queuing model to quickly estimate communication time. Our new modeling approach considers different design choices along multiple dimensions, namely (i) process-level parallelism, (ii) distribution of cores on multi-processor platform, (iii) application related parameters, and (iv) characteristics of datasets. The major contribution of our prediction approach is that FiM can provide an accurate prediction of parallel processing time for the datasets that have a much larger size than that of the training datasets. We evaluate our approach with NAS Parallel Benchmarks and real iterative data processing applications. We compare the predicted results (e.g., end-to-end execution time) with actual experimental measurements on a real distributed platform. We also compare our work with an existing prediction technique based on machine learning. We rank the number of processes according to the actual and predicted results from FiM and calculate the correlation between the actual and predicted rankings. Our results show that FiM obtains a high correlation in the range of 0.80 to 0.99, which indicates considerable accuracy of our technique. Such prediction provides data analysts a useful insight of optimal configuration of parallel resources (e.g., number of processes and number of cores) and also helps system designers to investigate the impact of changes in application parameters on system performance. © 2019 Association for Computing Machinery.",Communication network; Distributed systems; Execution time; Markov model; Parallel calculation; Performance modeling; Prediction; Queuing theory,Computation theory; Data handling; Data transfer; Distributed computer systems; Forecasting; Iterative methods; Machine components; Machine learning; Markov processes; Queueing theory; Stochastic models; Stochastic systems; Telecommunication networks; Distributed systems; Execution time; Markov model; Parallel calculation; Performance Model; Queuing theory; Benchmarking
Keddah: Network evaluation powered by simulating distributed application traffic,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068186608&doi=10.1145%2f3301503&partnerID=40&md5=a07fd592fc00f3790e61bc98f2b2488a,"As a distributed system, Hadoop heavily relies on the network to complete data-processing jobs. While the traffic generated by Hadoop jobs is critical for job execution performance, the actual behaviour of Hadoop network traffic is still poorly understood. This lack of understanding greatly complicates research relying on Hadoop workloads. In this article, we explore Hadoop traffic through empirical traces. We analyse the generated traffic of multiple types of MapReduce jobs, with varying input sizes, and cluster configuration parameters. We present Keddah, a toolchain for capturing, modelling, and reproducing Hadoop traffic, for use with network simulators to better capture the behaviour of Hadoop. By imitating the Hadoop traffic generation process and considering the YARN resource allocation, Keddah can be used to create Hadoop traffic workloads, enabling reproducible Hadoop research in more realistic scenarios. © 2019 Association for Computing Machinery.",Hadoop network traffic; Network behaviour of distributed applications; Network traffic simulation,Computer applications; Computer simulation; Cluster configurations; Distributed applications; Distributed systems; Network evaluation; Network simulators; Network traffic; Realistic scenario; Traffic generation; Data handling
Infinite Swapping using IID Samples,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068150161&doi=10.1145%2f3317605&partnerID=40&md5=0d19899735fc36d0dd05bcb9397651ad,"We propose a new method for estimating rare event probabilities when independent samples are available. It is assumed that the underlying probability measures satisfy a large deviation principle with a scaling parameter ε that we call temperature. We show how by combining samples at different temperatures, one can construct an estimator with greatly reduced variance. Although as presented here the method is not as broadly applicable as other rare event simulation methods, such as splitting or importance sampling, it does not require any problem-dependent constructions. © 2019 Association for Computing Machinery.",Large deviation principle; Monte Carlo method; Parallel tempering; Rare event simulation,Importance sampling; Independent samples; Large deviation principle; Parallel tempering; Probability measures; Rare event probabilities; Rare event simulation; Scaling parameter; Monte Carlo methods
Exact-differential simulation: Differential processing of large-scale discrete event simulations,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068155053&doi=10.1145%2f3301499&partnerID=40&md5=ef9634595ea99b13b5267482fd7e7340,"Using computer simulation to analyze large-scale discrete event systems requires repeated executions with various scenarios or parameters. Such repeated executions can induce significant redundancy in event processing when the modification from a prior scenario to a new scenario is relatively minor, and when the altered scenario influences only a small part of the simulation. For example, in a city-scale traffic simulation, an altered scenario of blocking one junction may only affect a small part of the city for considerable length of time. However, traditional simulation approaches would still repeat the simulation for the whole city even when the changes are minor. In this article, we propose a new redundancy reduction technique for large-scale discrete event simulations, called exact-differential simulation, which simulates only the altered portions of scenarios and their influences in repeated executions while still achieving the same results as the re-execution of entire simulations. This article presents the main concepts of the exact-differential simulation, the design of its algorithm, and an approach to build an exact-differential simulation middleware that supports multiple applications of discrete event simulation. We also evaluate our approach by using two case studies, PHOLD benchmark and a traffic simulation of Tokyo. © 2019 Association for Computing Machinery.",Differential processing; Incremental processing; Redundancy reduction; What-if simulation,Middleware; Redundancy; Traffic control; Urban planning; Event Processing; Incremental processing; Multiple applications; Re-execution; Redundancy reductions; Simulation approach; Traffic simulations; What-if simulation; Discrete event simulation
A data assimilation framework for discrete event simulations,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068157194&doi=10.1145%2f3301502&partnerID=40&md5=6d4a67eeed07c2ecf051f759e21addbb,"Discrete event simulation (DES) is traditionally used as an offline tool to help users to carry out analysis for complex systems. As real-time sensor data become more and more available, there is increasing interest of assimilating real-time data into DES to achieve on-line simulation to support real-time decision making. This article presents a data assimilation framework that works with DES models. Solutions are proposed to address unique challenges associated with data assimilation for DES. A tutorial example of discrete event road traffic simulation is developed to demonstrate the data assimilation framework as well as principles of data assimilation in general. This article makes contributions to the DES community by presenting a data assimilation framework for DES and a concrete tutorial example that helps readers to grasp the details of data assimilation for DES. © 2019 Association for Computing Machinery.",Data assimilation framework; Discrete event simulations; Sequential monte carlo methods,Decision making; Discrete event simulation; Data assimilation; Discrete events; Online simulation; Real time decision-making; Real time sensors; Real-time data; Road traffic simulation; Sequential Monte Carlo methods; Monte Carlo methods
Predicting the simulation budget in ranking and selection procedures,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068140478&doi=10.1145%2f3323715&partnerID=40&md5=54fdd7276a01c6f77227958da7252de2,"The goal of ranking and selection (R&S) procedures is to identify the best among a finite set of alternative systems evaluated by stochastic simulation, providing a probability guarantee on the quality of the solution. To solve large-scale R&S problems, especially in parallel computing platforms where variable numbers of cores might be used, it is helpful to be able to predict the simulation budget, which is almost always the dominant portion of the running time of a given procedure for a given problem. Non-trivial issues arise due to the need to estimate the system configuration. We propose a set of methods for predicting the simulation budget. Numerical results compare our predictions for several leading R&S procedures. © 2019 Association for Computing Machinery.",Ranking and selection; Running-time prediction; Stochastic optimization,Forecasting; Optimization; Simulation platform; Stochastic models; Alternative systems; Parallel computing platform; Ranking and selection; Ranking and selection procedures; Running time; Stochastic optimizations; Stochastic simulations; System configurations; Budget control
Exposing Inter-process Information for Efficient PDES of Spatial Stochastic Systems on Multicores,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065610889&doi=10.1145%2f3301500&partnerID=40&md5=b9e4bb35819c8b929deb71e03b65f203,"We present a new approach for efficient process synchronization in parallel discrete event simulation on multicore computers. We aim specifically at simulation of spatially extended stochastic system models where time intervals between successive inter-process events are highly variable and without lower bounds: This includes models governed by the mesoscopic Reaction-Diffusion Master Equation (RDME). A central part of our approach is a mechanism for optimism control, in which each process disseminates accurate information about timestamps of its future outgoing interprocess events to its neighbours. This information gives each process a precise basis for deciding when to pause local processing to reduce the risk of expensive rollbacks caused by future “delayed” incoming events. We apply our approach to a natural parallelization of the Next Subvolume Method (NSM) for simulating systems obeying RDME. Since this natural parallelization does not expose accurate timestamps of future interprocess events, we restructure it to expose such information, resulting in a simulation algorithm called Refined Parallel NSM (Refined PNSM). We have implemented Refined PNSM in a parallel simulator for spatial extended Markovian processes. On 32 cores, it achieves an efficiency ranging between 43–95% for large models, and on average 37% for small models, compared to an efficient sequential simulation without any code for parallelization. It is shown that the gain of restructuring the naive parallelization into Refined PNSM more than outweighs its overhead. We also show that our resulting simulator is superior in performance to existing simulators on multicores for comparable models. © 2019 Association for Computing Machinery.",Multicore; Optimism control; Parallel discrete-event simulation; PDES; Spatial stochastic simulation,Discrete event simulation; Simulators; Stochastic control systems; Stochastic systems; Multi core; Parallel discrete event simulations; PDES; Process information; Sequential simulation; Simulation algorithms; Spatial stochastic simulations; Stochastic system models; Stochastic models
PDES-A: Accelerators for parallel discrete event simulation implemented on FPGAS,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065626040&doi=10.1145%2f3302259&partnerID=40&md5=81e447a7e3c493ebc1d6916c7a157fcd,"In this article, we present experiences implementing a general Parallel Discrete Event Simulation (PDES) accelerator on a Field Programmable Gate Array (FPGA). The accelerator can be specialized to any particular simulation model by defining the object states and the event handling code, which are then synthesized into a custom accelerator for the given model. The accelerator consists of several event processors that can process events in parallel while maintaining the dependencies between them. Events are automatically sorted by a self-sorting event queue. The accelerator supports optimistic simulation by automatically keeping track of event history and supporting rollbacks. The architecture is limited in scalability locally by the communication and port bandwidth of the different structures. However, it is designed to allow multiple accelerators to be connected to scale up the simulation. We evaluate the design and explore several design trade-offs and optimizations. We show that the accelerator can scale to 64 concurrent event processors relative to the performance of a single event processor. At this point, the scalability becomes limited by contention on the shared structures within the datapath. To alleviate this bottleneck, we also develop a new version of the datapath that partitions the state and event space of the simulation but allows these partitions to share the use of the event processors. The new design substantially reduces contention and improves the performance with 64 processors from 49x to 62x relative to a single processor design. We went through two iterations of the design of PDES-A, first using Verilog and then using Chisel (for the partitioned version of the design). We report in this article on some observations in the differences in prototyping accelerators using these two different languages. PDES-A outperforms the ROSS simulator running on a 12-core Intel Xeon machine by a factor of 3.2x with less than 15% of the power consumption. Our future work includes building multiple interconnected PDES-A cores. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Accelerator; Coprocessor; FPGA; Parallel simulation; PDES,Acceleration; Coprocessor; Discrete event simulation; Economic and social effects; Field programmable gate arrays (FPGA); Parallel processing systems; Particle accelerators; Scalability; Different structure; Event processors; Optimistic simulation; Parallel discrete event simulations; Parallel simulations; PDES; Shared structures; Single processors; Integrated circuit design
Efficient parallel simulation over large-scale social contact networks,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063189941&doi=10.1145%2f3265749&partnerID=40&md5=be4dd19be9611a182589b8ea6bbaa946,"Social contact network (SCN) models the daily contacts between people in real life. It consists of agents and locations. When agents visit a location at the same time, the social interactions can be established among them. Simulations over SCN have been employed to study social dynamics such as disease spread among population. Because of the scale of SCN and the execution time requirement, the simulations are usually run in parallel. However, a challenge to the parallel simulation is that the structure of SCN is naturally skewed with a few hub locations that have far more visitors than others. These hub locations can cause load imbalance and heavy communication between partitions, which therefore impact the simulation performance. This article proposes a comprehensive solution to address this challenge. First, the hub locations are decomposed into small locations, so that SCN can be divided into partitions with better balanced workloads. Second, the agents are decomposed to exploit data locality, so that the overall communication across partitions can be greatly reduced. Third, two enhanced execution mechanisms are designed for locations and agents, respectively, to improve simulation parallelism. To evaluate the efficiency of the proposed solution, an epidemic simulation was developed and extensive experiments were conducted on two computer clusters using three SCN datasets with different scales. The results demonstrate that our approach can significantly improve the execution performance of the simulation. © 2019 Association for Computing Machinery.",Graph partitioning; Load balancing; Parallel and distributed simulation; Skewed degree distribution; Social network,Computer applications; Computer simulation; Resource allocation; Social networking (online); Epidemic simulations; Execution performance; Graph Partitioning; Parallel and distributed simulation; Parallel simulations; Simulation performance; Skewed degree distribution; Social interactions; Location
Managing pending events in sequential and parallel simulations using three-tier heap and two-tier ladder queue,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063189596&doi=10.1145%2f3265750&partnerID=40&md5=41fa0c10cc222e51a0555ae584ba8a75,"Performance of sequential and parallel Discrete Event Simulations (DES) is strongly influenced by the data structure used for managing and processing pending events. Accordingly, we propose and evaluate the effectiveness of our multi-tiered (two- and three-tier) data structures and our Two-tier Ladder Queue, for both sequential and optimistic parallel simulations on distributed memory platforms. Our experiments compare the performance of our data structures against a performance-tuned version of the Ladder Queue, which has been shown to outperform many other data structures for DES. The core simulation-based empirical assessments are in C++ and are based on 2,500 configurations of well-established PHOLD and PCS benchmarks. In addition, we use an Avian Influenza Epidemic Model (AIM) for experimental analyses. We have conducted experiments on two computing clusters with different hardware to ensure our results are reproducible.Moreover, to fully establish the robustness of our analysis and data structures, we have also implemented pertinent queues in Java and verified consistent, reproducible performance characteristics. Collectively, our analyses show that our three-tier heap and two-tier ladder queue outperform the Ladder Queue by 60x in some simulations, particularly those with higher concurrency per Logical Process (LP), in both sequential and Time Warp synchronized parallel simulations. © 2019 Association for Computing Machinery.",Binary heap; Discrete event simulation (DES); Fibonacci heap; Ladder queue; Optimistic parallel simulation; Timewarp,Data structures; Discrete event simulation; Distributed computer systems; Ladders; Queueing theory; Simulation platform; Binary heap; Distributed memory platforms; Experimental analysis; Fibonacci heap; Parallel discrete event simulations; Parallel simulations; Performance characteristics; Timewarp; Computer software
Guest editorial for the TOMACS special issue on the principles of advanced discrete simulation (PADS),2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063187592&doi=10.1145%2f3312749&partnerID=40&md5=17eea81c718d77634cf35789f50d4c8e,[No abstract available],,
Towards data-driven simulation modeling for mobile agent-based systems,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061280294&doi=10.1145%2f3289229&partnerID=40&md5=cd1d565c6f73feef3a56c66f5c0a512c,"Simulation models are widely used to study complex systems. Current simulation models are generally handcrafted using expert knowledge (knowledge-driven); however, this process is slow and introduces modeler bias. This article presents an approach towards data-driven simulation modeling by developing a framework that discovers simulation models in an automated way for mobile agent-based applications. The framework is comprised of three components: (1) a model space specifcation, (2) a search method (genetic algorithm), and (3) framework measurement metrics. The model space specifcation provides a formal specifcation for the general model structure from which various models can be generated. The search method is used to efciently search the model space for candidate models that exhibit desired behavior patterns. The fve framework measurement metrics: flexibility, comprehensibility, controllability, composability, and robustness, are developed to evaluate the overall framework. The results demonstrate that it is possible to discover a variety of interesting models using the framework. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Agent-based; Framework; Mobile agent,Genetic algorithms; Agent based; Agent-based applications; Behavior patterns; Candidate models; Current simulation; Data-driven simulation; Framework; Mobile agent-based systems; Mobile agents
Ranking and selection: A new sequential Bayesian procedure for use with common random numbers,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061214309&doi=10.1145%2f3241042&partnerID=40&md5=22132aac1d1865801122b85dcc57254d,"We introduce a new sampling scheme for selecting the best alternative out of a given set of systems that are evaluated with respect to their expected performances. We assume that the systems are simulated on a computer and that a joint observation of all systems has a multivariate normal distribution with unknown mean and unknown covariance matrix. In particular, the observations of the systems may be stochastically dependent as is the case if common random numbers are used for simulation. In each iteration of the algorithm, we allocate a fixed budget of simulation runs to the alternatives. We use a Bayesian set-up with a noninformative prior distribution and derive a new closed-form approximation for the posterior distributions that allows provision of a lower bound for the posterior probability of a correct selection (PCS). Iterations are continued until this lower bound is greater than 1 - α for a given α. We also introduce a new allocation strategy that allocates the available budget according to posterior error probabilities. Our procedure needs no additional prior parameters and can cope with different types of ranking and selection tasks. Our numerical experiments show that our strategy is superior to other procedures from the literature, namely, K N++ and Pluck. In all of our test scenarios, these procedures needed more observation and/or had an empirical PCS below the required 1 - α. Our procedure always had its empirical PCS above 1 - α, underlining the practicability of our approximation of the posterior distribution. © 2019 Association for Computing Machinery.",Approximate posterior; Bayesian model; Missing data; Ordinal optimization; Ranking and selection; Unknown covariance,Bayesian networks; Budget control; Iterative methods; Normal distribution; Random number generation; Approximate posterior; Bayesian model; Missing data; Ordinal optimization; Ranking and selection; Unknown covariance; Covariance matrix
"Replicated computational results (RCR) Report for ""Fast random integer generation in an interval""",2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061202742&doi=10.1145%2f3239569&partnerID=40&md5=2f03ac57c256447df46ab82c46652e76,"                             The article ""Fast Random Integer Generation in an Interval"" by Lemire (2018) addressed the problem of reducing the cost of machine instructions needed for the random generation of integer values in a generic interval [0,s). The approach taken by the author is the one of exploiting the rejection method (Neumann 1951) to build an algorithm that almost eliminates the need for performing integer division operations-the algorithm still exploits divisions by powers of two, implemented in the form of cheap shift operations. In more details, the likelihood of not requiring an integer division in the proposed algorithm is 2                             L                             -s/2                             L                             , where L denotes the number of bits used to represent integer values. The author also presents a comparative experimental study where the new algorithm, and its implementation for x86 processors, are compared with solutions offered by common software libraries for different programming languages.                          © 2019 Copyright held by the owner/author(s).",Random number generation,Computer applications; Computer simulation; Common software; Computational results; Integer division; Integer values; Machine instructions; Random generation; Rejection methods; Shift operations; Random number generation
An introduction to multiobjective simulation optimization,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061215838&doi=10.1145%2f3299872&partnerID=40&md5=6a6d1ebb6ceacf0a0649562aadcb9540,"The multiobjective simulation optimization (MOSO) problem is a nonlinear multiobjective optimization problem in which multiple simultaneous and conflicting objective functions can only be observed with stochastic error. We provide an introduction to MOSO at the advanced tutorial level, aimed at researchers and practitioners who wish to begin working in this emerging area. Our focus is exclusively on MOSO methods that characterize the entire eficient or Pareto-optimal set as the solution to the MOSO problem; later, this set may be used as input to the broader multicriteria decision-making process. Our introduction to MOSO includes an overview of existing theory, methods, and provably convergent algorithms that explicitly control sampling error for (1) MOSO on finite sets, called multiobjective ranking and selection; (2) MOSO with integer-ordered decision variables; and (3) MOSO with continuous decision variables. In the context of integer-ordered and continuous decision variables, we focus on methods that provably converge to a local eficient set under the natural ordering. We also discuss key open questions that remain in this emerging field. © 2019 Association for Computing Machinery.",Multiobjective simulation optimization; Stochastic multiobjective optimization,Decision making; Decision theory; Pareto principle; Stochastic systems; Conflicting objectives; Convergent algorithms; Decision variables; Multi criteria decision making; Multi-objective ranking; Nonlinear multiobjective optimization; Pareto-optimal sets; Simulation optimization; Multiobjective optimization
Fast random integer generation in an interval,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061198467&doi=10.1145%2f3230636&partnerID=40&md5=10102fed35c38f65896720886f10b053,"In simulations, probabilistic algorithms, and statistical tests, we often generate random integers in an interval (e.g., [0,s)). For example, random integers in an interval are essential to the Fisher-Yates random shuffle. Consequently, popular languages such as Java, Python, C++, Swift and Go include ranged random integer generation functions as part of their runtime libraries. Pseudo-random values are usually generated in words of a fixed number of bits (e.g., 32b, 64b) using algorithms such as a linear congruential generator. We need functions to convert such random words to random integers in an interval ([0,s)) without introducing statistical biases. The standard functions in programming languages such as Java involve integer divisions. Unfortunately, division instructions are relatively expensive. We review an unbiased function to generate ranged integers from a source of random words that avoids integer divisions with high probability. To establish the practical usefulness of the approach, we show that this algorithm can multiply the speed of unbiased random shuffling on x64 processors. Our proposed approach has been adopted by the Go language for its implementation of the shuffle function. © 2019 Copyright held by the owner/author(s).",Random number generation; Randomized algorithms; Rejection method,C++ (programming language); Integer programming; Java programming language; High probability; Integer division; Linear congruential generator; Probabilistic algorithm; Randomized Algorithms; Rejection methods; Run-time library; Statistical bias; Random number generation
A variational inference-based heteroscedastic Gaussian process approach for simulation metamodeling,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061180471&doi=10.1145%2f3299871&partnerID=40&md5=e766026b7e0821b183d47dfd1e56e83a,"In this article, we propose a variational Bayesian inference-based Gaussian process metamodeling approach (VBGP) that is suitable for the design and analysis of stochastic simulation experiments. This approach enables statistically and computationally eficient approximations to the mean and variance response surfaces implied by a stochastic simulation, while taking into full account the uncertainty in the heteroscedastic variance; furthermore, it can accommodate the situation where either one or multiple simulation replications are available at every design point. We demonstrate the superior performance of VBGP compared with existing simulation metamodeling methods through two numerical examples. © 2019 Association for Computing Machinery.",Het-eroscedasticity; Metamodeling; Simulation output analysis; Simulation theory; Variational inference,Bayesian networks; Gaussian distribution; Inference engines; Numerical methods; Stochastic models; Stochastic systems; Het-eroscedasticity; Metamodeling; Simulation output analysis; Simulation theory; Variational inference; Gaussian noise (electronic)
Visual analytics to identify temporal patterns and variability in simulations from cellular automata,2019,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061200617&doi=10.1145%2f3265748&partnerID=40&md5=249d1a539646cec10de9c045e0250f1d,"Cellular Automata (CA) are discrete simulation models, thus producing spatio-temporal data through experiments, as well as stochastic models, thus generating multi-run data. Identifying temporal patterns, such as cycles, is important to understand the behavior of the model. Assessing variability is also essential to estimate which parameter values may require more runs and what consensus emerges across simulation runs. However, these two tasks are currently arduous as the commonly employed slider-based visualizations offer little support to identify temporal trends or excessive model variability. In this article, we addressed these two tasks by developing, implementing, and evaluating a new visual analytics environment that uses several linked visualizations. Our empirical evaluation of the proposed environment assessed (i) whether modelers could identify temporal patterns and variability, (ii) how features of simulations impacted performances, and (iii) whether modelers can use the familiar slider-based visualization together with our new environment. Results shows that participants were confident on results obtained using our new environment. They were also able to accomplish the two target tasks without taking longer than they would with current solutions. Our qualitative analysis found that some participants saw value in switching between our proposed visualization and the commonly used slider-based version. In addition, we noted that errors were affected not only by the type of visualizations but also by specific features of the simulations. Future work may combine and adapt these visualizations depending on salient simulation parameters. © 2019 Copyright held by the owner/author(s).",Discrete simulations; Interactive visualization; Multi-run data; Scientific visualization; Simulation analysis,Cellular automata; Data visualization; Stochastic models; Stochastic systems; Discrete simulation model; Discrete simulations; Empirical evaluations; Interactive visualizations; Modeling variability; Multi-run data; Simulation analysis; Simulation parameters; Visualization
Guest editorial for the TOMACS special issue on the principles of advanced discrete simulation (PADS),2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055167690&doi=10.1145%2f3267459&partnerID=40&md5=6b2cf2befb1b5db17e54e5267e27b161,[No abstract available],,
On automated memoization in the field of simulation parameter studies,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054171043&doi=10.1145%2f3186316&partnerID=40&md5=c456c4a389ebd1431964c67a07c05f12,"Processes in computer simulations tend to be highly repetitive. In particular, parameter studies further exasperate the situation as the same model is repeatedly executed with only partially varying parameters. Consequently, computer simulations perform identical computations, with identical code, identical input, and hence identical output. These redundant computations waste significant amounts of time and energy. Memoization, dating back to 1968, enables the caching of such identical intermediate results, thereby significantly speeding up those computations. However, until now, automated approaches were limited to pure functions. At ACM SIGSIM-PADS 2016 we published, to the best of our knowledge, the first practical approach for automated memoization for impure code. In this work, we extend this approach and evaluate the performance characteristics of a number of extensions that deal with questions posed at PADS: (1) To reduce and bound the memory footprint, we investigate several cache eviction strategies. (2) We allow the original and the memoized code to coexist via a runtime-switch and analyze the crossover point, thereby mitigating memoization overhead. (3) By optionally persisting the Memoization Cache to disk, we expand the scope to exploratory parameter studies where cached results can now be reused across multiple simulation runs. Altogether, automated memoization for impure code is a valuable technique, the versatility of which we explore further in this article. It sped up a case study of an OFDM network simulation by a factor of more than 80 with an only marginal increase of memory consumption. © 2018 Association for Computing Machinery.",Accelerating parameter studies; Automatic memoization; Impure languages,Automation; Codes (symbols); Automated approach; Intermediate results; Memoization; Parameter studies; Performance characteristics; Redundant computation; Simulation parameters; Varying parameters; Cache memory
Exact simulation for a class of tempered stable and related distributions,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053397516&doi=10.1145%2f3184453&partnerID=40&md5=e7bd096781b8abaf2cd5447d76ceeeb4,"In this article, we develop a new scheme of exact simulation for a class of tempered stable (TS) and other related distributions with similar Laplace transforms. We discover some interesting integral representations for the underlying density functions that imply a unique simulation framework based on a backward recursive procedure. Therefore, the foundation of this simulation design is very different from existing schemes in the literature. It works pretty efficiently for some subclasses of TS distributions, where even the conventional acceptance-rejection mechanism can be avoided. It can also generate some other distributions beyond the TS family. For applications, this scheme could be easily adopted to generate a variety of TS-constructed random variables and TS-driven stochastic processes for modelling observational series in practice. Numerical experiments and tests are performed to demonstrate the accuracy and effectiveness of our scheme. © 2018 ACM.",Backward recursive scheme; Exact simulation; Exponentially tilted stable distribution; Leptokurtosis; Levy process; Levy subordinator; Monte carlo simulation; Stable distribution; Tempered stable distribution,Intelligent systems; Laplace transforms; Random processes; Stochastic systems; Exact simulation; Leptokurtosis; Levy process; Recursive scheme; Stable distributions; Subordinators; Monte Carlo methods
"Path-ZVA: General, efficient, and automated importance sampling for highly reliable markovian systems",2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053402202&doi=10.1145%2f3161569&partnerID=40&md5=f08fbaf92c8b886f06ad53cc45b6a991,"We introduce Path-ZVA: An efficient simulation technique for estimating the probability of reaching a rare goal state before a regeneration state in a (discrete-time) Markov chain. Standard Monte Carlo simulation techniques do not work well for rare events, so we use importance sampling; i.e., we change the probability measure governing the Markov chain such that transitions ""towards"" the goal state become more likely. To do this, we need an idea of distance to the goal state, so some level of knowledge of the Markov chain is required. In this article, we use graph analysis to obtain this knowledge. In particular, we focus on knowledge of the shortest paths (in terms of ""rare"" transitions) to the goal state. We show that only a subset of the (possibly huge) state space needs to be considered. This is effective when the high dependability of the system is primarily due to high component reliability, but less so when it is due to high redundancies. For several models, we compare our results to well-known importance sampling methods from the literature and demonstrate the large potential gains of our method. © 2018 ACM.",Highly reliable systems; Importance sampling; Rare-event simulation,Chains; Intelligent systems; Markov processes; Monte Carlo methods; Component reliability; Efficient simulation; Highly reliable; Highly reliable systems; Importance sampling method; Monte carlo simulation technique; Probability measures; Rare event simulation; Importance sampling
Variance and derivative estimation of virtual performance,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053423468&doi=10.1145%2f3209959&partnerID=40&md5=5d8831f5659bf79340d2682dc308a9ba,"Virtual performance is a class of time-dependent performance measures conditional on a particular event occurring at time τ0 for a (possibly) nonstationary stochastic process; virtual waiting time of a customer arriving to a queue at time τ0 is one example. Virtual statistics are estimators of the virtual performance. In this article, we go beyond the mean to propose estimators for the variance, and for the derivative of the mean with respect to time, of virtual performance, examining both their small-sample and asymptotic properties. We also provide a modified K-fold cross validation method for tuning the parameter k for the differencebased variance estimator, and we evaluate the performance of both variance and derivative estimators via controlled studies and a realistic illustration. The variance and derivative provide useful information that is not apparent in the mean of virtual performance. © 2018 ACM.",Nearest-neighbor regression; Output analysis; Queueing simulation,Random processes; Derivative estimation; K fold cross validations; Nearest neighbors; Non-stationary stochastic process; Output analysis; Queueing simulation; Time-dependent performance; Virtual waiting time of a customer; Stochastic systems
A role-dependent data-driven approach for high-density crowd behavior modeling,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053891500&doi=10.1145%2f3177776&partnerID=40&md5=089eda2afa1a2814f8ba9d07797fe2f9,"In this article, we propose a role-dependent (RD) data-driven modeling approach to simulate pedestrians' motion in high-density scenes. It is commonly observed that pedestrians behave quite differently when walking in a dense crowd. Some people explore routes toward their destinations. Meanwhile, some people deliberately follow others, leading to lane formation. Based on these observations, two roles are included in the proposed model: leader and follower. The motion behaviors of leader and follower are modeled separately. Leaders' behaviors are learned from real crowd motion data using state-action pairs, while followers' behaviors are calculated based on specific targets that are obtained dynamically during the simulation. The proposed RD model is trained and applied to different real-world datasets to evaluate its generality and effectiveness. The simulation results demonstrate that the RD model is capable of simulating crowd behaviors in crowded scenes realistically and reproducing collective crowd behaviors such as lane formation. © 2018 Association for Computing Machinery.",Crowd simulation; Data-driven models; Leader-follower behavior,Computer applications; Computer simulation; Crowd behavior; Crowd Simulation; Data-driven approach; Data-driven model; Lane formation; Leader-follower; Motion behavior; Real-world datasets; Behavioral research
Statistical analysis of simulation output from parallel computing,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053426775&doi=10.1145%2f3186327&partnerID=40&md5=a6fa11dd07be269012ec2e6f78690df8,"This article addresses statistical output analysis of transient simulations in the parallel computing environment with fixed computing time. Using parallel computing, most commonly used unbiased estimators based on the output sequence compromise. To rectify this issue, this article proposes an estimation procedure in the Bayesian framework. The proposed procedure is particularly useful when the computing time depends on the output value in each simulation replication. The effectiveness of our method is demonstrated through studies on queuing simulation and control chart simulation. © 2018 ACM.",Bias reduction; Markov chain Monte Carlo; Output analysis; Parallel computing; Transient simulation,Markov processes; Parallel processing systems; Analysis of simulations; Bias reduction; Estimation procedures; Markov Chain Monte-Carlo; Output analysis; Parallel-computing environment; Simulation replication; Transient simulation; Monte Carlo methods
Efficient simulation for expectations over the union of half-spaces,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053428319&doi=10.1145%2f3167969&partnerID=40&md5=a7fe96482a7ce826ae5a26df90363094,"We consider the problem of estimating expectations over the union of half-spaces. Such a problem arises in many applications such as option pricing and stochastic activity networks. More recent applications include systemic risk measurements of financial networks. Assuming that random variables follow a multivariate elliptical distribution, we develop a conditional Monte Carlo method and prove its asymptotic efficiencies. We then demonstrate the numerical performance of the proposed method in three different application areas. © 2018 ACM.",Conditional Monte Carlo; Elliptical distribution; Rare event simulation; Variance reduction,Economics; Geometry; Numerical methods; Stochastic systems; Asymptotic efficiency; Efficient simulation; Elliptical distributions; Multivariate elliptical distributions; Numerical performance; Rare event simulation; Stochastic activity networks; Variance reductions; Monte Carlo methods
NeMo: A massively parallel discrete-event simulation model for neuromorphic architectures,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053437674&doi=10.1145%2f3186317&partnerID=40&md5=29ab3f3e2345d99ea63d8893c879f715,"Neuromorphic computing is a broad category of non-von Neumann architectures that mimic biological nervous systems using hardware. Current research shows that this class of computing can execute data classification algorithms using only a tiny fraction of the power conventional CPUs require. This raises the larger research question: How might neuromorphic computing be used to improve application performance, power consumption, and overall system reliability of future supercomputers? To address this question, an open-source neuromorphic processor architecture simulator called NeMo is being developed. This effort will enable the design space exploration of potential heterogeneous compute systems that combine traditional CPUs, GPUs, and neuromorphic hardware. This article examines the design, implementation, and performance of NeMo. Demonstration of NeMo's efficient execution using 2,048 nodes of an IBM Blue Gene/Q system, modeling 8,388,608 neuromorphic processing cores is reported. The peak performance of NeMo is just over ten billion events-per-second when operating at this scale. © 2018 Association for Computing Machinery.",Biocomputing; Discrete-event; Massive parallel; Neural net architecture; Neuromorphic architecture; Non von Neumann architecture; Reverse computation; Time warp,Discrete event simulation; Green computing; Hardware; Program processors; Supercomputers; Bio-computing; Discrete events; Massive parallel; NET architecture; Neumann architecture; Neuromorphic Architectures; Reverse computation; Time Warp; Parallel architectures
Reusing search data in ranking and selection: What could possibly gowrong?,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053375300&doi=10.1145%2f3170503&partnerID=40&md5=c9f4cff50bc705115126e9535e9f5cf9,"It is tempting to reuse replications taken during a simulation optimization search as input to a ranking-andselection procedure. However, even when the random inputs used to generate replications are identically distributed and independent within and across systems, we show that for searches that use the observed performance of explored systems to identify new systems, the replications are conditionally dependent given the sequence of returned systems. Through simulation experiments, we demonstrate that reusing the replications taken during search in selection and subset-selection procedures can result in probabilities of correct and good selection well below the guaranteed levels. Based on these negative findings, we call into question the guarantees of established ranking-and-selection procedures that reuse search data. We also rigorously define guarantees for ranking-and-selection procedures after search and discuss how procedures that only provide guarantees in the preference zone are ill-suited to this setting. © 2018 ACM.",Ranking and selection; Search; Simulation optimization,Computer simulation; Random input; Ranking and selection; Ranking and selection procedures; Search; Simulation optimization; Subset selection; Computer applications
Controlling the time discretization bias for the supremum of brownian motion,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053377217&doi=10.1145%2f3177775&partnerID=40&md5=ec9714e7caa44ce4b5eef8381124bf21,"We consider the bias arising from time discretization when estimating the threshold crossing probability w(b) := P(supt ϵ[0,1] Bt > b), with (Bt )t ϵ[0,1] a standard BrownianMotion.We prove that if the discretization is equidistant, then to reach a given target value of the relative bias, the number of grid points has to grow quadratically in b, as b grows. When considering non-equidistant discretizations (with threshold-dependent grid points), we can substantially improve on this: we show that for such grids the required number of grid points is independent of b, and in addition we point out how they can be used to construct a strongly efficient algorithm for the estimation of w(b). Finally, we show how to apply the resulting algorithm for a broad class of stochastic processes; it is empirically shown that the threshold-dependent grid significantly outperforms its equidistant counterpart. © 2018 ACM.",Brownian Motion; Continuity correction; Discretization error; Hitting time; Optimal discretization; Rare-event simulation,Non Newtonian flow; Stochastic systems; Continuity corrections; Discretization errors; Hitting time; Optimal discretization; Rare event simulation; Brownian movement
Modeling large-scale slim fly networks using parallel discrete-event simulation,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053200466&doi=10.1145%2f3203406&partnerID=40&md5=cb350ab3d30b4413de2e16a75193f2ac,"As supercomputers approach exascale performance, the increased number of processors translates to an increased demand on the underlying network interconnect. The slim fly network topology, a new low-diameter, low-latency, and low-cost interconnection network, is gaining interest as one possible solution for next-generation supercomputing interconnect systems. In this article, we present a high-fidelity slim fly packet-level model leveraging the Rensselaer Optimistic Simulation System (ROSS) and Co-Design of Exascale Storage (CODES) frameworks. We validate the model with published work before scaling the network size up to an unprecedented 1 million compute nodes and confirming that the slim fly observes peak network throughput at extreme scale. In addition to synthetic workloads, we evaluate large-scale slim fly models with real communication workloads from applications in the Design Forward program with over 110,000 MPI processes. We show strong scaling of the slim fly model on an Intel cluster achieving a peak network packet transfer rate of 2.3 million packets per second and processing over 7 billion discrete events using 128 MPI tasks. Enabled by the strong performance capabilities of the model, we perform a detailed application trace and routing protocol performance study. Through analysis of metrics such as packet latency, hop count, and congestion, we find that the slim fly network is able to leverage simple minimal routing and achieve the same performance as more complex adaptive routing for tested DOE benchmark applications. © 2018 ACM.",Interconnection networks; Network topologies; Parallel discrete event simulation; Slim fly,Application programs; Benchmarking; Distributed computer systems; Interconnection networks (circuit switching); Supercomputers; Topology; Benchmark applications; Million packets per seconds; Network topology; Packet-level modeling; Parallel discrete event simulations; Performance capability; Routing protocol performance; Slim flies; Discrete event simulation
"Replicated Computations Results (RCR) report for ""reusing Search Data in Ranking and Selection: What Could Possibly Go Wrong?""",2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053370857&doi=10.1145%2f3185337&partnerID=40&md5=24f96bb8f0083e4d755c3c68affe7f23,"""Reusing Search Data in Ranking and Selection: What Could Possibly Go Wrong?"" [2] by Eckman and Henderson rigorously defines the statistical guarantees for ranking-and-selection (R&S) procedures after random search, and points out that the simulation replications collected in the search phase are conditionally dependent given the sequence of returned systems. Therefore, reusing the search data for R&S may affect the statistical guarantees. The authors further design random search algorithms to demonstrate that the correct selection guarantees of some ranking-and-selection procedures will be compromised if reusing the simulation replications taken during the search. This replicated computation report focuses on the reproducibility of the experiment results in the aforementioned article. © 2018 ACM.",Random search; Ranking and selection; Simulation optimization,Computer simulation; Random search algorithm; Random searches; Ranking and selection; Ranking and selection procedures; Reproducibilities; Simulation optimization; Simulation replication; Statistical guarantee; Computer applications
Combining simulation and emulation systems for smart grid planning and evaluation,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053207349&doi=10.1145%2f3186318&partnerID=40&md5=1d87ef71a88a410c480893eaacdf0b18,"Software-defined networking (SDN) enables efficient network management. As the technology matures, utilities are looking to integrate those benefits to their operations technology (OT) networks. To help the community to better understand and evaluate the effects of such integration, we develop DSSnet, a testing platform that combines a power distribution system simulator and an SDN-based network emulator for smart grid planning and evaluation. DSSnet relies on a container-based virtual time system to achieve efficient synchronization between the simulation and emulation systems. To enhance the system scalability and usability, we extend DSSnet to support a distributed controller environment. To enhance system fidelity, we extend the virtual time system to support kernel-based switches. We also evaluate the system performance of DSSnet and demonstrate the usability of DSSnet with a resilient demand response application case study. © 2018 ACM.",Electrical power system simulation; Network emulation; Smart grid; Software-defined networking,Electric power systems; Electric power transmission networks; Integration testing; Software defined networking; Time switches; Distributed controller; Electrical power system; Network emulation; Network emulators; Power distribution system; Smart grid; Software defined networking (SDN); System scalability; Smart power grids
Replicated computations results (RCR) report for “mesoscopic modelling of pedestrian movement using carma and its tools”,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042929126&doi=10.1145%2f3177773&partnerID=40&md5=a53277c1a9b966877e24235147f2add8,"“Mesoscopic modeling of pedestrian movement using Carma and its tools” uses Carma (Collective Adaptive Resource-sharing Markovian Agents), a specification language recently introduced for modeling CAS, to model spatially distributed systems in which the desired model lies between an individual-based (microscopic) and a population-based (macroscopic) spatial model. The impact on the system dynamics of changes to the topology of paths is studied via simulation. The provided experiments show that it is difficult to predict the effect of changes to the network structure and that even small variations can produce significant effects. This replicated computations results report focuses on the prototypical tool implementation used in the article to perform such analysis. The software was straightforward to install and use, and all the experimental results from the article could be reproduced. © 2018 ACM",Collective adaptive systems; Process algebra; RCR report; Stochastic simulation,Specification languages; Stochastic models; Stochastic systems; Superconducting materials; Mesoscopic modeling; Mesoscopic modelling; Network structures; Pedestrian movement; Process algebras; RCR report; Spatially-distributed system; Stochastic simulations; Modeling languages
A product-form model for the performance evaluation of a bandwidth allocation strategy in WSNs,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042916277&doi=10.1145%2f3155335&partnerID=40&md5=8f9498ada7a318e6b9bbb1073fa61188,"Wireless Sensor Networks (WSNs) are important examples of Collective Adaptive System, which consist of a set of motes that are spatially distributed in an indoor or outdoor space. Each mote monitors its surrounding conditions, such as humidity, intensity of light, temperature, and vibrations, but also collects complex information, such as images or small videos, and cooperates with the whole set of motes forming the WSN to allow the routing process. The traffic in the WSN consists of packets that contain the data harvested by the motes and can be classified according to the type of information that they carry. One pivotal problem in WSNs is the bandwidth allocation among the motes. The problem is known to be challenging due to the reduced computational capacity of the motes, their energy consumption constraints, and the fully decentralised network architecture. In this article, we study a novel algorithm to allocate the WSN bandwidth among the motes by taking into account the type of traffic they aim to send. Under the assumption of a mesh network and Poisson distributed harvested packets, we propose an analytical model for its performance evaluation that allows a designer to study the optimal configuration parameters. Although the Markov chain underlying the model is not reversible, we show it to be ρ-reversible under a certain renaming of states. By an extensive set of simulations, we show that the analytical model accurately approximates the performance of networks that do not satisfy the assumptions. The algorithm is studied with respect to the achieved throughput and fairness. We show that it provides a good approximation of the max-min fairness requirements. © 2018 ACM",Bandwidth allocation; Markov models; Product-forms; Wireless sensor networks,Analytical models; Bandwidth; Energy utilization; Frequency allocation; Markov processes; Network architecture; Sensor nodes; Computational capacity; Configuration parameters; Energy consumption constraints; Markov model; Performance evaluations; Product forms; Product-form models; Wireless sensor network (WSNs); Wireless sensor networks
A holistic approach for collaborative workload execution in volunteer clouds,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042922949&doi=10.1145%2f315533&partnerID=40&md5=7630453ed2288d2a86a1e17feb443ee2,"The demand for provisioning, using, and maintaining distributed computational resources is growing hand in hand with the quest for ubiquitous services. Centralized infrastructures such as cloud computing systems provide suitable solutions for many applications, but their scalability could be limited in some scenarios, such as in the case of latency-dependent applications. The volunteer cloud paradigm aims at overcoming this limitation by encouraging clients to offer their own spare, perhaps unused, computational resources. Volunteer clouds are thus complex, large-scale, dynamic systems that demand for self-adaptive capabilities to offer effective services, as well as modeling and analysis techniques to predict their behavior. In this article, we propose a novel holistic approach for volunteer clouds supporting collaborative task execution services able to improve the quality of service of compute-intensive workloads. We instantiate our approach by extending a recently proposed ant colony optimization algorithm for distributed task execution with a workloadbased partitioning of the overlay network of the volunteer cloud. Finally, we evaluate our approach using simulation-based statistical analysis techniques on a workload benchmark provided by Google. Our results show that the proposed approach outperforms some traditional distributed task scheduling algorithms in the presence of compute-intensive workloads. © 2018 ACM.",Ant colony optimization (ACO); Autonomic computing; Cloud computing; Collaborative computing; Collective adaptive systems; Computational fields; Multiagent optimization; Peer-to-peer (P2P); Task scheduling,Artificial intelligence; Autonomous agents; Cloud computing; Computer supported cooperative work; Large scale systems; Multitasking; Peer to peer networks; Quality of service; Scheduling algorithms; Ant Colony Optimization (ACO); Autonomic Computing; Computational field; Multiagent; Peer to peer; Task-scheduling; Ant colony optimization
Guest editorial for the special issue on formal methods for the quantitative evaluation of collective adaptive systems (FORECAST),2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042928375&doi=10.1145%2f3177772&partnerID=40&md5=d2c7011ee9083d631f0d24b39609ad5a,"The FORmal methods for the quantitative Evaluation of Collective Adaptive SysTems(FORECAST), held in Vienna, Austria, on July 8, 2016 as a satellite event of the 4th federated event on Software Technologies: Applications and Foundations (STAF 2016) was organized to raise awareness in the software engineering and formal methods communities of the particularities of CAS, and the design and control problems that they bring. The guest editors of this special issue served as co-chairs of the workshop's Program Committee and were responsible for its proceedings. FORECAST was sponsored by the FP7-ICT-600708 European project a Quantitative Approach to Management and Design of Collective and Adaptive Behaviors (QUANTICOL) that ran from 2013 to 2017.",Collective Adaptive Systems; Formal methods; System specification; Verification,"Adaptive control systems; Application programs; Forecasting; Specifications; Verification; Adaptive behavior; Design and control; Program committee; Quantitative approach; Quantitative evaluation; Software technology; System specification; Vienna , Austria; Formal methods"
Replicated Computations Results (RCR) report for “A holistic approach for collaborative workload execution in volunteer clouds”,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042906656&doi=10.1145%2f3182167&partnerID=40&md5=4107c5edd69622a9238f18b0ef47a158,"“A Holistic Approach for Collaborative Workload Execution in Volunteer Clouds” [3] proposes a novel approach to task scheduling in volunteer clouds. Volunteer clouds are decentralized cloud systems based on collaborative task execution, where clients voluntarily share their own unused computational resources. By using simulation-based statistical analysis techniques—in particular, statistical model checking—the authors show that their approach can outperform existing distributed task scheduling algorithms in the case of computation-intensive workloads. The analysis considered a realistic workload benchmark provided by Google. This replicated computations results report focuses on the prototypical tool implementation used in the article to perform such analysis. The software was straightforward to install and use, and a representative part of the experimental results from the article could be reproduced in reasonable time using a standard laptop. © 2018 ACM",Ant colony optimization (ACO); Autonomic computing; Cloud computing; Collaborative computing; Collective adaptive systems; Computational fields; Multiagent optimization; Peer-to-peer (P2P); RCR report; Task scheduling,Ant colony optimization; Artificial intelligence; Autonomous agents; Cloud computing; Computer supported cooperative work; Large scale systems; Model checking; Multitasking; Peer to peer networks; Scheduling algorithms; Ant Colony Optimization (ACO); Autonomic Computing; Computational field; Multiagent; Peer to peer; RCR report; Task-scheduling; Distributed computer systems
Mesoscopic modelling of pedestrian movement using carma and its tools,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042929044&doi=10.1145%2f3155338&partnerID=40&md5=90db28e4f68a4e6ff4ef49841ab37866,"In this article, we assess the suitability of the Carma (Collective Adaptive Resource-sharing Markovian Agents) modelling language for mesoscopic modelling of spatially distributed systems where the desired model lies between an individual-based (microscopic) spatial model and a population-based (macroscopic) spatial model. Our modelling approach is mesoscopic in nature because it does not model the movement of individuals as an agent-based simulation in two-dimensional space, nor does it make a continuous-space approximation of the density of a population of individuals using partial differential equations. The application that we consider is pedestrian movement along paths that are expressed as a directed graph. In the models presented, pedestrians move along path segments at rates that are determined by the presence of other pedestrians, and make their choice of the path segment to cross next at the intersections of paths. Information about the topology of the path network and the topography of the landscape can be expressed as separate functional and spatial aspects of the model by making use of Carma language constructs for representing space.We use simulation to study the impact on the system dynamics of changes to the topology of paths and show how Carma provides suitable modelling language constructs that make it straightforward to change the topology of the paths and other spatial aspects of the model without completely restructuring the Carma model. Our results indicate that it is difficult to predict the effect of changes to the network structure and that even small changes can have significant effects. © 2018 ACM.",Collective adaptive systems; Process algebra; Stochastic simulation,Computer simulation languages; Continuous time systems; Directed graphs; Modeling languages; Stochastic models; Stochastic systems; Superconducting materials; Systems analysis; Agent based simulation; Language constructs; Mesoscopic modelling; Pedestrian movement; Process algebras; Spatially-distributed system; Stochastic simulations; Two dimensional spaces; Topology
Design and verification of trusted collective adaptive systems,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042934638&doi=10.1145%2f3155337&partnerID=40&md5=3c2727486c83e4cea87b76b2615b9087,"Collective adaptive systems (CAS) often adopt cooperative operating strategies to run distributed decision-making mechanisms. Sometimes, their effectiveness massively relies on the collaborative nature of individuals’ behavior. Stimulating cooperation while preventing selfish and malicious behaviors is the main objective of trust and reputation models. These models are largely used in distributed, peer-to-peer environments and, therefore, represent an ideal framework for improving the robustness, as well as security, of CAS. In this article, we propose a formal framework for modeling and verifying trusted CAS. From the modeling perspective, mobility, adaptiveness, and trust-based interaction represent the main ingredients used to define a flexible and easy-to-use paradigm. Concerning analysis, formal automated techniques based on equivalence and model checking support the prediction of the CAS behavior and the verification of the underlying trust and reputation models, with the specific aim of estimating robustness with respect to the typical attacks conducted against webs of trust. © 2018 ACM",Collective adaptive systems; Concurrency theory; Reputation models; Trust,Adaptive systems; Behavioral research; Decision making; Distributed computer systems; Automated techniques; Concurrency theory; Distributed decision making; Peer-to-peer environments; Reputation models; Trust; Trust and reputation; Trust-based interactions; Model checking
Engineering resilient collective adaptive systems by self-stabilisation,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042915166&doi=10.1145%2f3177774&partnerID=40&md5=1603db8a0b59778f7c5af0349184dd04,"Collective adaptive systems are an emerging class of networked computational systems particularly suited for application domains such as smart cities, complex sensor networks, and the Internet of Things. These systems tend to feature large-scale, heterogeneity of communication model (including opportunistic peer-to-peer wireless interaction) and require inherent self-adaptiveness properties to address unforeseen changes in operating conditions. In this context, it is extremely difficult (if not seemingly intractable) to engineer reusable pieces of distributed behaviour to make them provably correct and smoothly composable. Building on the field calculus, a computational model (and associated toolchain) capturing the notion of aggregate network-level computation, we address this problem with an engineering methodology coupling formal theory and computer simulation. On the one hand, functional properties are addressed by identifying the largest-to-date field calculus fragment generating self-stabilising behaviour, guaranteed to eventually attain a correct and stable final state despite any transient perturbation in state or topology and including highly reusable building blocks for information spreading, aggregation, and time evolution. On the other hand, dynamical properties are addressed by simulation, empirically evaluating the different performances that can be obtained by switching between implementations of building blocks with provably equivalent functional properties. Overall, our methodology sheds light on how to identify core building blocks of collective behaviour and how to select implementations that improve system performance while leaving overall system function and resiliency properties unchanged. © 2018 ACM.",Aggregate computing; Collective adaptive systems; Distributed algorithms; Field calculus; Modeling; Self-stabilisation; Simulation,Adaptive systems; Aggregates; Calculations; Computation theory; Computer software reusability; Models; Parallel algorithms; Peer to peer networks; Sensor networks; Stabilization; Communication modeling; Engineering methodology; Field calculus; Information spreading; Networked computational systems; Self-stabilisation; Simulation; Transient perturbation; Distributed computer systems
Replicated computations results (RCR) Report for “design and verification of trusted collective adaptive systems”,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042916254&doi=10.1145%2f3170502&partnerID=40&md5=a503b4789d32c9c7450928965061a583,"The article “Design and Verification of Trusted Collective Adaptive Systems” by Aldini proposes a process-algebraic framework for modeling and verifying trusted collective adaptive systems. To favor reuse, the system and trust models can be specified separately, only to be integrated at the semantic level. Through a combination of behavioral equivalence checking and model checking against branching-time temporal logic with trust predicates, the framework allows comparative analyses of different trust models as well as analyses of the effects of attacks to the trust models. The applicability of the formal framework is illustrated by means of two representative use cases: the security analysis of a trust-incentive service management system and a comparison of two different reputation systems. This replicated computations results report focuses on the reproducibility of the experiments performed in the aforementioned article, i.e. on the automatic verification of properties against models of these use cases encoded in the well-known NuSMV model checker. It was straightforward to reproduce all results from the article in reasonable time using a standard laptop machine. © 2018 ACM",Collective adaptive systems; Model checking; RCR report; Reputation models; Trust,Adaptive systems; Semantics; Algebraic framework; Automatic verification; Behavioral equivalence; Comparative analysis; RCR report; Reputation models; Reputation systems; Trust; Model checking
ProPPA: Probabilistic programming for stochastic dynamical systems,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042484078&doi=10.1145%2f3154392&partnerID=40&md5=ecf2e449a27ecc229629c91112637e02,"Formal languages like process algebras have been shown to be effective tools in modelling a wide range of dynamic systems, providing a high-level description that is readily transformed into an executable model. However, their application is sometimes hampered because the quantitative details of many real-world systems of interest are not fully known. In contrast, in machine learning, there has been work to develop probabilistic programming languages, which provide system descriptions that incorporate uncertainty and leverage advanced statistical techniques to infer unknown parameters from observed data. Unfortunately, current probabilistic programming languages are typically too low-level to be suitable for complex modelling. In this article, we present a Probabilistic Programming Process Algebra (ProPPA), the first instance of the probabilistic programming paradigm being applied to a high-level, formal language, and its supporting tool suite. We explain the semantics of the language in terms of a quantitative generalisation of Constraint Markov Chains and describe the implementation of the language, discussing in some detail the different inference algorithms available and their domain of applicability. We conclude by illustrating the use of the language on simple but non-trivial case studies: here, ProPPA is shown to combine the elegance and simplicity of high-level formal modelling languages with an effective way of incorporating data, making it a promising tool for modelling studies. © 2018 ACM.",Parameter estimation; Probabilistic programming; Process algebra; Stochastic modelling,Algebra; Computer programming languages; Dynamical systems; Formal languages; Inference engines; Learning systems; Markov processes; Modeling languages; Object oriented programming; Parameter estimation; Semantics; Stochastic models; Stochastic systems; High level description; Inference algorithm; Probabilistic programming; Probabilistic programming language; Process algebras; Statistical techniques; Stochastic dynamical system; System description; High level languages
SCORE allocations for bi-objective ranking and selection,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040307387&doi=10.1145%2f3158666&partnerID=40&md5=3e268c5d8e1712d5b14d83ab9c22cd0e,"The bi-objective ranking and selection (R&S) problem is a special case of the multi-objective simulation optimization problem in which two conflicting objectives are known only through dependent Monte Carlo estimators, the decision space or number of systems is finite, and each system can be sampled to some extent. The solution to the bi-objective R&S problem is a set of systems with non-dominated objective vectors, called the set of Pareto systems. We exploit the special structure of the bi-objective problem to characterize the asymptotically optimal simulation budget allocation, which accounts for dependence between the objectives and balances the probabilities associated with two types of misclassification error. Like much of the R&S literature, our focus is on the case in which the simulation observations are bivariate normal. Assuming normality, we then use a certain asymptotic limit to derive an easily-implementable Sampling Criteria for Optimization using Rate Estimators (SCORE) sampling framework that approximates the optimal allocation and accounts for correlation between the objectives. Perhaps surprisingly, the limiting SCORE allocation exclusively controls for misclassification-by-inclusion events, in which non-Pareto systems are falsely estimated as Pareto. We also provide an iterative algorithm for implementation. Our numerical experience with the resulting SCORE framework indicates that it is fast and accurate for problems having up to ten thousand systems. © 2018 ACM.",Multi-objective simulation optimization; Ranking and selection,Budget control; Iterative methods; Monte Carlo methods; Optimization; Asymptotically optimal; Conflicting objectives; Iterative algorithm; Misclassification error; Misclassifications; Monte Carlo Estimators; Multi objective simulation optimization; Ranking and selection; Multiobjective optimization
Temporal integration of emulation and network simulators on linux multiprocessors,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042467476&doi=10.1145%2f3154386&partnerID=40&md5=ebe5f9b80e85d28f5622a43cfa34392b,"Integration of emulation and simulation in virtual time requires that emulated execution bursts be ascribed a duration in virtual time and that emulated execution and simulation executions be coordinated within this common virtual time basis. This article shows how the open-source tool TimeKeeper for coordinating emulations in virtual time can be integrated with three different existing software emulations/simulations (CORE, Mininet, and EMANE) and with two existing network simulators (ns-3 and S3F). The integration does not require modification to those tools. However, the information that TimeKeeper needs to administer these emulations has to be extracted from each. We discuss the issues and challenges we encounter there, and the solutions. The S3F integration is specialized and shows how we can treat bursts of emulated execution just like an event handler in a discrete-event simulation. Through these case studies, we show the impact that the time dilation factor has on available resources, execution time, and fidelity of causality and that deleterious behaviors suffered under best-effort management of emulation processes can be corrected by integration with TimeKeeper. The key contribution is that we have shown how, using TimeKeeper, it is possible to bring virtual time to many existing emulators without needing to change them. © 2017 ACM.",Core; EMANE; Emulation; Linux kernel; LXCs; Ns-3; S3F; Simulation; Time dilation; Virtualization,Discrete event simulation; Distributed computer systems; Integration; Linux; Open source software; Open systems; Virtualization; Core; EMANE; Emulation; Linux kernel; LXCs; Simulation; Time dilation; Computer operating systems
A survey of statistical model checking,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042465606&doi=10.1145%2f3158668&partnerID=40&md5=6ef90491e662158f903c8697409d90b2,"Interactive, distributed, and embedded systems often behave stochastically, for example, when inputs, message delays, or failures conform to a probability distribution. However, reasoning analytically about the behavior of complex stochastic systems is generally infeasible. While simulations of systems are commonly used in engineering practice, they have not traditionally been used to reason about formal specifications. Statistical model checking (SMC) addresses this weakness by using a simulation-based approach to reason about precise properties specified in a stochastic temporal logic. A specification for a communication system May state that within some time bound, the probability that the number of messages in a queue will be greater than 5 must be less than 0.01. Using SMC, executions of a stochastic system are first sampled, after which statistical techniques are applied to determine whether such a property holds. While the output of sample-based methods are not always correct, statistical inference can quantify the confidence in the result produced. In effect, SMC provides a more widely applicable and scalable alternative to analysis of properties of stochastic systems using numerical and symbolic methods. SMC techniques have been successfully applied to analyze systems with large state spaces in areas such as computer networking, security, and systems biology. In this article, we survey SMC algorithms, techniques, and tools, while emphasizing current limitations and tradeoffs between precision and scalability. © 2018 ACM.",Estimation; Hypothesis testing; Simulation; Statistical model checking; Temporal logic,Computer circuits; Embedded systems; Estimation; Formal specification; Numerical methods; Probability distributions; Specifications; Statistics; Stochastic models; Stochastic systems; Surveys; Temporal logic; Analysis of properties; Complex stochastic systems; Hypothesis testing; Simulation; Simulation based approaches; Statistical inference; Statistical model checking; Statistical techniques; Model checking
Estimating large delay probabilities in two correlated queues,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042480003&doi=10.1145%2f3158667&partnerID=40&md5=f5dc8c4fb4c0c551b7f388dab90fd6e7,"This article focuses on evaluating the probability that both components of a two-dimensional stochastic process will ever, but not necessarily at the same time, exceed some large level u. An important application is in determining the probability of large delays occurring in two correlated queues. Since exact analysis of this probability seems prohibitive, we focus on deriving asymptotics and on developing efficient simulations techniques. Large deviations theory is used to characterise logarithmic asymptotics. The second part of this article focuses on efficient simulation techniques. Using “nearest-neighbour random walk” as an example, we first show that a “naive” implementation of importance sampling, based on the decay rate, is not asymptotically efficient. A different approach, which we call partitioned importance sampling, is developed and shown to be asymptotically efficient. The results are illustrated through various simulation experiments. © 2018 ACM.",Importance sampling; Large deviations; Logarithmic asymptotics,Decay (organic); Probability; Random processes; Stochastic systems; Asymptotically efficient; Efficient simulation; Exact analysis; Large delays; Large deviations; Large deviations theory; Logarithmic asymptotics; Nearest neighbour; Importance sampling
Scalable cloning on large-Scale GPU platforms with application to time-Stepped simulations on grids,2018,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042487711&doi=10.1145%2f3158669&partnerID=40&md5=a41d48cd968f1bc341146d26a8d397c2,"Cloning is a technique to efficiently simulate a tree of multiple what-if scenarios that are unraveled during the course of a base simulation. However, cloned execution is highly challenging to realize on large, distributed memory computing platforms, due to the dynamic nature of the computational load across clones, and due to the complex dependencies spanning the clone tree. We present the conceptual simulation framework, algorithmic foundations, and runtime interface of CloneX, a new system we designed for scalable simulation cloning. It efficiently and dynamically creates whole logical copies of a dynamic tree of simulations across a large parallel system without full physical duplication of computation and memory. The performance of a prototype implementation executed on up to 1,024 graphical processing units of a supercomputing system has been evaluated with three benchmarks—heat diffusion, forest fire, and disease propagation models—delivering a speed up of over two orders of magnitude compared to replicated runs. The results demonstrate a significantly faster and scalable way to execute many what-if scenario ensembles of large simulations via cloning using the CloneX interface. 2018 Copyright is held by the owner,author's.",CUDA; Graphical processing units; Load balancing; Supercomputing; Time synchronization; What-if decision tree,Benchmarking; Computer simulation languages; Decision trees; Deforestation; Distributed computer systems; Genetic engineering; Graphics processing unit; Resource allocation; Algorithmic foundations; Conceptual simulations; CUDA; Graphical processing unit (GPUs); Prototype implementations; Supercomputing; Supercomputing systems; Time synchronization; Cloning
A factor-based Bayesian framework for risk analysis in stochastic simulations,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040309947&doi=10.1145%2f3154387&partnerID=40&md5=360758c23563f1b506b193325224192f,"Simulation is commonly used to study the random behaviors of large-scale stochastic systems with correlated inputs. Since the input correlation is often induced by latent common factors in many situations, to facilitate system diagnostics and risk management, we introduce a factor-based Bayesian framework that can improve both computational and statistical efficiency and provide insights for system risk analysis. Specifically, we develop a flexible Gaussian copula-based multivariate input model that can capture important properties in the real-world data. A nonparametric Bayesian approach is used to model marginal distributions, and it can capture the properties, including multi-modality and skewness. We explore the factor structure of the underlying generative processes for the dependence. Both input and simulation estimation uncertainty are characterized by the posterior distributions. In addition, we interpret the latent factors and estimate their effects on the system performance, which could be used to support diagnostics and decision making for large-scale stochastic systems. Our approach is supported by both asymptotic theory and empirical study. © 2017 ACM.",Factor model; Gaussian copula; Multivariate input model; Nonparametric model; Risk analysis; Simulation,Bayesian networks; Bayesian networks; Computation theory; Computation theory; Computational efficiency; Computational efficiency; Decision making; Decision making; Distribution functions; Distribution functions; Factor analysis; Factor analysis; Risk assessment; Risk assessment; Risk management; Risk management; Stochastic models; Stochastic models; Stochastic systems; Stochastic systems; Factor model; Factor model; Gaussian copula; Gaussian copula; Input modeling; Input modeling; Non-parametric model; Non-parametric model; Simulation; Simulation; Risk analysis; Risk analysis
"Replicated computational results (RCR) report for ""proPPA: Probabilistic programming for stochastic dynamical systems""",2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040132272&doi=10.1145%2f3161568&partnerID=40&md5=417864482df17f44a2ac93be1d0e1e1a,"""ProPPA: Probabilistic Programming for Stochastic Dynamical Systems,"" by Georgoulas, Hillston, and Sanguinetti, introduces the ProPPA formalism, which brings together ideas from stochastic process algebras with those from the paradigm of probabilistic programming. The article formally defines the ProPPA language and its semantics and presents a tool-set, along with results from illustrative examples. This replicated computational results report installs and runs the tool-set and repeats the simulation-based results from the article, finding that the published results are repeatable. © 2017 ACM.",Probabilistic programming; Process algebra; RCR report,Algebra; Dynamical systems; Random processes; Semantics; Computational results; Probabilistic programming; Process algebras; RCR report; Stochastic dynamical system; Stochastic process algebras; Stochastic systems
Moment-matching-based conjugacy approximation for Bayesian ranking and selection,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038617071&doi=10.1145%2f3149013&partnerID=40&md5=15a777a3ecac5824ed6188c2ba86b40e,"We study the conjugacy approximation models in the context of Bayesian ranking and selection with unknown correlations. Under the assumption of normal-inverse-Wishart prior distribution, the posterior distribution remains a normal-inverse-Wishart distribution thanks to the conjugacy property when all alternatives are sampled at each step. However, this conjugacy property no longer holds if only one alternative is sampled at a time, an appropriate setting when there is a limited budget on the number of samples. We propose two new conjugacy approximation models based on the idea of moment matching. Both of them yield closed-form Bayesian prior updating formulas.We apply these updating formulas in Bayesian ranking and selection using the knowledge gradient method and show the superiority of the proposed conjugacy approximation models in applications of wind farm placement and computer model calibration. © 2017 ACM.",Approximate conjugacy; Bayesian learning; Moment matching; Ranking and selection,Budget control; Gradient methods; Wind power; Approximation model; Bayesian learning; Conjugacy; Inverse wishart distributions; Moment-matching; Posterior distributions; Prior distribution; Ranking and selection; Inverse problems
Automated estimation of extreme steady-state quantiles via the maximum transformation,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038359450&doi=10.1145%2f3122864&partnerID=40&md5=d4f2880b44b9d29deb24f2fd721a116a,"We present Sequem, a sequential procedure that delivers point and confidence-interval (CI) estimators for extreme steady-state quantiles of a simulation-generated process. Because it is specified completely, Sequem can be implemented directly and applied automatically. The method is an extension of the Sequest procedure developed by Alexopoulos et al. in 2014 to estimate nonextreme steady-state quantiles. Sequem exploits a combination of batching, sectioning, and the maximum transformation technique to achieve the following: (i) reduction in point-estimator bias arising from the simulation's initial condition or from inadequate simulation run length; and (ii) adjustment of the CI half-length to compensate for the effects of skewness or autocorrelation on intermediate quantile point estimators computed from nonoverlapping batches of observations. Sequem's CIs are designed to satisfy user-specified requirements concerning coverage probability and absolute or relative precision. In an experimental evaluation based on seven processes selected to stresstest the procedure, Sequem exhibited uniformly good performance. © 2017 ACM.",Maximum transformation method; Method of batching; Method of sectioning; Quantile estimation; Sequential procedure; Steady-state simulation,Computer simulation; Method of batching; Method of sectioning; Quantile estimation; Sequential procedures; Steady-state simulations; Transformation methods; Computer applications
Green simulation: Reusing the output of repeated experiments,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033222978&doi=10.1145%2f3129130&partnerID=40&md5=4ed94ab0c0d945b64bbffc35e966bbf7,"We introduce a new paradigm in simulation experiment design and analysis, called ""green simulation,"" for the setting in which experiments are performed repeatedly with the same simulation model. Green simulation means reusing outputs from previous experiments to answer the question currently being asked of the simulation model. As one method for green simulation, we propose estimators that reuse outputs from previous experiments by weighting them with likelihood ratios, when parameters of distributions in the simulation model differ across experiments. We analyze convergence of these estimators as more experiments are repeated, while a stochastic process changes the parameters used in each experiment. As another method for green simulation, we propose an estimator based on stochastic kriging. We find that green simulation can reduce mean squared error by more than an order of magnitude in examples involving catastrophe bond pricing and credit risk evaluation. © 2017 ACM.",Likelihood ratio method; Multiple importance sampling; Score function method; Simulation metamodeling,Importance sampling; Mean square error; Random processes; Risk assessment; Stochastic systems; Catastrophe bonds; Credit risk evaluation; Experiment design; Likelihood ratio method; Likelihood ratios; Mean squared error; Metamodeling; Score function method; Parameter estimation
An efficient budget allocation approach forquantifying the impact of input uncertainty in stochastic simulation,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033218467&doi=10.1145%2f3129148&partnerID=40&md5=d6503f680927a171733722141b1fd255,"Simulations are often driven by input models estimated from finite real-world data.When we use simulations to assess the performance of a stochastic system, there exist two sources of uncertainty in the performance estimates: input and simulation estimation uncertainty. In this article, we develop a budget allocation approach that can efficiently employ the potentially tight simulation resource to construct a percentile confidence interval quantifying the impact of the input uncertainty on the system performance estimates, while controlling the simulation estimation error. Specifically, nonparametric bootstrap is used to generate samples of input models quantifying both the input distribution family and parameter value uncertainty. Then, the direct simulation is used to propagate the input uncertainty to the output response. Since each simulation run could be computationally expensive, given a tight simulation budget, we propose an efficient budget allocation approach that can balance the finite sampling error introduced by using finite bootstrapped samples to quantify the input uncertainty and the system response estimation error introduced by using finite replications to estimate the system response at each bootstrapped sample. Our approach is theoretically supported, and empirical studies also demonstrate that it has better and more robust performance than direct bootstrapping. © 2017 ACM.",Budget allocation; Confidence interval; Input uncertainty; Nonparametric bootstrap; Percentile estimation,Errors; Stochastic models; Stochastic systems; Uncertainty analysis; Budget allocation; Confidence interval; Estimation uncertainties; Input distributions; Input uncertainty; Non-parametric bootstraps; Simulation resource; Stochastic simulations; Budget control
"Replicated computations results (RCR) report for ""green simulation: Reusing the output of repeated experiments""",2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033219462&doi=10.1145%2f3129738&partnerID=40&md5=fa847287dd7b2ac680aa95e6e87320eb,"""Green Simulation: Reusing the Output of Repeated Experiments"" by Feng and Staum describes methods based on likelihood ratio or importance sampling theory for reusing the outputs of simulation experiments at previous parameter settings to augment and improve (by reducing the estimator variance) simulation experiments at new parameter settings. The article presents empirical results for two realistic examples in the area of finance; Matlab code for these examples was made available by the authors. The examples were straightforward to run without extensive knowledge of Matlab, and both experiment and scenario parameters can be altered easily. All experiment results in the article were reproduced. © 2017 ACM.",Financial engineering; Importance sampling; Variance reduction,Importance sampling; Financial engineering; Likelihood ratios; Matlab code; New parameters; Parameter setting; Variance reductions; MATLAB
Parametrized adomian decomposition method with optimum convergence,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033227374&doi=10.1145%2f3106373&partnerID=40&md5=83fe22c68253b912631ad506313753b0,"The classical Adomian decomposition method frequently used to solve linear and nonlinear algebraic or integro-differential equations of ordinary and partial type is revisited. Rewriting the technique in an elegant form, a parameter so-called as the convergence control parameter, is embedded into the method to control the convergence and the rate of convergence of the method. Besides the constant level curves for determining suitable values, an effective approach for obtaining the best possible convergence control parameter is later devised based on the squared residual error of the studied problem. The optimum Adomian decomposition method is proved to converge to the true solution where the classical Adomian decomposition method fails to converge. When both methods are convergent, the present algorithm is observed to accelerate the rate of convergence. Moreover, the restricted domain of convergent physical solution obtained by the classical Adomian method is shown to be greatly extended to a finer interval by the optimum Adomian decomposition method. The justification of the new scheme is made clear on several mathematical/physical examples selected from the open literature. Finally, an example is provided to demonstrate the better accuracy of the optimum Adomian decomposition method over the recently popular homotopy analysis method. © 2017 ACM.",Adomian decomposition method; Convergence; Convergence acceleration; Extending the domain of convergence; Failure of the method; residual error,Approximation theory; Differential equations; Integrodifferential equations; Nonlinear equations; Ordinary differential equations; Adomian Decomposition Method; Convergence; Convergence acceleration; Extending the domain of convergence; Residual error; Algebra
"Replicated computations results (RCR) report for ""mNO-PQRS: Max nonnegativity ordering-piecewise-quadratic rate smoothing""",2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041447566&doi=10.1145%2f3097350&partnerID=40&md5=2d93ab9e512e0a21378080df39f506c0,"The article ""MNOPQRS: Max Nonnegativity Ordering-Piecewise-Quadratic Rate Smoothing"" by Chen and Schmeiser constructs a smooth piecewise-quadratic rate estimate for a nonhomogeneuous Poisson process based on event counts over κ adjacent time intervals. The event times can be generated by generating a Poisson process with unit rate and inverting the cumulative rate function or by the thinning technique. The overall algorithm hasO(κ2) time complexity andO(κ) space requirements in the number of intervals. This replicated computation report focuses on the reproducibility of the experimental results in the aforementioned article. © 2017 ACM.",Nonhomogeneous poisson process; Piecewise-quadratic smoothing; Rate function,Computer applications; Computer simulation; Non-homogeneous Poisson process; Piece-wise; Poisson process; Quadratic rates; Rate functions; Reproducibilities; Space requirements; Time complexity; Poisson distribution
A Tool for xMAS-Based modeling and analysis of communication fabrics in Simulink,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028652385&doi=10.1145%2f3005446&partnerID=40&md5=0d68024ddabfbd9e2ddf177cede93a37,"The eXecutable Micro-Architectural Specification (xMAS) language developed in recent years finds an effective way to model on-chip communication fabrics and enables performance-bound analysis with network calculus at the micro-architectural level. For network-on-Chip (NoC) performance analysis, model validation is essential to ensure correctness and accuracy. In order to facilitate the xMAS modeling and corresponding analysis validation, this work presents a unified platform based on xMAS in Simulink. The platform provides a friendly graphical user interface for xMAS modeling and parameter setup by taking advantages of the Simulink modeling environment. The regulator and latency-rate sever are added to the xMAS primitive set to support typical flow and service behaviors. Hierarchical model build-up and Verilog-HDL code generation are essentially supported to manage complex models and to conduct cycle-accurate bit-accurate simulations. Based on the generated simulation models of xMAS, this tool is applied to evaluate the tightness of analytical delay bound results. We demonstrate the application as well as the work flow of the xMAS tool through a two-agent communication example and an all-to-one communication example with a tree topology. © 2017 ACM.",Network calculus; Network on chip; Performance analysis; Simulink,Calculations; Graphical user interfaces; Hierarchical systems; Servers; User interfaces; Architectural levels; Architectural specifications; Hierarchical model; Network calculus; Network-on-chip(NoC); On chip communication; Performance analysis; Simulink; Network-on-chip
Multilevel sequential monte carlo samplers for normalizing constants,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028622223&doi=10.1145%2f3092841&partnerID=40&md5=1a4c407e27a92ec41527269e062fac1f,"This article considers the Sequential Monte Carlo (SMC) approximation of ratios of normalizing constants associated to posterior distributions which in principle rely on continuum models. Therefore, the Monte Carlo estimation error and the discrete approximation error must be balanced. A multilevel strategy is utilized to substantially reduce the cost to obtain a given error level in the approximation as compared to standard estimators. Two estimators are considered and relative variance bounds are given. The theoretical results are numerically illustrated for two Bayesian inverse problems arising from elliptic Partial Differential Equations (PDEs). The examples involve the inversion of observations of the solution of (i) a one-dimensional Poisson equation to infer the diffusion coefficient, and (ii) a two-dimensional Poisson equation to infer the external forcing. © 2017 ACM.",Bayesian inverse problems; Multilevel Monte Carlo; Sequential Monte Carlo,Continuum mechanics; Differential equations; Errors; Inverse problems; Partial differential equations; Poisson equation; Bayesian; Discrete approximation; Elliptic partial differential equation; Monte-Carlo estimation; Normalizing constants; Posterior distributions; Sequential Monte Carlo; Two dimensional Poisson equation; Monte Carlo methods
ParTejas: A parallel simulator for multicore processors,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027396563&doi=10.1145%2f3077582&partnerID=40&md5=c0262ff26441780da833ecf92d99bc05,"In this article, we present the design of a novel parallel architecture simulator called ParTejas. ParTejas is a timing simulation engine that gets its execution traces from instrumented binaries using a fast sharedmemory- based mechanism. Subsequently, the waiting threads simulate the execution of multiple pipelines and an elaborate memory system with support for multilevel coherent caches. ParTejas is written in Java and primarily derives its speedups from the use of novel data structures. Specifically, it uses lock-free slot schedulers to design an entity called a parallel port that effectively models the contention at shared resources in the CPU and memory system. Parallel ports remove the need for fine-grained synchronization and allow each thread to use its local clock. Unlike conventional simulators that use barriers for synchronization at epoch boundaries, we use a sophisticated type of barrier, known as a phaser. A phaser allows threads to perform additional work without waiting for other threads to arrive at the barrier. Additionally, we use a host of Java-specific optimizations and use profiling to effectively schedule the threads. With all our optimizations, we demonstrate a speedup of 11.8× for a multi-issue in-order pipeline and 10.9× for an out-of-order pipeline with 64 threads, for a suite of seven Splash2 and Parsec benchmarks. The simulation error is limited to 2% to 4% as compared to strictly sequential simulation. © 2017 ACM.",Architectural simulator; parallel ports; Parallel simulation; ParTejas; phasers; slot scheduling; Tejas,Cache memory; Integrated circuit design; Java programming language; Pipelines; Scheduling; Simulators; Architectural simulators; Parallel port; Parallel simulations; ParTejas; phasers; Slot scheduling; Tejas; Parallel architectures
A virtual WLAN device model for high-fidelity wireless network emulation,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027401093&doi=10.1145%2f3067664&partnerID=40&md5=794aeeb226ce111afb2f86ff015a6a7b,"The recent popularization of mobile devices has increased the amount of communication traffic. Hence, it is necessary both in academia and industry to research load distribution methods for mobile networks. An evaluation environment for large-scale networks that behaves like a practical system is necessary to evaluate these methods, and either a physical environment or simulation environment can be used. However, physical and simulation environments each have their advantages and disadvantages. A physical environment is suitable for practical operation because it is possible to obtain data from a real environment. In contrast, the cost for a large number of nodes and the difficulty of field preparation are its disadvantages. Reproducing radio propagation is also a challenge. Network simulators solve the disadvantages of the physical environment by modeling the entire evaluation environment. However, they do not exactly reproduce the physical environment because the nodes are abstracted. This article presents an evaluation environment that combines a network simulator and virtual machines with virtual wireless Local Area Network (LAN) devices. The virtual machines reproduce the physical environment with high fidelity by running the programs of the physical machines, and the virtual wireless LAN devices make it possible to emulate wireless LAN communication using default operating system drivers. A network simulator and virtual machines also reduce the cost for nodes, ease the burden of field preparation, and reproduce radio propagation by modeling the evaluation environment. In the evaluation, the proposed method decreased the difference from the physical environment to 5% in terms of transmission control protocol throughput. In the case of user datagram protocol, the proposed method decreased the difference from the physical environment down to 1.7%. The number of virtual machines available on a host machine and the practical use of the proposed method are also discussed. © 2017 ACM.",Virtual device; Virtual Machine; Wireless LAN emulation; Wireless LAN simulation,Mobile telecommunication systems; Network security; Radio waves; Simulators; Virtual machine; Wave propagation; Wireless networks; Large-scale network; Network simulators; Physical environments; Simulation environment; User datagram protocol; Virtual devices; Wireless LAN communication; Wireless local area network (LAN); Wireless local area networks (WLAN)
MNO-PQRS: Max nonnegativity ordering - Piecewise-quadratic rate smoothing,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027405431&doi=10.1145%2f3067663&partnerID=40&md5=8f4eb673367cefb41369b774adee0953,"In both cyclic and finite-horizon contexts, piecewise-constant rate functions are commonly encountered in models with nonhomogeneous Poisson processes. We develop an algorithm, with no user-specified parameters, that returns a smoother rate function that maintains the expected number of arrivals. The algorithm proceeds in two steps: PQRS (Piecewise-Quadratic Rate Smoothing) returns a continuous and differentiable piecewise-quadratic function without regard to negativity. If negative rates occur, then MNO (Max Nonnegativity Ordering) returns the maximum of zero and another piecewise-quadratic function. MNO maintains continuity of rates and first derivatives, but with some exceptions. Our analysis allows fitting theMNO-PQRS function to require storage complexity of the order of the number of intervals and computational complexity of the order of the number of intervals squared. MNO-PQRS can be used as a stand-alone routine, or as an endgame for the authors' earlier algorithm, I-SMOOTH. © 2017 ACM.",I-SMOOTH; Nonhomogeneous Poisson process; Piecewise linear; Piecewise-constant rate functions; Random variates; System simulation,Poisson distribution; Non-homogeneous Poisson process; Piece-wise constants; Piecewise linear; Random variates; System simulations; Piecewise linear techniques
Modeling and simulation of extreme-scale fat-tree networks for HPC systems and data centers,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025086248&doi=10.1145%2f2988231&partnerID=40&md5=a1b49ee1fe0e94046c6d6403b55fa6a0,"As parallel and distributed systems are evolving toward extreme scale, for example, high-performance computing systems involve millions of cores and billion-way parallelism, and high-capacity storage systems require efficient access to petabyte or exabyte of data, many new challenges are posed on designing and deploying next-generation interconnection communication networks in these systems. Fat-tree networks have been widely used in both data centers and high-performance computing (HPC) systems in the past decades and are promising candidates of the next-generation extreme-scale networks. In this article, we present FatTreeSim, a simulation framework that supports modeling and simulation of extreme-scale fattree networks with the goal of understanding the design constraints of next-generation HPC and distributed systems and aiding the design and performance optimization of the applications running on these systems. We have systematically experimented FatTreeSim on Emulab and Blue Gene/Q and analyzed the scalability and fidelity of FatTreeSim with various network configurations. On the Blue Gene/Q Mira, FatTreeSim can achieve a peak performance of 305 million events per second using 16, 384 cores. Finally, we have applied FatTreeSim to simulate several large-scale Hadoop YARN applications to demonstrate its usability. © 2017 ACM.",Distributed system; Fat-tree network; High-performance computing; Parallel discrete event simulation,Data storage equipment; Digital storage; Discrete event simulation; Forestry; Next generation networks; Trees (mathematics); Distributed systems; Fat trees; Generation interconnection; High performance computing; High performance computing systems; Parallel and distributed systems; Parallel discrete event simulations; Performance optimizations; Distributed computer systems
Guest editorial for the TOMACS special issue on the Principles of Advanced Discrete Simulation (PADS),2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025110719&doi=10.1145%2f3084543&partnerID=40&md5=76eaaddd1e84fbfbead3219944edb13f,[No abstract available],,
Relaxing synchronization in parallel agent-based road traffic simulation,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022185479&doi=10.1145%2f2994143&partnerID=40&md5=fc137f07723293f6ef69692ddbe81b9d,"Large-scale agent-based traffic simulation is computationally intensive. Parallel computing can help to speed up agent-based traffic simulation. Parallelization of agent-based traffic simulations is generally achieved by decomposing the road network into subregions. The agents in each subregion are executed by a Logical Process (LP). There are data dependencies between LPs which require synchronization of LPs. An asynchronous protocol allows LPs to progress and communicate asynchronously. LPs use lookahead to indicate the time to synchronize with other LPs. Larger lookahead means less frequent synchronization operations. High synchronization overhead is still a major performance issue of large-scale parallel agent-based traffic simulations. In this article, two methods to increase the lookahead of LPs for an asynchronous protocol are developed. They take advantage of uncertainties in traffic simulation to relax synchronization without altering simulation results statistically. Efficiency of the proposed methods is investigated in the parallel agent-based traffic simulator SEMSim Traffic. Experiment results showed that the proposed methods are able to reduce overall running time of the parallel simulation compared to existing methods. © 2017 ACM.",Agent-based road traffic simulation; Asynchronous and conservative synchronization; Phrases; Relaxation,Roads and streets; Synchronization; Transportation; Vehicle actuated signals; Asynchronous protocols; Conservative Synchronization; Parallel simulations; Phrases; Relaxation; Road traffic simulation; Synchronization operation; Traffic simulations; Traffic control
A fine-grain time-sharing Time Warp system,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025076808&doi=10.1145%2f3013528&partnerID=40&md5=90276da051999a150fd771460dac230d,"Several techniques have been proposed to improve the performance of Parallel Discrete Event Simulation platforms relying on the Time Warp (optimistic) synchronization protocol. Among them we can mention optimized approaches for state restore, as well as techniques for load balancing or (dynamically) controlling the speculation degree, the latter being specifically targeted at reducing the incidence of causality errors leading to waste of computation. However, in state-of-the-art Time Warp systems, events' processing is not preemptable, which may prevent the possibility to promptly react to the injection of higher priority (say, lower timestamp) events. Delaying the processing of these events may, in turn, give rise to higher incidence of incorrect speculation. In this article, we present the design and realization of a fine-grain timesharing Time Warp system, to be run on multi-core Linux machines, which makes systematic use of event preemption in order to dynamically reassign the CPU to higher priority events/tasks. Our proposal is based on a truly dual mode execution, application versus platform, which includes a timer-interrupt-based support for bringing control back to platform mode for possible CPU reassignment according to very fine grain periods. The latter facility is offered by an ad-hoc timer-interrupt management module for Linux, which we release, together with the overall time-sharing support, within the open source ROOT-Sim platform. An experimental assessment based on the classical PHOLD benchmark and two real-world models is presented, which shows how our proposal effectively leads to the reduction of the incidence of causality errors, especially when running with higher degrees of parallelism. © 2017 ACM.",Optimistic synchronization,Computer operating systems; Discrete event simulation; Linux; Experimental assessment; Linux machines; Open sources; Optimistic synchronization; Parallel discrete event simulations; State of the art; Synchronization protocols; Time-sharing; Distributed computer systems
Transparently mixing undo logs and software reversibility for state recovery in optimistic PDES,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025084437&doi=10.1145%2f3077583&partnerID=40&md5=84a8714e4a5bf43e0f630c101fe2a702,"The Time Warp synchronization protocol for Parallel Discrete Event Simulation (PDES) is universally considered a viable solution to exploit the intrinsic simulation model parallelism and to provide model execution speedup. Yet it leads the PDES system to execute events in an order that may generate causal inconsistencies that need to be recovered via rollback, which requires restoration of a previous (consistent) simulation state whenever a causality violation is detected. The rollback operation is so critical for the performance of a Time Warp system that it has been extensively studied in the literature for decades to find approaches suitable to optimize it. The proposed solutions can be roughly classified as based on either checkpointing or reverse computing. In this article, we explore the practical design and implementation of a fully new approach based on the runtime generation of so-called undo code blocks, which are blocks of instructions implementing the reverse memory side effects generated by the forward execution of the events. However, this is not done by recomputing the original values to be restored, as instead it occurs in reverse computing schemes. Hence, the philosophy undo code blocks rely on is similar in spirit to that of undo-logs (as a form of checkpointing). Nevertheless, they are not data logs (as instead checkpoints are); rather, they are logs of instructions. Our proposal is fully transparent, thanks to the reliance on static software instrumentation (targeting the x86 architecture and Linux systems). Also, as we show, it can be combined with classical checkpointing to further improve the runtime behavior of the state recoverability support as a function of the workload. We also present experimental results related to our implementation, which is released as free software and fully integrated into the open source ROOT-Sim package. Experimental data support the viability and effectiveness of our proposal. © 2017 ACM.",Optimistic synchronization; Software reversibility; Time warp,Computer operating systems; Discrete event simulation; Distributed computer systems; Open source software; Restoration; Design and implementations; Model executions; Optimistic synchronization; Parallel discrete event simulations; Runtime behaviors; Software instrumentation; Synchronization protocols; Time Warp; Open systems
Automatic model generation for gate-level circuit PDES with reverse computation,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025106292&doi=10.1145%2f3046685&partnerID=40&md5=fe1caa2d29df43c6048f0da99f22fcfe,"Gate-level circuit simulation is an important step in the design and validation of complex circuits. This step of the process relies on existing libraries for gate specifications. We start with a generic gate model for Rensselaer's Optimistic Simulation System, a parallel discrete-event simulation framework. This generic model encompasses all functionality needed by optimistic simulation using reverse computation. We then describe a parser system that uses a standardized gate library to create a specific model for simulation. The generated model is composed of several functions, including those needed for an accurate model of timing behavior. To quantify the improvements that an automatically generated model can have over a hand written model, we compare two gate library models: an automatically generated LSI-10K library model and a previously investigated, handwritten, simplified GTECH library model. We conclude that the automatically generated model is a more accurate model of actual hardware. In comparison to previous results, we find that the automatically generated model is able to achieve better optimistic simulation performance when measured against conservative simulation. To test the automatically generated model, we evaluate the performance of a simulation of a full-scale OpenSPARC T2 processor model. This model consists of nearly 6 million LPs. We achieve a peak performance of 1.63 million events per second during a conservative simulation. To understand the relatively weaker performance of optimistic simulation, we investigate hot spots of event activity and visually identify a workload imbalance. © 2017 ACM.",Model development; Model development: Methodologies; Types of simulation: Discrete event; Types of simulation: Parallel,Automatic test pattern generation; Circuit simulation; Timing circuits; Automatic model generation; Automatically generated; Discrete events; Model development; Modeling for simulations; Optimistic simulation; Parallel discrete event simulations; Types of simulation: Parallel; Discrete event simulation
Cloning agent-based simulation,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022335616&doi=10.1145%2f3013529&partnerID=40&md5=a85b6ffd1527be867efe32a5b38ea530,"Simulation cloning is an efficient way to analyze multiple configurations in a parameter exploration task. A simulation model usually contains a set of tunable parameters for exploring different configurations of a system. To evaluate different design alternatives, multiple simulation instances need to be launched, each evaluating a different parameter configuration. It usually takes a considerable amount of time to execute these simulation instances. Simulation cloning is proposed to reuse computations among simulation instances and to shorten the overall execution time. It is a challenging task to design cloning strategies to explore the computation sharing among simulation instances while maintaining the correctness of execution. In this article, we propose two agent-based simulation (ABS) cloning strategies, the top-down cloning strategy and the bottom-up cloning strategy. The top-down cloning strategy is initially designed and can only be applied to limited scenarios. The bottom-up cloning strategy is an improved strategy to overcome the limitation of the top-down cloning strategy. In the experiments, the effectiveness of the two strategies is analyzed. To show the performance advantages and generality of the bottom-up cloning strategy, a large-scale ABS parameter exploration task is performed, and results are discussed in the article. © 2017 ACM.",Agent-based simulation; Complex systems; GPGPU; Simulation cloning; Speedup,Genetic engineering; Large scale systems; Program processors; Agent based simulation; Computation sharing; Design alternatives; GPGPU; Multiple configurations; Parameter exploration; Simulation cloning; Speedup; Cloning
"Replicated computations results (RCR) report for ""semantics and efficient simulation algorithms for an expressive multi-level modeling language""",2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018728036&doi=10.1145%2f3055539&partnerID=40&md5=bbcc908dca2131bce7a0fd7489dd90f2,"""Semantics and Efficient Simulation Algorithms on an Expressive Multi-Level Modeling Language,"" by Helms et al. presents new work on the domain-specific modelling and simulation language ML-Rules [Maus et al. 2011]. For the first time, the language is given a formal semantics that establishes the relationship between the language and its underlying mathematical model, continuous time Markov chains. Furthermore, subclasses of the language are identified for which it is possible to specify and implement more efficient approaches to simulation. These new algorithms are demonstrated on substantial case studies. This replicated computation report focuses on the ML-Rules modelling tool, specifically, the new algorithms as demonstrated in the case studies in the paper [Helms et al. 2017]. The software was straightforward to install and use, and all experimental results from the paper could be reproduced. © 2017 ACM.",Efficient Simulation Algorithms; Formal semantics; RCR report; Systems biology,Aluminum; Computer simulation languages; Continuous time systems; Formal methods; Markov processes; Semantics; Continuous time Markov chain; Domain-specific modelling; Efficient simulation; Formal Semantics; Modelling tools; Multilevel model; RCR report; Systems biology; Modeling languages
Multithreaded stochastic pdes for reactions and diffusions in neurons,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018746387&doi=10.1145%2f2987373&partnerID=40&md5=691e13f9d602a61d3f2d502b0d849a9f,"Cells exhibit stochastic behavior when the number of molecules is small. Hence a stochastic reaction-diffusion simulator capable of working at scale can provide a more accurate view of molecular dynamics within the cell. This article describes a parallel discrete event simulator, Neuron Time Warp-Multi Thread (NTW-MT), developed for the simulation of reaction diffusion models of neurons. To the best of our knowledge, this is the first parallel discrete event simulator oriented toward stochastic simulation of chemical reactions in a neuron. The simulator was developed as part of the NEURON project. NTW-MT is optimistic and thread based, which attempts to capitalize on multicore architectures used in high performance machines. It makes use of a multilevel queue for the pending event set and a single rollback message in place of individual antimessages to disperse contention and decrease the overhead of processing rollbacks. Global Virtual Time is computed asynchronously both within and among processes to get rid of the overhead for synchronizing threads. Memory usage is managed in order to avoid locking and unlocking when allocating and deallocating memory and to maximize cache locality. We verified our simulator on a calcium buffer model. We examined its performance on a calcium wave model, comparing it to the performance of a process based optimistic simulator and a threaded simulator which uses a single priority queue for each thread. Our multithreaded simulator is shown to achieve superior performance to these simulators. Finally, we demonstrated the scalability of our simulator on a larger Calcium-Induced Calcium Release (CICR) model and a more detailed CICR model. © 2016 ACM.",Multiple thread; PDES; Stochastic neuronal simulation,Cache memory; Calcium; Diffusion; Diffusion in liquids; Discrete event simulation; Distributed computer systems; Locks (fasteners); Molecular dynamics; Neurons; Reaction kinetics; Simulators; Software architecture; Stochastic models; Discrete-event simulators; High-performance machines; Multicore architectures; Multiple threads; PDES; Reaction-diffusion models; Stochastic neuronal simulation; Stochastic simulations; Stochastic systems
Semantics and efficient simulation algorithms of an expressive multilevel modeling language,2017,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018767343&doi=10.1145%2f2998499&partnerID=40&md5=d8cc6a9b8e03a276b6c98d39f41beba1,"The domain-specific modeling and simulation language ML-Rules is aimed at facilitating the description of cell biological systems at different levels of organization. Model states are chemical solutions that consist of dynamically nested, attributed entities. The model dynamics are described by rules that are constrained by arbitrary functions, which can operate on the entities' attributes, (nested) solutions, and the reaction kinetics. Thus, ML-Rules supports an expressive hierarchical, variable structure modeling of cell biological systems. The formal syntax and semantics of ML-Rules show that it is firmly rooted in continuous-time Markov chains. In addition to a generic stochastic simulation algorithm for ML-Rules, we introduce several specialized algorithms that are able to handle subclasses of ML-Rules more efficiently. The algorithms are compared in a performance study, leading to conclusions on the relation between expressive power and computational complexity of rule-based modeling languages. © 2017 ACM.",Formal languages; Multilevel modeling; Performance; Simulation algorithms,Bioinformatics; Biological systems; Computer simulation languages; Formal languages; Hierarchical systems; Markov processes; Reaction kinetics; Semantics; Stochastic models; Stochastic systems; Cell-biological systems; Continuous time Markov chain; Domain specific modeling; Multilevel model; Performance; Simulation algorithms; Stochastic simulation algorithms; Variable structure model; Modeling languages
Exploiting social capabilities in the minority game,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007014433&doi=10.1145%2f2996456&partnerID=40&md5=cebd93ed8beca61b1344bd0b81d3c2d8,"The minority game (MG) is an inductive binary-decision model that is able to study emergent behaviors in a population of agents who compete, through adaptation, for scarce resources. The original formulation of the game was inspired by the W.B. Arthur's El Farol Bar problem: a fixed number of people have to independently decide, each week, whether to go to a bar having a limited capacity. People's choices are only affected by the information about the number of visitors who attended the bar in the past weeks. Basic MG assumes that the information about the past game outcomes is publicly available, and it does not contemplate any communication among players. This article proposes the Dynamic Sociality Minority Game (DSMG). DSMG is an original variant of the classic MG where (1) information about the outcome of the previously played game step is only known to agents that really attended the bar the previous week, and (2) a dynamically established acquaintance network is introduced to propagate such information to nonattendant players. Specific settings of the game are identified in which DSMG is able to show a better coordination level among players with respect to the standard MG. Emergent properties of the DSMG along with players' wellness are thoroughly analyzed through agent-based simulation of a simple road-traffic model. © 2016 ACM 1049-3301/2016/11-ART6 $15.00.",Minority game; Multiagent learning; Simulation; Social ability,Computer applications; Computer simulation; Agent based simulation; Coordination levels; El Farol bar problem; Emergent behaviors; Minority game; Multi-agent learning; Simulation; Social abilities; Multi agent systems
Multivariate input uncertainty in output analysis for stochastic simulation,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994516289&doi=10.1145%2f2990190&partnerID=40&md5=01753a7b250b713f1a2203c7aee08442,"When we use simulations to estimate the performance of stochastic systems, the simulation is often driven by input models estimated from finite real-world data. A complete statistical characterization of system performance estimates requires quantifying both input model and simulation estimation errors. The components of input models in many complex systems could be dependent. In this paper, we represent the distribution of a random vector by its marginal distributions and a dependence measure: either product-moment or Spearman rank correlations. To quantify the impact from dependent input model and simulation estimation errors on system performance estimates, we propose a metamodel-assisted bootstrap framework that is applicable to cases when the parametric family of multivariate input distributions is known or unknown. In either case, we first characterize the input models by their moments that are estimated using real-world data. Then, we employ the bootstrap to quantify the input estimation error, and an equation-based stochastic kriging metamodel to propagate the input uncertainty to the output mean, which can also reduce the influence of simulation estimation error due to output variability. Asymptotic analysis provides theoretical support for our approach, while an empirical study demonstrates that it has good finite-sample performance. © 2016 ACM.",Bootstrap; Confidence interval; Gaussian process; Multivariate input uncertainty; NORTA; Output analysis,Asymptotic analysis; Errors; Sampling; Stochastic models; Stochastic systems; Bootstrap; Confidence interval; Gaussian Processes; Input uncertainty; NORTA; Output analysis; Uncertainty analysis
On the marginal standard error rule and the testing of initial transient deletion methods,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981351683&doi=10.1145%2f2961052&partnerID=40&md5=0046a8a28ce46e702c652ce2643b5f93,"In the planning of steady-state simulations, a central issue is the initial transient problem, in which an initial segment of the simulation output is adversely contaminated by initialization bias. Our article makes several contributions toward the analysis of this computational challenge. To begin, we introduce useful ways for measuring the magnitude of the initial transient effect in the single replication setting. We then analyze the marginal standard error rule (MSER) and prove that MSER's deletion point is determined, as the simulation time horizon tends to infinity, by the minimizer of a certain random walk. We use this insight, together with fluid limit intuition associated with queueing models, to generate two nonpathological examples in which at least one variant of MSER fails to accurately predict the duration of the initial transient. Our results suggest that the efficacy of a deletion procedure is sensitive to the choice of performance measure, and that the set of standard test problems on which initial transient procedures are tested should be significantly broadened. © 2016 ACM.",Algorithms; Fluid limits; I.6.6 [Simulation and modeling]: simulation output analysis; Initial transient problem; MSER; Performance; Queueing theory; Theory; Truncation procedures,Algorithms; Computation theory; Fluid limits; MSER; Performance; Simulation output analysis; Theory; Transient problems; Truncation procedure; Queueing theory
MTSS - A marine traffic simulation system and scenario studies for a major hub port,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981306120&doi=10.1145%2f2897512&partnerID=40&md5=5c195c92086fa3adaf798e8697688755,"Hub ports need to ensure that their navigational networks can fulfill increased demand in marine traffic. They also need to assess the possible impacts of an accident resulting in partial or complete closure of navigation channels. For lack of adequate analytical tools, modeling and simulation are the only means for such studies. To date, however, no adequate simulation tool exists for modeling and simulating the complex traffic at a large-scale hub port. The challenge is to efficiently model the large number of interacting vessels while accurately reflecting the navigational behaviors of various types of vessels whose movements must comply with prevailing protocols in a location- and situation-aware fashion. We present a systematic approach that enables the construction of a marine traffic simulation system called MTSS. MTSS was calibrated based on detailed analysis of historical records obtained from a major hub port, and it was validated by the domain experts. MTSS was used in a capacity study of marine traffic at a hub port that is unique in the scale and complexity of its waterway networks, the intricacies of its traffic patterns, and the required accuracy of the navigational behaviors of different types of vessels. The usefulness of MTSS is further demonstrated by applying it to assess the impacts of partial closure of a waterway under an emergency scenario. For large-scale hub ports, MTSS now opens up new possibilities of realistic scenario studies and disruption management. © 2016 ACM.",Design; Experiment design and simulation analysis; I.6.5 [model development]: modeling methodologies; Marine traffic modelling and simulation; Measurement; Model and simulation of navigational scenarios at hub ports; Verification,Complex networks; Design; Measurements; Traffic control; Verification; Disruption management; Experiment design; Marine traffic; Model and simulation; Modeling and simulating; Modeling methodology; Navigation channels; Realistic scenario; Navigation
A Bayesian approach to parameter inference in queueing networks,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048823581&doi=10.1145%2f2893480&partnerID=40&md5=17f254c3428327e2841eb47ea2b7d3e8,"The application of queueing network models to real-world applications often involves the task of estimating the service demand placed by requests at queueing nodes. In this article, we propose a methodology to estimate service demands in closed multiclass queueing networks based on Gibbs sampling. Ourmethodology requires measurements of the number of jobs at resources and can accept prior probabilities on the demands. Gibbs sampling is challenging to apply to estimation problems for queueing networks since it requires one to efficiently evaluate a likelihood function on the measured data. This likelihood function depends on the equilibrium solution of the network, which is difficult to compute in closed models due to the presence of the normalizing constant of the equilibrium state probabilities. To tackle this obstacle, we define a novel iterative approximation of the normalizing constant and show the improved accuracy of this approach, compared to existing methods, for use in conjunction with Gibbs sampling. We also demonstrate that, as a demand estimation tool, Gibbs sampling outperforms other popular Markov Chain Monte Carlo approximations. Experimental validation based on traces from a cloud application demonstrates the effectiveness of Gibbs sampling for service demand estimation in real-world studies. © 2016 ACM.",Demand estimation; Gibbs sampling; Normalizing constant approximation; Queueing,Bayesian networks; Iterative methods; Markov processes; Queueing theory; Demand estimation; Experimental validations; Gibbs sampling; Iterative approximations; Markov Chain Monte-Carlo; Multi-class queueing networks; Normalizing constants; Queueing; Queueing networks
Cycle-accurate network on chip simulation with Noxim,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007507361&doi=10.1145%2f2953878&partnerID=40&md5=c8313b239488f3339577edb2514fffdd,"The on-chip communication in current Chip-MultiProcessors (CMP) and MultiProcessor-SoC (MPSoC) is mainly based on the Network-on-Chip (NoC) design paradigm. Unfortunately, it is foreseen that conventional NoC architectures cannot sustain the performance, power, and reliability requirements demanded by the next generation of manycore architectures. Recently, emerging on-chip communication technologies, like wireless Networks-on-Chip (WiNoCs), have been proposed as candidate solutions for addressing the scalability limitations of conventional multi-hop NoC architectures. In a WiNoC, a subset of network nodes are equipped with a wireless interface which allows them long-range communication in a single hop. Assessing the performance and power figures of NoC and WiNoC architectures requires the availability of simulation tools that are often limited on modeling specific network configurations. This article presents Noxim, an open, configurable, extendible, cycle-accurate NoC simulator developed in SystemC, which allows to analyze the performance and power figures of both conventional wired NoC and emerging WiNoC architectures. © 2016 ACM.",,Multiprocessing systems; Network architecture; Programmable logic controllers; Servers; System-on-chip; Wireless interconnects; Chip multiprocessors; Long-range communications; Many-core architecture; Network configuration; Network-on-chip design; On chip communication; Reliability requirements; Wireless interfaces; Network-on-chip
Research challenges in parallel and distributed simulation,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969963487&doi=10.1145%2f2866577&partnerID=40&md5=8244d12e2b5df7bf49a183f8f7c9d9a7,"The parallel and distributed simulation field has evolved and grown from its origins in the 1970s and 1980s and remainsanactivefieldofresearchtothis day.Abrief overview of researchinthefieldispresented. Future research topics are explored including areas such as problem-driven simulation of large-scale systems and complex networks, exploitation of graphical processing unit hardware and cloud computing environments, predictive online simulation for system management and optimization, power and energy consumption in mobile platforms and data centers, and composition of heterogeneous simulations. © 2016 ACM.",Parallel discrete event simulation; Research challenges,Complex networks; Discrete event simulation; Energy utilization; Graphics processing unit; Green computing; Information management; Large scale systems; Online systems; Cloud computing environments; Graphical processing unit (GPUs); Heterogeneous simulation; Online simulation; Parallel and distributed simulation; Parallel discrete event simulations; Research challenges; System management; Distributed computer systems
"Replicated computational results (RCR) report for ""automatic moment-closure approximation of spatially distributed collective adaptive systems",2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964411277&doi=10.1145%2f2893479&partnerID=40&md5=302a2807519259082e0275099510d8fd,"""Automatic Moment-Closure Approximation of Spatially Distributed Collective Adaptive Systems"" by Feng, Hilston, and Galpin presents detailed simulation analysis results for three models of spatially distributed collective adaptive systems. In this replicated computational results report, the corresponding implementation together with a documentation that was provided to the reviewer by the authors are considered. The software was installed, and new simulation results were generated and compared to the original results. The installation of the software did not result in any problems, and the comparison of the results yielded that the published results are replicable. © 2016 ACM.",Model reduction; Moment-closure approximation; RCR report,Adaptive systems; Computer software; Spatial distribution; Computational results; Model reduction; Moment closure approximations; RCR report; Simulation analysis; Three models; Distributed computer systems
Real-time crowd simulation integrating potential fields and agent method,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964528964&doi=10.1145%2f2885496&partnerID=40&md5=d82f59527b1c71f854d4aac5674af190,"Crowd simulation is studied extensively in computer graphics, animation, and safety. A real-time crowd simulator has been developed based on potential fields and agent approach in this article. This simulator produces realistic complex heterogeneous motion and improves the simulation rates by at least 32% in comparison with the potential field results. The model of this simulator can efficiently tackle the problems in global optimal navigation, collision avoidance, and dynamic interaction; furthermore, it allows an agent to make independent decisions. © 2016 ACM.",Agent; Animation; Crowd dynamics; Crowd simulation; Human behavior; Potential field; Types of simulation,Agents; Animation; Computer graphics; Simulators; Crowd dynamics; Crowd Simulation; Human behaviors; Potential field; Types of simulation; Behavioral research
Automatic moment-closure approximation of spatially distributed collective adaptive systems,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964788287&doi=10.1145%2f2883608&partnerID=40&md5=d090c8eafd0b3a21315806e90e290873,"Spatially distributed collective adaptive systems are an important class of systems that pose significant challenges to modeling due to the size and complexity of their state spaces. This problem is acute when the dynamic behavior of the system must be captured, such as to predict system performance. In this article, we present an abstraction technique that automatically derives a moment-closure approximation of the dynamic behavior of a spatially distributed collective adaptive system from a discrete representation of the entities involved. The moment-closure technique is demonstrated to give accurate estimates of dynamic behavior, although the number of ordinary differential equations generated for the second-order joint moments can grow large in some cases. For these cases, we propose a rigorous model reduction technique and demonstrate its use to substantially reduce the computational effort with only limited impact on the accuracy if the reduction threshold is set appropriately. All techniques reported in this article are implemented in a tool that is freely available for download. © 2016 ACM.",Collective adaptive systems; Model reduction; Moment-closure approximation,Differential equations; Ordinary differential equations; Spatial distribution; Abstraction techniques; Computational effort; Dynamic behaviors; Model reduction; Moment closure; Moment closure approximations; Rigorous model; Second orders; Adaptive systems
Efficient Flattening Algorithm for Hierarchical and Dynamic Structure Discrete Event Models,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968861317&doi=10.1145%2f2875356&partnerID=40&md5=b93f7d160dc1b42a78533fbb05bb1fe5,"Discrete event models are widely used to replicate, analyze, and understand complex systems. DEVS (Discrete Event System Specification) formalism enables hierarchical modeling, so it provides an efficiency in the model development of complex models. However, the hierarchical modeling incurs prolonged simulation executions due to indirect event exchanges through the model hierarchy. Although direct event paths are applied to mitigate this overhead, the situation becomes even worse when a model changes its structures during simulation execution, called a dynamic structure model. This article suggests Coupling Relation Graph (CRG) and Strongly Coupled Component (SCC) concepts to improve hierarchical and dynamic structure DEVS simulation execution. CRG is a directed graph representing DEVS model structure, and SCC is a group of connected components in a CRG. Using CRG and SCC, this article presents (1) how to develop CRG from a DEVS model and (2) how to construct and update direct event paths with respect to dynamic structural changes. In particular, compared to the previous works, the proposed method focuses on the reduction of the updating costs for the direct event paths. Through theoretical and empirical analyses, this article shows that the proposed method significantly reduces the simulation execution time, especially when a simulation model contains lots of components and changes its model structures frequently. We expect that the proposed method would support the faster simulation executions of complex hierarchical and dynamic structure models. © 2016 ACM.",DEVS; dynamic structure model; Flattening algorithm; graph-based acceleration; Hierarchical model,Algorithms; Directed graphs; Graphic methods; Hierarchical systems; Model structures; Specifications; Connected component; DEVS; Discrete event models; Discrete event system specification; Dynamic structural changes; Dynamic structure models; Graph-based; Hierarchical model; Discrete event simulation
A large-deviation-based splitting estimation of power flow reliability,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958246311&doi=10.1145%2f2875342&partnerID=40&md5=bb40af98af5ca1d6863b5b012e689b3d,"Given the continued integration of intermittent renewable generators in electrical power grids, connection overloads are of increasing concern for grid operators. The risk of an overload due to injection variability can be described mathematically as a barrier-crossing probability of a function of a multidimensional stochastic process. Crude Monte Carlo is a well-known technique to estimate probabilities, but it may be computationally too intensive in this case as typical modern power grids rarely exhibit connection overloads. In this article, we derive an approximate rate function for the overload probability using results from large deviations theory. Based on this large deviations approximation, we apply a rare event simulation technique called splitting to estimate overload probabilities more efficiently than Crude Monte Carlo simulation. We show on example power grids with up to 11 stochastic power injections that for a fixed accuracy, Crude Monte Carlo would require tens to millions as many samples as the proposed splitting technique required. We investigate the balance between accuracy and workload of three splitting schemes, each based on a different approximation of the rate function. We justify the workload increase of large-deviation-based splitting compared to naive splitting - that is, splitting based on merely the Euclidean distance to the rare event set. For a fixed accuracy, naive splitting requires over 60 times as much CPU time as large-deviationbased splitting, illustrating its computational advantage. In these examples, naive splitting - unlike largedeviation- based splitting - requires even more CPU time than CMC simulation, illustrating its pitfall. © 2016 ACM.",Importance splitting; Large deviations theory; Monte carlo; Power grids; Rare event,Computation theory; Electric load flow; Electric power transmission networks; Intelligent systems; Probability; Random processes; Stochastic systems; Computational advantages; Importance splitting; Large deviations theory; Power grids; Rare event; Rare event simulation; Renewable generators; Splitting techniques; Monte Carlo methods
Actor-critic algorithms with online feature adaptation,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958247295&doi=10.1145%2f2868723&partnerID=40&md5=05c921bfb9df59d91cecd7ab8eb5fba2,"We develop two new online actor-critic control algorithms with adaptive feature tuning for Markov Decision Processes (MDPs). One of our algorithms is proposed for the long-run average cost objective, while the other works for discounted cost MDPs. Our actor-critic architecture incorporates parameterization both in the policy and the value function. A gradient search in the policy parameters is performed to improve the performance of the actor. The computation of the aforementioned gradient, however, requires an estimate of the value function of the policy corresponding to the current actor parameter. The value function, on the other hand, is approximated using linear function approximation and obtained from the critic. The error in approximation of the value function, however, results in suboptimal policies. In our article, we also update the features by performing a gradient descent on the Grassmannian of features to minimize a mean square Bellman error objective in order to find the best features. The aim is to obtain a good approximation of the value function and thereby ensure convergence of the actor to locally optimal policies. In order to estimate the gradient of the objective in the case of the average cost criterion, we utilize the policy gradient theorem, while in the case of the discounted cost objective, we utilize the simultaneous perturbation stochastic approximation (SPSA) scheme. We prove that our actor-critic algorithms converge to locally optimal policies. Experiments on two different settings show performance improvements resulting from our feature adaptation scheme. © 2016 ACM.",Actor-critic algorithms; Feature adaptation; Function approximation; Grassmann manifold; Markov decision processes; Online learning; Policy gradients; Residual gradient scheme; SPSA; Stochastic approximation; Temporal difference learning,Approximation algorithms; Approximation theory; Computation theory; Cost benefit analysis; Costs; Learning algorithms; Markov processes; Multimedia services; Optimization; Stochastic control systems; Stochastic systems; Actor-critic algorithm; Feature adaptation; Function approximation; Grassmann manifold; Markov Decision Processes; Online learning; Policy gradient; Residual gradient; SPSA; Stochastic approximations; Temporal difference learning; Algorithms
Static Network Reliability Estimation under the Marshall-Olkin Copula,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957084619&doi=10.1145%2f2775106&partnerID=40&md5=3096560c958286eb98d6ee1b090662e6,"In a static network reliability model, one typically assumes that the failures of the components of the network are independent. This simplifying assumption makes it possible to estimate the network reliability efficiently via specialized Monte Carlo algorithms. Hence, a natural question to consider is whether this independence assumption can be relaxed while still attaining an elegant and tractable model that permits an efficient Monte Carlo algorithm for unreliability estimation. In this article, we provide one possible answer by considering a static network reliability model with dependent link failures, based on a Marshall-Olkin copula, which models the dependence via shocks that take down subsets of components at exponential times, and propose a collection of adapted versions of permutation Monte Carlo (PMC, a conditional Monte Carlo method), its refinement called the turnip method, and generalized splitting (GS) methods to estimate very small unreliabilities accurately under this model. The PMC and turnip estimators have bounded relative error when the network topology is fixed while the link failure probabilities converge to 0, whereas GS does not have this property. But when the size of the network (or the number of shocks) increases, PMC and turnip eventually fail, whereas GS works nicely (empirically) for very large networks, with over 5,000 shocks in our examples. © 2016 ACM.",Conditional monte carlo; Dependent components; Marshall-olkin copula; Permutation monte carlo; Splitting; Static network reliability; Turnip,Reliability; Dependent components; Marshall-olkin copula; Splitting; Static networks; Turnip; Monte Carlo methods
Guests editors' editorial note on special issue of advances in cellular automata modeling,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957093188&doi=10.1145%2f2856511&partnerID=40&md5=f79fcc31e685b2c648e6ebce4f99c2a4,[No abstract available],,
Computing Bayesian means using simulation,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957078350&doi=10.1145%2f2735631&partnerID=40&md5=25c135ca76465ff1b12073e3c465c6ab,"This article is concerned with the estimation of α = E{r(Z)}, where Z is a random vector and the function values r(z)must be evaluated using simulation. Estimation problems of this form arise in the field of Bayesian simulation, where Z represents the uncertain (input) parameters of a system and r(z) is the expected performance of the system when Z = z. Our approach involves obtaining (possibly biased) simulation estimates of the function values r(z) for a number of different values of z, and then using a (possibly weighted) average of these estimates to estimate a. We start by considering the case where the chosen values of z are independent and identically distributed observations of the random vector Z (independent sampling). We analyze the resulting estimator as the total computational effort c grows and provide numerical results. Then we show that improved convergence rates can be obtained through the use of techniques other than independent sampling. Specifically, our results indicate that the use of quasi-random sequences yields a better convergence rate than independent sampling, and that in the presence of a suitable special structure, it may be possible to use other numerical integration techniques (such as Simpson's rule) to achieve the best possible rate c-1/2 as c→∞. Finally, we present and analyze a general framework of estimators for a that encompasses independent sampling, quasi-random sequences, and Simpson's rule as special cases. © 2016 ACM.",Bayesian simulation; Bias; Consistency; Convergence rate; Independent sampling; Nested simulation; Numerical integration; Quasirandom numbers; Simpson's rule,Computer applications; Computer simulation; Bayesian simulation; Bias; Consistency; Convergence rates; Nested simulation; Numerical integrations; Quasi-random numbers; Simpson's rule; Uncertainty analysis
Intensional couplings in variable-structure models: An exploration based on multilevel-DEVS,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957105697&doi=10.1145%2f2818641&partnerID=40&md5=a605c742cc282ac7219b749a46437f2a,"In modular, hierarchical modeling, couplings (connections) describe and constrain the communication, and thus interaction, between model components. Defining couplings between a large set of components in an extensional manner-listing all existing couplings individually-often proves to be rather tedious. Moreover, if models change their structure, that is, composition and interaction patterns and, in some cases, even their interfaces during simulation, questions about the consistency of the couplings arise. For instance, an extensionally defined coupling may refer to a model that no longer exists. Instead, an intensional coupling definition, based on attributes of the components to couple and dynamically translated into concrete couplings during simulation, promises to alleviate these problems. We propose a concept that combines a flexible, yet expressive, definition of couplings that rests on component interfaces announcing attributes of interest. However, intensional couplings come at a price, as they need to be translated during simulation; in variable-structure models, this translation has to happen frequently. We illuminate our concept based on a revision of the modeling formalism Multilevel Discrete Event System Specification (ML-DEVS). Developed for multilevel modeling and simulation, ML-DEVS exhibits another alternative to intensional couplings, that is, sharing parts of model states for up- and downward causation. The intricate interplay between these different types of couplings is revealed in the abstract simulator of ML-DEVS. © 2016 ACM.",Coupling schemes; DEVS; Dynamic interfaces; Hierarchical modeling; Intensional couplings; ML-DEVS; Modular modeling; Variable structure models,Couplings; Flexible couplings; Specifications; Coupling scheme; DEVS; Dynamic interface; Hierarchical model; ML-DEVS; Modular modeling; Variable structure model; Discrete event simulation
Morphological coevolution for fluid dynamical-related risk mitigation,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957068522&doi=10.1145%2f2856694&partnerID=40&md5=fa329c580f793345ff3b1efd6b2853ce,"In the lava flow mitigation context, the determination of areas exposed to volcanic risk is crucial for diminishing consequences in terms of human causalities and damages of material properties. In order to mitigate the destructive effects of lava flows along volcanic slopes, the building and positioning of artificial barriers is fundamental for controlling and slowing down the lava flow advance. In this article, an evolutionary computation-based decision support system for defining and optimizing volcanic hazard mitigation interventions is proposed. In particular, the SCIARA-fV2 Cellular Automata numerical model has been applied for simulating lava flows at Mt. Etna (Italy) volcano and Parallel Genetic Algorithms (PGA) adopted for optimizing protective measures construction by morphological evolution. The PGA application regarded the optimization of the position, orientation, and extension of earth barriers built to protect Rifugio Sapienza, a touristic facility located near the summit of the volcano. A preliminary release of the algorithm, called single barrier (SBA) approach, was initially considered. Subsequently, a second GA strategy, called Evolutionary Greedy Strategy (EGS), was implemented by introducing multibarrier protection measures in order to improve the efficiency of the final solution. Finally, a Coevolutionary Cooperative Strategy (CCS), has been introduced where all barriers are encoded in the genotype and, because all the constituents parts of the solution interact with the GA environment, a mechanism of cooperation between individuals has been favored. The study has produced extremely positive results and represents, to our knowledge, the first application of morphological evolution for lava flow mitigation. © 2016 ACM.",Cellular automata; Decision support system; Evolutionary computation; Genetic algorithms; Morphological evolution; Parallel computing,Artificial intelligence; Cellular automata; Evolutionary algorithms; Genetic algorithms; Optimization; Parallel processing systems; Volcanoes; Cooperative strategy; Destructive effects; Greedy strategies; Morphological evolution; Parallel genetic algorithms; Protection measures; Protective measures; Volcanic hazard mitigation; Decision support systems
MaD0: An ultrafast nonlinear pseudorandom number generator,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954498784&doi=10.1145%2f2856693&partnerID=40&md5=4fa9b4489b555dee2e4efe4478030315,"In this article, we present MaD0, an ultrafast nonlinear pseudorandom number generator (PRNG) for noncryptographic applications. MaD0 uses byte-oriented operations for state initialization and fast integeroriented operations for state transition and pseudorandom number generation. Its state transition follows a pseudorandom mapping. MaD0 generates high-quality pseudorandom numbers and reaches a generation speed of half cycle per byte on an Intel Core i3 processor. It has a state space of 2,240 bits and an expected period length around 21120. It also shows other good properties, such as fast recovery from biased states and ease of use. © 2016 ACM.",Nonlinear random number generator; Random mapping; Random number generation,Mapping; Number theory; Fast recovery; Period length; Pseudo random; Pseudo-random numbers; Random Mappings; Random number generators; State transitions; Ultrafast nonlinear; Random number generation
FNM: An enhanced null-message algorithm for parallel simulation of multicore systems,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957059502&doi=10.1145%2f2735630&partnerID=40&md5=25452355fb3346324ef6652ae9fa5253,"As multicore computer systems become increasingly complex, parallel simulation is becoming an important tool for exploring design space and evaluating design tradeoffs. The key to the success of parallel simulation is the ability to maintain a high degree of parallelism under synchronization constraints. In this article, an enhanced Null-message algorithm called FNM is presented that uses domain-specific knowledge to improve the performance of the basic Null-message algorithm. Based on their runtime states, the components of the simulation model can make a conservative forecast of future interprocess events. The forecast information is carried in the enhanced Null-messages, and, by combining the forecast from both sides of an interprocess link, FNM can achieve a dynamic system lookahead that is much greater than what the static system structure provides. This improved lookahead allows better exploitation of the simulation model's inherent parallelism and leads to better performance. Compared with the basic Null-message algorithm, FNM greatly reduces the amount of Null-messages and improves parallel simulation performance as a result, while at the same time it guarantees simulation correctness as the basic Null-message algorithm does. In tests on cycle-level models with up to 128 cores, FNM shows good scalability and proves to be an effective method. © 2016 ACM.",Domain-specific knowledge; Multicore systems; Null-message algorithm; Optimization; Parallel discrete event simulation,Discrete event simulation; Distributed computer systems; Forecasting; Optimization; Degree of parallelism; Domain-specific knowledge; Forecast information; Inherent parallelism; Multi-core systems; Parallel discrete event simulations; Parallel simulations; Synchronization constraints; Algorithms
Parallel expanded event simulation of tightly coupled systems,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954561427&doi=10.1145%2f2832909&partnerID=40&md5=e14aee9ff296c6ccd3d030bb63b57817,"The technical evolution of wireless communication technology and the need for accurately modeling these increasingly complex systems causes a steady growth in the complexity of simulation models. At the same time, multi-core systems have become the de facto standard hardware platform. Unfortunately, wireless systems pose a particular challenge for parallel execution due to a tight coupling of network entities in space and time. Moreover, model developers are often domain experts with no in-depth understanding of parallel and distributed simulation. In combination, both aspects severely limit the performance and the efficiency of existing parallelization techniques. We address these challenges by presenting parallel expanded event simulation, a novel modeling paradigm that extends discrete events with durations that span a period in simulated time. The resulting expanded events form the basis for a conservative synchronization scheme that considers overlapping expanded events eligible for parallel processing. We then put these concepts into practice by implementing Horizon, a parallel expanded event simulation framework specifically tailored to the characteristics of multi-core systems. Our evaluation shows that Horizon achieves considerable speedups in synthetic as well as real-world simulation models and considerably outperforms the current state-of-theart in distributed simulation. © 2016 ACM.",Conservative synchronization; Multi-core systems; Parallel discrete event simulation; Simulation modeling paradigm; Wireless systems,Complex networks; Computer simulation; Microprocessor chips; Wireless telecommunication systems; Conservative Synchronization; Distributed simulations; Multi-core systems; Parallel and distributed simulation; Parallel discrete event simulations; Parallelization techniques; Wireless communication technology; Wireless systems; Discrete event simulation
Modeling cache memory utilization on multicore using common pool resource game on cellular automata,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954488958&doi=10.1145%2f2812808&partnerID=40&md5=d72fe1c90c96aae5016753efa591fbd5,"Recent computing architectures are implemented by shared memory technologies to alleviate the high latency experienced by off-chip memory transfers, but the high architectural complexity of modern multicore processors has presented many questions. To tackle the design of efficient algorithms scheduling workloads over available cores, this article presents a parallel bioinspired model that simulates the utilization of shared memory on multicore systems. The proposed model is based on cellular automata (CA) and coupled with game theory principles. CA are selected due to their inherent parallelism and especially their ability to incorporate inhomogeneities. Furthermore, the novelty of the model is realized on the fact that multilevel CA are used to simulate the different levels of cache memory usually found in multicore processors. These characteristics make the model able to cope with the increasing diversity of cache memory hierarchies on modern and future processors. Nonetheless, by acquiring data from hardware performance counters and processing them with the proposed model online, the performance of the system can be calculated and a better scheduling strategy can be adopted in real time. The CA-based model was verified on the behavior of a real multicore system running amultithreaded application, and it successfully simulated the acceleration achieved by an increased number of cores available for the execution of the workload. More specifically, the example of common pool resource from game theory was used with two variations: a static and a variable initial endowment. The static variation of the model approximates slightly better the acceleration of a workload when the number of available processor cores increases, whereas the dynamic variation simulates better the moderate differences due to operation system's scheduler alternations on the same amount of cores. © 2016 ACM.",Cache memory contention; Cellular automata; Game theory; Multicore processors,Acceleration control; Algorithms; Cellular automata; Computer architecture; Game theory; Memory architecture; Natural resources; Parallel processing systems; Scheduling; Scheduling algorithms; Bio-inspired Models; Common pool resources; Computing architecture; Hardware performance counters; Inherent parallelism; Memory contentions; Multi-core processor; Scheduling strategies; Cache memory
Moving least squares regression for high-dimensional stochastic simulation metamodeling,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954484012&doi=10.1145%2f2724708&partnerID=40&md5=0d6cea95be2964d5fd20e4c5e68b7ddf,"Simulation metamodeling is building a statistical model based on simulation output as an approximation to the system performance measure being estimated by the simulation model. In high-dimensional metamodeling problems, larger numbers of design points are needed to build an accurate and precise metamodel. Metamodeling techniques that are functions of all of these design points experience difficulties because of numerical instabilities and high computation times. We introduce a procedure to implement a local smoothing method called Moving Least Squares (MLS) regression in high-dimensional stochastic simulation metamodeling problems. Although MLS regression is known to work well when there are a very large number of design points, current procedures are focused on two- and three-dimensional cases. Furthermore, our procedure accounts for the fact that we can make replications and control the placement of design points in stochastic simulation. We provide a bound on the expected approximation error, show that the MLS predictor is consistent under certain conditions, and test the procedure with two examples that demonstrate better results than other existing simulation metamodeling techniques. © 2016 ACM.",High-dimensional meta modeling; Locally weighted least squares regression; Moving least squares,Design; Regression analysis; Stochastic control systems; Stochastic models; Stochastic systems; Local smoothing method; Meta model; Meta-modeling technique; Moving least squares; Numerical instability; Stochastic simulations; System performance measures; Weighted least squares; Least squares approximations
The extrinsic noise effect on lateral inhibition differentiation waves,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954518921&doi=10.1145%2f2832908&partnerID=40&md5=31c3192046684e3119a1ea89b3215d67,"Multipotent differentiation, where cells adopt one of several cell fates, is a determinate and orchestrated procedure that often incorporates stochastic mechanisms in order to diversify cell types. How these stochastic phenomena interact to govern cell fate is poorly understood. Nonetheless, cell fate decision-making procedure is mainly regulated through the activation of differentiation waves and associated signaling pathways. In the current work, we focus on the Notch/Delta signaling pathway, which is not only known to trigger such waves but also is used to achieve the principle of lateral inhibition (i.e., a competition for exclusive fates through cross-signaling between neighboring cells). Such a process ensures unambiguous stochastic decisions influenced by intrinsic noise sources, such as those found in the regulation of signaling pathways, and extrinsic stochastic fluctuations attributed to microenvironmental factors. However, the effect of intrinsic and extrinsic noise on cell fate determination is an open problem. Our goal is to elucidate how the induction of extrinsic noise affects cell fate specification in a lateral inhibition mechanism. Using a stochastic Cellular Automaton with continuous state space, we show that extrinsic noise results in the emergence of steady-state furrow patterns of cells in a ""frustrated/transient"" phenotypic state. © 2016 ACM.",Cell fate; Cellular automata; Differentiation wave; Noise; Notch/delta,Cells; Cellular automata; Cytology; Decision making; Pigments; Signaling; Stochastic systems; Cell fates; Continuous State Space; Microenvironmental factors; Noise; Notch/delta; Stochastic cellular automata; Stochastic fluctuation; Stochastic phenomena; Cell signaling
PAM: Particle automata in modeling of multiscale biological systems,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957063355&doi=10.1145%2f2827696&partnerID=40&md5=afcf1d884c93ecd33d00e5a882458b71,"Serious problems with bridging multiple scales in the scope of a single numerical model make computer simulations too demanding computationally and highly unreliable. We present a new concept of modeling framework that integrates the particle method with graph dynamical systems, called the particle automata model (PAM). We assume that the mechanical response of a macroscopic system on internal or external stimuli can be simulated by the spatiotemporal dynamics of a graph of interacting particles representing fine-grained components of biological tissue, such as cells, cell clusters, or microtissue fragments.Meanwhile, the dynamics of microscopic processes can be represented by evolution of internal particle states represented by vectors of finite-state automata. To demonstrate the broad scope of application of PAM, we present three models of very different biological phenomena: blood clotting, tumor proliferation, and fungal wheat infection. We conclude that the generic and flexible modeling framework provided by PAM may contribute to more intuitive and faster development of computational models of complex multiscale biological processes. © 2016 ACM.",Blood flow; F. graminearum proliferation; Graph dynamical systems; Modeling using particles; Particle automata model; Tumor growth,Automata theory; Biology; Blood; Dynamical systems; Flow graphs; Tumors; Automata models; Blood flow; F. graminearum; Graph dynamical systems; Tumor growth; Biological systems
QRF: An optimization-based framework for evaluating complex stochastic networks,2016,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954548633&doi=10.1145%2f2724709&partnerID=40&md5=98e221a59f912392125b306ad18b8366,"The Quadratic Reduction Framework (QRF) is a numerical modeling framework to evaluate complex stochastic networks composed of resources featuring queueing, blocking, state-dependent behavior, service variability, temporal dependence, or a subset thereof. Systems of this kind are abstracted as network of queues for which QRF supports two common blocking mechanisms: blocking-after-service and repetitive-service random-destination. State-dependence is supported for both routing probabilities and service processes. To evaluate these models, we develop a novel mapping, called Blocking-Aware Quadratic Reduction (BQR), which can describe an intractably large Markov process by a large set of linear inequalities. Each model is then analyzed for bounds or approximate values of performance metrics using optimization programs that provide different levels of accuracy and error guarantees. Numerical results demonstrate that QRF offers very good accuracy and much greater scalability than exact analysis methods. © 2016 ACM.",Blocking; Queueing; State-dependence; Temporal dependence,Markov processes; Numerical methods; Queueing theory; Stochastic systems; Blocking; Linear inequalities; Optimization programs; Queueing; Routing probabilities; Service variability; State dependence; Temporal dependence; Complex networks
LN: A Flexible Algorithmic Framework for Layered Queueing Network Analysis,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198745728&doi=10.1145%2f3633457&partnerID=40&md5=0747196c658210fcb7dd0b9ad82708ef,"Layered queueing networks (LQNs) are an extension of ordinary queueing networks useful to model simultaneous resource possession and stochastic call graphs in distributed systems. Existing computational algorithms for LQNs have primarily focused on mean-value analysis. However, other solution paradigms, such as normalizing constant analysis and mean-field approximation, can improve the computation of LQN mean and transient performance metrics, state probabilities, and response time distributions. Motivated by this observation, we propose the first LQN meta-solver, called LN, that allows for the dynamic selection of the performance analysis paradigm to be iteratively applied to the submodels arising from layer decomposition. We report experiments where this added flexibility helps us to reduce the LQN solution errors. We also demonstrate that the meta-solver approach eases the integration of LQNs with other formalisms, such as caching models, enabling the analysis of more general classes of layered stochastic networks. Additionally, to support the accurate evaluation of the LQN submodels, we develop novel algorithms for homogeneous queueing networks consisting of an infinite server node and a set of identical queueing stations. In particular, we propose an exact method of moment algorithms, integration techniques for normalizing constants, and a fast non-iterative mean-value analysis technique.  © 2024 Copyright held by the owner/author(s).",computational algorithms; Layered queueing networks; meta-solver; multi-formalism; performance measures,Iterative methods; Method of moments; Network layers; Probability distributions; Queueing theory; Stochastic models; Stochastic systems; Value engineering; Algorithmic framework; Computational algorithm; Layered queueing networks; Mean value analysis; Meta-solv; Multi formalism; Normalizing constants; Performance measure; Simultaneous resource possession; Submodels; Queueing networks
End-to-End Statistical Model Checking for Parameterization and Stability Analysis of ODE Models,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198717662&doi=10.1145%2f3649438&partnerID=40&md5=584516b8eb4e462baf98da2284aefd7d,"We propose a simulation-based technique for the parameterization and the stability analysis of parametric Ordinary Differential Equations. This technique is an adaptation of Statistical Model Checking, often used to verify the validity of biological models, to the setting of Ordinary Differential Equations systems. The aim of our technique is to estimate the probability of satisfying a given property under the variability of the parameter or initial condition of the ODE, with any metrics of choice. To do so, we discretize the values space and use statistical model checking to evaluate each individual value w.r.t. provided data. Contrary to other existing methods, we provide statistical guarantees regarding our results that take into account the unavoidable approximation errors introduced through the numerical integration of the ODE system performed while simulating. In order to show the potential of our technique, we present its application to two case studies taken from the literature, one relative to the growth of a jellyfish population, and the other concerning a well-known oscillator model.  © 2024 Copyright held by the owner/author(s).",ordinary differential equations (ODEs); parameterization; stability analysis; Statistical model checking (SMC),Model checking; Numerical methods; Parameterization; Population statistics; Biological models; Differential equation systems; End to end; Ordinary differential equation; Ordinary differential equation models; Parameter conditions; Property; Stability analyze; Statistical model checking; Ordinary differential equations
"Introduction to the Special Issue on QEST 2022, Part 1",2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198841118&doi=10.1145%2f3671146&partnerID=40&md5=58711f877a48be4f4f025bf141727191,[No abstract available],,
Exact and Approximate Moment Derivation for Probabilistic Loops with Non-Polynomial Assignments,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198722921&doi=10.1145%2f3641545&partnerID=40&md5=497fafd256d26d5dbd95120d7bd52771,"Many stochastic continuous-state dynamical systems can be modeled as probabilistic programs with nonlinear non-polynomial updates in non-nested loops. We present two methods, one approximate and one exact, to automatically compute, without sampling, moment-based invariants for such probabilistic programs as closed-form solutions parameterized by the loop iteration. The exact method applies to probabilistic programs with trigonometric and exponential updates and is embedded in the Polar tool. The approximate method for moment computation applies to any nonlinear random function as it exploits the theory of polynomial chaos expansion to approximate non-polynomial updates as the sum of orthogonal polynomials. This translates the dynamical system to a non-nested loop with polynomial updates, and thus renders it conformable with the Polar tool that computes the moments of any order of the state variables. We evaluate our methods on an extensive number of examples ranging from modeling monetary policy to several physical motion systems in uncertain environments. The experimental results demonstrate the advantages of our approach with respect to the current state-of-the-art.  © 2024 Copyright held by the owner/author(s).",exponential updates; non-linear updates; polynomial chaos expansion; prob-solvable loops; Probabilistic programs; stochastic dynamical systems; trigonometric updates,Chaos theory; Computation theory; Dynamical systems; Iterative methods; Method of moments; Orthogonal functions; Stochastic systems; Chaos expansions; Exponential update; Non linear; Non-linear update; Polynomial chaos; Polynomial chaos expansion; Prob-solvable loop; Probabilistic programs; Stochastic dynamical system; Trigonometric update; Polynomials
Rate Lifting for Stochastic Process Algebra by Transition Context Augmentation,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198749697&doi=10.1145%2f3656582&partnerID=40&md5=ae55459e54f613ff55a5b3518e479528,"This article presents an algorithm for determining the unknown rates in the sequential processes of a Stochastic Process Algebra (SPA) model, provided that the rates in the combined flat model are given. Such a rate lifting is useful for model reverse engineering and model repair. Technically, the algorithm works by solving systems of nonlinear equations and - if necessary - adjusting the model's synchronisation structure, without changing its transition system. The adjustments cause an augmentation of a transition's context and thus enable additional control over the transition rate. The complete pseudo-code of the rate lifting algorithm is included and discussed in the article, and its practical usefulness is demonstrated by two case studies. The approach taken by the algorithm exploits some structural and behavioural properties of SPA systems, which are formulated here for the first time and could be very beneficial also in other contexts, such as compositional system verification.  © 2024 Copyright held by the owner/author(s).",Markov chain; model repair; rate lifting; Stochastic process algebra; structural and behavioural properties,Algebra; Markov processes; Nonlinear equations; Repair; Stochastic models; Stochastic systems; Additional control; Behavioral properties; Model repair; Model synchronization; Rate lifting; Sequential process; Stochastic process algebras; Structural and behavioral property; System of nonlinear equations; Transition system; Reverse engineering
RayNet: A Simulation Platform for Developing Reinforcement Learning-Driven Network Protocols,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198733810&doi=10.1145%2f3653975&partnerID=40&md5=481f5315a9720913b277cb47ddf5c76e,"Reinforcement Learning (RL) has gained significant momentum in the development of network protocols. However, RL-based protocols are still in their infancy, and substantial research is required to build deployable solutions. Developing a protocol based on RL is a complex and challenging process that involves several model design decisions and requires significant training and evaluation in real and simulated network topologies. Network simulators offer an efficient training environment for RL-based protocols because they are deterministic and can run in parallel. In this article, we introduce RayNet, a scalable and adaptable simulation platform for the development of RL-based network protocols. RayNet integrates OMNeT++, a fully programmable network simulator, with Ray/RLlib, a scalable training platform for distributed RL. RayNet facilitates the methodical development of RL-based network protocols so that researchers can focus on the problem at hand and not on implementation details of the learning aspect of their research. We developed a simple RL-based congestion control approach as a proof of concept showcasing that RayNet can be a valuable platform for RL-based research in computer networks, enabling scalable training and evaluation. We compared RayNet with ns3-gym, a platform with similar objectives to RayNet, and showed that RayNet performs better in terms of how fast agents can collect experience in RL environments.  © 2024 Copyright held by the owner/author(s).",Computer network protocols; congestion control; network simulations; reinforcement learning,Internet protocols; Simulation platform; Topology; Computer network protocols; Congestion control; Design decisions; Modeling designs; Network simulation; Network simulators; Real networks; Reinforcement learnings; Simulated networks; Simulation platform; Reinforcement learning
Projected Gaussian Markov Improvement Algorithm for High-Dimensional Discrete Optimization via Simulation,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198753870&doi=10.1145%2f3649463&partnerID=40&md5=46e9a86390ac2a297adf53d3cedd0c95,"This article considers a discrete optimization via simulation (DOvS) problem defined on a graph embedded in the high-dimensional integer grid. Several DOvS algorithms that model the responses at the solutions as a realization of a Gaussian Markov random field (GMRF) have been proposed exploiting its inferential power and computational benefits. However, the computational cost of inference increases exponentially in dimension. We propose the projected Gaussian Markov improvement algorithm (pGMIA), which projects the solution space onto a lower-dimensional space creating the region-layer graph to reduce the cost of inference. Each node on the region-layer graph can be mapped to a set of solutions projected to the node; these solutions form a lower-dimensional solution-layer graph. We define the response at each region-layer node to be the average of the responses within the corresponding solution-layer graph. From this relation, we derive the region-layer GMRF to model the region-layer responses. The pGMIA alternates between the two layers to make a sampling decision at each iteration. It first selects a region-layer node based on the lower-resolution inference provided by the region-layer GMRF, then makes a sampling decision among the solutions within the solution-layer graph of the node based on the higher-resolution inference from the solution-layer GMRF. To solve even higher-dimensional problems (e.g., 100 dimensions), we also propose the pGMIA+: a multi-layer extension of the pGMIA. We show that both pGMIA and pGMIA+ converge to the optimum almost surely asymptotically and empirically demonstrate their competitiveness against state-of-the-art high-dimensional Bayesian optimization algorithms.  © 2024 Copyright held by the owner/author(s).",Bayesian optimization; Gaussian Markov random field; high-dimensional discrete optimization via simulation; projection,Graph theory; Image segmentation; Inference engines; Integer programming; Iterative methods; Markov processes; Bayesian optimization; Discrete optimization; Gaussian Markov random field; Gaussians; High-dimensional; High-dimensional discrete optimization via simulation; Higher-dimensional; Node-based; Projection; Solution layers; Gaussian distribution
Sufficient Conditions for Central Limit Theorems and Confidence Intervals for Randomized Quasi-Monte Carlo Methods,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198704476&doi=10.1145%2f3643847&partnerID=40&md5=e2dfb00e0ffb35b7902e64afc54688a5,"Randomized quasi-Monte Carlo methods have been introduced with the main purpose of yielding a computable measure of error for quasi-Monte Carlo approximations through the implicit application of a central limit theorem over independent randomizations. But to increase precision for a given computational budget, the number of independent randomizations is usually set to a small value so that a large number of points are used from each randomized low-discrepancy sequence to benefit from the fast convergence rate of quasi-Monte Carlo. While a central limit theorem has been previously established for a specific but computationally expensive type of randomization, it is also known in general that fixing the number of randomizations and increasing the length of the sequence used for quasi-Monte Carlo can lead to a non-Gaussian limiting distribution. This paper presents sufficient conditions on the relative growth rates of the number of randomizations and the quasi-Monte Carlo sequence length to ensure a central limit theorem and also an asymptotically valid confidence interval. We obtain several results based on the Lindeberg condition for triangular arrays and expressed in terms of the regularity of the integrand and the convergence speed of the quasi-Monte Carlo method. We also analyze the resulting estimator's convergence rate.  © 2024 Copyright held by the owner/author(s).",central limit theorems; confidence intervals; Randomized quasi-Monte Carlo,Computation theory; Monte Carlo methods; Random processes; Central Limit Theorem; Computable measures; Computational budget; Condition; Confidence interval; Monte-carlo approximations; Quasi-Monte Carlo; Randomisation; Randomized quasi monte carlo methods; Randomized quasi-monte carlo; Budget control
"Introduction to the Special Issue for INFORMS Simulation Society (I-Sim) Workshop, 2021",2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193515934&doi=10.1145%2f3655711&partnerID=40&md5=af14dd476577b3d3bf8625becdada8c0,[No abstract available],,
"Overlapping Batch Confidence Intervals on Statistical Functionals Constructed from Time Series: Application to Quantiles, Optimization, and Estimation",2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193537573&doi=10.1145%2f3649437&partnerID=40&md5=49e4130bb848ae3adad7ece4d72c6f87,"We propose a general purpose confidence interval procedure (CIP) for statistical functionals constructed using data from a stationary time series. The procedures we propose are based on derived distribution-free analogues of the χ2 and Student's t random variables for the statistical functional context and hence apply in a wide variety of settings including quantile estimation, gradient estimation, M-estimation, Conditional Value at Risk (CVaR) estimation, and arrival process rate estimation, apart from more traditional statistical settings. Like the method of subsampling, we use overlapping batches (OB) of time-series data to estimate the underlying variance parameter; unlike subsampling and the bootstrap, however, we assume that the implied point estimator of the statistical functional obeys a central limit theorem (CLT) to help identify the weak asymptotics (called OB-x limits, x = I, II, III) of batched Studentized statistics. The OB-x limits, certain functionals of the Wiener process parameterized by the size of the batches and the extent of their overlap, form the essential machinery for characterizing dependence and, consequently, the correctness of the proposed CIPs. The message from extensive numerical experimentation is that in settings where a functional CLT on the point estimator is in effect, using large overlapping batches alongside OB-x critical values yields confidence intervals that are often of significantly higher quality than those obtained from more generic methods like subsampling or the bootstrap. We illustrate using examples from CVaR estimation, ARMA parameter estimation, and non-homogeneous Poisson process rate estimation; R and MATLAB code for OB-x critical values is available at web.ics.purdue.edu/ĝ1/4pasupath. © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesUncertainty quantification; confidence intervals; time series,Financial data processing; MATLAB; Numerical methods; Parameter estimation; Risk perception; Value engineering; Additional key word and phrasesuncertainty quantification; Conditional Value-at-Risk; Confidence interval; Critical value; Functionals; Key words; Optimisations; Rate estimation; Risk estimation; Times series; Time series
Contextual Ranking and Selection with Gaussian Processes and Optimal Computing Budget Allocation,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193478463&doi=10.1145%2f3633456&partnerID=40&md5=2afa94621ea4a094236bea171a2aa46e,"In many real-world problems, we are faced with the problem of selecting the best among a finite number of alternatives, where the best alternative is determined based on context specific information. In this work, we study the contextual Ranking and Selection problem under a finite-alternative-finite-context setting, where we aim to find the best alternative for each context. We use a separate Gaussian process to model the reward for each alternative and derive the large deviations rate function for both the expected and worst-case contextual probability of correct selection. We propose the GP-C-OCBA sampling policy, which uses the Gaussian process posterior to iteratively allocate observations to maximize the rate function. We prove its consistency and show that it achieves the optimal convergence rate under the assumption of a non-informative prior. Numerical experiments show that our algorithm is highly competitive in terms of sampling efficiency, while having significantly smaller computational overhead. © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesRanking and selextion; Gaussian process; optimal computing budget allocation,Budget control; Computational efficiency; Gaussian noise (electronic); Iterative methods; Additional key word and phrasesranking and selextion; Finite number; Gaussian Processes; Key words; Optimal computing budget allocation; Ranking and selection; Ranking problems; Rate functions; Real-world problem; Specific information; Gaussian distribution
Bayesian Optimisation for Constrained Problems,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193491228&doi=10.1145%2f3641544&partnerID=40&md5=d58be1f49ef9422a4b10dcfe33f1d67b,"Many real-world optimisation problems such as hyperparameter tuning in machine learning or simulation-based optimisation can be formulated as expensive-to-evaluate black-box functions. A popular approach to tackle such problems is Bayesian optimisation, which builds a response surface model based on the data collected so far, and uses the mean and uncertainty predicted by the model to decide what information to collect next. In this article, we propose a generalisation of the well-known Knowledge Gradient acquisition function that allows it to handle constraints. We empirically compare the new algorithm with four other state-of-the-art constrained Bayesian optimisation algorithms and demonstrate its superior performance. We also prove theoretical convergence in the infinite budget limit. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSimulation optimisation; Bayesian optimisation; constraints; Gaussian processes,Budget control; Additional key word and phrasessimulation optimization; Bayesian optimization; Constrained problem; Constraint; Gaussian Processes; Hyper-parameter; Key words; Optimisations; Optimization problems; Real-world optimization; Constrained optimization
Stochastic Approximation for Multi-period Simulation Optimization with Streaming Input Data,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193500150&doi=10.1145%2f3617595&partnerID=40&md5=fa911f4a7014991cc978c2d025e682cc,"We consider a continuous-valued simulation optimization (SO) problem, where a simulator is built to optimize an expected performance measure of a real-world system while parameters of the simulator are estimated from streaming data collected periodically from the system. At each period, a new batch of data is combined with the cumulative data and the parameters are re-estimated with higher precision. The system requires the decision variable to be selected in all periods. Therefore, it is sensible for the decision-maker to update the decision variable at each period by solving a more precise SO problem with the updated parameter estimate to reduce the performance loss with respect to the target system. We define this decision-making process as the multi-period SO problem and introduce a multi-period stochastic approximation (SA) framework that generates a sequence of solutions. Two algorithms are proposed: Re-start SA (ReSA) reinitializes the stepsize sequence in each period, whereas Warm-start SA (WaSA) carefully tunes the stepsizes, taking both fewer and shorter gradient-descent steps in later periods as parameter estimates become increasingly more precise. We show that under suitable strong convexity and regularity conditions, ReSA and WaSA achieve the best possible convergence rate in expected sub-optimality either when an unbiased or a simultaneous perturbation gradient estimator is employed, while WaSA accrues significantly lower computational cost as the number of periods increases. In addition, we present the regularized ReSA, which obviates the need to know the strong convexity constant and achieves the same convergence rate at the expense of additional computation. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMulti-period simulation optimization; multi-period stochastic approximation; simulation optimization under input model risk,Approximation theory; Decision making; Gradient methods; Optimization; Parameter estimation; Stochastic models; Additional key word and phrasesmulti-period simulation optimization; Input models; Key words; Modeling risk; Multi-period; Multi-period stochastic approximation; Simulation optimization; Simulation optimization under input model risk; Stochastic approximations; Stochastic systems
Reproducibility Report for the Article: Parallel Simulation of Quantum Networks with Distributed Quantum State Management,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193481150&doi=10.1145%2f3639704&partnerID=40&md5=2a1ccccfc167f4016a434967748a242f,"The examined article introduces a parallel version of SeQUeNCe, a Discrete Event Simulator for quantum networks. The authors have deposited their artifact on Zenodo, meeting the criteria for long-term preservation required by the Artifacts Available badge. The software within the artifact functions correctly with minor adjustments, aligning with the article's relevance and earning the Artifacts Evaluated - Functional badge. Additionally, due to the reasonable quality and customizability of the artifact, the Artifacts Evaluated - Reusable badge has also been awarded. The authors didn't request the Artifacts Evaluated - Reproduced badge and they did not include the scripts used to generate and display their experimental results. As a result, it has not been possible to replicate the results published in their article. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ACM TOMACS; Additional Key Words and PhrasesReplication of computational results,Distributed computer systems; Quality control; ACM TOMACS; Additional key word and phrasesreplication of computational result; Computational results; Key words; Parallel simulations; Parallel version; Quantum network; Quantum state; Reproducibilities; State management; Quantum theory
Parallel Simulation of Quantum Networks with Distributed Quantum State Management,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193488521&doi=10.1145%2f3634701&partnerID=40&md5=5a6bd1f0d9671fd0fa51cf038ffbc9e2,"Quantum network simulators offer the opportunity to cost-efficiently investigate potential avenues for building networks that scale with the number of users, communication distance, and application demands by simulating alternative hardware designs and control protocols. Several quantum network simulators have been recently developed with these goals in mind. As the size of the simulated networks increases, however, sequential execution becomes time-consuming. Parallel execution presents a suitable method for scalable simulations of large-scale quantum networks, but the unique attributes of quantum information create unexpected challenges. In this work, we identify requirements for parallel simulation of quantum networks and develop the first parallel discrete-event quantum network simulator by modifying the existing serial simulator SeQUeNCe. Our contributions include the design and development of a quantum state manager (QSM) that maintains shared quantum information distributed across multiple processes. We also optimize our parallel code by minimizing the overhead of the QSM and decreasing the amount of synchronization needed among processes. Using these techniques, we observe a speedup of 2 to 25 times when simulating a 1,024-node linear network topology using 2 to 128 processes. We also observe an efficiency greater than 0.5 for up to 32 processes in a linear network topology of the same size and with the same workload. We repeat this evaluation with a randomized workload on a caveman network. We also introduce several methods for partitioning networks by mapping them to different parallel simulation processes. We have released the parallel SeQUeNCe simulator as an open source tool alongside the existing sequential version. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesParallel discrete-event simulation; quantum network,Linear networks; Network topology; Quantum optics; Additional key word and phrasesparallel discrete-event simulation; Discrete-event simulations; Key words; Network simulators; Network topology; Parallel simulations; Quantum network; Quantum state; State management; User communication; Discrete event simulation
Stochastic Approximation for Estimating the Price of Stability in Stochastic Nash Games,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191755243&doi=10.1145%2f3632525&partnerID=40&md5=45a6399e6ce20c456356e1195611fff8,"The goal in this article is to approximate the Price of Stability (PoS) in stochastic Nash games using stochastic approximation (SA) schemes. PoS is among the most popular metrics in game theory and provides an avenue for estimating the efficiency of Nash games. In particular, evaluating the PoS can help with designing efficient networked systems, including communication networks and power market mechanisms. Motivated by the absence of efficient methods for computing the PoS, first we consider stochastic optimization problems with a nonsmooth and merely convex objective function and a merely monotone stochastic variational inequality (SVI) constraint. This problem appears in the numerator of the PoS ratio. We develop a randomized block-coordinate stochastic extra-(sub)gradient method where we employ a novel iterative penalization scheme to account for the mapping of the SVI in each of the two gradient updates of the algorithm. We obtain an iteration complexity of the order μ -4 that appears to be best known result for this class of constrained stochastic optimization problems, where μ denotes an arbitrary bound on suitably defined infeasibility and suboptimality metrics. Second, we develop an SA-based scheme for approximating the PoS and derive lower and upper bounds on the approximation error. To validate the theoretical findings, we provide preliminary simulation results on a networked stochastic Nash Cournot competition. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesStochastic optimization; Nash equilibrium; price of stability; variational inequality,Approximation theory; Constrained optimization; Gradient methods; Stochastic systems; Variational techniques; Additional key word and phrasesstochastic optimization; Key words; Nash equilibria; Optimisations; Price of Stability; Stochastic approximations; Stochastic Nash games; Stochastic optimization problems; Stochastic variational inequalities; Variational inequalities; Game theory
Divergence Reduction in Monte Carlo Neutron Transport with On-GPU Asynchronous Scheduling,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183330038&doi=10.1145%2f3626957&partnerID=40&md5=4737fe80d5a23a2e1a24f02bd6b16187,"While Monte Carlo Neutron Transport (MCNT) is near-embarrasingly parallel, the effectively unpredictable lifetime of neutrons can lead to divergence whenMCNT is evaluated on GPUs. Divergence is the phenomenon of adjacent threads in a warp executing different control flowpaths; on GPUS, it reduces performance because each work group may only execute one path at a time. The process of Thread Data Remapping (TDR) resolves these discrepancies by moving data across hardware such that data in the same warp will be processed through similar paths. A common issue among prior implementations of TDR is the synchronous nature of its remapping and processing cycles, which exhaustively sort data produced by prior processing passes and exhaustively evaluate the sorted data. In anotherwork, we defined amethod of remapping data through an asynchronous scheduler which allows for work to be stored in shared memory and deferred arbitrarily until that work is a viable option for low-divergence evaluation. This article surveys awider set of cases, with the goal of characterizing performance trends across a more comprehensive set of parameters. These parameters include cross sections of scattering/capturing/fission, use of implicit capture, source neutron counts, simulation time spans, and tuned memory allocations. Across these cases, we have recorded minimum and average execution times, as well as a heuristically tuned near-optimal memory allocation size for both synchronous and asynchronous scheduling. Across the collected data, it is shown that the asynchronous method is faster and more memory efficient in the majority of cases, and that it requires less tuning to achieve competitive performance.  © 2024 Copyright held by the owner/author(s).",Asynchronous; divergence; GPGPU; GPU; scheduling,Computer hardware; Memory architecture; Monte Carlo methods; Neutron flux; Program processors; Transport properties; % reductions; Asynchronoi; Divergence; Flow path; GPGPU; Monte Carlo neutron; Neutron transport; Performance; Remapping; Thread-data remapping; Graphics processing unit
Knowledge Equivalence in Digital Twins of Intelligent Systems,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183325491&doi=10.1145%2f3635306&partnerID=40&md5=201e70a9133f82b4cf0cc27bea308ecf,"A digital twin contains up-to-date data-driven models of the physical world being studied and can use simulation to optimise the physical world. However, the analysis made by the digital twin is valid and reliable only when the model is equivalent to the physical world. Maintaining such an equivalent model is challenging, especially when the physical systems being modelled are intelligent and autonomous. The article focuses in particular on digital twin models of intelligent systems where the systems are knowledge-aware but with limited capability. The digital twin improves the acting of the physical system at a meta-level by accumulating more knowledge in the simulated environment. The modelling of such an intelligent physical system requires replicating the knowledge-awareness capability in the virtual space. Novel equivalence maintaining techniques are needed, especially in synchronising the knowledge between the model and the physical system. This article proposes the notion of knowledge equivalence and an equivalence maintaining approach by knowledge comparison and updates. A quantitative analysis of the proposed approach confirms that compared to state equivalence, knowledge equivalence maintenance can tolerate deviation thus reducing unnecessary updates and achieve more Pareto efficient solutions for the tradeoff between update overhead and simulation reliability.  © 2024 Copyright held by the owner/author(s).",DDDAS; digital twins; equivalence checking; Knowledge management; multi-agent system,Equivalence classes; Intelligent systems; Knowledge management; Pareto principle; Reliability analysis; Data-driven model; DDDAS; Equivalence checking; Equivalent modeling; Knowledge awareness; Meta levels; Physical systems; Physical world; Simulated environment; Virtual spaces; Multi agent systems
A Prescriptive Simulation Framework with Realistic Behavioural Modelling for Emergency Evacuations,2024,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183328206&doi=10.1145%2f3633330&partnerID=40&md5=a257030c7d964ad2bd4baf1270351cde,"Emergency and crisis simulations play a pivotal role in equipping authorities worldwide with the necessary tools to minimize the impact of catastrophic events. Various studies have explored the integration of intelligence into Multi-Agent Systems (MAS) for crisis simulation. This involves incorporating psychological behaviours from the social sciences and utilizing data-driven machine learning models with predictive capabilities. A recent advancement in behavioural modelling is the Conscious Movement Model (CMM), designed to modulate an agent's movement patterns dynamically as the situation unfolds. Complementing this, the model incorporates a Conscious Movement Memory-Attention (CMMA) mechanism, enabling learnability through training on pedestrian trajectories extracted from video data. The CMMA facilitates mapping a pedestrian's attention to their surroundings and understanding how their past decisions influence their subsequent actions. This study proposes an efficient framework that integrates the trained CMM into a simulation model specifically tailored for emergency evacuations, ensuring realistic outcomes. The resulting simulation framework automates strategy management and planning for diverse emergency evacuation scenarios. A single-objective method is presented for generating prescriptive analytics, offering effective strategy options based on predefined operational rules. To validate the framework's efficacy, a case study of a theatre evacuation is conducted. In essence, this research establishes a robust simulation framework for crisis management, with a particular emphasis on modelling pedestrians during emergency evacuations. The framework generates prescriptive analytics to aid authorities in executing rescue and evacuation operations effectively.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",conscious movement model; crisis management; emergency evacuation; Modelling & simulation; predictive simulation; prescriptive analytics,Behavioral research; Predictive analytics; Behavioral model; Conscious movement model; Crisis management; Crisis simulations; Emergency evacuation; Modeling simulation; Movement model; Predictive simulations; Prescriptive analytic; Simulation framework; Multi agent systems
An Improved Model ofWavelet Leader Covariance for Estimating Multifractal Properties,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183329893&doi=10.1145%2f3631522&partnerID=40&md5=5a51697e3a26bbac217096c114f00038,"Complex systems often produce multifractal signals defined by stationary increments that exhibit power-law scaling properties. The Legendre transform of the domain-dependent scaling function that defines the power law is known as the multifractal spectrum. The multifractal spectrum can also be defined by a power-series expansion of the scaling function and in practice the first two leading coefficients of that series are estimated from the discrete wavelet transform of the signal. To quantify, validate, and compare simulations of complex systems with data collected empirically from the actual system, practitioners requiremethods for approximating the variance associated with estimates of these coefficients. In this work, we generalize a previously developed semi-parametric statistical model for the values extracted from a discrete multi-scale wavelet transform to include both within-scale and between-scale covariance dependencies. We employ multiplicative cascades to simulate multifractals with known parameters to illustrate the necessity for this generalization and to test the precision of our improved model. The combined within- and between-scale model of covariance results in a more accurate estimate of the expected variance of the coefficients extracted from an empirical data set.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",complex systems; covariance; Model validation and analysis; multi-scale analysis; multifractals; wavelets,Fractals; Large scale systems; Covariance; Model validation; Modeling analyzes; Multi fractals; Multi scale analysis; Multifractal properties; Multifractal spectrum; Scaling functions; Stationary increments; Wavelet; Discrete wavelet transforms
Introduction to the Special Issue on QEST 2021,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179673227&doi=10.1145%2f3631707&partnerID=40&md5=6accc3fab5633bcccf9287de718452d4,[No abstract available],,
DSMC Evaluation Stages: Fostering Robust and Safe Behavior in Deep Reinforcement Learning-Extended Version,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179628996&doi=10.1145%2f3607198&partnerID=40&md5=6b7b169c7e25fad78d39686137541ef2,"Neural networks (NN) are gaining importance in sequential decision-making. Deep reinforcement learning (DRL), in particular, is extremely successful in learning action policies in complex and dynamic environments. Despite this success, however, DRL technology is not without its failures, especially in safety-critical applications: (i) the training objective maximizes average rewards, which may disregard rare but critical situations and hence lack local robustness; (ii) optimization objectives targeting safety typically yield degenerated reward structures, which, for DRL to work, must be replaced with proxy objectives. Here, we introduce a methodology that can help to address both deficiencies. We incorporate evaluation stages (ES) into DRL, leveraging recent work on deep statistical model checking (DSMC), which verifies NN policies in Markov decision processes. Our ES apply DSMC at regular intervals to determine state space regions with weak performance. We adapt the subsequent DRL training priorities based on the outcome, (i) focusing DRL on critical situations and (ii) allowing to foster arbitrary objectives.We run case studies on two benchmarks. One of them is the Racetrack, an abstraction of autonomous driving that requires navigating a map without crashing into a wall. The other is MiniGrid, a widely used benchmark in the AI community. Our results show that DSMC-based ES can significantly improve both (i) and (ii). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDeep reinforcement learning; deep statistical model checking; evaluation stages,Behavioral research; Decision making; Deep learning; Markov processes; Model checking; Safety engineering; Additional key word and phrasesdeep reinforcement learning; Deep statistical model checking; Evaluation stage; Extended versions; Key words; Learning actions; Neural-networks; Reinforcement learnings; Sequential decision making; Statistical model checking; Reinforcement learning
Toward Data Center Digital Twins via Knowledge-based Model Calibration and Reduction,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179624317&doi=10.1145%2f3604283&partnerID=40&md5=32bf027cc2bd9da7e2179ff48ac862af,"Computational fluid dynamics (CFD) models have been widely used for prototyping data centers. Evolving them into high-fidelity and real-Time digital twins is desirable for the online operations of data centers. However, CFD models often have unsatisfactory accuracy and high computation overhead. Manually calibrating the CFD model parameters is tedious and labor-intensive. Existing automatic calibration approaches apply heuristics to search the model configurations. However, each search step requires a long-lasting process of repeatedly solving the CFD model, rendering them impractical, especially for complex CFD models. This article presents Kalibre, a knowledge-based neural surrogate approach that calibrates a CFD model by iterating four steps of (i) training a neural surrogate model, (ii) finding the optimal parameters through neural surrogate retraining, (iii) configuring the found parameters back to the CFD model, and (iv) validating the CFD model using sensor-measured data. Thus, the parameter search is offloaded to the lightweight neural surrogate. To speed up Kalibre's convergence, we incorporate prior knowledge in training data initialization and surrogate architecture design. With about ten hours of computation on a 64-core processor, Kalibre achieves mean absolute errors (MAEs) of 0.57°C and 0.88°C in calibrating the CFD models of two production data halls hosting thousands of servers. To accelerate CFD-based simulation, we further propose Kalibreduce that incorporates the energy balance principle to reduce the order of the calibrated CFD model. Evaluation shows the model reduction only introduces 0.1°C to 0.27°C extra errors while accelerating the CFD-based simulations by thousand times. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesData center; computational fluid dynamics; knowledge-based neural network; proper orthogonal decomposition; surrogate model,Knowledge based systems; Neural networks; Principal component analysis; Additional key word and phrasesdata center; Computational fluid dynamics modeling; Datacenter; Key words; Knowledge based neural networks; Model reduction; Orthogonal decomposition; Proper Orthogonal; Proper orthogonal decomposition; Surrogate modeling; Computational fluid dynamics
Performance Analysis of Work Stealing Strategies in Large-Scale Multithreaded Computing,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179625180&doi=10.1145%2f3584186&partnerID=40&md5=73b40b0e45d378ecfb72d812eedbf122,"Distributed systems use randomized work stealing to improve performance and resource utilization. In most prior analytical studies of randomized work stealing, jobs are considered to be sequential and are executed as a whole on a single server. In this article, we consider a homogeneous system of servers where parent jobs spawn child jobs that can feasibly be executed in parallel. When an idle server probes a busy server in an attempt to steal work, it may either steal a parent job or multiple child jobs.To approximate the performance of this system, we introduce a Quasi-Birth-Death Markov chain and express the performance measures of interest via its unique steady state. We perform simulation experiments that suggest that the approximation error tends to zero as the number of servers in the system becomes large. To further support this observation, we introduce a mean field model and show that its unique fixed point corresponds to the steady state of the Quasi-Birth-Death Markov chain. Using numerical experiments, we compare the performance of various simple stealing strategies as well as optimized strategies. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMatrix analytic methods; distributed computing,Distributed computer systems; Markov processes; Additional key word and phrasesmatrix analytic method; Analytic method; Distributed systems; Key words; Large-scales; Multithreaded; Performance; Performances analysis; Steady state; System use; Mean field theory
Using Cache or Credit for Parallel Ranking and Selection,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176143497&doi=10.1145%2f3618299&partnerID=40&md5=33555656e5caf80b55abfd0b5bb116e0,"In this article, we focus on ranking and selection procedures that sequentially allocate replications to systems by applying some acquisition function. We propose an acquisition function, called gCEI, which exploits the gradient of the complete expected improvement with respect to the number of replications. We prove that the gCEI procedure, which adopts gCEI as the acquisition function in a serial computing environment, achieves the asymptotically optimal static replication allocation of Glynn and Juneja in the limit under a normality assumption. We also propose two procedures, called caching and credit, that extend any acquisition-function-based procedure in a serial environment into both synchronous and asynchronous parallel environments. While allocating replications to systems, both procedures use persistence forecasts for the unavailable outputs of the currently running replications, but differ in usage of the available outputs. We prove that, under certain assumptions, the caching procedure achieves the same asymptotic allocation as in the serial environment. A similar result holds for the credit procedure using gCEI as the acquisition function. In terms of efficiency and effectiveness, the credit procedure empirically performs as well as the caching procedure, despite not carefully controlling the output history as the caching procedure does, and is faster than the serial version without any number-of-replications penalty due to using persistence forecasts. Both procedures are designed to solve small-to-medium-sized problems on computers with a modest number of processors, such as laptops and desktops as opposed to high-performance clusters, and are superior to state-of-the-art parallel procedures in this setting. © 2023 Association for Computing Machinery. All rights reserved.",parallel computing; ranking & selection; Simulation optimization,Asymptotically optimal; Asynchronous parallel; Computing environments; Expected improvements; Parallel com- puting; Ranking and selection; Ranking and selection procedures; Ranking selection; Serial computing; Simulation optimization; Mergers and acquisitions
Compositional Safe Approximation of Response Time Probability Density Function of Complex Workflows,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175993666&doi=10.1145%2f3591205&partnerID=40&md5=5a87d9c9ea9188262e5399f0ce48bd91,"We evaluate a stochastic upper bound on the response time Probability Density Function (PDF) of complex workflows through an efficient and accurate compositional approach. Workflows consist of activities having generally distributed stochastic durations with bounded supports, composed through sequence, choice/merge, and balanced/unbalanced split/join operators, possibly breaking the structure of well-formed nesting. Workflows are specified using a formalism defined in terms of Stochastic Time Petri Nets that permits decomposition into a hierarchy of subworkflows with positively correlated response times, guaranteeing that a stochastically larger end-To-end response time PDF is obtained when intermediate results are approximated by stochastically larger PDFs and when dependencies are simplified by replicating activities appearing in multiple subworkflows. In particular, an accurate stochastically larger PDF is obtained by combining shifted truncated Exponential terms with positive or negative rates. Experiments are performed on sets of manually and randomly generated models with increasing complexity, illustrating under which conditions different decomposition heuristics work well in terms of accuracy and complexity and showing that the proposed approach outperforms simulation having the same execution time.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesStochastic workflows; compositional evaluation; non-Markovian processes; randomly generated structured models; response time Probability Density Function; Stochastic Time Petri Nets,Function evaluation; Markov processes; Petri nets; Stochastic models; Stochastic systems; Additional key word and phrasesstochastic workflow; Compositional evaluation; Key words; Non-Markovian process; Randomly generated structured model; Response time probability density function; Stochastic time petri net; Stochastics; Structured modeling; Time Petri nets; Work-flows; Probability density function
SEH: Size Estimate Hedging Scheduling of Queues,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168757638&doi=10.1145%2f3580491&partnerID=40&md5=47a73ad2dee1aec2738a763b5fde920c,"For a single server system, Shortest Remaining Processing Time (SRPT) is an optimal size-based policy. In this article, we discuss scheduling a single-server system when exact information about the jobs' processing times is not available. When the SRPT policy uses estimated processing times, the underestimation of large jobs can significantly degrade performance. We propose an index-based policy with a single parameter, Size Estimate Hedging (SEH), that only uses estimated processing times for scheduling decisions. A job's priority is increased dynamically according to an SRPT rule until it is determined that it is underestimated, at which time the priority is frozen. Numerical results suggest that SEH has desirable performance for estimation error variance that is consistent with what is seen in practice.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesM/G/1; job size estimates,Additional key word and phrasesm/G/1; Job size; Job size estimate; Key words; Optimal size; Performance; Processing time; Server system; Shortest remaining processing time; Single server
Optimizing Reachability Probabilities for a Restricted Class of Stochastic Hybrid Automata via Flowpipe Construction,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174235958&doi=10.1145%2f3607197&partnerID=40&md5=6f649fca61a04cd31384ad9b6f13453f,"Stochastic hybrid automata (SHA) are a powerful tool to evaluate the dependability and safety of critical infrastructures. However, the resolution of nondeterminism, which is present in many purely hybrid models, is often only implicitly considered in SHA. This article instead proposes algorithms for computing maximum and minimum reachability probabilities for singular automata with urgent transitions and random clocks that follow arbitrary continuous probability distributions. We borrow a well-known approach from hybrid systems reachability analysis, namely flowpipe construction, which is then extended to optimize nondeterminism in the presence of random variables. First, valuations of random clocks that ensure reachability of specific goal states are extracted from the computed flowpipes, and second, reachability probabilities are computed by integrating over these valuations. We compute maximum and minimum probabilities for history-dependent prophetic and non-prophetic schedulers using set-based methods. The implementation featuring the library HyPro and the complexity of the approach are discussed in detail. Two case studies featuring nondeterministic choices show the feasibility of the approach.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesStochastic hybrid automata; cumulative distribution function; flowpipe construction,Automata theory; Distribution functions; Hybrid systems; Stochastic systems; Additional key word and phrasesstochastic hybrid automaton; Continuous probability distribution; Cumulative distribution function; Flowpipe construction; Hybrid automatons; Hybrid model; Key words; Non Determinism; Reachability; Stochastics; Clocks
NIM: Generative Neural Networks for Automated Modeling and Generation of Simulation Inputs,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168805392&doi=10.1145%2f3592790&partnerID=40&md5=12ababd6ecb2762ca52c0680935c4214,"Fitting stochastic input-process models to data and then sampling from them are key steps in a simulation study but highly challenging to non-experts. We present Neural Input Modeling (NIM), a Generative Neural Network (GNN) framework that exploits modern data-rich environments to automatically capture simulation input processes and then generate samples from them. The basic GNN that we develop, called NIM-VL, comprises (i) a variational autoencoder architecture that learns the probability distribution of the input data while avoiding overfitting and (ii) long short-term memory components that concisely capture statistical dependencies across time. We show how the basic GNN architecture can be modified to exploit known distributional properties - such as independent and identically distributed structure, nonnegativity, and multimodality - to increase accuracy and speed, as well as to handle multivariate processes, categorical-valued processes, and extrapolation beyond the training data for certain nonstationary processes. We also introduce an extension to NIM called Conditional Neural Input Modeling (CNIM), which can learn from training data obtained under various realizations of a (possibly time series valued) stochastic ""condition,""such as temperature or inflation rate, and then generate sample paths given a value of the condition not seen in the training data. This enables users to simulate a system under a specific working condition by customizing a pre-trained model; CNIM also facilitates what-if analysis. Extensive experiments show the efficacy of our approach. NIM can thus help overcome one of the key barriers to simulation for non-experts.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesInput modeling; distribution fitting; neural networks,Network architecture; Stochastic models; Stochastic systems; Additional key word and phrasesinput modeling; Automated generation; Automated modelling; Condition; Distribution fitting; Input models; Key words; Learn+; Neural-networks; Training data; Probability distributions
Learning to Simulate Sequentially Generated Data via Neural Networks and Wasserstein Training,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168799705&doi=10.1145%2f3583070&partnerID=40&md5=c68a7a7a08ec333292d0a25b65386a41,"We propose a new framework of a neural network-assisted sequential structured simulator to model, estimate, and simulate a wide class of sequentially generated data. Neural networks are integrated into the sequentially structured simulators in order to capture potential nonlinear and complicated sequential structures. Given representative real data, the neural network parameters in the simulator are estimated and calibrated through a Wasserstein training process, without restrictive distributional assumptions. The target of Wasserstein training is to enforce the joint distribution of the simulated data to match the joint distribution of the real data in terms of Wasserstein distance. Moreover, the neural network-assisted sequential structured simulator can flexibly incorporate various kinds of elementary randomness and generate distributions with certain properties such as heavy-tail, without the need to redesign the estimation and training procedures. Further, regarding statistical properties, we provide results on consistency and convergence rate for the estimation procedure of the proposed simulator, which are the first set of results that allow the training data samples to be correlated. We then present numerical experiments with synthetic and real data sets to illustrate the performance of the proposed simulator and estimation procedure.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSequential simulator; neural network; statistical properties; Wasserstein training,Additional key word and phrasessequential simulator; Estimation procedures; Joint distributions; Key words; Model estimates; Neural network parameters; Neural-networks; Sequential structure; Statistical properties; Wasserstein training
Uncertainty-aware Simulation of Adaptive Systems,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168795857&doi=10.1145%2f3589517&partnerID=40&md5=d6fcc87facd691b47d0a5849c7605493,"Adaptive systems manage and regulate the behavior of devices or other systems using control loops to automatically adjust the value of some measured variables to equal the value of a desired set-point. These systems normally interact with physical parts or operate in physical environments, where uncertainty is unavoidable. Traditional approaches to manage that uncertainty use either robust control algorithms that consider bounded variations of the uncertain variables and worst-case scenarios or adaptive control methods that estimate the parameters and change the control laws accordingly. In this article, we propose to include the sources of uncertainty in the system models as first-class entities using random variables to simulate adaptive and control systems more faithfully, including not only the use of random variables to represent and operate with uncertain values but also to represent decisions based on their comparisons. Two exemplar systems are used to illustrate and validate our proposal.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesModel-based software engineering; control systems; self-adaptive systems; uncertainty,Adaptive control systems; Computer software; Robust control; Uncertainty analysis; Additional key word and phrasesmodel-based software engineering; Bounded variations; Control loop; Key words; Physical environments; Self-adaptive system; Setpoints; Traditional approaches; Uncertain variables; Uncertainty; Random variables
"Replication of Computational Results Report for ""Automatic Reuse, Adaption, and Execution of Simulation Experiments via Provenance Patterns""",2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149410529&doi=10.1145%2f3577007&partnerID=40&md5=5985e732d0b2db597f773a5a649e365c,"In this article, a reproducibility study is presented, with reference to the computational results reported in the article ""Automatic Reuse, Adaption, and Execution of Simulation Experiments via Provenance Patterns,""by P. Wilsdorf, A. Wolpers, J. Hilton, F. Haack, and A. M. Uhrmacher. Based on the achieved results, the Artifacts Available badge is assigned. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesReproducibility; graph patterns; provenance; reuse; Simulation Experiment; simulation study,Additional key word and phrasesreproducibility; Computational results; Graph patterns; Key words; Provenance; Reproducibilities; Reuse; Simulation experiment; Simulation studies; Computer programming
Efficient Simulation of Sparse Graphs of Point Processes,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149407757&doi=10.1145%2f3565809&partnerID=40&md5=380316f43832cec3df7c9e44cfa8e0d7,"We derive new discrete event simulation algorithms for marked time point processes. The main idea is to couple a special structure, namely the associated local independence graph, as defined by Didelez, with the activity tracking algorithm of Muzy for achieving high-performance asynchronous simulations. With respect to classical algorithms, this allows us to drastically reduce the computational complexity, especially when the graph is sparse. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",computational complexity; discrete event simulation; Hawkes point processes; local independent graphs; Point processes,Discrete event simulation; Graph theory; Discrete-event simulations; Efficient simulation; Hawkes point process; Local independent graph; Point process; Simulation algorithms; Sparse graphs; Special structure; Time points; Computational complexity
"Automatic Reuse, Adaption, and Execution of Simulation Experiments via Provenance Patterns",2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149407346&doi=10.1145%2f3564928&partnerID=40&md5=43377ffd4be19811184a077a13399560,"Simulation experiments are typically conducted repeatedly during the model development process, for example, to revalidate if a behavioral property still holds after several model changes. Approaches for automatically reusing and generating simulation experiments can support modelers in conducting simulation studies in a more systematic and effective manner. They rely on explicit experiment specifications and, so far, on user interaction for initiating the reuse. Thereby, they are constrained to support the reuse of simulation experiments in a specific setting. Our approach now goes one step further by automatically identifying and adapting the experiments to be reused for a variety of scenarios. To achieve this, we exploit provenance graphs of simulation studies, which provide valuable information about the previous modeling and experimenting activities, and contain meta-information about the different entities that were used or produced during the simulation study. We define provenance patterns and associate them with a semantics, which allows us to interpret the different activities and construct transformation rules for provenance graphs. Our approach is implemented in a Reuse and Adapt framework for Simulation Experiments (RASE), which can interface with various modeling and simulation tools. In the case studies, we demonstrate the utility of our framework for (1) the repeated sensitivity analysis of an agent-based model of migration routes and (2) the cross-validation of two models of a cell signaling pathway. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph patterns; provenance; reuse; Simulation experiment; simulation study,Autonomous agents; Computational methods; Semantics; Sensitivity analysis; Simulation platform; User interfaces; Behavioral properties; Development process; Graph patterns; Model change; Model development; Provenance; Reuse; Simulation experiment; Simulation studies; User interaction; Cell signaling
Estimating Multiclass Service Demand Distributions Using Markovian Arrival Processes,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149390774&doi=10.1145%2f3570924&partnerID=40&md5=914b73a10a72e15da16637033e2c611e,"Building performance models for software services in DevOps is costly and error-prone. Accurate service demand distribution estimation is critical to precisely modeling queueing behaviors and performance prediction. However, current estimation methods focus on capturing the mean service demand, disregarding higher-order moments of the distribution that still can largely affect prediction accuracy. To address this limitation, we propose to estimate higher moments of the service demand distribution for a microservice from monitoring traces. We first generate a closed queueing model to abstract software performance and use it to model the departure process of requests completed by the software service as a Markovian arrival process (MAP). This allows formulating the estimation of service demand into an optimization problem, which aims to find the first multiple moments of the service demand distribution that maximize the likelihood of the MAP using generated the measured inter-departure times. We then estimate the service demand distribution for different classes of service with a maximum likelihood algorithm and novel heuristics to mitigate the computational cost of the optimization process for scalability. We apply our method to real traces from a microservice-based application and demonstrate that its estimations lead to greater prediction accuracy than exponential distributions assumed in traditional service demand estimation approaches for software services. © 2023 Association for Computing Machinery.",Markovian arrival process; maximum likelihood estimation; performance; queueing models; Service demand distribution,Application programs; Forecasting; Markov processes; Optimization; Queueing theory; Demand distribution; Markovian arrival process; Maximum-likelihood estimation; Multi-class service; Performance; Prediction accuracy; Queueing model; Service demand; Service demand distribution; Software services; Maximum likelihood estimation
Simulating the Impact of Dynamic Rerouting on Metropolitan-scale Traffic Systems,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149415567&doi=10.1145%2f3579842&partnerID=40&md5=4dfb2048d83ad3ae73ca4db816b6b318,"The rapid introduction of mobile navigation aides that use real-Time road network information to suggest alternate routes to drivers is making it more difficult for researchers and government transportation agencies to understand and predict the dynamics of congested transportation systems. Computer simulation is a key capability for these organizations to analyze hypothetical scenarios; however, the complexity of transportation systems makes it challenging for them to simulate very large geographical regions, such as multi-city metropolitan areas. In this article, we describe enhancements to the Mobiliti parallel traffic simulator to model dynamic rerouting behavior with the addition of vehicle controller actors and vehicle-To-controller reroute requests. The simulator is designed to support distributed-memory parallel execution using discrete event simulation and be scalable on high-performance computing platforms. We demonstrate the potential of the simulator by analyzing the impact of varying the population penetration rate of dynamic rerouting on the San Francisco Bay Area road network. Using high-performance parallel computing, we can simulate a day in the San Francisco Bay Area with 19 million vehicle trips with 50 percent dynamic rerouting penetration over a road network with 0.5 million nodes and 1 million links in less than three minutes. We present a sensitivity study on the dynamic rerouting parameters, discuss the simulator's parallel scalability, and analyze system-level impacts of changing the dynamic rerouting penetration. Furthermore, we examine the varying effects on different functional classes and geographical regions and present a validation of the simulation results compared to real-world data. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",actor-based modeling; dynamic vehicle rerouting; high-performance computing; Large-scale transportation simulation; parallel discrete event simulation,Discrete event simulation; Motor transportation; Real time systems; Roads and streets; Simulation platform; Transportation routes; Travel time; Vehicle actuated signals; Actor-based modeling; Dynamic rerouting; Dynamic vehicle rerouting; High-performance computing; Large-scale transportation; Large-scale transportation simulation; Parallel discrete-event simulation; Performance computing; Road network; Transportation simulations; Geographical regions
Batching Adaptive Variance Reduction,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149388122&doi=10.1145%2f3573386&partnerID=40&md5=abb0d8504db718ddd6a6d41dcc0528a6,"Adaptive Monte Carlo variance reduction is an effective framework for running a Monte Carlo simulation along with a parameter search algorithm for variance reduction, whereas an initialization step is required for preparing problem parameters in some instances. In spite of the effectiveness of adaptive variance reduction in various fields of application, the length of the preliminary phase has often been left unspecified for the user to determine on a case-by-case basis, much like in typical sequential frameworks. This uncertain element may possibly be even fatal in realistic finite-budget situations, since the pilot run may take most of the budget, or possibly use up all of it. To unnecessitate such an ad hoc initialization step, we develop a batching procedure in adaptive variance reduction, and provide an implementable formula of the learning rate in the parameter search which minimizes an upper bound of the theoretical variance of the empirical batch mean. We analyze decay rates of the minimized upper bound towards the minimal estimator variance with respect to the predetermined computing budget, and provide convergence results as the computing budget increases progressively when the batch size is fixed. Numerical examples are provided to support theoretical findings and illustrate the effectiveness of the proposed batching procedure. © 2023 Association for Computing Machinery.",batching; control variates; importance sampling; stochastic approximation; Variance reduction,Budget control; Decay (organic); Intelligent systems; Stochastic systems; Batching; Computing budget; Control variates; Initialization step; Monte Carlo's simulation; Problem parameters; Search Algorithms; Stochastic approximations; Upper Bound; Variance reductions; Importance sampling
A Personality-based Model of Emotional Contagion and Control in Crowd Queuing Simulations,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149358060&doi=10.1145%2f3577589&partnerID=40&md5=d0074743f2bc3f1588913c71a16c40f2,"Queuing is a frequent daily activity. However, long waiting lines equate to frustration and potential safety hazards. We present a novel, personality-based model of emotional contagion and control for simulating crowd queuing. Our model integrates the influence of individual personalities and interpersonal relationships. Through the interaction between the agents and the external environment parameters, the emotional contagion model based on well-known theories in psychology is used to complete the agents' behavior planning and path planning function. We combine the epidemiological SIR model with the cellular automaton model to capture various emotional modelling for multi-Agent simulations. The overall formulation involves different emotional parameters, such as patience, urgency, and friendliness, closely related to crowd queuing. In addition, to manage the order of the queue, governing agents are added to prevent the emotional outbreak. We perform qualitative and quantitative comparisons between our simulation results and real-world observations on various scenarios. Numerous experiments show that reasonably increasing the queue channel and adding governing agents can effectively improve the quality of queues. © 2023 Association for Computing Machinery.",Crowd simulation; emotional contagion; queue management,Multi agent systems; Queueing theory; Crowd Simulation; Daily activity; Emotional contagion; External environments; Interpersonal relationship; Model-based OPC; Potential safety hazards; Queue management; Queuing simulation; Waiting lines; Motion planning
"Virtual Time III, Part 1: Unified Virtual Time Synchronization for Parallel Discrete Event Simulation",2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149171103&doi=10.1145%2f3505248&partnerID=40&md5=f25ac9100f1e71318870c846c1d53394,"Algorithms for synchronization of parallel discrete event simulation have historically been divided between conservative methods that require lookahead but not rollback, and optimistic methods that require rollback but not lookahead. In this paper we present a new approach in the form of a framework called Unified Virtual Time (UVT) that unifies the two approaches, combining the advantages of both within a single synchronization theory. Whenever timely lookahead information is available, a logical process (LP) executes conservatively using an irreversible event handler. When lookahead information is not available the LP does not block, as it would in a classical conservative execution, but instead executes optimistically using a reversible event handler. The switch from conservative to optimistic synchronization and back is decided on an event-by-event basis by the simulator, transparently to the model code. UVT treats conservative synchronization algorithms as optional accelerators for an underlying optimistic synchronization algorithm, enabling the speed of conservative execution whenever it is applicable, but otherwise falling back on the generality of optimistic execution. We describe UVT in a novel way, based on fundamental invariants, monotonicity requirements, and synchronization rules. UVT permits zero-delay messages and pays careful attention to tie-handling using superposition. We prove that under fairly general conditions a UVT simulation always makes progress in virtual time.This is Part 1 of a trio of papers describing the UVT framework for PDES, mixing conservative and optimistic synchronization and integrating throttling control.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",antimessage; conservative; invariant; lookahead; monotonicity; optimistic; Parallel discrete event simulation; rollback; synchronization; throttling; Unified Virtual Time; UVT; virtual time,Discrete event simulation; Antimessage; Conservative; Invariant; Lookahead; Monotonicity; Optimistics; Parallel discrete-event simulation; Rollback; Throttling; Unified virtual time; Virtual-time; Synchronization
Towards Differentiable Agent-Based Simulation,2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149167181&doi=10.1145%2f3565810&partnerID=40&md5=c302fcc570a2231989936596008ea25a,"Simulation-based optimization using agent-based models is typically carried out under the assumption that the gradient describing the sensitivity of the simulation output to the input cannot be evaluated directly. To still apply gradient-based optimization methods, which efficiently steer the optimization towards a local optimum, gradient estimation methods can be employed. However, many simulation runs are needed to obtain accurate estimates if the input dimension is large. Automatic differentiation (AD) is a family of techniques to compute gradients of general programs directly. Here, we explore the use of AD in the context of time-driven agent-based simulations. By substituting common discrete model elements such as conditional branching with smooth approximations, we obtain gradient information across discontinuities in the model logic. On the examples of a synthetic grid-based model, an epidemics model, and a microscopic traffic model, we study the fidelity and overhead of the differentiable simulations as well as the convergence speed and solution quality achieved by gradient-based optimization compared with gradient-free methods. In traffic signal timing optimization problems with high input dimension, the gradient-based methods exhibit substantially superior performance. A further increase in optimization progress is achieved by combining gradient-free and gradient-based methods. We demonstrate that the approach enables gradient-based training of neural network-controlled simulation entities embedded in the model logic. Finally, we show that the performance overhead of differentiable agent-based simulations can be reduced substantially by exploiting sparsity in the model logic.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Agent-based simulation; Backpropagation; optimization,Autonomous agents; Backpropagation; Computational methods; Computer circuits; Simulation platform; Traffic signals; Agent based simulation; Agent-based model; Automatic differentiations; Gradient-based method; Input dimensions; Model logic; Optimisations; Performance; Simulation outputs; Simulation-based optimizations; Optimization
"Virtual Time III, Part 2: Combining Conservative and Optimistic Synchronization",2023,ACM Transactions on Modeling and Computer Simulation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149167445&doi=10.1145%2f3505249&partnerID=40&md5=dbb4bbd2431bc8ee9f64ad7e45adb4c5,"This is Part 2 of a trio of works intended to provide a unifying framework in which conservative and optimistic synchronization for parallel discrete event simulations can be freely and transparently combined in the same logical process on an event-by-event basis. In this article, we continue the outline of an approach called Unified Virtual Time (UVT) that was introduced in Part 1, showing in detail via two extended examples how conservative synchronization can be refactored and combined with optimistic synchronization in the UVT framework. We describe UVT versions of both a basic time windowing algorithm called Unified Simple Time Windows and a refactored version of the Chandy-Misra-Bryant Null Message algorithm called Unified CMB.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",antimessage; conservative; invariant; lookahead; monotonicity; optimistic; Parallel discrete event simulation; rollback; synchronization; throttling; Unified Virtual Time; UVT; virtual time,Discrete event simulation; Distributed computer systems; Antimessage; Conservative; Invariant; Lookahead; Monotonicity; Optimistics; Parallel discrete-event simulation; Rollback; Throttling; Unified virtual time; Virtual-time; Synchronization
